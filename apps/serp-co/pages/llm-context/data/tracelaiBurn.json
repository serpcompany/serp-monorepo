[
  {
    "owner": "tracel-ai",
    "repo": "burn",
    "content": "TITLE: Implementing the Neural Network Forward Pass with Burn in Rust\nDESCRIPTION: Defines the `forward` method for the neural network model, implementing the shape transformation and sequential data flow for an image input tensor. The method first reshapes the input to include a channel dimension, then applies two convolutions (with dropout in-between), a ReLU activation, adaptive pooling to a fixed size, and finally flattens before passing through two linear layers, with activations and dropout as regularization. Dependencies include Burn's layer and tensor modules and an appropriately initialized model. The input is a 3D tensor of images ([batch_size, height, width]); output is a 2D tensor ([batch_size, num_classes]). Key constraints: input shapes must match expectations, and all intermediate sizes must align with layer configurations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: Backend> Model<B> {\\n    /// # Shapes\\n    ///   - Images [batch_size, height, width]\\n    ///   - Output [batch_size, num_classes]\\n    pub fn forward(&self, images: Tensor<B, 3>) -> Tensor<B, 2> {\\n        let [batch_size, height, width] = images.dims();\\n\\n        // Create a channel at the second dimension.\\n        let x = images.reshape([batch_size, 1, height, width]);\\n\\n\\n        let x = self.conv1.forward(x); // [batch_size, 8, _, _]\\n        let x = self.dropout.forward(x);\\n        let x = self.conv2.forward(x); // [batch_size, 16, _, _]\\n        let x = self.dropout.forward(x);\\n        let x = self.activation.forward(x);\\n\\n        let x = self.pool.forward(x); // [batch_size, 16, 8, 8]\\n        let x = x.reshape([batch_size, 16 * 8 * 8]);\\n        let x = self.linear1.forward(x);\\n        let x = self.dropout.forward(x);\\n        let x = self.activation.forward(x);\\n\\n        self.linear2.forward(x) // [batch_size, num_classes]\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Training a Model with Configurable Learner and Dataloaders in Rust\nDESCRIPTION: Implements the end-to-end training process for a machine learning model using type-generic backends, configurable via a TrainingConfig struct. Functionally, it ensures artifact directory preparation, serializes the run config, seeds reproducibility, initializes train/test dataloaders (here using the MNIST dataset and a batcher), constructs a Learner with configurable metrics and checkpointing, and executes the training loop. Trained weights are saved in a persistent, portable format using the CompactRecorder, and all parameters are type-driven for flexibility and safety. Dependencies include trait bounds for `AutodiffBackend`, existing config types, loaders, batchers, metrics, recorders, and model/optimizer builders. Requires prior definition of the types and integration with a machine learning framework supporting these abstractions. Inputs: artifact directory, configuration struct, execution device; Output: trained model persisted to disk.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/training.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\npub fn train<B: AutodiffBackend>(artifact_dir: &str, config: TrainingConfig, device: B::Device) {\n    create_artifact_dir(artifact_dir);\n    config\n        .save(format!(\"{artifact_dir}/config.json\"))\n        .expect(\"Config should be saved successfully\");\n\n    B::seed(config.seed);\n\n    let batcher = MnistBatcher::default();\n\n    let dataloader_train = DataLoaderBuilder::new(batcher.clone())\n        .batch_size(config.batch_size)\n        .shuffle(config.seed)\n        .num_workers(config.num_workers)\n        .build(MnistDataset::train());\n\n    let dataloader_test = DataLoaderBuilder::new(batcher)\n        .batch_size(config.batch_size)\n        .shuffle(config.seed)\n        .num_workers(config.num_workers)\n        .build(MnistDataset::test());\n\n    let learner = LearnerBuilder::new(artifact_dir)\n        .metric_train_numeric(AccuracyMetric::new())\n        .metric_valid_numeric(AccuracyMetric::new())\n        .metric_train_numeric(LossMetric::new())\n        .metric_valid_numeric(LossMetric::new())\n        .with_file_checkpointer(CompactRecorder::new())\n        .devices(vec![device.clone()])\n        .num_epochs(config.num_epochs)\n        .summary()\n        .build(\n            config.model.init::<B>(&device),\n            config.optimizer.init(),\n            config.learning_rate,\n        );\n\n    let model_trained = learner.fit(dataloader_train, dataloader_test);\n\n    model_trained\n        .save_file(format!(\"{artifact_dir}/model\"), &CompactRecorder::new())\n        .expect(\"Trained model should be saved successfully\");\n}\n```\n\n----------------------------------------\n\nTITLE: Mapping Basic Tensor Operations - Markdown Table - Markdown\nDESCRIPTION: This snippet provides a Markdown table listing common tensor operations in Burn and their respective equivalents in the PyTorch library. It covers creation, query, manipulation, and assignment functions for tensors of types Int, Float, and Bool. The table serves as a migration guide and feature equivalence map for users familiar with both libraries. No external dependencies are required beyond a Markdown renderer. There is no code execution; the snippet is intended for documentation purposes only.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n### Basic Operations\n\nThose operations are available for all tensor kinds: `Int`, `Float`, and `Bool`.\n\n| Burn                                        | PyTorch Equivalent                                                        |\n| ------------------------------------------- | ------------------------------------------------------------------------- |\n| `Tensor::cat(tensors, dim)`                 | `torch.cat(tensors, dim)`                                                 |\n| `Tensor::empty(shape, device)`              | `torch.empty(shape, device=device)`                                       |\n| `Tensor::from_primitive(primitive)`         | N/A                                                                       |\n| `Tensor::stack(tensors, dim)`               | `torch.stack(tensors, dim)`                                               |\n| `tensor.all()`                              | `tensor.all()`                                                            |\n| `tensor.all_dim(dim)`                       | `tensor.all(dim)`                                                         |\n| `tensor.any()`                              | `tensor.any()`                                                            |\n| `tensor.any_dim(dim)`                       | `tensor.any(dim)`                                                         |\n| `tensor.chunk(num_chunks, dim)`             | `tensor.chunk(num_chunks, dim)`                                           |\n| `tensor.split(split_size, dim)`             | `tensor.split(split_size, dim)`                                           |\n| `tensor.split_with_sizes(split_sizes, dim)` | `tensor.split([split_sizes], dim)`                                        |\n| `tensor.device()`                           | `tensor.device`                                                           |\n| `tensor.dtype()`                            | `tensor.dtype`                                                            |\n| `tensor.dims()`                             | `tensor.size()`                                                           |\n| `tensor.equal(other)`                       | `x == y`                                                                  |\n| `tensor.expand(shape)`                      | `tensor.expand(shape)`                                                    |\n| `tensor.flatten(start_dim, end_dim)`        | `tensor.flatten(start_dim, end_dim)`                                      |\n| `tensor.flip(axes)`                         | `tensor.flip(axes)`                                                       |\n| `tensor.into_data()`                        | N/A                                                                       |\n| `tensor.into_primitive()`                   | N/A                                                                       |\n| `tensor.into_scalar()`                      | `tensor.item()`                                                           |\n| `tensor.narrow(dim, start, length)`         | `tensor.narrow(dim, start, length)`                                       |\n| `tensor.not_equal(other)`                   | `x != y`                                                                  |\n| `tensor.permute(axes)`                      | `tensor.permute(axes)`                                                    |\n| `tensor.movedim(src, dst)`                  | `tensor.movedim(src, dst)`                                                |\n| `tensor.repeat_dim(dim, times)`             | `tensor.repeat(*[times if i == dim else 1 for i in range(tensor.dim())])` |\n| `tensor.repeat(sizes)`                      | `tensor.repeat(sizes)`                                                    |\n| `tensor.reshape(shape)`                     | `tensor.view(shape)`                                                      |\n| `tensor.shape()`                            | `tensor.shape`                                                            |\n| `tensor.slice(ranges)`                      | `tensor[(*ranges,)]`                                                      |\n| `tensor.slice_assign(ranges, values)`       | `tensor[(*ranges,)] = values`                                             |\n| `tensor.squeeze(dim)`                       | `tensor.squeeze(dim)`                                                     |\n| `tensor.swap_dims(dim1, dim2)`              | `tensor.transpose(dim1, dim2)`                                            |\n| `tensor.to_data()`                          | N/A                                                                       |\n| `tensor.to_device(device)`                  | `tensor.to(device)`                                                       |\n| `tensor.transpose()`                        | `tensor.T`                                                                |\n| `tensor.unsqueeze()`                        | `tensor.unsqueeze(0)`                                                     |\n| `tensor.unsqueeze_dim(dim)`                 | `tensor.unsqueeze(dim)`                                                   |\n| `tensor.unsqueeze_dims(dims)`               | N/A                                                                       |\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Forward Pass for Classification - Burn - Rust\nDESCRIPTION: Defines a method for the Model struct that performs a forward pass on image tensors and computes cross-entropy loss against target tensors. It wraps the resulting loss, model output, and targets in a ClassificationOutput, which can be used for training metrics and further computation. Dependencies include the model definition, Burn's CrossEntropyLossConfig, and tensor abstractions; required parameters are image and target tensors, and the output is a ClassificationOutput object.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/training.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: Backend> Model<B> {\n    pub fn forward_classification(\n        &self,\n        images: Tensor<B, 3>,\n        targets: Tensor<B, 1, Int>,\n    ) -> ClassificationOutput<B> {\n        let output = self.forward(images);\n        let loss = CrossEntropyLossConfig::new()\n            .init(&output.device())\n            .forward(output.clone(), targets.clone());\n\n        ClassificationOutput::new(loss, output, targets)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Batcher Trait for MNIST Data Processing in Burn\nDESCRIPTION: Implements the Batcher trait for MnistBatcher to convert a collection of MnistItems into a batch of tensors. The implementation normalizes images, converts labels to tensors, and concatenates them into batches suitable for model training.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/data.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Clone, Debug)]\npub struct MnistBatch<B: Backend> {\n    pub images: Tensor<B, 3>,\n    pub targets: Tensor<B, 1, Int>,\n}\n\nimpl<B: Backend> Batcher<B, MnistItem, MnistBatch<B>> for MnistBatcher {\n    fn batch(&self, items: Vec<MnistItem>, device: &B::Device) -> MnistBatch<B> {\n        let images = items\n            .iter()\n            .map(|item| TensorData::from(item.image).convert::<B::FloatElem>())\n            .map(|data| Tensor::<B, 2>::from_data(data, device))\n            .map(|tensor| tensor.reshape([1, 28, 28]))\n            // Normalize: scale between [0,1] and make the mean=0 and std=1\n            // values mean=0.1307,std=0.3081 are from the PyTorch MNIST example\n            // https://github.com/pytorch/examples/blob/54f4572509891883a947411fd7239237dd2a39c3/mnist/main.py#L122\n            .map(|tensor| ((tensor / 255) - 0.1307) / 0.3081)\n            .collect();\n\n        let targets = items\n            .iter()\n            .map(|item| {\n                Tensor::<B, 1, Int>::from_data([(item.label as i64).elem::<B::IntElem>()], device)\n            })\n            .collect();\n\n        let images = Tensor::cat(images, 0);\n        let targets = Tensor::cat(targets, 0);\n\n        MnistBatch { images, targets }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Trained Burn Model in Rust\nDESCRIPTION: This Rust function demonstrates end-to-end inference with a pre-trained Burn model by loading model configuration and weights from artifact files, initializing the model, batching a single MNIST item, and executing a forward pass to predict its label. Dependencies include the Burn machine learning framework and definitions for MnistBatcher, MnistItem, and TrainingConfig. Inputs required are the path to the artifact directory, the computation device, and the MnistItem to infer, while outputs are printed predictions and expected labels. The method efficiently manages device placement and ensures artifact presence, but assumes artifacts are present and compatible with the model structure.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/inference.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n# use crate::{data::MnistBatcher, training::TrainingConfig};\\n# use burn::{\\n#     data::{dataloader::batcher::Batcher, dataset::vision::MnistItem},\\n#     prelude::*,\\n#     record::{CompactRecorder, Recorder},\\n# };\\n# \\npub fn infer<B: Backend>(artifact_dir: &str, device: B::Device, item: MnistItem) {\\n    let config = TrainingConfig::load(format!(\\\"{artifact_dir}/config.json\\\"))\\n        .expect(\\\"Config should exist for the model; run train first\\\");\\n    let record = CompactRecorder::new()\\n        .load(format!(\\\"{artifact_dir}/model\\\").into(), &device)\\n        .expect(\\\"Trained model should exist; run train first\\\");\\n\\n    let model = config.model.init::<B>(&device).load_record(record);\\n\\n    let label = item.label;\\n    let batcher = MnistBatcher::default();\\n    let batch = batcher.batch(vec![item], &device);\\n    let output = model.forward(batch.images);\\n    let predicted = output.argmax(1).flatten::<1>(0, 1).into_scalar();\\n\\n    println!(\\\"Predicted {} Expected {}\\\", predicted, label);\\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Model Configuration and Initialization with Burn in Rust\nDESCRIPTION: Defines the configuration structure (`ModelConfig`) for a neural network using Burn, including the derivation of the Config trait for serialization and the Debug trait for printing. The `init<B: Backend>(&self, device: &B::Device) -> Model<B>` method initializes a model instance by creating and initializing each neural network layer using the parameters from ModelConfig, passing the device as a reference. Dependencies include the Burn crate and its neural network modules. Key parameters like `num_classes`, `hidden_size`, and `dropout` control the architectural shape and regularization behavior of the network; default values (such as dropout) are specified using attribute macros. Inputs are the configuration fields, and outputs are an instantiated Model parameterized by Backend. Limitations include the need for a compatible Backend implementation and that only fields without defaults are required during construction.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Config, Debug)]\\npub struct ModelConfig {\\n    num_classes: usize,\\n    hidden_size: usize,\\n    #[config(default = \\\"0.5\\\")]\\n    dropout: f64,\\n}\\n\\nimpl ModelConfig {\\n    /// Returns the initialized model.\\n    pub fn init<B: Backend>(&self, device: &B::Device) -> Model<B> {\\n        Model {\\n            conv1: Conv2dConfig::new([1, 8], [3, 3]).init(device),\\n            conv2: Conv2dConfig::new([8, 16], [3, 3]).init(device),\\n            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),\\n            activation: Relu::new(),\\n            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init(device),\\n            linear2: LinearConfig::new(self.hidden_size, self.num_classes).init(device),\\n            dropout: DropoutConfig::new(self.dropout).init(),\\n        }\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Various Tensor Initialization Methods in Burn\nDESCRIPTION: Demonstrates different ways to initialize tensors in Burn, including from specific backends, generic backends, float arrays, integer slices, and custom types.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n// Initialization from a given Backend (Wgpu)\nlet tensor_1 = Tensor::<Wgpu, 1>::from_data([1.0, 2.0, 3.0], &device);\n\n// Initialization from a generic Backend\nlet tensor_2 = Tensor::<Backend, 1>::from_data(TensorData::from([1.0, 2.0, 3.0]), &device);\n\n// Initialization using from_floats (Recommended for f32 ElementType)\n// Will be converted to TensorData internally.\nlet tensor_3 = Tensor::<Backend, 1>::from_floats([1.0, 2.0, 3.0], &device);\n\n// Initialization of Int Tensor from array slices\nlet arr: [i32; 6] = [1, 2, 3, 4, 5, 6];\nlet tensor_4 = Tensor::<Backend, 1, Int>::from_data(TensorData::from(&arr[0..3]), &device);\n\n// Initialization from a custom type\n\nstruct BodyMetrics {\n    age: i8,\n    height: i16,\n    weight: f32\n}\n\nlet bmi = BodyMetrics{\n        age: 25,\n        height: 180,\n        weight: 80.0\n    };\nlet data  = TensorData::from([bmi.age as f32, bmi.height as f32, bmi.weight]);\nlet tensor_5 = Tensor::<Backend, 1>::from_data(data, &device);\n```\n\n----------------------------------------\n\nTITLE: Using Derive Macro to Create a Custom Burn Module - Rust\nDESCRIPTION: Illustrates how to use the #[derive(Module, Debug)] macro for a custom module struct in Burn. This enables automatic trait implementation for both serialization/debug features and Burn-specific module behavior, making the struct suitable for inclusion as a trainable model component. Only types that derive Burnâ€™s Module trait and are Debug-printable are supported. The snippet also demonstrates idiomatic field composition.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Module, Debug)]\\npub struct MyCustomModule<B: Backend> {\\n    linear1: Linear<B>,\\n    linear2: Linear<B>,\\n    activation: Relu,\\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Model Inference Post-Training with Burn in Rust\nDESCRIPTION: This Rust code snippet, intended for 'main.rs', demonstrates how to invoke the training function followed by the inference function using a specific backend setup in a Burn-based application. The code manages types for both standard and autodiff backends, prepares device and artifact paths, runs training with explicit model and optimizer configurations, and uses the trained model to infer the label of a specific test item from the MNIST dataset. Usage requires the dataset, device support, and pre-existing modules for training and inference; it expects to output both the predicted and true label for validation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/inference.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n# mod data;\\n# mod inference;\\n# mod model;\\n# mod training;\\n# \\n# use crate::{model::ModelConfig, training::TrainingConfig};\\n# use burn::{\\n#     backend::{Autodiff, Wgpu},\\n#     data::dataset::Dataset,\\n#     optim::AdamConfig,\\n# };\\n# \\n# fn main() {\\n#     type MyBackend = Wgpu<f32, i32>;\\n#     type MyAutodiffBackend = Autodiff<MyBackend>;\\n# \\n#     let device = burn::backend::wgpu::WgpuDevice::default();\\n#     let artifact_dir = \"/tmp/guide\";\\n#     crate::training::train::<MyAutodiffBackend>(\\n#         artifact_dir,\\n#         TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),\\n#         device.clone(),\\n#     );\\n    crate::inference::infer::<MyBackend>(\\n        artifact_dir,\\n        device,\\n        burn::data::dataset::vision::MnistDataset::test()\\n            .get(42)\\n            .unwrap(),\\n    );\\n# }\n```\n\n----------------------------------------\n\nTITLE: Defining Training Configuration and Initialization (Burn, Rust)\nDESCRIPTION: This code defines a configuration struct for MNIST training using Burn's configuration macros, allowing users to specify common hyperparameters (epochs, batch size, workers, seed, learning rate, model, optimizer). The runnable function initializes the backend seed, model, optimizer, batchers, and data loaders for training and testing, readying all components for a training loop. Dependencies include Burn's autodiff backend, model and optimizer configuration types, and DataLoaderBuilder with MnistDataset and MnistBatcher types. Inputs are device and configuration data; outputs are instantiated loaders, models, and optimizer instances. The snippet relies on prior implementation of model, optimizer, and batcher configs.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Config)]\npub struct MnistTrainingConfig {\n    #[config(default = 10)]\n    pub num_epochs: usize,\n    #[config(default = 64)]\n    pub batch_size: usize,\n    #[config(default = 4)]\n    pub num_workers: usize,\n    #[config(default = 42)]\n    pub seed: u64,\n    #[config(default = 1e-4)]\n    pub lr: f64,\n    pub model: ModelConfig,\n    pub optimizer: AdamConfig,\n}\n\npub fn run<B: AutodiffBackend>(device: &B::Device) {\n    // Create the configuration.\n    let config_model = ModelConfig::new(10, 1024);\n    let config_optimizer = AdamConfig::new();\n    let config = MnistTrainingConfig::new(config_model, config_optimizer);\n\n    B::seed(config.seed);\n\n    // Create the model and optimizer.\n    let mut model = config.model.init::<B>(&device);\n    let mut optim = config.optimizer.init();\n\n    // Create the batcher.\n    let batcher = MnistBatcher::default();\n\n    // Create the dataloaders.\n    let dataloader_train = DataLoaderBuilder::new(batcher.clone())\n        .batch_size(config.batch_size)\n        .shuffle(config.seed)\n        .num_workers(config.num_workers)\n        .build(MnistDataset::train());\n\n    let dataloader_test = DataLoaderBuilder::new(batcher)\n        .batch_size(config.batch_size)\n        .shuffle(config.seed)\n        .num_workers(config.num_workers)\n        .build(MnistDataset::test());\n\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting and Using a Tensor Backend in Burn - Rust\nDESCRIPTION: Shows how to select a computational backend (e.g., Wgpu, Candle, LibTorch, NdArray) for tensor computation in Burn and perform element-wise addition of two tensors. Illustrates type aliasing for backend, tensor creation with initial data and device, and printing computation results. Care should be taken to use compatible data and device arguments as per backend requirements; example supports runtime switching and generic code.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n// Choose from any of the supported backends.\\n// type Backend = Candle<f32, i64>;\\n// type Backend = LibTorch<f32>;\\n// type Backend = NdArray<f32>;\\ntype Backend = Wgpu;\\n\\n// Creation of two tensors.\\nlet tensor_1 = Tensor::<Backend, 2>::from_data([[2., 3.], [4., 5.]], &device);\\nlet tensor_2 = Tensor::<Backend, 2>::ones_like(&tensor_1);\\n\\n// Print the element-wise addition (done with the selected backend) of the two tensors.\\nprintln!(\\\"{}\\\", tensor_1 + tensor_2);\n```\n\n----------------------------------------\n\nTITLE: Creating a Neural Network Module with the Module Derive Macro in Rust\nDESCRIPTION: Example showing how to create a custom neural network module with the Module derive macro, similar to PyTorch's approach. This snippet demonstrates a position-wise feed forward network implementation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/module.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::module::Module;\nuse burn::tensor::backend::Backend;\n\n#[derive(Module, Debug)]\npub struct PositionWiseFeedForward<B: Backend> {\n    linear_inner: Linear<B>,\n    linear_outer: Linear<B>,\n    dropout: Dropout,\n    gelu: Gelu,\n}\n\nimpl<B: Backend> PositionWiseFeedForward<B> {\n    /// Normal method added to a struct.\n    pub fn forward<const D: usize>(&self, input: Tensor<B, D>) -> Tensor<B, D> {\n        let x = self.linear_inner.forward(input);\n        let x = self.gelu.forward(x);\n        let x = self.dropout.forward(x);\n\n        self.linear_outer.forward(x)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom GELU Activation with Burn Tensor API in Rust\nDESCRIPTION: This Rust snippet demonstrates how to implement a custom Gaussian Error Linear Unit (GELU) activation function using the Burn deep learning framework's tensor API. It leverages generic type parameters for the backend and tensor dimensionality, making it compatible with multiple execution backends supported by Burn. The function computes the GELU using element-wise operations and the error function, returning a new tensor as output. This design ensures portability and runtime kernel optimization, requiring Burn's core crate and a properly configured backend as dependencies. Inputs are tensors of arbitrary dimensionality, and outputs are tensors with identical shape. Note that the use of `SQRT_2` and `erf()` assumes these constants and methods are available in the imported Burn API context.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nfn gelu_custom<B: Backend, const D: usize>(x: Tensor<B, D>) -> Tensor<B, D> {\n    let x = x.clone() * ((x / SQRT_2).erf() + 1);\n    x / 2\n}\n```\n\n----------------------------------------\n\nTITLE: Converting and Loading PyTorch Model Weights Using Burn's Binary Format - Rust\nDESCRIPTION: Demonstrates how to convert a PyTorch .pt file to Burn's binary format using the PyTorchFileRecorder and NamedMpkFileRecorder, followed by loading the model in another Rust process for inference or training. Requires Burn, burn-import, and correct backend. This method is optimal for production as it removes the runtime dependency on burn-import and supports efficient loading. Key parameters include the device, file paths, and backend. Outputs a ready-to-use Rust Net struct with loaded weights.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_3\n\nLANGUAGE: Rust\nCODE:\n```\n// Convert the PyTorch model to Burn's binary format in\\n// build.rs or in a separate executable. Then, include the generated file\\n// in your project. See `examples/pytorch-import` for an example.\\n\\nuse crate::model;\\n\\nuse burn::record::{FullPrecisionSettings, NamedMpkFileRecorder, Recorder};\\nuse burn_import::pytorch::PyTorchFileRecorder;\\n\\ntype Backend = burn_ndarray::NdArray<f32>;\\n\\nfn main() {\\n    let device = Default::default();\\n    let recorder = PyTorchFileRecorder::<FullPrecisionSettings>::default()\\n    let record: model::NetRecord<B> = recorder\\n        .load(\"./conv2d.pt\".into(), &device)\\n        .expect(\"Should decode state successfully\");\\n\\n    // Save the model record to a file.\\n    let recorder = NamedMpkFileRecorder::<FullPrecisionSettings>::default();\\n    recorder\\n        .record(record, \"MY_FILE_OUTPUT_PATH\".into())\\n        .expect(\"Failed to save model record\");\\n}\\n\\n/// Load the model from the file in your source code (not in build.rs or script).\\nfn load_model() -> Net::<Backend> {\\n    let device = Default::default();\\n    let record = NamedMpkFileRecorder::<FullPrecisionSettings>::default()\\n        .load(\"./MY_FILE_OUTPUT_PATH\".into(), &device)\\n        .expect(\"Should decode state successfully\");\\n\\n    Net::<Backend>::init(&device).load_record(record)\\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Experiment Configuration with Derive Config in Rust\nDESCRIPTION: Defines a strongly-typed experiment configuration struct using a custom Config derive macro for seamless loading, validation, and persistence. Dependencies include the `Config` derive macro and required subconfig types (`ModelConfig`, `AdamConfig`). Each field is type-enforced, and the struct supports default values via attributes for key hyperparameters such as epoch count, batch size, worker threads, random seed, and learning rate. Used for parameterizing both model and optimizer setup and for persisting experiment setup via serialization.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/training.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Config)]\npub struct TrainingConfig {\n    pub model: ModelConfig,\n    pub optimizer: AdamConfig,\n    #[config(default = 10)]\n    pub num_epochs: usize,\n    #[config(default = 64)]\n    pub batch_size: usize,\n    #[config(default = 4)]\n    pub num_workers: usize,\n    #[config(default = 42)]\n    pub seed: u64,\n    #[config(default = 1.0e-4)]\n    pub learning_rate: f64,\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Module from Config on a Device Using Burn in Rust\nDESCRIPTION: This snippet illustrates extending the config struct with an init method for seamless module instantiation. It takes a device as an argument, instantiates submodules (e.g., Linear, Dropout) via their configs, and returns a fully initialized module. Dependencies include Backend traits and config structures for other subcomponents. The expected input is a config instance and device reference, and it produces an initialized module.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/config.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nimpl MyModuleConfig {\n    /// Create a module on the given device.\n    pub fn init<B: Backend>(&self, device: &B::Device) -> MyModule {\n        MyModule {\n            linear: LinearConfig::new(self.d_model, self.d_ff).init(device),\n            dropout: DropoutConfig::new(self.dropout).init(),\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Text Classification with CUDA Backend\nDESCRIPTION: Commands for training and inferencing text classification models using CUDA backend with optional FP16 support.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-classification/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n\n# Use the --release flag to really speed up training.\n# Add the f16 feature to run in f16. \n\n# AG News\ncargo run --example ag-news-train --release --features cuda   # Train on the ag news dataset\ncargo run --example ag-news-infer --release --features cuda   # Run inference on the ag news dataset\n```\n\n----------------------------------------\n\nTITLE: Instantiating MNIST Training and Test Datasets via File Parsing in Rust\nDESCRIPTION: This snippet implements train and test dataset creation for MNIST by composing several steps: downloading the dataset split, reading images and labels, pairing and collecting raw items, converting them using the BytesToImage mapper, and wrapping the result in a MapperDataset. All relevant parameters (paths, file formats) are set internally. The design supports both training and test datasets. The snippet assumes dependencies on Burn, file I/O modules, and the previously defined MnistItemRaw, MnistItem, and Mapper types.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_12\n\nLANGUAGE: rust\nCODE:\n```\nimpl MnistDataset {\n    /// Creates a new train dataset.\n    pub fn train() -> Self {\n        Self::new(\"train\")\n    }\n\n    /// Creates a new test dataset.\n    pub fn test() -> Self {\n        Self::new(\"test\")\n    }\n\n    fn new(split: &str) -> Self {\n        // Download dataset\n        let root = MnistDataset::download(split);\n\n        // Parse data as vector of images bytes and vector of labels\n        let images: Vec<Vec<u8>> = MnistDataset::read_images(&root, split);\n        let labels: Vec<u8> = MnistDataset::read_labels(&root, split);\n\n        // Collect as vector of MnistItemRaw\n        let items: Vec<_> = images\n            .into_iter()\n            .zip(labels)\n            .map(|(image_bytes, label)| MnistItemRaw { image_bytes, label })\n            .collect();\n\n        // Create the MapperDataset for InMemDataset<MnistItemRaw> to transform\n        // items (MnistItemRaw -> MnistItem)\n        let dataset = InMemDataset::new(items);\n        let dataset = MapperDataset::new(dataset, BytesToImage);\n\n        Self { dataset }\n    }\n\n#    /// Download the MNIST dataset files from the web.\n#    /// Panics if the download cannot be completed or the content of the file cannot be written to disk.\n#    fn download(split: &str) -> PathBuf {\n#        // Dataset files are stored un the burn-dataset cache directory\n#        let cache_dir = dirs::home_dir()\n#            .expect(\"Could not get home directory\")\n#            .join(\".cache\")\n#            .join(\"burn-dataset\");\n#        let split_dir = cache_dir.join(\"mnist\").join(split);\n#\n#        if !split_dir.exists() {\n#            create_dir_all(&split_dir).expect(\"Failed to create base directory\");\n#        }\n#\n#        // Download split files\n#        match split {\n#            \"train\" => {\n#                MnistDataset::download_file(TRAIN_IMAGES, &split_dir);\n#                MnistDataset::download_file(TRAIN_LABELS, &split_dir);\n#            }\n#            \"test\" => {\n#                MnistDataset::download_file(TEST_IMAGES, &split_dir);\n#                MnistDataset::download_file(TEST_LABELS, &split_dir);\n#            }\n#            _ => panic!(\"Invalid split specified {}\", split),\n#        };\n#\n#        split_dir\n#    }\n#\n#    /// Download a file from the MNIST dataset URL to the destination directory.\n#    /// File download progress is reported with the help of a [progress bar](indicatif).\n#    fn download_file<P: AsRef<Path>>(name: &str, dest_dir: &P) -> PathBuf {\n#        // Output file name\n#        let file_name = dest_dir.as_ref().join(name);\n#\n#        if !file_name.exists() {\n#            // Download gzip file\n#            let bytes = download_file_as_bytes(&format!(\"{URL}{name}.gz\"), name);\n#\n#            // Create file to write the downloaded content to\n#            let mut output_file = File::create(&file_name).unwrap();\n#\n#            // Decode gzip file content and write to disk\n#            let mut gz_buffer = GzDecoder::new(&bytes[..]);\n#            std::io::copy(&mut gz_buffer, &mut output_file).unwrap();\n#        }\n#\n#        file_name\n#    }\n#\n#    /// Read images at the provided path for the specified split.\n#    /// Each image is a vector of bytes.\n#    fn read_images<P: AsRef<Path>>(root: &P, split: &str) -> Vec<Vec<u8>> {\n#        let file_name = if split == \"train\" {\n#            TRAIN_IMAGES\n#        } else {\n#            TEST_IMAGES\n#        };\n#        let file_name = root.as_ref().join(file_name);\n#\n#        // Read number of images from 16-byte header metadata\n#        let mut f = File::open(file_name).unwrap();\n#        let mut buf = [0u8; 4];\n#        let _ = f.seek(SeekFrom::Start(4)).unwrap();\n#        f.read_exact(&mut buf)\n#            .expect(\"Should be able to read image file header\");\n#        let size = u32::from_be_bytes(buf);\n#\n#        let mut buf_images: Vec<u8> = vec![0u8; WIDTH * HEIGHT * (size as usize)];\n#        let _ = f.seek(SeekFrom::Start(16)).unwrap();\n#        f.read_exact(&mut buf_images)\n#            .expect(\"Should be able to read image file header\");\n#\n#        buf_images\n#            .chunks(WIDTH * HEIGHT)\n#            .map(|chunk| chunk.to_vec())\n#            .collect()\n#    }\n#\n#    /// Read labels at the provided path for the specified split.\n#    fn read_labels<P: AsRef<Path>>(root: &P, split: &str) -> Vec<u8> {\n#        let file_name = if split == \"train\" {\n#            TRAIN_LABELS\n#        } else {\n#            TEST_LABELS\n#        };\n#        let file_name = root.as_ref().join(file_name);\n#\n#        // Read number of labels from 8-byte header metadata\n#        let mut f = File::open(file_name).unwrap();\n#        let mut buf = [0u8; 4];\n```\n\n----------------------------------------\n\nTITLE: Printing the Initialized Model Using Burn in Rust\nDESCRIPTION: Demonstrates how to create and print an initialized model using the ModelConfig and Burn Wgpu backend. The snippet sets up type aliases, creates a device, initializes the model with specified parameters (10 output classes, 512 hidden units), and prints the model structure to stdout for inspection. Required dependencies are the Burn crate, a compatible GPU backend (Wgpu), and the model module. Inputs are model configuration arguments; output is the console representation of the model. This pattern is helpful for debugging and for understanding model architecture but requires a proper backend setup and relevant modules to be available.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nmod model;\\n\\nuse crate::model::ModelConfig;\\nuse burn::backend::Wgpu;\\n\\nfn main() {\\n    type MyBackend = Wgpu<f32, i32>;\\n\\n    let device = Default::default();\\n    let model = ModelConfig::new(10, 512).init::<MyBackend>(&device);\\n\\n    println!(\\\"{}\\\", model);\\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Dependencies for Burn Notebook in Rust\nDESCRIPTION: This snippet demonstrates how to declare dependencies within a Rust notebook environment, using a specialized syntax similar to Cargo.toml with the :dep prefix, as required by evcxr. The dependencies added here are 'burn' and 'burn-ndarray', necessary for tensor operations and use of ndarray backend. No standard imports are needed, but the crates must exist at relative paths as specified, and initial compilation may take extra time.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/basic-tensor-op.ipynb#_snippet_0\n\nLANGUAGE: Rust\nCODE:\n```\n// Dependency declarations for the notebook. WARNING: It may take a while to compile the first time.\n\n// The syntax is similar to the one used in the Cargo.toml file. Just prefix with :dep\n// See: https://github.com/evcxr/evcxr/blob/main/COMMON.md\n\n:dep burn = {path = \"../../crates/burn\"}\n:dep burn-ndarray = {path = \"../../crates/burn-ndarray\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing Adaptor Trait for Classification Output in Rust\nDESCRIPTION: This snippet demonstrates how to implement the Adaptor trait for a ClassificationOutput struct, allowing it to be used with multiple metrics such as Accuracy and Loss.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/metric.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n/// Simple classification output adapted for multiple metrics.\n#[derive(new)]\npub struct ClassificationOutput<B: Backend> {\n    /// The loss.\n    pub loss: Tensor<B, 1>,\n\n    /// The output.\n    pub output: Tensor<B, 2>,\n\n    /// The targets.\n    pub targets: Tensor<B, 1, Int>,\n}\n\nimpl<B: Backend> Adaptor<AccuracyInput<B>> for ClassificationOutput<B> {\n    fn adapt(&self) -> AccuracyInput<B> {\n        AccuracyInput::new(self.output.clone(), self.targets.clone())\n    }\n}\n\nimpl<B: Backend> Adaptor<LossInput<B>> for ClassificationOutput<B> {\n    fn adapt(&self) -> LossInput<B> {\n        LossInput::new(self.loss.clone())\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Text Classification with Metal Backend\nDESCRIPTION: Commands for training and inferencing text classification models using Metal backend for MacOS systems with optional FP16 support.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-classification/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n\n# Use the --release flag to really speed up training.\n# Add the f16 feature to run in f16. \n\n# AG News\ncargo run --example ag-news-train --release --features metal   # Train on the ag news dataset\ncargo run --example ag-news-infer --release --features metal   # Run inference on the ag news dataset\n```\n\n----------------------------------------\n\nTITLE: Inner Tensor Usage for Inference and Validation - Burn (Rust)\nDESCRIPTION: This Rust example demonstrates two functions for inference and validation using Burn tensors with and without autodiff capabilities. The example_validation function extracts the inner tensor from an autodiff tensor for validation by calling inner(), while example_inference works directly with the tensor for inference. Both require appropriate trait bounds: AutodiffBackend or Backend respectively. Requires Burn library with correct backend. The functions accept 2D tensors and demonstrate arithmetic operations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/autodiff.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n/// Use `B: AutodiffBackend`\nfn example_validation<B: AutodiffBackend>(tensor: Tensor<B, 2>) {\n    let inner_tensor: Tensor<B::InnerBackend, 2> = tensor.inner();\n    let _ = inner_tensor + 5;\n}\n\n/// Use `B: Backend`\nfn example_inference<B: Backend>(tensor: Tensor<B, 2>) {\n    let _ = tensor + 5;\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Dataset Trait for MNIST in Rust\nDESCRIPTION: This Rust code snippet implements the Dataset trait for the MnistDataset struct, allowing it to provide MNIST items by index and report its length. It depends on the existence of MnistDataset, the MapperDataset instance stored in self.dataset, and the MnistItem type. The get method returns an Option<MnistItem> for a given index, while len returns the total number of available items. Inputs include index (usize); outputs are Option<MnistItem> or usize. The trait implementation assumes that the dataset is held in memory; no external dependencies beyond Rust's type system and trait support are required.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_13\n\nLANGUAGE: rust\nCODE:\n```\nimpl Dataset<MnistItem> for MnistDataset {\n    fn get(&self, index: usize) -> Option<MnistItem> {\n        self.dataset.get(index)\n    }\n\n    fn len(&self) -> usize {\n        self.dataset.len()\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing the Custom Training and Validation Loop (Burn, Rust)\nDESCRIPTION: This snippet implements the training and validation process using explicit loops without pre-built abstractions. For each epoch, it runs forward passes, computes losses, calculates accuracy, backpropagates gradients, maps gradients to model parameters, and applies optimization steps. After training, it detaches the model from autodiff for validation to prevent gradient tracking. Main dependencies include the model, optimizer, loaders, CrossEntropyLoss, and accuracy utility. Expected inputs are data batches; outputs are logged metrics and updated model weights. It assumes a prior setup of configured models, optimizers, and data loaders, with constraints including explicit gradient-to-parameter mapping and manual step management.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub fn run<B: AutodiffBackend>(device: B::Device) {\n    ...\n\n    // Iterate over our training and validation loop for X epochs.\n    for epoch in 1..config.num_epochs + 1 {\n        // Implement our training loop.\n        for (iteration, batch) in dataloader_train.iter().enumerate() {\n            let output = model.forward(batch.images);\n            let loss = CrossEntropyLoss::new(None, &output.device())\n                .forward(output.clone(), batch.targets.clone());\n            let accuracy = accuracy(output, batch.targets);\n\n            println!(\n                \"[Train - Epoch {} - Iteration {}] Loss {:.3} | Accuracy {:.3} %\",\n                epoch,\n                iteration,\n                loss.clone().into_scalar(),\n                accuracy,\n            );\n\n            // Gradients for the current backward pass\n            let grads = loss.backward();\n            // Gradients linked to each parameter of the model.\n            let grads = GradientsParams::from_grads(grads, &model);\n            // Update the model using the optimizer.\n            model = optim.step(config.lr, model, grads);\n        }\n\n        // Get the model without autodiff.\n        let model_valid = model.valid();\n\n        // Implement our validation loop.\n        for (iteration, batch) in dataloader_test.iter().enumerate() {\n            let output = model_valid.forward(batch.images);\n            let loss = CrossEntropyLoss::new(None, &output.device())\n                .forward(output.clone(), batch.targets.clone());\n            let accuracy = accuracy(output, batch.targets);\n\n            println!(\n                \"[Valid - Epoch {} - Iteration {}] Loss {} | Accuracy {}\",\n                epoch,\n                iteration,\n                loss.clone().into_scalar(),\n                accuracy,\n            );\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining PartialDataset and ShuffledDataset for Splitting in Burn (Rust)\nDESCRIPTION: This code snippet demonstrates creating training and testing splits by chaining transformation datasets in Burn. It uses `PartialDataset` and `ShuffledDataset` to select portions of the dataset after randomization. The example defines a chained dataset type, computes split indices based on dataset length, and uses Rust's match statement for selecting the appropriate data subset. Inputs are the full dataset, split type (train/test), and length; output is a specialized dataset containing only the relevant split. Suitable for train/val/test partitioning workflows.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n// define chained dataset type here for brevity\ntype PartialData = PartialDataset<ShuffledDataset<DbPedia, DbPediaItem>>;\nlet len = dataset.len();\nlet split == \"train\"; // or \"val\"/\"test\"\n\nlet data_split = match split {\n            \"train\" => PartialData::new(dataset, 0, len * 8 / 10), // Get first 80% dataset\n            \"test\" => PartialData::new(dataset, len * 8 / 10, len), // Take remaining 20%\n            _ => panic!(\"Invalid split type\"),                     // Handle unexpected split types\n        };\n```\n\n----------------------------------------\n\nTITLE: Advanced: Multiple Optimizers and Parameter Grouping (Burn, Rust)\nDESCRIPTION: This snippet demonstrates splitting gradients for fine-grained control of optimization, allowing different learning rates or optimizers for different parts of the model. Gradients are extracted per module or specific parameters, and the optimizer is applied with different learning rates accordingly. Unused gradients are re-collected and applied in a final step. Dependencies include model submodules, parameter identification, and the GradientParams utility. This technique supports advanced optimization strategies, such as differential learning rates for convolutional layers or smaller rates for biases.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n// Start with calculating all gradients\nlet grads = loss.backward();\n\n// Now split the gradients into various parts.\nlet grads_conv1 = GradientParams::from_module(&mut grads, &model.conv1);\nlet grads_conv2 = GradientParams::from_module(&mut grads, &model.conv2);\n\n// You can step the model with these gradients, using different learning\n// rates for each param. You could also use an entirely different optimizer here!\nmodel = optim.step(config.lr * 2.0, model, grads_conv1);\nmodel = optim.step(config.lr * 4.0, model, grads_conv2);\n\n// For even more granular control you can split off individual parameter\n// eg. a linear bias usually needs a smaller learning rate.\nif let Some(bias) == model.linear1.bias {\n    let grads_bias = GradientParams::from_params(&mut grads, &model.linear1, &[bias.id]);\n    model = optim.step(config.lr * 0.1, model, grads_bias);\n}\n\n// Note that above calls remove gradients, so we can just get all \"remaining\" gradients.\nlet grads = GradientsParams::from_grads(grads, &model);\nmodel = optim.step(config.lr, model, grads);\n```\n\n----------------------------------------\n\nTITLE: Declaring Tensor Types in Burn\nDESCRIPTION: Demonstrates how to declare different types of tensors using the Tensor struct with various generic arguments for backend, dimensionality, and data type.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nTensor<B, D>           // Float tensor (default)\nTensor<B, D, Float>    // Explicit float tensor\nTensor<B, D, Int>      // Int tensor\nTensor<B, D, Bool>     // Bool tensor\n```\n\n----------------------------------------\n\nTITLE: Defining Configurable Structs with Burn Config in Rust\nDESCRIPTION: This snippet shows how to derive the Config trait for a Rust struct using the burn::config::Config macro, enabling default values and serialization. Dependencies include the burn crate, and the struct fields can have custom defaults using #[config(default = ...)]. The struct supports generated builder-like methods for concise configuration and is ready for serialization.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/config.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::config::Config;\n\n#[derive(Config)]\npub struct MyModuleConfig {\n    d_model: usize,\n    d_ff: usize,\n    #[config(default = 0.1)]\n    dropout: f64,\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Gradients with AutodiffBackend - Burn (Rust)\nDESCRIPTION: This Rust code demonstrates the computation of gradients using the Burn tensor library with a backend that implements the AutodiffBackend trait. The function clones a tensor, performs a backward pass to populate gradients, and retrieves them via grad and grad_remove methods. Dependencies: Burn library with a backend supporting AutodiffBackend. The function takes a 2D tensor and returns its gradients; input tensors and the backend must both support autodiff.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/autodiff.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nfn calculate_gradients<B: AutodiffBackend>(tensor: Tensor<B, 2>) -> B::Gradients {\n    let mut gradients = tensor.clone().backward();\n\n    let tensor_grad = tensor.grad(&gradients);        // get\n    let tensor_grad = tensor.grad_remove(&mut gradients); // pop\n\n    gradients\n}\n```\n\n----------------------------------------\n\nTITLE: Running MNIST Training with Burn CUDA Backend in Rust\nDESCRIPTION: This Rust code snippet demonstrates how to configure and use the Burn CUDA backend to train a model on the MNIST dataset using automatic differentiation. It is designed as a feature-gated module, requiring the \"cuda\" feature to be enabled in the crate. The snippet imports the relevant Burn and CUDA components, selects the default CUDA device, and starts training using Burn's MNIST example. Dependencies include the burn_cuda, burn_autodiff, and mnist crates as well as a system installation of CUDA 12.x configured in the PATH. The function expects CUDA-capable hardware and proper crate configuration; otherwise, compilation or runtime errors may occur.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-cuda/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n```rust\\n#[cfg(feature = \\\"cuda\\\")]\\nmod cuda {\\n    use burn_autodiff::Autodiff;\\n    use burn_cuda::{Cuda, CudaDevice};\\n    use mnist::training;\\n\\n    pub fn run() {\\n        let device = CudaDevice::default();\\n        training::run::<Autodiff<Cuda<f32, i32>>>(device);\\n    }\\n}\\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Burn Dependencies to Cargo.toml - Rust TOML\nDESCRIPTION: Configures the Rust project's Cargo.toml by specifying project metadata and adding the 'burn' crate with 'train', 'wgpu', and 'vision' features. This enables deep learning, GPU, and vision-related functionality. Dependencies are loaded upon 'cargo build'. Required parameters include the package name, version, edition, and the correct Burn crate version/features.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[package]\\nname = \\\"guide\\\"\\nversion = \\\"0.1.0\\\"\\nedition = \\\"2024\\\"\\n\\n[dependencies]\\nburn = { version = \\\"~0.18\\\", features = [\\\"train\\\", \\\"wgpu\\\", \\\"vision\\\"] }\n```\n\n----------------------------------------\n\nTITLE: Modifying and Saving Configurations Using Burn in Rust\nDESCRIPTION: This snippet demonstrates creating, modifying, and persisting a configuration object using builder-like methods automatically generated by the Burn Config derive macro. It shows instantiating the config, reading/writing its parameters, updating a field with with_ methods, and saving the config to a JSON file. The save method outputs to a specified filename and expects a writable destination.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/config.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nfn main() {\n    let config = MyModuleConfig::new(512, 2048);\n    println!(\"{}\", config.d_model); // 512\n    println!(\"{}\", config.d_ff); // 2048\n    println!(\"{}\", config.dropout); // 0.1\n    let config =  config.with_dropout(0.2);\n    println!(\"{}\", config.dropout); // 0.2\n\n    config.save(\"config.json\").unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Equivalent Model for Import - Burn, Rust\nDESCRIPTION: Defines an equivalent network architecture in the Burn framework using Rust, compatible with the previously exported PyTorch weights. Requires Burn crate and a compatible backend. The Net struct defines two Conv2d layers, with one configured without bias. Inputs are tensors; outputs are tensors after two Conv2d layers. This struct's definition should mirror the original PyTorch model for successful weight loading.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_1\n\nLANGUAGE: Rust\nCODE:\n```\nuse burn::{\\n    nn::conv::{Conv2d, Conv2dConfig},\\n    prelude::*,\\n};\\n\\n#[derive(Module, Debug)]\\npub struct Net<B: Backend> {\\n    conv1: Conv2d<B>,\\n    conv2: Conv2d<B>,\\n}\\n\\nimpl<B: Backend> Net<B> {\\n    /// Create a new model.\\n    pub fn init(device: &B::Device) -> Self {\\n        let conv1 = Conv2dConfig::new([2, 2], [2, 2])\\n            .init(device);\\n        let conv2 = Conv2dConfig::new([2, 2], [2, 2])\\n            .with_bias(false)\\n            .init(device);\\n        Self { conv1, conv2 }\\n    }\\n\\n    /// Forward pass of the model.\\n    pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 4> {\\n        let x = self.conv1.forward(x);\\n        self.conv2.forward(x)\\n    }\\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Random Tensor and Image Visualization\nDESCRIPTION: Demonstrates creating a random 3D tensor using Burn's tensor operations. Also includes code to create a simple diagonal line image using ImageBuffer from the image crate for visualization purposes.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/plots.ipynb#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n// Create a random tensor\nuse burn::tensor::Distribution;\nlet tensor: Tensor<B, 3> = Tensor::random([3, 256, 256], Distribution::Default, &Default::default());\n\n// TODO Use tenso to display plots\nimage::ImageBuffer::from_fn(256, 256, |x, y| {\n    if (x as i32 - y as i32).abs() < 3 {\n        image::Rgb([0, 0, 255])\n    } else {\n        image::Rgb([0, 0, 0])\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Defining a Learner Struct Generic Over Backend and Optimizer in Rust\nDESCRIPTION: This Rust code defines a struct named Learner that is generic over a backend B (constrained by the AutodiffBackend trait) and an optimizer O. The Learner encapsulates both a model and an optimizer, using concrete Model<B>. Dependencies include the Model<B> struct and implementation of AutodiffBackend for B. This approach is straightforward, directly associating the backend type with the model, and is best when using a fixed model structure.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nstruct Learner<B, O>\nwhere\n    B: AutodiffBackend,\n{\n    model: Model<B>,\n    optim: O,\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Module Visitor and Mapper Traits in Burn\nDESCRIPTION: Shows the definition of ModuleVisitor and ModuleMapper traits that enable custom operations on module parameters. These traits allow for operations like optimization and data collection on module parameters.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/module.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n/// Module visitor trait.\npub trait ModuleVisitor<B: Backend> {\n    /// Visit a float tensor in the module.\n    fn visit_float<const D: usize>(&mut self, id: ParamId, tensor: &Tensor<B, D>);\n    /// Visit an int tensor in the module.\n    fn visit_int<const D: usize>(&mut self, id: ParamId, tensor: &Tensor<B, D, Int>);\n    /// Visit a bool tensor in the module.\n    fn visit_bool<const D: usize>(&mut self, id: ParamId, tensor: &Tensor<B, D, Bool>);\n}\n\n/// Module mapper trait.\npub trait ModuleMapper<B: Backend> {\n    /// Map a float tensor in the module.\n    fn map_float<const D: usize>(&mut self, id: ParamId, tensor: Tensor<B, D>) -> Tensor<B, D>;\n    /// Map an int tensor in the module.\n    fn map_int<const D: usize>(&mut self, id: ParamId, tensor: Tensor<B, D, Int>) -> Tensor<B, D, Int>;\n    /// Map a bool tensor in the module.\n    fn map_bool<const D: usize>(&mut self, id: ParamId, tensor: Tensor<B, D, Bool>) -> Tensor<B, D, Bool>;\n}\n```\n\n----------------------------------------\n\nTITLE: Correct Tensor Operation with Cloning in Burn\nDESCRIPTION: Demonstrates the correct way to perform min-max normalization in Burn using explicit cloning to manage ownership and allow multiple uses of tensors.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet input = Tensor::<Wgpu, 1>::from_floats([1.0, 2.0, 3.0, 4.0], &device);\nlet min = input.clone().min();\nlet max = input.clone().max();\nlet input = (input.clone() - min.clone()).div(max - min);\nprintln!(\"{}\", input.to_data());// Success: [0.0, 0.33333334, 0.6666667, 1.0]\n\n// Notice that max, min have been moved in last operation so\n// the below print will give an error.\n// If we want to use them for further operations,\n// they will need to be cloned in similar fashion.\n// println!(\"{:?}\", min.to_data());\n```\n\n----------------------------------------\n\nTITLE: Checking Tensor Closeness in Burn\nDESCRIPTION: Compares two tensors element-wise and assesses their similarity at various tolerance levels. This function is useful for debugging tensor operations, validating numerical accuracy, and verifying model imports from other frameworks.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor::{check_closeness, Tensor};\ntype B = burn::backend::NdArray;\n\nlet device = Default::default();\nlet tensor1 = Tensor::<B, 1>::from_floats(\n    [1.0, 2.0, 3.0, 4.0, 5.0, 6.001, 7.002, 8.003, 9.004, 10.1],\n    &device,\n);\nlet tensor2 = Tensor::<B, 1>::from_floats(\n    [1.0, 2.0, 3.0, 4.000, 5.0, 6.0, 7.001, 8.002, 9.003, 10.004],\n    &device,\n);\n\ncheck_closeness(&tensor1, &tensor2);\n```\n\n----------------------------------------\n\nTITLE: Defining Metric Trait for Custom Metrics in Rust\nDESCRIPTION: This code snippet shows the definition of the Metric trait, which is used to implement custom metrics in the Burn AI framework. It includes methods for naming, updating, and clearing the metric state.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/metric.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n/// Metric trait.\n///\n/// # Notes\n///\n/// Implementations should define their own input type only used by the metric.\n/// This is important since some conflict may happen when the model output is adapted for each\n/// metric's input type.\npub trait Metric: Send + Sync {\n    /// The input type of the metric.\n    type Input;\n\n    /// The parameterized name of the metric.\n    ///\n    /// This should be unique, so avoid using short generic names, prefer using the long name.\n    ///\n    /// For a metric that can exist at different parameters (e.g., top-k accuracy for different\n    /// values of k), the name should be unique for each instance.\n    fn name(&self) -> String;\n\n    /// Update the metric state and returns the current metric entry.\n    fn update(&mut self, item: &Self::Input, metadata: &MetricMetadata) -> MetricEntry;\n    /// Clear the metric state.\n    fn clear(&mut self);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss Metric in Rust for Burn AI\nDESCRIPTION: This example demonstrates how to implement a custom Loss metric using the Metric trait. It includes the metric struct, input type, and implementation of required methods such as update, clear, and name.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/metric.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n/// The loss metric.\n#[derive(Default)]\npub struct LossMetric<B: Backend> {\n    state: NumericMetricState,\n    _b: B,\n}\n\n/// The loss metric input type.\n#[derive(new)]\npub struct LossInput<B: Backend> {\n    tensor: Tensor<B, 1>,\n}\n\n\nimpl<B: Backend> Metric for LossMetric<B> {\n    type Input = LossInput<B>;\n\n    fn update(&mut self, loss: &Self::Input, _metadata: &MetricMetadata) -> MetricEntry {\n        let [batch_size] = loss.tensor.dims();\n        let loss = loss\n            .tensor\n            .clone()\n            .mean()\n            .into_data()\n            .iter::<f64>()\n            .next()\n            .unwrap();\n\n        self.state.update(\n            loss,\n            batch_size,\n            FormatOptions::new(self.name()).precision(2),\n        )\n    }\n\n    fn clear(&mut self) {\n        self.state.reset()\n    }\n\n    fn name(&self) -> String {\n        \"Loss\".to_string()\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Model Weights - Python\nDESCRIPTION: Exports a simple convolutional neural network's weights in PyTorch using torch.save and the model's state_dict. Requires PyTorch installed. The Net class contains two Conv2d layers; the script saves their weights to 'conv2d.pt', which can then be imported into Burn. Inputs are model parameters; output is a .pt file containing learned weights. The code assumes CPU device for model, and reproducibility is set by torch.manual_seed.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\\nimport torch.nn as nn\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv1 = nn.Conv2d(2, 2, (2,2))\\n        self.conv2 = nn.Conv2d(2, 2, (2,2), bias=False)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.conv2(x)\\n        return x\\n\\nif __name__ == \"__main__\":\\n    torch.manual_seed(42)  # To make it reproducible\\n    model = Net().to(torch.device(\"cpu\"))\\n    model_weights = model.state_dict()\\n    torch.save(model_weights, \"conv2d.pt\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Burn Model Struct with Module Derive - Rust\nDESCRIPTION: Declares a convolutional neural network model as a Rust struct, annotated with #[derive(Module, Debug)] for automatic implementation of both Burn's Module trait and the Debug trait. The model consists of convolutional, pooling, dropout, linear, and activation layers, each tailored for image classification. Dependencies include the burn crate, and the struct is generic over a Backend trait to support various hardware backends. Fields must implement the Module trait; inputs are handled at instantiation, while outputs depend on subsequent forward method definitions.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::{\\n    nn::{\\n        conv::{Conv2d, Conv2dConfig},\\n        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},\\n        Dropout, DropoutConfig, Linear, LinearConfig, Relu,\\n    },\\n    prelude::*,\\n};\\n\\n#[derive(Module, Debug)]\\npub struct Model<B: Backend> {\\n    conv1: Conv2d<B>,\\n    conv2: Conv2d<B>,\\n    pool: AdaptiveAvgPool2d,\\n    dropout: Dropout,\\n    linear1: Linear<B>,\\n    linear2: Linear<B>,\\n    activation: Relu,\\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Step Function for a Generic Learner with Multiple Trait Bounds in Rust\nDESCRIPTION: This code provides an implementation block for the Learner struct, specifying generic parameters B (backend), M (model), and O (optimizer). The step function requires M to implement AutodiffModule<B>, O to implement Optimizer<M, B>, and B to implement AutodiffBackend. The function operates on MnistBatch<B>, although its body is empty. Dependencies include all related traits (AutodiffModule, Optimizer, AutodiffBackend) and the MnistBatch type. The approach tightly constrains generics but leads to a compilation error unless B is referenced by the self type or struct.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B, M, O> Learner<M, O>\nwhere\n    B: AutodiffBackend,\n    M: AutodiffModule<B>,\n    O: Optimizer<M, B>,\n{\n    pub fn step(&mut self, _batch: MnistBatch<B>) {\n        //\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Numeric Trait for Loss Metric in Rust\nDESCRIPTION: This snippet shows how to implement the Numeric trait for the LossMetric, allowing it to be plotted. It provides a method to retrieve the current value of the metric.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/metric.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: Backend> Numeric for LossMetric<B> {\n    fn value(&self) -> f64 {\n        self.state.value()\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Model with Binary File Recorder in Rust\nDESCRIPTION: This code demonstrates saving a machine learning model in binary format using Burn's BinFileRecorder with full precision. Invoking model.save_file with the binary-format recorder writes the model's weights to disk, suitable for environments requiring compact storage or further embedding. Dependencies include the BinFileRecorder, Burn, and appropriate serialization support, while the model must be initialized and the file path provided. All file operations are ensured with expect for validation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/saving-and-loading.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n// Save model in binary format with full precision\nlet recorder = BinFileRecorder::<FullPrecisionSettings>::new();\nmodel\n    .save_file(model_path, &recorder)\n    .expect(\"Should be able to save the model\");\n```\n\n----------------------------------------\n\nTITLE: Embedding and Loading Model Weights with BinBytesRecorder in Rust\nDESCRIPTION: This snippet loads model weights directly from embedded bytes using Burn's BinBytesRecorder with full precision. The static byte array is defined via include_bytes, which incorporates the model binary at compile time. The recorder's load method loads the weights from the byte array, associating them with the designated device, and the resulting record is loaded into the model instance. The approach is ideal for embedding models within binaries, provided the model size remains reasonable to avoid excess binary growth or memory usage.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/saving-and-loading.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n// Include the model file as a reference to a byte array\nstatic MODEL_BYTES: &[u8] = include_bytes!(\"path/to/model.bin\");\n\n// Load model binary record in full precision\nlet record = BinBytesRecorder::<FullPrecisionSettings>::default()\n    .load(MODEL_BYTES.to_vec(), device)\n    .expect(\"Should be able to load model the model weights from bytes\");\n\n// Load that record with the model\nmodel.load_record(record);\n```\n\n----------------------------------------\n\nTITLE: Shuffling a Dataset using ShuffledDataset in Burn (Rust)\nDESCRIPTION: This snippet illustrates shuffling a dataset for randomized access in Burn using the ShuffledDataset transform. An existing dataset instance (of type DbPedia) is passed along with a seed for deterministic shuffling, ensuring reproducibility. Typical usage includes shuffling prior to splitting data into training and test sets. Inputs are the dataset and numeric seed; output is a shuffled dataset supporting indexed access. Requires the dataset to implement Burn's Dataset trait.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nlet dataset = ShuffledDataset<DbPedia, DbPediaItem>::with_seed(dataset, 42);\n```\n\n----------------------------------------\n\nTITLE: Quantizing Module Weights in Burn\nDESCRIPTION: Example code for quantizing model weights after training using the Quantizer class with MinMax calibration and PerTensor quantization scheme with symmetric QInt8 type.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/quantization.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n// Quantization config\nlet mut quantizer = Quantizer {\n    calibration: Calibration::MinMax,\n    scheme: QuantizationScheme::PerTensor(QuantizationMode::Symmetric, QuantizationType::QInt8),\n};\n\n// Quantize the weights\nlet model = model.quantize_weights(&mut quantizer);\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Backend Traits for Tensor Operations in Burn (Rust)\nDESCRIPTION: This snippet defines two Rust traits: one for a Backend that exposes a fused matmul, addition, and ReLU function, and another for an autodiff-capable backend. The traits extend Burn's existing tensor backend traits, leveraging associated types for tensor primitives. These traits allow alternative or optimized low-level backend implementations for advanced tensor operations, meant to be implemented by backends that wish to support fused kernels. No explicit dependencies besides Burn are required, and the input and output are all FloatTensor types parameterized by the backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-cubecl-kernel.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n/// We create our own Backend trait that extends the Burn backend trait.\npub trait Backend: burn::tensor::backend::Backend {\n    fn fused_matmul_add_relu(\n        lhs: FloatTensor<Self>,\n        rhs: FloatTensor<Self>,\n        bias: FloatTensor<Self>,\n    ) -> FloatTensor<Self>;\n}\n\n/// We create our own AutodiffBackend trait that extends the Burn autodiff backend trait.\npub trait AutodiffBackend: Backend + burn::tensor::backend::AutodiffBackend {}\n```\n\n----------------------------------------\n\nTITLE: Gradient Accumulation for Custom Training (Burn, Rust)\nDESCRIPTION: This snippet shows how to accumulate gradients across multiple backward passes by utilizing the GradientsAccumulator. Gradients are computed using backward, mapped to model parameters, and then accumulated; finally, the accumulated gradients can be retrieved for a combined optimization step. Requires the GradientsAccumulator utility, model, and optimizer context. Useful for mini-batch training where gradient accumulation is preferred before a model update, removing the need for immediate optimizer steps after every batch.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nlet mut accumulator = GradientsAccumulator::new();\nlet grads = model.backward();\nlet grads = GradientsParams::from_grads(grads, &model);\naccumulator.accumulate(&model, grads); ...\nlet grads = accumulator.grads(); // Pop the accumulated gradients.\n```\n\n----------------------------------------\n\nTITLE: Initializing MnistBatcher Struct in Burn\nDESCRIPTION: Defines a simple MnistBatcher struct that will implement the Batcher trait to process MNIST dataset items. This struct serves as the foundation for converting individual data items into batched tensors.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/data.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse burn:{\n    data::{dataloader::batcher::Batcher, dataset::vision::MnistItem},\n    prelude::*,\n};\n\n\n#[derive(Clone, Default)]\npub struct MnistBatcher {}\n```\n\n----------------------------------------\n\nTITLE: Configuring Wgpu Backend and Training Setup in Rust\nDESCRIPTION: This snippet defines the main function that configures the Wgpu backend for GPU-based training, sets up the model and training parameters, and calls the train function. It uses the Autodiff wrapper for differentiability and specifies float and int types for the backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/backend.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse crate::{model::ModelConfig, training::TrainingConfig};\nuse burn:{\n    backend::{Autodiff, Wgpu},\n    optim::AdamConfig,\n};\n\nfn main() {\n    type MyBackend = Wgpu<f32, i32>;\n    type MyAutodiffBackend = Autodiff<MyBackend>;\n\n    let device = burn::backend::wgpu::WgpuDevice::default();\n    let artifact_dir = \"/tmp/guide\";\n    crate::training::train::<MyAutodiffBackend>(\n        artifact_dir,\n        TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),\n        device.clone(),\n    );\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Model Record and Initializing Model in Rust\nDESCRIPTION: This Rust snippet shows how to load serialized model weights from disk into a strongly-typed ModelRecord using NamedMpkFileRecorder, and then initialize a new model instance with those weights using load_record. The approach provides decoupling between the record and the model instance, supporting use cases like weight transfer or deferred instantiation. Required types such as ModelRecord and Model, as well as the associated backend and device, must be defined. The code ensures correct weight assignment and backend compatibility during re-initialization.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/saving-and-loading.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n// Load model record on the backend's default device\nlet record: ModelRecord<MyBackend> = NamedMpkFileRecorder::<FullPrecisionSettings>::new()\n    .load(model_path.into(), &device)\n    .expect(\"Should be able to load the model weights from the provided file\");\n\n// Initialize a new model with the loaded record/weights\nlet model = Model::init(&device).load_record(record);\n```\n\n----------------------------------------\n\nTITLE: Encapsulating Custom Tensor Operations via API Functions in Burn (Rust)\nDESCRIPTION: This snippet provides two API-level Rust functions to apply a fused matmul-add-ReLU operation using either the custom backend or a reference implementation built from primitive tensor ops. Both operate on rank-3 Tensor objects for type B implementing the custom Backend trait. The custom function leverages the backend for optimized execution, while the reference variant aids testing by constructing the result stepwise with matmul, addition, and activation. Inputs are three tensors, all expected to be on the same device; both functions return a rank-3 tensor with the fused output.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-cubecl-kernel.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n/// We define our custom implementation using the added function on our custom backend.\npub fn matmul_add_relu_custom<B: Backend>(\n    lhs: Tensor<B, 3>,\n    rhs: Tensor<B, 3>,\n    bias: Tensor<B, 3>,\n) -> Tensor<B, 3> {\n    let output = B::fused_matmul_add_relu(\n        lhs.into_primitive().tensor(),\n        rhs.into_primitive().tensor(),\n        bias.into_primitive().tensor(),\n    );\n\n    Tensor::from_primitive(TensorPrimitive::Float(output))\n}\n\n/// We define a reference implementation using basic tensor operations.\npub fn matmul_add_relu_reference<B: Backend>(\n    lhs: Tensor<B, 3>,\n    rhs: Tensor<B, 3>,\n    bias: Tensor<B, 3>,\n) -> Tensor<B, 3> {\n    let x = lhs.matmul(rhs) + bias;\n\n    activation::relu(x)\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Backend Trait for WGPU in Rust\nDESCRIPTION: This Rust code implements the custom backend trait for the WGPU backend. It sets up the kernel execution, handles tensor shapes and strides, and launches the kernel with the appropriate parameters.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n/// Implement our custom backend trait for the existing backend `WgpuBackend`.\nimpl<F: FloatElement, I: IntElement> Backend for CubeBackend<WgpuRuntime, F, I> {\n    fn fused_matmul_add_relu(\n        lhs: FloatTensor<Self>,\n        rhs: FloatTensor<Self>,\n        bias: FloatTensor<Self>,\n    ) -> FloatTensor<Self> {\n        // Define cube dim, hardcoded for simplicity.\n        let cube_dim = CubeDim { x: 16, y: 16, z: 1 };\n\n        lhs.assert_is_on_same_device(&rhs);\n        lhs.assert_is_on_same_device(&bias);\n\n        // For simplicity, make sure each tensor is continuous.\n        let lhs = into_contiguous(lhs);\n        let rhs = into_contiguous(rhs);\n        let bias = into_contiguous(bias);\n\n        // Get the matmul relevant shapes.\n        let ndims = lhs.shape.num_dims();\n        let num_rows = lhs.shape.dims[ndims - 2];\n        let num_cols = rhs.shape.dims[ndims - 1];\n\n        // Compute shape of output, while tracking number of batches.\n        let mut num_batches = 1;\n        let mut shape_out = vec![0; ndims];\n        for i in shape_out.clone().into_iter().take(ndims - 2) {\n            shape_out[i] = usize::max(lhs.shape.dims[i], rhs.shape.dims[i]);\n            num_batches *= shape_out[i];\n        }\n        shape_out[ndims - 2] = num_rows;\n        shape_out[ndims - 1] = num_cols;\n        let shape_out = Shape::from(shape_out);\n\n        // Create a buffer for the output tensor.\n        let buffer = lhs\n            .client\n            .empty(shape_out.num_elements() * core::mem::size_of::<F>());\n\n        // Create the output tensor primitive.\n        let output = CubeTensor::new_contiguous(\n            lhs.client.clone(),\n            lhs.device.clone(),\n            shape_out,\n            buffer,\n            F::dtype(),\n        );\n\n        // Create the kernel.\n        let kernel = FusedMatmulAddRelu::<F>::new(cube_dim);\n\n        // Build info buffer with tensor information needed by the kernel, such as shapes and strides.\n        let info = build_info::<_, F>(&[&lhs, &rhs, &output]);\n        let info_handle = lhs.client.create(bytemuck::cast_slice(&info));\n\n        // Declare the wgsl workgroup with the number of cubes in x, y and z.\n        let cubes_needed_in_x = f32::ceil(num_rows as f32 / cube_dim.x as f32) as u32;\n        let cubes_needed_in_y = f32::ceil(num_cols as f32 / cube_dim.y as f32) as u32;\n        let cube_count =\n            CubeCount::Static(cubes_needed_in_x, cubes_needed_in_y, num_batches as u32);\n\n        // Execute lazily the kernel with the launch information and the given buffers.\n        lhs.client.execute(\n            Box::new(SourceKernel::new(kernel, cube_dim)),\n            cube_count,\n            vec![\n                lhs.handle.binding(),\n                rhs.handle.binding(),\n                bias.handle.binding(),\n                output.handle.clone().binding(),\n                info_handle.binding(),\n            ],\n        );\n\n        // Return the output tensor.\n        output\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Rust Model from ONNX in build.rs (Burn, Rust)\nDESCRIPTION: This build script generates Rust code from an ONNX model by using burn_import's ModelGen. It is placed in build.rs and triggers during the Cargo build process. The script specifies the ONNX input file and the output directory for generated code, automating model conversion. Requires burn-import as a build dependency and an existing src/model/mnist.onnx file; the result is a Rust source file representing the model structure and weights.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse burn_import::onnx::ModelGen;\n\nfn main() {\n    // Generate the model code from the ONNX file.\n    ModelGen::new()\n        .input(\"src/model/mnist.onnx\")\n        .out_dir(\"model/\")\n        .run_from_script();\n}\n\n```\n\n----------------------------------------\n\nTITLE: Model Import, Device Initialization, and Loading Model State - Rust (Burn/ndarray)\nDESCRIPTION: This snippet shows how to utilize the generated model by importing it, initializing the device for NdArray backend, and constructing the model instance. The model is created with the default() method and parameterized over the selected backend. Model state loading is performed implicitly through Model::default(). Requires the generated your_model crate/module and the Burn/ndarray backend to be in scope.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/no-std.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse your_model::Model;\n\n// Get a default device for the backend\nlet device = BackendDevice::default();\n\n// Create a new model and load the state\nlet model: Model<Backend> = Model::default();\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Segmentation Mask Dataset from Item List in Burn (Rust)\nDESCRIPTION: This snippet illustrates creating an image segmentation dataset in Burn using pairs of image and mask file paths, along with class labels. Each tuple specifies an image and its corresponding mask, and classes are provided via a string array with integer indices corresponding to mask values. The constructed dataset is suitable for semantic segmentation tasks requiring per-pixel annotation. Requires Burn's dataset/image folder module; inputs are lists of (image, mask) pairs and class names.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n// Create a segmentation mask dataset from a list of items, where each\n// item is a tuple `(image path, mask path)` and a list of classes\n// corresponding to the integer values in the mask.\nlet items = vec![\n    (\n        \"path/to/images/image0.png\",\n        \"path/to/annotations/mask0.png\",\n    ),\n    (\n        \"path/to/images/image1.png\",\n        \"path/to/annotations/mask1.png\",\n    ),\n    (\n        \"path/to/images/image2.png\",\n        \"path/to/annotations/mask2.png\",\n    ),\n];\nlet dataset = ImageFolderDataset::new_segmentation_with_items(\n    items,\n    &[\n        \"cat\", // 0\n        \"dog\", // 1\n        \"background\", // 2\n    ],\n)\n.unwrap();\n```\n\n----------------------------------------\n\nTITLE: Setting Up Remote Distributed Burn Backend in Rust\nDESCRIPTION: This snippet details the server and client side setup for using Burn's Remote backend decorator to offload tensor computations to a remote machine, useful for distributed setups. The server (main_server) runs on a specific port, instantiated with a backend like Cuda. The client (main_client) connects over WebSocket and performs tensor initialization remotely. Dependencies are the 'burn' crate, network-enabled backend modules, and RemoteDevice utility. Inputs are tensor shapes and the remote URI; outputs are tensors residing on the remote hardware. Requires networking and both client/server runtime environments.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nfn main_server() {\n    // Start a server on port 3000.\n    burn::server::start::<burn::backend::Cuda>(Default::default(), 3000);\n}\n\nfn main_client() {\n    // Create a client that communicate with the server on port 3000.\n    use burn::backend::{Autodiff, RemoteBackend};\n\n    type Backend = Autodiff<RemoteDevice>;\n\n    let device = RemoteDevice::new(\"ws://localhost:3000\");\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], Distribution::Default, &device);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Dynamically Importing PyTorch Weights into Burn - Rust\nDESCRIPTION: Loads exported PyTorch model weights into a Burn model at runtime, using the burn-import and relevant backend crates. Requires burn, burn-import, a backend (e.g., burn_ndarray), and the exported .pt file. The example instantiates a Net struct and loads its weights from './conv2d.pt'. This dynamic approach needs the burn-import runtime dependency and is suitable for testing or flexible loading scenarios.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_2\n\nLANGUAGE: Rust\nCODE:\n```\nuse crate::model;\\n\\nuse burn::record::{FullPrecisionSettings, Recorder};\\nuse burn_import::pytorch::PyTorchFileRecorder;\\n\\ntype Backend = burn_ndarray::NdArray<f32>;\\n\\nfn main() {\\n    let device = Default::default();\\n    let record = PyTorchFileRecorder::<FullPrecisionSettings>::default()\\n        .load(\"./conv2d.pt\".into(), &device)\\n        .expect(\"Should decode state successfully\");\\n\\n    let model = model::Net::<Backend>::init(&device).load_record(record);\\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Module Instances Using Burn Backend in Rust\nDESCRIPTION: This snippet shows creating a device using Burn's Wgpu backend and initializing a module via the config's init method. It demonstrates integrating backend setup with configuration-driven module creation, which requires both the burn::backend::Wgpu import and the configuration object. The device is set up via Default::default, after which module instantiation is performed by invoking config.init with the device as argument.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/config.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::backend::Wgpu;\nlet device = Default::default();\nlet my_module = config.init::<Wgpu>(&device);\n```\n\n----------------------------------------\n\nTITLE: Running Burn Regression Example with Various Backends\nDESCRIPTION: This code snippet provides commands to run the regression example using different backends such as ndarray, tch (PyTorch), and wgpu. It includes options for CPU and GPU execution, as well as different BLAS implementations for the ndarray backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/simple-regression/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n# Use the --release flag to really speed up training.\necho \"Using ndarray backend\"\ncargo run --example regression --release --features ndarray                # CPU NdArray Backend - f32 - single thread\ncargo run --example regression --release --features ndarray-blas-openblas  # CPU NdArray Backend - f32 - blas with openblas\ncargo run --example regression --release --features ndarray-blas-netlib    # CPU NdArray Backend - f32 - blas with netlib\necho \"Using tch backend\"\nexport TORCH_CUDA_VERSION=cu124                                            # Set the cuda version\ncargo run --example regression --release --features tch-gpu                # GPU Tch Backend - f32\ncargo run --example regression --release --features tch-cpu                # CPU Tch Backend - f32\necho \"Using wgpu backend\"\ncargo run --example regression --release --features wgpu\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Label Image Classification Dataset from Item List in Burn (Rust)\nDESCRIPTION: This snippet demonstrates creating a multi-label image classification dataset from a list of file paths and label lists. Each tuple in the input associates an image path with multiple textual labels. The dataset is constructed by passing the list of items (image path plus label vector) and an array of all class labels. The resulting dataset supports multi-label classification with arbitrary class membership per image. Primary dependencies are Burn's dataset/image folder modules.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n// Create a multi-label image classification dataset from a list of items,\n// where each item is a tuple `(image path, labels)`, and a list of classes\n// in the dataset.\n//\n// For example:\nlet items = vec![\n    (\"root/dog/dog1.png\", vec![\"animal\".to_string(), \"dog\".to_string()]),\n    (\"root/cat/cat1.png\", vec![\"animal\".to_string(), \"cat\".to_string()]),\n];\nlet dataset = ImageFolderDataset::new_multilabel_classification_with_items(\n    items,\n    &[\"animal\", \"cat\", \"dog\"],\n)\n.unwrap();\n```\n\n----------------------------------------\n\nTITLE: Implementing Training and Validation Step Traits - Burn - Rust\nDESCRIPTION: Implements the TrainStep and ValidStep traits for the Model struct, enabling integration with Burn's training loop. The training step computes a forward pass and performs automatic differentiation by calling backward on the loss, while the validation step only computes the forward pass without gradients. Requires the model, input batches (MnistBatch), and that the backend implements either AutodiffBackend or Backend traits. Outputs either a TrainOutput struct containing model, gradients, and ClassificationOutput, or only the ClassificationOutput for validation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/training.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: AutodiffBackend> TrainStep<MnistBatch<B>, ClassificationOutput<B>> for Model<B> {\n    fn step(&self, batch: MnistBatch<B>) -> TrainOutput<ClassificationOutput<B>> {\n        let item = self.forward_classification(batch.images, batch.targets);\n\n        TrainOutput::new(self, item.loss.backward(), item)\n    }\n}\n\nimpl<B: Backend> ValidStep<MnistBatch<B>, ClassificationOutput<B>> for Model<B> {\n    fn step(&self, batch: MnistBatch<B>) -> ClassificationOutput<B> {\n        self.forward_classification(batch.images, batch.targets)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading imported ONNX models in Rust with different methods\nDESCRIPTION: This code snippet shows various ways to load an imported ONNX model in Rust, including creating a new instance, loading from a file, loading from embedded weights, and loading with default settings.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/onnx-model.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n// Create a new model instance with device. Initializes weights randomly and lazily.\n// You can load weights via `load_record` afterwards.\nlet model = Model::<Backend>::new(&device);\n\n// Load from a file (must specify weights file in the target output directory or copy it from there).\n// File type should match the record type specified in `ModelGen`.\nlet model = Model::<Backend>::from_file(\"path/to/weights\", &device);\n\n// Load from embedded weights (if embed_states was true)\nlet model = Model::<Backend>::from_embedded(&device);\n\n// Load from the out director location and load to default device (useful for testing)\nlet model = Model::<Backend>::default();\n```\n\n----------------------------------------\n\nTITLE: Defining the Dataset Trait in Burn (Rust)\nDESCRIPTION: This snippet defines the Dataset trait to be implemented by data types in Burn, establishing the interface for dataset access and iteration. The trait requires two methods: get, to retrieve items by index, and len, to obtain the total number of items. Implementors must ensure random constant-time access to a fixed-length collection as well as thread-safety via Send and Sync traits. No external dependencies beyond Rust's trait system are needed. Inputs are usize indices; outputs are Option<I> for get and usize for len.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\npub trait Dataset<I>: Send + Sync {\n    fn get(&self, index: usize) -> Option<I>;\n    fn len(&self) -> usize;\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Tensor Arithmetic using Burn in Rust\nDESCRIPTION: This snippet adds two tensors with matching shapes using Burn's operator overloading, producing a new tensor as the result and printing it for verification. Prior device setup is assumed, and both input tensors are created (filled with ones and a constant value, respectively) before the addition. The code demonstrates basic elementwise operations, but requires the tensors to have the same shape and reside on the same device and backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/basic-tensor-op.ipynb#_snippet_3\n\nLANGUAGE: Rust\nCODE:\n```\nlet x1: Tensor<B,2> = Tensor::ones([2, 2], &device);\nlet x2: Tensor<B,2> = Tensor::full([2, 2], 7.0, &device);\n\nlet x3 = x1 + x2;\n\nprintln!(\"x3 = {}\", x3);\n```\n\n----------------------------------------\n\nTITLE: Running an Autodiff Training Session with Burn WGPU Backend in Rust\nDESCRIPTION: This Rust snippet demonstrates how to use the Burn WGPU backend with the autodiff feature enabled. It enables the wgpu backend using a feature flag, imports necessary modules, and runs a training session leveraging autodiff and the Wgpu backend. Dependencies: burn_autodiff, burn_wgpu, mnist crate with its training module. The run function initializes a default WgpuDevice and passes it into a generic run function using specific generic parameters. The code is structured for modular use with feature gating. The input is a generic device; the output is controlled by the training library. If the 'wgpu' feature is not enabled, this block is ignored.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-wgpu/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n#[cfg(feature = \"wgpu\")]\nmod wgpu {\n    use burn_autodiff::Autodiff;\n    use burn_wgpu::{Wgpu, WgpuDevice};\n    use mnist::training;\n\n    pub fn run() {\n        let device = WgpuDevice::default();\n        training::run::<Autodiff<Wgpu<f32, i32>>>(device);\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Serde-Serializable Data Item and Loading a Hugging Face Dataset in Burn (Rust)\nDESCRIPTION: This snippet demonstrates defining a serializable Rust struct (DbPediaItem) with serde and Burn trait derives, and loading a Hugging Face dataset into a SqliteDataset using HuggingfaceDatasetLoader. The struct must implement Clone, Debug, Serialize, and Deserialize for compatibility with the storage and loader utilities. The loader instantiates a dataset from the specified Hugging Face dataset name and split. Key parameters include trait derives and the dataset name/split to load.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]\npub struct DbPediaItem {\n    pub title: String,\n    pub content: String,\n    pub label: usize,\n}\n\nfn main() {\n    let dataset: SqliteDataset<DbPediaItem> = HuggingfaceDatasetLoader::new(\"dbpedia_14\")\n        .dataset(\"train\") // The training split.\n        .unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Example Output from Burn ONNX Inference Execution (Bash)\nDESCRIPTION: This snippet provides the typical expected output when running the ONNX inference example in Burn. It shows compilation completion, execution of the Rust binary, and diagnostic results for image classification (including correct prediction, class output, and hyperlink for visual inspection). No input is required; serves as a demonstration for verifying proper setup and execution.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nFinished dev [unoptimized + debuginfo] target(s) in 0.13s\n    Running `burn/target/debug/onnx-inference 15`\n\nImage index: 15\nSuccess!\nPredicted: 5\nActual: 5\nSee the image online, click the link below:\nhttps://huggingface.co/datasets/ylecun/mnist/viewer/mnist/test?row=15\n```\n\n----------------------------------------\n\nTITLE: Defining Enum and Struct Modules for Model Composition in Burn (Rust)\nDESCRIPTION: This Rust code defines a typical modular network with an enum representing multiple convolutional options and nested struct modules for detailed composition. The Conv enum can represent either a DwsConv or regular Conv2d variant, and both DwsConv and Net structs are modular. These definitions are prerequisites for mapping imported weights into the correct variant and submodules using Burn's model import logic. No runtime logic is present; the focus is on structure and trait derivation via Module. Inputs and outputs depend on later network instantiation and loading.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Module, Debug)]\npub enum Conv<B: Backend> {\n    DwsConv(DwsConv<B>),\n    Conv(Conv2d<B>),\n}\n\n#[derive(Module, Debug)]\npub struct DwsConv<B: Backend> {\n    dconv: Conv2d<B>,\n    pconv: Conv2d<B>,\n}\n\n#[derive(Module, Debug)]\npub struct Net<B: Backend> {\n    conv: Conv<B>,\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Training and Test Datasets Using ImageFolderDataset - Rust\nDESCRIPTION: Shows how to instantiate the training and test splits using the ImageFolderDataset::new_classification method. Requires the Burn framework, correctly structured dataset folders, and the Rust standard environment. The main parameters are the paths to the train and test directories; on success, the datasets are ready for training and evaluation. Outputs loaded datasets; will 'unwrap' panic on failure, so error handling may be desired in production settings.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/custom-image-dataset/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet train_ds = ImageFolderDataset::new_classification(\"/path/to/cifar10/train\").unwrap();\\nlet test_ds = ImageFolderDataset::new_classification(\"/path/to/cifar10/test\").unwrap();\n```\n\n----------------------------------------\n\nTITLE: Executing Inference on MNIST ONNX Model in Burn (Burn, Rust)\nDESCRIPTION: This end-to-end Rust code sample shows usage of the generated ONNX MNIST model for inference with Burn. It initializes a backend device, instantiates the model and loads its state, creates a sample input tensor, and computes the model's output. Dependencies include the Burn framework and ndarry backend. The main function expects no parameters and prints the result tensor; the demo uses an all-zero input. Limitations: for demonstration, real inference should pass normalized image data.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor;\nuse burn::backend::ndarray::NdArray;\n\nuse onnx_inference::mnist::Model;\n\nfn main() {\n    // Get a default device for the models's backend\n    let device = Default::default();\n\n    // Create a new model and load the state\n    let model: Model<Backend> = Model::new(&device).load_state();\n\n    // Create a new input tensor (all zeros for demonstration purposes)\n    let input = tensor::Tensor::<NdArray<f32>, 4>::zeros([1, 1, 28, 28], &device);\n\n    // Run the model\n    let output = model.forward(input);\n\n    // Print the output\n    println!(\"{:?}\", output);\n}\n```\n\n----------------------------------------\n\nTITLE: Sampling Items Using SamplerDataset in Burn (Rust)\nDESCRIPTION: This snippet demonstrates initializing and sampling from a dataset using the SamplerDataset transform in Burn. It assumes the existence of a `SqliteDataset` containing items of type `DbPediaItem`, loading them via `HuggingfaceDatasetLoader`, and then wrapping the dataset for sampling a set number of items (here: 10,000). Key dependencies include Burn's dataset API and serde-serializable item types. Input parameters are a base dataset and a sample size; the returned dataset enables sampled iteration with optional replacement. Requires prior definition of `DbPediaItem` struct.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\ntype DbPedia = SqliteDataset<DbPediaItem>;\nlet dataset: DbPedia = HuggingfaceDatasetLoader::new(\"dbpedia_14\")\n        .dataset(\"train\").\n        .unwrap();\n\nlet dataset = SamplerDataset<DbPedia, DbPediaItem>::new(dataset, 10000);\n```\n\n----------------------------------------\n\nTITLE: Running Text Classification with WGPU Backend\nDESCRIPTION: Commands for training and inferencing text classification models using WGPU backend for GPU acceleration.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-classification/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n\n# Use the --release flag to really speed up training.\n\n# AG News\ncargo run --example ag-news-train --release --features wgpu   # Train on the ag news dataset\ncargo run --example ag-news-infer --release --features wgpu   # Run inference on the ag news dataset\n\n# DbPedia\ncargo run --example db-pedia-train --release --features wgpu  # Train on the db pedia dataset\ncargo run --example db-pedia-infer --release --features wgpu  # Run inference db pedia dataset\n```\n\n----------------------------------------\n\nTITLE: Running the Custom CSV Dataset Example in Burn Framework\nDESCRIPTION: Command to execute the custom-csv-dataset example which demonstrates loading and processing a diabetes dataset from CSV format using the Burn ML framework.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/custom-csv-dataset/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --example custom-csv-dataset\n```\n\n----------------------------------------\n\nTITLE: Running LSTM Inference with CUDA Backend\nDESCRIPTION: Command for running LSTM inference using the CUDA backend. This demonstrates the basic inference execution with GPU acceleration.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/modern-lstm/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --example lstm-infer --release --features cuda\n```\n\n----------------------------------------\n\nTITLE: Loading Datasets from CSV with InMemDataset in Rust\nDESCRIPTION: This snippet illustrates how to build an in-memory dataset from a CSV file using the Burn framework with the \\'csv\\' crate in Rust. It creates a CSV reader that expects tab-separated values, then loads the referenced file via \\'InMemDataset::from_csv\\'. Required dependencies include the \\'csv\\' crate and Burn. The main parameter is the CSV path and optional CSV reader configuration. Output is an in-memory dataset; error handling is via \\'unwrap\\'. Streaming datasets are not supported in this context.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\n// Build dataset from csv with tab ('\\t') delimiter.\n// The reader can be configured for your particular file.\nlet mut rdr = csv::ReaderBuilder::new();\nlet rdr = rdr.delimiter(b'\\t');\n\nlet dataset = InMemDataset::from_csv(\"path/to/csv\", rdr).unwrap();\n```\n\n----------------------------------------\n\nTITLE: Running Example Code with Cargo - Bash\nDESCRIPTION: This snippet shows how to execute the example neural network training and inference for the MNIST dataset using Cargo, Rust's package manager and build tool. It runs the predefined example named 'guide' located in the Burn project directory. No dependencies are required beyond a working Rust toolchain with Cargo, and all necessary project dependencies pre-installed. This command assumes you are located at Burn's root directory and will execute all code within the example, enabling users to follow along with the guide and replicate the training process.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --example guide\n```\n\n----------------------------------------\n\nTITLE: Running Burn ML Model Training in Console\nDESCRIPTION: This console command runs the Rust project in release mode to start the model training process. It will display a CLI dashboard showing the training progression.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/backend.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ncargo run --release\n```\n\n----------------------------------------\n\nTITLE: Inference and No Grad Context Blocks - PyTorch (Python)\nDESCRIPTION: These Python snippets show how inference or no-gradient contexts are created in PyTorch using torch.inference() and torch.no_grad() context managers. These blocks ensure that no gradients are calculated during model evaluation or inference, improving performance and preventing unnecessary computation. Requires the PyTorch library; code inside these blocks can use variables and functions as needed, but autograd is disabled within the scope.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/autodiff.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Inference mode\ntorch.inference():\n   # your code\n   ...\n\n# Or no grad\ntorch.no_grad():\n   # your code\n   ...\n```\n\n----------------------------------------\n\nTITLE: Configuring build dependencies for ONNX import in Rust\nDESCRIPTION: This snippet shows how to add the burn-import crate as a build dependency in the Cargo.toml file. It's required for generating Rust code from ONNX models during the build process.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/onnx-model.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[build-dependencies]\nburn-import = \"~0.18\"\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors in Burn using Rust\nDESCRIPTION: This snippet illustrates multiple methods for initializing tensors on a specified device: as empty, from a slice of floats (with reshaping), random values, a constant value, all zeros, and all ones. Dependencies include prior backend and device initialization via Burn and burn-ndarray. The code emphasizes parameterization via shape and device, and demonstrates that both statically and dynamically filled tensors can be constructed. Output is shown via println for inspection, but further tensor usage requires matching device and backend handling.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/basic-tensor-op.ipynb#_snippet_2\n\nLANGUAGE: Rust\nCODE:\n```\n// A device the tensors will be stored on\nlet device = <B as Backend>::Device::default();\n// Create an empty tensor for a given shape\nlet tensor: Tensor<B, 3> = Tensor::empty([1, 2, 3], &device);\nprintln!(\"Empty tensor: {}\", tensor);\n\n// Create a tensor from a slice of floats\nlet tensor: Tensor<B, 2> = Tensor::from_floats([1.0, 2.0, 3.0, 4.0], &device).reshape([2, 2]);\nprintln!(\"Tensor from slice: {}\", tensor);\n\n// Create a random tensor\nuse burn::tensor::Distribution;\nlet tensor: Tensor<B, 1> = Tensor::random([5], Distribution::Default, &device);\nprintln!(\"Random tensor: {}\", tensor);\n\n// Create a tensor using fill values, zeros, or ones\nlet tensor: Tensor<B,2> = Tensor::full([2, 2], 7.0, &device);\nlet tensor: Tensor<B,2> = Tensor::zeros([2, 2], &device);\nlet tensor: Tensor<B,2> = Tensor::ones([2, 2], &device);\n```\n\n----------------------------------------\n\nTITLE: Using imported ONNX model for inference in Rust\nDESCRIPTION: This code demonstrates how to use an imported ONNX model for inference in a Rust program. It includes creating a model instance, preparing input data, and performing forward pass to get the output.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/onnx-model.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor;\nuse burn_ndarray::{NdArray, NdArrayDevice};\nuse model::my_model::Model;\n\nfn main() {\n    let device = NdArrayDevice::default();\n\n    // Create model instance and load weights from target dir default device.\n    // (see more load options below in \"Loading and Using Models\" section)\n    let model: Model<NdArray<f32>> = Model::default();\n\n    // Create input tensor (replace with your actual input)\n    let input = tensor::Tensor::<NdArray<f32>, 4>::zeros([1, 3, 224, 224], &device);\n\n    // Perform inference\n    let output = model.forward(input);\n\n    println!(\"Model output: {:?}\", output);\n}\n```\n\n----------------------------------------\n\nTITLE: Alternative Tensor Creation with Type Annotations\nDESCRIPTION: This code snippet demonstrates an alternative way to create tensors using type annotations instead of the turbofish syntax.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/getting-started.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet tensor_1: Tensor<Backend, 2> = Tensor::from_data([[2., 3.], [4., 5.]]);\nlet tensor_2 = Tensor::ones_like(&tensor_1);\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Backend Trait for Specific Backends in Rust\nDESCRIPTION: These Rust examples implement the custom `Backend` trait for two backends: `burn_tch::LibTorch<E>` and `burn_ndarray::NdArray<E>`, showing how to provide a backend-specific implementation for `my_new_function` or rely on the default. Dependencies include the Burn backend interfaces and the corresponding tensor types. The parameters are type-generic; for `LibTorch`, a backend-specific implementation can improve performance, while `NdArray` defaults to the trait's implementation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nimpl<E: TchElement> Backend for burn_tch::LibTorch<E> {\n   fn my_new_function(tensor: TchTensor<E, 2>) -> TchTensor<E, 2> {\n      // My Tch implementation\n   }\n}\n\nimpl<E: NdArrayElement> Backend for burn_ndarray::NdArray<E> {\n    // No specific implementation, but the backend can still be used.\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Dequantization with ModuleMapper in Burn\nDESCRIPTION: Example implementation of a ModuleMapper for dequantizing model parameters when loading a quantized model, allowing storage in reduced precision while maintaining inference speed.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/quantization.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n/// Module mapper used to dequantize the model params being loaded.\npub struct Dequantize {}\n\nimpl<B: Backend> ModuleMapper<B> for Dequantize {\n    fn map_float<const D: usize>(\n        &mut self,\n        _id: ParamId,\n        tensor: Tensor<B, D>,\n    ) -> Tensor<B, D> {\n        tensor.dequantize()\n    }\n}\n\n// Load saved quantized model in floating point precision\nmodel = model\n    .load_file(file_path, recorder, &device)\n    .expect(\"Should be able to load the quantized model weights\")\n    .map(&mut Dequantize {});\n```\n\n----------------------------------------\n\nTITLE: Including generated ONNX model code in Rust module\nDESCRIPTION: This snippet shows how to include the generated Rust code for the ONNX model in a mod.rs file. It makes the generated model code available for use in the project.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/onnx-model.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\npub mod my_model {\n    include!(concat!(env!(\"OUT_DIR\"), \"/model/my_model.rs\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring advanced options for ONNX model import in Rust\nDESCRIPTION: This snippet illustrates advanced configuration options for the ModelGen struct when importing ONNX models. It includes options for specifying record type, precision, and state embedding.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/onnx-model.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nModelGen::new()\n    .input(\"path/to/model.onnx\")\n    .out_dir(\"model/\")\n    .record_type(RecordType::NamedMpk)\n    .half_precision(false)\n    .embed_states(false)\n    .run_from_script();\n```\n\n----------------------------------------\n\nTITLE: Implementing a Step Function as a Generic Method with Inline Trait Bounds in Rust\nDESCRIPTION: This code defines an implementation block for a struct (Learner2) generic over model and optimizer types (M, O), with the step method itself generic over B (backend). Trait bounds for B, M, and O are specified inline within the method signature and its where clause, allowing deferred trait checking until the function is used. This approach avoids the unconstrained type parameter error and provides flexibility. Dependencies are similar to previous implementations and include AutodiffBackend, AutodiffModule, Optimizer, and MnistBatch.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\n#[allow(dead_code)]\nimpl<M, O> Learner2<M, O> {\n    pub fn step<B: AutodiffBackend>(&mut self, _batch: MnistBatch<B>)\n    where\n        B: AutodiffBackend,\n        M: AutodiffModule<B>,\n        O: Optimizer<M, B>,\n    {\n        //\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Fused Matmul-Add-ReLU Kernel with CubeCL Macro (Rust)\nDESCRIPTION: This Rust code defines a kernel function for CubeCL using the #[cube(launch)] macro to perform a batched, fused matmul-add-ReLU operation. The kernel explicitly handles batch offsets, data strides, and multi-dimensional shape access. It first maps compute units to the correct data segment, computes the matrix multiplication sum, adds the bias, applies ReLU (max with zero), and writes to the output tensor. It accepts references to the input and output tensors, requiring types compatible with CubeCL and float support (trait F: Float). Inputs must be correctly shaped and contiguous as this is a lower-level kernel.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-cubecl-kernel.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse cubecl::{cube, prelude::*};\n\n#[cube(launch)]\npub fn fused_matmul_add_relu_kernel<F: Float>(\n    lhs: &Tensor<F>,\n    rhs: &Tensor<F>,\n    bias: &Tensor<F>,\n    output: &mut Tensor<F>,\n) {\n    let row = ABSOLUTE_POS_X;\n    let col = ABSOLUTE_POS_Y;\n    let batch = ABSOLUTE_POS_Z;\n\n    let n_rows = output.shape(output.rank() - 2);\n    let n_cols = output.shape(output.rank() - 1);\n    let dim_k = rhs.shape(rhs.rank() - 1);\n\n    if row >= n_rows || col >= n_cols {\n        return;\n    }\n\n    let offset_output = batch * n_rows * n_cols;\n    let mut offset_lhs = 0;\n    let mut offset_rhs = 0;\n\n    let batch_dims = output.rank() - 2;\n    for dim in 0..batch_dims {\n        offset_lhs += offset_output / output.stride(dim) % lhs.shape(dim) * lhs.stride(dim);\n        offset_rhs += offset_output / output.stride(dim) % rhs.shape(dim) * rhs.stride(dim);\n    }\n\n    let mut sum = F::new(0.0);\n    for k in 0..dim_k {\n        let lhs_index = row * dim_k + k;\n        let rhs_index = k * n_cols + col;\n\n        sum += lhs[offset_lhs + lhs_index] * rhs[offset_rhs + rhs_index];\n    }\n\n    let out_index = row * n_cols + col;\n    let index = offset_output + out_index;\n\n    output[index] = F::max(sum + bias[index], F::new(0.0));\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Parameter Clamping with ModuleMapper in Burn\nDESCRIPTION: Example implementation of the ModuleMapper trait for clamping module parameters to a specific range. This shows how to create custom operations that transform all parameters in a module.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/module.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n/// Clamp parameters into the range `[min, max]`.\npub struct Clamp {\n    /// Lower-bound of the range.\n    pub min: f32,\n    /// Upper-bound of the range.\n    pub max: f32,\n}\n\n// Clamp all floating-point parameter tensors between `[min, max]`.\nimpl<B: Backend> ModuleMapper<B> for Clamp {\n    fn map_float<const D: usize>(\n        &mut self,\n        _id: burn::module::ParamId,\n        tensor: burn::prelude::Tensor<B, D>,\n    ) -> burn::prelude::Tensor<B, D> {\n        tensor.clamp(self.min, self.max)\n    }\n}\n\n// Clamp module mapper into the range `[-0.5, 0.5]`\nlet mut clamp = Clamp {\n    min: -0.5,\n    max: 0.5,\n};\nlet model = model.map(&mut clamp);\n```\n\n----------------------------------------\n\nTITLE: Saving Model with MessagePack Recorder in Rust\nDESCRIPTION: This code snippet demonstrates saving a trained machine learning model using Burn's NamedMpkFileRecorder in Rust. The model is serialized in MessagePack format with full precision settings by calling model.save_file and providing the recorder and target file path. The code expects the correct dependencies to be present, such as Burn, MessagePack serialization (via rmp_serde), and a valid model instance. The model path must be specified, and the expect call ensures the operation succeeds or panics with a meaningful message.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/saving-and-loading.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n// Save model in MessagePack format with full precision\nlet recorder = NamedMpkFileRecorder::<FullPrecisionSettings>::new();\nmodel\n    .save_file(model_path, &recorder)\n    .expect(\"Should be able to save the model\");\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model Conversion Command\nDESCRIPTION: Command to generate IR and Burn graph from an ONNX model. This command processes the ONNX file and outputs the model files in the specified output directory.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-import/DEVELOPMENT.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo r -- ./onnx-tests/tests/<op>/<op>.onnx ./out\n```\n\n----------------------------------------\n\nTITLE: Declaring getrandom Dependency for WASM in Rust (TOML)\nDESCRIPTION: This snippet shows how to add the getrandom dependency to your Cargo.toml file with the wasm_js feature flag enabled. The configuration disables default features and explicitly enables the wasm_js feature, which is necessary for random number generation support in browser-based WebAssembly environments. No external libraries are needed beyond getrandom 0.3.2, and this must be added under the [dependencies] section.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/web-assembly.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\ngetrandom = { version = \"0.3.2\", default-features = false, features = [\n    \"wasm_js\",\n] }\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables in Cargo Config for LibTorch - toml\nDESCRIPTION: This TOML configuration snippet sets LIBTORCH and LD_LIBRARY_PATH as environment variables in .cargo/config.toml, allowing Rust builds to automatically include these settings without needing shell export commands. Suitable for persistent development environments; values must be customized per installation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_14\n\nLANGUAGE: toml\nCODE:\n```\n[env]\nLD_LIBRARY_PATH = \"/absolute/path/to/libtorch/lib\"\nLIBTORCH = \"/absolute/path/to/libtorch/libtorch\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Image Classification Dataset from Folder Structure in Burn (Rust)\nDESCRIPTION: This snippet shows how to create an image classification dataset in Burn using ImageFolderDataset. Images are expected to be stored in subfolders corresponding to class labels. The dataset is constructed by providing the root path of the folder structure, and the loader infers classes based on directory names. The function returns a dataset suitable for classification tasks over the found classes. The only dependency is the Burn image loader; the input is the root path of the image dataset.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n// Create an image classification dataset from the root folder,\n// where images for each class are stored in their respective folder.\n//\n// For example:\n// root/dog/dog1.png\n// root/dog/dog2.png\n// ...\n// root/cat/cat1.png\nlet dataset = ImageFolderDataset::new_classification(\"path/to/dataset/root\").unwrap();\n```\n\n----------------------------------------\n\nTITLE: Initializing and Saving Model Weights with MessagePack in Rust\nDESCRIPTION: Here, a machine learning model is initialized on a specified device, and then saved using the NamedMpkFileRecorder for MessagePack serialization in full precision. The device is determined via Default::default(), and the Model is instantiated via its init method. This setup ensures the model exists with valid weights prior to saving. All dependencies (Burn, device backend, and recorder) must be available, and the snippet facilitates seamless model checkpointing for later restoration.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/saving-and-loading.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n// Create a dummy initialized model to save\nlet device = Default::default();\nlet model = Model::<MyBackend>::init(&device);\n\n// Save model in MessagePack format with full precision\nlet recorder = NamedMpkFileRecorder::<FullPrecisionSettings>::new();\nmodel\n    .save_file(model_path, &recorder)\n    .expect(\"Should be able to save the model\");\n```\n\n----------------------------------------\n\nTITLE: Organizing Tensor Operations in Rust Traits\nDESCRIPTION: Shows how tensor operations are organized into different traits based on their applicability to different tensor kinds. This structure allows for clear separation of concerns and easy extensibility.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/project-architecture/tensor.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\ntrait BaseTensorOp { /* ... */ }\ntrait NumericTensorOp { /* ... */ }\ntrait FloatTensorOp { /* ... */ }\ntrait IntTensorOp { /* ... */ }\ntrait BoolTensorOp { /* ... */ }\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LibTorch CUDA on Linux - shell\nDESCRIPTION: This snippet sets the LIBTORCH and LD_LIBRARY_PATH environment variables to point to the manually-installed LibTorch CUDA backend for Linux. Both variables should be set before building or running burn-tch with CUDA support; ensure the CUDA installation is also present in PATH and LD_LIBRARY_PATH.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nexport LIBTORCH=/absolute/path/to/libtorch/\nexport LD_LIBRARY_PATH=/absolute/path/to/libtorch/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Extracting Model Configuration from a .pt File - Burn, Rust\nDESCRIPTION: Extracts configuration from a PyTorch .pt file using config_from_file and serializes it to JSON for later use. Requires burn, burn_import, and serde for config representation. NetConfig is a custom struct with various fields, demonstrating diverse configuration values. The key parameters are the .pt file path and top-level key. Outputs the decoded configuration struct and saves it to 'my_config.json'.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_4\n\nLANGUAGE: Rust\nCODE:\n```\nuse std::collections::HashMap;\\n\\nuse burn::config::Config;\\nuse burn_import::pytorch::config_from_file;\\n\\n#[derive(Debug, Config)]\\nstruct NetConfig {\\n    n_head: usize,\\n    n_layer: usize,\\n    d_model: usize,\\n    some_float: f64,\\n    some_int: i32,\\n    some_bool: bool,\\n    some_str: String,\\n    some_list_int: Vec<i32>,\\n    some_list_str: Vec<String>,\\n    some_list_float: Vec<f64>,\\n    some_dict: HashMap<String, String>,\\n}\\n\\nfn main() {\\n    let path = \"weights_with_config.pt\";\\n    let top_level_key = Some(\"my_config\");\\n    let config: NetConfig = config_from_file(path, top_level_key).unwrap();\\n    println!(\"{:#?}\", config);\\n\\n    // After extracting, it's recommended you save it as a json file.\\n    config.save(\"my_config.json\").unwrap();\\n}\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Version Environment Variable on Windows - PowerShell\nDESCRIPTION: This PowerShell snippet assigns the TORCH_CUDA_VERSION environment variable for use with the Torch backend on Windows. It must be set prior to dependency resolution by cargo to select the appropriate CUDA runtime. Requires proper driver and CUDA version compatibility, with no direct input or output.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\n$Env:TORCH_CUDA_VERSION = \"cu124\"\n```\n\n----------------------------------------\n\nTITLE: Importing Burn Packages and Setting up Backend\nDESCRIPTION: Imports essential components from the Burn library, including Tensor and the NdArray backend. Also imports visualization library and creates a type alias for the backend to simplify tensor creation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/plots.ipynb#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n// Import packages\nuse burn::tensor::Tensor;\nuse burn_ndarray::NdArray\n\n// Import plotting library\nuse evcxr_image::ImageDisplay;\n\n// Type alias for the backend\ntype B = NdArray<f32>;\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting LibTorch CUDA Distribution on Windows - PowerShell\nDESCRIPTION: This PowerShell snippet downloads the CUDA-enabled LibTorch (version 12.6) for Windows and extracts it with Expand-Archive. The archive URL specifies both the platform and CUDA version. The content is extracted to the current directory for later configuration steps.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_11\n\nLANGUAGE: powershell\nCODE:\n```\nwget https://download.pytorch.org/libtorch/cu126/libtorch-win-shared-with-deps-2.6.0%2Bcu126.zip -OutFile libtorch.zip\nExpand-Archive libtorch.zip\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural Network Module in Rust with Burn Framework\nDESCRIPTION: Demonstrates how to create a position-wise feed forward neural network module using Burn's nn module system. The implementation includes a forward pass with linear layers, dropout, and GELU activation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::nn;\nuse burn::module::Module;\nuse burn::tensor::backend::Backend;\n\n#[derive(Module, Debug)]\npub struct PositionWiseFeedForward<B: Backend> {\n    linear_inner: nn::Linear<B>,\n    linear_outer: nn::Linear<B>,\n    dropout: nn::Dropout,\n    gelu: nn::Gelu,\n}\n\nimpl<B: Backend> PositionWiseFeedForward<B> {\n    pub fn forward<const D: usize>(&self, input: Tensor<B, D>) -> Tensor<B, D> {\n        let x = self.linear_inner.forward(input);\n        let x = self.gelu.forward(x);\n        let x = self.dropout.forward(x);\n\n        self.linear_outer.forward(x)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining the NdArray Backend and Device for Burn Inference - Rust\nDESCRIPTION: This snippet imports Burn\\'s NdArray backend and Tensor, then defines type aliases for the Backend (NdArray using f32 data) and BackendDevice (the device associated with the selected backend). This setup is required for subsequent tensor computations and model inference using the Burn framework. The backend selection is static and expected to be used throughout the inference workflow.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/no-std.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::{backend::NdArray, tensor::Tensor};\n\ntype Backend = NdArray<f32>;\ntype BackendDevice = <Backend as burn::tensor::backend::Backend>::Device;\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor Structure in Rust\nDESCRIPTION: Explains the structure of the Tensor type in Burn, which uses generic parameters for backend, dimensionality, and tensor kind. This design allows for a single, flexible tensor type that can be specialized as needed.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/project-architecture/tensor.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nstruct Tensor<B: Backend, const D: usize, K: TensorKind = Float> { /* ... */ }\n```\n\n----------------------------------------\n\nTITLE: Downloading and Unzipping LibTorch CPU Distribution on MacOS - shell\nDESCRIPTION: This shell snippet downloads and extracts the LibTorch CPU binary distribution for MacOS. It fetches a prebuilt archive for the current platform and version, then unpacks it. This is required for using tch-rs with a CPU backend on MacOS.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nwget -O libtorch.zip https://download.pytorch.org/libtorch/cpu/libtorch-macos-x86_64-2.6.0.zip\nunzip libtorch.zip\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Setting Up for MPS (Metal) Backend - shell\nDESCRIPTION: This shell snippet installs torch and its Python dependencies for enabling MPS (Metal Performance Shaders) acceleration on MacOS via pip, sets an environment variable to instruct tch-rs to use the Python-based LibTorch, and updates DYLD_LIBRARY_PATH. Only required for MacOS 12.3+ when official MPS binaries are unavailable.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npip install torch==2.6.0 numpy==1.26.4 setuptools\nexport LIBTORCH_USE_PYTORCH=1\nexport DYLD_LIBRARY_PATH=/path/to/pytorch/lib:$DYLD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Running Burn Dataset Audio Example with Cargo - Shell\nDESCRIPTION: Shows how to execute the Burn Dataset's SpeechCommands example with the audio feature enabled by using Cargo. This command requires the Rust toolchain with Cargo installed and the Burn Dataset project checked out. The feature flag 'audio' activates audio-specific dataset handling for the run; outputs and logs depend on the dataset and example code structure.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-dataset/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncargo run --example speech_commands --features audio\n```\n\n----------------------------------------\n\nTITLE: Enabling Autodiff with Burn Backends in Rust\nDESCRIPTION: This snippet demonstrates wrapping a base backend (here, Wgpu) with the Autodiff decorator to add automatic differentiation capability. Dependencies include the 'burn' crate with backend and tensor features. The example shows creation of 2D tensors, manipulation via arithmetic and matrix operations, and computing gradients via backward(). Inputs are randomly generated tensors; outputs include the gradient of y after backpropagation. The key limitation is that backward() can only be called on backends supporting autodiff, preventing misuse.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::backend::{Autodiff, Wgpu};\nuse burn::tensor::{Distribution, Tensor};\n\nfn main() {\n    type Backend = Autodiff<Wgpu>;\n\n    let x: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default);\n    let y: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default).require_grad();\n\n    let tmp = x.clone() + y.clone();\n    let tmp = tmp.matmul(x);\n    let tmp = tmp.exp();\n\n    let grads = tmp.backward();\n    let y_grad = y.grad(&grads).unwrap();\n    println!(\"{y_grad}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom GELU Activation using Burn Tensor API in Rust\nDESCRIPTION: This snippet demonstrates how to implement a custom GELU (Gaussian Error Linear Unit) activation function using Burn's high-level tensor API in Rust. It leverages Burn's 'Backend' trait for generic backend support and supports tensors with arbitrary dimensions via Rust generics. Parameters include a tensor 'x' and a backend type 'B', with the code making use of element-wise operations and efficient memory handling. The output is a tensor of the same shape as the input, and the snippet enables runtime automatic kernel fusion and hardware adaptation in Burn backends. Dependencies: Burn Rust library with tensor, math, and backend modules.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nfn gelu_custom<B: Backend, const D: usize>(x: Tensor<B, D>) -> Tensor<B, D> {\n    let x = x.clone() * ((x / SQRT_2).erf() + 1);\n    x / 2\n}\n```\n\n----------------------------------------\n\nTITLE: Loading with Attribute Remapping using PyTorchFileRecorder - Burn, Rust\nDESCRIPTION: Demonstrates how to remap attribute keys using regular expressions when loading PyTorch weights into Burn, useful for cases where model architectures between source and target differ. Requires Burn, burn_import, and knowledge of regex key patterns. The .with_key_remap call adjusts attribute key names, so e.g. 'conv.conv1' becomes 'conv1'. It's best suited for advanced use where the field structure between models isn't a one-to-one match.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_6\n\nLANGUAGE: Rust\nCODE:\n```\nlet device = Default::default();\\nlet load_args = LoadArgs::new(\"tests/key_remap/key_remap.pt\".into())\\n    // Remove \"conv\" prefix, e.g. \"conv.conv1\" -> \"conv1\"\\n    .with_key_remap(\"conv\\\\.(.*)\", \"$1\");\\n\\nlet record = PyTorchFileRecorder::<FullPrecisionSettings>::default()\\n    .load(load_args, &device)\\n    .expect(\"Should decode state successfully\");\\n\\nlet model = Net::<Backend>::init(&device).load_record(record);\n```\n\n----------------------------------------\n\nTITLE: Implementing Matrix Multiplication Kernel in WGSL\nDESCRIPTION: This WGSL code defines a compute shader for matrix multiplication with add and ReLU operations. It handles batched operations and includes support for broadcasting.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_2\n\nLANGUAGE: wgsl\nCODE:\n```\n@group(0)\n@binding(0)\nvar<storage, read_write> lhs: array<{{ elem }}>;\n\n@group(0)\n@binding(1)\nvar<storage, read_write> rhs: array<{{ elem }}>;\n\n@group(0)\n@binding(2)\nvar<storage, read_write> bias: array<{{ elem }}>;\n\n@group(0)\n@binding(3)\nvar<storage, read_write> output: array<{{ elem }}>;\n\n@group(0)\n@binding(4)\nvar<storage, read_write> info: array<u32>;\n\nconst BLOCK_SIZE = {{ workgroup_size_x }}u;\n\n@compute\n@workgroup_size({{ workgroup_size_x }}, {{ workgroup_size_y }}, 1)\nfn main(\n    @builtin(global_invocation_id) global_id: vec3<u32>,\n    @builtin(local_invocation_index) local_idx: u32,\n    @builtin(workgroup_id) workgroup_id: vec3<u32>,\n) {\n    // Indices\n    let row = workgroup_id.x * BLOCK_SIZE + (local_idx / BLOCK_SIZE);\n    let col = workgroup_id.y * BLOCK_SIZE + (local_idx % BLOCK_SIZE);\n    let batch = global_id.z;\n\n    // Basic information\n    let dim = info[0];\n    let n_rows = info[6u * dim - 1u];\n    let n_cols = info[6u * dim];\n    let K = info[5u * dim - 1u];\n\n    // Returns if outside the output dimension\n    if row >= n_rows || col >= n_cols {\n        return;\n    }\n\n    // Calculate the corresponding offsets with support for broadcasting.\n    let offset_output = batch * n_rows * n_cols;\n    var offset_lhs: u32 = 0u;\n    var offset_rhs: u32 = 0u;\n\n    let batch_dims = dim - 2u;\n    for (var b: u32 = 1u; b <= batch_dims; b++) {\n        let stride_lhs = info[b];\n        let stride_rhs = info[b + dim];\n        let stride_output = info[b + 2u * dim];\n        let shape_lhs = info[b + 3u * dim];\n        let shape_rhs = info[b + 4u * dim];\n\n        offset_lhs += offset_output / stride_output % shape_lhs * stride_lhs;\n        offset_rhs += offset_output / stride_output % shape_rhs * stride_rhs;\n    }\n\n    // Basic matmul implementation\n    var sum = 0.0;\n    for (var k: u32 = 0u; k < K; k++) {\n        let lhs_index = row * K + k;\n        let rhs_index = k * n_cols + col;\n\n        sum += lhs[offset_lhs + lhs_index] * rhs[offset_rhs + rhs_index];\n    }\n\n    let output_index = row * n_cols + col;\n    let index = offset_output + output_index;\n\n    // Add and ReLU\n    output[index] = max(sum + bias[index], 0.0);\n}\n```\n\n----------------------------------------\n\nTITLE: Composing Multiple Burn Backends with Router in Rust\nDESCRIPTION: This snippet illustrates using the Router decorator to combine multiple backends (here, Wgpu and NdArray) for heterogeneous device operation, allowing flexible dispatch of operations between GPU and CPU. Dependencies include 'burn', and its backend, tensor, wgpu, ndarray, and router modules. Two devices are manually instantiated, and random tensors are allocated explicitly on designated devices. Inputs are device descriptors; outputs are tensors allocated on respective devices. This is especially useful for workflows requiring both GPU and CPU computations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor::{Distribution, Tensor};\nuse burn::backend::{\n    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,\n};\n\nfn main() {\n    type Backend = Router<(Wgpu, NdArray)>;\n\n    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));\n    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);\n\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_0);\n    let tensor_cpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_1);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Controlling Tensor Display Precision in Rust\nDESCRIPTION: This example shows how to control the number of decimal places displayed when printing a tensor. It uses Rust's formatting syntax to limit the output to two decimal places.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nprintln!(\"{:.2}\", tensor);\n```\n\n----------------------------------------\n\nTITLE: Defining a PositionWiseFeedForward Neural Network Module in Rust\nDESCRIPTION: This snippet demonstrates how to declare a neural network module with parameters and implement its forward pass using the Burn framework. It showcases the use of the Module trait, Backend trait, and various neural network components.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::nn;\nuse burn::module::Module;\nuse burn::tensor::backend::Backend;\n\n#[derive(Module, Debug)]\npub struct PositionWiseFeedForward<B: Backend> {\n    linear_inner: nn::Linear<B>,\n    linear_outer: nn::Linear<B>,\n    dropout: nn::Dropout,\n    gelu: nn::Gelu,\n}\n\nimpl<B: Backend> PositionWiseFeedForward<B> {\n    pub fn forward<const D: usize>(&self, input: Tensor<B, D>) -> Tensor<B, D> {\n        let x = self.linear_inner.forward(input);\n        let x = self.gelu.forward(x);\n        let x = self.dropout.forward(x);\n\n        self.linear_outer.forward(x)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Backend Trait Extension in Rust\nDESCRIPTION: This Rust code defines a new trait called `Backend` that extends the existing Burn backend trait, introducing a custom function `my_new_function`. The default implementation allows all existing backends to support this operation out of the box, though specialization can be added for improved performance. Required prerequisites include an understanding of Rust traits and the Burn framework's backend architecture. The input is a 2-dimensional tensor, with the output being another tensor of the same type and shape.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\npub trait Backend: burn::tensor::backend::Backend {\n    fn my_new_function(tensor: B::TensorPrimitive<2>) -> B::TensorPrimitive<2> {\n        // You can define a basic implementation reusing the Burn Backend API.\n        // This can be useful since all backends will now automatically support\n        // your model. But performance can be improved for this new\n        // operation by implementing this block in specific backends.\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ONNX to Burn Operator Conversion\nDESCRIPTION: The process requires changes in three main areas: adding the operator to the appropriate node type in burn/node, implementing the conversion in onnx/to_burn.rs, and specifying dimension inference rules in onnx-ir/rank_inference.rs. This example focuses on implementing the 'powf' binary operator.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/guides/adding-a-new-operation-to-burn.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n// 1. Add to BinaryNode in burn/node/binary.rs\nPow(\"pow\"),\n\n// 2. Add to match statement in onnx/to_burn.rs\nNodeType::Pow => pow_conversion(node)?,\n\n// 3. Add dimension inference in onnx-ir/rank_inference.rs\nNodeType::Pow => binary_broadcast_rank_inference(node, input_ranks),\n```\n\n----------------------------------------\n\nTITLE: Implementing Autodiff Backend with WGPU in Rust\nDESCRIPTION: Demonstrates how to use the Autodiff backend decorator with WGPU to enable automatic differentiation capabilities. Shows tensor operations and gradient computation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::backend::{Autodiff, Wgpu};\nuse burn::tensor::{Distribution, Tensor};\n\nfn main() {\n    type Backend = Autodiff<Wgpu>;\n\n    let x: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default);\n    let y: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default).require_grad();\n\n    let tmp = x.clone() + y.clone();\n    let tmp = tmp.matmul(x);\n    let tmp = tmp.exp();\n\n    let grads = tmp.backward();\n    let y_grad = y.grad(&grads).unwrap();\n    println!(\"{y_grad}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Detection Dataset from a COCO Dataset in Burn (Rust)\nDESCRIPTION: This code demonstrates loading an object detection dataset in Burn using COCO-format annotations and associated images. The function accepts paths to a COCO JSON annotation file and the directory of image files. Output is a dataset ready for bounding box object detection tasks, supporting Burn's vision/data APIs. Requires access to both the annotation JSON and extracted image root. No other dependencies are specified.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\n// Create an object detection dataset from a COCO dataset. Currently only\n// the import of object detection data (bounding boxes) is supported.\n//\n// COCO offers separate annotation and image archives for training and\n// validation, paths to the unpacked files need to be passed as parameters:\n\nlet dataset = ImageFolderDataset::new_coco_detection(\n    \"/path/to/coco/instances_train2017.json\",\n    \"/path/to/coco/images/train2017\"\n)\n.unwrap();\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom and Reference WGPU Kernel Operations in Rust\nDESCRIPTION: Implements two functions: a custom implementation using the newly defined backend trait, and a reference implementation using basic tensor operations. These functions provide a consistent API for the fused matmul-add-ReLU operation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n/// We define our custom implementation using the added function on our custom backend.\npub fn matmul_add_relu_custom<B: Backend>(\n    lhs: Tensor<B, 3>,\n    rhs: Tensor<B, 3>,\n    bias: Tensor<B, 3>,\n) -> Tensor<B, 3> {\n    let output = B::fused_matmul_add_relu(\n        lhs.into_primitive().tensor(),\n        rhs.into_primitive().tensor(),\n        bias.into_primitive().tensor(),\n    );\n\n    Tensor::from_primitive(TensorPrimitive::Float(output))\n}\n\n/// We define a reference implementation using basic tensor operations.\npub fn matmul_add_relu_reference<B: Backend>(\n    lhs: Tensor<B, 3>,\n    rhs: Tensor<B, 3>,\n    bias: Tensor<B, 3>,\n) -> Tensor<B, 3> {\n    let x = lhs.matmul(rhs) + bias;\n\n    activation::relu(x)\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Testing Burn Libraries Across Multiple Targets â€“ Shell\nDESCRIPTION: This snippet provides shell commands to install architecture-specific Rust targets, build the burn-related packages for regular and cross-compiled environments, and run tests. It requires rustup and cargo to be installed, as well as access to the appropriate build environments. Key parameters include the specification of build targets (such as thumbv7m-none-eabi or wasm32-unknown-unknown) using rustup and cargo, and the use of RUSTFLAGS for single-core safety. The inputs are Rust source files for the relevant packages, and the outputs are compiled binaries and test results; users must ensure that all necessary targets are supported on their host system.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-no-std-tests/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# install the new targets if not installed previously\\nrustup target add thumbv6m-none-eabi\\nrustup target add thumbv7m-none-eabi\\nrustup target add wasm32-unknown-unknown\\n\\n# build for various targets \\ncargo build # regular build\\ncargo build --target thumbv7m-none-eabi\\ncargo build --target wasm32-unknown-unknown\\nRUSTFLAGS=\"--cfg portable_atomic_unsafe_assume_single_core\" cargo build --target thumbv6m-none-eabi\\n\\n# test\\ncargo test\\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Rust Compiler Type and Method Missing Errors for Operator Implementation (Shell)\nDESCRIPTION: This shell code snippet attests to the compiler errors produced when an operator (op) has not been correctly implemented for the required Burn tensor type in Rust. It includes output for mismatched types, missing methods, and expected/actual types from Rust's compiler, such as E0308 for type mismatch and E0599 for an undefined method. Key dependencies are the Rust compiler (rustc), the Burn tensor API, and that the relevant traits and functions are implemented for all applicable tensor types. Inputs are the source file locations and the failed Rust code, and outputs are the compiler's diagnostic error messages. The snippet aids developers in locating and interpreting compilation errors attributable to incomplete operator trait implementations in Burn.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/frequently-encountered-issues/issues-while-adding-ops.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nerror[E0308]: mismatched types --> {burn_dir}/target/debug/build/onnx-tests-fed12aaf3671687f/out/model/pow.rs:48:45 | 48 | let pow1_out1 = input1.clone().powf(input1); | ---- ^^^^^^ expected `f32`, found `Tensor<B, 4>` | | | arguments to this method are incorrect | = note: expected type `f32` found struct `Tensor<B, 4>`\n\nnote: method defined here --> {burn_dir}/burn-tensor/src/tensor/api/float.rs:65:12 | 65 | pub fn powf(self, value: f32) -> Self { | ^^^^\n\nerror[E0599]: no method named `powf_scalar` found for struct `Tensor` in the current scope --> {burn_dir}/target/debug/build/onnx-tests-fed12aaf3671687f/out/model/pow.rs:50:35 | 50 | let pow2_out1 = pow1_out1.powf_scalar(cast1_out1); | ^^^^^^^^^^^ method not found in `Tensor<B, 4>`\n\nerror[E0599]: no method named `powi` found for struct `Tensor` in the current scope --> {burn_dir}/target/debug/build/onnx-tests-fed12aaf3671687f/out/model/pow_int.rs:49:40 | 49 | let pow1_out1 = input1.clone().powi(input1); | ^^^^ method not found in `Tensor<B, 4, Int>` Some errors have detailed explanations: E0308, E0599.\nFor more information about an error, try `rustc --explain E0308`. error: could not compile `onnx-tests` (test \"onnx_tests\") due to 3 previous errors\n```\n\n----------------------------------------\n\nTITLE: Displaying Floating Point Assertion Failures During Operator Tests (Shell)\nDESCRIPTION: This shell snippet shows Rust test output for a comparison assertion failure due to minute floating point differences in test results when running operator validation in Burn. It demonstrates the panic message produced by a failed assert_eq! in unit tests and includes the expected and actual tensor values. The error occurs during execution of Rust tests and is useful for distinguishing between precise equality and approximate checks (suggesting use of assert_approx_eq). No explicit dependencies are required, but this applies to projects using Rust's cargo test infrastructure. The output inputs are error results from test execution.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/frequently-encountered-issues/issues-while-adding-ops.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n---- fusion::base::tests::maxmin::tests::test_mean_dim_2d stdout ---- thread 'fusion::base::tests::maxmin::tests::test_mean_dim_2d' panicked at burn-wgpu/src/fusion/base.rs:185:5: assertion `left == right` failed left: Data { value: [1.0, 4.0], shape: Shape { dims: [2, 1] } } right: Data { value: [0.99999994, 3.9999998], shape: Shape { dims: [2, 1] } } ----\n\ntests::maxmin::tests::test_mean_dim_2d stdout ---- thread 'tests::maxmin::tests::test_mean_dim_2d' panicked at burn-wgpu/src/lib.rs:49:5: assertion `left == right` failed left: Data { value: [1.0, 4.0], shape: Shape { dims: [2, 1] } } right: Data { value: [0.99999994, 3.9999998], shape: Shape { dims: [2, 1] } }\n```\n\n----------------------------------------\n\nTITLE: Defining a Learner Struct Generic Over Backend, Model, and Optimizer Using PhantomData in Rust\nDESCRIPTION: This code defines the Learner3 struct with generics B (backend), M (model), and O (optimizer), and introduces PhantomData<B> as the _b field. PhantomData is used to satisfy the Rust compiler's requirement that every generic parameter must be used in the struct. This lets you encode type relationships without storing actual data of type B. Dependencies include std::marker::PhantomData and relevant model/optimizer/backend traits. This solution is suitable when backend is only required for typing, not for fields.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nstruct Learner3<B, M, O> {\n    model: M,\n    optim: O,\n    _b: PhantomData<B>,\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Backend Traits for WGPU Kernel in Rust\nDESCRIPTION: Defines custom Backend and AutodiffBackend traits that extend Burn's backend traits. These traits introduce a new fused operation for matrix multiplication, addition, and ReLU activation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n/// We create our own Backend trait that extends the Burn backend trait.\npub trait Backend: burn::tensor::backend::Backend {\n    fn fused_matmul_add_relu(\n        lhs: FloatTensor<Self>,\n        rhs: FloatTensor<Self>,\n        bias: FloatTensor<Self>,\n    ) -> FloatTensor<Self>;\n}\n\n/// We create our own AutodiffBackend trait that extends the Burn autodiff backend trait.\npub trait AutodiffBackend: Backend + burn::tensor::backend::AutodiffBackend {}\n```\n\n----------------------------------------\n\nTITLE: Defining and Implementing Kernel Source in Rust\nDESCRIPTION: This Rust code defines the kernel source and implements the KernelSource trait for the FusedMatmulAddRelu struct. It uses templating to populate the WGSL code with appropriate variables.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n// Source the kernel written in WGSL.\nkernel_wgsl!(FusedMatmulAddReluRaw, \"./kernel.wgsl\");\n\n// Define our kernel type with cube information.\n#[derive(new, Debug)]\nstruct FusedMatmulAddRelu<E: FloatElement> {\n    cube_dim: CubeDim,\n    _elem: PhantomData<E>,\n}\n\n// Implement the dynamic kernel trait for our kernel type.\nimpl<E: FloatElement> KernelSource for FusedMatmulAddRelu<E> {\n    fn source(&self) -> SourceTemplate {\n        // Extend our raw kernel with cube size information using the\n        // `SourceTemplate` trait.\n        FusedMatmulAddReluRaw::new()\n            .source()\n            .register(\"workgroup_size_x\", self.cube_dim.x.to_string())\n            .register(\"workgroup_size_y\", self.cube_dim.y.to_string())\n            .register(\"elem\", E::type_name())\n            .register(\"int\", \"i32\")\n    }\n\n    fn id(&self) -> cubecl::KernelId {\n        cubecl::KernelId::new::<Self>().info(self.cube_dim)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Learner Struct Generic Over Model and Optimizer in Rust\nDESCRIPTION: This snippet defines a Rust struct named Learner that is generic over types M (the model) and O (the optimizer), without directly constraining the generics at the struct definition stage. This enables deferring trait constraints to later implementation blocks, improving flexibility. Dependencies include trait implementations for the types used as M and O. This pattern is common and convenient, allowing the same struct to support a variety of model-optimizer pairings.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nstruct Learner<M, O> {\n    model: M,\n    optim: O,\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Rust code from ONNX model in build script\nDESCRIPTION: This code snippet demonstrates how to use the ModelGen struct from burn-import to generate Rust code from an ONNX model file. It's typically used in a build.rs file to automate the code generation process during project build.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/onnx-model.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse burn_import::onnx::ModelGen;\n\nfn main() {\n    ModelGen::new()\n        .input(\"src/model/my_model.onnx\")\n        .out_dir(\"model/\")\n        .run_from_script();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Backend Trait for Autodiff in Rust\nDESCRIPTION: This snippet shows how to implement a custom backend trait for Autodiff, including forward and backward passes for a fused matmul_add_relu operation. It demonstrates state management, gradient computation, and handling of tracked vs untracked operations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: Backend, C: CheckpointStrategy> Backend for Autodiff<B, C> {\n    fn fused_matmul_add_relu(\n        lhs: FloatTensor<Self>,\n        rhs: FloatTensor<Self>,\n        bias: FloatTensor<Self>,\n    ) -> FloatTensor<Self> {\n        #[derive(Debug)]\n        struct FusedMatmulAddReluBackward;\n\n        impl<B: Backend> Backward<B, 3> for FusedMatmulAddReluBackward {\n            type State = (NodeID, NodeID, FloatTensor<B>, Shape);\n\n            fn backward(\n                self,\n                ops: Ops<Self::State, 3>,\n                grads: &mut Gradients,\n                checkpointer: &mut Checkpointer,\n            ) {\n                let [node_lhs, node_rhs, node_bias] = ops.parents;\n                let grad = grads.consume::<B>(&ops.node);\n\n                let (lhs_state, rhs_state, output, shape_bias) = ops.state;\n                let lhs: FloatTensor<B> = checkpointer.retrieve_node_output(lhs_state);\n                let rhs: FloatTensor<B> = checkpointer.retrieve_node_output(rhs_state);\n\n                let shape_lhs = lhs.shape();\n                let shape_rhs = rhs.shape();\n\n                let grad_output = B::relu_backward(output, grad);\n\n                let grad_lhs = broadcast_shape::<B>(\n                    B::float_matmul(grad_output.clone(), B::float_transpose(rhs)),\n                    &shape_lhs,\n                );\n                let grad_rhs = broadcast_shape::<B>(\n                    B::float_matmul(B::float_transpose(lhs), grad_output.clone()),\n                    &shape_rhs,\n                );\n                let grad_bias = broadcast_shape::<B>(grad_output, &shape_bias);\n\n                if let Some(node) = node_bias {\n                    grads.register::<B>(node.id, grad_bias);\n                }\n                if let Some(node) = node_lhs {\n                    grads.register::<B>(node.id, grad_lhs);\n                }\n                if let Some(node) = node_rhs {\n                    grads.register::<B>(node.id, grad_rhs);\n                }\n            }\n        }\n\n        match FusedMatmulAddReluBackward\n            .prepare::<C>([lhs.node.clone(), rhs.node.clone(), bias.node.clone()])\n            .compute_bound()\n            .stateful()\n        {\n            OpsKind::Tracked(mut prep) => {\n                let lhs_state = prep.checkpoint(&lhs);\n                let rhs_state = prep.checkpoint(&rhs);\n                let bias_shape = bias.primitive.shape();\n\n                let output = B::fused_matmul_add_relu(\n                    lhs.primitive.clone(),\n                    rhs.primitive.clone(),\n                    bias.primitive,\n                );\n\n                let state = (lhs_state, rhs_state, output.clone(), bias_shape);\n\n                prep.finish(state, output)\n            }\n            OpsKind::UnTracked(prep) => {\n                let output = B::fused_matmul_add_relu(lhs.primitive, rhs.primitive, bias.primitive);\n                prep.finish(output)\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for PyTorch to ONNX Export (Bash)\nDESCRIPTION: This command installs the required Python packages (torch, torchvision, onnx) to enable PyTorch MNIST model training and export to ONNX format. It should be run in a Python environment. No additional input is required; packages will be available globally or in the active environment. No output beyond package installation success/failure.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install torch torchvision onnx\n```\n\n----------------------------------------\n\nTITLE: Printing Source Model Keys and Tensor Information Using PyTorchFileRecorder - Burn, Rust\nDESCRIPTION: Illustrates how to print key mappings and tensor shapes when loading weights to assist with debugging. Uses .with_debug_print in LoadArgs to output the original and remapped keys, tensor shapes, and dtypes. Prerequisite is Burn and burn_import with a compatible .pt file. Outputs debug information to stdout, helping align source and target model structures.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_7\n\nLANGUAGE: Rust\nCODE:\n```\nlet device = Default::default();\\nlet load_args = LoadArgs::new(\"tests/key_remap/key_remap.pt\".into())\\n    // Remove \"conv\" prefix, e.g. \"conv.conv1\" -> \"conv1\"\\n    .with_key_remap(\"conv\\\\.(.*)\", \"$1\")\\n    .with_debug_print(); // Print the keys and remapped keys\\n\\nlet record = PyTorchFileRecorder::<FullPrecisionSettings>::default()\\n    .load(load_args, &device)\\n    .expect(\"Should decode state successfully\");\\n\\nlet model = Net::<Backend>::init(&device).load_record(record);\n```\n\n----------------------------------------\n\nTITLE: Defining Input Tensor and Running Model Inference - Rust\nDESCRIPTION: This snippet demonstrates how to define a two-dimensional input tensor from floats, assigning it to the appropriate device, and then run the forward inference pass using Burn's API. The output tensor represents the model's result. Requires a properly initialized model object and device as described previously. The tensor shape, input values, and output structure depend on the actual model's interface.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/no-std.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n// Define the tensor\nlet input = Tensor::<Backend, 2>::from_floats([[input]], &device);\n\n// Run the model on the input\nlet output = model.forward(input);\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Backend Trait for CubeBackend to Launch Fused Kernel (Rust)\nDESCRIPTION: This code implements the previously defined custom Backend trait for CubeBackend, specializing how the fused matmul-add-ReLU operation is executed. It manages cube dimension setup, asserts tensor device compatibility, ensures tensors are contiguous, computes batch and output shapes, allocates output buffers, and launches the CubeCL kernel with the correct shape and memory arguments. The code also enforces output tensor construction is safe and device correct within Rust, though kernel execution can still cause unsafe memory mutations. Expected inputs are float tensors on the same device and proper dimensions; output is a tensor suitable for further computation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-cubecl-kernel.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n/// Implement our custom backend trait for the generic `CubeBackend`.\nimpl<R: CubeRuntime, F: FloatElement, I: IntElement> Backend for CubeBackend<R, F, I> {\n    fn fused_matmul_add_relu(\n        lhs: FloatTensor<Self>,\n        rhs: FloatTensor<Self>,\n        bias: FloatTensor<Self>,\n    ) -> FloatTensor<Self> {\n        // Define cube dim, hardcoded for simplicity.\n        let cube_dim = CubeDim { x: 16, y: 16, z: 1 };\n\n        lhs.assert_is_on_same_device(&rhs);\n        lhs.assert_is_on_same_device(&bias);\n\n        // For simplicity, make sure each tensor is continuous.\n        let lhs = into_contiguous(lhs);\n        let rhs = into_contiguous(rhs);\n        let bias = into_contiguous(bias);\n\n        // Get the matmul relevant shapes.\n        let ndims = lhs.shape.num_dims();\n        let num_rows = lhs.shape.dims[ndims - 2];\n        let num_cols = rhs.shape.dims[ndims - 1];\n\n        // Compute shape of output, while tracking number of batches.\n        let mut num_batches = 1;\n        let mut shape_out = vec![0; ndims];\n        for i in shape_out.clone().into_iter().take(ndims - 2) {\n            shape_out[i] = usize::max(lhs.shape.dims[i], rhs.shape.dims[i]);\n            num_batches *= shape_out[i];\n        }\n        shape_out[ndims - 2] = num_rows;\n        shape_out[ndims - 1] = num_cols;\n        let shape_out = Shape::from(shape_out);\n\n        // Create a buffer for the output tensor.\n        let buffer = lhs\n            .client\n            .empty(shape_out.num_elements() * core::mem::size_of::<F>());\n\n        // Create the output tensor primitive.\n        // Create the output tensor primitive.\n        let output = CubeTensor::new_contiguous(\n            lhs.client.clone(),\n            lhs.device.clone(),\n            shape_out,\n            buffer,\n            F::dtype(),\n        );\n\n        // Declare the wgsl workgroup with the number of cubes in x, y and z.\n        let cubes_needed_in_x = f32::ceil(num_rows as f32 / cube_dim.x as f32) as u32;\n        let cubes_needed_in_y = f32::ceil(num_cols as f32 / cube_dim.y as f32) as u32;\n        let cube_count =\n            CubeCount::Static(cubes_needed_in_x, cubes_needed_in_y, num_batches as u32);\n\n        // Execute lazily the kernel with the launch information and the given buffers. For\n        // simplicity, no vectorization is performed\n        fused_matmul_add_relu_kernel::launch::<F, R>(\n            &lhs.client,\n            cube_count,\n            cube_dim,\n            lhs.as_tensor_arg::<F>(1),\n            rhs.as_tensor_arg::<F>(1),\n            bias.as_tensor_arg::<F>(1),\n            output.as_tensor_arg::<F>(1),\n        );\n\n        // Return the output tensor.\n        output\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Recursion Limit for Rust WGPU Projects\nDESCRIPTION: This snippet sets the recursion limit for the Rust compiler to avoid errors related to deeply nested types when compiling projects that use the Burn WGPU backend. By increasing the limit to 256 using an inner attribute, it ensures compatibility with complex types and trait bounds arising in autodiff and WGPU dependency chains. This code must appear near the beginning of your main.rs or lib.rs file. There are no additional inputs or outputs, but omitting it may cause compilation to fail on certain project configurations. No runtime effect.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-wgpu/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n#![recursion_limit = \"256\"]\n\n```\n\n----------------------------------------\n\nTITLE: Customizing Module Display in Burn\nDESCRIPTION: Shows how to implement the ModuleDisplay trait to customize how modules are displayed when printed. This allows for better debugging and visualization of module structure and parameters.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/module.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Module, Debug)]\n#[module(custom_display)]\npub struct PositionWiseFeedForward<B: Backend> {\n    linear_inner: Linear<B>,\n    linear_outer: Linear<B>,\n    dropout: Dropout,\n    gelu: Gelu,\n}\n\nimpl<B: Backend> ModuleDisplay for PositionWiseFeedForward<B> {\n    /// Custom settings for the display of the module.\n    /// If `None` is returned, the default settings will be used.\n    fn custom_settings(&self) -> Option<burn::module::DisplaySettings> {\n        DisplaySettings::new()\n            // Will show all attributes (default is false)\n            .with_show_all_attributes(false)\n            // Will show each attribute on a new line (default is true)\n            .with_new_line_after_attribute(true)\n            // Will show the number of parameters (default is true)\n            .with_show_num_parameters(true)\n            // Will indent by 2 spaces (default is 2)\n            .with_indentation_size(2)\n            // Will show the parameter ID (default is false)\n            .with_show_param_id(false)\n            // Convenience method to wrap settings in Some()\n            .optional()\n    }\n\n    /// Custom content to be displayed.\n    /// If `None` is returned, the default content will be used\n    /// (all attributes of the module)\n    fn custom_content(&self, content: Content) -> Option<Content> {\n        content\n            .add(\"linear_inner\", &self.linear_inner)\n            .add(\"linear_outer\", &self.linear_outer)\n            .add(\"anything\", \"anything_else\")\n            .optional()\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Parameter Clamping with Gradient Tracking in Burn\nDESCRIPTION: Advanced implementation of the ModuleMapper trait for clamping parameters while preserving autodiff gradient tracking. This ensures parameters remain differentiable after transformation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/module.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: AutodiffBackend> ModuleMapper<B> for Clamp {\n    fn map_float<const D: usize>(\n        &mut self,\n        _id: burn::module::ParamId,\n        tensor: burn::prelude::Tensor<B, D>,\n    ) -> burn::prelude::Tensor<B, D> {\n        let is_require_grad = tensor.is_require_grad();\n\n        let mut tensor = Tensor::from_inner(tensor.inner().clamp(self.min, self.max));\n\n        if is_require_grad {\n            tensor = tensor.require_grad();\n        }\n\n        tensor\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing AutodiffBackend Trait for CubeBackend\nDESCRIPTION: A minimal implementation of the AutodiffBackend trait for the CubeBackend, allowing it to work with the autodiff system. This enables differentiation capabilities for the custom backend implementation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-cubecl-kernel.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nimpl<R: CubeRuntime, F: FloatElement, I: IntElement> AutodiffBackend\n    for Autodiff<CubeBackend<R, F, I>>\n{\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Model with MessagePack Recorder in Rust\nDESCRIPTION: This snippet loads a model previously saved in MessagePack format using the NamedMpkFileRecorder with full precision settings. Using model.load_file, the weights are read from disk and assigned back to the model, with device configuration for computation. It requires the same recorder type as used for saving and robust handling of the model path and device. The snippet demonstrates typical deserialization usage in Burn and validates the operation with expect, ensuring correct recovery of the model state.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/saving-and-loading.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n// Load model in full precision from MessagePack file\nlet recorder = NamedMpkFileRecorder::<FullPrecisionSettings>::new();\nmodel = model\n    .load_file(model_path, &recorder, device)\n    .expect(\"Should be able to load the model weights from the provided file\");\n```\n\n----------------------------------------\n\nTITLE: Specifying Top-Level Key for PyTorch State Dict Loading with Burn (Rust)\nDESCRIPTION: This Rust snippet demonstrates how to configure the loading of PyTorch model weights where the state_dict is nested under a custom key inside a checkpoint file. It involves the use of LoadArgs to specify the path and the top-level key, and shows loading the record using PyTorchFileRecorder with FullPrecisionSettings. Dependencies include the Burn library, PyTorchFileRecorder, LoadArgs, and a compatible device. Inputs are the checkpoint file and the key; the output is a decoded record ready for model assignment. Error handling is performed via expect.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet device = Default::default();\nlet load_args = LoadArgs::new(\"tiny.en.pt\".into())\n    .with_top_level_key(\"my_state_dict\");\n\nlet record = PyTorchFileRecorder::<FullPrecisionSettings>::default()\n    .load(load_args, &device)\n    .expect(\"Should decode state successfully\")\n```\n\n----------------------------------------\n\nTITLE: Matching and Registering ONNX Nodes in graph conversion - Rust\nDESCRIPTION: This snippet illustrates how the Burn framework's ONNX converter matches incoming ONNX nodes to the appropriate conversion function using Rust's match statement. Each NodeType case registers the result of a conversion function (e.g., add_conversion, squeeze_conversion) that produces a Burn node. This modular approach allows extension for new ONNX ops by adding an entry to the match block. Dependencies: src/onnx/to_burn.rs, the BurnGraph struct, the NodeType enum, and conversion functions. Input is each ONNX node, and the output is a populated Burn graph with registered operation-specific nodes.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/guides/onnx-to-burn-conversion-tool.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nimpl OnnxGraph {\n    pub fn into_burn<PS: PrecisionSettings + 'static>(self) -> BurnGraph<PS> {\n        let mut graph = BurnGraph::<PS>::default();\n        let mut unsupported_ops = vec![];\n\n        for node in self.nodes {\n            match node.node_type {\n                NodeType::Add => graph.register(Self::add_conversion(node)),\n                // Other operations...\n                NodeType::Squeeze => graph.register(Self::squeeze_conversion(node)),\n                // Add new operations here\n            }\n        }\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Building the Burn MNIST Model for WebAssembly - Shell Script\nDESCRIPTION: This shell command compiles the Rust-based Burn MNIST inference engine for the web, using a selectable backend ('ndarray' or 'wgpu'). 'ndarray' is for general use while 'wgpu' leverages browsers with WebGPU support. The script must be executed within the project directory and requires Rust, wasm32 target, and dependencies defined in Cargo.toml. The input '{backend}' is a placeholder for the desired backend, with outputs suitable for in-browser inference.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/mnist-inference-web/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n```shell\\n./build-for-web.sh {backend}\\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing AutodiffBackend Trait for WGPU Backend in Rust\nDESCRIPTION: This snippet demonstrates how to implement the AutodiffBackend trait for the WGPU backend. It's a simple implementation that allows the WGPU backend to be used with autodiff functionality.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-wgpu-kernel.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nimpl<G: GraphicsApi, F: FloatElement, I: IntElement> AutodiffBackend for Autodiff<WgpuBackend<G, F, I>>\n{\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Burn Model Struct for Training - Rust\nDESCRIPTION: Depicts how to instantiate the previously defined Model struct, provided all dependencies and generics are resolved. Includes full type and struct definitions for context. Used as a foundation for future training loops or experiments, dependent on user implementation of the model logic. Most dependencies and generics, including Backend and Burn types, must be in scope, and parameter values must be provided as required.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n# use burn::{\\n#     nn::{\\n#         conv::{Conv2d, Conv2dConfig},\\n#         pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},\\n#         Dropout, DropoutConfig, Linear, LinearConfig, Relu,\\n#     },\\n#     prelude::*,\\n# };\\n#\\n# #[derive(Module, Debug)]\\n# pub struct Model<B: Backend> {\\n#     conv1: Conv2d<B>,\\n#     conv2: Conv2d<B>,\\n#     pool: AdaptiveAvgPool2d,\\n#     dropout: Dropout,\\n#     linear1: Linear<B>,\\n#     linear2: Linear<B>,\\n#     activation: Relu,\\n# }\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Model and Burn Graph in Rust\nDESCRIPTION: Command to run the ONNX to Burn conversion tool, generating IR and Burn graph from an ONNX model. This step is crucial when adding support for new operators.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/guides/onnx-to-burn-conversion-tool.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo r -- ./onnx-tests/tests/<op>/<op>.onnx ./out\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Backend for Multi-Device Computation\nDESCRIPTION: Demonstrates using the Router backend decorator to compose multiple backends (WGPU and NdArray) for mixed CPU/GPU computation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor::{Distribution, Tensor};\nuse burn::backend::{\n    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,\n};\n\nfn main() {\n    type Backend = Router<(Wgpu, NdArray)>;\n\n    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));\n    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);\n\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_0);\n    let tensor_cpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_1);\n}\n```\n\n----------------------------------------\n\nTITLE: Training a Model with Burn via Cargo - Shell\nDESCRIPTION: This shell snippet demonstrates how to initiate model training using Burn by running the 'train' binary in release mode via Cargo. Requires Rust and Cargo to be installed, with the 'burn' project containing a 'train' binary. Expects any necessary model configuration to be predefined in the project. Input: No direct input, uses project configuration; Output: Trained model artifacts are typically saved to the project directory. Limitation: Assumes correct Cargo and Rust setup, and the presence of a valid 'train' binary.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/guide/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --bin train --release\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Print Options in Burn\nDESCRIPTION: Sets custom global print options for tensor display, allowing control over precision, threshold for summarization, and edge items to show when summarizing large tensors.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor::{set_print_options, PrintOptions};\n\nlet print_options = PrintOptions {\n    precision: Some(2),\n    ..Default::default()\n};\n\nset_print_options(print_options);\n```\n\n----------------------------------------\n\nTITLE: Running Burn Examples with Cargo\nDESCRIPTION: Command line instruction for executing Burn example projects using Cargo from the root of the Burn repository.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/examples.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo run --example <example name>\n```\n\n----------------------------------------\n\nTITLE: Augmenting Burn Backends with Kernel Fusion in Rust\nDESCRIPTION: This snippet shows how to decorate a backend with kernel fusion capability using the Fusion decorator (stacked here with Autodiff and Wgpu). Kernel fusion is supported on WGPU and CUDA backends. Dependencies are the 'burn' crate with backend and tensor modules. The example creates gradients through fused kernel operations for increased computational efficiency. The same code structure as the Autodiff example is used, showing composability of backend decorators.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::backend::{Autodiff, Fusion, Wgpu};\nuse burn::tensor::{Distribution, Tensor};\n\nfn main() {\n    type Backend = Autodiff<Fusion<Wgpu>>;\n\n    let x: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default);\n    let y: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default).require_grad();\n\n    let tmp = x.clone() + y.clone();\n    let tmp = tmp.matmul(x);\n    let tmp = tmp.exp();\n\n    let grads = tmp.backward();\n    let y_grad = y.grad(&grads).unwrap();\n    println!(\"{y_grad}\");\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Backend Trait for Autodiff in Burn\nDESCRIPTION: Implementation of the fused_matmul_add_relu function for the Autodiff backend wrapper, enabling gradient computation for the custom operation. It creates a backward pass implementation that computes gradients for each input tensor (lhs, rhs, and bias) using basic operations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/custom-cubecl-kernel.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n// Implement our custom backend trait for any backend that also implements our custom backend trait.\nimpl<B: Backend, C: CheckpointStrategy> Backend for Autodiff<B, C> {\n    fn fused_matmul_add_relu(\n        lhs: FloatTensor<Self>,\n        rhs: FloatTensor<Self>,\n        bias: FloatTensor<Self>,\n    ) -> FloatTensor<Self> {\n        // Create our zero-sized type that will implement the Backward trait.\n        #[derive(Debug)]\n        struct FusedMatmulAddReluBackward;\n\n        // Implement the backward trait for the given backend B, the node gradient\n        // with three other gradients to calculate (lhs, rhs, and bias).\n        impl<B: Backend> Backward<B, 3> for FusedMatmulAddReluBackward {\n            // Our state that we must build during the forward pass to compute the backward pass.\n            //\n            // Note that we could improve the performance further by only keeping the state of\n            // tensors that are tracked, improving memory management, but for simplicity, we avoid\n            // that part.\n            type State = (NodeID, NodeID, FloatTensor<B>, Shape);\n\n            fn backward(\n                self,\n                ops: Ops<Self::State, 3>,\n                grads: &mut Gradients,\n                checkpointer: &mut Checkpointer,\n            ) {\n                // Get the nodes of each variable.\n                let [node_lhs, node_rhs, node_bias] = ops.parents;\n                // Fetch the gradient for the current node.\n                let grad = grads.consume::<B>(&ops.node);\n\n                // Set our state.\n                let (lhs_state, rhs_state, output, shape_bias) = ops.state;\n                let lhs: FloatTensor<B> = checkpointer.retrieve_node_output(lhs_state);\n                let rhs: FloatTensor<B> = checkpointer.retrieve_node_output(rhs_state);\n\n                // Fetch shapes of our tensor to support broadcasting.\n                let shape_lhs = lhs.shape();\n                let shape_rhs = rhs.shape();\n\n                // Compute the gradient of the output using the already existing `relu_backward`\n                // function in the basic Burn backend trait.\n                let grad_output = B::relu_backward(output, grad);\n\n                // Compute the lhs gradient, which is the derivative of matmul with support for\n                // broadcasting.\n                let grad_lhs = broadcast_shape::<B>(\n                    B::float_matmul(grad_output.clone(), B::float_transpose(rhs)),\n                    &shape_lhs,\n                );\n                // Compute the rhs gradient, which is the derivative of matmul with support for\n                // broadcasting.\n                let grad_rhs = broadcast_shape::<B>(\n                    B::float_matmul(B::float_transpose(lhs), grad_output.clone()),\n                    &shape_rhs,\n                );\n                // The add derivative is only 1, so we just need to support broadcasting to\n                // compute the bias gradient.\n                let grad_bias = broadcast_shape::<B>(grad_output, &shape_bias);\n\n                // Register the gradient for each variable based on whether they are marked as\n                // `tracked`.\n                if let Some(node) = node_bias {\n                    grads.register::<B>(node.id, grad_bias);\n                }\n                if let Some(node) = node_lhs {\n                    grads.register::<B>(node.id, grad_lhs);\n                }\n                if let Some(node) = node_rhs {\n                    grads.register::<B>(node.id, grad_rhs);\n                }\n            }\n        }\n\n        // Prepare a stateful operation with each variable node and corresponding graph.\n        //\n        // Each node can be fetched with `ops.parents` in the same order as defined here.\n        match FusedMatmulAddReluBackward\n            .prepare::<C>([lhs.node.clone(), rhs.node.clone(), bias.node.clone()])\n            // Marks the operation as compute bound, meaning it will save its\n            // state instead of recomputing itself during checkpointing\n            .compute_bound()\n            .stateful()\n        {\n            OpsKind::Tracked(mut prep) => {\n                // When at least one node is tracked, we should register our backward step.\n\n                // The state consists of what will be needed for this operation's backward pass.\n                // Since we need the parents' outputs, we must checkpoint their ids to retrieve\n                // their node output at the beginning of the backward pass. We can also save\n                // utilitary data such as the bias shape. If we also need this operation's output,\n                // we can either save it in the state or recompute it.\n                // during the backward pass. Here we choose to save it in the state because it's a\n                // compute bound operation.\n                let lhs_state = prep.checkpoint(&lhs);\n                let rhs_state = prep.checkpoint(&rhs);\n                let bias_shape = bias.primitive.shape();\n\n                let output = B::fused_matmul_add_relu(\n                    lhs.primitive.clone(),\n                    rhs.primitive.clone(),\n                    bias.primitive,\n                );\n\n                let state = (lhs_state, rhs_state, output.clone(), bias_shape);\n\n                prep.finish(state, output)\n            }\n            OpsKind::UnTracked(prep) => {\n                // When no node is tracked, we can just compute the original operation without\n                // keeping any state.\n                let output = B::fused_matmul_add_relu(lhs.primitive, rhs.primitive, bias.primitive);\n                prep.finish(output)\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Basic Tensor Information in Rust\nDESCRIPTION: This snippet demonstrates how to display detailed information about a tensor using Rust's println! macro. It shows the tensor's data, shape, device, backend, kind, and dtype.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nlet tensor = Tensor::<Backend, 2>::full([2, 3], 0.123456789, &Default::default());\nprintln!(\"{}\", tensor);\n```\n\n----------------------------------------\n\nTITLE: Supporting and Specializing the Backward Pass in a Custom Backend Trait with Burn Autodiff in Rust\nDESCRIPTION: These Rust snippets demonstrate how to implement the custom `Backend` trait for the `burn_autodiff::Autodiff<B>` backend to support backward passes, with or without providing a new implementation for `my_new_function`. The default behavior uses the parent implementation, but specialized logic (including custom backward computations) can be provided for training-performance gains. This pattern is useful for integrating custom autodifferentiation strategies or kernels in Burn. Inputs/outputs are autodiff tensor types, and constraints are imposed by the generic `Backend` requirement.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/backend-extension/README.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nimpl<B: Backend> Backend for burn_autodiff::Autodiff<B> {\n    // No specific implementation; autodiff will work with the default\n    // implementation. Useful if you still want to train your model, but\n    // observe performance gains mostly during inference.\n}\n\nimpl<B: Backend> Backend for burn_autodiff::Autodiff<B> {\n   fn my_new_function(tensor: AutodiffTensor<E, 2>) -> AutodiffTensor<E, 2> {\n      // My own backward implementation, generic over my custom Backend trait.\n      //\n      // You can add a new method `my_new_function_backward` to your custom backend\n      // trait if you want to invoke a custom kernel during the backward pass.\n   }\n}\n\nimpl<E: TchElement> Backend for burn_autodiff::Autodiff<burn_tch::LibTorch<E>> {\n   fn my_new_function(tensor: AutodiffTensor<E, 2>) -> AutodiffTensor<E, 2> {\n      // My own backward implementation, generic over a backend implementation.\n      //\n      // This is another way to call a custom kernel for the backward pass that\n      // doesn't require the addition of a new `backward` function in the custom backend.\n      // This is useful if you don't want all backends to support training, reducing\n      // the need for extra code when you know your model will only be trained on one\n      // specific backend.\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing 1-Dimensional Tensor in Burn\nDESCRIPTION: Shows how to correctly initialize a 1-dimensional tensor with 5 elements using the from_floats method. It also demonstrates an incorrect initialization for comparison.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nlet floats = [1.0, 2.0, 3.0, 4.0, 5.0];\n\n// Get the default device\nlet device = Default::default();\n\n// correct: Tensor is 1-Dimensional with 5 elements\nlet tensor_1 = Tensor::<Backend, 1>::from_floats(floats, &device);\n\n// incorrect: let tensor_1 = Tensor::<Backend, 5>::from_floats(floats, &device);\n// this will lead to an error and is for creating a 5-D tensor\n```\n\n----------------------------------------\n\nTITLE: Running Text Classification with Torch GPU Backend\nDESCRIPTION: Commands for training and inferencing text classification models using Torch GPU backend. Supports optional FP16 operations with CUDA device compatibility.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n\n# Use the --release flag to really speed up training.\n# Use the f16 feature if your CUDA device supports FP16 (half precision) operations. May not work well on every device.\n\nexport TORCH_CUDA_VERSION=cu124  # Set the cuda version (CUDA users)\n\n# AG News\ncargo run --example ag-news-train --release --features tch-gpu   # Train on the ag news dataset\ncargo run --example ag-news-infer --release --features tch-gpu   # Run inference on the ag news dataset\n\n# DbPedia\ncargo run --example db-pedia-train --release --features tch-gpu  # Train on the db pedia dataset\ncargo run --example db-pedia-infer --release --features tch-gpu  # Run inference db pedia dataset\n```\n\n----------------------------------------\n\nTITLE: Incorrect Tensor Operation Due to Ownership in Burn\nDESCRIPTION: Illustrates an incorrect attempt at min-max normalization that fails due to Rust's ownership rules. This example demonstrates why cloning is necessary in Burn.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nlet input = Tensor::<Wgpu, 1>::from_floats([1.0, 2.0, 3.0, 4.0], &device);\nlet min = input.min();\nlet max = input.max();\nlet input = (input - min).div(max - min);\n```\n\n----------------------------------------\n\nTITLE: Basic Tensor Operations with Burn and WGPU Backend\nDESCRIPTION: This code snippet shows how to create tensors, perform element-wise addition, and print the result using Burn with the WGPU backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/getting-started.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::tensor::Tensor;\nuse burn::backend::Wgpu;\n\n// Type alias for the backend to use.\ntype Backend = Wgpu;\n\nfn main() {\n    let device = Default::default();\n    // Creation of two tensors, the first with explicit values and the second one with ones, with the same shape as the first\n    let tensor_1 = Tensor::<Backend, 2>::from_data([[2., 3.], [4., 5.]], &device);\n    let tensor_2 = Tensor::<Backend, 2>::ones_like(&tensor_1);\n\n    // Print the element-wise addition (done with the WGPU backend) of the two tensors.\n    println!(\"{}\", tensor_1 + tensor_2);\n}\n```\n\n----------------------------------------\n\nTITLE: Building WebAssembly Assets for Burn Image Classification (Bash)\nDESCRIPTION: This snippet instructs users to execute a shell script that compiles Rust code for the project into WebAssembly and generates other required assets. Dependency: Rust toolchain with appropriate targets for wasm, required build assets, and the build-for-web.sh script itself. This script must be executed from the project root directory. The input is a direct shell command, and output is a set of build artifacts.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/image-classification-web/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./build-for-web.sh\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Backend for Tensor Operations in Burn (Rust)\nDESCRIPTION: This snippet imports the required Burn framework modules and defines a concrete backend type alias 'B' for the NdArray backend using 32-bit floats. Prerequisites include valid installation of Burn and burn-ndarray crates, as well as Rust's standard configuration for tensor operations. The use statement provides access to prelude utilities, and type aliasing ensures that all subsequent tensors are constructed using the ndarray backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/basic-tensor-op.ipynb#_snippet_1\n\nLANGUAGE: Rust\nCODE:\n```\n// Import packages\nuse burn::prelude::*;\nuse burn_ndarray::NdArray;\n\n// Type alias for the backend\ntype B = NdArray<f32>;\n```\n\n----------------------------------------\n\nTITLE: Example Project Directory Structure\nDESCRIPTION: Illustrates the typical file structure of a Burn example package, showing the organization of source files, examples, and configuration.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/examples.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nexamples/burn-example\nâ”œâ”€â”€ Cargo.toml\nâ”œâ”€â”€ examples\nâ”‚   â”œâ”€â”€ example1.rs      ---> compiled to example1 binary\nâ”‚   â”œâ”€â”€ example2.rs      ---> compiled to example2 binary\nâ”‚   â””â”€â”€ ...\nâ””â”€â”€ src\n    â”œâ”€â”€ lib.rs           ---> this is the root file for a library\n    â”œâ”€â”€ module1.rs\n    â”œâ”€â”€ module2.rs\n    â””â”€â”€ ...\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for Training Artifacts\nDESCRIPTION: Example directory layout showing how burn-train organizes training artifacts including checkpoints, logs, and metrics for both training and validation phases. The structure shows files using compressed message pack format (.mpk.gz) for model, optimizer and scheduler states.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/learner.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nâ”œâ”€â”€ experiment.log\nâ”œâ”€â”€ checkpoint\nâ”‚   â”œâ”€â”€ model-1.mpk.gz\nâ”‚   â”œâ”€â”€ optim-1.mpk.gz\nâ”‚   â””â”€â”€ scheduler-1.mpk.gz\nâ”‚   â”œâ”€â”€ model-2.mpk.gz\nâ”‚   â”œâ”€â”€ optim-2.mpk.gz\nâ”‚   â””â”€â”€ scheduler-2.mpk.gz\nâ”œâ”€â”€ train\nâ”‚   â”œâ”€â”€ epoch-1\nâ”‚   â”‚   â”œâ”€â”€ Accuracy.log\nâ”‚   â”‚   â””â”€â”€ Loss.log\nâ”‚   â””â”€â”€ epoch-2\nâ”‚       â”œâ”€â”€ Accuracy.log\nâ”‚       â””â”€â”€ Loss.log\nâ””â”€â”€ valid\n    â”œâ”€â”€ epoch-1\n    â”‚   â”œâ”€â”€ Accuracy.log\n    â”‚   â””â”€â”€ Loss.log\n    â””â”€â”€ epoch-2\n        â”œâ”€â”€ Accuracy.log\n        â””â”€â”€ Loss.log\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch MNIST Model to ONNX (Bash)\nDESCRIPTION: Runs a Python script that trains a PyTorch MNIST model and exports it to ONNX, given source code in pytorch/mnist.py. No command line arguments are required. The output is an ONNX file suitable for later import into Burn. Requires Python and relevant dependencies as prerequisites.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 pytorch/mnist.py\n```\n\n----------------------------------------\n\nTITLE: Mapping Raw MNIST Items to Structured Format with MapperDataset in Rust\nDESCRIPTION: This code defines types and a mapping implementation for converting flat byte images into 2D array tensors suitable for ML models. It includes WIDTH and HEIGHT constants, an MnistItem struct with image as a 2D array and label, and a BytesToImage mapping struct implementing the Mapper trait. The mapping function checks dimensionality, fills the image array, and creates structured items. Dependencies include the Burn Mapper trait, serialization traits, and Rust\\'s debug facilities.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\nconst WIDTH: usize = 28;\nconst HEIGHT: usize = 28;\n\n# /// MNIST item.\n# #[derive(Deserialize, Serialize, Debug, Clone)]\npub struct MnistItem {\n    /// Image as a 2D array of floats.\n    pub image: [[f32; WIDTH]; HEIGHT],\n\n    /// Label of the image.\n    pub label: u8,\n}\n\nstruct BytesToImage;\n\nimpl Mapper<MnistItemRaw, MnistItem> for BytesToImage {\n    /// Convert a raw MNIST item (image bytes) to a MNIST item (2D array image).\n    fn map(&self, item: &MnistItemRaw) -> MnistItem {\n        // Ensure the image dimensions are correct.\n        debug_assert_eq!(item.image_bytes.len(), WIDTH * HEIGHT);\n\n        // Convert the image to a 2D array of floats.\n        let mut image_array = [[0f32; WIDTH]; HEIGHT];\n        for (i, pixel) in item.image_bytes.iter().enumerate() {\n            let x = i % WIDTH;\n            let y = i / HEIGHT;\n            image_array[y][x] = *pixel as f32;\n        }\n\n        MnistItem {\n            image: image_array,\n            label: item.label,\n        }\n    }\n}\n\ntype MappedDataset = MapperDataset<InMemDataset<MnistItemRaw>, BytesToImage, MnistItemRaw>;\n\n# /// The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000\n# /// images per class. There are 60,000 training images and 10,000 test images.\n# ///\n# /// The data is downloaded from the web from the [CVDF mirror](https://github.com/cvdfoundation/mnist).\npub struct MnistDataset {\n    dataset: MappedDataset,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a New Burn Application with Cargo\nDESCRIPTION: This snippet shows how to create a new Rust application using Cargo, add Burn as a dependency, and build the project.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/getting-started.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ncargo new my_burn_app\ncd my_burn_app\ncargo add burn --features wgpu\ncargo build\n```\n\n----------------------------------------\n\nTITLE: Increasing Recursion Limit in Rust for Burn and wgpu\nDESCRIPTION: This snippet shows the required configuration for projects using Burn with the wgpu backend. Due to deep trait bounds and associated type nesting in wgpu, the default Rust recursion_limit is often insufficient and must be increased (to at least 256) by adding the attribute at the top of main.rs or lib.rs. No dependencies required, but must be placed before any other code. Not doing so may result in compilation errors related to type evaluation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/README.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n#![recursion_limit = \"256\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Remote Backend for Distributed Computing\nDESCRIPTION: Shows setup of Remote backend decorator for distributed tensor operations with client-server architecture.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nfn main_server() {\n    // Start a server on port 3000.\n    burn::server::start::<burn::backend::Cuda>(Default::default(), 3000);\n}\n\nfn main_client() {\n    // Create a client that communicate with the server on port 3000.\n    use burn::backend::{Autodiff, RemoteBackend};\n\n    type Backend = Autodiff<RemoteDevice>;\n\n    let device = RemoteDevice::new(\"ws://localhost:3000\");\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], Distribution::Default, &device);\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Burn Prelude Module\nDESCRIPTION: This code snippet shows how to import commonly used structs and macros from Burn using the prelude module.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/getting-started.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::prelude::*;\n```\n\n----------------------------------------\n\nTITLE: Running Text Classification with Torch CPU Backend\nDESCRIPTION: Commands for training and inferencing text classification models using Torch CPU backend for systems without GPU support.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-classification/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n\n# Use the --release flag to really speed up training.\n\n# AG News\ncargo run --example ag-news-train --release --features tch-cpu   # Train on the ag news dataset\ncargo run --example ag-news-infer --release --features tch-cpu   # Run inference on the ag news dataset\n\n# DbPedia\ncargo run --example db-pedia-train --release --features tch-cpu  # Train on the db pedia dataset\ncargo run --example db-pedia-infer --release --features tch-cpu  # Run inference db pedia dataset\n```\n\n----------------------------------------\n\nTITLE: Adding Embedded-Alloc and Burn to Cargo.toml for no_std Inference - TOML\nDESCRIPTION: This snippet demonstrates the necessary entries for Cargo.toml when working with ONNX inference on an embedded device without standard library support. It specifies the embedded-alloc crate for heap allocation, configures the Burn library using the ndarray backend, and includes burn-import as a build dependency to support automatic Rust model code generation. Dependencies must be placed in the correct Cargo.toml sections, and appropriate versions should be used. The configuration assumes the project will use feature flags to disable default features in Burn.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/no-std.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nembedded-alloc = \"0.6.0\" # Only if there is no default allocator for your chip\nburn = { version = \"0.18\", default-features = false, features = [\"ndarray\"] } # Backend must be ndarray\n\n[build-dependencies]\nburn-import = { version = \"0.18\" } # Used to auto generate the rust code to import the model\n```\n\n----------------------------------------\n\nTITLE: Cloning and Running MNIST Example with Burn in Bash\nDESCRIPTION: This Bash snippet details the steps for cloning the Burn repository and executing the included MNIST example leveraging various backends and hardware features. Dependencies include Cargo (Rust package manager), the Burn repository, and optionally OpenBLAS, Netlib, Tch for GPU/CPU support. Key parameters involve differing features for selecting computational backends and device acceleration, and the sequence sets up both CPU and GPU runs, including CUDA configuration for Tch. Expected output is a series of MNIST training runs, with options to optimize performance using the release flag and multi-threaded BLAS. Ensure the required system dependencies (e.g., CUDA for GPU, OpenBLAS/Netlib libraries) are installed as needed.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/mnist/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Use the --release flag to really speed up training.\\necho \"Using ndarray backend\"\\ncargo run --example mnist --release --features ndarray                # CPU NdArray Backend - f32 - single thread\\ncargo run --example mnist --release --features ndarray-blas-openblas  # CPU NdArray Backend - f32 - blas with openblas\\ncargo run --example mnist --release --features ndarray-blas-netlib    # CPU NdArray Backend - f32 - blas with netlib\\necho \"Using tch backend\"\\nexport TORCH_CUDA_VERSION=cu124                                       # Set the cuda version\\ncargo run --example mnist --release --features tch-gpu                # GPU Tch Backend - f32\\ncargo run --example mnist --release --features tch-cpu                # CPU Tch Backend - f32\\necho \"Using wgpu backend\"\\ncargo run --example mnist --release --features wgpu\n```\n\n----------------------------------------\n\nTITLE: Documenting Burn vs PyTorch Numeric Tensor Operations - Markdown\nDESCRIPTION: Presents a Markdown table listing numeric tensor operations for Burn and their equivalent PyTorch API calls. It is intended for quick cross-referencing by developers familiar with either framework. No executable code is included; all content is presentational, and the main parameters relate to tensor shapes, device locations, scalar values, indices, and dimensions as typical in tensor computations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/tensor.md#_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n### Numeric Operations\n\nThose operations are available for numeric tensor kinds: `Float` and `Int`.\n\n| Burn                                                            | PyTorch Equivalent                             |\n| --------------------------------------------------------------- | ---------------------------------------------- |\n| `Tensor::eye(size, device)`                                     | `torch.eye(size, device=device)`               |\n| `Tensor::full(shape, fill_value, device)`                       | `torch.full(shape, fill_value, device=device)` |\n| `Tensor::ones(shape, device)`                                   | `torch.ones(shape, device=device)`             |\n| `Tensor::zeros(shape, device)`                                  | `torch.zeros(shape, device=device)`            |\n| `tensor.abs()`                                                  | `torch.abs(tensor)`                            |\n| `tensor.add(other)` or `tensor + other`                         | `tensor + other`                               |\n| `tensor.add_scalar(scalar)` or `tensor + scalar`                | `tensor + scalar`                              |\n| `tensor.all_close(other, atol, rtol)`                           | `torch.allclose(tensor, other, atol, rtol)`    |\n| `tensor.argmax(dim)`                                            | `tensor.argmax(dim)`                           |\n| `tensor.argmin(dim)`                                            | `tensor.argmin(dim)`                           |\n| `tensor.argsort(dim)`                                           | `tensor.argsort(dim)`                          |\n| `tensor.argsort_descending(dim)`                                | `tensor.argsort(dim, descending=True)`         |\n| `tensor.bool()`                                                 | `tensor.bool()`                                |\n| `tensor.clamp(min, max)`                                        | `torch.clamp(tensor, min=min, max=max)`        |\n| `tensor.clamp_max(max)`                                         | `torch.clamp(tensor, max=max)`                 |\n| `tensor.clamp_min(min)`                                         | `torch.clamp(tensor, min=min)`                 |\n| `tensor.contains_nan()`                                         | N/A                                            |\n| `tensor.div(other)` or `tensor / other`                         | `tensor / other`                               |\n| `tensor.div_scalar(scalar)` or `tensor / scalar`                | `tensor / scalar`                              |\n| `tensor.equal_elem(other)`                                      | `tensor.eq(other)`                             |\n| `tensor.full_like(fill_value)`                                  | `torch.full_like(tensor, fill_value)           |\n| `tensor.gather(dim, indices)`                                   | `torch.gather(tensor, dim, indices)`           |\n| `tensor.greater(other)`                                         | `tensor.gt(other)`                             |\n| `tensor.greater_elem(scalar)`                                   | `tensor.gt(scalar)`                            |\n| `tensor.greater_equal(other)`                                   | `tensor.ge(other)`                             |\n| `tensor.greater_equal_elem(scalar)`                             | `tensor.ge(scalar)`                            |\n| `tensor.is_close(other, atol, rtol)`                            | `torch.isclose(tensor, other, atol, rtol)`     |\n| `tensor.is_nan()`                                               | `torch.isnan(tensor)`                          |\n| `tensor.lower(other)`                                           | `tensor.lt(other)`                             |\n| `tensor.lower_elem(scalar)`                                     | `tensor.lt(scalar)`                            |\n| `tensor.lower_equal(other)`                                     | `tensor.le(other)`                             |\n| `tensor.lower_equal_elem(scalar)`                               | `tensor.le(scalar)`                            |\n| `tensor.mask_fill(mask, value)`                                 | `tensor.masked_fill(mask, value)`              |\n| `tensor.mask_where(mask, value_tensor)`                         | `torch.where(mask, value_tensor, tensor)`      |\n| `tensor.max()`                                                  | `tensor.max()`                                 |\n| `tensor.max_abs()`                                              | `tensor.abs().max()`                           |\n| `tensor.max_abs_dim(dim)`                                       | `tensor.abs().max(dim, keepdim=True)`          |\n| `tensor.max_dim(dim)`                                           | `tensor.max(dim, keepdim=True)`                |\n| `tensor.max_dim_with_indices(dim)`                              | N/A                                            |\n| `tensor.max_pair(other)`                                        | `torch.Tensor.max(a,b)`                        |\n| `tensor.mean()`                                                 | `tensor.mean()`                                |\n| `tensor.mean_dim(dim)`                                          | `tensor.mean(dim, keepdim=True)`               |\n| `tensor.min()`                                                  | `tensor.min()`                                 |\n| `tensor.min_dim(dim)`                                           | `tensor.min(dim, keepdim=True)`                |\n| `tensor.min_dim_with_indices(dim)`                              | N/A                                            |\n| `tensor.min_pair(other)`                                        | `torch.Tensor.min(a,b)`                        |\n| `tensor.mul(other)` or `tensor * other`                         | `tensor * other`                               |\n| `tensor.mul_scalar(scalar)` or `tensor * scalar`                | `tensor * scalar`                              |\n| `tensor.neg()` or `-tensor`                                     | `-tensor`                                      |\n| `tensor.not_equal_elem(scalar)`                                 | `tensor.ne(scalar)`                            |\n| `tensor.ones_like()`                                            | `torch.ones_like(tensor)`                      |\n| `tensor.one_hot(num_classes)`                                   | `torch.nn.functional.one_hot`                  |\n| `tensor.one_hot_fill(num_classes, on_value, off_value, axis)`   | N/A                                            |\n| `tensor.pad(pads, value)`                                       | `torch.nn.functional.pad(input, pad, value)`   |\n| `tensor.powf(other)` or `tensor.powi(intother)`                 | `tensor.pow(other)`                            |\n| `tensor.powf_scalar(scalar)` or `tensor.powi_scalar(intscalar)` | `tensor.pow(scalar)`                           |\n| `tensor.prod()`                                                 | `tensor.prod()`                                |\n| `tensor.prod_dim(dim)`                                          | `tensor.prod(dim, keepdim=True)`               |\n| `tensor.rem(other)` or `tensor % other`                         | `tensor % other`                               |\n| `tensor.scatter(dim, indices, values)`                          | `tensor.scatter_add(dim, indices, values)`     |\n| `tensor.select(dim, indices)`                                   | `tensor.index_select(dim, indices)`            |\n| `tensor.select_assign(dim, indices, values)`                    | N/A                                            |\n| `tensor.sign()`                                                 | `tensor.sign()`                                |\n| `tensor.sort(dim)`                                              | `tensor.sort(dim).values`                      |\n| `tensor.sort_descending(dim)`                                   | `tensor.sort(dim, descending=True).values`     |\n| `tensor.sort_descending_with_indices(dim)`                      | `tensor.sort(dim, descending=True)`            |\n| `tensor.sort_with_indices(dim)`                                 | `tensor.sort(dim)`                             |\n| `tensor.sub(other)` or `tensor - other`                         | `tensor - other`                               |\n| `tensor.sub_scalar(scalar)` or `tensor - scalar`                | `tensor - scalar`                              |\n| `tensor.sum()`                                                  | `tensor.sum()`                                 |\n| `tensor.sum_dim(dim)`                                           | `tensor.sum(dim, keepdim=True)`                |\n| `tensor.topk(k, dim)`                                           | `tensor.topk(k, dim).values`                   |\n| `tensor.topk_with_indices(k, dim)`                              | `tensor.topk(k, dim)`                          |\n| `tensor.tril(diagonal)`                                         | `torch.tril(tensor, diagonal)`                 |\n| `tensor.triu(diagonal)`                                         | `torch.triu(tensor, diagonal)`                 |\n| `tensor.zeros_like()`                                           | `torch.zeros_like(tensor)`                     |\n```\n\n----------------------------------------\n\nTITLE: Generating Rust Model Import Code with ModelGen - Rust\nDESCRIPTION: This code snippet demonstrates the use of the ModelGen utility to generate Rust source files for model import. The configuration designates the source model input, output directory, specifies binary serialization with RecordType::Bincode, and embeds model states directly in the output. The .run_from_script() finalizes code generation, expected to run at build time. This process requires that ModelGen and RecordType are in scope and that the correct model path and output directory are provided.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/no-std.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nModelGen::new()\n    .input(my_model)\n    .out_dir(\"model/\")\n    .record_type(RecordType::Bincode)\n    .embed_states(true)\n    .run_from_script();\n```\n\n----------------------------------------\n\nTITLE: Printing Model Structure with Burn via Cargo - Shell\nDESCRIPTION: This shell snippet is used to print the model architecture or metadata by running the 'print' binary through Cargo in release mode with the Burn framework. Prerequisites are Rust, Cargo, and an implemented 'print' binary in the project. Input: No direct input, uses project model definitions; Output: Human-readable printout of the model details to the console. Limitation: Relies on appropriate implementation of the 'print' binary and assumes the trained model exists.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/guide/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --bin print --release\n```\n\n----------------------------------------\n\nTITLE: Installing Evcxr Jupyter Kernel with Cargo - Shell\nDESCRIPTION: This shell snippet demonstrates installation of the Evcxr Jupyter kernel using Cargo, which is required for executing Rust code in Jupyter Notebooks. The command downloads and compiles the 'evcxr_jupyter' crate, adding it to the user's Rust binaries. Cargo (the Rust package manager) must be installed before executing this command. The output is the installation of the evcxr_jupyter binary; no parameters are needed, but sufficient permissions to install Rust binaries are required.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncargo install evcxr_jupyter\n```\n\n----------------------------------------\n\nTITLE: Declaring and Initializing a Global Allocator with embedded-alloc - Rust\nDESCRIPTION: This snippet sets up a global heap allocator in systems without a standard allocator using the embedded-alloc crate. The LlffHeap is declared as the allocator, and memory for the heap is statically allocated using MaybeUninit. The heap size must match or exceed the model's memory requirements. The initialization occurs at runtime, inside the main async function, using unsafe code to provide a mutable pointer to the allocator. Embassy's executor provides async context for embedded Rust. This is required for dynamic memory allocation in a no_std Rust environment.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/no-std.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse embedded_alloc::LlffHeap as Heap;\n\n#[global_allocator]\nstatic HEAP: Heap = Heap::empty();\n\n#[embassy_executor::main]\nasync fn main(_spawner: Spawner) {\n\t{\n        use core::mem::MaybeUninit;\n        const HEAP_SIZE: usize = 100 * 1024; // This is dependent on the model size in memory.\n        static mut HEAP_MEM: [MaybeUninit<u8>; HEAP_SIZE] = [MaybeUninit::uninit(); HEAP_SIZE];\n        unsafe { HEAP.init(&raw mut HEAP_MEM as usize, HEAP_SIZE) }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Including ONNX Model Code in Rust Module (Burn, Rust)\nDESCRIPTION: This Rust code includes the generated ONNX model code by using Rust's macro facilities to embed content from the OUT_DIR into a module. It is placed in the model's mod.rs, allowing the ONNX model (converted by burn-import) to be accessible under the mnist module. Requires burn-import to generate the corresponding mnist.rs at build time; the macro concatenates an environment variable to reach the output file. No inputs; the output is a module exposing model types/functions for inference.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\npub mod mnist {\n    include!(concat!(env!(\"OUT_DIR\"), \"/model/mnist.rs\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Recursion Limit for WGPU Backend\nDESCRIPTION: This code snippet demonstrates how to set the recursion limit to resolve compilation errors related to complex type nesting within the WGPU dependency chain.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/getting-started.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n#![recursion_limit = \"256\"]\n```\n\n----------------------------------------\n\nTITLE: Exposing Model Module in Library Root (Burn, Rust)\nDESCRIPTION: This Rust snippet demonstrates how to expose the model submodule (containing the ONNX-based MNIST model) from the library's root. By adding the model module and using a public re-export, the model's symbols can be accessed from the main/inference binary. Requires the mod.rs/module from previous step. No parameters; exposes mnist types for further integration.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\npub mod model;\n\npub use model::mnist::*;\n```\n\n----------------------------------------\n\nTITLE: Running Burn ONNX Inference from Command Line (Bash)\nDESCRIPTION: This snippet demonstrates how to execute the ONNX inference Rust binary for an MNIST model using Burn. It requires the project to be built and the binary to be available at the specified path. Providing an image index as a command line argument allows inference for a specific sample from the data set; expected output will show the predicted and actual class, along with optional diagnostics. No external dependencies are required beyond Rust and cargo.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/onnx-inference/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo run -- 15\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting LibTorch CPU Distribution on Windows - PowerShell\nDESCRIPTION: This PowerShell snippet downloads the LibTorch CPU distribution for Windows and extracts it using built-in commands. It's a two-step process: the first line downloads the zip file into the current directory, and the second unpacks it. Used to provision tch-rs dependencies on Windows with CPU backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_7\n\nLANGUAGE: powershell\nCODE:\n```\nwget https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-2.6.0%2Bcpu.zip -OutFile libtorch.zip\nExpand-Archive libtorch.zip\n```\n\n----------------------------------------\n\nTITLE: Adding the Model Module to main.rs - Rust\nDESCRIPTION: Demonstrates how to include the newly-defined model module in the main.rs of the Rust project using the 'mod' declaration. This enables access to model definitions throughout the binary. No dependencies aside from the existence of 'src/model.rs'; add this at the top-level scope before main if defining executable behavior.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nmod model;\\n#\\n# fn main() {\\n# }\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LibTorch CPU on Windows - PowerShell\nDESCRIPTION: These commands configure the LIBTORCH environment variable and update the Windows PATH to include the path to LibTorch. This is essential to ensure that burn-tch or dependent crates can find the libraries during build and runtime. Both variables should point to the LibTorch root directory.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_8\n\nLANGUAGE: powershell\nCODE:\n```\n$Env:LIBTORCH = \"/absolute/path/to/libtorch/\"\n$Env:Path += \";/absolute/path/to/libtorch/\"\n```\n\n----------------------------------------\n\nTITLE: Cloning and Running Burn Text Generation Example on Mac (Bash)\nDESCRIPTION: This bash snippet outlines the commands for Mac users to clone the Burn repository and execute the text generation example. It avoids setting CUDA-specific environment variables, making it suitable for Mac environments without CUDA support. Users must have git, cargo, and Rust installed. The output is the running of the Burn text generation example in release mode, leveraging maximum performance possible on the Mac.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-generation/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\\ncd burn\\n\\n# Use the --release flag to really speed up training.\\ncargo run --example text-generation --release\n```\n\n----------------------------------------\n\nTITLE: Running Text Classification with ndarray Backend\nDESCRIPTION: Commands for training and inferencing using ndarray backend with support for different matrix multiplication techniques including BLAS implementations.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-classification/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\ncd burn\n\n# Use the --release flag to really speed up training.\n\n# Replace ndarray by ndarray-blas-netlib, ndarray-blas-openblas or ndarray-blas-accelerate for different matmul techniques\n\n# AG News\ncargo run --example ag-news-train --release --features ndarray   # Train on the ag news dataset\ncargo run --example ag-news-infer --release --features ndarray   # Run inference on the ag news dataset\n\n# DbPedia\ncargo run --example db-pedia-train --release --features ndarray  # Train on the db pedia dataset\ncargo run --example db-pedia-infer --release --features ndarray  # Run inference db pedia dataset\n```\n\n----------------------------------------\n\nTITLE: Running the Training Example with Different Backends - Shell\nDESCRIPTION: Describes shell commands to run the image classification training example either with the Torch GPU (CUDA) or WGPU backend using the cargo tool. Requires an installed Rust toolchain, Burn dependencies, and an environment matching the backend requirements (CUDA for tch-gpu). The TORCH_CUDA_VERSION variable must be set for GPU mode; the '--features wgpu' switch enables the WGPU backend for broader hardware compatibility. Both commands build and execute the example.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/custom-image-dataset/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport TORCH_CUDA_VERSION=cu124\\ncargo run --example custom-image-dataset --release --features tch-gpu\n```\n\nLANGUAGE: shell\nCODE:\n```\ncargo run --example custom-image-dataset --release --features wgpu\n```\n\n----------------------------------------\n\nTITLE: Organizing Dataset Folder Structure for Image Classification - Shell\nDESCRIPTION: Demonstrates the directory structure required for loading the CIFAR-10 dataset with the ImageFolderDataset utility. Each class label is represented as a subfolder within 'train' and 'test' directories, and a labels.txt file is present. No dependencies are required for the structure, but accurate setup is critical for the dataset loader to function correctly. This structure enables automatic label discovery and sample assignment based on folder names.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/custom-image-dataset/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncifar10\\nâ”œâ”€â”€ labels.txt\\nâ”œâ”€â”€ test\\nâ”‚   â”œâ”€â”€ airplane\\nâ”‚   â”œâ”€â”€ automobile\\nâ”‚   â”œâ”€â”€ bird\\nâ”‚   â”œâ”€â”€ cat\\nâ”‚   â”œâ”€â”€ deer\\nâ”‚   â”œâ”€â”€ dog\\nâ”‚   â”œâ”€â”€ frog\\nâ”‚   â”œâ”€â”€ horse\\nâ”‚   â”œâ”€â”€ ship\\nâ”‚   â””â”€â”€ truck\\nâ””â”€â”€ train\\n    â”œâ”€â”€ airplane\\n    â”œâ”€â”€ automobile\\n    â”œâ”€â”€ bird\\n    â”œâ”€â”€ cat\\n    â”œâ”€â”€ deer\\n    â”œâ”€â”€ dog\\n    â”œâ”€â”€ frog\\n    â”œâ”€â”€ horse\\n    â”œâ”€â”€ ship\\n    â””â”€â”€ truck\n```\n\n----------------------------------------\n\nTITLE: Implementing NodeCodegen Trait for New Operators in Rust\nDESCRIPTION: Outline of the NodeCodegen trait implementation required for integrating new operators into the Burn framework. This code structure is essential for defining how new nodes generate code during graph compilation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/guides/onnx-to-burn-conversion-tool.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nimpl NodeCodegen for SqueezeNode {\n    fn output_types(&self) -> Vec<TensorType> {\n        // Define output tensor types\n    }\n\n    fn input_types(&self) -> Vec<TensorType> {\n        // Define input tensor types\n    }\n\n    fn forward(&self) -> TokenStream {\n        // Generate Rust code for forward pass using quote! macro\n    }\n\n    fn into_node(self) -> Node {\n        // Wrap the specific node in a general Node type\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Training WGAN MNIST Burn Example on Various Backends - Shell\nDESCRIPTION: These shell commands show how to run the WGAN MNIST example in the Burn framework using different backend features by passing flags to Cargo. The commands include setups for CUDA, Wgpu, Torch (GPU and CPU), and NdArray-based BLAS backends targeting f32 precision. The Tch GPU command also requires setting the TORCH_CUDA_VERSION environment variable before running. Each command trains the WGAN model for MNIST using the selected feature backend; output and performance may vary depending on the compute backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/wgan/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Cuda backend\\ncargo run --example wgan-mnist --release --features cuda\\n\\n# Wgpu backend\\ncargo run --example wgan-mnist --release --features wgpu\\n\\n# Tch GPU backend\\nexport TORCH_CUDA_VERSION=cu124 # Set the cuda version\\ncargo run --example wgan-mnist --release --features tch-gpu\\n\\n# Tch CPU backend\\ncargo run --example wgan-mnist --release --features tch-cpu\\n\\n# NdArray backend (CPU)\\ncargo run --example wgan-mnist --release --features ndarray                # f32 - single thread\\ncargo run --example wgan-mnist --release --features ndarray-blas-openblas  # f32 - blas with openblas\\ncargo run --example wgan-mnist --release --features ndarray-blas-netlib    # f32 - blas with netlib\n```\n\n----------------------------------------\n\nTITLE: Accessing the Demo Application in Browser (Plaintext URL)\nDESCRIPTION: This snippet represents the URL to access the running demo in a web browser. It assumes the local server is up on port 8000. No dependencies aside from a running local web server. Expected input: user enters this URL in a browser; Output: the demo frontend loads.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/image-classification-web/README.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://localhost:8000\n```\n\n----------------------------------------\n\nTITLE: Defining Nested PyTorch Model for Attribute Mapping - Python\nDESCRIPTION: Shows a nested PyTorch model with inner ConvModule used to illustrate attribute remapping when loading into a different Burn model structure. Requires PyTorch. The example has ConvModule embedded inside a Net class for demonstration purposes, facilitating explanation of how Burn maps flat and nested attributes. Outputs are model definitions, used mainly for illustrating remapping logic.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass ConvModule(nn.Module):\\n    def __init__(self):\\n        super(ConvModule, self).__init__()\\n        self.conv1 = nn.Conv2d(2, 2, (2,2))\\n        self.conv2 = nn.Conv2d(2, 2, (2,2), bias=False)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.conv2(x)\\n        return x\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv = ConvModule()\\n\\n    def forward(self, x):\\n        x = self.conv(x)\\n        return x\n```\n\n----------------------------------------\n\nTITLE: Running LSTM Training with Different Backends\nDESCRIPTION: Commands for training LSTM models using various backend options including CUDA, WGPU, TCH (GPU/CPU), and NdArray backends. Each command demonstrates how to run the training example with specific feature flags.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/modern-lstm/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Cuda backend\ncargo run --example lstm-train --release --features cuda\n\n# Wgpu backend\ncargo run --example lstm-train --release --features wgpu\n\n# Tch GPU backend\nexport TORCH_CUDA_VERSION=cu124 # Set the cuda version\ncargo run --example lstm-train --release --features tch-gpu\n\n# Tch CPU backend\ncargo run --example lstm-train --release --features tch-cpu\n\n# NdArray backend (CPU)\ncargo run --example lstm-train --release --features ndarray\ncargo run --example lstm-train --release --features ndarray-blas-openblas\ncargo run --example lstm-train --release --features ndarray-blas-netlib\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LibTorch CPU on MacOS - shell\nDESCRIPTION: This snippet sets the LIBTORCH and DYLD_LIBRARY_PATH environment variables for MacOS to use the manually installed LibTorch CPU backend with burn-tch. These must be set before building the project; both require absolute paths to the local libtorch installation. No output is generated.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport LIBTORCH=/absolute/path/to/libtorch/\nexport DYLD_LIBRARY_PATH=/absolute/path/to/libtorch/lib:$DYLD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Cloning and Running Burn Text Generation Example with CUDA (Bash)\nDESCRIPTION: This bash snippet provides the steps for setting up and executing the Burn text generation example with CUDA support. It first clones the Burn repository and changes to the project directory. The script sets the TORCH_CUDA_VERSION environment variable to specify the CUDA version (cu124) and runs the text generation example using cargo in release mode for improved performance. Dependencies include git, cargo, Rust, and a compatible CUDA toolkit. The expected input is shell access with the ability to run these commands, and the output is the execution of the text-generation example.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/text-generation/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tracel-ai/burn.git\\ncd burn\\n\\n# Use the --release flag to really speed up training.\\nexport TORCH_CUDA_VERSION=cu124\\ncargo run --example text-generation --release\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference with Burn via Cargo - Shell\nDESCRIPTION: This shell snippet shows how to perform inference on a pre-trained model using the Burn framework by executing the 'infer' binary in release mode with Cargo. Dependencies include Rust, Cargo, a functional 'infer' binary, and that a model has already been trained. Input: Utilizes the trained model from previous steps; Output: Inference results, depending on the implementation, typically printed to the console or output file. Limitation: Assumes the existence of a valid 'infer' binary and compatible model artifacts.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/guide/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --bin infer --release\n```\n\n----------------------------------------\n\nTITLE: Downloading and Unzipping LibTorch CPU Distribution on Linux - shell\nDESCRIPTION: This shell snippet downloads and unpacks the LibTorch CPU distribution required by the tch-rs crate for Linux systems. The first command fetches the archive using wget, and the second extracts it. The static URL points to PyTorch 2.6.0 for CPU; output is the extracted libtorch directory.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget -O libtorch.zip https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-2.6.0%2Bcpu.zip\nunzip libtorch.zip\n```\n\n----------------------------------------\n\nTITLE: Registering Evcxr Kernel in Jupyter - Shell\nDESCRIPTION: This shell command registers the Evcxr Jupyter kernel with Jupyter, enabling users to select 'Rust' as a kernel option in notebook environments. The 'evcxr_jupyter --install' command ensures Jupyter recognizes the kernel for executing Rust code in notebooks. This requires the evcxr_jupyter binary to be installed (typically via Cargo, as described previously). On success, the kernel becomes available for use in any supported Jupyter frontend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nevcxr_jupyter --install\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Burn Inference - Bash\nDESCRIPTION: Displays the expected terminal output after running model inference with the specified image index. Output includes compilation status, execution path, the image index processed, predicted and actual digit values, and a URL to view the source image online. This output helps users verify correct operation and interpret model results. No dependencies are required to review this output, but it presumes successful prior execution of the Rust command.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/pytorch-import/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nFinished dev [unoptimized + debuginfo] target(s) in 0.13s\n    Running `burn/target/debug/onnx-inference 15`\n\nImage index: 15\nSuccess!\nPredicted: 5\nActual: 5\nSee the image online, click the link below:\nhttps://huggingface.co/datasets/ylecun/mnist/viewer/mnist/test?row=15\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LibTorch CPU on Linux - shell\nDESCRIPTION: This snippet configures the LIBTORCH and LD_LIBRARY_PATH environment variables so that the Rust build process can find the manually installed LibTorch CPU distribution on Linux. Must be run before building burn-tch or dependent crates; paths should be absolute. No explicit output.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport LIBTORCH=/absolute/path/to/libtorch/\nexport LD_LIBRARY_PATH=/absolute/path/to/libtorch/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Managing Artifact Directories in Rust with std::fs\nDESCRIPTION: Utility function to ensure a clean directory for training artifacts by removing any existing directory and recreating it. Uses Rust's standard library for file system manipulation (`std::fs`). The parameter is the path to the artifact directory; the function is called prior to training to guarantee logs, checkpoints, and model files are fresh and not contaminated by old runs. No return value and minimal error handling: errors on directory removal or creation are ignored using `.ok()`. Should be used with caution in production environments.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/training.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nfn create_artifact_dir(artifact_dir: &str) {\n    // Remove existing artifacts before to get an accurate learner summary\n    std::fs::remove_dir_all(artifact_dir).ok();\n    std::fs::create_dir_all(artifact_dir).ok();\n}\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Burn Executable - Bash\nDESCRIPTION: Shows the command to execute a Rust-built inference binary using cargo run with an argument. This example demonstrates how to pass an image index (here, 15) to the inference executable. Requires Rust toolchain, the Burn library, and previously imported (converted) model weights. Output is produced on the command line and indicates both predicted and actual label results from the MNIST dataset.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/pytorch-import/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo run -- 15\n```\n\n----------------------------------------\n\nTITLE: Running Device Validation Binaries for Burn Torch Backend - shell\nDESCRIPTION: This shell command group demonstrates how to validate the Burn backend installation by running device-specific binaries for CPU, CUDA, or MPS. Requires previous successful build and a working device setup. Each command executes the corresponding binary and outputs the result for that backend.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncargo run --bin cpu --release\ncargo run --bin cuda --release\ncargo run --bin mps --release\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure Example\nDESCRIPTION: Directory structure showing the layout of a Raspberry Pi Pico ONNX inference project, including source files, model directory, and TensorFlow training scripts.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/raspberry-pi-pico/README.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nraspberry-pi-pico\nâ”œâ”€â”€ Cargo.lock\nâ”œâ”€â”€ Cargo.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ build.rs\nâ”œâ”€â”€ memory.x\nâ”œâ”€â”€ src\nâ”‚   â”œâ”€â”€ bin\nâ”‚   â”‚   â””â”€â”€ main.rs\nâ”‚   â”œâ”€â”€ lib.rs\nâ”‚   â””â”€â”€ model\nâ”‚       â”œâ”€â”€ mod.rs\nâ”‚       â””â”€â”€ sine.onnx\nâ””â”€â”€ tensorflow\n    â”œâ”€â”€ requirements.txt\n    â””â”€â”€ train.py\n```\n\n----------------------------------------\n\nTITLE: Running the Burn MNIST Web Server - Shell Script\nDESCRIPTION: This shell command launches a local web server that serves the built Burn MNIST WebAssembly demo. It assumes the model has already been built and will be accessible at http://localhost:8000/. The environment must support shell execution and provide the necessary permissions. No additional parameters are needed. This is essential for previewing and testing the inference application locally.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/mnist-inference-web/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n```shell\\n./run-server.sh\\n```\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Cargo Config with Library Paths Using Bash - bash\nDESCRIPTION: This bash snippet creates a .cargo directory if it does not already exist, then writes a config.toml file with LD_LIBRARY_PATH and LIBTORCH settings via a heredoc. This is an alternative to manual environment variable export; LD_LIBRARY_PATH includes the previous value for chaining. Useful for scripting setup steps.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmkdir .cargo\ncat <<EOF > .cargo/config.toml\n[env]\nLD_LIBRARY_PATH = \"/absolute/path/to/libtorch/lib:$LD_LIBRARY_PATH\"\nLIBTORCH = \"/absolute/path/to/libtorch/libtorch\"\nEOF\n```\n\n----------------------------------------\n\nTITLE: Example Compilation Error Output from Rust Compiler for Unconstrained Type Parameter\nDESCRIPTION: This snippet contains console output from the Rust compiler describing an error (E0207) due to an unconstrained type parameter B in the previous implementation block. The error states that B is not constrained by the impl trait, self type, or predicates, highlighting a common pitfall in Rust generics.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/custom-training-loop.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n1. the type parameter `B` is not constrained by the impl trait, self type, or predicates\n   unconstrained type parameter [E0207]\n```\n\n----------------------------------------\n\nTITLE: Defining Burn Router Documentation\nDESCRIPTION: Documentation header explaining the purpose of the Burn Router as a multi-backend extension that handles tensor operation routing.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-router/README.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Burn Router\n\nA multi-backend extension that forwards the tensor operations to the appropriate backend.\n```\n\n----------------------------------------\n\nTITLE: Defining MNIST Raw Item Struct in Rust\nDESCRIPTION: This snippet declares a Rust struct, MnistItemRaw, for the raw representation of a single MNIST sample in Burn datasets. \\'image_bytes\\' stores the flattened image as a byte vector, and \\'label\\' stores the digit class. Serialization and debugging traits are derived via standard Rust mechanisms. There are no external dependencies for the struct itself but it is intended for use with MNIST parsing functions.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/building-blocks/dataset.md#_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\n# #[derive(Deserialize, Debug, Clone)]\nstruct MnistItemRaw {\n    pub image_bytes: Vec<u8>,\n    pub label: u8,\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Dependencies for Burn Notebook\nDESCRIPTION: Declares dependencies required for the notebook, including burn core library, burn-ndarray backend, and image libraries for visualization. The dependencies are specified in a format similar to Cargo.toml.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/notebook/plots.ipynb#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n// Dependency declarations for the notebook. WARNING: It may take a while to compile the first time.\n\n// The syntax is similar to the one used in the Cargo.toml file. Just prefix with :dep\n// See: https://github.com/evcxr/evcxr/blob/main/COMMON.md\n\n:dep burn = {path = \"../../crates/burn\"}\n:dep burn-ndarray = {path = \"../../crates/burn-ndarray\"}\n\n// The following dependencies are used for plotting\n:dep image = \"0.23\"\n:dep evcxr_image = \"1.1\"\n```\n\n----------------------------------------\n\nTITLE: Exporting CUDA Version Environment Variable for Burn Torch Backend - shell\nDESCRIPTION: This snippet sets the TORCH_CUDA_VERSION environment variable to select the CUDA distribution before retrieving the tch-rs dependency with cargo. This allows the project to use a specific CUDA runtime for the Torch backend. Requires a compatible CUDA version and proper driver support; input is the environment variable, and no output is produced.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport TORCH_CUDA_VERSION=cu124\n```\n\n----------------------------------------\n\nTITLE: Rust Module Declaration Examples\nDESCRIPTION: Demonstrates different visibility levels for Rust modules within a crate, including public, crate-public, and private modules.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/examples.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\npub mod data;\npub mod inference;\npub(crate) mod model;\nmod training;\n```\n\n----------------------------------------\n\nTITLE: Launching Local Web Server for Burn Demo (Bash)\nDESCRIPTION: This code runs a shell script to start a local web server, which serves the compiled files for browser-based testing. Prerequisite: The run-server.sh script must exist and have execution permissions. It listens (typically) on port 8000 and serves web assets. Input: executes script; Output: a running local HTTP server.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/image-classification-web/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./run-server.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LibTorch CUDA on Windows - PowerShell\nDESCRIPTION: This PowerShell sequence sets the LIBTORCH environment variable and modifies the PATH to include the directory containing the CUDA-enabled LibTorch libraries for Windows. This ensures proper linking for burn-tch with CUDA support during build and execution. Both paths must be absolute.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_12\n\nLANGUAGE: powershell\nCODE:\n```\n$Env:LIBTORCH = \"/absolute/path/to/libtorch/\"\n$Env:Path += \";/absolute/path/to/libtorch/\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Overview section of the Burn Contributor's Book describing the main documentation sections and their purposes\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/overview.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Overview\n\nWelcome to The Burn Contributor's Book ðŸ‘‹\n\nThis book will help you get acquainted with the internals of the Burn deep learning framework and\nprovide some detailed guidance on how to contribute to the project.\n\nWe have crafted some sections for you:\n\n- [Getting Started](./getting-started): Much like the [Burn Book](https://burn.dev/burn-book/) which\n  targets users, we'll start with the fundamentals, guiding you through tasks like setting up the\n  development environment, running tests, and what you should check prior to each commit.\n\n- [Project Architecture](./project-architecture): This section will give you an in-depth look at the\n  architecture of Burn.\n\n- [Guides](./guides): We provide some guides on how to do specific tasks, such as adding a new\n  operations to Burn.\n\n- [Frequently Encountered Issues](./frequently-encountered-issues): If you are running into an issue\n  that has you stumped, this is the section to check out prior to asking on the\n  [Discord](https://discord.gg/uPEBbYYDB6). It's a collection of errors encountered by contributors,\n  what caused them, and how they were resolved.\n```\n\n----------------------------------------\n\nTITLE: Generating MNIST Images with WGAN Burn Example - Shell\nDESCRIPTION: This shell command demonstrates the generation of MNIST digit images using the trained WGAN model in the Burn framework. It invokes the `wgan-generate` binary with the chosen backend, here specified as CUDA, but any supported backend's feature flags could be used. The command generates and saves new digit images according to the backend and model configuration. Prior training is required, and corresponding model outputs are saved in the appropriate output directory based on the implementation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/wgan/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --example wgan-generate --release --features cuda\n```\n\n----------------------------------------\n\nTITLE: Downloading and Unzipping LibTorch CUDA Distribution on Linux - shell\nDESCRIPTION: This shell snippet downloads and extracts the LibTorch CUDA (version 12.6) distribution for Linux. It is required for GPU acceleration with tch-rs using CUDA. The URL and filename include the CUDA version, and the extracted folder is required for subsequent environment configuration.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-tch/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nwget -O libtorch.zip https://download.pytorch.org/libtorch/cu126/libtorch-cxx11-abi-shared-with-deps-2.6.0%2Bcu126.zip\nunzip libtorch.zip\n```\n\n----------------------------------------\n\nTITLE: Installing mdbook Using Cargo - Bash\nDESCRIPTION: This bash snippet demonstrates the installation of mdbook, a documentation tool used for building and serving the Burn Book and Contributor Book. It requires that Rust and Cargo are already installed on the system. The command downloads and builds the mdbook binary as a global utility, which can then be used in conjunction with project documentation workflows. The snippet expects network access to crates.io, and may require elevated permissions in some environments.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/getting-started/setting-up-the-environment.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo install mdbook\n\n```\n\n----------------------------------------\n\nTITLE: Weight File Structure for Enum Modules (Text Format)\nDESCRIPTION: This snippet specifies the textual format of keys, shapes, and data types for a weights file that aligns with Rust model definitions. Each entry declares the top-level key path (representing nested model attributes), the tensor shape, and its data type as read by the PyTorchFileRecorder import logic. Serves as a template or reference for how PyTorch-exported parameters map to Rust-side model variants and fields. Typically produced from a PyTorch save and consumed during Burn model loading.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/import/pytorch-model.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n---\nKey: conv.dconv.bias\nShape: [2]\nDtype: F32\n---\nKey: conv.dconv.weight\nShape: [2, 1, 3, 3]\nDtype: F32\n---\nKey: conv.pconv.bias\nShape: [2]\nDtype: F32\n---\nKey: conv.pconv.weight\nShape: [2, 2, 1, 1]\nDtype: F32\n\n```\n\n----------------------------------------\n\nTITLE: Setting ROCm Path Environment Variable for Burn Tests\nDESCRIPTION: Instructions for setting up the environment variable required to run tests for the ROCm HIP backend. The variable should point to the ROCm installation directory.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-rocm/README.md#_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\nTo execute the tests for this backend set an environment variable called `ROCM_PATH` or `CUBECL_ROCM_PATH` to the installation path of ROCm. It is often `/opt/rocm`.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment Using UV Shell Commands\nDESCRIPTION: This shell code block lists commands for creating and syncing a Python virtual environment with all required dependencies for ONNX and PyTorch model generation using the uv tool. Prerequisites include having the uv package manager installed and being in the appropriate project directory. The commands ensure the creation of a .venv directory and install the required packages. Inputs are the commands to run in the terminal; the output is a populated virtual environment ready for further ONNX model generation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn-import/onnx-tests/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd crates/burn-import/onnx-tests\nuv sync # or uv sync -f\n```\n\n----------------------------------------\n\nTITLE: Version Bump Command for Burn Project\nDESCRIPTION: Command using cargo-edit to bump the minor version number of the project. This is used when preparing for a new release.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncargo set-version --bump minor\n```\n\n----------------------------------------\n\nTITLE: Configuring Rustflags for WASM Target in Cargo (TOML)\nDESCRIPTION: This code snippet configures the getrandom backend for the wasm32-unknown-unknown target by specifying rustflags within a .cargo/config.toml file. The provided rustflags explicitly set getrandom_backend to wasm_js, which is required so that the correct random source is chosen in WebAssembly builds. The configuration should be placed in the .cargo/config.toml file at the project root.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/web-assembly.md#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[target.wasm32-unknown-unknown]\nrustflags = ['--cfg', 'getrandom_backend=\"wasm_js\"']\n```\n\n----------------------------------------\n\nTITLE: Replacing debug settings in Cargo.toml for VSCode debugging\nDESCRIPTION: When using the VSCode debugger with launch.json settings and breakpoints, you need to replace 'debug = 0' with 'debug = true' in the root Cargo.toml file to enable proper debugging functionality.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/contributor-book/src/getting-started/configuring-your-editor.md#_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\ndebug = true\n```\n\n----------------------------------------\n\nTITLE: Setting RUSTFLAGS for getrandom Backend in WebAssembly (Shell)\nDESCRIPTION: This shell snippet demonstrates how to set the RUSTFLAGS environment variable to specify the getrandom_backend as wasm_js before building or running a Rust/WASM project. This is an alternative to setting the flag in .cargo/config.toml and may be used in CI, shell scripts, or interactive sessions. It is suitable for a single shell session or command.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/advanced/web-assembly.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nRUSTFLAGS='--cfg getrandom_backend=\"wasm_js\"'\n```\n\n----------------------------------------\n\nTITLE: Adding Backward Compatibility Feature for Model Loading\nDESCRIPTION: Shows how to enable backward compatibility feature flag for loading model records from versions prior to 0.14.0\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\nfeatures = [..., \"record-backward-compat\"]\n```\n\n----------------------------------------\n\nTITLE: Initializing a New Rust Project with Cargo - Console\nDESCRIPTION: Creates a new Rust project directory named 'guide' using Cargo. This scaffolds the standard Rust project structure including Cargo.toml and a src directory. No dependencies are required beyond a Rust toolchain with Cargo installed. No parameters are needed; this sets up the baseline for adding Burn-related source files.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/burn-book/src/basic-workflow/model.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ncargo new guide\n```\n\n----------------------------------------\n\nTITLE: Implementing Fusion Backend with Autodiff in Rust\nDESCRIPTION: Shows how to combine the Fusion and Autodiff backend decorators with WGPU for kernel fusion optimization and automatic differentiation.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/crates/burn/README.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse burn::backend::{Autodiff, Fusion, Wgpu};\nuse burn::tensor::{Distribution, Tensor};\n\nfn main() {\n    type Backend = Autodiff<Fusion<Wgpu>>;\n\n    let x: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default);\n    let y: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default).require_grad();\n\n    let tmp = x.clone() + y.clone();\n    let tmp = tmp.matmul(x);\n    let tmp = tmp.exp();\n\n    let grads = tmp.backward();\n    let y_grad = y.grad(&grads).unwrap();\n    println!(\"{y_grad}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorFlow and ONNX\nDESCRIPTION: This snippet defines the exact versions of TensorFlow, tf2onnx, ONNX, and NumPy required for a Python project. It ensures compatibility between these libraries for machine learning and numerical computing tasks.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/examples/raspberry-pi-pico/tensorflow/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ntensorflow==2.15.1\ntf2onnx==1.16.1\nonnx==1.17.0\nnumpy==2.0.1\n```\n\n----------------------------------------\n\nTITLE: VS Code Settings Configuration for Burn Project\nDESCRIPTION: JSON configuration for VS Code settings to enable debugging in the Burn project. This overrides the default debug settings in Cargo.toml to enable debugging during development.\nSOURCE: https://github.com/tracel-ai/burn/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"rust-analyzer.runnables.extraEnv\": {\n      \"CARGO_PROFILE_DEV_DEBUG\": true\n   }\n}\n```"
  }
]
[
  {
    "owner": "open-telemetry",
    "repo": "opentelemetry-collector-contrib",
    "content": "TITLE: Configuring Filter Processor with Multiple Conditions in YAML\nDESCRIPTION: A comprehensive example showing how to configure the filter processor to drop spans, span events, metrics, datapoints, and logs based on multiple conditions using OTTL expressions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter/ottl:\n    error_mode: ignore\n    traces:\n      span:\n        - 'attributes[\"container.name\"] == \"app_container_1\"'\n        - 'resource.attributes[\"host.name\"] == \"localhost\"'\n        - 'name == \"app_3\"'\n      spanevent:\n        - 'attributes[\"grpc\"] == true'\n        - 'IsMatch(name, \".*grpc.*\")'\n    metrics:\n      metric:\n          - 'name == \"my.metric\" and resource.attributes[\"my_label\"] == \"abc123\"'\n          - 'type == METRIC_DATA_TYPE_HISTOGRAM'\n      datapoint:\n          - 'metric.type == METRIC_DATA_TYPE_SUMMARY'\n          - 'resource.attributes[\"service.name\"] == \"my_service_name\"'\n    logs:\n      log_record:\n        - 'IsMatch(body, \".*password.*\")'\n        - 'severity_number < SEVERITY_NUMBER_WARN'\n```\n\n----------------------------------------\n\nTITLE: Converting Exponential Histogram to Explicit Histogram in OpenTelemetry\nDESCRIPTION: This function converts an ExponentialHistogram to an Explicit Histogram. It requires a distribution algorithm ('upper', 'midpoint', 'uniform', or 'random') and an array of ExplicitBounds. The function may result in loss of precision and does not support negative bucket counts.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nconvert_exponential_histogram_to_histogram(\"random\", [0.0, 10.0, 100.0, 1000.0, 10000.0])\n```\n\n----------------------------------------\n\nTITLE: JSON Parser Configuration with Embedded Operations\nDESCRIPTION: Example YAML configuration showing how to set up a JSON parser with embedded timestamp and severity parsers. The example demonstrates both a standalone configuration and one with embedded operations that parse timestamp and severity from specific JSON fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/parsers.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Standalone json parser\n- type: json_parser\n\n# Regex parser with embedded timestamp and severity parsers\n- type: json_parser\n  timestamp:\n    parse_from: attributes.ts\n    layout_type: strptime\n    layout: '%Y-%m-%d'\n  severity:\n    parse_from: attributes.sev\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Query Receiver in YAML\nDESCRIPTION: Example configuration showing how to set up SQL query receiver with both logs and metrics queries. Demonstrates connection configuration, logs query with tracking, and metrics query with grouping.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlqueryreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlquery:\n    driver: postgres\n    datasource: \"host=localhost port=5432 user=postgres password=s3cr3t sslmode=disable\"\n    queries:\n      - sql: \"select * from my_logs where log_id > $$1\"\n        tracking_start_value: \"10000\"\n        tracking_column: log_id\n        logs:\n          - body_column: log_body\n      - sql: \"select count(*) as count, genre from movie group by genre\"\n        metrics:\n          - metric_name: movie.genres\n            value_column: \"count\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Envoy for Client Authentication with mTLS\nDESCRIPTION: YAML configuration for Envoy proxy that implements client certificate authentication. It requires client certificates and validates Subject Alternative Names with tenant and group identifiers for granular access control.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntyped_config:\n    \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\n    require_client_certificate: true\n    common_tls_context:\n    # h2 If the listener is going to support both HTTP/2 and HTTP/1.1.\n    alpn_protocols: \"h2\"\n    tls_certificates:\n    - certificate_chain: \n        filename: \"/etc/envoy.crt\"\n        private_key: \n        filename: \"/etc/envoy.key\"\n    validation_context:\n        match_typed_subject_alt_names:\n        - san_type: URI\n        matcher:\n            Match tenant by two level info: group id and tenant-id.\n            exact: \"aprn:trace-client:certmgr:::group-x:/ig/5003178/uv/tenant-a\"\n        trusted_ca:\n        filename: \"/etc/ca.crt\"\n```\n\n----------------------------------------\n\nTITLE: Comprehensive k8sattributes Processor Configuration\nDESCRIPTION: A detailed YAML configuration example for the k8sattributes processor, including authentication, filtering, metadata extraction, and pod association rules. This configuration demonstrates various options available for customizing the processor's behavior.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nk8sattributes:\nk8sattributes/2:\n  auth_type: \"serviceAccount\"\n  passthrough: false\n  filter:\n    # only retrieve pods running on the same node as the collector\n    node_from_env_var: KUBE_NODE_NAME\n  extract:\n    # The attributes provided in 'metadata' will be added to associated resources\n    metadata:\n      - k8s.pod.name\n      - k8s.pod.uid\n      - k8s.deployment.name\n      - k8s.namespace.name\n      - k8s.node.name\n      - k8s.pod.start_time\n    labels:\n     # This label extraction rule takes the value 'app.kubernetes.io/component' label and maps it to the 'app.label.component' attribute which will be added to the associated resources\n     - tag_name: app.label.component\n       key: app.kubernetes.io/component\n       from: pod\n  pod_association:\n    - sources:\n        # This rule associates all resources containing the 'k8s.pod.ip' attribute with the matching pods. If this attribute is not present in the resource, this rule will not be able to find the matching pod.\n        - from: resource_attribute\n          name: k8s.pod.ip\n    - sources:\n        # This rule associates all resources containing the 'k8s.pod.uid' attribute with the matching pods. If this attribute is not present in the resource, this rule will not be able to find the matching pod.\n        - from: resource_attribute\n          name: k8s.pod.uid\n    - sources:\n        # This rule will use the IP from the incoming connection from which the resource is received, and find the matching pod, based on the 'pod.status.podIP' of the observed pods\n        - from: connection\n```\n\n----------------------------------------\n\nTITLE: Configuring Include/Exclude Attribute Filtering in YAML\nDESCRIPTION: YAML configuration schema for setting up include/exclude filtering rules in the attribute processor. Defines matching criteria for services, spans, logs, metrics, and their attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nattributes:\n    # include and/or exclude can be specified. However, the include properties\n    # are always checked before the exclude properties.\n    {include, exclude}:\n      # At least one of services, span_names or attributes must be specified.\n      # It is supported to have more than one specified, but all of the specified\n      # conditions must evaluate to true for a match to occur.\n\n      # match_type controls how items in \"services\", \"span_names\", and \"attributes\"\n      # arrays are interpreted. Possible values are \"regexp\" or \"strict\".\n      # This is a required field.\n      match_type: {strict, regexp}\n\n      # regexp is an optional configuration section for match_type regexp.\n      regexp:\n        # < see \"Match Configuration\" below >\n\n      # services specify an array of items to match the service name against.\n      # A match occurs if the span service name matches at least one of the items.\n      # This is an optional field.\n      services: [<item1>, ..., <itemN>]\n\n      # resources specifies a list of resources to match against.\n      # A match occurs if the input data resources matches at least one of the items.\n      # This is an optional field.\n      resources:\n          # Key specifies the resource to match against.\n        - key: <key>\n          # Value specifies the exact value to match against.\n          # If not specified, a match occurs if the key is present in the resources.\n          value: {value}\n\n      # libraries specify a list of items to match the implementation library against.\n      # A match occurs if the input data implementation library matches at least one of the items.\n      # This is an optional field.\n      libraries: [<item1>, ..., <itemN>]\n          # Name specifies the library to match against.\n        - name: <name>\n          # Version specifies the exact version to match against.\n          # This is an optional field.\n          # If the field is not set, any version will match.\n          # If the field is set to an empty string, only an\n          # empty string version will match.\n          version: {version}\n\n      # The span name must match at least one of the items.\n      # This is an optional field.\n      span_names: [<item1>, ..., <itemN>]\n\n      # The span kind must match at least one of the items.\n      # This is an optional field.\n      span_kinds: [<item1>, ..., <itemN>]\n\n      # The log body must match at least one of the items.\n      # Currently only string body types are supported.\n      # This is an optional field.\n      log_bodies: [<item1>, ..., <itemN>]\n\n      # The log severity text must match at least one of the items.\n      # This is an optional field.\n      log_severity_texts: [<item1>, ..., <itemN>]\n\n      # The log severity number defines how to match against a log record's\n      # SeverityNumber, if defined.\n      # This is an optional field.\n      log_severity_number:\n        # Min is the lowest severity that may be matched.\n        # e.g. if this is plog.SeverityNumberInfo, \n        # INFO, WARN, ERROR, and FATAL logs will match.\n        min: <int>\n        # MatchUndefined controls whether logs with \"undefined\" severity matches.\n        # If this is true, entries with undefined severity will match.\n        match_undefined: <bool>\n\n      # The metric name must match at least one of the items.\n      # This is an optional field.\n      metric_names: [<item1>, ..., <itemN>]\n\n      # Attributes specifies the list of attributes to match against.\n      # All of these attributes must match for a match to occur.\n      # This is an optional field.\n      attributes:\n          # Key specifies the attribute to match against.\n        - key: <key>\n          # Value specifies the value to match against.\n          # If not specified, a match occurs if the key is present in the attributes.\n          value: {value}\n```\n\n----------------------------------------\n\nTITLE: Practical Tail Sampling Configuration Example with Multiple Rules\nDESCRIPTION: A comprehensive YAML configuration example for the tail sampling processor that implements seven specific rules. The configuration demonstrates how to handle services not ready for tail sampling, sample probes at different rates, handle noisy endpoints, apply service-specific sampling, and implement force sampling and blocking rules.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntail_sampling:\n  decision_wait: 10s\n  num_traces: 100\n  expected_new_traces_per_sec: 10\n  policies: [\n      {\n        # Rule 1: use always_sample policy for services that don't belong to team_a and are not ready to use tail sampling\n        name: backwards-compatibility-policy,\n        type: and,\n        and:\n          {\n            and_sub_policy:\n              [\n                {\n                  name: services-using-tail_sampling-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    {\n                      key: service.name,\n                      values:\n                        [\n                          list,\n                          of,\n                          services,\n                          using,\n                          tail_sampling,\n                        ],\n                      invert_match: true,\n                    },\n                },\n                { name: sample-all-policy, type: always_sample },\n              ],\n          },\n      },\n      # BEGIN: policies for team_a\n      {\n        # Rule 2: low sampling for readiness/liveness probes\n        name: team_a-probe,\n        type: and,\n        and:\n          {\n            and_sub_policy:\n              [\n                {\n                  # filter by service name\n                  name: service-name-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    {\n                      key: service.name,\n                      values: [service-1, service-2, service-3],\n                    },\n                },\n                {\n                  # filter by route\n                  name: route-live-ready-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    {\n                      key: http.route,\n                      values: [/live, /ready],\n                      enabled_regex_matching: true,\n                    },\n                },\n                {\n                  # apply probabilistic sampling\n                  name: probabilistic-policy,\n                  type: probabilistic,\n                  probabilistic: { sampling_percentage: 0.1 },\n                },\n              ],\n          },\n      },\n      {\n        # Rule 3: low sampling for a noisy endpoint\n        name: team_a-noisy-endpoint-1,\n        type: and,\n        and:\n          {\n            and_sub_policy:\n              [\n                {\n                  name: service-name-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    { key: service.name, values: [service-1] },\n                },\n                {\n                  # filter by route\n                  name: route-name-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    {\n                      key: http.route,\n                      values: [/v1/name/.+],\n                      enabled_regex_matching: true,\n                    },\n                },\n                {\n                  # apply probabilistic sampling\n                  name: probabilistic-policy,\n                  type: probabilistic,\n                  probabilistic: { sampling_percentage: 1 },\n                },\n              ],\n          },\n      },\n      {\n        # Rule 4: high sampling for other endpoints\n        name: team_a-service-1,\n        type: and,\n        and:\n          {\n            and_sub_policy:\n              [\n                {\n                  name: service-name-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    { key: service.name, values: [service-1] },\n                },\n                {\n                  # invert match - apply to all routes except the ones specified\n                  name: route-name-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    {\n                      key: http.route,\n                      values: [/v1/name/.+],\n                      enabled_regex_matching: true,\n                      invert_match: true,\n                    },\n                },\n                {\n                  # apply probabilistic sampling\n                  name: probabilistic-policy,\n                  type: probabilistic,\n                  probabilistic: { sampling_percentage: 100 },\n                },\n              ],\n          },\n      },\n      {\n        # Rule 5: always sample if there is an error\n        name: team_a-status-policy,\n        type: and,\n        and:\n          {\n            and_sub_policy:\n              [\n                {\n                  name: service-name-policy,\n                  type: string_attribute,\n                  string_attribute:\n                    {\n                      key: service.name,\n                      values:\n                        [\n                          list,\n                          of,\n                          services,\n                          using,\n                          tail_sampling,\n                        ],\n                    },\n                },\n                {\n                  name: trace-status-policy,\n                  type: status_code,\n                  status_code: { status_codes: [ERROR] },\n                },\n              ],\n          },\n      },\n      {\n        # Rule 6:\n        # always sample if the force_sample attribute is set to true\n        name: team_a-force-sample,\n        type: boolean_attribute,\n        boolean_attribute: { key: app.force_sample, value: true },\n      },\n    {\n      # Rule 7:\n      # never sample if the do_not_sample attribute is set to true\n      name: team_a-do-not-sample,\n      type: boolean_attribute,\n      boolean_attribute: { key: app.do_not_sample, value: true, invert_match: true },\n    },\n      # END: policies for team_a\n    ]\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Transform Processor Configuration Example\nDESCRIPTION: Complete example showing transform processor configuration for traces, metrics, and logs with various OTTL transformations and conditions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  trace_statements:\n    - keep_keys(span.attributes, [\"service.name\", \"service.namespace\", \"cloud.region\", \"process.command_line\"])\n    - replace_pattern(span.attributes[\"process.command_line\"], \"password\\\\=[^\\\\s]*(\\\\s?)\", \"password=***\")\n    - limit(span.attributes, 100, [])\n    - truncate_all(span.attributes, 4096)\n  metric_statements:\n    - keep_keys(resource.attributes, [\"host.name\"])\n    - truncate_all(resource.attributes, 4096)\n    - set(metric.description, \"Sum\") where metric.type == \"Sum\"\n    - convert_sum_to_gauge() where metric.name == \"system.processes.count\"\n    - convert_gauge_to_sum(\"cumulative\", false) where metric.name == \"prometheus_metric\"\n  log_statements:\n    - set(log.severity_text, \"FAIL\") where log.body == \"request failed\"\n    - replace_all_matches(log.attributes, \"/user/*/list/*\", \"/user/{userId}/list/{listId}\")\n    - replace_all_patterns(log.attributes, \"value\", \"/account/\\\\d{4}\", \"/account/{accountId}\")\n    - set(log.body, log.attributes[\"http.route\"])\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Operator Target Allocator Configuration\nDESCRIPTION: Configuration for integrating with OpenTelemetry Operator's Target Allocator to dynamically fetch scrape targets.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  prometheus:\n    target_allocator:\n      endpoint: http://my-targetallocator-service\n      interval: 30s\n      collector_id: collector-1\n```\n\n----------------------------------------\n\nTITLE: Extracting Severity from Unstructured Logs in OpenTelemetry Collector\nDESCRIPTION: This example demonstrates how to extract and set the severity of an unstructured log based on patterns in the log body. It uses IsMatch function to identify severity levels and sets the corresponding severity number.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  log_statements:\n    - set(log.severity_number, SEVERITY_NUMBER_INFO) where IsString(log.body) and IsMatch(log.body, \"\\\\sINFO\\\\s\")\n    - set(log.severity_number, SEVERITY_NUMBER_WARN) where IsString(log.body) and IsMatch(log.body, \"\\\\sWARN\\\\s\")\n    - set(log.severity_number, SEVERITY_NUMBER_ERROR) where IsString(log.body) and IsMatch(log.body, \"\\\\sERROR\\\\s\")\n```\n\n----------------------------------------\n\nTITLE: Basic Prometheus Receiver Configuration in YAML\nDESCRIPTION: Example configuration for the Prometheus receiver in OpenTelemetry Collector showing how to define scrape targets and Kubernetes service discovery.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    prometheus:\n      config:\n        scrape_configs:\n          - job_name: 'otel-collector'\n            scrape_interval: 5s\n            static_configs:\n              - targets: ['0.0.0.0:8888']\n          - job_name: k8s\n            kubernetes_sd_configs:\n            - role: pod\n            relabel_configs:\n            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n              regex: \"true\"\n              action: keep\n            metric_relabel_configs:\n            - source_labels: [__name__]\n              regex: \"(request_duration_seconds.*|response_duration_seconds.*)\"\n              action: keep\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Cluster Receiver with Custom Node Conditions\nDESCRIPTION: Example YAML configuration for the Kubernetes Cluster receiver with custom node conditions to report. The receiver will emit metrics for Ready and MemoryPressure conditions with values of 1 (True), 0 (False), or -1 (Unknown).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nk8s_cluster:\n  node_conditions_to_report:\n    - Ready\n    - MemoryPressure\n...\n```\n\n----------------------------------------\n\nTITLE: Minimal OpenTelemetry Collector Configuration for GCP Export\nDESCRIPTION: YAML configuration for OpenTelemetry Collector that receives OTLP data and exports to Google Cloud Platform. It includes recommended components like memory_limiter and batch processors to ensure stability and performance.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlecloudexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n      http:\nexporters:\n  googlecloud:\n    log:\n      default_log_name: opentelemetry.io/collector-exported-log\nprocessors:\n  memory_limiter:\n    check_interval: 1s\n    limit_percentage: 65\n    spike_limit_percentage: 20\n  batch:\n  resourcedetection:\n    detectors: [gcp]\n    timeout: 10s\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [googlecloud]\n    metrics:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [googlecloud]\n    logs:\n      receivers: [otlp]\n      processors: [memory_limiter, batch]\n      exporters: [googlecloud]\n```\n\n----------------------------------------\n\nTITLE: Advanced Prometheus Receiver Configuration with Additional Options\nDESCRIPTION: Configuration example including top-level options like trim_metric_suffixes, use_start_time_metric, and a custom start time metric regex pattern.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    prometheus:\n      trim_metric_suffixes: true\n      use_start_time_metric: true\n      start_time_metric_regex: foo_bar_.*\n      config:\n        scrape_configs:\n          - job_name: 'otel-collector'\n            scrape_interval: 5s\n            static_configs:\n              - targets: ['0.0.0.0:8888']\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector\nDESCRIPTION: YAML configuration for setting up an OpenTelemetry Collector to receive telemetry data, including receivers, processors, exporters, and service pipelines.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/telemetrygen/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n\nprocessors:\n  batch:\n\nexporters:\n  debug:\n    verbosity: detailed\n\nservice:\n  pipelines:\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug]\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug]\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Creating ConfigMap for OpenTelemetry Collector\nDESCRIPTION: Kubernetes ConfigMap definition containing the OpenTelemetry Collector configuration with k8sobjects receiver setup.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sobjectsreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\ndata:\n  config.yaml: |\n    receivers:\n      k8sobjects:\n        objects:\n          - name: pods\n            mode: pull\n          - name: events\n            mode: watch\n    exporters:\n      otlp:\n        endpoint: <OTLP_ENDPOINT>\n        tls:\n          insecure: true\n\n    service:\n      pipelines:\n        logs:\n          receivers: [k8sobjects]\n          exporters: [otlp]\nEOF\n```\n\n----------------------------------------\n\nTITLE: Cluster-scoped RBAC Configuration for k8sattributes Processor\nDESCRIPTION: YAML configuration for setting up cluster-scoped RBAC permissions required by the k8sattributes processor. This includes creating a ServiceAccount, ClusterRole, and ClusterRoleBinding to grant necessary permissions for accessing Kubernetes resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: collector\n  namespace: <OTEL_COL_NAMESPACE>\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otel-collector\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"namespaces\", \"nodes\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otel-collector\nsubjects:\n- kind: ServiceAccount\n  name: collector\n  namespace: <OTEL_COL_NAMESPACE>\nroleRef:\n  kind: ClusterRole\n  name: otel-collector\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Querying Trace Removal Age Metric in Prometheus\nDESCRIPTION: This Prometheus query tracks how long traces remain in the buffer before being removed. It's useful for preemptively preventing dropped traces by comparing the results to the 'decision_wait' configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: prometheus\nCODE:\n```\notelcol_processor_tail_sampling_sampling_trace_removal_age\n```\n\n----------------------------------------\n\nTITLE: Configuring Tail Sampling Processor in OpenTelemetry\nDESCRIPTION: Complete configuration example showing all available sampling policies including always_sample, latency, numeric_attribute, probabilistic, status_code, string_attribute, rate_limiting, span_count, trace_state, boolean_attribute, ottl_condition, and composite policies. Includes settings for decision wait time, trace cache sizes, and expected trace rates.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  tail_sampling:\n    decision_wait: 10s\n    num_traces: 100\n    expected_new_traces_per_sec: 10\n    decision_cache:\n      sampled_cache_size: 100_000\n      non_sampled_cache_size: 100_000\n    policies:\n      [\n          {\n            name: test-policy-1,\n            type: always_sample\n          },\n          {\n            name: test-policy-2,\n            type: latency,\n            latency: {threshold_ms: 5000, upper_threshold_ms: 10000}\n          },\n          {\n            name: test-policy-3,\n            type: numeric_attribute,\n            numeric_attribute: {key: key1, min_value: 50, max_value: 100}\n          },\n          {\n            name: test-policy-4,\n            type: probabilistic,\n            probabilistic: {sampling_percentage: 10}\n          },\n          {\n            name: test-policy-5,\n            type: status_code,\n            status_code: {status_codes: [ERROR, UNSET]}\n          },\n          {\n            name: test-policy-6,\n            type: string_attribute,\n            string_attribute: {key: key2, values: [value1, value2]}\n          },\n          {\n            name: test-policy-7,\n            type: string_attribute,\n            string_attribute: {key: key2, values: [value1, val*], enabled_regex_matching: true, cache_max_size: 10}\n          },\n          {\n            name: test-policy-8,\n            type: rate_limiting,\n            rate_limiting: {spans_per_second: 35}\n         },\n         {\n            name: test-policy-9,\n            type: string_attribute,\n            string_attribute: {key: url.path, values: [\\/health, \\/metrics], enabled_regex_matching: true, invert_match: true}\n         },\n         {\n            name: test-policy-10,\n            type: span_count,\n            span_count: {min_spans: 2, max_spans: 20}\n         },\n         {\n             name: test-policy-11,\n             type: trace_state,\n             trace_state: { key: key3, values: [value1, value2] }\n         },\n         {\n              name: test-policy-12,\n              type: boolean_attribute,\n              boolean_attribute: {key: key4, value: true}\n         },\n         {\n              name: test-policy-13,\n              type: ottl_condition,\n              ottl_condition: {\n                   error_mode: ignore,\n                   span: [\n                        \"attributes[\\\"test_attr_key_1\\\"] == \\\"test_attr_val_1\\\"\",\n                        \"attributes[\\\"test_attr_key_2\\\"] != \\\"test_attr_val_1\\\"\",\n                   ],\n                   spanevent: [\n                        \"name != \\\"test_span_event_name\\\"\",\n                        \"attributes[\\\"test_event_attr_key_2\\\"] != \\\"test_event_attr_val_1\\\"\",\n                   ]\n              }\n         },\n         {\n            name: and-policy-1,\n            type: and,\n            and: {\n              and_sub_policy: \n              [\n                {\n                  name: test-and-policy-1,\n                  type: numeric_attribute,\n                  numeric_attribute: { key: key1, min_value: 50, max_value: 100 }\n                },\n                {\n                    name: test-and-policy-2,\n                    type: string_attribute,\n                    string_attribute: { key: key2, values: [ value1, value2 ] }\n                },\n              ]\n            }\n         },\n         {\n            name: composite-policy-1,\n            type: composite,\n            composite:\n              {\n                max_total_spans_per_second: 1000,\n                policy_order: [test-composite-policy-1, test-composite-policy-2, test-composite-policy-3],\n```\n\n----------------------------------------\n\nTITLE: Configuring Receiver Creator with Multiple Observers and Receivers in YAML\nDESCRIPTION: This snippet demonstrates a full configuration of the Receiver Creator component, including Kubernetes, host, and Kafka topic observers, along with various receiver types such as Prometheus, Redis, SQL Server, and Kubelet stats. It also includes resource attribute configurations and service pipeline definitions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  k8s_observer:\n    observe_nodes: true\n    observe_services: true\n    observe_ingresses: true\n  host_observer:\n  kafkatopics_observer:\n    brokers: [\"1.2.3.4:9093\"]\n    protocol_version: 3.9.0\n    topic_regex: \"^foo_topic[0-9]$\"\n    topics_sync_interval: 5s\n\nreceivers:\n  receiver_creator/1:\n    watch_observers: [k8s_observer]\n    receivers:\n      prometheus_simple:\n        rule: type == \"pod\" && annotations[\"prometheus.io/scrape\"] == \"true\"\n        config:\n          metrics_path: '`\"prometheus.io/path\" in annotations ? annotations[\"prometheus.io/path\"] : \"/metrics\"`'\n          endpoint: '`endpoint`:`\"prometheus.io/port\" in annotations ? annotations[\"prometheus.io/port\"] : 9090`'\n        resource_attributes:\n          an.attribute: a.value\n          app.version: '`labels[\"app_version\"]`'\n\n      redis/1:\n        rule: type == \"port\" && port == 6379\n        config:\n          password: secret\n          collection_interval: '`pod.annotations[\"collection_interval\"]`'\n\n      redis/2:\n        rule: type == \"port\" && port == 6379\n\n      sqlserver:\n        rule: type == \"port\" && pod.name matches \"(?i)mssql\"\n        config:\n          server: '`host`'\n          port: '`port`'\n          username: sa\n          password: password\n\n    resource_attributes:\n      pod:\n        service.name: '`labels[\"service_name\"]`'\n        app: '`labels[\"app\"]`'\n      port:\n        service.name: '`pod.labels[\"service_name\"]`'\n        app: '`pod.labels[\"app\"]`'\n  receiver_creator/2:\n    watch_observers: [host_observer]\n    receivers:\n      redis/on_host:\n        rule: type == \"port\" && port == 6379 && is_ipv6 == true\n        resource_attributes:\n          service.name: redis_on_host\n  receiver_creator/3:\n    watch_observers: [k8s_observer]\n    receivers:\n      kubeletstats:\n        rule: type == \"k8s.node\"\n        config:\n          auth_type: serviceAccount\n          collection_interval: 10s\n          endpoint: '`endpoint`:`kubelet_endpoint_port`'\n          extra_metadata_labels:\n            - container.id\n          metric_groups:\n            - container\n            - pod\n            - node\n      httpcheck:\n        rule: type == \"k8s.service\" && annotations[\"prometheus.io/probe\"] == \"true\"\n        config:\n          targets:\n          - endpoint: 'http://`endpoint`:`\"prometheus.io/port\" in annotations ? annotations[\"prometheus.io/port\"] : 9090``\"prometheus.io/path\" in annotations ? annotations[\"prometheus.io/path\"] : \"/health\"`'\n            method: GET\n          collection_interval: 10s\n  receiver_creator/4:\n    watch_observers: [k8s_observer]\n    receivers:\n      httpcheck:\n        rule: type == \"k8s.ingress\" && annotations[\"prometheus.io/probe\"] == \"true\"\n        config:\n          targets:\n          - endpoint: '`scheme`://`endpoint`:`port``\"prometheus.io/path\" in annotations ? annotations[\"prometheus.io/path\"] : \"/health\"`'\n            method: GET\n          collection_interval: 10s\n  receiver_creator/logs:\n    watch_observers: [ k8s_observer ]\n    receivers:\n      filelog/busybox:\n        rule: type == \"pod.container\" && container_name == \"busybox\"\n        config:\n          include:\n            - /var/log/pods/`pod.namespace`_`pod.name`_`pod.uid`/`container_name`/*.log\n          include_file_name: false\n          include_file_path: true\n          operators:\n            - id: container-parser\n              type: container\n            - type: add\n              field: attributes.log.template\n              value: busybox\n      filelog/lazybox:\n        rule: type == \"pod.container\" && container_name == \"lazybox\"\n        config:\n          include:\n            - /var/log/pods/`pod.namespace`_`pod.name`_`pod.uid`/`container_name`/*.log\n          include_file_name: false\n          include_file_path: true\n          operators:\n            - id: container-parser\n              type: container\n            - type: add\n              field: attributes.log.template\n              value: lazybox\n  receiver_creator/kafka:\n    watch_observers: [kafkatopics_observer]\n    receivers:\n      kafka:\n        rule: type == \"kafka.topics\"\n        config:\n          protocol_version: 3.9.0\n          topic: '`endpoint`'\n          encoding: text\n          brokers: [\"1.2.3.4:9093\"]\n          initial_offset: earliest\n          header_extraction:\n            extract_headers: true\n            headers: [\"index\", \"source\", \"sourcetype\", \"host\"]\n\nprocessors:\n  exampleprocessor:\n\nexporters:\n  exampleexporter:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [receiver_creator/1, receiver_creator/2, receiver_creator/3, receiver_creator/4]\n      processors: [exampleprocessor]\n      exporters: [exampleexporter]\n    logs:\n      receivers: [receiver_creator/logs, receiver_creator/kafka]\n      processors: [exampleprocessor]\n      exporters: [exampleexporter]\n  extensions: [k8s_observer, host_observer, kafkatopics_observer]\n```\n\n----------------------------------------\n\nTITLE: Local Testing Configuration\nDESCRIPTION: Configuration for testing load balancing locally with multiple backend receivers running on different ports.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp/loadbalancer:\n    protocols:\n      grpc:\n        endpoint: localhost:4317\n  otlp/backend-1:\n    protocols:\n      grpc:\n        endpoint: localhost:55690\n  otlp/backend-2:\n    protocols:\n      grpc:\n        endpoint: localhost:55700\n  otlp/backend-3:\n    protocols:\n      grpc:\n        endpoint: localhost:55710\n  otlp/backend-4:\n    protocols:\n      grpc:\n        endpoint: localhost:55720\n```\n\n----------------------------------------\n\nTITLE: Copying Metrics in OpenTelemetry\nDESCRIPTION: This function copies the current metric, adding it to the end of the metric slice. It allows optional parameters to set the new metric's name, description, and unit. It's recommended to use a Where clause to avoid matching the new metric in subsequent operations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\ncopy_metric(name=\"http.request.status_code\", unit=\"s\") where metric.name == \"http.status_code\n```\n\nLANGUAGE: plaintext\nCODE:\n```\ncopy_metric(desc=\"new desc\") where metric.description == \"old desc\"\n```\n\n----------------------------------------\n\nTITLE: Implementing PushMetrics Function in Go\nDESCRIPTION: Pseudocode implementation of the PushMetrics function that handles metric data conversion and grouping before export to Prometheus remote write format. The function maps incoming metrics to TimeSeries and manages their storage before export.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/DESIGN.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfunc PushMetrics(metricsData) {\n\n // Create a map that stores distinct TimeSeries\n map := make(map[String][]TimeSeries)\n\n for metric in metricsData:\n\t for point in metric:\n\t   // Generate signature string\n\t   sig := pointSignature(metric, point)\n\n\t   // Find corresponding TimeSeries in map\n\t   // Add to TimeSeries\n\n\t  // Sends TimeSeries to backend\n  export(map)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Exporter in YAML\nDESCRIPTION: Example YAML configuration for the Prometheus exporter, including endpoint, TLS settings, namespace, labels, and other options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  prometheus:\n    endpoint: \"1.2.3.4:1234\"\n    tls:\n      ca_file: \"/path/to/ca.pem\"\n      cert_file: \"/path/to/cert.pem\"\n      key_file: \"/path/to/key.pem\"\n    namespace: test-space\n    const_labels:\n      label1: value1\n      \"another label\": spaced value\n    send_timestamps: true\n    metric_expiration: 180m\n    enable_open_metrics: true\n    add_metric_suffixes: false\n    resource_to_telemetry_conversion:\n      enabled: true\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry File Exporter YAML Configuration\nDESCRIPTION: YAML configuration examples showing different file exporter setups including rotation settings, compression, and flush intervals.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/fileexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  file/no_rotation:\n    path: ./foo\n\n  file/rotation_with_default_settings:\n    path: ./foo\n    rotation:\n\n  file/rotation_with_custom_settings:\n    path: ./foo\n    rotation:\n      max_megabytes: 10\n      max_days: 3\n      max_backups: 3\n      localtime: true\n    format: proto\n    compression: zstd\n\n  file/flush_every_5_seconds:\n    path: ./foo\n    flush_interval: 5\n```\n\n----------------------------------------\n\nTITLE: Configuring OIDC Authenticator Extension in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up the OIDC authenticator extension for the OpenTelemetry Collector. It includes settings for the OIDC issuer, audience, and username claim, as well as how to integrate it with an OTLP receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/oidcauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  oidc:\n    issuer_url: http://localhost:8080/auth/realms/opentelemetry\n    issuer_ca_path: /etc/pki/tls/cert.pem\n    audience: account\n    username_claim: email\n\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        auth:\n          authenticator: oidc\n\nprocessors:\n\nexporters:\n  debug:\n    verbosity: detailed\n\nservice:\n  extensions: [oidc]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: []\n      exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Graph with Virtual Nodes in OpenTelemetry Collector\nDESCRIPTION: YAML configuration that sets up service graph monitoring with support for uninstrumented services identification. Includes OTLP receiver, service graph connector with dimension tracking, and Prometheus exporter configuration. Defines pipelines for both traces and metrics processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/servicegraphconnector/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nconnectors:\n  servicegraph:\n    dimensions:\n      - db.system\n      - messaging.system\n    virtual_node_peer_attributes:\n      - db.name\n      - db.system\n      - messaging.system\n      - peer.service\n    virtual_node_extra_label: true\n\nexporters:\n  prometheus/servicegraph:\n    endpoint: localhost:9090\n    namespace: servicegraph\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [servicegraph]\n    metrics/servicegraph:\n      receivers: [servicegraph]\n      exporters: [prometheus/servicegraph]\n```\n\n----------------------------------------\n\nTITLE: K8s Attributes Annotation Extraction Configuration\nDESCRIPTION: Example configuration showing how to extract pod annotation values using regex patterns and assign them to custom tags.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nannotations:\n  - tag_name: a2 # extracts value of annotation with key `annotation2` with regexp and inserts it as a tag with key `a2`\n    key: annotation2\n    regex: field=(?P<value>.+)\n    from: pod\n```\n\n----------------------------------------\n\nTITLE: Extracting Kubernetes Attributes in YAML Configuration\nDESCRIPTION: Example YAML configuration for extracting attributes from Kubernetes pod labels and annotations. This snippet demonstrates how to configure the k8sattributes processor to extract metadata from pods, namespaces, and nodes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nextract:\n  annotations:\n    - tag_name: a1 # extracts value of annotation from pods with key `annotation-one` and inserts it as a tag with key `a1`\n      key: annotation-one\n      from: pod\n    - tag_name: a2 # extracts value of annotation from namespaces with key `annotation-two` and inserts it as a tag with key `a2`\n      key: annotation-two\n      from: namespace\n    - tag_name: a3 # extracts value of annotation from nodes with key `annotation-three` and inserts it as a tag with key `a3`\n      key: annotation-three\n      from: node\n  labels:\n    - tag_name: l1 # extracts value of label from namespaces with key `label1` and inserts it as a tag with key `l1`\n      key: label1\n      from: namespace\n    - tag_name: l2 # extracts value of label from pods with key `label2` and inserts it as a tag with key `l2`\n      key: label2\n      from: pod\n    - tag_name: l3 # extracts value of label from nodes with key `label3` and inserts it as a tag with key `l3`\n      key: label3\n      from: node\n```\n\n----------------------------------------\n\nTITLE: Complete Kubernetes Deployment Configuration\nDESCRIPTION: Full example of a Kubernetes deployment with ConfigMap and Pod configuration including OpenTelemetry hints for both nginx and redis containers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-conf\ndata:\n  nginx.conf: |\n    user  nginx;\n    worker_processes  1;\n    error_log  /dev/stderr warn;\n    pid        /var/run/nginx.pid;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      include       /etc/nginx/mime.types;\n      default_type  application/octet-stream;\n\n      log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                        '$status $body_bytes_sent \"$http_referer\" '\n                        '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n      access_log  /dev/stdout main;\n      server {\n          listen 80;\n          server_name localhost;\n\n          location /nginx_status {\n              stub_status on;\n          }\n      }\n      include /etc/nginx/conf.d/*;\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n      annotations:\n        io.opentelemetry.discovery.metrics.6379/enabled: \"true\"\n        io.opentelemetry.discovery.metrics.6379/scraper: redis\n        io.opentelemetry.discovery.metrics.6379/config: |\n          collection_interval: \"20s\"\n          timeout: \"10s\"\n\n        io.opentelemetry.discovery.metrics.80/enabled: \"true\"\n        io.opentelemetry.discovery.metrics.80/scraper: nginx\n        io.opentelemetry.discovery.metrics.80/config: |\n          endpoint: \"http://`endpoint`/nginx_status\"\n          collection_interval: \"30s\"\n          timeout: \"20s\"\n\n        io.opentelemetry.discovery.logs.redis/enabled: \"true\"\n        io.opentelemetry.discovery.logs.redis/config: |\n          max_log_size: \"4MiB\"\n          operators:\n            - type: container\n              id: container-parser\n            - id: some\n              type: add\n              field: attributes.tag\n              value: logs_hints\n\n        io.opentelemetry.discovery.logs.webserver/enabled: \"true\"\n        io.opentelemetry.discovery.logs.webserver/config: |\n          max_log_size: \"3MiB\"\n    spec:\n      volumes:\n        - name: nginx-conf\n          configMap:\n            name: nginx-conf\n            items:\n              - key: nginx.conf\n                path: nginx.conf\n      containers:\n        - name: webserver\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n              name: webserver\n          volumeMounts:\n            - mountPath: /etc/nginx/nginx.conf\n              readOnly: true\n              subPath: nginx.conf\n              name: nginx-conf\n        - image: redis\n          imagePullPolicy: IfNotPresent\n          name: redis\n          ports:\n            - name: redis\n              containerPort: 6379\n              protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Basic Kafka Receiver Configuration in YAML\nDESCRIPTION: A minimal configuration example for the Kafka receiver that specifies only the Kafka protocol version to use.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkareceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafka:\n    protocol_version: 2.0.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Sematext Exporter in YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up the Sematext Exporter with custom timeout, sending queue, retry on failure, region, and metrics settings. It includes options for controlling payload size and specifying the Sematext Monitoring App token.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sematextexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntimeout: 500ms\nsending_queue:\n  enabled: true\n  num_consumers: 3\n  queue_size: 10\nretry_on_failure:\n  enabled: true\n  initial_interval: 1s\n  max_interval: 3s\n  max_elapsed_time: 10s\nregion: US  \nmetrics:\n  app_token: 2064e37c-4fac-45f6-831d-922d43fde759\n  payload_max_lines: 100\n  payload_max_bytes: 1000\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Disk Metrics in OpenTelemetry Collector Configuration (YAML)\nDESCRIPTION: Configuration snippet showing how to disable specific disk metrics in the OpenTelemetry Collector. This YAML configuration can be applied to any of the default disk metrics to prevent them from being emitted.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/diskscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Parsing Message Field as JSON with YAML Configuration\nDESCRIPTION: A basic configuration for json_parser operator that parses the field 'body.message' as JSON and replaces the body with the parsed content.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: json_parser\n  parse_from: body.message\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Event Hub Receiver in YAML\nDESCRIPTION: Example configuration for setting up the Azure Event Hub receiver with connection details, partition settings, consumer group, offset, and format specifications. Includes optional time format configurations for logs and metrics processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azureeventhubreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azureeventhub:\n    connection: Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName\n    partition: foo\n    group: bar\n    offset: \"1234-5566\"\n    format: \"azure\"\n    # optional\n    time_formats:\n      # All supported time format. Default is empty string array, which means using the current iso8601 parser. The format is based on https://pkg.go.dev/time#Layout. If no time-zone info, will use UTC time.\n      logs: [\"01/02/2006 15:04:05\",\"2006-01-02 15:04:05\",\"2006-01-02T15:04:05Z07:00\"]\n      metrics: [\"01/02/2006 15:04:05\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector for ECS Task and Container-Level Metrics in YAML\nDESCRIPTION: This configuration collects both task and container-level metrics from the ECS task metadata endpoint, filters and transforms them, and exports them to Amazon CloudWatch using the awsemf exporter. It includes receivers, processors, and exporters sections, as well as a service pipeline definition.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsecscontainermetricsreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    awsecscontainermetrics:\n\nprocessors:\n    filter:\n        metrics:\n            include:\n                match_type: regexp\n                metric_names:\n                    - .*memory.reserved\n                    - .*memory.utilized\n                    - .*cpu.reserved\n                    - .*cpu.utilized\n                    - .*network.rate.rx\n                    - .*network.rate.tx\n                    - .*storage.read_bytes\n                    - .*storage.write_bytes\n    metricstransform:\n        transforms:\n            - include: ecs.task.memory.utilized\n              action: update\n              new_name: MemoryUtilized\n            - include: ecs.task.memory.reserved\n              action: update\n              new_name: MemoryReserved\n            - include: ecs.task.cpu.utilized\n              action: update\n              new_name: CpuUtilized\n            - include: ecs.task.cpu.reserved\n              action: update\n              new_name: CpuReserved\n            - include: ecs.task.network.rate.rx\n              action: update\n              new_name: NetworkRxBytes\n            - include: ecs.task.network.rate.tx\n              action: update\n              new_name: NetworkTxBytes\n            - include: ecs.task.storage.read_bytes\n              action: update\n              new_name: StorageReadBytes\n            - include: ecs.task.storage.write_bytes\n              action: update\n              new_name: StorageWriteBytes\n    resource:\n        attributes:\n            - key: ClusterName\n              from_attribute: aws.ecs.cluster.name\n              action: insert\n            - key: aws.ecs.cluster.name\n              action: delete\n            - key: ServiceName\n              from_attribute: aws.ecs.service.name\n              action: insert\n            - key: aws.ecs.service.name\n              action: delete\n            - key: TaskId\n              from_attribute: aws.ecs.task.id\n              action: insert\n            - key: aws.ecs.task.id\n              action: delete\n            - key: TaskDefinitionFamily\n              from_attribute: aws.ecs.task.family\n              action: insert\n            - key: aws.ecs.task.family\n              action: delete\n            - key: ContainerName\n              from_attribute: container.name\n              action: insert\n            - key: container.name\n              action: delete                  \nexporters:\n    awsemf:\n        namespace: ECS/ContainerInsights\n        log_group_name:  '/aws/ecs/containerinsights/{ClusterName}/performance'\n        log_stream_name: '{TaskId}'\n        resource_to_telemetry_conversion:\n            enabled: true\n        dimension_rollup_option: NoDimensionRollup\n        metric_declarations:\n            - dimensions: [[ClusterName], [ClusterName, TaskDefinitionFamily]]\n              metric_name_selectors: \n                - MemoryUtilized \n                - MemoryReserved \n                - CpuUtilized\n                - CpuReserved\n                - NetworkRxBytes\n                - NetworkTxBytes\n                - StorageReadBytes\n                - StorageWriteBytes\n            - dimensions: [[ClusterName], [ClusterName, TaskDefinitionFamily, ContainerName]]\n              metric_name_selectors: [container.*]\n     \nservice:\n    pipelines:\n        metrics:\n            receivers: [awsecscontainermetrics]\n            processors: [filter, metricstransform, resource]\n            exporters: [awsemf]\n```\n\n----------------------------------------\n\nTITLE: Configuring Alertmanager Exporter in YAML\nDESCRIPTION: Example configuration for the Alertmanager exporter showing all available settings including endpoint configuration, severity levels, TLS settings, queue management, and retry policies. Demonstrates both default and custom instance configurations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/alertmanagerexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  alertmanager:\n  alertmanager/2:\n    endpoint: \"https://a.new.alertmanager.target:9093\"\n    severity: \"debug\"\n    severity_attribute: \"foo\"\n    api_version: \"v2\"\n    event_labels: [\"foo\", \"bar\"]\n    tls:\n      cert_file: /var/lib/mycert.pem\n      key_file: /var/lib/key.pem\n    timeout: 10s\n    sending_queue:\n      enabled: true\n      num_consumers: 2\n      queue_size: 10\n    retry_on_failure:\n      enabled: true\n      initial_interval: 10s\n      max_interval: 60s\n      max_elapsed_time: 10m\n    generator_url: \"opentelemetry-collector\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Metrics Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic configuration for the Host Metrics receiver specifying collection interval, initial delay, root path, and scrapers to use.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhostmetrics:\n  collection_interval: <duration> # default = 1m\n  initial_delay: <duration> # default = 1s\n  root_path: <string>\n  scrapers:\n    <scraper1>:\n    <scraper2>:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Filtering Specific Metric and Value in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop specific datapoints based on metric name and value conditions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    error_mode: ignore\n    metrics:\n      datapoint:\n        - metric.name == \"k8s.pod.phase\" and value_int == 4\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Transform Processor Structure in YAML\nDESCRIPTION: Example of the advanced configuration structure for the transform processor, showing how to organize transform statements by signal type with error handling and conditions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  <trace|metric|log>_statements:\n    - context: string\n      error_mode: propagate\n      conditions: \n        - string\n        - string\n      statements:\n        - string\n        - string\n        - string\n    - context: string\n      error_mode: silent\n      statements:\n        - string\n        - string\n        - string\n```\n\n----------------------------------------\n\nTITLE: Creating Role for Namespace-Scoped OpenTelemetry Collector in Kubernetes\nDESCRIPTION: This YAML configuration defines a Role with permissions to access various Kubernetes resources within a specific namespace. It's an alternative to the ClusterRole for scenarios where cluster-wide access is not required or allowed.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\n  namespace: default\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n      - pods\n      - pods/status\n      - replicationcontrollers\n      - replicationcontrollers/status\n      - services\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - apps\n    resources:\n      - daemonsets\n      - deployments\n      - replicasets\n      - statefulsets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - daemonsets\n      - deployments\n      - replicasets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - batch\n    resources:\n      - jobs\n      - cronjobs\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - autoscaling\n    resources:\n      - horizontalpodautoscalers\n    verbs:\n      - get\n      - list\n      - watch\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Redaction Processor in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up the Redaction processor. It includes options for allowing/blocking keys, ignoring specific attributes, blocking key patterns and values, and configuring hash functions and summary verbosity.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/redactionprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  redaction:\n    allow_all_keys: false\n    allowed_keys:\n      - description\n      - group\n      - id\n      - name\n    ignored_keys:\n      - safe_attribute\n    blocked_key_patterns:\n      - \".*token.*\"\n      - \".*api_key.*\"\n    blocked_values:\n      - \"4[0-9]{12}(?:[0-9]{3})?\"\n      - \"(5[1-5][0-9]{14})\"\n    allowed_values:\n      - \".+@mycompany.com\"\n    hash_function: md5\n    summary: debug\n```\n\n----------------------------------------\n\nTITLE: Configuring otlpjson Connector in OpenTelemetry Collector\nDESCRIPTION: Example configuration for setting up the otlpjson connector in OpenTelemetry Collector. This configuration defines a pipeline where filelog receiver collects logs, the otlpjson connector extracts OTLP data from them, and then routes the extracted telemetry to appropriate debug exporters for metrics, logs, and traces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/otlpjsonconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include:\n      - /var/log/foo.log\n\nexporters:\n  debug:\n\nconnectors:\n  otlpjson:\n\nservice:\n  pipelines:\n    logs/raw:\n      receivers: [filelog]\n      exporters: [otlpjson]\n    metrics/otlp:\n      receivers: [otlpjson]\n      exporters: [debug]\n    logs/otlp:\n      receivers: [otlpjson]\n      exporters: [debug]\n    traces/otlp:\n      receivers: [otlpjson]\n      exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Configuring System Hostname Detection\nDESCRIPTION: YAML configuration for the system detector specifying hostname sources to use only OS-provided hostname instead of FQDN.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/system:\n    detectors: [\"system\"]\n    system:\n      hostname_sources: [\"os\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Extra Metadata Labels in YAML\nDESCRIPTION: Example configuration for collecting additional metadata labels such as container IDs from the kubelet API, enabling more detailed metric labeling for better identification of resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 10s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${env:K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\n    extra_metadata_labels:\n      - container.id\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Authenticator Extension in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up the Basic Authenticator extension for both server and client authentication in the OpenTelemetry Collector. It includes settings for htpasswd file or inline credentials for server authentication, and username/password for client authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/basicauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  basicauth/server:\n    htpasswd: \n      file: .htpasswd\n      inline: |\n        ${env:BASIC_AUTH_USERNAME}:${env:BASIC_AUTH_PASSWORD}\n  \n  basicauth/client:\n    client_auth: \n      username: username\n      password: password\n\nreceivers:\n  otlp:\n    protocols:\n      http:\n        auth:\n          authenticator: basicauth/server\n\nprocessors:\n\nexporters:\n  otlp:\n    auth:\n      authenticator: basicauth/client\n\nservice:\n  extensions: [basicauth/server, basicauth/client]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: []\n      exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Configuring Ack Extension in OpenTelemetry Collector YAML\nDESCRIPTION: YAML configuration example for setting up the Ack extension in the OpenTelemetry Collector. It configures storage parameters, sets up a Splunk HEC receiver with the Ack extension, and establishes the service pipeline with the extension enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/ackextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  ack:\n    storage:\n    max_number_of_partition: 1000000\n    max_number_of_pending_acks_per_partition: 1000000\n\nreceivers:\n  splunk_hec:\n    ack_extension: ack\n\nservice:\n  extensions: [ack]\n  pipelines:\n    logs:\n      receivers: [splunk_hec]\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Service Account Authentication in YAML\nDESCRIPTION: Example configuration for using Kubernetes service account authentication with the Kubelet Stats Receiver, including the required downward API usage for accessing the node name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: K8S_NODE_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: spec.nodeName\n```\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 20s\n    auth_type: \"serviceAccount\"\n    endpoint: \"https://${env:K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\nexporters:\n  file:\n    path: \"fileexporter.txt\"\nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n      exporters: [file]\n```\n\n----------------------------------------\n\nTITLE: Complete Attributes Processor Example in YAML\nDESCRIPTION: Comprehensive example showing multiple attribute processing actions including deletion, upsert, update, insert, hash, and type conversion.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  attributes/example:\n    actions:\n      - key: db.table\n        action: delete\n      - key: redacted_span\n        value: true\n        action: upsert\n      - key: copy_key\n        from_attribute: key_original\n        action: update\n      - key: account_id\n        value: 2245\n        action: insert\n      - key: account_password\n        action: delete\n      - key: account_email\n        action: hash\n      - key: http.status_code\n        action: convert\n        converted_type: int\n```\n\n----------------------------------------\n\nTITLE: Overriding Error Mode in OpenTelemetry Collector\nDESCRIPTION: This example shows how to override the default error mode for specific statements within a transformation. It demonstrates setting different error modes for different sets of operations on logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  # default error mode applied to all context statements\n  error_mode: propagate\n  log_statements:\n    # overrides the default error mode for these statements\n    - error_mode: ignore\n      statements:\n        - merge_maps(log.cache, ParseJSON(log.body), \"upsert\") where IsMatch(log.body, \"^\\\\{\")\n        - set(log.attributes[\"attr1\"], log.cache[\"attr1\"])\n\n    # uses the default error mode\n    - statements:\n        - set(log.attributes[\"namespace\"], log.attributes[\"k8s.namespace.name\"])\n```\n\n----------------------------------------\n\nTITLE: Migrating from Routing Processor to Routing Connector in OpenTelemetry\nDESCRIPTION: Example showing how to migrate from the deprecated routing processor to the routing connector. The connector routes to pipelines instead of directly to exporters and uses OTTL for condition evaluation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/routingprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconnectors:\n  routing:\n    match_once: true\n    default_pipelines: [traces/jaeger]\n    table:\n    - context: request\n      condition: request[\"X-Tenant\"] == \"acme\"\n      pipelines: [traces/jaeger/acme]\nexporters:\n  jaeger:\n    endpoint: localhost:14250\n  jaeger/acme:\n    endpoint: localhost:24250\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [routing]\n    traces/jaeger:\n      receivers: [routing]\n      exporters: [jaeger]\n    traces/jaeger/acme:\n      receivers: [routing]\n      exporters: [jaeger/acme]\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Observer with Prometheus Receiver\nDESCRIPTION: Complete configuration example showing how to set up the ECS Observer extension with Prometheus receiver for service discovery. Includes configuration for cluster settings, filters, and relabeling rules.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  ecs_observer:\n    refresh_interval: 60s # format is https://golang.org/pkg/time/#ParseDuration\n    cluster_name: 'Cluster-1' # cluster name need manual config\n    cluster_region: 'us-west-2' # region can be configured directly or use AWS_REGION env var\n    result_file: '/etc/ecs_sd_targets.yaml' # the directory for file must already exists\n    services:\n      - name_pattern: '^retail-.*$'\n    docker_labels:\n      - port_label: 'ECS_PROMETHEUS_EXPORTER_PORT'\n    task_definitions:\n      - job_name: 'task_def_1'\n        metrics_path: '/metrics'\n        metrics_ports:\n          - 9113\n          - 9090\n        arn_pattern: '.*:task-definition/nginx:[0-9]+'\n\nreceivers:\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: \"ecs-task\"\n          file_sd_configs:\n            - files:\n                - '/etc/ecs_sd_targets.yaml'\n          relabel_configs:\n            - source_labels: [ __meta_ecs_cluster_name ]\n              action: replace\n              target_label: ClusterName\n            - source_labels: [ __meta_ecs_service_name ]\n              action: replace\n              target_label: ServiceName\n            - action: labelmap\n              regex: ^__meta_ecs_container_labels_(.+)$\n              replacement: '$$1'\n\nprocessors:\n  batch:\n\nexporters:\n  awsemf:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [ prometheus ]\n      processors: [ batch ]\n      exporters: [ awsemf ]\n  extensions: [ ecs_observer ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Metrics in CloudWatch Metrics Receiver\nDESCRIPTION: Example configuration showing how to set up the receiver to collect specific named metrics from AWS EC2 and S3 services with dimensions and aggregation settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscloudwatchmetricsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nawscloudwatchmetrics:\n  region: us-east-1\n  poll_interval: 1m\n  metrics:\n    named:\n      - namespace: \"AWS/EC2\"\n        metric_name: \"CPUUtilization\"\n        period: \"5m\"\n        aws_aggregation: \"Sum\"\n        dimensions:\n          - Name: \"InstanceId\"\n            Value: \"i-1234567890abcdef0\"\n      - namespace: \"AWS/S3\"\n        metric_name: \"BucketSizeBytes\"\n        period: \"5m\"\n        aws_aggregation: \"p99\"\n        dimensions:\n          - Name: \"BucketName\"\n            Value: \"OpenTelemetry\"\n          - Name: \"StorageType\"\n            Value: \"StandardStorage\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Failover Connector with Priority Levels in OpenTelemetry Collector\nDESCRIPTION: Example configuration for the failover connector that demonstrates how to set up multiple priority levels with different pipelines. It shows a traces pipeline with three priority levels, where the first level has two pipelines and subsequent levels have one pipeline each.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/failoverconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconnectors:\n  failover:\n    priority_levels:\n      - [traces/first, traces/also_first]\n      - [traces/second]\n      - [traces/third]\n    retry_interval: 10s\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [failover]\n    traces/first:\n      receivers: [failover]\n      exporters: [otlp/first]\n    traces/second:\n      receivers: [failover]\n      exporters: [otlp/second]\n    traces/third:\n      receivers: [failover]\n      exporters: [otlp/third]\n    traces/also_first:\n      receivers: [failover]\n      exporters: [otlp/fourth]\n```\n\n----------------------------------------\n\nTITLE: ECS Observer Filter Configuration Examples\nDESCRIPTION: Examples of different filter configurations including service name patterns, task definitions, and docker labels for discovering Prometheus targets.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\necs_observer:\n  job_name: 'ecs-sd-job'\n  services:\n    - name_pattern: ^retail-.*$\n      container_name_pattern: ^java-api-v[12]$\n    - name_pattern: game\n      metrics_path: /v3/343\n      job_name: guilty-spark\n  task_definitions:\n    - arn_pattern: '*memcached.*'\n    - arn_pattern: '^proxy-.*$'\n      metrics_ports:\n        - 9113\n        - 9090\n      metrics_path: /internal/metrics\n  docker_labels:\n    - port_label: ECS_PROMETHEUS_EXPORTER_PORT\n    - port_label: ECS_PROMETHEUS_EXPORTER_PORT_V2\n      metrics_path_label: ECS_PROMETHEUS_EXPORTER_METRICS_PATH\n```\n\n----------------------------------------\n\nTITLE: Configuring Span Name Modification in YAML\nDESCRIPTION: This snippet demonstrates how to configure the Span Processor to modify span names based on attributes. It specifies which attributes to use and how to separate them in the new name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/spanprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nspan:\n  name:\n    from_attributes: [\"db.svc\", \"operation\"]\n    separator: \"::\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Pipeline with Logzio Exporter in YAML\nDESCRIPTION: This snippet shows the configuration for a logging pipeline using the Logzio exporter. It includes file log receiver setup, batch processing, resource detection, and exporter configuration for Logzio logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/logzioexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: [ \"/private/var/log/*.log\" ] # MacOs system logs\n    include_file_name: false\n    include_file_path: true \n    operators:\n      - type: move\n        from: attributes[\"log.file.path\"]\n        to: attributes[\"log_file_path\"]\n    attributes:\n      type: <<your-logzio-type>>\nprocessors:\n  batch:\n    send_batch_size: 10000\n    timeout: 1s\n  resourcedetection/system:\n    detectors: [ \"system\" ]\n    system:\n      hostname_sources: [ \"os\" ]\nexporters:\n  logzio/logs:\n    account_token: \"LOGZIOlogsTOKEN\"\n    region: \"us\"\nservice:\n  pipelines:\n    logs:\n      receivers: [filelog]\n      processors: [ resourcedetection/system, batch ]\n      exporters: [logzio/logs]\n  telemetry:\n    logs:\n      level: \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Process Scraper in Host Metrics Receiver\nDESCRIPTION: Configuration for the process scraper with options to include/exclude specific processes and control error handling behavior for various process metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocess:\n  <include|exclude>:\n    names: [ <process name>, ... ]\n    match_type: <strict|regexp>\n  mute_process_all_errors: <true|false>\n  mute_process_name_error: <true|false>\n  mute_process_exe_error: <true|false>\n  mute_process_io_error: <true|false>\n  mute_process_user_error: <true|false>\n  mute_process_cgroup_error: <true|false>\n  scrape_process_delay: <time>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector with Load Balancing in YAML\nDESCRIPTION: This YAML configuration sets up an OpenTelemetry Collector with load balancing and multiple pipelines for traces and logs. It defines exporters, including a load balancing exporter with OTLP protocol settings and static resolver for backend hostnames.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n\nexporters:\n  debug:\n  loadbalancing:\n    protocol:\n      otlp:\n        timeout: 1s\n        tls:\n          insecure: true\n    resolver:\n      static:\n        hostnames:\n        - localhost:55690\n        - localhost:55700\n        - localhost:55710\n        - localhost:55720\n\nservice:\n  pipelines:\n    traces/loadbalancer:\n      receivers:\n        - otlp/loadbalancer\n      processors: []\n      exporters:\n        - loadbalancing\n\n    traces/backend-1:\n      receivers:\n        - otlp/backend-1\n      processors: []\n      exporters:\n        - debug\n\n    traces/backend-2:\n      receivers:\n        - otlp/backend-2\n      processors: []\n      exporters:\n        - debug\n\n    traces/backend-3:\n      receivers:\n        - otlp/backend-3\n      processors: []\n      exporters:\n        - debug\n\n    traces/backend-4:\n      receivers:\n        - otlp/backend-4\n      processors: []\n      exporters:\n        - debug\n\n    logs/loadbalancer:\n      receivers:\n        - otlp/loadbalancer\n      processors: []\n      exporters:\n        - loadbalancing\n    logs/backend-1:\n      receivers:\n        - otlp/backend-1\n      processors: []\n      exporters:\n        - debug\n    logs/backend-2:\n      receivers:\n        - otlp/backend-2\n      processors: []\n      exporters:\n        - debug\n    logs/backend-3:\n      receivers:\n        - otlp/backend-3\n      processors: []\n      exporters:\n        - debug\n    logs/backend-4:\n      receivers:\n        - otlp/backend-4\n      processors: []\n      exporters:\n        - debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Exporter with Basic Authentication in YAML\nDESCRIPTION: Example YAML configuration for the Elasticsearch exporter using basic authentication. It demonstrates setting up the exporter with an endpoint, authentication, and pipeline configuration for logs and traces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/elasticsearchexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  elasticsearch:\n    endpoint: https://elastic.example.com:9200\n    auth:\n      authenticator: basicauth\n\nextensions:\n  basicauth:\n    client_auth:\n      username: elastic\n      password: changeme\n\n······\n\nservice:\n  extensions: [basicauth]\n  pipelines:\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [elasticsearch]\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [elasticsearch]\n```\n\n----------------------------------------\n\nTITLE: Creating Database Schema for OpenTelemetry Data in DeltaStream\nDESCRIPTION: This SQL script creates tables for storing OpenTelemetry data including logs, traces, and metrics in a DeltaStream database. The tables use UUIDs as primary keys and establish relationships through foreign keys. The schema supports storing attributes, resources, and scope information for all telemetry types.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n----- Logs\n\nCREATE TABLE otel.log\n(\n        log_id                   VARCHAR (uuid),            -- generated\n        trace_id                 VARCHAR(32),\n        span_id                  VARCHAR(16),\n        time_unix_nano           TIMESTAMP,\n        observed_time_unix_nano  TIMESTAMP,\n        severity_id              TINYINT,\n        severity_text            VARCHAR(8),\n        body                     VARCHAR,\n        flags                    INT,\n        PRIMARY KEY (log_id)\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.log_attribute\n(\n        log_id       VARCHAR (uuid),\n        key          VARCHAR(256, dict),\n        string_value VARCHAR(256),\n        bool_value   BOOLEAN,\n        int_value    INT,\n        double_value DOUBLE,\n        bytes_value  BYTES,\n        PRIMARY KEY (log_id, key),\n        SHARD KEY (log_id),\n        FOREIGN KEY (log_id) REFERENCES otel.log(log_id) AS fk_log\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.log_resource_attribute\n(\n        log_id  VARCHAR (uuid),              -- generated\n        key          VARCHAR(256, dict),\n        string_value VARCHAR,\n        bool_value   BOOLEAN,\n        int_value    INT,\n        double_value DOUBLE,\n        bytes_value  BYTES,\n        PRIMARY KEY (log_id, key),\n        SHARD KEY (log_id),\n        FOREIGN KEY (log_id) REFERENCES otel.log(log_id) AS fk_log_resource\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.log_scope_attribute\n(\n        log_id     VARCHAR (uuid),              -- generated\n        scope_name   VARCHAR(64, dict),\n        scope_ver    VARCHAR(16, dict),\n        key          VARCHAR(256, dict),\n        string_value VARCHAR,\n        bool_value   BOOLEAN,\n        int_value    INT,\n        double_value DOUBLE,\n        bytes_value  BYTES,\n        PRIMARY KEY (log_id, key),\n        SHARD KEY (log_id),\n        FOREIGN KEY (log_id) REFERENCES otel.log(log_id) AS fk_log_scope\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\n----- Traces\n\nCREATE TABLE \"otel\".\"trace_span\"\n(\n    \"id\" UUID (primary_key) NOT NULL,\n    \"trace_id\" VARCHAR (32) NOT NULL,\n    \"span_id\" VARCHAR (16) NOT NULL,\n    \"parent_span_id\" VARCHAR (16),\n    \"trace_state\" VARCHAR (256),\n    \"name\" VARCHAR (256, dict) NOT NULL,\n    \"span_kind\" TINYINT (dict),\n    \"start_time_unix_nano\" TIMESTAMP NOT NULL,\n    \"end_time_unix_nano\" TIMESTAMP NOT NULL,\n    \"dropped_attributes_count\" INTEGER,\n    \"dropped_events_count\" INTEGER,\n    \"dropped_links_count\" INTEGER,\n    \"message\" VARCHAR(256),\n    \"status_code\" TINYINT (dict)\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"trace_span_attribute\"\n(\n    \"span_id\" UUID (primary_key, shard_key) NOT NULL,\n    \"key\" VARCHAR (primary_key, 256, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (span_id) references otel.trace_span(id) as fk_span\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"trace_resource_attribute\"\n(\n    span_id VARCHAR (UUID) NOT NULL,\n    \"key\" VARCHAR (256, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    PRIMARY KEY (span_id, key),\n    SHARD KEY (span_id),\n    FOREIGN KEY (span_id) references otel.trace_span(id) as fk_span_resource\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"trace_scope_attribute\"\n(\n    \"span_id\" UUID (primary_key) NOT NULL,\n    \"name\" VARCHAR (256, dict),\n    \"version\" VARCHAR (256, dict),\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (span_id),\n    FOREIGN KEY (span_id) references otel.trace_span(id) as fk_span_scope\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"trace_event_attribute\"\n(\n    \"span_id\" UUID (primary_key) NOT NULL,\n    \"event_name\" VARCHAR (128, dict) NOT NULL,\n    \"time_unix_nano\" TIMESTAMP,\n    \"key\" VARCHAR (primary_key, 128) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (span_id),\n    FOREIGN KEY (span_id) references otel.trace_span(id) as fk_span_event\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"trace_link_attribute\"\n(\n    \"link_span_id\" UUID (primary_key) NOT NULL,\n    \"trace_id\" VARCHAR (32),\n    \"span_id\" VARCHAR (16),\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" TINYINT,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (link_span_id),\n    FOREIGN KEY (link_span_id) references otel.trace_span(id) as fk_span_link\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\n------ METRICS\n\n------ GAUGE\n\nCREATE TABLE otel.metric_gauge\n(\n    gauge_id UUID (primary_key, shard_key) not null,\n    metric_name varchar(256) not null,\n    metric_description varchar (256),\n    metric_unit varchar (256)\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_gauge_datapoint\n(\n    gauge_id UUID (primary_key, shard_key) not null,\n    id UUID (primary_key) not null,\n    start_time_unix TIMESTAMP NOT NULL,\n    time_unix TIMESTAMP NOT NULL,\n    gauge_value DOUBLE,\n    flags INT,\n    FOREIGN KEY (gauge_id) references otel.metric_gauge(gauge_id) as fk_gauge_datapoint\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_gauge_datapoint_attribute\"\n(\n    \"gauge_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (gauge_id) references otel.metric_gauge(gauge_id) as fk_gauge_datapoint_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_gauge_datapoint_exemplar\n(\n    \"gauge_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    time_unix TIMESTAMP NOT NULL,\n    gauge_value DOUBLE,\n    \"trace_id\" VARCHAR (32),\n    \"span_id\" VARCHAR (16),\n    FOREIGN KEY (gauge_id) references otel.metric_gauge(gauge_id) as fk_gauge_datapoint_exemplar\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_gauge_datapoint_exemplar_attribute\n(\n    \"gauge_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (gauge_id) references otel.metric_gauge(gauge_id) as fk_gauge_datapoint_exemplar_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_gauge_resource_attribute\"\n(\n    \"gauge_id\" UUID (primary_key) NOT NULL,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (gauge_id),\n    FOREIGN KEY (gauge_id) references otel.metric_gauge(gauge_id) as fk_gauge_resource_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_gauge_scope_attribute\"\n(\n    \"gauge_id\" UUID (primary_key) NOT NULL,\n    \"name\" VARCHAR (256),\n    \"version\" VARCHAR (256),\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (gauge_id),\n    FOREIGN KEY (gauge_id) references otel.metric_gauge(gauge_id) as fk_gauge_scope_attribute\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\n------- SUM\n\nCREATE TABLE otel.metric_sum\n(\n    sum_id UUID (primary_key, shard_key) not null,\n    metric_name varchar (256) not null,\n    metric_description varchar (256),\n    metric_unit varchar (256),\n    aggregation_temporality INTEGER,\n    is_monotonic BOOLEAN\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_sum_datapoint\n(\n    sum_id UUID (primary_key, shard_key) not null,\n    id UUID (primary_key) not null,\n    start_time_unix TIMESTAMP NOT NULL,\n    time_unix TIMESTAMP NOT NULL,\n    sum_value DOUBLE,\n    flags INT,\n    FOREIGN KEY (sum_id) references otel.metric_sum(sum_id) as fk_sum_datapoint\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_sum_datapoint_attribute\"\n(\n    \"sum_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (sum_id) references otel.metric_sum(sum_id) as fk_sum_datapoint_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_sum_datapoint_exemplar\n(\n    \"sum_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    time_unix TIMESTAMP NOT NULL,\n    sum_value DOUBLE,\n    \"trace_id\" VARCHAR (32),\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCensus Receiver in YAML\nDESCRIPTION: Basic YAML configuration to enable the OpenCensus receiver in the OpenTelemetry Collector. This snippet shows the minimal required configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/opencensusreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  opencensus:\n```\n\n----------------------------------------\n\nTITLE: Complete Collector Pipeline Configuration\nDESCRIPTION: Comprehensive example showing a collector configuration with OTLP receivers, samplers, spanmetrics connector, and multiple pipeline definitions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/schemaprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\nexporter:\n  otlp:\n    endpoint: \"localhost:4317\"\nprocessors:\n  probabilistic_sampler:\n    hash_seed: 22\n    sampling_percentage: 15\n    sampling_priority: priority\nconnectors:\n  spanmetrics:\n    dimensions:\n      - name: http.method\n      - name: http.status_code\n      - name: service.name\n      - name: deployment.environment\nservice:\n  pipelines:\n    traces/red:\n      receivers: [otlp]\n      exporters: [spanmetrics]\n    traces/sampled:\n      receivers: [otlp]\n      processors: [probabilistic_sampler]\n      exporters: [otlp]\n    metrics:\n      receivers: [spanmetrics]\n      exporters: [otlp]\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Transform Configuration with Conditions\nDESCRIPTION: Example showing how to use the advanced transform configuration with conditions for metric and log statements, demonstrating how to set descriptions for Sum metrics and process log bodies based on conditions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  metric_statements:\n    - error_mode: propagate\n      conditions:\n        - metric.type == METRIC_DATA_TYPE_SUM\n      statements:\n        - set(metric.description, \"Sum\")\n\n  log_statements:\n    - conditions:\n        - IsMap(log.body) and log.body[\"object\"] != nil\n      statements:\n        - set(log.body, log.attributes[\"http.route\"])\n```\n\n----------------------------------------\n\nTITLE: Adding a Value to Resource in YAML\nDESCRIPTION: This example demonstrates how to add a value to the resource of an entry using the 'add' operator in YAML configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: resource.key2\n  value: val2\n```\n\n----------------------------------------\n\nTITLE: Toggling Datatype in YAML\nDESCRIPTION: Changes the datatype of system.cpu.usage metrics from integer to double.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n# toggle the datatype of cpu usage from int (the default) to double\ninclude: system.cpu.usage\naction: update\noperations:\n  - action: toggle_scalar_data_type\n```\n\n----------------------------------------\n\nTITLE: Configuring CollectD Receiver in OpenTelemetry Collector\nDESCRIPTION: YAML configuration examples for the CollectD receiver showing both default and custom configurations. The example demonstrates how to configure the endpoint, attributes prefix, and timeout settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/collectdreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  collectd:\n  collectd/one:\n    attributes_prefix: \"dap_\"\n    endpoint: \"localhost:12345\"\n    timeout: \"50s\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Server Receiver in YAML\nDESCRIPTION: Example YAML configuration for the SQL Server receiver, showing basic setup and direct connection options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlserver:\n    collection_interval: 10s\n  sqlserver/1:\n    collection_interval: 5s\n    username: sa\n    password: securepassword\n    server: 0.0.0.0\n    port: 1433\n```\n\n----------------------------------------\n\nTITLE: Architectural Overview of k8slogreceiver Component\nDESCRIPTION: Diagram showing the high-level architecture of the k8slogreceiver component, illustrating how it connects to Kubernetes API, Docker API, and CRI API, and how data flows between Source, Poller, and Reader components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/design.md#2025-04-10_snippet_0\n\nLANGUAGE: ascii\nCODE:\n```\n   K8s\n   API ─────┐\n            │\n Docker     │\n  API ────┐ │\n          │ │\nCri       │ │\nAPI ────┐ │ │\n        │ │ │\n        ▼ ▼ ▼\n    ┌──────────────┐         ┌───────────┐\n    │    Source    │────────▶│   Poller  │\n    └──────────────┘         └───────────┘\n          │                       │\n          │                       │ \n          ▼                       │\n    ┌───────────────┐             │\n    │  Reader       │◀────────────┘\n    └───────────────┘\n```\n\n----------------------------------------\n\nTITLE: Example Spans Structure After Compaction\nDESCRIPTION: A Go representation showing spans structure after applying compaction with the groupbyattrs processor. Spans with matching Resource and InstrumentationLibrary attributes are now grouped together.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: go\nCODE:\n```\nResource {host.name=\"localhost\"}\n  InstrumentationLibrary {name=\"MyLibrary\"}\n  Spans\n    Span {span_id=1, ...}\n    Span {span_id=3, ...}\n    Span {span_id=4, ...}\n  InstrumentationLibrary {name=\"OtherLibrary\"}\n  Spans\n    Span {span_id=2, ...}\n\nResource {host.name=\"otherhost\"}\n  InstrumentationLibrary {name=\"MyLibrary\"}\n  Spans\n    Span {span_id=5, ...}\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenTelemetry Collector Contrib in Kubernetes\nDESCRIPTION: This YAML configuration creates a Deployment for the OpenTelemetry Collector Contrib. It specifies the container image, configuration mounting, and service account to use. The collector is configured to use a ConfigMap named 'otelcontribcol' for its configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: otelcontribcol\n  template:\n    metadata:\n      labels:\n        app: otelcontribcol\n    spec:\n      serviceAccountName: otelcontribcol\n      containers:\n      - name: otelcontribcol\n        image: otel/opentelemetry-collector-contrib\n        args: [\"--config\", \"/etc/config/config.yaml\"]\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n        imagePullPolicy: IfNotPresent\n      volumes:\n        - name: config\n          configMap:\n            name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with TLS Authentication in YAML\nDESCRIPTION: Example configuration for the Kubelet Stats Receiver using TLS authentication with certificate files to connect to the kubelet API server on a Kubernetes node.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 20s\n    initial_delay: 1s\n    auth_type: \"tls\"\n    ca_file: \"/path/to/ca.crt\"\n    key_file: \"/path/to/apiserver.key\"\n    cert_file: \"/path/to/apiserver.crt\"\n    endpoint: \"https://192.168.64.1:10250\"\n    insecure_skip_verify: true\nexporters:\n  file:\n    path: \"fileexporter.txt\"\nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n      exporters: [file]\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRoleBinding for OpenTelemetry Collector in Kubernetes\nDESCRIPTION: This YAML configuration creates a ClusterRoleBinding that binds the previously created ClusterRole to a ServiceAccount named 'otelcontribcol' in the default namespace.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: otelcontribcol\nsubjects:\n- kind: ServiceAccount\n  name: otelcontribcol\n  namespace: default\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Simple Receiver with TLS\nDESCRIPTION: Example configuration showing how to set up the prometheus_simple receiver with TLS authentication, custom collection interval, and service account usage. The configuration also includes pipeline setup for exporting metrics to SignalFx.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/simpleprometheusreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    receivers:\n      prometheus_simple:\n        collection_interval: 10s\n        use_service_account: true\n        endpoint: \"172.17.0.5:9153\"\n        tls:\n          ca_file: \"/path/to/ca\"\n          cert_file: \"/path/to/cert\"\n          key_file: \"/path/to/key\"\n          insecure_skip_verify: true\n    exporters:\n      signalfx:\n        access_token: <SIGNALFX_ACCESS_TOKEN>\n        url: <SIGNALFX_INGEST_URL>\n\n    service:\n      pipelines:\n        metrics:\n          receivers: [prometheus_simple]\n          exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Combining Attributes in OpenTelemetry Collector\nDESCRIPTION: This example shows how to combine two attributes into a new attribute using the Concat function. It demonstrates string concatenation with a delimiter in the context of trace processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  trace_statements:\n    # Use Concat function to combine any number of string, separated by a delimiter.\n    - set(resource.attributes[\"test\"], Concat([resource.attributes[\"foo\"], resource.attributes[\"bar\"]], \" \"))\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Resource Detection with Custom Retry Parameters\nDESCRIPTION: YAML configuration for customizing the EC2 metadata client retry behavior, allowing adjustment of the maximum number of attempts and backoff time.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/ec2:\n    detectors: [\"ec2\"]\n    ec2:\n      max_attempts: 10\n      max_backoff: 5m\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Processing for Datadog Exporter\nDESCRIPTION: Example configuration for setting up batch processing with reduced batch sizes to handle payload size limits in Datadog exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datadogexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch:  # To be used by other exporters\n    timeout: 1s\n    # Default value for send_batch_size is 8192\n  batch/datadog:\n    send_batch_max_size: 100\n    send_batch_size: 10\n    timeout: 10s\n...\nservice:\n  pipelines:\n    metrics:\n      receivers: ...\n      processors: [batch/datadog]\n      exporters: [datadog]\n```\n\n----------------------------------------\n\nTITLE: Converting All Sum Metrics with Cumulative to Delta Processor\nDESCRIPTION: Configuration example showing how to convert all sum metrics from cumulative to delta format by specifying the metric type rather than individual metric names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/cumulativetodeltaprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: cumulativetodelta\n    cumulativetodelta:\n\n        # Convert all sum metrics\n        include:\n            metric_types:\n              - sum\n```\n\n----------------------------------------\n\nTITLE: Using String Converter for Type Conversion in OpenTelemetry\nDESCRIPTION: The String Converter transforms various value types into strings. It handles strings, byte arrays, maps, slices, and pcommon.Value types, with special formatting for each. If the value is empty or parsing fails, nil is returned.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_54\n\nLANGUAGE: go\nCODE:\n```\nString(value)\n```\n\n----------------------------------------\n\nTITLE: Configuring F5 Big-IP Receiver in YAML\nDESCRIPTION: Example YAML configuration for the F5 Big-IP receiver. It specifies the collection interval, endpoint, username, password, and TLS settings. The password is set using an environment variable for security.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/bigipreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  bigip:\n    collection_interval: 10s\n    endpoint: https://localhost:443\n    username: otelu\n    password: ${env:BIGIP_PASSWORD}\n    tls:\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Using Different Collection Frequencies for Host Metrics\nDESCRIPTION: Advanced configuration example showing how to configure multiple hostmetrics receivers with different collection intervals for different metric types.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  hostmetrics:\n    collection_interval: 30s\n    scrapers:\n      cpu:\n      memory:\n\n  hostmetrics/disk:\n    collection_interval: 1m\n    scrapers:\n      disk:\n      filesystem:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [hostmetrics, hostmetrics/disk]\n```\n\n----------------------------------------\n\nTITLE: Configuring osquery Receiver in YAML\nDESCRIPTION: Example configuration for the osquery receiver showing how to set up collection interval, extensions socket path, and specify queries to run against the osquery daemon. This configuration enables querying certificates and block devices information.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/osqueryreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  osquery:\n    collection_internal: 10s\n    extensions_socket: /var/osquery/osquery.em\n    queries:\n      - \"select * from certificates\"\n      - \"select * from block_devices\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Spanmetrics with Prometheus Remote Write in YAML\nDESCRIPTION: This YAML configuration shows how to use the spanmetrics connector with the Prometheus Remote Write exporter. It sets up OTLP receivers, configures the spanmetrics connector with a custom namespace, and defines pipelines for processing traces and metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/spanmetricsconnector/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      http:\n      grpc:\n\nexporters:\n  prometheusremotewrite:\n    endpoint: http://localhost:9090/api/v1/write\n    target_info:\n      enabled: true\n\nconnectors:\n  spanmetrics:\n    namespace: span.metrics\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [spanmetrics]\n    metrics:\n      receivers: [spanmetrics]\n      exporters: [prometheusremotewrite]\n```\n\n----------------------------------------\n\nTITLE: Collecting Top N Query Performance Statistics in SQL Server\nDESCRIPTION: This SQL query collects performance statistics for the most resource-intensive queries on a SQL Server instance. It first creates a common table expression (CTE) to gather aggregated metrics, then joins with system views to obtain the actual query text and execution plans. The script uses parameters for lookback time period and instance name filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/databaseTopQueryWithoutInstanceName.txt#2025-04-10_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nwith qstats as (\n\tSELECT TOP(@topNValue)\n\t\tREPLACE(@@SERVERNAME,'\\\\',':') AS [sql_instance],\n\t\tHOST_NAME() AS [computer_name],\n\t\tMAX(qs.plan_handle) AS query_plan_handle,\n\t\tqs.query_hash AS query_hash,\n\t\tqs.query_plan_hash AS query_plan_hash,\n\t\tSUM(qs.execution_count) AS execution_count,\n\t\tSUM(qs.total_elapsed_time) AS total_elapsed_time,\n\t\tSUM(qs.total_worker_time) AS total_worker_time,\n\t\tSUM(qs.total_logical_reads) AS total_logical_reads,\n\t\tSUM(qs.total_physical_reads) AS total_physical_reads,\n\t\tSUM(qs.total_logical_writes) AS total_logical_writes,\n\t\tSUM(qs.total_rows) AS total_rows,\n\t\tSUM(qs.total_grant_kb) as total_grant_kb\n\tFROM sys.dm_exec_query_stats AS qs\n\tWHERE qs.last_execution_time BETWEEN DATEADD(SECOND, @lookbackTime, GETDATE()) AND GETDATE() AND (@instanceName = '' OR @@SERVERNAME = @instanceName)\n\tGROUP BY\n\t\tqs.query_hash,\n\t\tqs.query_plan_hash\n)\nSELECT qs.*,\n\tSUBSTRING(st.text, (stats.statement_start_offset / 2) + 1,\n\t\t\t ((CASE statement_end_offset\n\t\t\t\t   WHEN -1 THEN DATALENGTH(st.text)\n\t\t\t\t   ELSE stats.statement_end_offset END - stats.statement_start_offset) / 2) + 1) AS query_text,\n\tISNULL(qp.query_plan, '') AS query_plan\nFROM qstats AS qs\n\t\tINNER JOIN sys.dm_exec_query_stats AS stats on qs.query_plan_handle = stats.plan_handle\n\t\tCROSS APPLY sys.dm_exec_query_plan(qs.query_plan_handle) AS qp\n\t\tCROSS APPLY sys.dm_exec_sql_text(qs.query_plan_handle) AS st;\n```\n\n----------------------------------------\n\nTITLE: Prometheus Remote Write YAML Configuration Example\nDESCRIPTION: Sample YAML configuration for the Prometheus Remote Write exporter in the OpenTelemetry Collector. It includes HTTP endpoint settings, authentication options, TLS configuration, and various request parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/DESIGN.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  prometheus_remote_write:\n    http_endpoint: <string>\n    # Prefix to metric name\n    namespace: <string>\n    # Labels to add to each TimeSeries\n    const_labels:\n        [label: <string>]\n    # Allow users to add any header; only required headers listed here\n    headers:\n        [X-Prometheus-Remote-Write-Version:<string>]\n        [Tenant-id:<int>]\n    request_timeout: <int>\n\n    # ************************************************************************\n    # below are configurations copied from Prometheus remote write config   \n    # ************************************************************************\n    # Sets the `Authorization` header on every remote write request with the\n    # configured username and password.\n    # password and password_file are mutually exclusive.\n    basic_auth:\n    [ username: <string> ]\n    [ password: <string> ]\n    [ password_file: <string> ]\n\n    # Sets the `Authorization` header on every remote write request with\n    # the configured bearer token. It is mutually exclusive with `bearer_token_file`.\n    [ bearer_token: <string> ]\n\n    # Sets the `Authorization` header on every remote write request with the bearer token\n    # read from the configured file. It is mutually exclusive with `bearer_token`.\n    [ bearer_token_file: /path/to/bearer/token/file ]\n\n    # Configures the remote write request's TLS settings.\n    tls_config:\n        # CA certificate to validate API server certificate with.\n        [ ca_file: <filename> ]\n\n        # Certificate and key files for client cert authentication to the server.\n        [ cert_file: <filename> ]\n        [ key_file: <filename> ]\n\n        # ServerName extension to indicate the name of the server.\n        # https://tools.ietf.org/html/rfc4366#section-3.1\n        [ server_name: <string> ]\n\n        # Disable validation of the server certificate.\n        [ insecure_skip_verify: <boolean> ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Log Groups for Cloudwatch Receiver in YAML\nDESCRIPTION: Example YAML configuration for the Cloudwatch receiver using named log groups. It specifies the region, poll interval, and exact log group and stream to collect from.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscloudwatchreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nawscloudwatch:\n  region: us-west-1\n  logs:\n    poll_interval: 5m\n    groups:\n      named:\n        /aws/eks/dev-0/cluster: \n          names: [kube-apiserver-ea9c831555adca1815ae04b87661klasdj]\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver for Multiline Log Parsing in YAML\nDESCRIPTION: This configuration demonstrates how to set up the Filelog receiver to handle multiline log entries. It uses a line start pattern to identify the beginning of each log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include:\n    - /var/log/example/multiline.log\n    multiline:\n      line_start_pattern: ^Exception\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Observer Extension\nDESCRIPTION: Example configuration for the k8s_observer extension showing authentication setup, node specification, and resource observation settings along with receiver creator integration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  k8s_observer:\n    auth_type: serviceAccount\n    node: ${env:K8S_NODE_NAME}\n    observe_pods: true\n    observe_nodes: true\n    observe_services: true\n    observe_ingresses: true\n\nreceivers:\n  receiver_creator:\n    watch_observers: [k8s_observer]\n    receivers:\n      redis:\n        rule: type == \"port\" && pod.name matches \"redis\"\n        config:\n          password: '`pod.labels[\"SECRET\"]`'\n      kubeletstats:\n        rule: type == \"k8s.node\"\n        config:\n          auth_type: serviceAccount\n          collection_interval: 10s\n          endpoint: \"`endpoint`:`kubelet_endpoint_port`\"\n          extra_metadata_labels:\n            - container.id\n          metric_groups:\n            - container\n            - pod\n            - node\n```\n\n----------------------------------------\n\nTITLE: Disabling Individual CouchDB Metrics\nDESCRIPTION: YAML configuration example showing how to disable specific metrics in the CouchDB receiver. Each metric can be individually disabled by setting the 'enabled' property to false under the metric name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/couchdbreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Transform Processor for Resource Attribute Conversion\nDESCRIPTION: Example configuration for using the transform processor to copy resource attributes into metric labels.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessor:\n  transform:\n    metric_statements:\n      - context: datapoint\n        statements:\n        - set(attributes[\"namespace\"], resource.attributes[\"k8s.namespace.name\"])\n        - set(attributes[\"container\"], resource.attributes[\"k8s.container.name\"])\n        - set(attributes[\"pod\"], resource.attributes[\"k8s.pod.name\"])\n```\n\n----------------------------------------\n\nTITLE: Routing Logs Based on Tenant HTTP Header in OpenTelemetry\nDESCRIPTION: Configures the routing connector to direct logs to different pipelines based on the 'X-Tenant' HTTP header in the request. Uses the request context to route 'acme' and 'ecorp' tenants to dedicated pipelines, with all other logs going to a default pipeline.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    otlp:\n\nexporters:\n  file/other:\n    path: ./other.log\n  file/acme:\n    path: ./acme.log\n  file/ecorp:\n    path: ./ecorp.log\n\nconnectors:\n  routing:\n    default_pipelines: [logs/other]\n    table:\n      - context: request\n        condition: request[\"X-Tenant\"] == \"acme\"\n        pipelines: [logs/acme]\n      - context: request\n        condition: request[\"X-Tenant\"] == \"ecorp\"\n        pipelines: [logs/ecorp]\n\nservice:\n  pipelines:\n    logs/in:\n      receivers: [otlp]\n      exporters: [routing]\n    logs/acme:\n      receivers: [routing]\n      exporters: [file/acme]\n    logs/ecorp:\n      receivers: [routing]\n      exporters: [file/ecorp]\n    logs/other:\n      receivers: [routing]\n      exporters: [file/other]\n```\n\n----------------------------------------\n\nTITLE: Creating OpenTelemetry Metric Histogram Tables in SQL\nDESCRIPTION: Defines SQL tables for storing OpenTelemetry metric histogram data, including metadata, datapoints, buckets, attributes, and exemplars. Uses UUID primary keys and foreign key relationships for data integrity.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE otel.metric_histogram\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    metric_name varchar (256) not null,\n    metric_description varchar (256),\n    metric_unit varchar (256),\n    aggregation_temporality int8\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_histogram_datapoint\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    id UUID (primary_key) not null,\n    start_time_unix TIMESTAMP,\n    time_unix TIMESTAMP NOT NULL,\n    count LONG,\n    data_sum DOUBLE,\n    data_min DOUBLE,\n    data_max DOUBLE,\n    flags INT,\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_datapoint\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_histogram_datapoint_bucket_count\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    datapoint_id UUID (primary_key) not null,\n    count_id UUID (primary_key) not null,\n    count LONG,\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_datapoint_bucket_count\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_histogram_datapoint_explicit_bound\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    datapoint_id UUID (primary_key) not null,\n    bound_id UUID (primary_key) not null,\n    explicit_bound DOUBLE,\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_datapoint_explicit_bound\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_histogram_datapoint_attribute\"\n(\n    \"histogram_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_datapoint_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_histogram_datapoint_exemplar\n(\n    \"histogram_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    time_unix TIMESTAMP NOT NULL,\n    histogram_value DOUBLE,\n    \"trace_id\" VARCHAR (32),\n    \"span_id\" VARCHAR (16),\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_datapoint_exemplar\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_histogram_datapoint_exemplar_attribute\n(\n    \"histogram_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_datapoint_exemplar_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_histogram_resource_attribute\"\n(\n    \"histogram_id\" UUID (primary_key) NOT NULL,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (histogram_id),\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_resource_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_histogram_scope_attribute\"\n(\n    \"histogram_id\" UUID (primary_key) NOT NULL,\n    \"name\" VARCHAR (256),\n    \"version\" VARCHAR (256),\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (histogram_id),\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_scope_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n```\n\n----------------------------------------\n\nTITLE: Setting Stream Lifetime for OTELARROW Exporter\nDESCRIPTION: Configuration for managing stream lifetime in OTELARROW exporter to avoid issues with keepalive settings. This example sets a timeout of 30s and a maximum stream lifetime of 9m30s, which should be less than the server's keepalive parameter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/otelarrowexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otelarrow:\n    timeout: 30s\n    arrow:\n      max_stream_lifetime: 9m30s\n    endpoint: ...\n    tls: ...\n```\n\n----------------------------------------\n\nTITLE: Splunk HEC Receiver Basic and Advanced Configuration\nDESCRIPTION: Example YAML configuration showing basic and advanced setup of the Splunk HEC receiver. Demonstrates configuration options including TLS settings, access token handling, custom paths, and HEC metadata mapping to OpenTelemetry attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/splunkhecreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  splunk_hec:\n  splunk_hec/advanced:\n    access_token_passthrough: true\n    tls:\n      cert_file: /test.crt\n      key_file: /test.key\n    raw_path: \"/raw\"\n    hec_metadata_to_otel_attrs:\n      source: \"mysource\"\n      sourcetype: \"mysourcetype\"\n      index: \"myindex\"\n      host: \"myhost\"\n    ack: \n      extension: ack/in_memory\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Processor in YAML\nDESCRIPTION: This YAML configuration example demonstrates how to set up the resource processor to modify resource attributes. It shows various actions like upsert, insert, and delete on different attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourceprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resource:\n    attributes:\n    - key: cloud.availability_zone\n      value: \"zone-1\"\n      action: upsert\n    - key: k8s.cluster.name\n      from_attribute: k8s-cluster\n      action: insert\n    - key: redundant-attribute\n      action: delete\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Generation Processor in YAML\nDESCRIPTION: This snippet shows the YAML configuration structure for the metrics generation processor. It defines rules for creating new metrics based on existing ones, specifying the metric name, type of operation, and required fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricsgenerationprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: metricsgeneration\n    metricsgeneration:\n\n        # specify the metric generation rules\n        rules:\n              # Name of the new metric. This is a required field.\n            - name: <new_metric_name>\n\n              # Unit for the new metric being generated.\n              unit: <new_metric_unit>\n\n              # type describes how the new metric will be generated. It can be one of `calculate` or `scale`.  calculate generates a metric applying the given operation on two operand metrics. scale operates only on operand1 metric to generate the new metric.\n              type: {calculate, scale}\n\n              # This is a required field. This must be a gauge or sum metric.\n              metric1: <first_operand_metric>\n\n              # This field is required only if the type is \"calculate\". When required, this must be a gauge or sum metric.\n              metric2: <second_operand_metric>\n\n              # Operation specifies which arithmetic operation to apply. It must be one of the five supported operations.\n              operation: {add, subtract, multiply, divide, percent}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pure Storage FlashArray Receiver with Multiple Arrays\nDESCRIPTION: Complete configuration example showing how to set up the Pure Storage FlashArray receiver for two arrays - one using the OpenMetrics exporter and another using native on-box metrics. Includes bearer token authentication, endpoint configuration, and various collection settings for array components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/purefareceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  bearertokenauth/array01:\n    token: \"...\"\n  bearertokenauth/array02:\n    token: \"...\"\n\nreceivers:\n  purefa/array01:\n    fa_array_name: foobar01\n    endpoint: http://127.0.0.1:9490/metrics\n    array:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    hosts:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    directories:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    pods:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    volumes:\n      - address: array01\n        auth:\n          authenticator: bearertokenauth/array01\n    env: dev\n    settings:\n      reload_intervals:\n        array: 20s\n        hosts: 60s\n        directories: 60s\n        pods: 60s\n        volumes: 60s\n\n  purefa/array02:\n    fa_array_name: foobar02\n    endpoint: https://127.0.0.1/metrics\n    tls:\n      insecure_skip_verify: true\n    array:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    hosts:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    directories:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    pods:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    volumes:\n      - address: array02\n        auth:\n          authenticator: bearertokenauth/array02\n    env: production\n    settings:\n      reload_intervals:\n        array: 20s\n        hosts: 60s\n        directories: 60s\n        pods: 60s\n        volumes: 60s\n\nservice:\n  extensions: [bearertokenauth/array01,bearertokenauth/array02]\n  pipelines:\n    metrics:\n      receivers: [purefa/array01,purefa/array02]\n```\n\n----------------------------------------\n\nTITLE: STEF Exporter Compression Configuration in YAML\nDESCRIPTION: Configuration example showing how to enable zstd compression for the STEF exporter. This is currently the only supported compression method.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/stefexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlp:\n    ...\n    compression: zstd\n```\n\n----------------------------------------\n\nTITLE: Configuring KubeletStats Receiver with Node Utilization Metrics\nDESCRIPTION: Configures the KubeletStats receiver to collect CPU and memory utilization metrics for containers and pods. It includes authentication setup and node identification using environment variables.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    kubeletstats:\n      collection_interval: 10s\n      auth_type: 'serviceAccount'\n      endpoint: '${env:K8S_NODE_NAME}:10250'\n      node: '${env:K8S_NODE_NAME}'\n      k8s_api_config:\n        auth_type: serviceAccount\n      metrics:\n        k8s.container.cpu.node.utilization:\n          enabled: true\n        k8s.pod.cpu.node.utilization:\n          enabled: true\n        k8s.container.memory.node.utilization:\n          enabled: true\n        k8s.pod.memory.node.utilization:\n          enabled: true\n```\n\n----------------------------------------\n\nTITLE: Routing Logs Based on Region Attribute in OpenTelemetry\nDESCRIPTION: Demonstrates routing logs to region-specific pipelines based on the 'region' attribute in the log context. Routes logs with 'east' or 'west' region attributes to dedicated pipelines, with all other logs going to a default pipeline.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    otlp:\n\nexporters:\n  file/other:\n    path: ./other.log\n  file/east:\n    path: ./east.log\n  file/west:\n    path: ./west.log\n\nconnectors:\n  routing:\n    default_pipelines: [logs/other]\n    table:\n      - context: log\n        condition: attributes[\"region\"] == \"east\"\n        pipelines: [logs/east]\n      - context: log\n        condition: attributes[\"region\"] == \"west\"\n        pipelines: [logs/west]\n\nservice:\n  pipelines:\n    logs/in:\n      receivers: [otlp]\n      exporters: [routing]\n    logs/east:\n      receivers: [routing]\n      exporters: [file/east]\n    logs/west:\n      receivers: [routing]\n      exporters: [file/west]\n    logs/other:\n      receivers: [routing]\n      exporters: [file/other]\n```\n\n----------------------------------------\n\nTITLE: Sumo Logic Exporter Main Configuration in YAML\nDESCRIPTION: Complete configuration structure for the Sumo Logic exporter including endpoint settings, compression options, format configurations, and queue/retry parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sumologicexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  # ...\n  sumologic:\n    # unique URL generated for your HTTP Source, this is the address to send data to\n    endpoint: <HTTP_Source_URL>\n    # Compression encoding format, empty string means no compression, default = gzip\n    compression: {gzip, deflate, zstd, \"\"}\n    # max HTTP request body size in bytes before compression (if applied),\n    # default = 1_048_576 (1MB)\n    max_request_body_size: <max_request_body_size>\n\n    # format to use when sending logs to Sumo Logic, default = otlp\n    log_format: {otlp, json, text}\n\n    # format to use when sending metrics to Sumo Logic, default = otlp\n    metric_format: {otlp, prometheus}\n\n    # Decompose OTLP Histograms into individual metrics\n    # default = false\n    decompose_otlp_histograms: {true, false}\n\n    # timeout is the timeout for every attempt to send data to the backend\n    # maximum connection timeout is 55s, default = 30s\n    timeout: <timeout>\n\n    # defines if sticky session support is enable.\n    # default=false\n    sticky_session_enabled: {true, false}\n\n    retry_on_failure:\n      # default = true\n      enabled: {true, false}\n      # time to wait after the first failure before retrying\n      initial_interval: <initial_interval>\n      # is the upper bound on backoff\n      max_interval: <max_interval>\n      # is the maximum amount of time spent trying to send a batch\n      max_elapsed_time: <max_elapsed_time>\n\n    sending_queue:\n      # default = false\n      enabled: {true, false}\n      # number of consumers that dequeue batches\n      num_consumers: <num_consumers>\n      # enables persistence using specified storage extension\n      storage: <storage_name>\n      # maximum number of batches kept in memory\n      queue_size: <queue_size>\n```\n\n----------------------------------------\n\nTITLE: Using HasAttrKeyOnDatapoint Function in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop metrics containing a specific attribute key on any datapoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# Drops metrics containing the 'bad.metric' attribute key\nfilter/keep_good_metrics:\n  error_mode: ignore\n  metrics:\n    metric:\n      - 'HasAttrKeyOnDatapoint(\"bad.metric\")'\n```\n\n----------------------------------------\n\nTITLE: Configuring Sigv4 Authentication with Role Assumption in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the OpenTelemetry Collector showing how to set up the Sigv4 authenticator extension with role assumption to access AWS services. This configuration shows how to export metrics to Prometheus Remote Write endpoint in AWS.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  sigv4auth:\n    assume_role:\n      arn: \"arn:aws:iam::123456789012:role/aws-service-role/access\"\n      sts_region: \"us-east-1\"\n\nreceivers:\n  hostmetrics:\n    scrapers:\n      memory:\n\nexporters:\n  prometheusremotewrite:\n    endpoint: \"https://aps-workspaces.us-west-2.amazonaws.com/workspaces/ws-XXX/api/v1/remote_write\"\n    auth:\n      authenticator: sigv4auth\n\nservice:\n  extensions: [sigv4auth]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [prometheusremotewrite]\n```\n\n----------------------------------------\n\nTITLE: Configuring Routing Processor with OTTL Statements in OpenTelemetry\nDESCRIPTION: Example configuration for the routing processor using OpenTelemetry Transformation Language (OTTL) statements as routing conditions. It demonstrates how to route based on resource attributes and perform attribute manipulation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/routingprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  routing:\n    default_exporters:\n    - jaeger\n    error_mode: ignore\n    table:\n      - statement: route() where resource.attributes[\"X-Tenant\"] == \"acme\"\n        exporters: [jaeger/acme]\n      - statement: delete_key(resource.attributes, \"X-Tenant\") where IsMatch(resource.attributes[\"X-Tenant\"], \".*corp\")\n        exporters: [jaeger/ecorp]\n\nexporters:\n  jaeger:\n    endpoint: localhost:14250\n  jaeger/acme:\n    endpoint: localhost:24250\n  jaeger/ecorp:\n    endpoint: localhost:34250\n```\n\n----------------------------------------\n\nTITLE: Sample Log Entries for Plaintext File Tailing\nDESCRIPTION: These are example log entries that the plaintext file tailing configuration can parse. They include timestamp, severity, and message components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n2023-06-19 05:20:50 ERROR This is a test error message\n2023-06-20 12:50:00 DEBUG This is a test debug message\n```\n\n----------------------------------------\n\nTITLE: General Resource Detection Processor Configuration in YAML\nDESCRIPTION: Example YAML configuration showing the general structure of the resource detection processor configuration, including detector selection and attribute overriding options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\ndetectors: [ <string> ]\noverride: <bool>\nattributes: [ <string> ]\n```\n\nLANGUAGE: yaml\nCODE:\n```\nresourcedetection:\n  detectors: [system, ec2]\n  system:\n    resource_attributes:\n      host.name:\n        enabled: true\n      host.id:\n        enabled: false\n  ec2:\n    resource_attributes:\n      host.name:\n        enabled: false\n      host.id:\n        enabled: true\n```\n\nLANGUAGE: yaml\nCODE:\n```\nresourcedetection:\n  detectors: [system]\n  attributes: ['host.name', 'host.id']\n```\n\nLANGUAGE: yaml\nCODE:\n```\nresourcedetection:\n  detectors: [system]\n  system:\n    resource_attributes:\n      host.name:\n        enabled: true\n      host.id:\n        enabled: true\n      os.type:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector with ClickHouse Exporter\nDESCRIPTION: Example YAML configuration showing how to set up the OpenTelemetry Collector with ClickHouse exporter. Demonstrates settings for logs, traces, and metrics pipelines with batching, compression, and retry logic.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  examplereceiver:\nprocessors:\n  batch:\n    timeout: 5s\n    send_batch_size: 100000\nexporters:\n  clickhouse:\n    endpoint: tcp://127.0.0.1:9000?dial_timeout=10s\n    database: otel\n    async_insert: true\n    ttl: 72h\n    compress: lz4\n    create_schema: true\n    logs_table_name: otel_logs\n    traces_table_name: otel_traces\n    timeout: 5s\n    metrics_tables:\n      gauge: \n        name: \"otel_metrics_gauge\"\n      sum: \n        name: \"otel_metrics_sum\"\n      summary: \n        name: \"otel_metrics_summary\"\n      histogram: \n        name: \"otel_metrics_histogram\"\n      exponential_histogram: \n        name: \"otel_metrics_exp_histogram\"\n    retry_on_failure:\n      enabled: true\n      initial_interval: 5s\n      max_interval: 30s\n      max_elapsed_time: 300s\nservice:\n  pipelines:\n    logs:\n      receivers: [ examplereceiver ]\n      processors: [ batch ]\n      exporters: [ clickhouse ]\n```\n\n----------------------------------------\n\nTITLE: Configuring NSX-T Receiver in OpenTelemetry Collector\nDESCRIPTION: Example configuration for setting up the NSX-T receiver with metrics pipeline. Shows how to configure endpoint, credentials, timeout settings, and metric filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/nsxtreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  nsxt:\n    endpoint: https://nsx-manager\n    username: admin\n    password: password\n    timeout: 60s\n    metrics:\n      nsxt.node.cpu.utilization:\n        enabled: false\n\nexporters:\n  file:\n    path: \"./content.json\"\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [nsxt]\n      exporters: [file]\n```\n\n----------------------------------------\n\nTITLE: Basic OpenTelemetry Routing with Default Pipelines\nDESCRIPTION: Basic routing configuration example showing how to route logs based on environment and region attributes using default_pipelines. Uses match_once: false to allow multiple matches.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrouting:\n  match_once: false\n  default_pipelines: [ logs/default ]\n  table:\n    - condition: attributes[\"env\"] == \"prod\"\n       pipelines: [ logs/prod ]\n    - condition: attributes[\"env\"] == \"dev\"\n       pipelines: [ logs/dev ]\n    - condition: attributes[\"region\"] == \"east\"\n       pipelines: [ logs/east ]\n    - condition: attributes[\"region\"] == \"west\"\n       pipelines: [ logs/west ]\n\nservice:\n  pipelines:\n    logs/in::exporters: [routing]\n    logs/default::receivers: [routing]\n    logs/prod::receivers: [routing]\n    logs/dev::receivers: [routing]\n    logs/east::receivers: [routing]\n    logs/west::receivers: [routing]\n```\n\n----------------------------------------\n\nTITLE: Configuring DataSet Exporters for Logs and Traces in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration sets up processors, DataSet exporters for logs and traces, and defines service pipelines. It includes settings for API keys, buffer configurations, and serverHost attribute handling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datasetexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  attributes:\n    - key: serverHost\n      action: insert\n      from_attribute: container_id\n  resource:\n    attributes:\n      - key: serverHost\n        from_attribute: node_id\n        action: insert      \n\nexporters:\n  dataset/logs:\n    # DataSet API URL, https://app.eu.scalyr.com for DataSet EU instance\n    dataset_url: https://app.scalyr.com\n    # API Key\n    api_key: your_api_key\n    buffer:\n      # Send buffer to the API at least every 5s\n      max_lifetime: 5s\n      # Group data based on these attributes\n      group_by:\n        - container_id\n      # try to send data to the DataSet for at most 30s during shutdown\n      retry_shutdown_timeout: 30s\n    server_host:\n      # If the serverHost attribute is not specified or empty,\n      # use the value from the env variable SERVER_HOST\n      server_host: ${env:SERVER_HOST}\n      # If server_host is not set, use the hostname value\n      use_hostname: true\n\n  dataset/traces:\n    # DataSet API URL, https://app.eu.scalyr.com for DataSet EU instance\n    dataset_url: https://app.scalyr.com\n    # API Key\n    api_key: your_api_key\n    buffer:\n      max_lifetime: 15s\n      group_by:\n        - resource_service.instance.id\n\nservice:\n  pipelines:\n    logs:\n      receivers: [otlp]\n      processors: [batch, attributes]\n      # add dataset among your exporters\n      exporters: [dataset/logs]\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      # add dataset among your exporters\n      exporters: [dataset/traces]\n```\n\n----------------------------------------\n\nTITLE: Configuring Zipkin Exporter in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for Zipkin exporter with different TLS options. Shows three configurations: one without TLS, one with TLS, and one with TLS but skipping certificate verification.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/zipkinexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  zipkin/nontls:\n    endpoint: \"http://some.url:9411/api/v2/spans\"\n    format: proto\n    default_service_name: unknown-service\n\n  zipkin/withtls:\n    endpoint: \"https://some.url:9411/api/v2/spans\"\n\n  zipkin/tlsnoverify:\n    endpoint: \"https://some.url:9411/api/v2/spans\"\n    tls:\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional MySQL Metrics in YAML\nDESCRIPTION: Example YAML configuration to enable optional MySQL metrics. This configuration demonstrates how to enable specific metrics that are not turned on by default in the collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Metrics Collection in YAML\nDESCRIPTION: Basic configuration to receive metrics from MongoDB Atlas using environment variables for authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    public_key: ${env:MONGODB_ATLAS_PUBLIC_KEY}\n    private_key: ${env:MONGODB_ATLAS_PRIVATE_KEY}\n```\n\n----------------------------------------\n\nTITLE: Type Checking and Validation Examples\nDESCRIPTION: Examples of using various type checking functions like IsBool, IsDouble, IsInt, IsString, and IsMatch.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_29\n\nLANGUAGE: Go\nCODE:\n```\nIsBool(false)\nIsBool(42)\nIsDouble(log.body)\nIsInt(log.attributes[\"maybe a int\"])\nIsString(log.body)\nIsMatch(span.attributes[\"http.path\"], \"foo\")\n```\n\n----------------------------------------\n\nTITLE: Parsing Simplified XML to JSON - Basic Event Structure\nDESCRIPTION: Demonstrates parsing a simple XML event document with nested elements into equivalent JSON structure. Shows how hierarchical XML data is transformed while preserving data types.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_46\n\nLANGUAGE: xml\nCODE:\n```\n<event>\n    <id>1</id>\n    <user>jane</user>\n    <details>\n      <time>2021-10-01T12:00:00Z</time>\n      <description>Something happened</description>\n      <cause>unknown</cause>\n    </details>\n</event>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event\": {\n    \"id\": 1,\n    \"user\": \"jane\",\n    \"details\": {\n      \"time\": \"2021-10-01T12:00:00Z\",\n      \"description\": \"Something happened\",\n      \"cause\": \"unknown\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Exporter with Basic Auth\nDESCRIPTION: Example configuration showing how to set up the OpenSearch exporter with basic authentication, including endpoint configuration and pipeline setup for traces. Uses the basicauth extension for authentication credentials.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/opensearchexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  basicauth/client:\n  client_auth:\n    username: username\n    password: password\n    \nexporters:\n  opensearch/trace:\n    http:\n      endpoint: https://opensearch.example.com:9200\n      auth:\n        authenticator: basicauth/client\n# ······\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [opensearch/trace]\n      processors: [batch]\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk HEC Exporter in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet shows the basic settings for the Splunk HEC exporter, including application name and version for telemetry tracking, heartbeat interval, and telemetry configuration with metric name overrides and custom attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/splunkhecexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Application name is used to track telemetry information for Splunk App's using HEC by App name.\nsplunk_app_name: \"OpenTelemetry-Collector Splunk Exporter\"\n# Application version is used to track telemetry information for Splunk App's using HEC by App version.\nsplunk_app_version: \"v0.0.1\"\nheartbeat:\n  interval: 30s\ntelemetry:\n  enabled: true\n  override_metrics_names:\n    otelcol_exporter_splunkhec_heartbeats_sent: app_heartbeats_success_total\n    otelcol_exporter_splunkhec_heartbeats_failed: app_heartbeats_failed_total\n  extra_attributes:\n    dataset_name: SplunkCloudBeaverStack\n    custom_key: custom_value\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver for JSON File Tailing in YAML\nDESCRIPTION: This snippet demonstrates how to configure the Filelog receiver to tail a simple JSON log file. It includes settings for file path and a JSON parser operator with timestamp parsing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: [ /var/log/myservice/*.json ]\n    operators:\n      - type: json_parser\n        timestamp:\n          parse_from: attributes.time\n          layout: '%Y-%m-%d %H:%M:%S'\n```\n\n----------------------------------------\n\nTITLE: Displaying Error Count Metrics Example\nDESCRIPTION: Example of error count metrics generated by the span metrics connector, showing dimensions including service name, span name, span kind, and error status code.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/spanmetricsconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ntraces.span.metrics.calls{service.name=\"shipping\",span.name=\"get_shipping/{shippingId},span.kind=\"SERVER\",status.code=\"Error\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cassandra Exporter in YAML\nDESCRIPTION: Example YAML configuration for the Cassandra exporter showing all available options including connection details, keyspace settings, replication strategy, compression algorithm, and authentication credentials.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/cassandraexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  cassandra:\n    dsn: 127.0.0.1\n    port: 9042\n    timeout: 10s\n    keyspace: \"otel\"\n    trace_table: \"otel_spans\"\n    replication:\n      class: \"SimpleStrategy\"\n      replication_factor: 1\n    compression:\n      algorithm: \"ZstdCompressor\"\n    auth:\n      username: \"your-username\"\n      password: \"your-password\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Pipelines for SignalFx in YAML\nDESCRIPTION: This configuration example shows how to set up the OpenTelemetry Collector service pipelines for metrics, logs, and traces using the SignalFx receiver and exporter. It includes processors for memory limiting and batching.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n    logs:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n    traces:\n      receivers: [zipkin]\n      processors: []\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Configuring Team-Based Filters in OpenTelemetry\nDESCRIPTION: Sets up filter processors to route telemetry data to different teams based on attribute values. Filters metrics data points based on team-specific attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:  \n  filter/teamA:\n    metrics:\n      datapoint:\n          - 'attributes[\"your_label\"] != \"teamA\"'\n  filter/teamB:\n    metrics:\n      datapoint:\n          - 'attributes[\"your_label\"] != \"teamB\"'\n```\n\n----------------------------------------\n\nTITLE: Trace Parser Configuration Fields Table\nDESCRIPTION: Markdown table defining the configuration fields available for the trace_parser operator, including field names, default values, and descriptions of their purposes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/trace_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Field                    | Default          | Description |\n| ---                      | ---              | ---         |\n| `id`                     | `trace_parser`   | A unique identifier for the operator. |\n| `output`                 | Next in pipeline | The `id` for the operator to send parsed entries to. |\n| `trace_id.parse_from`    | `trace_id`       | A [field](../types/field.md) that indicates the field to be parsed as a trace ID. |\n| `span_id.parse_from`     | `span_id`        | A [field](../types/field.md) that indicates the field to be parsed as a span ID. |\n| `trace_flags.parse_from` | `trace_flags`    | A [field](../types/field.md) that indicates the field to be parsed as trace flags. |\n| `on_error`               | `send`           | The behavior of the operator if it encounters an error. See [on_error](../types/on_error.md). |\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Graph Connector with Custom Settings\nDESCRIPTION: YAML configuration example for the Service Graph Connector with custom latency histogram buckets, dimensions, and store settings. It also includes the necessary pipeline configuration to use the connector between OTLP receiver and Prometheus exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/servicegraphconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nconnectors:\n  servicegraph:\n    latency_histogram_buckets: [100ms, 250ms, 1s, 5s, 10s]\n    dimensions:\n      - dimension-1\n      - dimension-2\n    store:\n      ttl: 1s\n      max_items: 10\n\nexporters:\n  prometheus/servicegraph:\n    endpoint: localhost:9090\n    namespace: servicegraph\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [servicegraph]\n    metrics/servicegraph:\n      receivers: [servicegraph]\n      exporters: [prometheus/servicegraph]\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray Segment JSON Structure for DynamoDB Operation\nDESCRIPTION: A complete AWS X-Ray segment showing a DynamoDB operation that failed with ResourceNotFoundException. The JSON structure includes trace identification, timing information, error details, stack traces, and nested subsegments that track the request flow through various components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/invalidNamespace.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f29ab21-d4ebf299219a65bd5c31d6da\",\n    \"id\": \"88ad1df59cd7a7be\",\n    \"name\": \"DDB\",\n    \"start_time\": 1596566305.535414,\n    \"end_time\": 1596566305.5928545,\n    \"fault\": true,\n    \"cause\": {\n        \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n        \"exceptions\": [\n            {\n                \"id\": \"3e9e11e3ab3fba60\",\n                \"type\": \"dynamodb.ResourceNotFoundException\",\n                \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                \"stack\": [\n                    {\n                        \"path\": \"runtime/proc.go\",\n                        \"line\": 203,\n                        \"label\": \"main\"\n                    },\n                    {\n                        \"path\": \"runtime/asm_amd64.s\",\n                        \"line\": 1373,\n                        \"label\": \"goexit\"\n                    }\n                ],\n                \"remote\": true\n            }\n        ]\n    },\n    \"user\": \"xraysegmentdump\",\n    \"aws\": {\n        \"xray\": {\n            \"sdk_version\": \"1.1.0\",\n            \"sdk\": \"X-Ray for Go\"\n        }\n    },\n    \"service\": {\n        \"compiler_version\": \"go1.14.6\",\n        \"compiler\": \"gc\"\n    },\n    \"subsegments\": [\n        {\n            \"id\": \"7df694142c905d8d\",\n            \"name\": \"DDB.DescribeExistingTableAndPutToMissingTable\",\n            \"start_time\": 1596566305.5354965,\n            \"end_time\": 1596566305.5928457,\n            \"fault\": true,\n            \"cause\": {\n                \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n                \"exceptions\": [\n                    {\n                        \"id\": \"e2ba8a2109451f5b\",\n                        \"type\": \"dynamodb.ResourceNotFoundException\",\n                        \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                        \"stack\": [\n                            {\n                                \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/capture.go\",\n                                \"line\": 48,\n                                \"label\": \"Capture\"\n                            },\n                            {\n                                \"path\": \"sampleapp/sample.go\",\n                                \"line\": 41,\n                                \"label\": \"ddbExpectedFailure\"\n                            },\n                            {\n                                \"path\": \"sampleapp/sample.go\",\n                                \"line\": 36,\n                                \"label\": \"main\"\n                            },\n                            {\n                                \"path\": \"runtime/proc.go\",\n                                \"line\": 203,\n                                \"label\": \"main\"\n                            },\n                            {\n                                \"path\": \"runtime/asm_amd64.s\",\n                                \"line\": 1373,\n                                \"label\": \"goexit\"\n                            }\n                        ],\n                        \"remote\": true\n                    }\n                ]\n            },\n            \"annotations\": {\n                \"DDB.DescribeExistingTableAndPutToMissingTable.Annotation\": \"anno\"\n            },\n            \"metadata\": {\n                \"default\": {\n                    \"DDB.DescribeExistingTableAndPutToMissingTable.AddMetadata\": \"meta\"\n                }\n            },\n            \"subsegments\": [\n                {\n                    \"id\": \"7318c46a385557f5\",\n                    \"name\": \"dynamodb\",\n                    \"start_time\": 1596566305.5355225,\n                    \"end_time\": 1596566305.5873947,\n                    \"namespace\": \"invalidNs\",\n                    \"http\": {\n                        \"response\": {\n                            \"status\": 200,\n                            \"content_length\": 713\n                        }\n                    },\n                    \"aws\": {\n                        \"operation\": \"DescribeTable\",\n                        \"region\": \"us-west-2\",\n                        \"request_id\": \"29P5V7QSAKHS4LNL56ECAJFF3BVV4KQNSO5AEMVJF66Q9ASUAAJG\",\n                        \"retries\": 0,\n                        \"table_name\": \"xray_sample_table\"\n                    },\n                    \"subsegments\": [\n                        {\n                            \"id\": \"0239834271dbee25\",\n                            \"name\": \"marshal\",\n                            \"start_time\": 1596566305.5355248,\n                            \"end_time\": 1596566305.5355635,\n                            \"Dummy\": false\n                        },\n                        {\n                            \"id\": \"23cf5bb60e4f66b1\",\n                            \"name\": \"attempt\",\n                            \"start_time\": 1596566305.5355663,\n                            \"end_time\": 1596566305.5873196,\n                            \"subsegments\": [\n                                {\n                                    \"id\": \"417b81b977b9563b\",\n                                    \"name\": \"connect\",\n                                    \"start_time\": 1596566305.5357504,\n                                    \"end_time\": 1596566305.575329,\n                                    \"metadata\": {\n                                        \"http\": {\n                                            \"connection\": {\n                                                \"reused\": false,\n                                                \"was_idle\": false\n                                            }\n                                        }\n                                    },\n                                    \"subsegments\": [\n                                        {\n                                            \"id\": \"0cab02b318413eb1\",\n                                            \"name\": \"dns\",\n                                            \"start_time\": 1596566305.5357957,\n                                            \"end_time\": 1596566305.5373216,\n                                            \"metadata\": {\n                                                \"http\": {\n                                                    \"dns\": {\n                                                        \"addresses\": [\n                                                            {\n                                                                \"IP\": \"52.94.10.94\",\n                                                                \"Zone\": \"\"\n                                                            }\n                                                        ],\n                                                        \"coalesced\": false\n                                                    }\n                                                }\n                                            },\n                                            \"Dummy\": false\n                                        },\n                                        {\n                                            \"id\": \"f8dbc5c6b291017e\",\n                                            \"name\": \"dial\",\n                                            \"start_time\": 1596566305.5373297,\n                                            \"end_time\": 1596566305.537964,\n                                            \"metadata\": {\n                                                \"http\": {\n                                                    \"connect\": {\n                                                        \"network\": \"tcp\"\n                                                    }\n                                                }\n                                            },\n                                            \"Dummy\": false\n                                        },\n                                        {\n                                            \"id\": \"e2deb66ecaa769a5\",\n                                            \"name\": \"tls\",\n                                            \"start_time\": 1596566305.5380135,\n                                            \"end_time\": 1596566305.5753162,\n                                            \"metadata\": {\n                                                \"http\": {\n                                                    \"tls\": {\n                                                        \"cipher_suite\": 49199,\n                                                        \"did_resume\": false,\n                                                        \"negotiated_protocol\": \"http/1.1\",\n                                                        \"negotiated_protocol_is_mutual\": true\n                                                    }\n                                                }\n                                            },\n                                            \"Dummy\": false\n                                        }\n                                    ],\n                                    \"Dummy\": false\n                                },\n                                {\n                                    \"id\": \"a70bfab91597c7a2\",\n                                    \"name\": \"request\",\n                                    \"start_time\": 1596566305.5753367,\n                                    \"end_time\": 1596566305.5754144,\n                                    \"Dummy\": false\n                                },\n                                {\n                                    \"id\": \"c05331c26d3e8a7f\",\n                                    \"name\": \"response\",\n                                    \"start_time\": 1596566305.5754204,\n                                    \"end_time\": 1596566305.5872962,\n                                    \"Dummy\": false\n                                }\n                            ],\n                            \"Dummy\": false\n                        }\n```\n\n----------------------------------------\n\nTITLE: Output JSON after GoTime Layout Parsing\nDESCRIPTION: This JSON shows the output entry after parsing the timestamp using the GoTime layout. The timestamp field is now populated with the parsed and formatted date.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2020-06-05T13:50:27-05:00\",\n  \"body\": {\n    \"timestamp_field\": \"Jun 5 13:50:27 EST 2020\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Document ID from Span Name in YAML\nDESCRIPTION: This example demonstrates how to extract a document ID from a span name using a regular expression. It adds the extracted ID as a new attribute to the span.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/spanprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nspan/to_attributes:\n  name:\n    to_attributes:\n      rules:\n        - ^/api/v1/document/(?P<documentId>.*)/update$\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Monitoring Receiver in YAML\nDESCRIPTION: Example configuration for the Google Cloud Monitoring receiver showing how to set up metric collection intervals, project ID, and specific metrics to monitor. Includes options for collection interval, project identification, and metric filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/googlecloudmonitoringreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  googlecloudmonitoring:\n    collection_interval: 2m # Can be specified in seconds (s), minutes (m), or hours (h)\n    project_id: my-project-id\n    metrics_list:\n      - metric_name: \"compute.googleapis.com/instance/cpu/usage_time\"\n      - metric_name: \"connectors.googleapis.com/flex/instance/cpu/usage_time\"\n```\n\n----------------------------------------\n\nTITLE: K8slog Receiver Configuration Fields\nDESCRIPTION: Core configuration fields for the K8slog receiver, including discovery mode settings and extraction rules.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Field            | Default            | Description                                                                                                      |\n|------------------|--------------------|------------------------------------------------------------------------------------------------------------------|\n| `discovery.mode` | `daemonset-stdout` | The mode of discovery. Only `daemonset-stdout` is supported now. `daemonset-file` and `sidecar` are coming soon. |\n| `extract`        |                    | The rules to extract metadata from pods and containers. TODO default values.                                     |\n```\n\n----------------------------------------\n\nTITLE: Advanced Round-Robin Configuration with Multiple Pipelines in YAML\nDESCRIPTION: Complete configuration example demonstrating how to use the round-robin connector to preprocess data and then scale throughput by distributing the load across multiple exporter instances, with resource detection and batch processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/roundrobinconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\nprocessors:\n  resourcedetection:\n  batch:\nexporters:\n  prometheusremotewrite/1:\n  prometheusremotewrite/2:\nconnectors:\n  roundrobin:\nservice:\n  pipelines:\n    metrics:\n      receivers: [otlp]\n      processors: [resourcedetection, batch]\n      exporters: [roundrobin]\n    metrics/1:\n      receivers: [roundrobin]\n      exporters: [prometheusremotewrite/1]\n    metrics/2:\n      receivers: [roundrobin]\n      exporters: [prometheusremotewrite/2]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector with Loki Exporter (Pre-Migration)\nDESCRIPTION: YAML configuration for the OpenTelemetry Collector using the deprecated Loki exporter. It sets up basic authentication and defines the Loki endpoint for log pushing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  basicauth/loki:\n    client_auth:\n      username: <<username>>\n      password: <<password>>\n\nexporters:\n  loki:\n    auth:\n      authenticator: basicauth/loki\n    endpoint: https://loki.example.com:3100/loki/api/v1/push\n\nservice:\n  extensions: [basicauth/loki]\n  pipelines:\n    logs:\n      receivers: [...]\n      processors: [...]\n      exporters: [loki, ...]\n```\n\n----------------------------------------\n\nTITLE: XML Element Removal Example\nDESCRIPTION: Shows how the RemoveXML function removes specific elements based on XPath expressions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_51\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Log>\n  <Record>\n    <ID>00001</ID>\n    <Name type=\"archive\"></Name>\n    <Data>Some data</Data>\n  </Record>\n  <Record>\n    <ID>00002</ID>\n    <Name type=\"user\"></Name>\n    <Data>Some data</Data>\n  </Record>\n</Log>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Log>\n  <Record>\n    <ID>00002</ID>\n    <Name type=\"user\"></Name>\n    <Data>Some data</Data>\n  </Record>\n</Log>\n```\n\n----------------------------------------\n\nTITLE: Configuring StatsD Receiver in YAML\nDESCRIPTION: Example configuration for the StatsD receiver, demonstrating various optional settings including endpoint, transport, aggregation interval, and timer histogram mapping.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/statsdreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  statsd:\n  statsd/2:\n    endpoint: \"localhost:8127\"\n    aggregation_interval: 70s\n    enable_metric_type: true\n    is_monotonic_counter: false\n    timer_histogram_mapping:\n      - statsd_type: \"histogram\"\n        observer_type: \"gauge\"\n      - statsd_type: \"timing\"\n        observer_type: \"histogram\"\n        histogram: \n          max_size: 100\n      - statsd_type: \"distribution\"\n        observer_type: \"summary\"\n        summary: \n          percentiles: [0, 10, 50, 90, 95, 100]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector for Kinetica Exporter\nDESCRIPTION: Sample YAML configuration for the OpenTelemetry collector with Kinetica exporter. Defines receivers for OTLP and Prometheus, a batch processor, and the Kinetica exporter with connection details. Sets up pipelines for traces, metrics, and logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: localhost:4317\n\n  prometheus:\n      config:\n        scrape_configs:\n          - job_name: 'ki_stats'\n            honor_labels: true\n            static_configs:\n              - targets: ['172.31.32.21:9010', '172.31.32.15:9010', '172.31.32.16:9010', '172.31.32.18:9010', '172.31.33.29:9010', '172.31.32.19:9010', '172.31.32.26:9010', '172.31.32.20:9010', '172.31.32.17:9010']\n\nprocessors:\n  batch:\n\nexporters:\n  kinetica:\n    host: http://localhost:9191/\n    schema: otel\n    username: admin\n    password: password\n    bypasssslcertcheck: true\n    logconfigfile: log_config.yaml\n\n\nservice:\n  pipelines:\n    traces:\n      receivers:\n      - otlp\n      processors:\n      - batch\n      exporters:\n      - kinetica\n    metrics:\n      receivers:\n      - otlp\n      - prometheus\n      processors:\n      - batch\n      exporters:\n      - kinetica\n    logs:\n      receivers:\n      - otlp\n      processors:\n      - batch\n      exporters:\n      - kinetica\n```\n\n----------------------------------------\n\nTITLE: Creating New Metric from Existing Metric in YAML\nDESCRIPTION: Configuration example showing how to create a new metric 'host.cpu.utilization' from existing 'host.cpu.usage' metric\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Loki Receiver in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up the Loki receiver with both HTTP and gRPC protocols. It specifies endpoints for each protocol and enables the use of incoming timestamps.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/lokireceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  loki:\n    protocols:\n      http:\n        endpoint: 0.0.0.0:3500\n      grpc:\n        endpoint: 0.0.0.0:3600\n    use_incoming_timestamp: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Monitor Exporter with Connection String in YAML\nDESCRIPTION: Example YAML configuration for the Azure Monitor Exporter using the recommended connection string method. This includes the InstrumentationKey and IngestionEndpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuremonitorexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  azuremonitor:\n    connection_string: \"InstrumentationKey=00000000-0000-0000-0000-000000000000;IngestionEndpoint=https://ingestion.azuremonitor.com/\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AKS Resource Detection with Cluster Name\nDESCRIPTION: YAML configuration for AKS resource detection with cluster name detection enabled, which derives the name from IMDS infrastructure resource group.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/aks:\n    detectors: [aks]\n    timeout: 2s\n    override: false\n    aks:\n      resource_attributes:\n        k8s.cluster.name:\n          enabled: true\n```\n\n----------------------------------------\n\nTITLE: Regex Parsing with Timestamp Configuration\nDESCRIPTION: Configuration example demonstrating regex parsing with timestamp extraction and formatting using strptime layout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_parser.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_parser\n  regex: '^Time=(?P<timestamp_field>\\d{4}-\\d{2}-\\d{2}), Host=(?P<host>[^,]+), Type=(?P<type>.*)$'\n  timestamp:\n    parse_from: body.timestamp_field\n    layout_type: strptime\n    layout: '%Y-%m-%d'\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Lambda Resource Detection\nDESCRIPTION: YAML configuration for detecting AWS Lambda resources using runtime environment variables provided by the Lambda service.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/lambda:\n    detectors: [env, lambda]\n    timeout: 0.2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Using replace_all_patterns function in OTTL\nDESCRIPTION: The replace_all_patterns function replaces segments in string values or keys of a pcommon.Map that match a regex pattern with a replacement string, operating on either keys or values based on the mode parameter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_6\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_patterns(resource.attributes, \"value\", \"/account/\\\\d{4}\", \"/account/{accountId}\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_patterns(resource.attributes, \"key\", \"/account/\\\\d{4}\", \"/account/{accountId}\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_patterns(resource.attributes, \"key\", \"^kube_([0-9A-Za-z]+_)\", \"k8s.$$1.\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_patterns(resource.attributes, \"key\", \"^kube_([0-9A-Za-z]+_)\", \"$$1.\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_patterns(resource.attributes, \"key\", \"^kube_([0-9A-Za-z]+_)\", \"$$1.\", SHA256, \"k8s.%s\")\n```\n\n----------------------------------------\n\nTITLE: Advanced Faro Exporter Configuration with TLS Options\nDESCRIPTION: Extended configuration example showing how to configure the Faro exporter with TLS options. This example includes both a standard HTTPS configuration and one that skips TLS verification, which can be useful for testing environments.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/faroexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  faro:\n    endpoint: \"https://faro.example.com/collect\"\n\n  faro/tlsnoverify:\n    endpoint: \"https://faro.example.com/collect\"\n    tls:\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Sum Metrics in YAML\nDESCRIPTION: Configuration structure for Sum metrics in the Signal to Metrics connector, using OTTL expressions to extract values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/signaltometricsconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsum:\n  value: <ottl_value_expression>\n```\n\n----------------------------------------\n\nTITLE: Total Instance Counter Configuration Example\nDESCRIPTION: Example configuration showing how to collect the _Total instance CPU metrics separately, which is required since _Total is dropped when collected with other instances.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsperfcountersreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nwindowsperfcounters:\n  metrics:\n    processor.time.total:\n      description: Total CPU active and idle time\n      unit: \"%\"\n      gauge:\n  collection_interval: 30s\n  perfcounters:\n    - object: \"Processor\"\n      instances:\n          - \"_Total\"\n      counters:\n        - name: \"% Processor Time\"\n          metric: processor.time.total\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple Stdout Operator in YAML\nDESCRIPTION: Basic configuration example showing how to set up a stdout operator with a custom identifier. The operator will write entries to stdout in JSON format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/stdout.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- id: my_stdout\n  type: stdout\n```\n\n----------------------------------------\n\nTITLE: Configuring Flexible Resource Mapping for Logs in YAML\nDESCRIPTION: This snippet shows how to configure flexible resource mapping for logs in the LogicMonitor exporter. It allows specifying the resource mapping operation as 'OR' to consider any matching resource attribute.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/logicmonitorexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  logicmonitor:\n    endpoint: https://company.logicmonitor.com/rest\n    headers:\n      Authorization: Bearer <token>\n    logs:\n      resource_mapping_op: \"OR\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Netflow Receiver in OpenTelemetry Collector\nDESCRIPTION: Example configuration for setting up the netflow receiver to listen for both netflow and sflow data on different ports. Includes recommended batch processor configuration and service pipeline setup.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/netflowreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  netflow:\n    - scheme: netflow\n      port: 2055\n      sockets: 16\n      workers: 32\n  netflow/sflow:\n    - scheme: sflow\n      port: 6343\n      sockets: 16\n      workers: 32\n\nprocessors:\n  batch:\n    send_batch_size: 2000\n    timeout: 30s\n\nexporters:\n  debug:\n    verbosity: detailed\n\nservice:\n  pipelines:\n    logs:\n      receivers: [netflow, netflow/sflow]\n      processors: [batch]\n      exporters: [debug]\n  telemetry:\n    logs:\n      level: debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Aerospike Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for setting up the Aerospike receiver in OpenTelemetry Collector. This configuration specifies the endpoint, collection interval, and other parameters for connecting to and monitoring an Aerospike database instance.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/aerospikereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    aerospike:\n        endpoint: \"localhost:3000\"\n        tlsname: \"\"\n        collect_cluster_metrics: false\n        collection_interval: 30s\n```\n\n----------------------------------------\n\nTITLE: Wavefront Receiver YAML Configuration\nDESCRIPTION: Example configuration for the Wavefront receiver showing both default and custom settings. Demonstrates how to configure the endpoint, TCP idle timeout, and CollectD tag extraction.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/wavefrontreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  wavefront:\n  wavefront/allsettings:\n    endpoint: localhost:8080\n    tcp_idle_timeout: 5s\n    extract_collectd_tags: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Fluent Forward Receiver in OpenTelemetry Collector\nDESCRIPTION: Basic YAML configuration example for the Fluent Forward receiver that listens on all interfaces on port 8006. This configuration enables the collector to receive logs via the Fluent Forward protocol.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/fluentforwardreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  fluentforward:\n    endpoint: 0.0.0.0:8006\n```\n\n----------------------------------------\n\nTITLE: Complete Splunk HEC Exporter Configuration Example in YAML\nDESCRIPTION: A comprehensive example configuration for the Splunk HEC exporter showing all major settings including authentication token, endpoint URL, source, sourcetype, index, connection settings, and TLS configuration options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/splunkhecexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  splunk_hec:\n    # Splunk HTTP Event Collector token.\n    token: \"00000000-0000-0000-0000-0000000000000\"\n    # URL to a Splunk instance to send data to.\n    endpoint: \"https://splunk:8088/services/collector\"\n    # Optional Splunk source: https://docs.splunk.com/Splexicon:Source\n    source: \"otel\"\n    # Optional Splunk source type: https://docs.splunk.com/Splexicon:Sourcetype\n    sourcetype: \"otel\"\n    # Splunk index, optional name of the Splunk index targeted.\n    index: \"metrics\"\n    # Maximum HTTP connections to use simultaneously when sending data. Defaults to 100.\n    max_idle_conns: 200\n    # Whether to disable gzip compression over HTTP. Defaults to false.\n    disable_compression: false\n    # HTTP timeout when sending data. Defaults to 10s.\n    timeout: 10s\n    tls:\n      # Whether to skip checking the certificate of the HEC endpoint when sending data over HTTPS. Defaults to false.\n      insecure_skip_verify: false\n      # Path to the CA cert to verify the server being connected to.\n      ca_file: /certs/ExampleCA.crt\n      # Path to the TLS cert to use for client connections when TLS client auth is required.\n      cert_file: /certs/HECclient.crt\n      # Path to the TLS key to use for TLS required connections.\n      key_file: /certs/HECclient.key\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Receiver in YAML\nDESCRIPTION: Example configuration for the PostgreSQL receiver, including endpoint, credentials, database selection, collection interval, and TLS settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/postgresqlreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  postgresql:\n    endpoint: localhost:5432\n    transport: tcp\n    username: otel\n    password: ${env:POSTGRESQL_PASSWORD}\n    databases:\n      - otel\n    collection_interval: 10s\n    tls:\n      insecure: false\n      insecure_skip_verify: false\n      ca_file: /home/otel/authorities.crt\n      cert_file: /home/otel/mypostgrescert.crt\n      key_file: /home/otel/mypostgreskey.key\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Flink Metrics in YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to disable specific metrics in the Flink configuration. It shows the structure for disabling a metric by setting its 'enabled' property to false.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/flinkmetricsreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Extracting Attributes from Span Name in YAML\nDESCRIPTION: This configuration shows how to extract attributes from a span name using regular expressions. It includes options for stopping after the first match and keeping the original name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/spanprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspan/to_attributes:\n  name:\n    to_attributes:\n      rules:\n        - regexp-rule1\n        - regexp-rule2\n        - regexp-rule3\n      break_after_match: <true|false>\n      keep_original_name: <true|false>\n```\n\n----------------------------------------\n\nTITLE: Configuring Recombine Operator for Kubernetes CRI Logs\nDESCRIPTION: Configuration example that recombines Kubernetes logs in the CRI format by identifying the log tag that indicates whether an entry is part of a longer line (P) or the final entry (F).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: file_input\n  include:\n    - ./input.log\n- type: regex_parser\n  regex: '^(?P<timestamp>[^\\s]+) (?P<stream>\\w+) (?P<logtag>\\w) (?P<message>.*)'\n- type: recombine\n  combine_field: body.message\n  combine_with: \"\"\n  is_last_entry: \"body.logtag == 'F'\"\n  overwrite_with: \"newest\"\n```\n\n----------------------------------------\n\nTITLE: Regex Parser with Severity Configuration\nDESCRIPTION: Example showing how to use severity parsing with regex_parser operator\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_parser\n  regexp: '^StatusCode=(?P<severity_field>\\d{3}), Host=(?P<host>[^,]+)'\n  severity:\n    parse_from: body.severity_field\n    mapping:\n      warn: 5xx\n      error: 4xx\n      info: 3xx\n      debug: 2xx\n```\n\n----------------------------------------\n\nTITLE: Configuring LogicMonitor Exporter with Bearer Token in YAML\nDESCRIPTION: This snippet demonstrates how to configure the LogicMonitor exporter using a bearer token for authentication. It requires setting the endpoint and providing the bearer token in the Authorization header.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/logicmonitorexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  logicmonitor:\n    endpoint: \"https://<company_name>.logicmonitor.com/rest\"\n    headers:\n      Authorization: Bearer <bearer token of logicmonitor>\n```\n\n----------------------------------------\n\nTITLE: Advanced Expvar Receiver Configuration\nDESCRIPTION: Extended configuration for the Expvar Receiver that customizes the endpoint, timeout, collection interval, and selectively enables specific metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/expvarreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  expvar:\n    endpoint: \"http://localhost:8000/custom/path\"\n    timeout: 1s\n    collection_interval: 30s\n    metrics:\n      process.runtime.memstats.total_alloc:\n        enabled: true\n      process.runtime.memstats.mallocs:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Elasticsearch Metrics in YAML\nDESCRIPTION: Configuration example showing how to enable optional metrics in Elasticsearch that are not emitted by default. The configuration uses a YAML structure where each metric can be individually enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/elasticsearchreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring OpAMP Agent Extension in YAML\nDESCRIPTION: This snippet demonstrates how to configure the OpAMP agent extension in the OpenTelemetry Collector's configuration file. It shows the minimal required setup for connecting to an OpAMP server using WebSocket transport.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/opampextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  opamp:\n    server:\n      ws:\n        endpoint: wss://127.0.0.1:4320/v1/opamp\n```\n\n----------------------------------------\n\nTITLE: Slow Span Query in ClickHouse\nDESCRIPTION: SQL query to find slow spans in traces based on duration\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp,\n       TraceId,\n       SpanId,\n       ParentSpanId,\n       SpanName,\n       SpanKind,\n       ServiceName,\n       Duration,\n       StatusCode,\n       StatusMessage,\n       toString(SpanAttributes),\n       toString(ResourceAttributes),\n       toString(Events.Name),\n       toString(Links.TraceId)\nFROM otel_traces\nWHERE ServiceName = 'clickhouse-exporter'\n  AND Duration > 1 * 1e9\n  AND Timestamp >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: GeoIP Processor YAML Configuration\nDESCRIPTION: Example YAML configuration for the GeoIP processor showing how to specify the MaxMind provider, set the processing context to record level, and define custom IP address attributes to process.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/geoipprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: geoip\n    geoip:\n      providers:\n        maxmind:\n          database_path: /tmp/mygeodb\n      context: record\n      attributes: [client.address, source.address, custom.address]\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Storage Extension in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for setting up the Redis Storage extension in OpenTelemetry Collector. Shows both basic and advanced configuration with options for endpoint, password, database selection, key expiration, and prefix customization.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/storage/redisstorageextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  redis_storage:\n  redis_storage/all_settings:\n    endpoint: localhost:6379\n    password: \"\"\n    db: 0\n    expiration: 5m\n    prefix: test_\n\nservice:\n  extensions: [redis_storage, redis_storage/all_settings]\n  pipelines:\n    traces:\n      receivers: [nop]\n      exporters: [nop]\n\n# Data pipeline is required to load the config.\nreceivers:\n  nop:\nexporters:\n  nop:\n```\n\n----------------------------------------\n\nTITLE: Configuring SSH Check Receiver in YAML\nDESCRIPTION: Example configuration for the SSH Check Receiver. It specifies the endpoint, username, password, and collection interval for connecting to an SSH server.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sshcheckreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sshcheck:\n    endpoint: localhost:2222\n    username: otelu\n    password: $OTELP\n    collection_interval: 60s\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Service Receiver with Specific Services\nDESCRIPTION: Configuration example for monitoring specific Windows services with a custom collection interval. The receiver will only track the services explicitly listed in the include_services array.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsservicereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nwindowsservice:\n  collection_interval: <duration> # default = 1m\n  include_services:\n    - service1\n    - service2\n    - service3\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write Exporter in YAML\nDESCRIPTION: Basic configuration example for the Prometheus Remote Write Exporter, including endpoint specification and Write-Ahead-Log (WAL) settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  prometheusremotewrite:\n    endpoint: \"https://my-cortex:7900/api/v1/push\"\n    wal: # Enabling the Write-Ahead-Log for the exporter.\n      directory: ./prom_rw # The directory to store the WAL in\n      buffer_size: 100 # Optional count of elements to be read from the WAL before truncating; default of 300\n      truncate_frequency: 45s # Optional frequency for how often the WAL should be truncated. It is a time.ParseDuration; default of 1m\n    resource_to_telemetry_conversion:\n      enabled: true # Convert resource attributes to metric labels\n```\n\n----------------------------------------\n\nTITLE: JSON Webhook Payload Example for Single Log Record\nDESCRIPTION: Example of a JSON webhook payload when split_logs_at_newline is set to false. This configuration will create a single log record with the entire content as the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/webhookeventreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{\n\"name\": \"francis\",\n\"city\": \"newyork\"\n}\na fifth line\n```\n\n----------------------------------------\n\nTITLE: Using TruncateTime Converter for Time Rounding in OpenTelemetry\nDESCRIPTION: The TruncateTime Converter rounds a time value down to a multiple of the specified duration using Golang's time.Truncate function. It requires a time.Time object and a time.Duration, often created using the Duration converter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_63\n\nLANGUAGE: go\nCODE:\n```\nTruncateTime(time, duration)\n```\n\n----------------------------------------\n\nTITLE: Extracting Document ID and Keeping Original Span Name in YAML\nDESCRIPTION: This configuration extracts a document ID from the span name while keeping the original span name intact. It adds the extracted ID as a new attribute to the span.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/spanprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nspan/to_attributes_keep_original_name:\n  name:\n    to_attributes:\n      keep_original_name: true\n      rules:\n        - ^/api/v1/document/(?P<documentId>.*)/update$\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth2 Client Credentials Authenticator for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration demonstrates how to set up the OAuth2 Client Credentials authenticator extension and use it with both HTTP and gRPC exporters. It includes configuration for client credentials, token endpoint, scopes, TLS settings, timeout, and token expiry buffer.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/oauth2clientauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  oauth2client:\n    client_id: someclientid\n    client_secret: someclientsecret\n    endpoint_params:\n      audience: someaudience\n    token_url: https://example.com/oauth2/default/v1/token\n    scopes: [\"api.metrics\"]\n    # tls settings for the token client\n    tls:\n      insecure: true\n      ca_file: /var/lib/mycert.pem\n      cert_file: certfile\n      key_file: keyfile\n    # timeout for the token client\n    timeout: 2s\n    # buffer time before token expiry to refresh\n    expiry_buffer: 10s\n    \nreceivers:\n  hostmetrics:\n    scrapers:\n      memory:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  otlphttp/withauth:\n    endpoint: http://localhost:9000\n    auth:\n      authenticator: oauth2client\n      \n  otlp/withauth:\n    endpoint: 0.0.0.0:5000\n    tls:\n      ca_file: /tmp/certs/ca.pem\n    auth:\n      authenticator: oauth2client\n\nservice:\n  extensions: [oauth2client]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [otlphttp/withauth, otlp/withauth]\n```\n\n----------------------------------------\n\nTITLE: Mathematical and Utility Function Examples\nDESCRIPTION: Examples of using mathematical and utility functions like Log, Len, and IsValidLuhn.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_30\n\nLANGUAGE: Go\nCODE:\n```\nLog(span.attributes[\"duration_ms\"])\nLen(log.body)\nIsValidLuhn(\"17893729974\")\n```\n\n----------------------------------------\n\nTITLE: Configuring EC2 Resource Detection with Tag Filtering\nDESCRIPTION: YAML configuration example for EC2 resource detection that demonstrates how to specify tag patterns to include as resource attributes using regular expressions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/ec2:\n    detectors: [\"ec2\"]\n    ec2:\n      # A list of regex's to match tag keys to add as resource attributes can be specified\n      tags:\n        - ^tag1$\n        - ^tag2$\n        - ^label.*$\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Receiver in OpenTelemetry\nDESCRIPTION: Example configuration for the Snowflake metrics receiver showing required fields like username, password, account, warehouse and optional settings like collection interval and specific metric enabling/disabling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/snowflakereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  snowflake:\n    username: snowflakeuser\n    password: securepassword\n    account: bigbusinessaccount\n    warehouse: metricWarehouse\n    collection_interval: 5m\n    metrics:\n      snowflake.database.bytes_scanned.avg:\n        enabled: true\n      snowflake.query.bytes_deleted.avg:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Using Split Converter for String Separation in OpenTelemetry\nDESCRIPTION: The Split Converter separates a string by the specified delimiter and returns an array of substrings. It requires a target string and a delimiter string, returning an error if the target is not a string or doesn't exist.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_52\n\nLANGUAGE: go\nCODE:\n```\nSplit(target, delimiter)\n```\n\n----------------------------------------\n\nTITLE: Configuring ASAP Client Authentication with OTLP Exporters\nDESCRIPTION: Example configuration showing how to set up ASAP client authentication with both HTTP and gRPC OTLP exporters. Demonstrates setting up the extension with key parameters like key_id, issuer, audience, private_key, and TTL, along with the corresponding exporter configurations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/asapauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  asapclient:\n    # The `kid` as specified by the asap specification.\n    key_id: somekeyid\n    # The `iss` as specified by the asap specification.\n    issuer: someissuer\n    # The `aud` as specified by the asap specification.\n    audience:\n      - someservice\n      - someotherservice\n    # The private key of the client, used to sign the token. For an example, see `testdata/config.yaml`.\n    private_key: ${env:ASAP_PRIVATE_KEY}\n    # The time until expiry of each given token. The token will be cached and then re-provisioned upon expiry. \n    # For more info see the \"exp\" claim in the asap specification: https://s2sauth.bitbucket.io/spec/#access-token-generation\n    ttl: 60s\n    \nexporters:\n  otlphttp/withauth:\n    endpoint: http://localhost:9000\n    auth:\n      authenticator: asapclient\n\n  otlp/withauth:\n    endpoint: 0.0.0.0:5000\n    ca_file: /tmp/certs/ca.pem\n    auth:\n      authenticator: asapclient\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Kinesis Exporter in YAML\nDESCRIPTION: Example configuration for setting up the AWS Kinesis exporter with basic settings including stream name, region and IAM role. This configuration demonstrates the minimal required setup for exporting data to a Kinesis stream.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awskinesisexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  awskinesis:\n    aws:\n      stream_name: raw-trace-stream\n      region: us-east-1\n      role: arn:test-role\n```\n\n----------------------------------------\n\nTITLE: Adding a Value to Body Using Expression in YAML\nDESCRIPTION: This example shows how to add a value to the body using an expression with the 'add' operator in YAML configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: body.key2\n  value: EXPR(body.key1 + \"_suffix\")\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRole for OpenTelemetry Collector in Kubernetes\nDESCRIPTION: This YAML configuration defines a ClusterRole with permissions to access various Kubernetes resources required by the OpenTelemetry Collector. It grants read access to pods, services, nodes, and other cluster-wide resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - events\n  - namespaces\n  - namespaces/status\n  - nodes\n  - nodes/spec\n  - pods\n  - pods/status\n  - replicationcontrollers\n  - replicationcontrollers/status\n  - resourcequotas\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - daemonsets\n  - deployments\n  - replicasets\n  - statefulsets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - jobs\n  - cronjobs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n    - autoscaling\n  resources:\n    - horizontalpodautoscalers\n  verbs:\n    - get\n    - list\n    - watch\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Sumo Logic Extension in OpenTelemetry Collector\nDESCRIPTION: Example configuration showing how to set up the Sumo Logic extension with hostmetrics receiver and Sumo Logic exporter. Demonstrates basic configuration including installation token, collector name, and time zone settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sumologicextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  sumologic:\n    installation_token: <token>\n    collector_name: my_collector\n    time_zone: Europe/Warsaw\n\nreceivers:\n  hostmetrics:\n    collection_interval: 30s\n    scrapers:\n      load:\n\nprocessors:\n\nexporters:\n  sumologic:\n    auth:\n      authenticator: sumologic # Specify the name of the authenticator extension\n\nservice:\n  extensions: [sumologic]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [sumologic]\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Receiver with Connection Pool in YAML\nDESCRIPTION: Example configuration for the PostgreSQL receiver with connection pool settings, including max idle time, max lifetime, and connection limits.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/postgresqlreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  postgresql:\n    endpoint: localhost:5432\n    transport: tcp\n    username: otel\n    password: ${env:POSTGRESQL_PASSWORD}\n    connection_pool:\n      max_idle_time: 10m\n      max_lifetime: 0\n      max_idle: 2\n      max_open: 5\n```\n\n----------------------------------------\n\nTITLE: Calculating Collection Interval for GitHub API Rate Limits\nDESCRIPTION: Mathematical formula to calculate the ideal collection interval based on the number of repositories and hourly rate limit. This helps avoid hitting GitHub API rate limits when scraping repositories.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/githubreceiver/internal/scraper/githubscraper/README.md#2025-04-10_snippet_0\n\nLANGUAGE: math\nCODE:\n```\n\\text{collection\\_interval (seconds)} = \\frac{4n}{r/3600}\n```\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    \\text{where:} \\\\\n    n &= \\text{number of repositories} \\\\\n    r &= \\text{hourly rate limit} \\\\\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Transform Processor Feature Gate Configuration\nDESCRIPTION: Example configuration for enabling the transform.flatten.logs feature gate, which allows distinct copying of resource and scope for each log record before transformation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  flatten_data: true\n  log_statements:\n    - set(resource.attributes[\"to\"], log.attributes[\"from\"])\n```\n\n----------------------------------------\n\nTITLE: UnixMicro Time Converter in Golang\nDESCRIPTION: The UnixMicro Converter returns the microseconds elapsed since January 1, 1970 UTC from a time.Time object. Returns an int64 value representing microseconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_65\n\nLANGUAGE: go\nCODE:\n```\nUnixMicro(Time(\"02/04/2023\", \"%m/%d/%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics Configuration in YAML\nDESCRIPTION: Configuration snippet showing how to enable optional network metrics in the OpenTelemetry Collector. This allows users to turn on specific metrics that are disabled by default.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/networkscraper/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: K8s Attributes OTTL Pattern Extraction\nDESCRIPTION: Updated configuration using OTTL ExtractPatterns function to extract and transform annotation values, replacing the deprecated regex fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n- set(cache[\"annotations\"], ExtractPatterns(attributes[\"k8s.pod.annotations[\"annotation2\"], \"field=(?P<value>.+)\"))\n- set(k8s.pod.annotations[\"a2\"], cache[\"annotations\"][\"value\"])\n```\n\n----------------------------------------\n\nTITLE: Full Sample Collector Configuration with StatsD Receiver\nDESCRIPTION: A complete example of an OpenTelemetry Collector configuration including the StatsD receiver, file exporter, and a metrics pipeline.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/statsdreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  statsd:\n    endpoint: \"localhost:8125\" # default\n    aggregation_interval: 60s  # default\n    enable_metric_type: false   # default\n    is_monotonic_counter: false # default\n    timer_histogram_mapping:\n      - statsd_type: \"histogram\"\n        observer_type: \"histogram\"\n        histogram:\n          max_size: 50\n      - statsd_type: \"distribution\"\n        observer_type: \"histogram\"\n        histogram: \n          max_size: 50    \n      - statsd_type: \"timing\"\n        observer_type: \"summary\"\n\nexporters:\n  file:\n    path: ./test.json\n\nservice:\n  pipelines:\n    metrics:\n     receivers: [statsd]\n     exporters: [file]\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Metrics in OpenTelemetry Collector\nDESCRIPTION: YAML configuration snippet showing how to disable a specific MongoDB metric in the OpenTelemetry Collector configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Receiver in YAML\nDESCRIPTION: Example YAML configuration for the Elasticsearch receiver, showing how to set various options including nodes to monitor, indices to track, authentication, and collection interval.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/elasticsearchreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  elasticsearch:\n    metrics:\n      elasticsearch.node.fs.disk.available:\n        enabled: false\n    nodes: [\"_local\"]\n    skip_cluster_metrics: true\n    indices: [\".geoip_databases\"]\n    endpoint: http://localhost:9200\n    username: otel\n    password: password\n    collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Basic Coralogix Exporter Configuration in YAML\nDESCRIPTION: Basic configuration example for the Coralogix exporter showing essential settings including domain, private key, application naming, and queue configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  coralogix:\n    domain: \"coralogix.com\"\n    private_key: \"xxx\"\n    application_name_attributes:\n    - \"service.namespace\"\n    subsystem_name_attributes:\n    - \"service.name\"\n    application_name: \"MyBusinessEnvironment\"\n    subsystem_name: \"MyBusinessSystem\"\n    sending_queue:\n      sizer: bytes\n      batch:\n        min_size: 4194304 \n        max_size: 8388608\n    timeout: 30s\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRoleBinding\nDESCRIPTION: Kubernetes ClusterRoleBinding to associate the ServiceAccount with the ClusterRole.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sobjectsreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: otelcontribcol\nsubjects:\n- kind: ServiceAccount\n  name: otelcontribcol\n  namespace: default\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector for ECS Task-Level Metrics in YAML\nDESCRIPTION: This configuration collects 8 task-level metrics from the ECS task metadata endpoint, filters and transforms them, and exports them to Amazon CloudWatch using the awsemf exporter. It includes receivers, processors, and exporters sections, as well as a service pipeline definition.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsecscontainermetricsreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awsecscontainermetrics: # collect 52 metrics\n\nprocessors:\n  filter: # filter metrics\n    metrics:\n      include:\n        match_type: strict\n        metric_names: # select only 8 task level metrics out of 52\n          - ecs.task.memory.reserved\n          - ecs.task.memory.utilized\n          - ecs.task.cpu.reserved\n          - ecs.task.cpu.utilized\n          - ecs.task.network.rate.rx\n          - ecs.task.network.rate.tx\n          - ecs.task.storage.read_bytes\n          - ecs.task.storage.write_bytes\n  metricstransform: # update metric names\n    transforms:\n      - include: ecs.task.memory.utilized\n        action: update\n        new_name: MemoryUtilized\n      - include: ecs.task.memory.reserved\n        action: update\n        new_name: MemoryReserved\n      - include: ecs.task.cpu.utilized\n        action: update\n        new_name: CpuUtilized\n      - include: ecs.task.cpu.reserved\n        action: update\n        new_name: CpuReserved\n      - include: ecs.task.network.rate.rx\n        action: update\n        new_name: NetworkRxBytes\n      - include: ecs.task.network.rate.tx\n        action: update\n        new_name: NetworkTxBytes\n      - include: ecs.task.storage.read_bytes\n        action: update\n        new_name: StorageReadBytes\n      - include: ecs.task.storage.write_bytes\n        action: update\n        new_name: StorageWriteBytes\n  resource:\n    attributes: # rename resource attributes which will be used as dimensions\n      - key: ClusterName\n        from_attribute: aws.ecs.cluster.name\n        action: insert\n      - key: aws.ecs.cluster.name\n        action: delete\n      - key: ServiceName\n        from_attribute: aws.ecs.service.name\n        action: insert\n      - key: aws.ecs.service.name\n        action: delete\n      - key: TaskId\n        from_attribute: aws.ecs.task.id\n        action: insert\n      - key: aws.ecs.task.id\n        action: delete\n      - key: TaskDefinitionFamily\n        from_attribute: aws.ecs.task.family\n        action: insert\n      - key: aws.ecs.task.family\n        action: delete\nexporters:\n  awsemf:\n    namespace: ECS/ContainerInsights\n    log_group_name: '/aws/ecs/containerinsights/{ClusterName}/performance'\n    log_stream_name: '{TaskId}'  # TaskId placeholder will be replaced with actual value\n    resource_to_telemetry_conversion:\n      enabled: true\n    dimension_rollup_option: NoDimensionRollup\n    metric_declarations:\n      dimensions: [ [ ClusterName ], [ ClusterName, TaskDefinitionFamily ] ]\n      metric_name_selectors: [ . ]\nservice:\n  pipelines:\n    metrics:\n      receivers: [awsecscontainermetrics ]\n      processors: [filter, metricstransform, resource]\n      exporters: [ awsemf ]\n```\n\n----------------------------------------\n\nTITLE: Attribute-Based Span Query in ClickHouse\nDESCRIPTION: SQL query to find spans with specific attributes in traces\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp,\n       TraceId,\n       SpanId,\n       ParentSpanId,\n       SpanName,\n       SpanKind,\n       ServiceName,\n       Duration,\n       StatusCode,\n       StatusMessage,\n       toString(SpanAttributes),\n       toString(ResourceAttributes),\n       toString(Events.Name),\n       toString(Links.TraceId)\nFROM otel_traces\nWHERE ServiceName = 'clickhouse-exporter'\n  AND SpanAttributes['peer.service'] = 'telemetrygen-server'\n  AND Timestamp >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: AWS CloudMap Resolver Configuration\nDESCRIPTION: Configuration example for load balancing using AWS CloudMap service discovery, including namespace and service name settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: localhost:4317\n\nprocessors:\n\nexporters:\n  loadbalancing:\n    protocol:\n      otlp:\n        # all options from the OTLP exporter are supported\n        # except the endpoint\n        timeout: 3s\n    resolver:\n      aws_cloud_map:\n        namespace: aws-namespace\n        service_name: aws-otel-col-service-name\n        interval: 30s\n\nservice:\n  pipelines:\n    traces:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n    logs:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n```\n\n----------------------------------------\n\nTITLE: Configuring Journald Input in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML snippet demonstrates how to configure the journald input with multiple filtering options. It includes settings for units, priority, identifiers, and specific matches.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/journaldreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n  matches:\n    - _SYSTEMD_UNIT: ssh\n    - _SYSTEMD_UNIT: kubelet\n      _UID: \"1000\"\n  units:\n    - kubelet\n    - systemd\n  priority: info\n  identifiers:\n    - systemd\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Zstd Compression Levels in YAML\nDESCRIPTION: Example showing how to configure two exporters with different Zstd compression levels. One uses level 10 for best compression, while the other uses level 1 for fastest compression.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/otelarrowexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otelarrow/best:\n    compression: zstd  # describes gRPC-level compression (default \"zstd\")\n    arrow:\n      zstd:\n        level: 10      # describes gRPC-level compression level (default 5)\n  otelarrow/fastest:\n    compression: zstd\n    arrow:\n      zstd:\n        level: 1       # 1 is the \"fastest\" compression level\n```\n\n----------------------------------------\n\nTITLE: Configuring Interval Processor in YAML\nDESCRIPTION: Configuration schema for the interval processor showing optional settings for interval duration and pass-through options. The interval setting controls the export frequency of aggregated metrics, while pass_through options determine if gauges and summaries should bypass aggregation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/intervalprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninterval:\n  # The interval in which the processor should export the aggregated metrics. \n  [ interval: <duration> | default = 60s ]\n  \n  pass_through:\n    # Whether gauges should be aggregated or passed through to the next component as they are\n    [ gauge: <bool> | default = false ]\n    # Whether summaries should be aggregated or passed through to the next component as they are\n    [ summary: <boo>l | default = false ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Faro Exporter in YAML for OpenTelemetry Collector\nDESCRIPTION: Basic configuration example for the Faro exporter that demonstrates how to set up the exporter with an endpoint URL, custom timeout, and API key header. This is the minimum configuration needed to start sending telemetry data to a Faro endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/faroexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  faro:\n    endpoint: https://faro.example.com/collect\n    timeout: 10s\n    headers:\n      X-API-Key: \"my-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Routing Logs Based on Severity and Tenant HTTP Header in OpenTelemetry\nDESCRIPTION: Demonstrates advanced log routing using both log context and request context. Routes low-level logs (below ERROR) to cheap storage and routes the remaining logs based on the 'X-Tenant' HTTP header in the request.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    otlp:\n\nexporters:\n  file/cheap:\n    path: ./cheap.log\n  file/acme:\n    path: ./acme.log\n  file/ecorp:\n    path: ./ecorp.log\n\nconnectors:\n  routing:\n    table:\n      - context: log\n        condition: severity_number < SEVERITY_NUMBER_ERROR\n        pipelines: [logs/cheap]\n      - context: request\n        condition: request[\"X-Tenant\"] == \"acme\"\n        pipelines: [logs/acme]\n      - context: request\n        condition: request[\"X-Tenant\"] == \"ecorp\"\n        pipelines: [logs/ecorp]\n\nservice:\n  pipelines:\n    logs/in:\n      receivers: [otlp]\n      exporters: [routing]\n    logs/cheap:\n      receivers: [routing]\n      exporters: [file/cheap]\n    logs/acme:\n      receivers: [routing]\n      exporters: [file/acme]\n    logs/ecorp:\n      receivers: [routing]\n      exporters: [file/ecorp]\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Identity Authentication (User-based) in Azure Auth Extension\nDESCRIPTION: YAML configuration example for the Azure authenticator extension using user-based managed identity authentication. This requires specifying a client_id for the managed identity.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/azureauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  azureauth:\n    managed_identity:\n      client_id: ${CLIENT_ID}\n```\n\n----------------------------------------\n\nTITLE: Input/Output Example for Multi-Section Field Retention\nDESCRIPTION: Example showing how the retain operator selectively keeps fields from each section (resource, attributes, body) of the log entry while removing all other fields. Only the specifically identified fields remain in the output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md#2025-04-10_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n     \"key1\": \"val1\",\n     \"key2\": \"val2\"\n  },\n  \"attributes\": {\n     \"key3\": \"val3\",\n     \"key4\": \"val4\"\n  },\n  \"body\": {\n    \"key5\": \"val5\",\n    \"key6\": \"val6\",\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n     \"key1\": \"val1\",\n  },\n  \"attributes\": {\n     \"key3\": \"val3\",\n  },\n  \"body\": {\n    \"key5\": \"val5\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Alert Listening in YAML\nDESCRIPTION: Configuration to listen for alerts from MongoDB Atlas using the default listen mode.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    alerts:\n      enabled: true\n      secret: \"some_secret\"\n      endpoint: \"0.0.0.0:7706\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SAP HANA Receiver in OpenTelemetry\nDESCRIPTION: YAML configuration example showing how to set up the SAP HANA receiver with basic settings including endpoint, collection interval, and metric filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/saphanareceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  saphana:\n    endpoint: \"localhost:33015\"\n    collection_interval: 60s\n    metrics:\n      saphana.cpu.used:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Honeycomb Marker Exporter in YAML\nDESCRIPTION: Example configuration for the Honeycomb Marker Exporter showing how to set up markers for Kubernetes backoff events. The configuration demonstrates using environment variables for API keys and setting up log condition rules.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/honeycombmarkerexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  honeycombmarker:\n    api_key: {{env:HONEYCOMB_API_KEY}}\n    markers:\n      # Creates a new marker anytime the exporter sees a k8s event with a reason of Backoff\n      - type: k8s-backoff-events\n        rules:\n          log_conditions:\n            - IsMap(body) and IsMap(body[\"object\"]) and body[\"object\"][\"reason\"] == \"Backoff\"\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Exporter in YAML\nDESCRIPTION: Example YAML configuration for the InfluxDB exporter, including endpoint, authentication, dimensions, and queue settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/influxdbexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  influxdb:\n    endpoint: http://localhost:8080\n    timeout: 500ms\n    org: my-org\n    bucket: my-bucket\n    token: my-token\n    span_dimensions:\n    - service.name\n    - span.name\n    log_record_dimensions:\n    - service.name\n    metrics_schema: telegraf-prometheus-v1\n\n    sending_queue:\n      enabled: true\n      num_consumers: 3\n      queue_size: 10\n\n    retry_on_failure:\n      enabled: true\n      initial_interval: 1s\n      max_interval: 3s\n      max_elapsed_time: 10s\n```\n\n----------------------------------------\n\nTITLE: Attribute-Based Log Query in ClickHouse\nDESCRIPTION: SQL query to find logs with specific attribute values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE LogAttributes['container_name'] = '/example_flog_1'\n  AND TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Testing Metrics Scraper with CompareMetrics in Go\nDESCRIPTION: Example of how to test a metrics scraper by comparing actual scraped metrics with expected metrics from a JSON file. Uses pmetrictest.CompareMetrics with options to ignore timestamps.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/pdatatest/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfunc TestMetricsScraper(t *testing.T) {\n\tscraper := newScraper(componenttest.NewNopReceiverCreateSettings(), createDefaultConfig().(*Config))\n\trequire.NoError(t, scraper.start(context.Background(), componenttest.NewNopHost()))\n\tactualMetrics, err := require.NoError(t, scraper.scrape(context.Background()))\n\trequire.NoError(t, err)\n\n\texpectedFile, err := readMetrics(filepath.Join(\"testdata\", \"expected.json\"))\n\trequire.NoError(err)\n\n\trequire.NoError(t, pmetrictest.CompareMetrics(expectedMetrics, actualMetrics, pmetrictest.IgnoreStartTimestamp(), \n\t\tpmetrictest.IgnoreTimestamp()))\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring New Relic Exporter Timestamp in YAML\nDESCRIPTION: YAML configuration snippet for fixing timestamp value for cumulative metrics in the New Relic exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_46\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  newrelic:\n    # Fix timestamp value for cumulative metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Query Receiver with Metrics and Logs\nDESCRIPTION: Example configuration for SQL Query receiver using PostgreSQL driver. Shows configuration for both log collection with attribute tracking and metric generation from aggregated query results. Includes datasource configuration and query definitions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlqueryreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlquery:\n    driver: postgres\n    datasource: \"host=localhost port=5432 user=postgres password=s3cr3t sslmode=disable\"\n    storage: file_storage\n    queries:\n      - sql: \"select * from my_logs where log_id > $$1\"\n        tracking_start_value: \"10000\"\n        tracking_column: log_id\n        logs:\n          - body_column: log_body\n            attribute_columns: [ \"log_attribute_1\", \"log_attribute_2\" ]\n      - sql: \"select count(*) as count, genre from movie group by genre\"\n        metrics:\n          - metric_name: movie.genres\n            value_column: \"count\"\n            attribute_columns: [\"genre\"]\n            static_attributes:\n              dbinstance: mydbinstance\n```\n\n----------------------------------------\n\nTITLE: Configuring UDP Receiver for Syslog in OpenTelemetry Collector\nDESCRIPTION: This YAML snippet shows the configuration for a UDP receiver for Syslog using the RFC3164 protocol. It specifies the listen address and sets the timezone to UTC.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/syslogreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  syslog:\n    udp:\n      listen_address: \"0.0.0.0:54526\"\n    protocol: rfc3164\n    location: UTC\n```\n\n----------------------------------------\n\nTITLE: Configuring Batching for Coralogix Exporter in YAML\nDESCRIPTION: This YAML configuration enables and configures batching capabilities for the Coralogix exporter. It sets the flush timeout, minimum and maximum number of items to batch.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  coralogix:\n    batcher:\n      enabled: true # Enable batching\n      flush_timeout: 3s # Flush timeout\n      min_size_items: 8888 # Minimum number of items to flush\n      max_size_items: 10000 # Maximum number of items to batch\n```\n\n----------------------------------------\n\nTITLE: Resource Attributes Configuration for Single-Writer Principle in OpenTelemetry YAML\nDESCRIPTION: Configuration showing the required resource attributes added by the signaltometrics component to maintain the single-writer principle. These attributes include service name, namespace, and instance ID of the OpenTelemetry collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/signaltometricsconnector/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsignaltometrics.service.name: <service_name_of_the_otel_collector>\nsignaltometrics.service.namespace: <service_namespace_of_the_otel_collector>\nsignaltometrics.service.instance.id: <service_instance_id_of_the_otel_collector>\n```\n\n----------------------------------------\n\nTITLE: Configuring Solace Receiver with Basic Authentication in YAML\nDESCRIPTION: This snippet demonstrates a basic configuration for the Solace receiver using SASL plain authentication. It specifies the broker address, authentication credentials, and the telemetry queue to receive trace data from.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/solacereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  solace:\n    broker: [localhost:5671]\n    auth:\n      sasl_plain:\n        username: otel\n        password: otel01$\n    queue: queue://#telemetry-profile123\n\nservice:\n  pipelines:\n    traces:\n      receivers: [solace]\n```\n\n----------------------------------------\n\nTITLE: Basic AWS S3 Exporter Configuration in YAML\nDESCRIPTION: Example configuration for AWS S3 exporter showing basic setup with region and bucket configuration, plus optional queue and timeout settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awss3exporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  awss3:\n    s3uploader:\n      region: 'eu-central-1'\n      s3_bucket: 'databucket'\n      s3_prefix: 'metric'\n\n    # Optional (disabled by default)\n    sending_queue:\n      enabled: true\n      num_consumers: 10\n      queue_size: 100\n\n    # Optional (5s by default)\n    timeout: 20s\n```\n\n----------------------------------------\n\nTITLE: daemonset-file Mode Architecture\nDESCRIPTION: Diagram showing the architecture of the daemonset-file mode, which collects logs from files by calculating actual file paths using graph driver and mount point information from container runtimes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/design.md#2025-04-10_snippet_2\n\nLANGUAGE: ascii\nCODE:\n```\n  k8sapi     ┌──────────────┐    docker/cri-containerd\n    │        │              │             │\n    └───────▶│    Source    │◀────────────┘\n   medatada  │              │ graph driver, \n             │              │ mount points, env\n             └──────────────┘\n                  │ assosiate\n                  ▼\n              ┌──────────────┐\n              │   Poller     │ find files based on includes mount point.\n              └──────────────┘\n                  │ create\n                  ▼\n              ┌──────────────┐\n              │   Reader     │\n              └──────────────┘\n                  │ read files\n                  ▼\n                 files\n```\n\n----------------------------------------\n\nTITLE: Adding an Object to Body in YAML\nDESCRIPTION: This configuration demonstrates how to add a nested object to the body of an entry using the 'add' operator in YAML.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: body.key2\n  value:\n    nestedkey: nestedvalue\n```\n\n----------------------------------------\n\nTITLE: Routing Logs to Different Parsers\nDESCRIPTION: Demonstrates how to route logs to different parsers based on the format field in the body. Routes JSON format logs to json parser and syslog format to syslog parser.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/router.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: router\n  routes:\n    - output: my_json_parser\n      expr: 'body.format == \"json\"'\n    - output: my_syslog_parser\n      expr: 'body.format == \"syslog\"'\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Kubeconfig Authentication in YAML\nDESCRIPTION: Example configuration for using kubeconfig authentication to access Kubelet metrics via the API server proxy, specifying a context and using environment variables for the node name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 20s\n    auth_type: \"kubeConfig\"\n    context: \"my-context\"\n    insecure_skip_verify: true\n    endpoint: \"${env:K8S_NODE_NAME}\"\nexporters:\n  file:\n    path: \"fileexporter.txt\"\nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n      exporters: [file]\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Exceptions Connector Pipeline with Prometheus and Loki in YAML\nDESCRIPTION: A more complex configuration example for the exceptions connector, demonstrating how to set up pipelines using OTLP receiver, Prometheus remote write exporter, and Loki exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/exceptionsconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n      http:\n\nexporters:\n  prometheusremotewrite:\n    endpoint: http://prometheus:9090/api/v1/write\n  loki:\n    endpoint: http://loki:3100/loki/api/v1/push\n\nconnectors:\n  exceptions:\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [exceptions]\n    metrics:\n      receivers: [exceptions]\n      exporters: [prometheusremotewrite]\n    logs:\n      receivers: [exceptions]\n      exporters: [loki]\n```\n\n----------------------------------------\n\nTITLE: Configuring Named Pipe Receiver in OpenTelemetry Collector\nDESCRIPTION: Example configuration for setting up the Named Pipe Receiver. This configuration specifies the path to the named pipe (/tmp/pipe) and sets the file permissions mode to 0600.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/namedpipereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  namedpipe:\n    path: /tmp/pipe\n    mode: 0600\n```\n\n----------------------------------------\n\nTITLE: Basic Schema Processor Configuration\nDESCRIPTION: Example configuration showing how to set up the schema processor with prefetch URLs and target schemas for translation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/schemaprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  schema:\n    prefetch:\n      - https://opentelemetry.io/schemas/1.9.0\n    targets:\n      - https://opentelemetry.io/schemas/1.6.1\n      - http://example.com/telemetry/schemas/1.0.1\n```\n\n----------------------------------------\n\nTITLE: Querying Non-verbose Pipeline Health Status in JSON\nDESCRIPTION: Example JSON response for a non-verbose pipeline health status query. It includes only the overall pipeline status without component details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"start_time\": \"2024-01-18T17:39:15.87324-08:00\",\n    \"healthy\": true,\n    \"status\": \"StatusOK\",\n    \"status_time\": \"2024-01-18T17:39:15.874236-08:00\"\n}\n```\n\n----------------------------------------\n\nTITLE: Namespace-scoped RBAC Configuration for k8sattributes Processor\nDESCRIPTION: YAML configuration for setting up namespace-scoped RBAC permissions for the k8sattributes processor. This includes creating a ServiceAccount, Role, and RoleBinding to grant necessary permissions for accessing Kubernetes resources within a specific namespace.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: otel-collector\n  namespace: <OTEL_COL_NAMESPACE>\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: otel-collector\n  namespace: <WORKLOAD_NAMESPACE>\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"replicasets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: otel-collector\n  namespace: <WORKLOAD_NAMESPACE>\nsubjects:\n- kind: ServiceAccount\n  name: otel-collector\n  namespace: <OTEL_COL_NAMESPACE>\nroleRef:\n  kind: Role\n  name: otel-collector\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger Receiver Protocols in YAML\nDESCRIPTION: This snippet demonstrates how to configure the Jaeger receiver with different protocols. It shows a basic configuration with just gRPC and another configuration specifying a custom endpoint for gRPC.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/jaegerreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  jaeger:\n    protocols:\n      grpc:\n  jaeger/withendpoint:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:14260\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Deduplication with Conditional Processing in OpenTelemetry YAML\nDESCRIPTION: This configuration example demonstrates how to set up the logdedup processor to only deduplicate logs that match specific conditions. It filters logs where either the Attribute 'ID' equals 1 or the Resource Attribute 'service.name' equals 'my-service'. The configuration also specifies a 60-second deduplication interval, adds a dedup_count attribute, and sets the timezone to America/Los_Angeles.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logdedupprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    filelog:\n        include: [./example/*.log]\nprocessors:\n    logdedup:\n        conditions:\n            - attributes[\"ID\"] == 1\n            - resource.attributes[\"service.name\"] == \"my-service\"\n        interval: 60s\n        log_count_attribute: dedup_count\n        timezone: 'America/Los_Angeles'\nexporters:\n    googlecloud:\n\nservice:\n    pipelines:\n        logs:\n            receivers: [filelog]\n            processors: [logdedup]\n            exporters: [googlecloud]\n```\n\n----------------------------------------\n\nTITLE: Configuring Memcached Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the Memcached receiver in OpenTelemetry Collector. It defines how to connect to a Memcached instance to collect metrics, specifying endpoint, collection interval, and transport protocol.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/memcachedreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  memcached:\n    endpoint: \"localhost:11211\"\n    collection_interval: 10s\n    transport: tcp\n```\n\n----------------------------------------\n\nTITLE: Configuring Skywalking Receiver with GRPC and HTTP Protocols\nDESCRIPTION: Example configuration for setting up the Skywalking receiver with both gRPC and HTTP protocols. The configuration includes protocol-specific endpoints and pipeline configurations for both traces and metrics collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/skywalkingreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  skywalking:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:11800\n      http:\n        endpoint: 0.0.0.0:12800\n\nservice:\n  pipelines:\n    traces:\n      receivers: [skywalking]\n    metrics:\n      receivers: [skywalking]\n      \n```\n\n----------------------------------------\n\nTITLE: Configuring Google Managed Service for Prometheus Exporter in YAML\nDESCRIPTION: Example YAML configuration for setting up the Google Managed Service for Prometheus Exporter with receivers, processors, and exporters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlemanagedprometheusexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    prometheus:\n        config:\n          scrape_configs:\n            # Add your prometheus scrape configuration here.\n            # Using kubernetes_sd_configs with namespaced resources (e.g. pod)\n            # ensures the namespace is set on your metrics.\n            - job_name: 'kubernetes-pods'\n                kubernetes_sd_configs:\n                - role: pod\n                relabel_configs:\n                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n                action: keep\n                regex: true\n                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n                action: replace\n                target_label: __metrics_path__\n                regex: (.+)\n                - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n                action: replace\n                regex: (.+):(?:\\d+);(\\d+)\n                replacement: $$1:$$2\n                target_label: __address__\n                - action: labelmap\n                regex: __meta_kubernetes_pod_label_(.+)\nprocessors:\n    batch:\n        # batch metrics before sending to reduce API usage\n        send_batch_max_size: 200\n        send_batch_size: 200\n        timeout: 5s\n    memory_limiter:\n        # drop metrics if memory usage gets too high\n        check_interval: 1s\n        limit_percentage: 65\n        spike_limit_percentage: 20\n    resourcedetection:\n        # detect cluster name and location\n        detectors: [gcp]\n        timeout: 10s\n    transform:\n      # \"location\", \"cluster\", \"namespace\", \"job\", \"instance\", and \"project_id\" are reserved, and \n      # metrics containing these labels will be rejected.  Prefix them with exported_ to prevent this.\n      metric_statements:\n      - context: datapoint\n        statements:\n        - set(attributes[\"exported_location\"], attributes[\"location\"])\n        - delete_key(attributes, \"location\")\n        - set(attributes[\"exported_cluster\"], attributes[\"cluster\"])\n        - delete_key(attributes, \"cluster\")\n        - set(attributes[\"exported_namespace\"], attributes[\"namespace\"])\n        - delete_key(attributes, \"namespace\")\n        - set(attributes[\"exported_job\"], attributes[\"job\"])\n        - delete_key(attributes, \"job\")\n        - set(attributes[\"exported_instance\"], attributes[\"instance\"])\n        - delete_key(attributes, \"instance\")\n        - set(attributes[\"exported_project_id\"], attributes[\"project_id\"])\n        - delete_key(attributes, \"project_id\")\n\nexporters:\n    googlemanagedprometheus:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [memory_limiter, batch, transform, resourcedetection]\n      exporters: [googlemanagedprometheus]\n```\n\n----------------------------------------\n\nTITLE: Configuring Leader Elector Extension in YAML\nDESCRIPTION: This snippet demonstrates how to configure the k8s_leader_elector extension in the OpenTelemetry Collector configuration file. It includes settings for authentication, lease name, and namespace.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/k8sleaderelector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  my_awesome_receiver:\n    k8s_leader_elector: k8s_leader_elector\nextensions:\n  k8s_leader_elector:\n    auth_type: kubeConfig\n    lease_name: foo\n    lease_namespace: default\n\nservice:\n  extensions: [k8s_leader_elector]\n  pipelines:\n    metrics:\n      receivers: [my_awesome_receiver]\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Loki Exporter\nDESCRIPTION: Example YAML configuration for the OpenTelemetry Collector Loki exporter, showing how to set the endpoint and control default labels.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  loki:\n    endpoint: https://loki.example.com:3100/loki/api/v1/push\n    default_labels_enabled:\n      exporter: false\n      job: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus API Server in OpenTelemetry Collector\nDESCRIPTION: Example configuration for enabling the Prometheus API server in the OpenTelemetry Collector. This configures the server to expose Prometheus API endpoints on localhost:9090 for accessing targets, metadata, and other Prometheus information.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  prometheus:\n    api_server:\n      enabled: true\n      server_config:\n        endpoint: \"localhost:9090\"\n```\n\n----------------------------------------\n\nTITLE: Configuring K8s Attributes Agent Passthrough\nDESCRIPTION: Basic configuration for k8sattributes processor in agent mode with passthrough enabled to detect IP addresses without making K8s API calls.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nk8sattributes:\n  passthrough: true\n```\n\n----------------------------------------\n\nTITLE: Using HasAttrOnDatapoint Function in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop metrics containing a specific attribute key and value on any datapoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# Drops metrics containing the 'bad.metric' attribute key and 'true' value\nfilter/keep_good_metrics:\n  error_mode: ignore\n  metrics:\n    metric:\n      - 'HasAttrOnDatapoint(\"bad.metric\", \"true\")'\n```\n\n----------------------------------------\n\nTITLE: Configuring URI Parser for Absolute URI Parsing in YAML\nDESCRIPTION: This snippet shows how to configure the uri_parser operator to parse the body.message field as an absolute URI. It demonstrates the YAML configuration and provides example input and output JSON.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/uri_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: uri_parser\n  parse_from: body.message\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"https://dev:pass@google.com/app?user_id=2&token=001\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"host\": \"google.com\",\n    \"path\": \"/app\",\n    \"query\": {\n      \"user_id\": [\n        \"2\"\n      ],\n      \"token\": [\n        \"001\"\n      ]\n    },\n    \"scheme\": \"https\",\n    \"user\": \"dev\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Redis Metrics in YAML\nDESCRIPTION: Configuration example for enabling optional Redis metrics that are not collected by default. Shows the YAML structure for activating specific metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/redisreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver for Compressed Log Files in YAML\nDESCRIPTION: This configuration shows how to set up the Filelog receiver to read gzip compressed log files. It specifies the compression type as 'gzip' for files with the .gz extension.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include:\n    - /var/log/example/compressed.log.gz\n    compression: gzip\n```\n\n----------------------------------------\n\nTITLE: Configuring Pulsar Exporter in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the Pulsar exporter showing how to set up basic connectivity, TLS authentication, and timeouts. This configuration specifies the Pulsar endpoint, topic name, encoding format, and security settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/pulsarexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  pulsar:\n    endpoint: pulsar://localhost:6650\n    topic: otlp-spans\n    encoding: otlp_proto\n    auth:\n      tls:\n        cert_file: cert.pem\n        key_file: key.pem\n    timeout: 10s\n    tls_allow_insecure_connection: false\n    tls_trust_certs_file_path: ca.pem\n```\n\n----------------------------------------\n\nTITLE: Legacy Endpoint Configuration\nDESCRIPTION: Pre-v0.76.0 configuration example showing endpoint-based setup for traces, metrics, and logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  coralogix:\n    traces:\n      endpoint: \"ingress.coralogix.com:443\"\n    metrics:\n      endpoint: \"ingress.coralogix.com:443\"\n    logs:\n      endpoint: \"ingress.coralogix.com:443\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Exporter Attributes in YAML\nDESCRIPTION: YAML configuration snippet for updating Datadog exporter attributes to tags mapping and setting consistent hostname and default metrics behavior.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_44\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  datadog:\n    # Update attributes to tags mapping\n    # Consistent hostname and default metrics behavior\n```\n\n----------------------------------------\n\nTITLE: Enabling Feature Gates for Prometheus Receiver\nDESCRIPTION: Command-line options to enable various feature gates for the Prometheus receiver, including support for retrieving start time from created metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n\"--feature-gates=receiver.prometheusreceiver.UseCreatedMetric\"\n```\n\n----------------------------------------\n\nTITLE: Dropping Data Based on Resource Attribute in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop traces based on a Kubernetes pod name pattern using resource attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    error_mode: ignore\n    traces:\n      span:\n        - IsMatch(resource.attributes[\"k8s.pod.name\"], \"my-pod-name.*\")\n```\n\n----------------------------------------\n\nTITLE: Fixing Invalid OTTL Statement Combinations in Transform Processor\nDESCRIPTION: Example showing how to correctly separate incompatible OTTL statements into different groups when they can't be used together in the same context.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetric_statements:\n  - statements:\n    - convert_sum_to_gauge() where metric.name == \"system.processes.count\"\n    - limit(datapoint.attributes, 100, [\"host.name\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter with Access Token and Headers in YAML\nDESCRIPTION: This example configuration for the SignalFx exporter includes settings for access token, realm, timeout, and custom headers. It demonstrates how to set up basic authentication and additional HTTP headers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    access_token: <replace_with_actual_access_token>\n    access_token_passthrough: true\n    headers:\n      added-entry: \"added value\"\n      dot.test: test\n    realm: us1\n    timeout: 5s\n    max_idle_conns: 80\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Receiver with Service Principal Authentication\nDESCRIPTION: Example configuration for Azure Blob Receiver using service principal authentication method. Includes settings for tenant ID, client credentials, storage account URL, and event hub endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azureblobreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azureblob:\n    auth: service_principal\n    service_principal:\n      tenant_id: \"${tenant_id}\"\n      client_id: \"${client_id}\"\n      client_secret: \"${env:CLIENT_SECRET}\"\n    storage_account_url: https://accountName.blob.core.windows.net\n    event_hub:\n      endpoint: Endpoint=sb://oteldata.servicebus.windows.net/;SharedAccessKeyName=otelhubbpollicy;SharedAccessKey=mPJVubIK5dJ6mLfZo1ucsdkLysLSQ6N7kddvsIcmoEs=;EntityPath=otellhub\n```\n\n----------------------------------------\n\nTITLE: Removing Legacy Resource Attributes Feature Gate\nDESCRIPTION: Command-line option to remove redundant legacy resource attributes in favor of newer attribute names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n\"--feature-gates=receiver.prometheusreceiver.RemoveLegacyResourceAttributes\"\n```\n\n----------------------------------------\n\nTITLE: Configuring FlinkMetrics Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for setting up the FlinkMetrics receiver in the OpenTelemetry Collector. This demonstrates how to specify the Flink endpoint URL and collection interval.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/flinkmetricsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  flinkmetrics:\n    endpoint: http://localhost:8081\n    collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Configuring Health Check Extension V1 in YAML\nDESCRIPTION: Example configuration for the V1 Health Check Extension showing how to set up multiple instances with custom endpoints, TLS settings, and health status paths.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  health_check:\n  health_check/1:\n    endpoint: \"localhost:13\"\n    tls:\n      ca_file: \"/path/to/ca.crt\"\n      cert_file: \"/path/to/cert.crt\"\n      key_file: \"/path/to/key.key\"\n    path: \"/health/status\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Histogram Processing with KQL Update Policies\nDESCRIPTION: KQL script that demonstrates advanced histogram data processing with update policies in Azure Data Explorer. It creates dedicated tables for histogram buckets and histogram aggregates, with functions to extract and transform the relevant data from the main metrics table.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuredataexplorerexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: kql\nCODE:\n```\n.create table HistoBucketData (Timestamp: datetime, MetricName: string , MetricType: string , Value: double, LE: double, Host: string , ResourceAttributes: dynamic, MetricAttributes: dynamic )\n\n.create function \nwith ( docstring = \"Histo bucket processing function\", folder = \"UpdatePolicyFunctions\") ExtractHistoColumns()\n{\n    OTELMetrics\n    | where MetricType == 'Histogram' and MetricName has \"_bucket\"\n    | extend f=parse_json(MetricAttributes)\n    | extend le=todouble(f.le)\n    | extend M_name=replace_string(MetricName, '_bucket','')\n    | project Timestamp, MetricName=M_name, MetricType, MetricValue, LE=le, Host, ResourceAttributes, MetricAttributes\n}\n\n.alter table HistoBucketData policy update \n@'[{ \"IsEnabled\": true, \"Source\": \"OTELMetrics\",\"Query\": \"ExtractHistoColumns()\", \"IsTransactional\": false, \"PropagateIngestionProperties\": false}]'\n\n//Below code creates a table which only contains count and sum values of Histogram metric type and attaches an update policy to it\n\n .create table HistoData (Timestamp: datetime, MetricName: string , MetricType: string , Count: double, Sum: double, Host: string , ResourceAttributes: dynamic, MetricAttributes: dynamic)\n\n .create function \nwith ( docstring = \"Histo sum count processing function\", folder = \"UpdatePolicyFunctions\") ExtractHistoCountColumns()\n{\n   OTELMetrics\n    | where MetricType =='Histogram'\n    | where MetricName has \"_count\"\n    | extend Count=MetricValue\n    | extend M_name=replace_string(MetricName, '_bucket','')\n    | join kind=inner (OTELMetrics\n    | where MetricType =='Histogram'\n    | where MetricName has \"_sum\"\n    | project Sum = MetricValue , Timestamp)\n on Timestamp | project Timestamp, MetricName=M_name, MetricType, Count, Sum, Host, ResourceAttributes, MetricAttributes\n}\n\n.alter table HistoData policy update \n@'[{ \"IsEnabled\": true, \"Source\": \"RawMetricsData\",\"Query\": \"ExtractHistoCountColumns()\", \"IsTransactional\": false, \"PropagateIngestionProperties\": false}]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced UDP Protocol Options for Jaeger Receiver in YAML\nDESCRIPTION: This snippet shows how to set advanced configuration options for UDP protocols (thrift_binary and thrift_compact) in the Jaeger receiver. It includes settings for queue size, max packet size, number of workers, and socket buffer size.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/jaegerreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprotocols:\n  thrift_binary:\n    endpoint: 0.0.0.0:6832\n    queue_size: 5_000\n    max_packet_size: 131_072\n    workers: 50\n    socket_buffer_size: 8_388_608\n```\n\n----------------------------------------\n\nTITLE: Implementing RoundTrip Method for Sigv4 Authentication in Go\nDESCRIPTION: Implements the RoundTrip method to sign HTTP requests with Sigv4. It clones the request, adds AWS SDK info to the User-Agent, hashes the payload, and signs the request before sending.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_9\n\nLANGUAGE: go\nCODE:\n```\nfunc (si *signingRoundTripper) RoundTrip(req *http.Request) (*http.Response, error) {\n    reqBody, err := req.GetBody()\n    if err != nil {\n        return nil, err\n    }\n\n    content, err := io.ReadAll(reqBody)\n    reqBody.Close()\n    if err != nil {\n        return nil, err\n    }\n    body := bytes.NewReader(content)\n\n    // Clone request to ensure thread safety.\n    // Impl. as helper function\n    req2 := cloneRequest(req)\n\n    // Add the runtime information to the User-Agent header of the request\n    ua := req2.Header.Get(\"User-Agent\")\n    if len(ua) > 0 {\n        ua = ua + \" \" + si.awsSDKInfo\n    } else {\n        ua = si.awsSDKInfo\n    }\n    req2.Header.Set(\"User-Agent\", ua)\n\n\t// Hash the request\n\th := sha256.New()\n\t_, _ = io.Copy(h, body)\n\tpayloadHash := hex.EncodeToString(h.Sum(nil))\n\n\t// Use user provided service/region if specified, use inferred service/region if not, then sign the request\n\tservice, region := si.inferServiceAndRegion(req)\n\tcreds, err := (*si.credsProvider).Retrieve(req.Context())\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error retrieving credentials: %w\", err)\n\t}\n\terr = si.signer.SignHTTP(req.Context(), creds, req2, payloadHash, service, region, time.Now())\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error signing the request: %w\", err)\n\t}\n\n\t// Send the request\n\treturn si.transport.RoundTrip(req2)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring vCenter Receiver in YAML\nDESCRIPTION: Example configuration for the vCenter receiver showing basic setup with endpoint, credentials, collection interval and initial delay settings. The password is specified using an environment variable for security.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/vcenterreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  vcenter:\n    endpoint: http://localhost:15672\n    username: otelu\n    password: ${env:VCENTER_PASSWORD}\n    collection_interval: 5m\n    initial_delay: 1s\n    metrics: []\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenShift Resource Detection in YAML\nDESCRIPTION: Example YAML configuration for the OpenShift resource detector. It includes required ClusterRole permissions and processor configuration options with TLS settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nkind: ClusterRole\nmetadata:\n  name: otel-collector\nrules:\n- apiGroups: [\"config.openshift.io\"]\n  resources: [\"infrastructures\", \"infrastructures/status\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/openshift:\n    detectors: [openshift]\n    timeout: 2s\n    override: false\n    openshift: # optional\n      address: \"https://api.example.com\"\n      token: \"token\"\n      tls:\n        insecure: false\n        ca_file: \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\n```\n\n----------------------------------------\n\nTITLE: Renaming Attributes in OpenTelemetry Collector\nDESCRIPTION: This example shows two methods to rename an attribute: setting a new attribute and deleting the old one, or updating the key using regex. Both methods use the 'ignore' error mode.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  trace_statements:\n    - set(resource.attributes[\"namespace\"], resource.attributes[\"k8s.namespace.name\"])\n    - delete_key(resource.attributes, \"k8s.namespace.name\")\n```\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  trace_statements:\n    - replace_all_patterns(resource.attributes, \"key\", \"k8s\\\\.namespace\\\\.name\", \"namespace\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Resource Detection with Tag Filtering\nDESCRIPTION: YAML configuration for Azure resource detection with tag filtering to include specific Azure resource tags as resource attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/azure:\n    detectors: [\"azure\"]\n    azure:\n      # A list of regex's to match tag keys to add as resource attributes can be specified\n      tags:\n        - ^tag1$\n        - ^tag2$\n        - ^label.*$\n```\n\n----------------------------------------\n\nTITLE: Configuring File Stats Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the File Stats Receiver showing how to specify files to monitor with a glob pattern, set collection interval, and define initial delay before the receiver starts collecting metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filestatsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filestats:\n    include: /tmp/files/*\n    collection_interval: 10s\n    initial_delay: 1s\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Log Receiver in OpenTelemetry Collector\nDESCRIPTION: A simple configuration example for the TCP log receiver that listens for incoming log data on port 54525 across all network interfaces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/tcplogreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  tcplog:\n    listen_address: \"0.0.0.0:54525\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Receiver with Connection String Authentication\nDESCRIPTION: Example configuration for Azure Blob Receiver using connection string authentication method. Shows required settings for connection string and event hub endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azureblobreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azureblob:\n    connection_string: DefaultEndpointsProtocol=https;AccountName=accountName;AccountKey=+idLkHYcL0MUWIKYHm2j4Q==;EndpointSuffix=core.windows.net\n    event_hub:\n      endpoint: Endpoint=sb://oteldata.servicebus.windows.net/;SharedAccessKeyName=otelhubbpollicy;SharedAccessKey=mPJVubIK5dJ6mLfZo1ucsdkLysLSQ6N7kddvsIcmoEs=;EntityPath=otellhub\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the MySQL receiver with basic authentication and statement event monitoring settings. This configuration demonstrates how to connect to a MySQL server and collect metrics with customized collection intervals.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mysql:\n    endpoint: localhost:3306\n    username: otel\n    password: ${env:MYSQL_PASSWORD}\n    database: otel\n    collection_interval: 10s\n    initial_delay: 1s\n    statement_events:\n      digest_text_limit: 120\n      time_limit: 24h\n      limit: 250\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipeline with Datadog Connector and Sampling\nDESCRIPTION: Example configuration showing how to set up traces and metrics pipelines with the Datadog connector, including probabilistic sampling and batch processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/datadogconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 20\n\nconnectors:\n  datadog/connector:\n\nexporters:\n  datadog:\n    api:\n      key: ${env:DD_API_KEY}\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [datadog/connector]\n\n    traces/2:\n      receivers: [datadog/connector]\n      processors: [batch, probabilistic_sampler]\n      exporters: [datadog]\n\n    metrics:\n      receivers: [datadog/connector]\n      processors: [batch]\n      exporters: [datadog]\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Metrics JSON Structure\nDESCRIPTION: JSON array containing SQL Server performance counter metrics. Includes measurements for transactions, user-configurable counters, and workload group statistics. Each metric entry contains counter name, type, instance details, measurement category, and associated values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[\n   {\n      \"counter\":\"Free Space in tempdb (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Transactions\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"61824\"\n   },\n   {\n      \"counter\":\"Version Store Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Transactions\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining RBAC Role for Leader Elector in YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes RBAC Role that grants necessary permissions for the Leader Elector extension to manage lease objects in the coordination.k8s.io API group.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/k8sleaderelector/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: my-lease\n  namespace: default\nrules:\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - get\n  - list\n  - watch\n  - create\n  - update\n  - patch\n  - delete\n```\n\n----------------------------------------\n\nTITLE: Configuring Jaeger Remote Sampling Extension in YAML\nDESCRIPTION: YAML configuration for the Jaeger Remote Sampling extension showing three different configurations: one using a remote gRPC endpoint, and two using file sources (local and HTTP).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/jaegerremotesampling/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  jaegerremotesampling:\n    source:\n      reload_interval: 30s\n      remote:\n        endpoint: jaeger-collector:14250\n  jaegerremotesampling/1:\n    source:\n      reload_interval: 1s\n      file: /etc/otelcol/sampling_strategies.json\n  jaegerremotesampling/2:\n    source:\n      reload_interval: 1s\n      file: http://jaeger.example.com/sampling_strategies.json\n```\n\n----------------------------------------\n\nTITLE: Input JSON Example for Attribute Aggregation\nDESCRIPTION: Sample input JSON with attributes prefixed with 'pod_' that will be aggregated under a common key through the attribute aggregation feature.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"pod_a\": \"x\",\n  \"pod_b\": \"y\",\n  \"pod_c\": \"z\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Secure and Insecure OTELARROW Exporters in YAML\nDESCRIPTION: Example configuration showing both secure and insecure OTELARROW exporters with TLS settings. The secure exporter uses certificate and key files, while the insecure one uses the insecure TLS flag.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/otelarrowexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otelarrow/secure:\n    endpoint: external-collector:4317\n    tls:\n      cert_file: file.cert\n      key_file: file.key\n  otelarrow/insecure:\n    endpoint: internal-collector:4317\n    tls:\n      insecure: true\n```\n\n----------------------------------------\n\nTITLE: Copying Values from Attributes to Body in OpenTelemetry Collector (YAML)\nDESCRIPTION: This configuration copies a value from the attributes field to the body field. It transfers the value from attributes.key to body.newkey.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- type: copy\n  from: attributes.key\n  to: body.newkey\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Kubernetes API for Volume Metadata in YAML\nDESCRIPTION: Example configuration for collecting additional volume metadata from the Kubernetes API, enabling detailed labeling for persistent volume claims with information from the underlying storage resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 10s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${env:K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\n    extra_metadata_labels:\n      - k8s.volume.type\n    k8s_api_config:\n      auth_type: serviceAccount\n```\n\n----------------------------------------\n\nTITLE: Configuring Workload Identity Authentication in Azure Auth Extension\nDESCRIPTION: YAML configuration example for the Azure authenticator extension using workload identity authentication for Kubernetes. This requires specifying client_id, federated_token_file, and tenant_id parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/azureauthextension/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  azureauth:\n    workload_identity:\n      client_id: ${CLIENT_ID}\n      federated_token_file: ${FILE}\n      tenant_id: ${TENANT_ID}\n```\n\n----------------------------------------\n\nTITLE: Using Format Converter in Go for String Formatting\nDESCRIPTION: The Format converter applies fmt.Sprintf-style formatting to create formatted strings. It takes a format string and an array of arguments, supporting all format specifiers from Go's fmt package.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_23\n\nLANGUAGE: go\nCODE:\n```\nFormat(\"%02d\", [log.attributes[\"priority\"]])\n```\n\nLANGUAGE: go\nCODE:\n```\nFormat(\"%04d-%02d-%02d\", [Year(Now()), Month(Now()), Day(Now())])\n```\n\nLANGUAGE: go\nCODE:\n```\nFormat(\"%s/%s/%04d-%02d-%02d.log\", [resource.attributes[\"hostname\"], log.body[\"program\"], Year(Now()), Month(Now()), Day(Now())])\n```\n\n----------------------------------------\n\nTITLE: Stanza Package Split Configuration\nDESCRIPTION: Example of the new split configuration approach in the stanza package after renaming and restructuring tokenize package.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG-API.md#2025-04-10_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\n// New split config usage\nsplit.Config{\n  Func: split.NewLineStartSplitFunc(),\n  PreserveLeading: true,\n  PreserveTrailing: false\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Dropped Traces Metric in Prometheus\nDESCRIPTION: This Prometheus query retrieves the number of traces dropped too early by the tail sampling processor. It's useful for monitoring load issues and determining if the 'num_traces' configuration needs adjustment.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: prometheus\nCODE:\n```\notelcol_processor_tail_sampling_sampling_trace_dropped_too_early\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta to Rate Processor in YAML\nDESCRIPTION: YAML configuration example for the deltatorateprocessor showing how to specify the list of delta sum metrics to be converted to rates. The configuration requires listing the metric names to identify delta sum metrics for rate calculation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/deltatorateprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: deltatorate\n    deltatorate:\n\n        # list the delta sum metrics to calculate the rate. This is a required field.\n        metrics:\n            - <metric_1_name>\n            - <metric_2_name>\n            .\n            .\n            - <metric_n_name>\n```\n\n----------------------------------------\n\nTITLE: Input JSON for Basic CSV Parsing\nDESCRIPTION: Example JSON input for the basic CSV parsing configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"1,debug,Debug Message\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon ECS Resource Detection\nDESCRIPTION: YAML configuration for Amazon ECS resource detection that uses environment variables and the Task Metadata Endpoint to gather ECS-specific resource attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/ecs:\n    detectors: [env, ecs]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Performance Profiler Extension in OpenTelemetry Collector\nDESCRIPTION: Basic YAML configuration for enabling the Performance Profiler extension in OpenTelemetry Collector. This minimal configuration uses default settings with the profiler endpoint accessible on localhost:1777.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/pprofextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  pprof:\n```\n\n----------------------------------------\n\nTITLE: Basic Expvar Receiver Configuration\nDESCRIPTION: Minimal configuration for the Expvar Receiver that uses default settings to scrape metrics from http://localhost:8000/debug/vars every 60 seconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/expvarreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  expvar:\n```\n\n----------------------------------------\n\nTITLE: Load Balancing with Persistent Queue and Retry Configuration\nDESCRIPTION: Advanced configuration example demonstrating persistent queue, retry mechanisms, and timeout settings for the load balancing exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: localhost:4317\n\nprocessors:\n\nexporters:\n  loadbalancing:\n    timeout: 10s\n    retry_on_failure:\n      enabled: true\n      initial_interval: 5s\n      max_interval: 30s\n      max_elapsed_time: 300s\n    sending_queue:\n      enabled: true\n      num_consumers: 2\n      queue_size: 1000\n      storage: file_storage/otc\n    routing_key: \"service\"\n    protocol:\n      otlp:\n        # all options from the OTLP exporter are supported\n        # except the endpoint\n        timeout: 1s\n        sending_queue:\n          enabled: true\n    resolver:\n      static:\n        hostnames:\n        - backend-1:4317\n        - backend-2:4317\n        - backend-3:4317\n        - backend-4:4317\n\nextensions:\n  file_storage/otc:\n    directory: /var/lib/storage/otc\n    timeout: 10s\n\nservice:\n  extensions: [file_storage]\n  pipelines:\n    traces:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n    logs:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Receiver in YAML\nDESCRIPTION: Example configuration for the MongoDB receiver, including host specification, authentication, collection interval, and TLS settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodb:\n    hosts:\n      - endpoint: localhost:27017\n    username: otel\n    password: ${env:MONGODB_PASSWORD}\n    collection_interval: 60s\n    initial_delay: 1s\n    tls:\n      insecure: true\n      insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Check Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the TCP Check Receiver that demonstrates how to set up multiple TCP endpoint targets with different configurations. The configuration shows how to specify endpoints and set dialer timeout values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/tcpcheckreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  tcpcheck:\n    targets:\n      - endpoint: example.com:443\n        dialer:\n          timeout: 15s\n      - endpoint: foobar.com:8080\n        dialer:\n          timeout: 15s\n      - endpoint: localhost:10901\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS S3 Receiver with Text Encoding Extension\nDESCRIPTION: Example configuration for the AWS S3 Receiver that specifies time range, S3 bucket details, and custom text encoding extension for retrieving telemetry data from S3.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awss3receiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextension:\n  # example of text encoding extension\n  text_encoding:\n    encoding: utf8\n    marshaling_separator: \"\\n\"\n    unmarshaling_separator: \"\\r?\\n\"\n    \nreceivers:\n  awss3:\n    starttime: \"2024-01-01 01:00\"\n    endtime: \"2024-01-02\"\n    s3downloader:\n        region: \"us-west-1\"\n        s3_bucket: \"mybucket\"\n        s3_prefix: \"trace\"\n        s3_partition: \"minute\"\n    encodings:\n      - extension: text_encoding\n        suffix: \".txt\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Spanmetrics Connector in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration demonstrates how to set up the spanmetrics connector with various options including histogram buckets, dimensions, exemplars, and resource metrics key attributes. It defines pipelines for both traces and metrics processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/spanmetricsconnector/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  nop:\n\nexporters:\n  nop:\n\nconnectors:\n  spanmetrics:\n    histogram:\n      explicit:\n        buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]\n    dimensions:\n      - name: http.method\n        default: GET\n      - name: http.status_code\n    exemplars:\n      enabled: true\n    exclude_dimensions: ['status.code']\n    dimensions_cache_size: 1000\n    aggregation_temporality: \"AGGREGATION_TEMPORALITY_CUMULATIVE\"    \n    metrics_flush_interval: 15s\n    metrics_expiration: 5m\n    events:\n      enabled: true\n      dimensions:\n        - name: exception.type\n        - name: exception.message\n    resource_metrics_key_attributes:\n      - service.name\n      - telemetry.sdk.language\n      - telemetry.sdk.name\n    include_instrumentation_scope:\n      - express\n\nservice:\n  pipelines:\n    traces:\n      receivers: [nop]\n      exporters: [spanmetrics]\n    metrics:\n      receivers: [spanmetrics]\n      exporters: [nop]\n```\n\n----------------------------------------\n\nTITLE: HTTP Check Receiver Configuration\nDESCRIPTION: Example YAML configuration for the HTTP Check receiver showing multiple target configurations with different endpoints, methods, and headers. Includes complete pipeline setup with batch processor and debug exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/httpcheckreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  httpcheck:\n    collection_interval: 30s\n    targets:\n      - method: \"GET\"\n        endpoints:\n          - \"https://opentelemetry.io\"\n      - method: \"GET\"\n        endpoints: \n          - \"http://localhost:8080/hello1\"\n          - \"http://localhost:8080/hello2\"\n        headers:\n          Authorization: \"Bearer <your_bearer_token>\"\n      - method: \"GET\"\n        endpoint: \"http://localhost:8080/hello\"\n        headers:\n          Authorization: \"Bearer <your_bearer_token>\"\nprocessors:\n  batch:\n    send_batch_max_size: 1000\n    send_batch_size: 100\n    timeout: 10s\nexporters:\n  debug:\n    verbosity: detailed\nservice:\n  pipelines:\n    metrics:\n      receivers: [httpcheck]\n      processors: [batch]\n      exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Format File Output in YAML\nDESCRIPTION: An advanced configuration for the file_output operator that uses a custom format template to write log entries to a specified file path.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/file_output.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: file_output\n  path: /tmp/output.log\n  format: \"Time: {{.Timestamp}} Body: {{.Body}}\\n\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Container Insights for ECS Instance Metrics\nDESCRIPTION: This YAML configuration sets up the AWS Container Insights receiver and AWS EMF exporter for collecting instance-level metrics in an ECS cluster. It defines the collection interval, processors, exporters, and metric declarations for various instance metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscontainerinsightreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awscontainerinsightreceiver:\n    collection_interval: 10s\n    container_orchestrator: ecs\n\nprocessors:\n  batch/metrics:\n    timeout: 60s\n\nexporters:\n  awsemf:\n    namespace: ContainerInsightsEC2Instance\n    log_group_name: '/aws/ecs/containerinsights/{ClusterName}/performance'\n    log_stream_name: 'instanceTelemetry/{ContainerInstanceId}'\n    resource_to_telemetry_conversion:\n      enabled: true\n    dimension_rollup_option: NoDimensionRollup\n    parse_json_encoded_attr_values: [Sources]\n    metric_declarations:\n      # instance metrics\n      - dimensions: [ [ ContainerInstanceId, InstanceId, ClusterName] ]\n        metric_name_selectors:\n          - instance_cpu_utilization\n          - instance_memory_utilization\n          - instance_network_total_bytes\n          - instance_cpu_reserved_capacity\n          - instance_memory_reserved_capacity\n          - instance_number_of_running_tasks\n          - instance_filesystem_utilization\n      - dimensions: [ [ClusterName] ]\n        metric_name_selectors:\n          - instance_cpu_utilization\n          - instance_memory_utilization\n          - instance_network_total_bytes\n          - instance_cpu_reserved_capacity\n          - instance_memory_reserved_capacity\n          - instance_number_of_running_tasks\n          - instance_cpu_usage_total\n          - instance_cpu_limit\n          - instance_memory_working_set\n          - instance_memory_limit\n  debug:\n    verbosity: detailed\nservice:\n  pipelines:\n    metrics:\n      receivers: [awscontainerinsightreceiver]\n      processors: [batch/metrics]\n      exporters: [awsemf,debug]\n```\n\n----------------------------------------\n\nTITLE: Using ToKeyValueString Converter for Map Formatting in OpenTelemetry\nDESCRIPTION: The ToKeyValueString Converter transforms a pcommon.Map into a string of key-value pairs with customizable delimiters. It supports optional sorting and handles nested structures by converting them to JSON strings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_58\n\nLANGUAGE: go\nCODE:\n```\nToKeyValueString(target, Optional[delimiter], Optional[pair_delimiter], Optional[sort_output])\n```\n\n----------------------------------------\n\nTITLE: Configuring Attribute Hints for Loki Labels\nDESCRIPTION: YAML configuration example showing how to use the attributes processor to hint the Loki exporter which attributes should be used as labels.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  attributes:\n    actions:\n      - action: insert\n        key: loki.attribute.labels\n        value: event.domain, event.name\n\n  resource:\n    attributes:\n      - action: insert\n        key: loki.resource.labels\n        value: service.name, service.namespace\n```\n\n----------------------------------------\n\nTITLE: Using Base64Decode Converter in Go\nDESCRIPTION: The Base64Decode converter takes a base64 encoded string and returns the decoded string. This converter is deprecated and users are advised to use the Decode function instead.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_11\n\nLANGUAGE: Go\nCODE:\n```\nBase64Decode(\"aGVsbG8gd29ybGQ=\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nBase64Decode(resource.attributes[\"encoded field\"])\n```\n\n----------------------------------------\n\nTITLE: Collecting SQL Server Performance Counters Using T-SQL\nDESCRIPTION: Complex T-SQL script that collects performance metrics from sys.dm_os_performance_counters. Includes engine edition validation, performance counter collection, and calculation of ratio-based counters. Returns metrics in a format suitable for monitoring systems.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryWithoutInstanceName.txt#2025-04-10_snippet_0\n\nLANGUAGE: tsql\nCODE:\n```\nSET DEADLOCK_PRIORITY -10;\nIF SERVERPROPERTY('EngineEdition') NOT IN (2,3,4) BEGIN /*NOT IN Standard,Enterprise,Express*/\n\tDECLARE @ErrorMessage AS nvarchar(500) = 'Connection string Server:'+ @@ServerName + ',Database:' + DB_NAME() +' is not a SQL Server Standard, Enterprise or Express. This query is only supported on these editions.';\n\tRAISERROW (@ErrorMessage,11,1)\n\tRETURN\nEND\n\nDECLARE\n\t @SqlStatement AS nvarchar(max)\n\t,@MajorMinorVersion AS int = CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),4) AS int)*100 + CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),3) AS int)\n\nDECLARE @PCounters TABLE\n(\n\t [object_name] nvarchar(128)\n\t,[counter_name] nvarchar(128)\n\t,[instance_name] nvarchar(128)\n\t,[cntr_value] bigint\n\t,[cntr_type] int\n\tPRIMARY KEY([object_name], [counter_name], [instance_name])\n);\n\nWITH PerfCounters AS (\nSELECT DISTINCT\n\t RTRIM(spi.[object_name]) [object_name]\n\t,RTRIM(spi.[counter_name]) [counter_name]\n\t,RTRIM(spi.[instance_name]) AS [instance_name]\n\t,CAST(spi.[cntr_value] AS bigint) AS [cntr_value]\n\t,spi.[cntr_type]\n\tFROM sys.dm_os_performance_counters AS spi\n\tWHERE\n\t\tcounter_name IN (\n\t\t\t 'SQL Compilations/sec'\n\t\t\t,'SQL Re-Compilations/sec'\n\t\t\t,'User Connections'\n\t\t\t,'Batch Requests/sec'\n\t\t\t,'Logouts/sec'\n\t\t\t,'Logins/sec'\n\t\t\t,'Processes blocked'\n\t\t\t,'Latch Waits/sec'\n\t\t\t,'Average Latch Wait Time (ms)'\n\t\t\t,'Full Scans/sec'\n\t\t\t,'Index Searches/sec'\n\t\t\t,'Page Splits/sec'\n\t\t\t,'Page lookups/sec'\n\t\t\t,'Page reads/sec'\n\t\t\t,'Page writes/sec'\n\t\t\t,'Readahead pages/sec'\n\t\t\t,'Lazy writes/sec'\n\t\t\t,'Checkpoint pages/sec'\n\t\t\t,'Table Lock Escalations/sec'\n\t\t\t,'Page life expectancy'\n\t\t\t,'Log File(s) Size (KB)'\n\t\t\t,'Log File(s) Used Size (KB)'\n\t\t\t,'Data File(s) Size (KB)'\n\t\t\t,'Transactions/sec'\n\t\t\t,'Write Transactions/sec'\n\t\t\t,'Active Transactions'\n\t\t\t,'Log Growths'\n\t\t\t,'Active Temp Tables'\n\t\t\t,'Logical Connections'\n\t\t\t,'Temp Tables Creation Rate'\n\t\t\t,'Temp Tables For Destruction'\n\t\t\t,'Free Space in tempdb (KB)'\n\t\t\t,'Version Store Size (KB)'\n\t\t\t,'Memory Grants Pending'\n\t\t\t,'Memory Grants Outstanding'\n\t\t\t,'Free list stalls/sec'\n\t\t\t,'Buffer cache hit ratio'\n\t\t\t,'Buffer cache hit ratio base'\n\t\t\t,'Database Pages'\n\t\t\t,'Backup/Restore Throughput/sec'\n\t\t\t,'Total Server Memory (KB)'\n\t\t\t,'Target Server Memory (KB)'\n\t\t\t,'Log Flushes/sec'\n\t\t\t,'Log Flush Wait Time'\n\t\t\t,'Memory broker clerk size'\n\t\t\t,'Log Bytes Flushed/sec'\n\t\t\t,'Bytes Sent to Replica/sec'\n\t\t\t,'Log Send Queue'\n\t\t\t,'Bytes Sent to Transport/sec'\n\t\t\t,'Sends to Replica/sec'\n\t\t\t,'Bytes Sent to Transport/sec'\n\t\t\t,'Sends to Transport/sec'\n\t\t\t,'Bytes Received from Replica/sec'\n\t\t\t,'Receives from Replica/sec'\n\t\t\t,'Flow Control Time (ms/sec)'\n\t\t\t,'Flow Control/sec'\n\t\t\t,'Resent Messages/sec'\n\t\t\t,'Redone Bytes/sec'\n\t\t\t,'XTP Memory Used (KB)'\n\t\t\t,'Transaction Delay'\n\t\t\t,'Log Bytes Received/sec'\n\t\t\t,'Log Apply Pending Queue'\n\t\t\t,'Redone Bytes/sec'\n\t\t\t,'Recovery Queue'\n\t\t\t,'Log Apply Ready Queue'\n\t\t\t,'CPU usage %'\n\t\t\t,'CPU usage % base'\n\t\t\t,'Queued requests'\n\t\t\t,'Requests completed/sec'\n\t\t\t,'Blocked tasks'\n\t\t\t,'Active memory grant amount (KB)'\n\t\t\t,'Disk Read Bytes/sec'\n\t\t\t,'Disk Read IO Throttled/sec'\n\t\t\t,'Disk Read IO/sec'\n\t\t\t,'Disk Write Bytes/sec'\n\t\t\t,'Disk Write IO Throttled/sec'\n\t\t\t,'Disk Write IO/sec'\n\t\t\t,'Used memory (KB)'\n\t\t\t,'Forwarded Records/sec'\n\t\t\t,'Background Writer pages/sec'\n\t\t\t,'Percent Log Used'\n\t\t\t,'Log Send Queue KB'\n\t\t\t,'Redo Queue KB'\n\t\t\t,'Mirrored Write Transactions/sec'\n\t\t\t,'Group Commit Time'\n\t\t\t,'Group Commits/Sec'\n\t\t\t,'Workfiles Created/sec'\n\t\t\t,'Worktables Created/sec'\n\t\t\t,'Distributed Query'\n\t\t\t,'DTC calls'\n\t\t\t,'Query Store CPU usage'\n\t\t\t,'Query Store physical reads'\n\t\t\t,'Query Store logical reads'\n\t\t\t,'Query Store logical writes'\n\t\t\t,'Execution Errors'\n\t\t) OR (\n\t\t\tspi.[object_name] LIKE '%User Settable%'\n\t\t\tOR spi.[object_name] LIKE '%SQL Errors%'\n\t\t\tOR spi.[object_name] LIKE '%Batch Resp Statistics%'\n\t\t) OR (\n\t\t\tspi.[instance_name] IN ('_Total')\n\t\t\tAND spi.[counter_name] IN (\n\t\t\t\t 'Lock Timeouts/sec'\n\t\t\t\t,'Lock Timeouts (timeout > 0)/sec'\n\t\t\t\t,'Number of Deadlocks/sec'\n\t\t\t\t,'Lock Waits/sec'\n\t\t\t\t,'Latch Waits/sec'\n\t\t\t)\n\t\t)\n)\n\nINSERT INTO @PCounters SELECT * FROM PerfCounters;\n\nSELECT\n\t 'sqlserver_performance' AS [measurement]\n\t,REPLACE(@@SERVERNAME,'\\\\',':') AS [sql_instance]\n\t,HOST_NAME() AS [computer_name]\n\t,pc.[object_name] AS [object]\n\t,pc.[counter_name] AS [counter]\n\t,CASE pc.[instance_name] WHEN '_Total' THEN 'Total' ELSE ISNULL(pc.[instance_name],'') END AS [instance]\n\t,CAST(CASE WHEN pc.[cntr_type] = 537003264 AND pc1.[cntr_value] > 0 THEN (pc.[cntr_value] * 1.0) / (pc1.[cntr_value] * 1.0) * 100 ELSE pc.[cntr_value] END AS float(10)) AS [value]\n\t,CAST(pc.[cntr_type] AS varchar(25)) AS [counter_type]\nFROM @PCounters AS pc\nLEFT OUTER JOIN @PCounters AS pc1\n\tON (\n\t\tpc.[counter_name] = REPLACE(pc1.[counter_name],' base','')\n\t\tOR pc.[counter_name] = REPLACE(pc1.[counter_name],' base',' (ms)')\n\t)\n\tAND pc.[object_name] = pc1.[object_name]\n\tAND pc.[instance_name] = pc1.[instance_name]\n\tAND pc1.[counter_name] LIKE '%base'\nWHERE\n\tpc.[counter_name] NOT LIKE '% base'\n\nOPTION(RECOMPILE)\n```\n\n----------------------------------------\n\nTITLE: Configuring Logs Discovery Annotations for Kubernetes Pods\nDESCRIPTION: This snippet shows how to use Kubernetes Pod annotations to configure log collection. It includes examples of enabling discovery and specifying configuration details for the filelog receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nio.opentelemetry.discovery.logs/enabled: \"true\"\nio.opentelemetry.discovery.logs/config: |\n  include_file_name: true\n  max_log_size: \"2MiB\"\n  operators:\n    - type: container\n      id: container-parser\n    - type: regex_parser\n      regex: \"^(?P<time>\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}) (?P<sev>[A-Z]*) (?P<msg>.*)$\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Carbon Receiver with YAML in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up the Carbon receiver with custom settings, including a regex parser. It shows both basic configuration and advanced parser settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/carbonreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  carbon/receiver_settings:\n    endpoint: localhost:8080\n    transport: udp\n  carbon/regex:\n    parser:\n      type: regex\n      config:\n        rules:\n          - regexp: \"(?P<key_base>test)\\\\.env(?P<key_env>[^.]*)\\\\.(?P<key_host>[^.]*)\"\n            name_prefix: \"name-prefix\"\n            labels:\n              dot.key: dot.value\n              key: value\n            type: cumulative\n          - regexp: \"(?P<key_just>test)\\\\.(?P<key_match>.*)\"\n        name_separator: \"_\"\n```\n\n----------------------------------------\n\nTITLE: Basic Configuration for Group By Attributes Processor in YAML\nDESCRIPTION: A simple configuration example showing how to set up the groupbyattrs processor with specific attribute keys to use for grouping telemetry data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  groupbyattrs:\n    keys:\n      - foo\n      - bar\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Spark Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the Apache Spark receiver showing how to set the collection interval, endpoint URL, and specific application names to monitor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/apachesparkreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  apachespark:\n    collection_interval: 60s\n    endpoint: http://localhost:4040\n    application_names:\n    - PythonStatusAPIDemo\n    - PythonLR\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Spanner Receiver in YAML\nDESCRIPTION: Example YAML configuration for the Google Cloud Spanner Receiver, including settings for collection interval, initial delay, query limits, and project/instance/database specifications.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/googlecloudspannerreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  googlecloudspanner:\n    collection_interval: 60s\n    initial_delay: 1s\n    top_metrics_query_max_rows: 100\n    backfill_enabled: true\n    cardinality_total_limit: 200000\n    hide_topn_lockstats_rowrangestartkey: false\n    truncate_text: false\n    projects:\n      - project_id: \"spanner project 1\"\n        service_account_key: \"path to spanner project 1 service account json key\"\n        instances:\n          - instance_id: \"id1\"\n            databases:\n              - \"db11\"\n              - \"db12\"\n          - instance_id: \"id2\"\n            databases:\n              - \"db21\"\n              - \"db22\"\n      - project_id: \"spanner project 2\"\n        service_account_key: \"path to spanner project 2 service account json key\"\n        instances:\n          - instance_id: \"id3\"\n            databases:\n              - \"db31\"\n              - \"db32\"\n          - instance_id: \"id4\"\n            databases:\n              - \"db41\"\n              - \"db42\"\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Proxy Extension in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the AWS Proxy extension that forwards unauthenticated requests to AWS services. Shows configuration options including endpoint, proxy address, TLS settings, region, role ARN, AWS endpoint, local mode, and service name selection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/awsproxy/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  awsproxy:\n    endpoint: 0.0.0.0:2000\n    proxy_address: \"\"\n    tls:\n      insecure: false\n      server_name_override: \"\"\n    region: \"\"\n    role_arn: \"\"\n    aws_endpoint: \"\"\n    local_mode: false\n    service_name: \"xray\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Zipkin Encoding Extension for Kafka Receiver\nDESCRIPTION: Example YAML configuration showing how to set up a Zipkin encoding extension and apply it to a Kafka receiver. The configuration demonstrates defining the encoding protocol, version, and referencing it in a receiver configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  zipkin_encoding:\n    protocol: zipkin_proto\n    version: v2\n\nreceivers:\n  kafka:\n    encoding: zipkin_encoding\n    # ... other configuration values\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Service Receiver with Exclusion Pattern\nDESCRIPTION: Configuration example for monitoring all Windows services except those explicitly excluded. This approach monitors every service on the host machine except for those listed in the exclude_services array.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsservicereceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nwindowsservice:\n  collection_interval: <duration> # default = 1m\n  include_services:\n  exclude_services:\n    - service3\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Pipelines for SignalFx in YAML\nDESCRIPTION: This snippet shows how to set up the service pipelines for both metrics and logs when using the SignalFx receiver and exporter. It's important to configure both pipelines when enabling SignalFx components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/signalfxreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n    logs:\n      receivers: [signalfx]\n      processors: [memory_limiter, batch]\n      exporters: [signalfx]\n```\n\n----------------------------------------\n\nTITLE: Input/Output JSON Example with Timestamp Processing\nDESCRIPTION: Example showing JSON parsing combined with timestamp extraction from the parsed content, resulting in an updated timestamp field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"{\\\"key\\\": \\\"val\\\", \\\"seconds_since_epoch\\\": 1136214245}\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2006-01-02T15:04:05-07:00\",\n  \"body\": {\n    \"key\": \"val\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Riak Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the Riak receiver showing required and optional parameters. This configuration specifies the endpoint, credentials, and collection interval for monitoring a Riak node.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/riakreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  riak:\n    endpoint: http://localhost:8098\n    username: otelu\n    password: ${env:RIAK_PASSWORD}\n    collection_interval: 60s\n```\n\n----------------------------------------\n\nTITLE: Accessing Memory Profile using Go's pprof Tool\nDESCRIPTION: Command for analyzing memory usage (heap) using Go's pprof tool by connecting to the collector's profiling endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/pprofextension/README.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngo tool pprof http://localhost:1777/debug/pprof/heap\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple File Output in YAML\nDESCRIPTION: A basic configuration for the file_output operator that writes JSON-formatted log entries to a specified file path.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/file_output.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: file_output\n  path: /tmp/output.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Format for Loki Exporter\nDESCRIPTION: YAML configuration example showing how to set the log format for the Loki exporter using a resource attribute hint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resource:\n    attributes:\n    - action: insert\n      key: loki.format\n      value: logfmt\n```\n\n----------------------------------------\n\nTITLE: URL Parser in Golang for Complex URL\nDESCRIPTION: A function to parse a complex URL string with username, password, port, query parameters, and fragment. Extracts all components into a structured object.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_74\n\nLANGUAGE: go\nCODE:\n```\nURL(\"http://myusername:mypassword@www.example.com:80/foo.gif?key1=val1&key2=val2#fragment\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Encoding Extension with JSON Protocol\nDESCRIPTION: YAML configuration example for setting up the OTLP encoding extension using the JSON protocol for data encoding.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/otlpencodingextension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  otlp_encoding:\n    protocol: otlp_json\n```\n\n----------------------------------------\n\nTITLE: Kafka Receiver with Header Extraction\nDESCRIPTION: Configuration example demonstrating how to extract Kafka record headers and add them as resource attributes to the telemetry data, with a specific topic configured.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkareceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafka:\n    topic: test\n    header_extraction: \n      extract_headers: true\n      headers: [\"header1\", \"header2\"]\n```\n\n----------------------------------------\n\nTITLE: Converting Prometheus Counter to OpenTelemetry Metric (First Scrape)\nDESCRIPTION: Demonstrates how a Prometheus counter metric is converted to OpenTelemetry format during the first scrape. The example shows the creation of metric descriptors, timeseries with proper label handling, and point values with timestamps.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_3\n\nLANGUAGE: go\nCODE:\n```\nmetrics := []*metricspb.Metric{\n  {\n    MetricDescriptor: &metricspb.MetricDescriptor{\n      Name:      \"http_requests_total\",\n      Type:      metricspb.MetricDescriptor_CUMULATIVE_DOUBLE,\n      LabelKeys: []*metricspb.LabelKey{{Key: \"method\"}, {Key: \"code\"}}},\n    Timeseries: []*metricspb.TimeSeries{\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"post\", HasValue: true}, {Value: \"200\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: startTimestamp, Value: &metricspb.Point_DoubleValue{DoubleValue: 1027.0}},\n        },\n      },\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"post\", HasValue: false}, {Value: \"400\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: startTimestamp, Value: &metricspb.Point_DoubleValue{DoubleValue: 3.0}},\n        },\n      },\n    },\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Probabilistic Sampler with Custom Attribute Source\nDESCRIPTION: YAML configuration example for sampling logs according to their logID attribute instead of the default trace ID.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/probabilisticsamplerprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 15\n    attribute_source: record # possible values: one of record or traceID\n    from_attribute: logID # value is required if the source is not traceID\n```\n\n----------------------------------------\n\nTITLE: Jaeger Sampling Strategy JSON Configuration\nDESCRIPTION: Example JSON configuration for Jaeger sampling strategies, defining service-specific and default sampling rules. Includes probabilistic and rate-limiting strategies for different services and operations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/jaegerremotesampling/README.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"service_strategies\": [\n    {\n      \"service\": \"foo\",\n      \"type\": \"probabilistic\",\n      \"param\": 0.8,\n      \"operation_strategies\": [\n        {\n          \"operation\": \"op1\",\n          \"type\": \"probabilistic\",\n          \"param\": 0.2\n        },\n        {\n          \"operation\": \"op2\",\n          \"type\": \"probabilistic\",\n          \"param\": 0.4\n        }\n      ]\n    },\n    {\n      \"service\": \"bar\",\n      \"type\": \"ratelimiting\",\n      \"param\": 5\n    }\n  ],\n  \"default_strategy\": {\n    \"type\": \"probabilistic\",\n    \"param\": 0.5,\n    \"operation_strategies\": [\n      {\n        \"operation\": \"/health\",\n        \"type\": \"probabilistic\",\n        \"param\": 0.0\n      },\n      {\n        \"operation\": \"/metrics\",\n        \"type\": \"probabilistic\",\n        \"param\": 0.0\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Event Log Input in YAML\nDESCRIPTION: Simple configuration example showing how to set up the windows_eventlog_input operator to monitor the application channel.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/windows_eventlog_input.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: windows_eventlog_input\n  channel: application\n```\n\n----------------------------------------\n\nTITLE: Configuring OpAMP Supervisor for OpenTelemetry Collector in YAML\nDESCRIPTION: This YAML configuration file defines the settings for the OpAMP Supervisor. It includes server connection details, OpAMP capabilities, storage settings, and agent-specific configurations for managing the OpenTelemetry Collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/opampsupervisor/specification/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  endpoint: wss://example.com/opamp\n\ncapabilities:\n  accepts_remote_config:\n  reports_effective_config:\n  accepts_packages:\n  reports_own_metrics:\n  reports_own_logs:\n  reports_own_traces:\n  accepts_other_connection_settings:\n  accepts_restart_command:\n  reports_health:\n\nstorage:\n  directory: /path/to/dir\n\nagent:\n  executable: /opt/otelcol/bin/otelcol\n  orphan_detection_interval: 5s\n  bootstrap_timeout: 3s\n  args:\n  env:\n  run_as: myuser\n  config_files: \n    - /etc/otelcol/config.yaml\n  access_dirs:\n    read:\n      allow: [/var/log]\n      deny: [/var/log/secret_logs]\n    write:\n      allow: [/var/otelcol]\n  description:\n    identifying_attributes:\n      client.id: \"01HWWSK84BMT7J45663MBJMTPJ\"\n    non_identifying_attributes:\n      custom.attribute: \"custom-value\"\n  health_check_port:\n  opamp_server_port:\n```\n\n----------------------------------------\n\nTITLE: Configuring Bearer Token Authentication in OpenTelemetry Collector\nDESCRIPTION: This configuration example demonstrates how to set up bearer token authentication for exporters in OpenTelemetry Collector. It shows various authentication scenarios including using a static token, token from a file, custom scheme, and multiple valid tokens. The example also illustrates how to reference these authenticators in OTLP exporters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/bearertokenauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  bearertokenauth:\n    token: \"somerandomtoken\"\n    filename: \"file-containing.token\"\n  bearertokenauth/withscheme:\n    scheme: \"Bearer\"\n    token: \"randomtoken\"\n  bearertokenauth/multipletokens:\n    scheme: \"Bearer\"\n    tokens:\n      - \"randomtoken\"\n      - \"thistokenalsoworks\"\n\nreceivers:\n  hostmetrics:\n    scrapers:\n      memory:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  otlp/withauth:\n    endpoint: 0.0.0.0:5000\n    ca_file: /tmp/certs/ca.pem\n    auth:\n      authenticator: bearertokenauth\n\n  otlphttp/withauth:\n    endpoint: http://localhost:9000\n    auth:\n      authenticator: bearertokenauth/withscheme\n\nservice:\n  extensions: [bearertokenauth, bearertokenauth/withscheme]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [otlp/withauth, otlphttp/withauth]\n```\n\n----------------------------------------\n\nTITLE: Configuring Filelog Receiver for Plaintext File Tailing in YAML\nDESCRIPTION: This configuration shows how to set up the Filelog receiver to tail a plaintext log file. It uses a regex parser operator to extract timestamp, severity, and message from each log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: [ /simple.log ]\n    operators:\n      - type: regex_parser\n        regex: '^(?P<time>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (?P<sev>[A-Z]*) (?P<msg>.*)$'\n        timestamp:\n          parse_from: attributes.time\n          layout: '%Y-%m-%d %H:%M:%S'\n        severity:\n          parse_from: attributes.sev\n```\n\n----------------------------------------\n\nTITLE: Wavefront Metric Format Example\nDESCRIPTION: Shows the standard format for Wavefront metric data points. Each line contains a metric name, value, optional timestamp, source, and optional point tags.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/wavefrontreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<metricName> <metricValue> [<timestamp>] source=<source> [pointTags]\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Adding Value Using Expression\nDESCRIPTION: This JSON demonstrates the output entry after adding a value to the body using an expression with the 'add' operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n    \"key2\": \"val1_suffix\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Log Collection in YAML\nDESCRIPTION: Configuration to receive logs from MongoDB Atlas, including audit and host logs for a specific project.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    logs:\n      enabled: true\n      projects: \n        - name: \"project 1\"\n          collect_audit_logs: true\n          collect_host_logs: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Principal Authentication with Certificate in Azure Auth Extension\nDESCRIPTION: YAML configuration example for the Azure authenticator extension using service principal authentication with a client certificate path. This requires specifying client_id, tenant_id, and client_certificate_path parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/azureauthextension/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  azureauth:\n    service_principal:\n      client_id: ${CLIENT_ID}\n      tenant_id: ${TENANT_ID}\n      client_certificate_path: ${CLIENT_CERTIFICATE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Configuring Health Check Extension V2 in YAML\nDESCRIPTION: Sample configuration for the V2 Health Check Extension showing how to enable both HTTP and gRPC services with component health monitoring options. Includes settings for error handling and recovery behavior.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  healthcheckv2:\n    use_v2: true\n    component_health:\n      include_permanent_errors: false\n      include_recoverable_errors: true\n      recovery_duration: 5m\n    http:\n      endpoint: \"localhost:13133\"\n      status:\n        enabled: true\n        path: \"/health/status\"\n      config:\n        enabled: true\n        path: \"/health/config\"\n    grpc:\n      endpoint: \"localhost:13132\"\n      transport: \"tcp\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Cleanup on Start for File Storage Extension in Go\nDESCRIPTION: Adds a new flag 'cleanup_on_start' to the compaction section of the file storage extension. When set to true, it removes all temporary files in the compaction directory that start with 'tempdb'. This helps clean up files left over if a previous process was killed during compaction.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_15\n\nLANGUAGE: go\nCODE:\n```\nextension/storage/filestorage: New flag cleanup_on_start for the compaction section (default=false).\n```\n\n----------------------------------------\n\nTITLE: Configuring InfluxDB Receiver in YAML\nDESCRIPTION: Example YAML configuration for setting up the InfluxDB receiver in the OpenTelemetry Collector. It specifies the endpoint for the line protocol receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/influxdbreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  influxdb:\n    endpoint: 0.0.0.0:8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Exporter with YAML in OpenTelemetry Collector\nDESCRIPTION: This YAML configuration demonstrates how to set up the Azure Blob Exporter with connection string authentication, custom containers, text encoding, and append blob functionality. It includes extensions for zpages and text encoding.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azureblobexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  zpages:\n    endpoint: localhost:55679\n  text_encoding:\n    encoding: utf8\n    marshaling_separator: \"\\n\"\n    unmarshaling_separator: \"\\r?\\n\"\n\nexporter:\n  azureblob/1:\n    url: \"https://<your-account>.blob.core.windows.net/\"\n    container:\n      logs: \"logs\"\n      metrics: \"metrics\"\n      traces: \"traces\"\n    auth:\n      type: \"connection_string\"\n      connection_string: \"DefaultEndpointsProtocol=https;AccountName=<your-acount>;AccountKey=<account-key>;EndpointSuffix=core.windows.net\"\n    encodings:\n      logs: text_encoding\n    append_blob:\n      enabled: true\n      separator: \"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Apache Metrics in YAML Configuration\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable specific Apache metrics in the OpenTelemetry Collector. It can be applied to any of the default metrics listed in the document.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/apachereceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Grouping Metrics by Resource Labels in YAML\nDESCRIPTION: Groups metrics into separate ResourceMetrics with their own resource labels, useful for organizing metrics from different sources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\n# Group metrics from one single ResourceMetrics and report them as multiple ResourceMetrics.\n# \n# ex: Consider pod and container metrics collected from Kubernetes. Both the metrics are recorded under one ResourceMetric\n# applying this transformation will result in two separate ResourceMetric packets with corresponding resource labels in the resource headers\n#\n# instead of regular $ use double dollar $$. Because $ is treated as a special character.\n\n\n- include: ^k8s\\.pod\\.(.*)$$\n  match_type: regexp\n  action: group\n  group_resource_labels: {\"resource.type\": \"k8s.pod\", \"source\": \"kubelet\"}\n- include: ^container\\.(.*)$$\n  match_type: regexp\n  action: group\n  group_resource_labels: {\"resource.type\": \"container\", \"source\": \"kubelet\"}\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output from Windows Event Log Receiver\nDESCRIPTION: An example of the JSON output produced by the Windows Event Log Receiver, showing various fields such as channel, computer name, event ID, and message.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowseventlogreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"channel\": \"Application\",\n    \"computer\": \"computer name\",\n    \"event_id\":\n    {\n        \"id\": 10,\n        \"qualifiers\": 0\n    },\n    \"keywords\": \"[Classic]\",\n    \"level\": \"Information\",\n    \"message\": \"Test log\",\n    \"opcode\": \"Info\",\n    \"provider\":\n    {\n        \"event_source\": \"\",\n        \"guid\": \"\",\n        \"name\": \"otel\"\n    },\n    \"record_id\": 12345,\n    \"system_time\": \"2022-04-15T15:28:08.898974100Z\",\n    \"task\": \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Metrics Data Point Field Definitions\nDESCRIPTION: Defines the available fields in the metrics data point context, including their purposes and data types. These fields are used to access and process various aspects of metric data points in the OpenTelemetry Collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/contexts/ottldatapoint/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| datapoint.flags | the flags of the data point being processed | int64 |\n| datapoint.count | the count of the data point being processed | int64 |\n| datapoint.sum | the sum of the data point being processed | float64 |\n| datapoint.bucket_counts | the bucket counts of the data point being processed | []uint64 |\n| datapoint.explicit_bounds | the explicit bounds of the data point being processed | []float64 |\n| datapoint.scale | the scale of the data point being processed | int64 |\n| datapoint.zero_count | the zero_count of the data point being processed | int64 |\n| datapoint.quantile_values | the quantile_values of the data point being processed | pmetric.SummaryDataPointValueAtQuantileSlice |\n```\n\n----------------------------------------\n\nTITLE: Configuring Severity Attribute Mapping in YAML\nDESCRIPTION: Configuration for mapping internal log attributes to visible fields in Sumo Logic. This example shows how to map the 'severity_number' attribute to a field named 'loglevel'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n  field_attributes:\n      severity_number:\n        enabled: true\n        name: \"loglevel\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Histogram Metrics in YAML\nDESCRIPTION: Configuration structure for Histogram metrics in the Signal to Metrics connector, including bucket definition, count, and value extraction using OTTL expressions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/signaltometricsconnector/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nhistogram:\n  buckets: []float64\n  count: <ottl_value_expression>\n  value: <ottl_value_expression>\n```\n\n----------------------------------------\n\nTITLE: Kafka Receiver with SASL and TLS Authentication\nDESCRIPTION: Configuration example showing how to connect to Kafka using SASL authentication with SCRAM-SHA-512 mechanism and TLS encryption.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkareceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafka:\n    tls:\n    auth:\n      sasl:\n        username: \"user\"\n        password: \"secret\"\n        mechanism: \"SCRAM-SHA-512\"\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Forwarder Extension in YAML\nDESCRIPTION: Example configuration for the HTTP Forwarder extension showing ingress and egress settings. Demonstrates how to set up endpoints, custom headers, and timeout values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/httpforwarderextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  http_forwarder:\n    ingress:\n      endpoint: localhost:7070\n    egress:\n      endpoint: http://target/\n      headers:\n        otel_http_forwarder: dev\n      timeout: 5s\n```\n\n----------------------------------------\n\nTITLE: Generating Traces with Telemetry Generator\nDESCRIPTION: Commands to generate traces using the telemetry generator, demonstrating options for duration-based and count-based trace generation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/telemetrygen/README.md#2025-04-10_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ntelemetrygen traces --otlp-insecure --duration 5s\n```\n\nLANGUAGE: console\nCODE:\n```\ntelemetrygen traces --otlp-insecure --traces 1\n```\n\n----------------------------------------\n\nTITLE: Kubernetes OpenTelemetry Collector Configuration\nDESCRIPTION: Kubernetes manifest for deploying OpenTelemetry Collector with file exporter configuration and volume mounts.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/fileexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f - <<EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: fileexporter\nspec:\n  config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n          http:\n    processors:\n\n    exporters:\n      debug:\n      file:\n        path: /data/metrics.json\n\n    service:\n      pipelines:\n        metrics:\n          receivers: [otlp]\n          processors: []\n          exporters: [debug,file]\n  volumes:\n    - name: file\n      emptyDir: {}\n  volumeMounts: \n    - name: file\n      mountPath: /data\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCensus Exporter in YAML\nDESCRIPTION: Example configuration for OpenCensus exporters showing both secure and insecure TLS configurations. The secure configuration requires certificate and key files, while the insecure configuration bypasses TLS security.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/opencensusexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  opencensus:\n    endpoint: opencensus2:55678\n    tls:\n      cert_file: file.cert\n      key_file: file.key\n  opencensus/2:\n    endpoint: opencensus2:55678\n    tls:\n      insecure: true\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Check Receiver in OpenTelemetry\nDESCRIPTION: Example configuration showing how to set up the TLS Check Receiver to monitor both local certificate files and remote endpoints. Demonstrates basic setup, file path monitoring, and custom timeout configuration for local services.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/tlscheckreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  tlscheck:\n    targets:\n      # Monitor a local PEM file\n      - file_path: /etc/istio/certs/cert-chain.pem\n      \n      # Monitor a remote endpoint\n      - endpoint: example.com:443\n      \n      # Monitor a local service with a custom timeout\n      - endpoint: localhost:10901\n        dialer: \n          timeout: 15s\n```\n\n----------------------------------------\n\nTITLE: Configuring Trace Parser in OpenTelemetry\nDESCRIPTION: Direct configuration of the trace_parser operator for parsing trace context fields. This is a specialized operator dedicated to trace parsing with simplified inline field configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/trace.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: trace_parser\n  trace_id:\n    parse_from: attributes.trace_id\n  span_id:\n    parse_from: attributes.span_id\n  trace_flags:\n    parse_from: attributes.trace_flags\n```\n\n----------------------------------------\n\nTITLE: Dropping Non-HTTP Spans in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop spans that don't have HTTP request method attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    error_mode: ignore\n    traces:\n      span:\n        - attributes[\"http.request.method\"] == nil\n```\n\n----------------------------------------\n\nTITLE: Configuring Sigv4 Authentication with Web Identity Token in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the OpenTelemetry Collector showing how to set up the Sigv4 authenticator extension with web identity token for OIDC authentication. This approach is useful for authenticating from on-prem systems or other cloud providers to AWS services.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  sigv4auth:\n    assume_role:\n      arn: \"arn:aws:iam::123456789012:role/aws-service-role/access\"\n      web_identity_token_file: \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n\nreceivers:\n  hostmetrics:\n    scrapers:\n      memory:\n\nexporters:\n  prometheusremotewrite:\n    endpoint: \"https://aps-workspaces.us-west-2.amazonaws.com/workspaces/ws-XXX/api/v1/remote_write\"\n    auth:\n      authenticator: sigv4auth\n\nservice:\n  extensions: [sigv4auth]\n  pipelines:\n    metrics:\n      receivers: [hostmetrics]\n      processors: []\n      exporters: [prometheusremotewrite]\n```\n\n----------------------------------------\n\nTITLE: Moving Value from Body to Resource in YAML\nDESCRIPTION: This configuration shows how to move a value from the body to the resource section of a log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: move\n  from: body.uuid\n  to: resource.uuid\n```\n\n----------------------------------------\n\nTITLE: Service Principal Authentication Configuration - YAML\nDESCRIPTION: Configuration example for Azure Monitor receiver using Service Principal authentication method with secret.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azuremonitorreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azuremonitor:\n    subscription_ids: [\"${subscription_id}\"]\n    tenant_id: \"${tenant_id}\"\n    client_id: \"${client_id}\"\n    client_secret: \"${env:CLIENT_SECRET}\"\n    cloud: AzureUSGovernment\n    resource_groups:\n      - \"${resource_group1}\"\n      - \"${resource_group2}\"\n    services:\n      - \"${service1}\"\n      - \"${service2}\"\n    collection_interval: 60s\n    initial_delay: 1s\n```\n\n----------------------------------------\n\nTITLE: Input/Output JSON Example for Capture Group Replacement\nDESCRIPTION: Example demonstrating how regex capture groups process a JSON log entry by extracting and concatenating content from within curly braces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_replace.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"{a}{bb}{ccc}\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"abbccc\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Kubernetes Lease Object using Shell Command\nDESCRIPTION: This command shows how to delete a Kubernetes lease object using kubectl. It requires specifying the namespace and lease name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/k8sleaderelector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete leases.coordination.k8s.io -n <namespace> <lease_name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Objects Receiver\nDESCRIPTION: Example configuration for the k8sobjects receiver showing pull and watch modes for pods and events with label and field selectors.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sobjectsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nk8sobjects:\n  auth_type: serviceAccount\n  objects:\n    - name: pods\n      mode: pull\n      label_selector: environment in (production),tier in (frontend)\n      field_selector: status.phase=Running\n      interval: 15m\n    - name: events\n      mode: watch\n      group: events.k8s.io\n      namespaces: [default]\n```\n\n----------------------------------------\n\nTITLE: Configuring SSH/SFTP Metric Settings in YAML\nDESCRIPTION: YAML configuration template for enabling/disabling SSH and SFTP metrics. Used to customize which metrics are collected by the OpenTelemetry collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sshcheckreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Removing a Value from Body\nDESCRIPTION: This shows the resulting JSON entry after the remove operator has removed 'key1' from the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": { }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Metrics in vCenter Receiver\nDESCRIPTION: YAML configuration example for enabling optional metrics in the vCenter receiver. This snippet shows how to enable a specific metric that is not enabled by default in the collector configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/vcenterreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Attributes for Metrics in YAML\nDESCRIPTION: Example configuration for customizing resource attributes to be included in the final metrics, with optional default values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/signaltometricsconnector/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ninclude_resource_attributes:\n  - key: resource.foo # Include resource.foo attribute if present\n  - key: resource.bar # Always include resource.bar attribute, default to bar\n    default_value: bar\n```\n\n----------------------------------------\n\nTITLE: Configuring URI Parser for Relative URI Parsing in YAML\nDESCRIPTION: This snippet demonstrates how to configure the uri_parser operator to parse the body.message field as a relative URI. It includes the YAML configuration and example input and output JSON.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/uri_parser.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: uri_parser\n  parse_from: body.message\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"/app?user=admin\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"path\": \"/app\",\n    \"query\": {\n      \"user\": [\n        \"admin\"\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using ExtractPatterns Converter in Go\nDESCRIPTION: The ExtractPatterns converter returns a pcommon.Map struct that is a result of extracting named capture groups from the target string using a regex pattern. It requires at least one named capture group in the pattern.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_20\n\nLANGUAGE: Go\nCODE:\n```\nExtractPatterns(resource.attributes[\"k8s.change_cause\"], \"GIT_SHA=(?P<git.sha>\\w+)\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nExtractPatterns(log.body, \"^(?P<timestamp>\\\\w+ \\\\w+ [0-9]+:[0-9]+:[0-9]+) (?P<hostname>([A-Za-z0-9-_]+)) (?P<process>\\\\w+)(\\\\[(?P<pid>\\\\d+)\\\\])?: (?P<message>.*)$\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Log Receiver for Container Stdout Collection\nDESCRIPTION: Example configuration for collecting stdout logs from all containers using the k8slog receiver in daemonset mode. The configuration includes a recombine operator to handle multi-line logs by identifying new log entries with timestamp patterns and limiting maximum log size.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  k8slog:\n    discovery:\n      mode: daemonset-stdout\n    operators:\n      - type: recombine\n        combine_field: body\n        is_first_entry: body matches \"^\\\\d{4}-\\\\d{2}-\\\\d{2}\"\n        max_log_size: 128kb\n        source_identifier: attributes[\"k8s.pod.uid\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon EKS Resource Detection\nDESCRIPTION: YAML configuration for detecting EKS resources using environment variables and EKS-specific detection methods.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/eks:\n    detectors: [env, eks]\n    timeout: 15s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Metric Format Configuration Example\nDESCRIPTION: Example showing how to configure metric output format with custom name and attributes for the Memory Committed Bytes counter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsperfcountersreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  windowsperfcounters:\n    metrics:\n      bytes.committed:\n        description: the number of bytes committed to memory\n        unit: By\n        gauge:\n    collection_interval: 30s\n    perfcounters:\n    - object: Memory\n      counters:\n        - name: Committed Bytes\n          metric: bytes.committed\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [windowsperfcounters]\n```\n\n----------------------------------------\n\nTITLE: Defining Circular Dependencies in OpenTelemetry Collector Contrib\nDESCRIPTION: This snippet documents known circular dependencies between modules in the OpenTelemetry Collector Contrib project. It includes dependencies between Datadog components and OTel Arrow components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/tidylist/allow-circular.txt#2025-04-10_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# This file lists modules that are known to have intra-repository circular dependencies.\n# The `make tidylist` command will check against this list and error out if circular dependencies\n# are accidentally added or removed.\n\n# exporter/datadog <-> connector/datadog\nexporter/datadogexporter\nconnector/datadogconnector\n\n# receiver/otelarrow <-> internal/otelarrow <-> exporter/otelarrow\nreceiver/otelarrowreceiver\nexporter/otelarrowexporter\ninternal/otelarrow\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Task Definition Filters in YAML\nDESCRIPTION: YAML configuration example for filtering ECS tasks based on task definition ARN patterns. This can be used for both long-running applications and short-running batch jobs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Example 1: Matches all the tasks created from task definition that contains memcached in its arn\narn_pattern: \"*memcached.*\"\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics Example Format\nDESCRIPTION: Example of Prometheus text format metrics showing type hints, help text, and multiple data points for a container CPU metric.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_0\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP container_cpu_load_average_10s Value of container cpu load average over the last 10 seconds.\n# TYPE container_cpu_load_average_10s gauge\ncontainer_cpu_load_average_10s{id=\"/\",image=\"\",name=\"\"} 0\ncontainer_cpu_load_average_10s{id=\"/000-metadata\",image=\"\",name=\"\"} 0\ncontainer_cpu_load_average_10s{id=\"/001-sysfs\",image=\"\",name=\"\"} 0\n```\n\n----------------------------------------\n\nTITLE: Renaming Labels for Multiple Metrics with Regex in YAML\nDESCRIPTION: Renames the 'state' label to 'cpu_state' for all system.cpu metrics using a regular expression pattern.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# for all system.cpu metrics, rename the label state to cpu_state\ninclude: ^system\\.cpu\\.\naction: update\noperations:\n  - action: update_label\n    label: state\n    new_label: cpu_state\n```\n\n----------------------------------------\n\nTITLE: Adding Labels to Specific Metrics in YAML\nDESCRIPTION: Adds a version label to system.cpu.usage metrics with the collector version as the value.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n# for system.cpu.usage_time, add label `version` with value `opentelemetry collector vX.Y.Z` to all points\ninclude: system.cpu.usage\naction: update\noperations:\n  - action: add_label\n    new_label: version\n    new_value: opentelemetry collector {{version}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Selective Metrics for Oracle DB Receiver in YAML\nDESCRIPTION: Example configuration demonstrating how to enable or disable specific metrics for the Oracle DB receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/oracledbreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  oracledb:\n    datasource: \"oracle://otel:password@localhost:51521/XE\"\n    metrics:\n      oracledb.query.cpu_time:\n        enabled: false\n      oracledb.query.physical_read_requests:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Windows Event Log Collection in YAML\nDESCRIPTION: Example YAML configuration for setting up a remote Windows event log receiver. This configuration specifies the application channel to monitor and provides remote server connection details including server name, credentials, and domain.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowseventlogreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    windowseventlog:\n        channel: application\n        remote:\n            server:   \"remote-server\"\n            username: \"user\"\n            password: \"password\"\n            domain:   \"domain\"\n```\n\n----------------------------------------\n\nTITLE: MongoDB Atlas Metrics Schema Definition\nDESCRIPTION: Markdown table schema defining the attributes, units, metric types and values for MongoDB Atlas monitoring metrics. Each metric includes detailed attribute specifications and possible values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Description | Values |\n| ---- | ----------- | ------ |\n| cpu_state | CPU state | Str: ``kernel``, ``user``, ``nice``, ``iowait``, ``irq``, ``softirq``, ``guest``, ``steal`` |\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example: Prometheus-v2 Metrics\nDESCRIPTION: Example of Prometheus-v2 metrics in InfluxDB Line Protocol format. It includes gauge, counter, histogram, and summary metrics with various labels and values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/influxdbreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: influxdb\nCODE:\n```\nprometheus,foo=bar cpu_temp=87.332\nprometheus,method=post,code=200 http_requests_total=1027\nprometheus,method=post,code=400 http_requests_total=3\nprometheus,le=0.05 http_request_duration_seconds_bucket=24054\nprometheus,le=0.1  http_request_duration_seconds_bucket=33444\nprometheus,le=0.2  http_request_duration_seconds_bucket=100392\nprometheus,le=0.5  http_request_duration_seconds_bucket=129389\nprometheus,le=1    http_request_duration_seconds_bucket=133988\nprometheus         http_request_duration_seconds_count=144320,http_request_duration_seconds_sum=53423\nprometheus,quantile=0.01 rpc_duration_seconds=3102\nprometheus,quantile=0.05 rpc_duration_seconds=3272\nprometheus,quantile=0.5  rpc_duration_seconds=4773\nprometheus,quantile=0.9  rpc_duration_seconds=9001\nprometheus,quantile=0.99 rpc_duration_seconds=76656\nprometheus               rpc_duration_seconds_count=1.7560473e+07,rpc_duration_seconds_sum=2693\n```\n\n----------------------------------------\n\nTITLE: UUID Generator in Golang\nDESCRIPTION: A function that generates a v4 UUID string with no input parameters required. Used for creating unique identifiers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_76\n\nLANGUAGE: go\nCODE:\n```\nUUID()\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Task Observer Extension in YAML\nDESCRIPTION: Example configuration showing how to set up the ECS Task Observer extension with a receiver creator for Redis monitoring. Demonstrates custom endpoint configuration, port labels, and refresh interval settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecstaskobserver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  ecs_task_observer:\n    endpoint: http://my.task.metadata.endpoint\n    port_labels: [A_DOCKER_LABEL_CONTAINING_DESIRED_PORT, ANOTHER_DOCKER_LABEL_CONTAINING_DESIRED_PORT]\n    refresh_interval: 10s\n\nreceivers:\n  receiver_creator:\n    receivers:\n      redis:\n        rule: type == \"container\" && name matches \"redis\"\n        config:\n          password: `container.labels[\"SECRET\"]`\n    watch_observers: [ecs_task_observer]\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional MongoDB Atlas Metrics in YAML\nDESCRIPTION: YAML configuration snippet showing how to enable optional metrics in MongoDB Atlas monitoring. This configuration can be applied to enable specific metrics that are not emitted by default.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/documentation.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Access Log Polling in YAML\nDESCRIPTION: Configuration to poll access logs from MongoDB Atlas API for a specific project and cluster.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    public_key: <redacted>\n    private_key: <redacted>\n    logs:\n      enabled: true\n      projects:\n      - name: Project 0\n        include_clusters: [Cluster0]\n        access_logs:\n          page_size: 20000\n          max_pages: 10\n          poll_interval: 5m\n    storage: file_storage\n```\n\n----------------------------------------\n\nTITLE: Configuring Top Query Collection in YAML\nDESCRIPTION: YAML configuration for enabling top query collection and query sample collection with custom parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sqlserver:\n    collection_interval: 5s\n    username: sa\n    password: securepassword\n    server: 0.0.0.0\n    port: 1433\n    top_query_collection:\n      enabled: true\n      lookback_time: 60\n      max_query_sample_count: 1000\n      top_query_count: 200\n    query_sample_collection:\n      enabled: true\n      max_rows_per_query: 1450\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Metrics in YAML Configuration\nDESCRIPTION: This YAML snippet shows how to disable specific metrics in the collector configuration. Replace <metric_name> with the actual metric name to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/saphanareceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Creating a New Metric from Two Existing Metrics in YAML\nDESCRIPTION: This example demonstrates how to create a new metric 'pod.cpu.utilized' by dividing 'pod.cpu.usage' by 'node.cpu.limit' using the calculate operation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricsgenerationprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# create pod.cpu.utilized following (pod.cpu.usage / node.cpu.limit)\nrules:\n    - name: pod.cpu.utilized\n      type: calculate\n      metric1: pod.cpu.usage\n      metric2: node.cpu.limit\n      operation: divide\n```\n\n----------------------------------------\n\nTITLE: Collapsing Multiple Spaces using regex_replace Operator in YAML\nDESCRIPTION: Configuration example that uses the regex_replace operator to replace multiple consecutive spaces with a single space in the body field of log entries.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_replace.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_replace\n  regex: \" +\"\n  replace_with: \" \"\n  field: body\n```\n\n----------------------------------------\n\nTITLE: Output Example for Docker Format Logs\nDESCRIPTION: Processed output showing how the recombine operator joins multiple Docker format log entries into a single log message with proper newlines and extracted Kubernetes resource metadata.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2024-03-30 08:31:20.545192187 +0000 UTC\",\n  \"body\": \"2024-07-03T13:50:49.526Z  WARN 1 --- [http-nio-8080-exec-6] c.m.antifraud.FraudDetectionController   : checkOrder\\njava.net.ConnectException: Failed to connect to\",\n  \"attributes\": {\n    \"time\": \"2024-03-30T08:31:20.545192187Z\",\n    \"log.iostream\":                \"stdout\",\n    \"log.file.path\": \"/var/log/pods/some_kube-controller-kind-control-plane_49cc7c1fd3702c40b2686ea7486091d6/kube-controller/1.log\"\n  },\n  \"resource\": {\n    \"attributes\": {\n      \"k8s.pod.name\":                \"kube-controller-kind-control-plane\",\n      \"k8s.pod.uid\":                 \"49cc7c1fd3702c40b2686ea7486091d6\",\n      \"k8s.container.name\":          \"kube-controller\",\n      \"k8s.container.restart_count\": \"1\",\n      \"k8s.namespace.name\":          \"some\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Transform Processor Format in YAML\nDESCRIPTION: Example showing the basic format for configuring transform processor with string statements.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  <trace|metric|log>_statements:\n    - string\n    - string\n    - string\n```\n\n----------------------------------------\n\nTITLE: Using Both Include and Exclude in Cumulative to Delta Processor\nDESCRIPTION: Configuration example showing how to include metrics with 'metric' in their name but exclude histogram metrics, demonstrating how to combine include and exclude filters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/cumulativetodeltaprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: cumulativetodelta\n    cumulativetodelta:\n\n        # Convert cumulative sum metrics with 'metric' in their name,\n        # but exclude histogram metrics\n        include:\n            metrics:\n                - \".*metric.*\"\n            match_type: regexp\n        exclude:\n          metric_types:\n            - histogram\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple File Input in YAML\nDESCRIPTION: A basic configuration example for the file_input operator that monitors a single log file. This configuration includes only the essential parameters to start collecting logs from a specific file path.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/file_input.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: file_input\n  include:\n    - ./test.log\n```\n\n----------------------------------------\n\nTITLE: Demonstrating body_with_inline_attributes Mode JSON Output Format\nDESCRIPTION: Example JSON output showing how the body_with_inline_attributes mode formats logs with their resource attributes and log attributes as key-value pairs in the JSON structure. The example shows two log entries in an array format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/jsonlogencodingextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"body\": {\n      \"log\": \"test\"\n    },\n    \"resourceAttributes\": {\n      \"test\": \"logs-test\"\n    },\n    \"logAttributes\": {\n      \"foo\": \"bar\"\n    }\n  },\n  {\n    \"body\": \"log testing\",\n    \"resource\": {\n      \"test\": \"logs-test\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Scaling an Existing Metric to Create a New Metric in YAML\nDESCRIPTION: This example shows how to create a new metric 'pod.memory.usage.bytes' by scaling the value of 'pod.memory.usage.megabytes' using the scale operation and a multiplication factor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricsgenerationprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# create pod.memory.usage.bytes from pod.memory.usage.megabytes\nrules:\n    - name: pod.memory.usage.bytes\n      unit: Bytes\n      type: scale\n      metric1: pod.memory.usage.megabytes\n      operation: multiply\n      scale_by: 1048576\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Parser Operator in YAML\nDESCRIPTION: Example configuration for a standalone time_parser operator. It parses a timestamp from a specific field in the log entry using the strptime layout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: time_parser\n  parse_from: body.timestamp_field\n  layout_type: strptime\n  layout: '%Y-%m-%d'\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Adding Value to Resource\nDESCRIPTION: This JSON shows the output entry after adding a value to the resource using the 'add' operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"key2\": \"val2\"\n  },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Routing Processor in OpenTelemetry Collector\nDESCRIPTION: Example configuration for the routing processor that routes telemetry data based on a tenant attribute. It defines routing rules to direct data from the 'acme' tenant to a specific Jaeger exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/routingprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  routing:\n    from_attribute: X-Tenant\n    default_exporters: [jaeger]\n    table:\n    - value: acme\n      exporters: [jaeger/acme]\nexporters:\n  jaeger:\n    endpoint: localhost:14250\n  jaeger/acme:\n    endpoint: localhost:24250\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [routing]\n      exporters: [jaeger, jaeger/acme]\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle DB Receiver with Secondary Option in YAML\nDESCRIPTION: Example configuration for the Oracle DB receiver using the secondary configuration option with separate connection parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/oracledbreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  oracledb:\n    endpoint: localhost:51521\n    password: p@sswo%d\n    service: XE\n    username: otel\n```\n\n----------------------------------------\n\nTITLE: Configuring GitLab Receiver with YAML\nDESCRIPTION: Example configuration for the GitLab receiver showing webhook setup with custom endpoint, paths, secret authentication, and required headers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/gitlabreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    gitlab:\n        webhook:\n            endpoint: localhost:19418\n            path: /events\n            health_path: /health\n            secret: ${env:SECRET_STRING_VAR}\n            required_headers:\n                WAF-Header: \"value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Read-Only Endpoint in YAML\nDESCRIPTION: Example configuration for accessing the Kubelet read-only metrics endpoint on port 10255 without authentication, suitable for environments where the read-only Kubelet endpoint is enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 20s\n    auth_type: \"none\"\n    endpoint: \"http://${env:K8S_NODE_NAME}:10255\"\nexporters:\n  file:\n    path: \"fileexporter.txt\"\nservice:\n  pipelines:\n    metrics:\n      receivers: [kubeletstats]\n      exporters: [file]\n```\n\n----------------------------------------\n\nTITLE: Migrating from match_once Using Multiple Routers in OpenTelemetry\nDESCRIPTION: Demonstrates how to migrate from the deprecated 'match_once: false' configuration by splitting a router into multiple parallel routers. This approach allows the same data to match routes in different routers, achieving similar functionality.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrouting/env:\n  table:\n    - condition: attributes[\"env\"] == \"prod\"\n       pipelines: [ logs/prod ]\n    - condition: attributes[\"env\"] == \"dev\"\n       pipelines: [ logs/dev ]\nrouting/region:\n  table:\n    - condition: attributes[\"region\"] == \"east\"\n       pipelines: [ logs/east ]\n    - condition: attributes[\"region\"] == \"west\"\n       pipelines: [ logs/west ]\n\nservice:\n  pipelines:\n    logs/in::exporters: [routing/env, routing/region]\n    logs/prod::receivers: [routing/env]\n    logs/dev::receivers: [routing/env]\n    logs/east::receivers: [routing/region]\n    logs/west::receivers: [routing/region]\n```\n\n----------------------------------------\n\nTITLE: Migrating from match_once Using Multiple Routers in OpenTelemetry\nDESCRIPTION: Demonstrates how to migrate from the deprecated 'match_once: false' configuration by splitting a router into multiple parallel routers. This approach allows the same data to match routes in different routers, achieving similar functionality.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nrouting/env:\n  table:\n    - condition: attributes[\"env\"] == \"prod\"\n       pipelines: [ logs/prod ]\n    - condition: attributes[\"env\"] == \"dev\"\n       pipelines: [ logs/dev ]\nrouting/region:\n  table:\n    - condition: attributes[\"region\"] == \"east\"\n       pipelines: [ logs/east ]\n    - condition: attributes[\"region\"] == \"west\"\n       pipelines: [ logs/west ]\n\nservice:\n  pipelines:\n    logs/in::exporters: [routing/env, routing/region]\n    logs/prod::receivers: [routing/env]\n    logs/dev::receivers: [routing/env]\n    logs/east::receivers: [routing/region]\n    logs/west::receivers: [routing/region]\n```\n\n----------------------------------------\n\nTITLE: Configuring K8s Node Metadata Resource Detection in YAML\nDESCRIPTION: Example YAML configuration for the K8s Node Metadata resource detector. It includes required ClusterRole permissions and processor configuration options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nkind: ClusterRole\nmetadata:\n  name: otel-collector\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n```\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/k8snode:\n    detectors: [k8snode]\n```\n\nLANGUAGE: yaml\nCODE:\n```\n        env:\n          - name: K8S_NODE_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: spec.nodeName\n```\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/k8snode:\n    detectors: [k8snode]\n    k8snode:\n      node_from_env_var: \"my_custom_var\"\n```\n\nLANGUAGE: yaml\nCODE:\n```\n        env:\n          - name: my_custom_var\n            valueFrom:\n              fieldRef:\n                fieldPath: spec.nodeName\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Inclusion Paths for Kubernetes in YAML\nDESCRIPTION: A YAML configuration snippet showing how to include logs from the 'default' Kubernetes namespace by using path patterns. This example demonstrates the pattern matching syntax for including specific log files while leaving the exclude list empty.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninclude:\n    - /var/log/pods/default_*/*/*.log\nexclude: []\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter Character Limits in YAML\nDESCRIPTION: YAML configuration snippet for handling character limits on metric names and dimensions in the SignalFx exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_45\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    # Handle character limits on metric names and dimensions\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in Splunk Enterprise\nDESCRIPTION: YAML configuration to enable optional metrics that are not enabled by default in Splunk Enterprise monitoring. Each metric can be individually enabled using this configuration pattern.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/splunkenterprisereceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Data Sources for Service Graph Visualization\nDESCRIPTION: YAML configuration for setting up Prometheus and Tempo data sources in Grafana to visualize service graphs. It defines the necessary URLs and links the Tempo data source to Prometheus for service map functionality.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/servicegraphconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: 1\ndatasources:\n  # Prometheus backend where metrics are sent\n  - name: Prometheus\n    type: prometheus\n    uid: prometheus\n    url: <prometheus-url>\n    jsonData:\n        httpMethod: GET\n    version: 1\n  - name: Tempo\n    type: tempo\n    uid: tempo\n    url: <tempo-url>\n    jsonData:\n      httpMethod: GET\n      serviceMap:\n        datasourceUid: 'prometheus'\n    version: 1\n```\n\n----------------------------------------\n\nTITLE: UnixSeconds Time Converter in Golang\nDESCRIPTION: The UnixSeconds Converter returns the seconds elapsed since January 1, 1970 UTC from a time.Time object. Returns an int64 value representing seconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_68\n\nLANGUAGE: go\nCODE:\n```\nUnixSeconds(Time(\"02/04/2023\", \"%m/%d/%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Settings for API URL in SignalFx Exporter (YAML)\nDESCRIPTION: Example configuration for setting up TLS with a custom certificate authority file for secure connections to the SignalFx API endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napi_tls:\n    ca_file: \"/etc/opt/certs/ca.pem\"\n```\n\n----------------------------------------\n\nTITLE: Nested JSON Array Structure\nDESCRIPTION: Example of a complex JSON array containing nested objects and arrays\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_array_parser.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\\\"Hello\\\", 42, {\\\"name\\\": \\\"Alice\\\", \\\"age\\\": 25}, [1, 2, 3], true, null]\n```\n\n----------------------------------------\n\nTITLE: Configuring CORS for OpenCensus Receiver in YAML\nDESCRIPTION: YAML configuration example for enabling CORS (Cross-Origin Resource Sharing) on the OpenCensus receiver. This allows specifying allowed origins for cross-origin requests.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/opencensusreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  opencensus:\n    cors_allowed_origins:\n    - http://test.com\n    # Origins can have wildcards with *, use * by itself to match any origin.\n    - https://*.example.com\n```\n\n----------------------------------------\n\nTITLE: Basic Severity Mapping Configuration in YAML\nDESCRIPTION: Example of basic severity mapping configuration showing different ways to map values to severity levels\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmapping:\n  # single value to be parsed as \"error\"\n  error: oops\n\n  # list of values to be parsed as \"warn\"\n  warn:\n    - hey!\n    - YSK\n\n  # range of values to be parsed as \"info\"\n  info:\n    - min: 300\n      max: 399\n\n  # special value representing the range 200-299, to be parsed as \"debug\"\n  debug: 2xx\n\n  # single value to be parsed as a \"info3\"\n  info3: medium\n\n  # mix and match the above concepts\n  fatal:\n    - really serious\n    - min: 9001\n      max: 9050\n    - 5xx\n```\n\n----------------------------------------\n\nTITLE: JSON Input/Output Examples\nDESCRIPTION: Example JSON showing input and output transformations for severity parsing\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"default\",\n  \"body\": {\n    \"severity_field\": \"ERROR\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Group by Trace Processor in YAML\nDESCRIPTION: Example YAML configuration for the Group by Trace processor, showing default settings and a custom configuration with specific wait duration, number of traces, and workers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbytraceprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  groupbytrace:\n  groupbytrace/2:\n    wait_duration: 10s\n    num_traces: 1000\n    num_workers: 2\n```\n\n----------------------------------------\n\nTITLE: Aggregating Label Values in YAML\nDESCRIPTION: Combines multiple label values into a single value with summation for system.memory.usage metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n# aggregate data points with state label value slab_reclaimable & slab_unreclaimable using summation into slab\ninclude: system.memory.usage\naction: update\noperations:\n  - action: aggregate_label_values\n    label: state\n    aggregated_values: [ slab_reclaimable, slab_unreclaimable ]\n    new_value: slab \n    aggregation_type: sum\n```\n\n----------------------------------------\n\nTITLE: Configuring Probabilistic Sampler with Sampling Priority\nDESCRIPTION: YAML configuration example showing how to use a custom attribute named 'priority' to influence sampling decisions for log records.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/probabilisticsamplerprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 15\n    sampling_priority: priority\n```\n\n----------------------------------------\n\nTITLE: Default Chrony Receiver Configuration in YAML\nDESCRIPTION: Default configuration for the Chrony receiver showing the standard endpoint and timeout settings. The endpoint points to the default chronyd socket and includes a 10-second timeout for responses.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/chronyreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nchrony/defaults:\n  endpoint: unix:///var/run/chrony/chronyd.sock # The default port by chronyd to allow cmd access\n  timeout: 10s # Allowing at least 10s for chronyd to respond before giving up\n\nchrony:\n  # This will result in the same configuration as above\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS EMF Exporter with Metric Declarations\nDESCRIPTION: This configuration demonstrates how to use metric declarations to selectively export metrics with specific dimension sets. It includes examples of exporting metrics without labels and metrics with specific dimension combinations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awsemfexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  awsemf:\n    region: 'us-west-2'\n    output_destination: stdout\n    dimension_rollup_option: \"NoDimensionRollup\"\n    metric_declarations:\n      - dimensions: [[]]\n        metric_name_selectors:\n          # Metric without label\n          - \"^node_load15$\"\n      - dimensions: [[device, fstype], []]\n        metric_name_selectors:\n          - \"^node_filesystem_readonly$\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NTP Receiver in OpenTelemetry Collector\nDESCRIPTION: Example configuration for the NTP receiver showing how to set the endpoint, collection interval, and initial delay. The configuration specifies connection to pool.ntp.org with hourly collection and a 5-minute initial delay.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/ntpreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  ntp:\n    endpoint: pool.ntp.org:123\n    collection_interval: 1h\n    initial_delay: 5m\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter with Specific CPU Metrics in YAML\nDESCRIPTION: This configuration example shows how to include only specific CPU metrics in the SignalFx exporter. It demonstrates including cpu.idle metrics and cpu.interrupt metrics with a specific dimension.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    include_metrics:\n      - metric_name: \"cpu.idle\"\n      - metric_name: \"cpu.interrupt\"\n        dimensions:\n          cpu: [\"*\"]\n```\n\n----------------------------------------\n\nTITLE: Updating LogQL Query for Resource Attributes\nDESCRIPTION: Example of how to update a LogQL query to use the new format for querying resource attributes in Loki after migrating from the OpenTelemetry Collector Loki exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: logql\nCODE:\n```\nBEFORE\n{exporter=\"OTLP\", job=\"frontend\"} | json | resources_deployment_environment=\"production\"\n\nAFTER\n{service_name=\"frontend\"} | deployment_environment=\"production\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Data Points by Label Value in YAML\nDESCRIPTION: Removes data points with a specific label value from system.cpu.usage metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# deletes all data points with the label value 'idle' of the label 'state'\ninclude: system.cpu.usage\naction: update\noperations:\n  - action: delete_label_value\n    label: state\n    label_value: idle\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Metrics in YAML Configuration\nDESCRIPTION: This YAML configuration snippet shows how to disable a default metric in the kubeletstats collector. Replace <metric_name> with the specific metric you want to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Transform Processor for Hardware Metrics in YAML\nDESCRIPTION: This snippet shows how to configure a transform processor using OTTL to map required attributes for BMC Helix, including entityName, entityTypeId, and instanceName based on metric names and existing attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/bmchelixexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntransform/hw_to_helix:\n   # Apply transformations to all metrics\n    metric_statements:\n\n      - context: datapoint\n        statements:\n          # Create a new attribute 'entityName' with the value of 'id'\n          - set(attributes[\"entityName\"], attributes[\"id\"]) where attributes[\"id\"] != nil\n          # Create a new attribute 'instanceName' with the value of 'name'\n          - set(attributes[\"instanceName\"], attributes[\"name\"]) where attributes[\"name\"] != nil\n\n      - context: datapoint\n        conditions:\n          - IsMatch(metric.name, \".*\\.agent\\..*\")\n        statements:\n          - set(attributes[\"entityName\"], attributes[\"host.id\"]) where attributes[\"host.id\"] != nil\n          - set(attributes[\"instanceName\"], attributes[\"service.name\"]) where attributes[\"service.name\"] != nil\n          - set(attributes[\"entityTypeId\"], \"agent\")\n\n      - context: datapoint\n        statements:\n          # Mapping entityTypeId based on metric names and attributes\n          - set(attributes[\"entityTypeId\"], \"connector\") where IsMatch(metric.name, \".*\\.connector\\..*\")\n          - set(attributes[\"entityTypeId\"], \"host\") where IsMatch(metric.name, \".*\\.host\\..*\") or attributes[\"hw.type\"] == \"host\"\n          - set(attributes[\"entityTypeId\"], \"battery\") where IsMatch(metric.name, \"hw\\.battery\\..*\") or attributes[\"hw.type\"] == \"battery\"\n          - set(attributes[\"entityTypeId\"], \"blade\") where IsMatch(metric.name, \"hw\\.blade\\..*\") or attributes[\"hw.type\"] == \"blade\"\n          - set(attributes[\"entityTypeId\"], \"cpu\") where IsMatch(metric.name, \"hw\\.cpu\\..*\") or attributes[\"hw.type\"] == \"cpu\"\n          - set(attributes[\"entityTypeId\"], \"disk_controller\") where IsMatch(metric.name, \"hw\\.disk_controller\\..*\") or attributes[\"hw.type\"] == \"disk_controller\"\n          - set(attributes[\"entityTypeId\"], \"enclosure\") where IsMatch(metric.name, \"hw\\.enclosure\\..*\") or attributes[\"hw.type\"] == \"enclosure\"\n          - set(attributes[\"entityTypeId\"], \"fan\") where IsMatch(metric.name, \"hw\\.fan\\..*\") or attributes[\"hw.type\"] == \"fan\"\n          - set(attributes[\"entityTypeId\"], \"gpu\") where IsMatch(metric.name, \"hw\\.gpu\\..*\") or attributes[\"hw.type\"] == \"gpu\"\n          - set(attributes[\"entityTypeId\"], \"led\") where IsMatch(metric.name, \"hw\\.led\\..*\") or attributes[\"hw.type\"] == \"led\"\n          - set(attributes[\"entityTypeId\"], \"logical_disk\") where IsMatch(metric.name, \"hw\\.logical_disk\\..*\") or attributes[\"hw.type\"] == \"logical_disk\"\n          - set(attributes[\"entityTypeId\"], \"lun\") where IsMatch(metric.name, \"hw\\.lun\\..*\") or attributes[\"hw.type\"] == \"lun\"\n          - set(attributes[\"entityTypeId\"], \"memory\") where IsMatch(metric.name, \"hw\\.memory\\..*\") or attributes[\"hw.type\"] == \"memory\"\n          - set(attributes[\"entityTypeId\"], \"network\") where IsMatch(metric.name, \"hw\\.network\\..*\") or attributes[\"hw.type\"] == \"network\"\n          - set(attributes[\"entityTypeId\"], \"other_device\") where IsMatch(metric.name, \"hw\\.other_device\\..*\") or attributes[\"hw.type\"] == \"other_device\"\n          - set(attributes[\"entityTypeId\"], \"physical_disk\") where IsMatch(metric.name, \"hw\\.physical_disk\\..*\") or attributes[\"hw.type\"] == \"physical_disk\"\n          - set(attributes[\"entityTypeId\"], \"power_supply\") where IsMatch(metric.name, \"hw\\.power_supply\\..*\") or attributes[\"hw.type\"] == \"power_supply\"\n          - set(attributes[\"entityTypeId\"], \"robotics\") where IsMatch(metric.name, \"hw\\.robotics\\..*\") or attributes[\"hw.type\"] == \"robotics\"\n          - set(attributes[\"entityTypeId\"], \"tape_drive\") where IsMatch(metric.name, \"hw\\.tape_drive\\..*\") or attributes[\"hw.type\"] == \"tape_drive\"\n          - set(attributes[\"entityTypeId\"], \"temperature\") where IsMatch(metric.name, \"hw\\.temperature.*\") or attributes[\"hw.type\"] == \"temperature\"\n          - set(attributes[\"entityTypeId\"], \"vm\") where IsMatch(metric.name, \"hw\\.vm\\..*\") or attributes[\"hw.type\"] == \"vm\"\n          - set(attributes[\"entityTypeId\"], \"voltage\") where IsMatch(metric.name, \"hw\\.voltage.*\") or attributes[\"hw.type\"] == \"voltage\"\n```\n\n----------------------------------------\n\nTITLE: Migrating Source Templates Configuration in YAML\nDESCRIPTION: Example demonstrating how to migrate from the old source template configuration to using the Transform Processor in the new collector architecture.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sumologicexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# before switch to new collector\nexporters:\n  sumologic:\n    source_category: \"%{foo}/constant/%{bar}\"\n# after switch to new collector\nprocessors:\n  transformprocessor:\n    log_statements:\n      context: log\n      statements:\n        # set default value to unknown\n        - set(attributes[\"foo\"], \"unknown\") where attributes[\"foo\"] == nil\n        - set(attributes[\"bar\"], \"unknown\") where attributes[\"foo\"] == nil\n        # set _sourceCategory as \"%{foo}/constant/%{bar}\"\n        - set(resource.attributes[\"_sourceCategory\"], Concat([attributes[\"foo\"], \"/constant/\", attributes[\"bar\"]], \"\"))\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Kafka Metrics in YAML\nDESCRIPTION: YAML configuration template for enabling optional Kafka metrics that are not collected by default. This configuration can be applied to any optional metric to enable its collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkametricsreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Using Basic Configuration Style for Transform Processor\nDESCRIPTION: Example of the basic configuration style for transform processor, which simplifies statement organization by automatically determining the appropriate context.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmetric_statements:\n  - limit(datapoint.attributes, 100, [\"host.name\"])\n  - convert_sum_to_gauge() where metric.name == \"system.processes.count\" \n```\n\n----------------------------------------\n\nTITLE: Configuring Cgroup Runtime Extension in YAML\nDESCRIPTION: Example YAML configuration for the cgroupruntime extension showing how to set GOMAXPROCS and GOMEMLIMIT options. Demonstrates setting the enabled flag and memory ratio parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/cgroupruntimeextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextension:\n    # processor name: cgroupruntime\n    cgroupruntime:\n      gomaxprocs:\n        enabled: true\n      gomemlimit:\n        enabled: true\n        ratio: 0.8\n```\n\n----------------------------------------\n\nTITLE: Configuring SAPM Receiver in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration example demonstrates how to set up the SAPM receiver with custom endpoint, access token passthrough, and TLS settings. It shows the basic structure and available options for the SAPM receiver configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sapmreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  sapm:\n    endpoint: localhost:7276\n    access_token_passthrough: true\n    tls:\n      cert_file: /test.crt\n      key_file: /test.key\n```\n\n----------------------------------------\n\nTITLE: Configuring Detailed Sum Connector with Conditions\nDESCRIPTION: Advanced configuration example demonstrating how to process logs with conditions and attribute mapping. Shows usage of the conditions field for filtering and attributes field for creating separate datapoints based on unique attribute combinations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/sumconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo:\nconnectors:\n  sum:\n    logs:\n      checkout.total:\n        source_attribute: total.payment\n        conditions:\n          - attributes[\"total.payment\"] != \"NULL\"\n        attributes:\n          - key: payment.processor\n            default_value: unspecified_processor\nexporters:\n  bar:\n\nservice:\n  pipelines:\n    metrics/sum:\n       receivers: [sum]\n       exporters: [bar]\n    logs:\n       receivers: [foo]\n       exporters: [sum]\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray Trace Segment JSON Structure\nDESCRIPTION: This JSON structure represents an AWS X-Ray trace segment. It includes details about subsegments, timings, and an error scenario involving a DynamoDB resource not found exception. The structure is nested and contains information about the execution flow of a distributed application.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/ddbSample.txt#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"acfaa7e3fe3aab03\",\n    \"name\": \"response\",\n    \"start_time\": 1596566305.5875454,\n    \"end_time\": 1596566305.592695,\n    \"Dummy\": false\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"ba8d350c0e8cdc4b\",\n    \"name\": \"wait\",\n    \"start_time\": 1596566305.592807,\n    \"end_time\": 1596566305.5928102,\n    \"fault\": true,\n    \"cause\": {\n        \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n        \"exceptions\": [\n            {\n                \"id\": \"5a07f08a8c260405\",\n                \"type\": \"dynamodb.ResourceNotFoundException\",\n                \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                \"stack\": [\n                    {\n                        \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/aws.go\",\n                        \"line\": 149,\n                        \"label\": \"glob..func8\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/aws/request/handlers.go\",\n                        \"line\": 267,\n                        \"label\": \"(*HandlerList).Run\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/aws/request/request.go\",\n                        \"line\": 535,\n                        \"label\": \"(*Request).Send\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/service/dynamodb/api.go\",\n                        \"line\": 3414,\n                        \"label\": \"(*DynamoDB).PutItemWithContext\"\n                    },\n                    {\n                        \"path\": \"sampleapp/sample.go\",\n                        \"line\": 62,\n                        \"label\": \"ddbExpectedFailure.func1\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/capture.go\",\n                        \"line\": 45,\n                        \"label\": \"Capture\"\n                    },\n                    {\n                        \"path\": \"sampleapp/sample.go\",\n                        \"line\": 41,\n                        \"label\": \"ddbExpectedFailure\"\n                    },\n                    {\n                        \"path\": \"sampleapp/sample.go\",\n                        \"line\": 36,\n                        \"label\": \"main\"\n                    },\n                    {\n                        \"path\": \"runtime/proc.go\",\n                        \"line\": 203,\n                        \"label\": \"main\"\n                    },\n                    {\n                        \"path\": \"runtime/asm_amd64.s\",\n                        \"line\": 1373,\n                        \"label\": \"goexit\"\n                    }\n                ],\n                \"remote\": true\n            }\n        ]\n    },\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: File Log Receiver Configuration Table\nDESCRIPTION: Markdown table documenting all configuration fields, their default values, and descriptions for the file log receiver component.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Field                                 | Default                              | Description                                                                                                                                                                                                                                     |\n|---------------------------------------|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `include`                             | required                             | A list of file glob patterns that match the file paths to be read.                                                                                                                                                                                              |\n| `exclude`                             | []                                   | A list of file glob patterns to exclude from reading. This is applied against the paths matched by `include`.                                                                                                                                                   |\n| `exclude_older_than`                  |                                      | Exclude files whose modification time is older than the specified [age](#time-parameters).                                                                                                                                                                      |\n| `start_at`                            | `end`                                | At startup, where to start reading logs from the file. Options are `beginning` or `end`.                                                                                                                                                                        |\n| `multiline`                           |                                      | A `multiline` configuration block. See [below](#multiline-configuration) for more details.                                                                                                                                                                      |\n| `force_flush_period`                  | `500ms`                              | [Time](#time-parameters) since last time new data was found in the file, after which a partial log at the end of the file may be emitted.                                                                                                                       |\n| `encoding`                            | `utf-8`                              | The encoding of the file being read. See the list of [supported encodings below](#supported-encodings) for available options.                                                                                                                                   |\n| `preserve_leading_whitespaces`        | `false`                              | Whether to preserve leading whitespaces.                                                                                                                                                                                                                        |\n| `preserve_trailing_whitespaces`       | `false`                              | Whether to preserve trailing whitespaces.                                                                                                                                                                                                                       |\n| `include_file_name`                   | `true`                               | Whether to add the file name as the attribute `log.file.name`.                                                                                                                                                                                                  |\n| `include_file_path`                   | `false`                              | Whether to add the file path as the attribute `log.file.path`.                                                                                                                                                                                                  |\n| `include_file_name_resolved`          | `false`                              | Whether to add the file name after symlinks resolution as the attribute `log.file.name_resolved`.                                                                                                                                                               |\n| `include_file_path_resolved`          | `false`                              | Whether to add the file path after symlinks resolution as the attribute `log.file.path_resolved`.                                                                                                                                                               |\n| `include_file_owner_name`             | `false`                              | Whether to add the file owner name as the attribute `log.file.owner.name`. Not supported for windows.                                                                                                                                                           |\n| `include_file_owner_group_name`       | `false`                              | Whether to add the file group name as the attribute `log.file.owner.group.name`. Not supported for windows.                                                                                                                                                     |\n| `include_file_record_number`          | `false`                              | Whether to add the record number in the file as the attribute `log.file.record_number`.                                                                                                                                                                         |\n| `poll_interval`                       | 200ms                                | The [duration](#time-parameters) between filesystem polls.                                                                                                                                                                                                      |\n| `fingerprint_size`                    | `1kb`                                | The number of bytes with which to identify a file. The first bytes in the file are used as the fingerprint. Decreasing this value at any point will cause existing fingerprints to forgotten, meaning that all files will be read from the beginning (one time) |\n| `initial_buffer_size`                 | `16KiB`                              | The initial size of the to read buffer for headers and logs, the buffer will be grown as necessary. Larger values may lead to unnecessary large buffer allocations, and smaller values may lead to lots of copies while growing the buffer.                     |\n| `max_log_size`                        | `1MiB`                               | The maximum size of a log entry to read. A log entry will be truncated if it is larger than `max_log_size`. Protects against reading large amounts of data into memory.                                                                                         |\n| `max_concurrent_files`                | 1024                                 | The maximum number of log files from which logs will be read concurrently. If the number of files matched in the `include` pattern exceeds this number, then files will be processed in batches.                                                                |\n| `max_batches`                         | 0                                    | Only applicable when files must be batched in order to respect `max_concurrent_files`. This value limits the number of batches that will be processed during a single poll interval. A value of 0 indicates no limit.                                           |\n| `delete_after_read`                   | `false`                              | If `true`, each log file will be read and then immediately deleted. Requires that the `filelog.allowFileDeletion` feature gate is enabled. Must be `false` when `start_at` is set to `end`.                                                                     |\n| `acquire_fs_lock`                     | `false`                              | Whether to attempt to acquire a filesystem lock before reading a file (Unix only).                                                                                                                                                                              |\n| `attributes`                          | {}                                   | A map of `key: value` pairs to add to the entry's attributes.                                                                                                                                                                                                   |\n| `resource`                            | {}                                   | A map of `key: value` pairs to add to the entry's resource.                                                                                                                                                                                                     |\n| `operators`                           | []                                   | An array of [operators](../../pkg/stanza/docs/operators/README.md#what-operators-are-available). See below for more details.                                                                                                                                    |\n```\n\n----------------------------------------\n\nTITLE: SQL Server Execution Plan XML Structure\nDESCRIPTION: Complex SQL Server execution plan XML showing nested loop joins with scalar operations and aggregations. Includes cost estimates, parallel execution details, and output column references.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/queryTextAndPlanQueryData.txt#2025-04-10_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\nimateCPU=\\\"0.000226396\\\" AvgRowSize=\\\"8184\\\" EstimatedTotalSubtreeCost=\\\"0.0192068\\\" Parallel=\\\"0\\\" EstimateRebinds=\\\"0\\\" EstimateRewinds=\\\"0\\\" EstimatedExecutionMode=\\\"Row\\\"><OutputList><ColumnReference Column=\\\"Union1105\\\"/><ColumnReference Column=\\\"Union1106\\\"/><ColumnReference Column=\\\"Expr1161\\\"/><ColumnReference Column=\\\"Expr1162\\\"/><ColumnReference Column=\\\"Expr1163\\\"/><ColumnReference Column=\\\"Expr1164\\\"/><ColumnReference Column=\\\"Expr1165\\\"/><ColumnReference Column=\\\"Expr1166\\\"/><ColumnReference Column=\\\"Expr1167\\\"/><ColumnReference Column=\\\"Expr1168\\\"/><ColumnReference Column=\\\"Expr1169\\\"/><ColumnReference Column=\\\"Union1246\\\"/><ColumnReference Column=\\\"Union1247\\\"/><ColumnReference Table=\\\"[FNGETQUERYPLAN]\\\" Column=\\\"query_plan\\\"/><ColumnReference Table=\\\"[FNGETSQL]\\\" Column=\\\"text\\\"/></OutputList>\n```\n\n----------------------------------------\n\nTITLE: Configuring Service Principal Authentication with Client Secret in Azure Auth Extension\nDESCRIPTION: YAML configuration example for the Azure authenticator extension using service principal authentication with a client secret. This requires specifying client_id, tenant_id, and client_secret parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/azureauthextension/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  azureauth:\n    service_principal:\n      client_id: ${CLIENT_ID}\n      tenant_id: ${TENANT_ID}\n      client_secret: ${CLIENT_SECRET}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Kinetica OpenTelemetry Exporter\nDESCRIPTION: Default log configuration for the Kinetica OpenTelemetry Exporter using Uber zap package and lumberjack for rotating logs. Specifies log level, output paths, error paths, encoder settings, and log rotation parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nlevel: 'info'\ndevelopment: true\ndisableCaller: false\ndisableStacktrace: false\nencoding: 'console'\nencoderConfig:\n  messageKey: 'msg'\n  levelKey: 'level'\n  timeKey: 'ts'\n  nameKey: 'logger'\n  callerKey: 'caller'\n  functionKey: 'function'\n  stacktraceKey: 'stacktrace'\n  skipLineEnding: false\n  lineEnding: \"\\n\"\n  levelEncoder: 'capital'\n  timeEncoder: 'iso8601'\n  durationEncoder: 'string'\n  callerEncoder: 'full'\n  nameEncoder: 'full'\n  consoleSeparator: ' | '\noutputPaths:\n  # Implements loggin to the console\n  - 'stdout'\n  # Implements rolling logs using lumberjack logger; config parameters are supplied as\n  # query params. Here maxSize is 10MB after which the logger rolls over; maximum\n  # number of backups (maxBackups) kept is 5 and maxAge is 10 days.\n  # The name of the log file in this case is \"logs/kinetica-exporter.log\" where the\n  # \"logs\" directory is under the current directory on the local machine.\n  - 'lumberjack://localhost/logs/kinetica-exporter.log?maxSize=10&maxBackups=5&maxAge=10'\nerrorOutputPaths:\n  - 'stderr'\n  - './logs/error_logs'\ninitialFields:\n  app: 'kinetica-exporter'\n```\n\n----------------------------------------\n\nTITLE: RFC3164 Log Record Example - JSON Input\nDESCRIPTION: Example of a log record following the RFC3164 protocol format with basic attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/syslogexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"\",\n  \"timeUnixNano\": 1697062455000000000,\n  \"attributes\":\n  {\n    \"appname\": \"su\",\n    \"hostname\": \"mymachine\",\n    \"message\": \"'su root' failed for lonvick on /dev/pts/8\",\n    \"priority\": 34\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Non-verbose Collector Health Status in JSON\nDESCRIPTION: Example JSON response for a non-verbose collector health status query. It includes only the overall collector status without pipeline and component details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"start_time\": \"2024-01-18T17:39:15.87324-08:00\",\n    \"healthy\": true,\n    \"status\": \"StatusRecoverableError\",\n    \"error\": \"rpc error: code = ResourceExhausted desc = resource exhausted\",\n    \"status_time\": \"2024-01-18T17:39:35.875024-08:00\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Journald Input Configuration with Units and Priority\nDESCRIPTION: Basic configuration example showing how to set up journald_input with specific units to monitor and priority level filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/journald_input.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n  units:\n    - ssh\n    - kubelet\n  priority: info\n```\n\n----------------------------------------\n\nTITLE: Configuring Active Directory DS Receiver in YAML\nDESCRIPTION: Example configuration for the Active Directory DS receiver showing how to set collection interval and disable specific metrics. The receiver connects to Windows Performance Counters to collect Active Directory domain controller metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/activedirectorydsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  active_directory_ds:\n    collection_interval: 10s\n    metrics:\n      # Disable the active_directory.ds.replication.network.io metric from being emitted\n      active_directory.ds.replication.network.io:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Testing Secure Trace Collection with Telemetrygen\nDESCRIPTION: Shell command for sending a test trace to the secure collection pipeline. It uses mTLS with client certificates to authenticate to the tracing endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ cd examples/secure-tracing\n$ chmod +x telemetrygen\n$ ./telemetrygen traces --traces 1 --otlp-endpoint 127.0.0.1:10000 --ca-cert 'certs/ca.crt' --mtls --client-cert 'certs/tracing-client.crt' --client-key 'certs/tracing-client.key'\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Discovery Annotations for Kubernetes Pods\nDESCRIPTION: This snippet demonstrates how to use Kubernetes Pod annotations to configure metrics collection. It includes examples of enabling discovery, defining scrapers, and specifying configuration details for metrics collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nio.opentelemetry.discovery.metrics/enabled: \"true\"\nio.opentelemetry.discovery.metrics/scraper: \"nginx\"\nio.opentelemetry.discovery.metrics/config: |\n  endpoint: \"http://`endpoint`/nginx_status\"\n  collection_interval: \"20s\"\n  initial_delay: \"20s\"\n  read_buffer_size: \"10\"\n  xyz: \"abc\"\n\nio.opentelemetry.discovery.metrics.80/config: |\n  endpoint: \"http://`endpoint`/nginx_status\"\n```\n\n----------------------------------------\n\nTITLE: Default Credentials Authentication Configuration - YAML\nDESCRIPTION: Configuration example for Azure Monitor receiver using Environment Variables for authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azuremonitorreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azuremonitor:\n    subscription_ids: [\"${subscription_id}\"]\n    auth: \"default_credentials\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in YAML\nDESCRIPTION: Configuration snippet to enable optional metrics in the filesystem receiver. Applied by setting enabled: true for specific metric names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/filesystemscraper/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Detailed Datadog Connector Configuration Options\nDESCRIPTION: Comprehensive configuration options for the Datadog connector including trace filtering, span name remapping, stats computation settings, and buffer configurations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/datadogconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconnectors:\n    datadog/connector:\n      traces:\n        # ignore_resources: [\"(GET|POST) /healthcheck\"]\n        # span_name_remappings:\n        #   io.opentelemetry.javaagent.spring.client: spring.client\n        #   instrumentation:express.server: express\n        #   go.opentelemetry.io_contrib_instrumentation_net_http_otelhttp.client: http.client\n        # span_name_as_resource_name: true\n        # compute_stats_by_span_kind: true\n        # peer_tags_aggregation: false\n        # trace_buffer: 1000\n        # peer_tags: [\"tag\"]\n        # resource_attributes_as_container_tags: [\"cloud.availability_zone\", \"cloud.region\"]\n        # bucket_interval: 30s\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes Node Name via Downward API\nDESCRIPTION: Defines how to set up the node name in a Kubernetes pod spec using the downward API to properly identify the node for metrics collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: K8S_NODE_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: spec.nodeName\n```\n\n----------------------------------------\n\nTITLE: Using FormatTime Converter in Go for Time Formatting\nDESCRIPTION: The FormatTime converter formats a time.Time value into a human-readable string according to the specified format. It supports standard Go layout formatting with additional custom substitutions for date and time components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_24\n\nLANGUAGE: go\nCODE:\n```\nFormatTime(Time(\"02/04/2023\", \"%m/%d/%Y\"), \"%A %h %e %Y\")\n```\n\nLANGUAGE: go\nCODE:\n```\nFormatTime(UnixNano(span.attributes[\"time_nanoseconds\"]), \"%b %d %Y %H:%M:%S\")\n```\n\nLANGUAGE: go\nCODE:\n```\nFormatTime(TruncateTime(spanevent.time, Duration(\"10h 20m\"))), \"%Y-%m-%d %H:%M:%S\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Encoding Extension in YAML\nDESCRIPTION: Default configuration for the text_encoding extension. It specifies the encoding, marshaling separator, and unmarshaling separator for processing log data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/textencodingextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  text_encoding:\n    encoding: utf8\n    marshaling_separator: \"\\n\"\n    unmarshaling_separator: \"\\r?\\n\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CouchDB Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the CouchDB receiver showing how to specify the endpoint, authentication credentials, and collection interval. The password is referenced from an environment variable for security.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/couchdbreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  couchdb:\n    endpoint: http://localhost:5984\n    username: otelu\n    password: ${env:COUCHDB_PASSWORD}\n    collection_interval: 60s\n```\n\n----------------------------------------\n\nTITLE: Encoding Sampling Threshold in Trace Context Tracestate\nDESCRIPTION: Example of how 25% sampling is encoded in a tracing Span's tracestate field using the 'ot' section designator with 'th' for threshold.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/probabilisticsamplerprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntracestate: ot=th:c\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper Receiver in YAML\nDESCRIPTION: Example configuration for the Zookeeper receiver showing how to set the endpoint, collection interval, and initial delay parameters. This configuration connects to a local Zookeeper instance on port 2181, collects metrics every 20 seconds, and starts after a 1 second delay.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/zookeeperreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  zookeeper:\n    endpoint: \"localhost:2181\"\n    collection_interval: 20s\n    initial_delay: 1s\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Authentication in Azure Auth Extension\nDESCRIPTION: YAML configuration example for the Azure authenticator extension using default authentication. This method is not recommended for production environments.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/azureauthextension/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  azureauth:\n    use_default: true\n```\n\n----------------------------------------\n\nTITLE: Filtering Entries with Regex Pattern in YAML\nDESCRIPTION: This snippet demonstrates how to configure the filter operator to drop entries whose message field matches a specific regex pattern. It uses the 'matches' operator to compare the body.message field against the regex '^LOG: .* END$'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/filter.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: filter\n  expr: 'body.message matches \"^LOG: .* END$\"'\n  output: my_output\n```\n\n----------------------------------------\n\nTITLE: Configuring AVRO Log Encoding Extension in YAML\nDESCRIPTION: Example configuration for the AVRO log encoding extension showing how to specify an Avro schema. The schema defines a record with string and integer fields for key-value data points.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/avrologencodingextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  avro_log_encoding:\n    schema: |\n      {\n        \"type\" : \"record\",\n        \"namespace\" : \"example\",\n        \"name\" : \"Datapoint\",\n        \"fields\" : [\n          { \"name\" : \"Key\" , \"type\" : \"string\" },\n          { \"name\" : \"Value\" , \"type\" : \"int\" }\n        ]\n      }\n```\n\n----------------------------------------\n\nTITLE: RBAC Permissions for KubeletStats Receiver\nDESCRIPTION: Defines the necessary Kubernetes RBAC permissions for the KubeletStats receiver to access node statistics and metadata. Includes permissions for basic and extended metric collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otel-collector\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes/stats\"]\n    verbs: [\"get\"]\n    \n  # Only needed if you are using extra_metadata_labels or\n  # are collecting the request/limit utilization metrics\n  - apiGroups: [\"\"]\n    resources: [\"nodes/proxy\"]\n    verbs: [\"get\"]\n```\n\n----------------------------------------\n\nTITLE: Generating Metrics with Telemetry Generator\nDESCRIPTION: Command to generate metrics using the telemetry generator for a specified duration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/telemetrygen/README.md#2025-04-10_snippet_6\n\nLANGUAGE: console\nCODE:\n```\ntelemetrygen metrics --duration 5s --otlp-insecure\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic UDP Log Receiver in YAML\nDESCRIPTION: Basic configuration example showing how to set up a UDP log receiver listening on port 54525. Demonstrates the minimal required configuration with just the listen_address field specified.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/udplogreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  udplog:\n    listen_address: \"0.0.0.0:54525\"\n```\n\n----------------------------------------\n\nTITLE: Log Deduplication with Include Fields in YAML\nDESCRIPTION: Configuration example showing how to specify which fields should be considered for deduplication matching, with custom interval and timezone settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logdedupprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    filelog:\n        include: [./example/*.log]\nprocessors:\n    logdedup:\n        include_fields:\n          - attributes.id\n          - attributes.name\n        interval: 60s\n        log_count_attribute: dedup_count\n        timezone: 'America/Los_Angeles'\nexporters:\n    googlecloud:\n\nservice:\n    pipelines:\n        logs:\n            receivers: [filelog]\n            processors: [logdedup]\n            exporters: [googlecloud]\n```\n\n----------------------------------------\n\nTITLE: Creating SQL Schema for OpenTelemetry Metrics Summary in Kinetica\nDESCRIPTION: SQL schema definitions for OpenTelemetry metrics in Kinetica database, including tables for summary metrics, datapoints, attributes, and quantile values. All tables include primary keys, foreign key relationships, and use table properties to avoid errors if tables already exist.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE otel.metric_summary\n(\n    summary_id UUID (primary_key, shard_key) not null,\n    metric_name varchar (256) not null,\n    metric_description varchar (256),\n    metric_unit varchar (256)\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_summary_datapoint\n(\n    summary_id UUID (primary_key, shard_key) not null,\n    id UUID (primary_key) not null,\n    start_time_unix TIMESTAMP,\n    time_unix TIMESTAMP NOT NULL,\n    count LONG,\n    data_sum DOUBLE,\n    flags INT,\n    FOREIGN KEY (summary_id) references otel.metric_summary(summary_id) as fk_summary_datapoint\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_summary_datapoint_attribute\"\n(\n    \"summary_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (summary_id) references otel.metric_summary(summary_id) as fk_summary_datapoint_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_summary_datapoint_quantile_values\n(\n    summary_id UUID (primary_key, shard_key) not null,\n    datapoint_id UUID (primary_key) not null,\n    quantile_id UUID (primary_key) not null,\n    quantile DOUBLE,\n    value DOUBLE,\n    FOREIGN KEY (summary_id) references otel.metric_summary(summary_id) as fk_summary_datapoint_quantile\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_summary_resource_attribute\"\n(\n    \"summary_id\" UUID (primary_key) NOT NULL,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (summary_id),\n    FOREIGN KEY (summary_id) references otel.metric_summary(summary_id) as fk_summary_resource_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_summary_scope_attribute\"\n(\n    \"summary_id\" UUID (primary_key) NOT NULL,\n    \"name\" VARCHAR (256),\n    \"version\" VARCHAR (256),\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (summary_id),\n    FOREIGN KEY (summary_id) references otel.metric_summary(summary_id) as fk_summary_scope_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n```\n\n----------------------------------------\n\nTITLE: Creating Service Account\nDESCRIPTION: Kubernetes manifest for creating a ServiceAccount for the OpenTelemetry Collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sobjectsreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: otelcontribcol\n  name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuration for Spans Compaction Using Batch and GroupByAttrs\nDESCRIPTION: A YAML configuration showing how to set up both batch and groupbyattrs processors in a pipeline to compact spans with matching Resource and InstrumentationLibrary attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  batch:\n  groupbyattrs:\n\npipelines:\n  traces:\n    processors: [batch, groupbyattrs/grouping]\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Verbose Linear Operator Sequence with Default IDs in YAML\nDESCRIPTION: This example shows the same linear sequence as before, but with explicit 'id' and 'output' fields to demonstrate their default behavior in a linear sequence.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/operators.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: my-log.json\n    operators:\n      - type: json_parser\n        id: json_parser\n        output: remove\n      - type: remove\n        id: remove\n        field: attributes.foo\n        output: add\n      - type: add\n        id: add\n        key: attributes.bar\n        value: baz\n        # the last operator automatically outputs from the receiver\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRole for RBAC\nDESCRIPTION: Kubernetes ClusterRole definition with required permissions for pods and events access.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sobjectsreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - events\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: \n  - \"events.k8s.io\"\n  resources:\n  - events\n  verbs:\n  - watch\n  - list\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon Elastic Beanstalk Resource Detection\nDESCRIPTION: YAML configuration for detecting Elastic Beanstalk resources by reading the X-Ray configuration file available on Beanstalk instances.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/elastic_beanstalk:\n    detectors: [env, elastic_beanstalk]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Pulsar Receiver in OpenTelemetry Collector\nDESCRIPTION: Example configuration for setting up a Pulsar receiver with TLS authentication, custom topic subscription, and secure connections. Demonstrates core settings including endpoint, topic, subscription name, consumer name, and TLS certificate configurations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/pulsarreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  pulsar:\n    endpoint: pulsar://localhost:6650\n    topic: otlp-spans\n    subscription: otlp_spans_sub\n    consumer_name: otlp_spans_sub_1\n    encoding: otlp_proto\n    auth:\n      tls:\n        cert_file: cert.pem\n        key_file: key.pem\n    tls_allow_insecure_connection: false\n    tls_trust_certs_file_path: ca.pem\n```\n\n----------------------------------------\n\nTITLE: Adding Labels to Multiple Metrics with Regex Pattern in YAML\nDESCRIPTION: Adds a version label to all system metrics by using a regular expression pattern match.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# for all system metrics, add label `version` with value `opentelemetry collector vX.Y.Z` to all points\ninclude: ^system\\.\nmatch_type: regexp\naction: update\noperations:\n  - action: add_label\n    new_label: version\n    new_value: opentelemetry collector {{version}}\n```\n\n----------------------------------------\n\nTITLE: Starting Secure Tracing Services with Docker Compose\nDESCRIPTION: Shell command to bring up Envoy and OpenTelemetry Collection Pipeline using Docker Compose. This starts the complete secure tracing infrastructure.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ docker compose up\n```\n\n----------------------------------------\n\nTITLE: Configuring GCP Resource Detection\nDESCRIPTION: YAML configuration for setting up GCP and environment variable detectors with a 2 second timeout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/gcp:\n    detectors: [env, gcp]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Querying Verbose Pipeline Health Status in JSON\nDESCRIPTION: Example JSON response for a verbose pipeline health status query. It includes the overall pipeline status and individual component statuses within the pipeline.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"start_time\": \"2024-01-18T17:27:12.570394-08:00\",\n    \"healthy\": true,\n    \"status\": \"StatusOK\",\n    \"status_time\": \"2024-01-18T17:27:12.571625-08:00\",\n    \"components\": {\n        \"exporter:otlphttp/staging\": {\n            \"healthy\": true,\n            \"status\": \"StatusOK\",\n            \"status_time\": \"2024-01-18T17:27:12.571615-08:00\"\n        },\n        \"processor:batch\": {\n            \"healthy\": true,\n            \"status\": \"StatusOK\",\n            \"status_time\": \"2024-01-18T17:27:12.571621-08:00\"\n        },\n        \"receiver:otlp\": {\n            \"healthy\": true,\n            \"status\": \"StatusOK\",\n            \"status_time\": \"2024-01-18T17:27:12.571625-08:00\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CloudWatch Logs Subscription Filter Encoding in YAML\nDESCRIPTION: This snippet demonstrates how to configure the AWS Logs encoding extension for CloudWatch Logs Subscription Filters. It sets up the extension and a receiver that uses the encoding.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/awslogsencodingextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  awslogs_encoding/cloudwatch:\n    format: cloudwatch_logs_subscription_filter\n\nreceivers:\n  awsfirehose:\n    endpoint: :1234\n    encoding: awslogs_encoding/cloudwatch\n```\n\n----------------------------------------\n\nTITLE: Basic Windows Performance Counter Configuration\nDESCRIPTION: Basic configuration structure for the Windows Performance Counters receiver showing the main configuration options including collection interval, metrics definition, and performance counter specifications.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsperfcountersreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nwindowsperfcounters:\n  collection_interval: <duration> # default = \"1m\"\n  initial_delay: <duration> # default = \"1s\"\n  metrics:\n    <metric name>:\n      description: <description>\n      unit: <unit type>\n      gauge:\n    <metric name>:\n      description: <description>\n      unit: <unit type>\n      sum:\n        aggregation: <cumulative or delta>\n        monotonic: <true or false>\n  perfcounters:\n    - object: <object name>\n      instances: [<instance name>]*\n      counters:\n        - name: <counter name>\n          metric: <metric name>\n          attributes:\n            <key>: <value>\n          recreate_query: <true or false>\n```\n\n----------------------------------------\n\nTITLE: Environment-Based Metrics Count Configuration\nDESCRIPTION: Advanced configuration showing how to count metrics and datapoints based on environment attributes while maintaining default span counting.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo/traces:\n  foo/metrics:\n  foo/logs:\nexporters:\n  bar/all_types:\n  bar/counts_only:\nconnectors:\n  count:\n    metrics:\n      my.prod.metric.count:\n        conditions:\n         - `attributes[\"env\"] == \"prod\"\n      my.test.metric.count:\n        conditions:\n         - `attributes[\"env\"] == \"test\"\n    datapoints:\n      my.prod.datapoint.count:\n        conditions:\n         - `attributes[\"env\"] == \"prod\"\n      my.test.datapoint.count:\n        conditions:\n         - `attributes[\"env\"] == \"test\"\nservice:\n  pipelines:\n    traces:\n      receivers: [foo/traces]\n      exporters: [bar/all_types, count]\n    metrics:\n      receivers: [foo/metrics]\n      exporters: [bar/all_types, count]\n    metrics/counts:\n      receivers: [count]\n      exporters: [bar/counts_only]\n```\n\n----------------------------------------\n\nTITLE: Removing a Layer from Body in YAML\nDESCRIPTION: This example demonstrates how to remove a wrapper layer from the body of a log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- type: move\n  from: body.wrapper\n  to: body\n```\n\n----------------------------------------\n\nTITLE: Sending StatsD Messages to Receiver\nDESCRIPTION: Shell commands for sending StatsD metrics to the receiver using netcat, demonstrating both IPv4 and IPv6 support.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/statsdreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\necho \"test.metric:42|c|#myKey:myVal\" | nc -w 1 -u -4 localhost 8125;\necho \"test.metric:42|c|#myKey:myVal\" | nc -w 1 -u -6 localhost 8125;\n```\n\n----------------------------------------\n\nTITLE: Input: Raw Huawei Cloud Metrics Format\nDESCRIPTION: Example of raw metrics data from Huawei Cloud showing CPU utilization, memory utilization, and bandwidth usage metrics with their respective units, timestamps, and dimension values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/huaweicloudcesreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n      \"unit\": \"%\",\n      \"datapoints\": [\n        { \"average\": 10, \"timestamp\": 1722580500000 },\n        { \"average\": 20, \"timestamp\": 1722580800000 }\n      ],\n      \"namespace\": \"SYS.ECS\",\n      \"metric_name\": \"cpu_util\",\n      \"dimensions\": [\n        {\n          \"name\": \"instance_id\",\n          \"value\": \"faea5b75-e390-4e2b-8733-9226a9026070\"\n        }\n      ]\n    },\n    {\n      \"unit\": \"%\",\n      \"datapoints\": [\n        { \"average\": 30, \"timestamp\": 1722580500000 },\n        { \"average\": 40, \"timestamp\": 1722580800000 }\n      ],\n      \"namespace\": \"SYS.ECS\",\n      \"metric_name\": \"mem_util\",\n      \"dimensions\": [\n        {\n          \"name\": \"instance_id\",\n          \"value\": \"abcea5b75-e390-4e2b-8733-9226a9026070\"\n        }\n      ]\n    },\n    {\n      \"unit\": \"bit/s\",\n      \"datapoints\": [\n        { \"average\": 1024, \"timestamp\": 1722580500000},\n        { \"average\": 2048, \"timestamp\": 1722580800000}\n      ],\n      \"namespace\": \"SYS.VPC\",\n      \"metric_name\": \"upstream_bandwidth_usage\",\n      \"dimensions\": [\n        {\n          \"name\": \"publicip_id\",\n          \"value\": \"test-baae-4dd9-ad3f-1234\"\n        }\n      ]\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Resource Detection\nDESCRIPTION: YAML configuration for detecting Azure resources by querying the Azure Instance Metadata Service.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/azure:\n    detectors: [env, azure]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Journald Receiver in YAML\nDESCRIPTION: Example YAML configuration for the Journald receiver, specifying directory, units to read from, and priority level.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/journaldreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  journald:\n    directory: /run/log/journal\n    units:\n      - ssh\n      - kubelet\n      - docker\n      - containerd\n    priority: info\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta to Cumulative Processor in YAML\nDESCRIPTION: YAML configuration for the deltatocumulative processor. It allows specifying how long until stale series are removed (max_stale) and the maximum number of streams to track (max_streams).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/deltatocumulativeprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    deltatocumulative:\n        # how long until a series not receiving new samples is removed\n        [ max_stale: <duration> | default = 5m ]\n \n        # upper limit of streams to track. new streams exceeding this limit\n        # will be dropped\n        [ max_streams: <int> | default = 9223372036854775807 (max int) ]\n\n```\n\n----------------------------------------\n\nTITLE: Sending UDP Log Messages Using Netcat\nDESCRIPTION: Example showing how to send test log messages to the UDP input using netcat command.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/udp_input.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ nc -u localhost 54525 <<EOF\nheredoc> message1\nheredoc> message2\nheredoc> EOF\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Alert Polling in YAML\nDESCRIPTION: Configuration to poll alerts from MongoDB Atlas API, including project and cluster specifications.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    public_key: <redacted>\n    private_key: <redacted>\n    alerts:\n      enabled: true\n      mode: poll\n      projects:\n      - name: Project 0\n        include_clusters: [Cluster0]\n      poll_interval: 1m\n    storage: file_storage\n```\n\n----------------------------------------\n\nTITLE: Configuring HAProxy Receiver in YAML\nDESCRIPTION: This snippet demonstrates how to configure the HAProxy receiver in the OpenTelemetry Collector's configuration file. It shows the basic setup with an endpoint and collection interval.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/haproxyreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nhaproxy:\n  endpoint: file:///var/run/haproxy.ipc\n  collection_interval: 1m\n  metrics:\n    \n```\n\n----------------------------------------\n\nTITLE: Describing Metrics for OpenTelemetry Load Balancer Exporter\nDESCRIPTION: This section lists and explains the metrics recorded by the OpenTelemetry load balancer exporter. It includes metrics for resolutions, number of backends, backend updates, latency, and outcome.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n## Metrics\n\nThe following metrics are recorded by this exporter:\n\n* `otelcol_loadbalancer_num_resolutions` represents the total number of resolutions performed by the resolver specified in the tag `resolver`, split by their outcome (`success=true|false`). For the static resolver, this should always be `1` with the tag `success=true`.\n* `otelcol_loadbalancer_num_backends` informs how many backends are currently in use. It should always match the number of items specified in the configuration file in case the `static` resolver is used, and should eventually (seconds) catch up with the DNS changes. Note that DNS caches that might exist between the load balancer and the record authority will influence how long it takes for the load balancer to see the change.\n* `otelcol_loadbalancer_num_backend_updates` records how many of the resolutions resulted in a new list of backends. Use this information to understand how frequent your backend updates are and how often the ring is rebalanced. If the DNS hostname is always returning the same list of IP addresses but this metric keeps increasing, it might indicate a bug in the load balancer.\n* `otelcol_loadbalancer_backend_latency` measures the latency for each backend.\n* `otelcol_loadbalancer_backend_outcome` counts what the outcomes were for each endpoint, `success=true|false`.\n```\n\n----------------------------------------\n\nTITLE: Defining PrometheusECSTarget Struct in Go\nDESCRIPTION: Defines a struct representing a Prometheus ECS target with fields for address, labels, and EC2 instance information. Includes serialization methods for converting between different formats.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: go\nCODE:\n```\n// PrometheusECSTarget contains address and labels extracted from a running ECS task \n// and its underlying EC2 instance (if available).\n// \n// For serialization\n// - FromLabels and ToLabels converts it between map[string]string.\n// - FromTargetYAML and ToTargetYAML converts it between prometheus file discovery format in YAML. \n// - FromTargetJSON and ToTargetJSON converts it between prometheus file discovery format in JSON. \ntype PrometheusECSTarget struct {\n\tAddress                string            `json:\"address\"`\n\tMetricsPath            string            `json:\"metrics_path\"`\n\tJob                    string            `json:\"job\"`\n\tTaskDefinitionFamily   string            `json:\"task_definition_family\"`\n\tTaskDefinitionRevision int               `json:\"task_definition_revision\"`\n\tTaskLaunchType         string            `json:\"task_launch_type\"`\n\tTaskGroup              string            `json:\"task_group\"`\n\tTaskTags               map[string]string `json:\"task_tags\"`\n\tContainerName          string            `json:\"container_name\"`\n\tContainerLabels        map[string]string `json:\"container_labels\"`\n\tHealthStatus           string            `json:\"health_status\"`\n\tEC2InstanceId          string            `json:\"ec2_instance_id\"`\n\tEC2InstanceType        string            `json:\"ec2_instance_type\"`\n\tEC2Tags                map[string]string `json:\"ec2_tags\"`\n\tEC2VPCId               string            `json:\"ec2_vpc_id\"`\n\tEC2PrivateIP           string            `json:\"ec2_private_ip\"`\n\tEC2PublicIP            string            `json:\"ec2_public_ip\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring BMC Helix Exporter with Optional Settings in YAML\nDESCRIPTION: This snippet demonstrates how to configure the BMC Helix exporter with optional settings such as timeout and retry options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/bmchelixexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  bmchelix/helix2:\n    endpoint: https://company.onbmc.com\n    api_key: <api-key>\n    timeout: 20s\n    retry_on_failure:\n      enabled: true\n      initial_interval: 5s\n      max_interval: 1m\n      max_elapsed_time: 8m\n```\n\n----------------------------------------\n\nTITLE: Configuring EKS Resource Detection with Cluster Name\nDESCRIPTION: YAML configuration for EKS resource detection with cluster name detection enabled, which requires EC2 instance permissions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/eks:\n    detectors: [env, eks]\n    timeout: 15s\n    override: false\n    eks:\n      resource_attributes:\n        k8s.cluster.name:\n          enabled: true\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes ConfigMap for OpenTelemetry Collector\nDESCRIPTION: Bash commands to create a Kubernetes ConfigMap containing the OpenTelemetry Collector configuration with the Kubernetes Cluster receiver. The configuration sets up metrics and logs pipelines with a debug exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\ndata:\n  config.yaml: |\n    receivers:\n      k8s_cluster:\n        collection_interval: 10s\n    exporters:\n      debug:\n    service:\n      pipelines:\n        metrics:\n          receivers: [k8s_cluster]\n          exporters: [debug]\n        logs/entity_events:\n          receivers: [k8s_cluster]\n          exporters: [debug]\nEOF\n```\n\n----------------------------------------\n\nTITLE: Matching Metrics Resource Attributes with Regular Expressions in pdatatest\nDESCRIPTION: Example of using pdatatest to match metrics with resource attributes or metric attribute values using regular expressions. This allows for partial matching of dimension values rather than requiring exact matches.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_25\n\nLANGUAGE: go\nCODE:\n```\nMatchResourceAttributeValue(\"node_id\", \"cloud-node\")\n```\n\nLANGUAGE: go\nCODE:\n```\nMatchMetricAttributeValue(\"hostname\", \"container-tomcat-\", \"gauge.one\", \"sum.one\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Receiver Creator with Discovery in Kubernetes\nDESCRIPTION: This snippet shows how to enable the discovery feature for Kubernetes environments using the k8sobserver. It demonstrates the configuration to enable automatic receiver generation based on Pod annotations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceiver_creator/metrics:\n  watch_observers: [ k8s_observer ]\n  discovery:\n     enabled: true\n     # Define which receivers should be ignored when provided through annotations\n     # ignore_receivers: []\n```\n\n----------------------------------------\n\nTITLE: Excluding Metrics with Regular Expressions in Cumulative to Delta Processor\nDESCRIPTION: Configuration example showing how to exclude metrics containing the word 'metric' from conversion, while converting all other cumulative metrics to delta format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/cumulativetodeltaprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: cumulativetodelta\n    cumulativetodelta:\n\n        # Convert cumulative sum or histogram metrics to delta\n        # if and only if 'metric' is not in the name\n        exclude:\n            metrics:\n                - \".*metric.*\"\n            match_type: regexp\n```\n\n----------------------------------------\n\nTITLE: Configuring Experimental Batcher with File Storage Queue\nDESCRIPTION: Configuration example for enabling the experimental batcher with persistent queue storage using file_storage extension.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/otelarrowexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otelarrow:\n    batcher:\n\t  enabled: true\n    sending_queue:\n      enabled: true\n      storage: file_storage/otc\nextensions:\n  file_storage/otc:\n    directory: /var/lib/storage/otc\n```\n\n----------------------------------------\n\nTITLE: Configuring Team-Specific Coralogix Exporters\nDESCRIPTION: Defines separate Coralogix exporters for different teams with unique private keys while maintaining common endpoint and naming configurations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  coralogix/teamA:\n    metrics:\n      endpoint: \"otel-metrics.coralogix.com:443\"\n    private_key: <private_key_for_teamA>\n    application_name: \"MyBusinessEnvironment\"\n    subsystem_name: \"MyBusinessSystem\"\n  coralogix/teamB:\n    metrics:\n      endpoint: \"otel-metrics.coralogix.com:443\"\n    private_key: <private_key_for_teamB>\n    application_name: \"MyBusinessEnvironment\"\n    subsystem_name: \"MyBusinessSystem\"\n```\n\n----------------------------------------\n\nTITLE: Basic Probabilistic Sampler Configuration for 15% Sampling\nDESCRIPTION: YAML configuration example showing how to set up the probabilistic sampler processor to sample 15% of log records according to trace ID.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/probabilisticsamplerprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  probabilistic_sampler:\n    sampling_percentage: 15\n```\n\n----------------------------------------\n\nTITLE: Configuring Splunk Enterprise Receiver with Basic Authentication in YAML\nDESCRIPTION: Example configuration for the Splunk Enterprise Receiver that demonstrates setting up basic authentication for indexer and cluster master nodes, with custom timeout settings. Shows the complete pipeline setup including extensions, receivers, and exporters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/splunkenterprisereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n    basicauth/indexer:\n        client_auth:\n            username: admin\n            password: securityFirst\n    basicauth/cluster_master:\n        client_auth:\n            username: admin\n            password: securityFirst\n\nreceivers:\n    splunkenterprise:\n        indexer:\n            auth: \n              authenticator: basicauth/indexer\n            endpoint: \"https://localhost:8089\"\n            timeout: 45s\n        cluster_master:\n            auth: \n              authenticator: basicauth/cluster_master\n            endpoint: \"https://localhost:8089\"\n            timeout: 45s\n\nexporters:\n  debug:\n    verbosity: basic\n\nservice:\n  extensions: [basicauth/indexer, basicauth/cluster_master]\n  pipelines:\n    metrics:\n      receivers: [splunkenterprise]\n      exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Parsing Key-Value Pairs with Custom Delimiter in YAML\nDESCRIPTION: Configuration for the key_value_parser operator using a colon as the delimiter for key-value pairs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: key_value_parser\n  parse_from: message\n  delimiter: \":\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Metadata Sync Behavior in YAML\nDESCRIPTION: YAML configuration for setting the processor to wait for metadata synchronization before becoming ready. It includes options for enabling the wait and setting a timeout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nwait_for_metadata: true\nwait_for_metadata_timeout: 10s\n```\n\n----------------------------------------\n\nTITLE: Moving Log Body to Attribute in OpenTelemetry Collector\nDESCRIPTION: This example demonstrates how to set an attribute 'body' to the value of the log body. It uses the 'ignore' error mode to prevent errors when accessing or setting attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  log_statements:\n    - set(log.attributes[\"body\"], log.body)\n```\n\n----------------------------------------\n\nTITLE: Configuring IIS Receiver in YAML\nDESCRIPTION: Example configuration for the IIS receiver showing how to set the collection interval and initial delay parameters. The collection_interval determines how often metrics are emitted, while initial_delay sets how long to wait before starting collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/iisreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    receivers:\n      iis:\n        collection_interval: 10s\n        initial_delay: 1s\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Root Path for Container-based Host Metrics Collection\nDESCRIPTION: Configuration example showing how to set the root_path when collecting host metrics from inside a container, typically used with a host filesystem mount.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  hostmetrics:\n    root_path: /hostfs\n```\n\n----------------------------------------\n\nTITLE: Configuring Key Value Parser in YAML\nDESCRIPTION: Basic configuration for the key_value_parser operator to parse the 'message' field into key-value pairs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: key_value_parser\n  parse_from: message\n```\n\n----------------------------------------\n\nTITLE: Input Example for Docker Format Logs\nDESCRIPTION: Sample input JSON lines showing Docker format logs that need to be recombined. These entries contain JSON-encoded log messages with timestamps and are from a Kubernetes container.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": \"{\\\"log\\\":\\\"2024-07-03T13:50:49.526Z  WARN 1 --- [http-nio-8080-exec-6] c.m.antifraud.FraudDetectionController   : checkOrder\\\",\\\"stream\\\":\\\"stdout\\\",\\\"time\\\":\\\"2024-03-30T08:31:20.545192187Z\\\"}\",\n  \"log.file.path\": \"/var/log/pods/some_kube-controller-kind-control-plane_49cc7c1fd3702c40b2686ea7486091d6/kube-controller/1.log\"\n}\n{\n  \"timestamp\": \"\",\n  \"body\": \"{\\\"log\\\":\\\"java.net.ConnectException: Failed to connect to\\\",\\\"stream\\\":\\\"stdout\\\",\\\"time\\\":\\\"2024-03-30T08:31:20.545192187Z\\\"}\",\n  \"log.file.path\": \"/var/log/pods/some_kube-controller-kind-control-plane_49cc7c1fd3702c40b2686ea7486091d6/kube-controller/1.log\"\n}\n```\n\n----------------------------------------\n\nTITLE: EMF Log Format Example\nDESCRIPTION: Example JSON structure for an EMF (Embedded Metric Format) log with custom log group and stream names, including CloudWatch metrics configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awscloudwatchlogsexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_aws\": {\n    \"Timestamp\": 1574109732004,\n    \"LogGroupName\": \"Foo\",\n    \"LogStreamName\": \"Bar\",\n    \"CloudWatchMetrics\": [\n      {\n        \"Namespace\": \"MyApp\",\n        \"Dimensions\": [[\"Operation\"]],\n        \"Metrics\": [\n          {\n            \"Name\": \"ProcessingLatency\",\n            \"Unit\": \"Milliseconds\",\n            \"StorageResolution\": 60\n          }\n        ]\n      }\n    ]\n  },\n  \"Operation\": \"Aggregator\",\n  \"ProcessingLatency\": 100\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Composite Sub-Policy and Rate Allocation in Tail Sampling Processor\nDESCRIPTION: This YAML snippet defines a composite sub-policy configuration with rate allocation for the tail sampling processor. It includes three different policy types: numeric_attribute, string_attribute, and always_sample, with specific rate allocations for two of the policies.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncomposite_sub_policy:\n  [\n    {\n      name: test-composite-policy-1,\n      type: numeric_attribute,\n      numeric_attribute: {key: key1, min_value: 50}\n    },\n    {\n      name: test-composite-policy-2,\n      type: string_attribute,\n      string_attribute: {key: key2, values: [value1, value2]}\n    },\n    {\n      name: test-composite-policy-3,\n      type: always_sample\n    }\n  ],\nrate_allocation:\n  [\n    {\n      policy: test-composite-policy-1,\n      percent: 50\n    },\n    {\n      policy: test-composite-policy-2,\n      percent: 25\n    }\n  ]\n```\n\n----------------------------------------\n\nTITLE: Output JSON After Allowlist and Denylist Processing\nDESCRIPTION: The transformed JSON result after applying the nest_attributes rules. Note how kubernetes.host.name and kubernetes.host.address are nested, while other attributes remain flat or are excluded based on the configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n\"kubernetes.container_name\": \"xyz\",\n\"kubernetes\": {\n  \"host\": {\n    \"name\": \"the host\",\n    \"address\": \"127.0.0.1\"\n    }\n  },\n\"kubernetes.host.naming_convention\": \"random\",\n\"kubernetes.namespace_name\": \"sumologic\",\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Regexp and Metric Types in Cumulative to Delta Processor\nDESCRIPTION: Configuration example showing how to convert only sum metrics with 'metric' in their name, combining regular expression filtering with metric type filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/cumulativetodeltaprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: cumulativetodelta\n    cumulativetodelta:\n\n        # Convert cumulative sum metrics to delta\n        # if and only if 'metric' is in the name\n        include:\n            metrics:\n                - \".*metric.*\"\n            match_type: regexp\n            metric_types:\n              - sum\n```\n\n----------------------------------------\n\nTITLE: DataSet Exporter Configuration with Empty Prefixes\nDESCRIPTION: Configuration example with all logging options enabled but using empty prefix strings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datasetexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlogs:\n  export_resource_info_on_event: true\n  export_resource_prefix: \"\"\n  export_scope_info_on_event: true\n  export_scope_prefix: \"\"\n  decompose_complex_message_field: true\n  decomposed_complex_message_prefix: \"\"\n  export_separator: \"-\"\n  export_distinguishing_suffix: \"_\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Attribute Setting in OpenTelemetry Collector\nDESCRIPTION: This example demonstrates how to set an attribute conditionally if it doesn't exist. It uses the 'ignore' error mode to prevent errors when accessing non-existent attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  trace_statements:\n    # accessing a map with a key that does not exist will return nil. \n    - set(span.attributes[\"test\"], \"pass\") where span.attributes[\"test\"] == nil\n```\n\n----------------------------------------\n\nTITLE: Input/Output Example for Resource Field Retention\nDESCRIPTION: Example showing how the retain operator selectively keeps specified resource fields while preserving other sections of the log entry. Only the specified resource fields are retained in the output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n     \"key1\": \"val1\",\n     \"key2\": \"val2\",\n     \"key3\": \"val3\"\n  },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n     \"key1\": \"val1\",\n     \"key2\": \"val2\",\n  },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Collector Start Time Fallback Feature\nDESCRIPTION: Command-line option to enable using the collector start time as a fallback for metric start time when process_start_time_seconds is unavailable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n\"--feature-gates=receiver.prometheusreceiver.UseCollectorStartTimeFallback\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle DB Receiver with Primary Option in YAML\nDESCRIPTION: Example configuration for the Oracle DB receiver using the primary configuration option with a datasource connection string.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/oracledbreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  oracledb:\n    datasource: \"oracle://otel:password@localhost:51521/XE\"\n```\n\n----------------------------------------\n\nTITLE: Defining AWS X-Ray Trace Segment Structure in JSON\nDESCRIPTION: This JSON structure defines an AWS X-Ray trace segment, including subsegments for various operations and detailed error information for a failed DynamoDB PutItem request. It captures timing, fault data, and a comprehensive stack trace of the ResourceNotFoundException.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/invalidNamespace.txt#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"acfaa7e3fe3aab03\",\n    \"name\": \"response\",\n    \"start_time\": 1596566305.5875454,\n    \"end_time\": 1596566305.592695,\n    \"Dummy\": false,\n    \"id\": \"ba8d350c0e8cdc4b\",\n    \"name\": \"wait\",\n    \"start_time\": 1596566305.592807,\n    \"end_time\": 1596566305.5928102,\n    \"fault\": true,\n    \"cause\": {\n        \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n        \"exceptions\": [\n            {\n                \"id\": \"5a07f08a8c260405\",\n                \"type\": \"dynamodb.ResourceNotFoundException\",\n                \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                \"stack\": [\n                    {\n                        \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/aws.go\",\n                        \"line\": 149,\n                        \"label\": \"glob..func8\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/aws/request/handlers.go\",\n                        \"line\": 267,\n                        \"label\": \"(*HandlerList).Run\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/aws/request/request.go\",\n                        \"line\": 535,\n                        \"label\": \"(*Request).Send\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/service/dynamodb/api.go\",\n                        \"line\": 3414,\n                        \"label\": \"(*DynamoDB).PutItemWithContext\"\n                    },\n                    {\n                        \"path\": \"sampleapp/sample.go\",\n                        \"line\": 62,\n                        \"label\": \"ddbExpectedFailure.func1\"\n                    },\n                    {\n                        \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/capture.go\",\n                        \"line\": 45,\n                        \"label\": \"Capture\"\n                    },\n                    {\n                        \"path\": \"sampleapp/sample.go\",\n                        \"line\": 41,\n                        \"label\": \"ddbExpectedFailure\"\n                    },\n                    {\n                        \"path\": \"sampleapp/sample.go\",\n                        \"line\": 36,\n                        \"label\": \"main\"\n                    },\n                    {\n                        \"path\": \"runtime/proc.go\",\n                        \"line\": 203,\n                        \"label\": \"main\"\n                    },\n                    {\n                        \"path\": \"runtime/asm_amd64.s\",\n                        \"line\": 1373,\n                        \"label\": \"goexit\"\n                    }\n                ],\n                \"remote\": true\n            }\n        ]\n    },\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Assign Keys Operator for Body Field in YAML\nDESCRIPTION: YAML configuration for the assign_keys operator that transforms a list in the body field into a map with specified keys. The keys parameter defines the names to assign to each element in the list.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/assign_keys.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: assign_keys\n  field: body\n  keys: [\"foo\", \"bar\", \"charlie\", \"foxtrot\"]\n```\n\n----------------------------------------\n\nTITLE: Filtering Entries Using Environment Variable in YAML\nDESCRIPTION: This configuration filters entries by comparing the message field with the value of an environment variable. It uses the env() function to access the value of 'MY_ENV_VARIABLE'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/filter.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: filter\n  expr: 'body.message == env(\"MY_ENV_VARIABLE\")'\n  output: my_output\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRole\nDESCRIPTION: Kubernetes ClusterRole definition with necessary permissions for the k8s_events receiver to access Kubernetes resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8seventsreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - events\n  - namespaces\n  - namespaces/status\n  - nodes\n  - nodes/spec\n  - pods\n  - pods/status\n  - replicationcontrollers\n  - replicationcontrollers/status\n  - resourcequotas\n  - services\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - daemonsets\n  - deployments\n  - replicasets\n  - statefulsets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - jobs\n  - cronjobs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n    - autoscaling\n  resources:\n    - horizontalpodautoscalers\n  verbs:\n    - get\n    - list\n    - watch\nEOF\n```\n\n----------------------------------------\n\nTITLE: Routing with Default Fallback\nDESCRIPTION: Illustrates routing configuration with a default fallback option. Routes JSON format logs to specific parser while sending all other logs to a catchall handler.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/router.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: router\n  routes:\n    - output: my_json_parser\n      expr: 'body.format == \"json\"'\n  default: catchall\n```\n\n----------------------------------------\n\nTITLE: Creating Metric with Regexp Label Matching in YAML\nDESCRIPTION: Configuration example demonstrating creation of new metric with regexp-based label value matching\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\nmatch_type: regexp\nexperimental_match_labels: {\"pod\": \"(.|\\\\s)*\\\\S(.|\\\\s)*\"}\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Redis Receiver Environment Variable Configuration\nDESCRIPTION: Example showing how to configure the Redis receiver using environment variables for sensitive values like passwords.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/redisreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  redis:\n    endpoint: \"localhost:6379\"\n    collection_interval: 10s\n    password: ${env:REDIS_PASSWORD}\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray Segment Document for DynamoDB Operation with ResourceNotFoundException\nDESCRIPTION: This JSON document represents an AWS X-Ray trace segment for a DynamoDB operation that encountered a ResourceNotFoundException. It contains detailed timing information, stack traces, metadata about the AWS SDK, service information, and nested subsegments showing the full request flow including DNS, TLS, and HTTP operations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/ddbSample.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f29ab21-d4ebf299219a65bd5c31d6da\",\n    \"id\": \"88ad1df59cd7a7be\",\n    \"name\": \"DDB\",\n    \"start_time\": 1596566305.535414,\n    \"end_time\": 1596566305.5928545,\n    \"fault\": true,\n    \"cause\": {\n        \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n        \"exceptions\": [\n            {\n                \"id\": \"3e9e11e3ab3fba60\",\n                \"type\": \"dynamodb.ResourceNotFoundException\",\n                \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                \"stack\": [\n                    {\n                        \"path\": \"runtime/proc.go\",\n                        \"line\": 203,\n                        \"label\": \"main\"\n                    },\n                    {\n                        \"path\": \"runtime/asm_amd64.s\",\n                        \"line\": 1373,\n                        \"label\": \"goexit\"\n                    }\n                ],\n                \"remote\": true\n            }\n        ]\n    },\n    \"user\": \"xraysegmentdump\",\n    \"aws\": {\n        \"xray\": {\n            \"sdk_version\": \"1.1.0\",\n            \"sdk\": \"X-Ray for Go\"\n        }\n    },\n    \"service\": {\n        \"compiler_version\": \"go1.14.6\",\n        \"compiler\": \"gc\"\n    },\n    \"subsegments\": [\n        {\n            \"id\": \"7df694142c905d8d\",\n            \"name\": \"DDB.DescribeExistingTableAndPutToMissingTable\",\n            \"start_time\": 1596566305.5354965,\n            \"end_time\": 1596566305.5928457,\n            \"fault\": true,\n            \"cause\": {\n                \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n                \"exceptions\": [\n                    {\n                        \"id\": \"e2ba8a2109451f5b\",\n                        \"type\": \"dynamodb.ResourceNotFoundException\",\n                        \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                        \"stack\": [\n                            {\n                                \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/capture.go\",\n                                \"line\": 48,\n                                \"label\": \"Capture\"\n                            },\n                            {\n                                \"path\": \"sampleapp/sample.go\",\n                                \"line\": 41,\n                                \"label\": \"ddbExpectedFailure\"\n                            },\n                            {\n                                \"path\": \"sampleapp/sample.go\",\n                                \"line\": 36,\n                                \"label\": \"main\"\n                            },\n                            {\n                                \"path\": \"runtime/proc.go\",\n                                \"line\": 203,\n                                \"label\": \"main\"\n                            },\n                            {\n                                \"path\": \"runtime/asm_amd64.s\",\n                                \"line\": 1373,\n                                \"label\": \"goexit\"\n                            }\n                        ],\n                        \"remote\": true\n                    }\n                ]\n            },\n            \"annotations\": {\n                \"DDB.DescribeExistingTableAndPutToMissingTable.Annotation\": \"anno\"\n            },\n            \"metadata\": {\n                \"default\": {\n                    \"DDB.DescribeExistingTableAndPutToMissingTable.AddMetadata\": \"meta\"\n                }\n            },\n            \"subsegments\": [\n                {\n                    \"id\": \"7318c46a385557f5\",\n                    \"name\": \"dynamodb\",\n                    \"start_time\": 1596566305.5355225,\n                    \"end_time\": 1596566305.5873947,\n                    \"namespace\": \"aws\",\n                    \"http\": {\n                        \"response\": {\n                            \"status\": 200,\n                            \"content_length\": 713\n                        }\n                    },\n                    \"aws\": {\n                        \"operation\": \"DescribeTable\",\n                        \"region\": \"us-west-2\",\n                        \"request_id\": \"29P5V7QSAKHS4LNL56ECAJFF3BVV4KQNSO5AEMVJF66Q9ASUAAJG\",\n                        \"retries\": 0,\n                        \"table_name\": \"xray_sample_table\"\n                    },\n                    \"subsegments\": [\n                        {\n                            \"id\": \"0239834271dbee25\",\n                            \"name\": \"marshal\",\n                            \"start_time\": 1596566305.5355248,\n                            \"end_time\": 1596566305.5355635,\n                            \"Dummy\": false\n                        },\n                        {\n                            \"id\": \"23cf5bb60e4f66b1\",\n                            \"name\": \"attempt\",\n                            \"start_time\": 1596566305.5355663,\n                            \"end_time\": 1596566305.5873196,\n                            \"subsegments\": [\n                                {\n                                    \"id\": \"417b81b977b9563b\",\n                                    \"name\": \"connect\",\n                                    \"start_time\": 1596566305.5357504,\n                                    \"end_time\": 1596566305.575329,\n                                    \"metadata\": {\n                                        \"http\": {\n                                            \"connection\": {\n                                                \"reused\": false,\n                                                \"was_idle\": false\n                                            }\n                                        }\n                                    },\n                                    \"subsegments\": [\n                                        {\n                                            \"id\": \"0cab02b318413eb1\",\n                                            \"name\": \"dns\",\n                                            \"start_time\": 1596566305.5357957,\n                                            \"end_time\": 1596566305.5373216,\n                                            \"metadata\": {\n                                                \"http\": {\n                                                    \"dns\": {\n                                                        \"addresses\": [\n                                                            {\n                                                                \"IP\": \"52.94.10.94\",\n                                                                \"Zone\": \"\"\n                                                            }\n                                                        ],\n                                                        \"coalesced\": false\n                                                    }\n                                                }\n                                            },\n                                            \"Dummy\": false\n                                        },\n                                        {\n                                            \"id\": \"f8dbc5c6b291017e\",\n                                            \"name\": \"dial\",\n                                            \"start_time\": 1596566305.5373297,\n                                            \"end_time\": 1596566305.537964,\n                                            \"metadata\": {\n                                                \"http\": {\n                                                    \"connect\": {\n                                                        \"network\": \"tcp\"\n                                                    }\n                                                }\n                                            },\n                                            \"Dummy\": false\n                                        },\n                                        {\n                                            \"id\": \"e2deb66ecaa769a5\",\n                                            \"name\": \"tls\",\n                                            \"start_time\": 1596566305.5380135,\n                                            \"end_time\": 1596566305.5753162,\n                                            \"metadata\": {\n                                                \"http\": {\n                                                    \"tls\": {\n                                                        \"cipher_suite\": 49199,\n                                                        \"did_resume\": false,\n                                                        \"negotiated_protocol\": \"http/1.1\",\n                                                        \"negotiated_protocol_is_mutual\": true\n                                                    }\n                                                }\n                                            },\n                                            \"Dummy\": false\n                                        }\n                                    ],\n                                    \"Dummy\": false\n                                },\n                                {\n                                    \"id\": \"a70bfab91597c7a2\",\n                                    \"name\": \"request\",\n                                    \"start_time\": 1596566305.5753367,\n                                    \"end_time\": 1596566305.5754144,\n                                    \"Dummy\": false\n                                },\n                                {\n                                    \"id\": \"c05331c26d3e8a7f\",\n                                    \"name\": \"response\",\n                                    \"start_time\": 1596566305.5754204,\n                                    \"end_time\": 1596566305.5872962,\n                                    \"Dummy\": false\n                                }\n                            ],\n                            \"Dummy\": false\n                        }\n```\n\n----------------------------------------\n\nTITLE: Configuring NGINX Receiver in YAML\nDESCRIPTION: Example configuration for the NGINX receiver showing how to set the endpoint URL and collection interval. The receiver fetches metrics from the NGINX status endpoint and supports customizable collection intervals.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/nginxreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  nginx:\n    endpoint: \"http://localhost:80/status\"\n    collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter with Include Metrics in YAML\nDESCRIPTION: This snippet demonstrates how to configure the SignalFx exporter with include_metrics option to override default metric exclusions. It shows how to include specific CPU metrics and system.cpu.time with certain dimensions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    include_metrics:\n      - metric_names: [cpu.interrupt, cpu.user, cpu.system]\n      - metric_name: system.cpu.time\n        dimensions:\n          state: [interrupt, user, system]\n```\n\n----------------------------------------\n\nTITLE: Configuring Active Directory DS Metrics in YAML\nDESCRIPTION: Example configuration to disable specific metrics in the Active Directory DS metrics collection. This YAML configuration can be applied to selectively disable any of the default metrics that are otherwise emitted automatically.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/activedirectorydsreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Exponential Histogram Metrics in YAML\nDESCRIPTION: Configuration structure for Exponential Histogram metrics in the Signal to Metrics connector, including max size, count, and value extraction using OTTL expressions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/signaltometricsconnector/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexponential_histogram:\n  max_size: <int64>\n  count: <ottl_value_expression>\n  value: <ottl_value_expression>\n```\n\n----------------------------------------\n\nTITLE: Multi-Type Pipeline Configuration\nDESCRIPTION: Complex configuration showing how to count spans, metrics, and logs with separate export backends.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo/traces:\n  foo/metrics:\n  foo/logs:\nexporters:\n  bar/all_types:\n  bar/counts_only:\nconnectors:\n  count:\nservice:\n  pipelines:\n    traces:\n      receivers: [foo/traces]\n      exporters: [bar/all_types, count]\n    metrics:\n      receivers: [foo/metrics]\n      exporters: [bar/all_types, count]\n    logs:\n      receivers: [foo/logs]\n      exporters: [bar/all_types, count]\n    metrics/counts:\n      receivers: [count]\n      exporters: [bar/counts_only]\n```\n\n----------------------------------------\n\nTITLE: SQL Server Query Metrics JSON Structure\nDESCRIPTION: JSON data structure containing comprehensive query execution metrics including instance details, query identifiers, execution statistics, and performance counters. The structure captures key monitoring data points like elapsed time, worker time, I/O statistics, and resource utilization.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/queryTextAndPlanQueryInvalidData.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n\t{\n\t\t\"sql_instance\": \"sqlserver\",\n\t\t\"computer_name\": \"DESKTOP-GHAEGRD\",\n\t\t\"query_plan_handle\": \"0x06000100E2B3C02AA042A7001000000001000000000000000000000000000000000000000000000000000000\",\n\t\t\"query_hash\": \"0x37849E874171E3F3\",\n\t\t\"query_plan_hash\": \"0xD3112909429A1B50\",\n\t\t\"execution_count\": \"6A\",\n\t\t\"total_elapsed_time\": \"3846\",\n\t\t\"total_worker_time\": \"3845A\",\n\t\t\"total_logical_reads\": \"3A\",\n\t\t\"total_physical_reads\": \"5A\",\n\t\t\"total_logical_writes\": \"4A\",\n\t\t\"total_rows\": \"2A\",\n\t\t\"total_grant_kb\": \"3096A\",\n\t\t\"query_text\": \"[msdb].[dbo].[sysjobhistory].[run_duration] as [sjh].[run_duration]/(10000)*(3600)+[msdb].[dbo].[sysjobhistory].[run_duration] as [sjh].[run_duration]%(10000)/(100)*(60)+[msdb].[dbo].[sysjobhistory].[run_duration] as [sjh].[run_duration]%(100)\",\n\t\t\"query_plan\": \"<ShowPlanXML\"\n\t}\n]\n```\n\n----------------------------------------\n\nTITLE: Input JSON Example for Allowlist and Denylist Processing\nDESCRIPTION: An example input JSON object with various kubernetes-related attributes that will be processed according to the nest_attributes configuration rules.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n\"kubernetes.container_name\": \"xyz\",\n\"kubernetes.host.name\": \"the host\",\n\"kubernetes.host.naming_convention\": \"random\",\n\"kubernetes.host.address\": \"127.0.0.1\",\n\"kubernetes.namespace_name\": \"sumologic\",\n}\n```\n\n----------------------------------------\n\nTITLE: Example LogRecord Structure\nDESCRIPTION: Sample structure of a LogRecord showing various attribute levels including body, resource, scope, and custom attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datasetexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nLog\n- body:\n  - b: 1\n  - x: \"b\"\n- resource:\n  - r: 2\n  - x: \"r\"\n- scope:\n  - s: 3\n  - x: \"s\"\n- attribute:\n  - a: 4\n  - x: \"a\"\n  - map:\n    - m1: 5\n    - m2: 6\n```\n\n----------------------------------------\n\nTITLE: Configuring Load Scraper in Host Metrics Receiver\nDESCRIPTION: Configuration for the load scraper with option to divide the average load by the number of logical CPUs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nload:\n  cpu_average: <false|true>\n```\n\n----------------------------------------\n\nTITLE: Default Configuration for Cumulative to Delta Processor\nDESCRIPTION: The default configuration that converts all cumulative sum and histogram metrics to delta metrics when no include or exclude filters are specified.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/cumulativetodeltaprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: cumulativetodelta\n    cumulativetodelta:\n        # If include/exclude are not specified\n        # convert all cumulative sum or histogram metrics to delta\n```\n\n----------------------------------------\n\nTITLE: Configuring Consul Resource Detection\nDESCRIPTION: YAML configuration for detecting Consul resources by querying a Consul agent's configuration endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/consul:\n    detectors: [env, consul]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Workload Identity Authentication Configuration - YAML\nDESCRIPTION: Configuration example for Azure Monitor receiver using Azure Workload Identity authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azuremonitorreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azuremonitor:\n    subscription_ids: [\"${subscription_id}\"]\n    auth: \"workload_identity\"\n    tenant_id: \"${env:AZURE_TENANT_ID}\"\n    client_id: \"${env:AZURE_CLIENT_ID}\"\n    federated_token_file: \"${env:AZURE_FEDERATED_TOKEN_FILE}\"\n```\n\n----------------------------------------\n\nTITLE: EC2 IAM Policy Configuration\nDESCRIPTION: JSON policy configuration for EC2 IAM role to enable tag description permissions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:DescribeTags\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Adding String to Body\nDESCRIPTION: This JSON shows the output entry after adding a string value to the body using the 'add' operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n    \"key2\": \"val2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Recombine with max_unmatched_batch_size set to 1\nDESCRIPTION: Configuration example where max_unmatched_batch_size is set to 1, causing log entries to not be combined before the match occurs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n- type: recombine\n  combine_field: body\n  is_first_entry: body == 'log1'\n  max_unmatched_batch_size: 1\n```\n\n----------------------------------------\n\nTITLE: Input and Output Example for Attributes Field Transformation\nDESCRIPTION: JSON example showing the transformation of a list in a nested attributes field into a key-value map. The input contains a list in attributes.input, and the output shows these values mapped to the specified keys while other fields remain unchanged.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/assign_keys.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"attributes\": {\n    \"input\": [1, \"debug\"],\n    \"field2\": \"unchanged\",\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"attributes\": {\n    \"input\": {\n      \"foo\": 1, \n      \"bar\": \"debug\",\n    },\n    \"field2\": \"unchanged\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Encoding Sampling Information in Log Records\nDESCRIPTION: Example of how 25% sampling with an explicit randomness value is encoded in log records using attributes for threshold and randomness.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/probabilisticsamplerprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nsampling.threshold: c\nsampling.randomness: e05a99c8df8d32\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Web Server Receiver in YAML\nDESCRIPTION: Basic configuration example for the Apache Web Server receiver showing how to specify the endpoint for collecting server statistics. The receiver connects to the server-status endpoint to gather metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/apachereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  apache:\n    endpoint: \"http://localhost:8080/server-status?auto\"\n```\n\n----------------------------------------\n\nTITLE: Basic Log Deduplication Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to set up log deduplication with a 60-second interval, custom count attribute name, and specific timezone setting. Uses filelog receiver and Google Cloud exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logdedupprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    filelog:\n        include: [./example/*.log]\nprocessors:\n    logdedup:\n        interval: 60s\n        log_count_attribute: dedup_count\n        timezone: 'America/Los_Angeles'\nexporters:\n    googlecloud:\n\nservice:\n    pipelines:\n        logs:\n            receivers: [filelog]\n            processors: [logdedup]\n            exporters: [googlecloud]\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Collection in Aerospike\nDESCRIPTION: YAML configuration example showing how to disable specific metrics in the Aerospike collector. Each metric can be individually enabled or disabled using the enabled flag.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/aerospikereceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Service-Specific Log Query in ClickHouse\nDESCRIPTION: SQL query to find logs from a specific service within the last hour\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE ServiceName = 'clickhouse-exporter'\n  AND TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Metrics in YAML\nDESCRIPTION: Example YAML configuration to disable a specific Elasticsearch metric. This snippet shows how to set the 'enabled' flag to false for a given metric.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/elasticsearchreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Parser with Basic Header\nDESCRIPTION: This snippet demonstrates how to configure the csv_parser operator with a basic header to parse the 'message' field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: csv_parser\n  header: id,severity,message\n```\n\n----------------------------------------\n\nTITLE: Starting Couchbase Environment with Docker Compose\nDESCRIPTION: Docker Compose command to start Couchbase, OpenTelemetry Collector, and Prometheus services with clean environment settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/couchbase/README.md#2025-04-10_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose up -d --remove-orphans --build --force-recreate\n```\n\n----------------------------------------\n\nTITLE: Schema URL Format Example\nDESCRIPTION: Demonstrates the structure of a schema URL showing how it's composed of Schema Family and Schema Version components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/schemaprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n|                       Schema URL                           |\n| https://example.com/telemetry/schemas/ |  |      1.0.1     |\n|             Schema Family              |  | Schema Version |\n```\n\n----------------------------------------\n\nTITLE: Copying Values from Body to Resource in OpenTelemetry Collector (YAML)\nDESCRIPTION: This snippet demonstrates how to configure the copy operator to transfer a value from the body field to the resource field. It copies the value from body.key to resource.newkey.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: copy\n  from: body.key\n  to: resource.newkey\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Metrics in Riak Collector Configuration (YAML)\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable specific metrics in the Riak collector. It allows users to selectively turn off individual metrics by setting the 'enabled' flag to false.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/riakreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Basic Redis Receiver Configuration\nDESCRIPTION: YAML configuration example showing basic Redis receiver setup with endpoint, collection interval and password configuration using environment variables.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/redisreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  redis:\n    endpoint: \"localhost:6379\"\n    collection_interval: 10s\n    password: ${env:REDIS_PASSWORD}\n```\n\n----------------------------------------\n\nTITLE: Configuring Recombine with max_unmatched_batch_size set to 0\nDESCRIPTION: Configuration example where max_unmatched_batch_size is set to 0, causing all log entries to be combined into a single entry before the match occurs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- type: recombine\n  combine_field: body\n  is_first_entry: body == 'log1'\n  max_unmatched_batch_size: 0\n```\n\n----------------------------------------\n\nTITLE: Copying Attributes with Regex Matching in OpenTelemetry Collector\nDESCRIPTION: This example shows how to copy resource attributes matching a regular expression to a new attribute location. It demonstrates the use of keep_matching_keys function to filter attributes based on a regex pattern.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  trace_statements:\n    - statements:\n        - set(resource.cache[\"attrs\"], resource.attributes)\n        - keep_matching_keys(resource.cache[\"attrs\"], \"pod_labels_.*\")\n        - set(resource.attributes[\"kubernetes.labels\"], resource.cache[\"attrs\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Minimal metadata.yaml for OpenTelemetry Component\nDESCRIPTION: This YAML snippet demonstrates the minimal required fields for a metadata.yaml file when creating a new component for the OpenTelemetry Collector Contrib project. It includes the component type, status, stability, and codeowners.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntype: <name of your component, such as apache, http, haproxy, postgresql>\n\nstatus:\n  class: <class of component, one of cmd, connector, exporter, extension, processor or receiver>\n  stability:\n    development: [<pick the signals supported: logs, metrics, traces. For extension, use \"extension\">]\n  codeowners:\n    active: [<github account of the sponsor, such as alice>, <your GitHub account if you are already an OpenTelemetry member>]\n```\n\n----------------------------------------\n\nTITLE: Creating Collector Deployment\nDESCRIPTION: Bash command to create a Kubernetes Deployment for the OpenTelemetry Collector using the configured ServiceAccount and ConfigMap.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: otelcontribcol\n  template:\n    metadata:\n      labels:\n        app: otelcontribcol\n    spec:\n      serviceAccountName: otelcontribcol\n      containers:\n      - name: otelcontribcol\n        image: otelcontribcol:latest\n        args: [\"--config\", \"/etc/config/config.yaml\"]\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n        imagePullPolicy: IfNotPresent\n      volumes:\n        - name: config\n          configMap:\n            name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Cloud Connector in YAML\nDESCRIPTION: Example configuration for the Grafana Cloud Connector component. It specifies host identifiers and metrics flush interval settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/grafanacloudconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconnectors:\n  grafanacloud:\n    host_identifiers: [\"host.id\"]\n    metrics_flush_interval: 60s\n```\n\n----------------------------------------\n\nTITLE: Schema Processor Target Configuration\nDESCRIPTION: Example showing how to configure the schema processor with a specific target semantic version.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/schemaprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  schema:\n    targets:\n      - https://opentelemetry.io/schemas/1.26.0\n```\n\n----------------------------------------\n\nTITLE: Disabling Retry on Failure for Splunk HEC Exporter in YAML\nDESCRIPTION: Example configuration for completely disabling the retry on failure mechanism for the Splunk HEC exporter. This should only be used as a last resort after trying to adjust the retry parameters to resolve memory issues.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/splunkhecexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  splunk_hec:\n    retry_on_failure:\n      enabled: false\n```\n\n----------------------------------------\n\nTITLE: Creating doc.go File for OpenTelemetry Component\nDESCRIPTION: This Go snippet demonstrates the structure of a doc.go file for a new OpenTelemetry Collector Contrib component. It includes the necessary copyright notice, generate pragma, and package declaration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#2025-04-10_snippet_5\n\nLANGUAGE: go\nCODE:\n```\n// Copyright The OpenTelemetry Authors\n// SPDX-License-Identifier: Apache-2.0\n\n//go:generate mdatagen metadata.yaml\n\n// Package fooreceiver bars.\npackage fooreceiver // import \"github.com/open-telemetry/opentelemetry-collector-contrib/receiver/fooreceiver\"\n```\n\n----------------------------------------\n\nTITLE: Input JSON Entry for Removing a Value from Resource\nDESCRIPTION: This shows a sample JSON input entry with a resource field that will be removed by the remove operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"otherkey\": \"val\"\n  },\n  \"attributes\": {  },\n  \"body\": {\n    \"key\": \"val\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variable Resource Detection\nDESCRIPTION: YAML configuration for setting up the environment variable detector with a 2 second timeout and override disabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/env:\n    detectors: [env]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Log Deduplication with Field Exclusions in YAML\nDESCRIPTION: Configuration example demonstrating how to exclude specific fields from deduplication matching, including nested fields and fields containing dots in their names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logdedupprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    filelog:\n        include: [./example/*.log]\nprocessors:\n    logdedup:\n        exclude_fields:\n          - body.timestamp\n          - attributes.host\\.name\n          - attributes.src.ip\nexporters:\n    googlecloud:\n\nservice:\n    pipelines:\n        logs:\n            receivers: [filelog]\n            processors: [logdedup]\n            exporters: [googlecloud]\n```\n\n----------------------------------------\n\nTITLE: Using Regular Expressions with Cumulative to Delta Processor\nDESCRIPTION: Configuration example showing how to use regular expressions to select metrics containing the word 'metric' for conversion from cumulative to delta format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/cumulativetodeltaprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n    # processor name: cumulativetodelta\n    cumulativetodelta:\n\n        # Convert cumulative sum or histogram metrics to delta\n        # if and only if 'metric' is in the name\n        include:\n            metrics:\n                - \".*metric.*\"\n            match_type: regexp\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Snowflake Metrics in YAML\nDESCRIPTION: Configuration example for enabling optional metrics in Snowflake monitoring. Each metric can be individually enabled by setting the enabled flag to true.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/snowflakereceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Output JSON for CSV Parsing with Custom Delimiter\nDESCRIPTION: Example JSON output after applying CSV parsing with a custom delimiter (tab).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": {\n    \"id\": \"1\",\n    \"severity\": \"debug\",\n    \"message\": \"Debug Message\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Logs to TCP Input Using Netcat\nDESCRIPTION: Demonstrates how to send log messages to the configured TCP input using the netcat (nc) command in bash.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/tcp_input.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ nc localhost 54525 <<EOF\nheredoc> message1\nheredoc> message2\nheredoc> EOF\n```\n\n----------------------------------------\n\nTITLE: Hash Action Configuration in YAML\nDESCRIPTION: Configuration for the hash action that applies SHA1 hashing to attribute values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- key: <key>\n  action: hash\n  pattern: <regular pattern>\n```\n\n----------------------------------------\n\nTITLE: Basic GitHub Receiver Configuration with Collection Interval\nDESCRIPTION: Basic YAML configuration for GitHub receiver showing collection interval and scraper setup.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/githubreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ngithub:\n    collection_interval: <duration> #default = 30s recommended 300s\n    scrapers:\n        scraper/config-1:\n        scraper/config-2:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Example Input and Output JSON for Body to Attributes Copy Operation\nDESCRIPTION: This example demonstrates the transformation when copying from body.key2 to attributes.newkey. The input shows the original data structure, and the output shows the copied value in the attributes object.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n    \"key2\": \"val2\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n      \"newkey\": \"val2\"\n  },\n  \"body\": {\n    \"key3\": \"val1\",\n    \"key2\": \"val2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using replace_match function in OTTL\nDESCRIPTION: The replace_match function replaces an entire string if it matches a glob pattern with a replacement string, optionally applying a conversion function.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_7\n\nLANGUAGE: go\nCODE:\n```\nreplace_match(span.attributes[\"http.target\"], \"/user/*/list/*\", \"/user/{userId}/list/{listId}\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_match(span.attributes[\"http.target\"], \"/user/*/list/*\", \"/user/{userId}/list/{listId}\", SHA256, \"/user/%s\")\n```\n\n----------------------------------------\n\nTITLE: Adding and Removing Fields with YAML Configuration\nDESCRIPTION: Example YAML configuration showing how to use the add and remove operators with fields to modify log entries.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/field.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: body.key3\n  value: val3\n- type: remove\n  field: body.key2.nested_key1\n- type: add\n  field: attributes.my_attribute\n  value: my_attribute_value\n```\n\n----------------------------------------\n\nTITLE: Configuring Regexp Match Settings in YAML\nDESCRIPTION: YAML configuration schema for regular expression matching settings, including cache configuration options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# regexp is an optional configuration section for match_type regexp.\nregexp:\n  # cacheenabled determines whether match results are LRU cached to make subsequent matches faster.\n  # Cache size is unlimited unless cachemaxnumentries is also specified.\n  cacheenabled: <bool>\n  # cachemaxnumentries is the max number of entries of the LRU cache; ignored if cacheenabled is false.\n  cachemaxnumentries: <int>\n```\n\n----------------------------------------\n\nTITLE: Enabling Ordering for Google Cloud Pubsub Exporter\nDESCRIPTION: Configuration example demonstrating how to enable message ordering using a specific resource attribute as the ordering key.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlecloudpubsubexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  googlecloudpubsub:\n    project: my-project\n    topic: projects/my-project/topics/otlp-traces\n    ordering:\n      enabled: true\n      from_resource_attribute: some.resource.attribute.key\n      remove_resource_attribute: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Resource Detection\nDESCRIPTION: YAML configuration for setting up both environment and Docker detectors with a 2 second timeout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/docker:\n    detectors: [env, docker]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Setting up SAP HANA Monitoring User Permissions\nDESCRIPTION: SQL script to create a restricted monitoring user and role with the necessary SELECT permissions on monitoring views required for the receiver to function.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/saphanareceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n--Create the user\nCREATE RESTRICTED USER otel_monitoring_user PASSWORD <password>;\n\n--Enable user login\nALTER USER otel_monitoring_user ENABLE CLIENT CONNECT;\n\n--Create the monitoring role\nCREATE ROLE OTEL_MONITORING;\n\n--Grant permissions to the relevant views\nGRANT CATALOG READ TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_BACKUP_CATALOG TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_BLOCKED_TRANSACTIONS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_CONNECTIONS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_CS_ALL_COLUMNS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_CS_TABLES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_DATABASE TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_DISKS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_HOST_RESOURCE_UTILIZATION TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_LICENSES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_RS_TABLES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_COMPONENT_MEMORY TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_MEMORY TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_REPLICATION TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_STATISTICS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICE_THREADS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_SERVICES TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_VOLUME_IO_TOTAL_STATISTICS TO OTEL_MONITORING;\nGRANT SELECT ON SYS.M_WORKLOAD TO OTEL_MONITORING;\nGRANT SELECT ON _SYS_STATISTICS.STATISTICS_CURRENT_ALERTS TO OTEL_MONITORING;\n\n--Add the OTEL_MONITOR role to the monitoring user\nGRANT OTEL_MONITORING TO otel_monitoring_user;\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Event Log Receiver in YAML\nDESCRIPTION: A simple YAML configuration example for the Windows Event Log Receiver, specifying the application channel to monitor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowseventlogreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    windowseventlog:\n        channel: application\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynatrace Resource Detection in YAML\nDESCRIPTION: Example YAML configuration for the Dynatrace resource detector. It demonstrates how to add the Dynatrace detector to the list of detectors with recommended settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/dynatrace:\n    override: false\n    detectors: [dynatrace]\n```\n\n----------------------------------------\n\nTITLE: Generating Logs with Telemetry Generator\nDESCRIPTION: Command to generate logs using the telemetry generator for a specified duration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/telemetrygen/README.md#2025-04-10_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ntelemetrygen logs --duration 5s --otlp-insecure\n```\n\n----------------------------------------\n\nTITLE: Configuring Unquote Operator for Attributes in YAML\nDESCRIPTION: Configuration example showing how to unquote a specific attribute field. This targets an attribute called 'foo' for the unquote operation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/unquote.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: unquote\n  field: attributes.foo\n```\n\n----------------------------------------\n\nTITLE: Configuring Regex Parser with Timestamp Parsing in YAML\nDESCRIPTION: Example configuration for a regex_parser operator with timestamp parsing. It extracts a timestamp field from the log entry and parses it using the strptime layout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_parser\n  regex: '^Time=(?P<timestamp_field>\\d{4}-\\d{2}-\\d{2}), Host=(?P<host>[^,]+)'\n  timestamp:\n    parse_from: attributes.timestamp_field\n    layout_type: strptime\n    layout: '%Y-%m-%d'\n```\n\n----------------------------------------\n\nTITLE: Querying Late Span Age Metric in Prometheus\nDESCRIPTION: This Prometheus query tracks the age of late-arriving spans. It's useful for understanding the impact of late spans on sampling decisions and adjusting the 'decision_wait' configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: prometheus\nCODE:\n```\notelcol_processor_tail_sampling_sampling_late_span_age\n```\n\n----------------------------------------\n\nTITLE: Collecting SQL Server Performance Metrics\nDESCRIPTION: This SQL script collects various performance metrics from SQL Server. It checks for compatible SQL Server editions, declares variables and temporary tables, and then queries system views to gather performance counter data. The script covers a wide range of metrics including CPU usage, memory utilization, I/O statistics, and SQL Server-specific performance indicators.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryWithInstanceName.txt#2025-04-10_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSET DEADLOCK_PRIORITY -10;\nIF SERVERPROPERTY('EngineEdition') NOT IN (2,3,4) BEGIN /*NOT IN Standard,Enterprise,Express*/\n\tDECLARE @ErrorMessage AS nvarchar(500) = 'Connection string Server:'+ @@ServerName + ',Database:' + DB_NAME() +' is not a SQL Server Standard, Enterprise or Express. This query is only supported on these editions.';\n\tRAISERROR (@ErrorMessage,11,1)\n\tRETURN\nEND\n\nDECLARE\n\t @SqlStatement AS nvarchar(max)\n\t,@MajorMinorVersion AS int = CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),4) AS int)*100 + CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),3) AS int)\n\nDECLARE @PCounters TABLE\n(\n\t [object_name] nvarchar(128)\n\t,[counter_name] nvarchar(128)\n\t,[instance_name] nvarchar(128)\n\t,[cntr_value] bigint\n\t,[cntr_type] int\n\tPRIMARY KEY([object_name], [counter_name], [instance_name])\n);\n\nWITH PerfCounters AS (\nSELECT DISTINCT\n\t RTRIM(spi.[object_name]) [object_name]\n\t,RTRIM(spi.[counter_name]) [counter_name]\n\t,RTRIM(spi.[instance_name]) AS [instance_name]\n\t,CAST(spi.[cntr_value] AS bigint) AS [cntr_value]\n\t,spi.[cntr_type]\n\tFROM sys.dm_os_performance_counters AS spi\n\tWHERE\n\t\tcounter_name IN (\n\t\t\t 'SQL Compilations/sec'\n\t\t\t,'SQL Re-Compilations/sec'\n\t\t\t,'User Connections'\n\t\t\t,'Batch Requests/sec'\n\t\t\t,'Logouts/sec'\n\t\t\t,'Logins/sec'\n\t\t\t,'Processes blocked'\n\t\t\t,'Latch Waits/sec'\n\t\t\t,'Average Latch Wait Time (ms)'\n\t\t\t,'Full Scans/sec'\n\t\t\t,'Index Searches/sec'\n\t\t\t,'Page Splits/sec'\n\t\t\t,'Page lookups/sec'\n\t\t\t,'Page reads/sec'\n\t\t\t,'Page writes/sec'\n\t\t\t,'Readahead pages/sec'\n\t\t\t,'Lazy writes/sec'\n\t\t\t,'Checkpoint pages/sec'\n\t\t\t,'Table Lock Escalations/sec'\n\t\t\t,'Page life expectancy'\n\t\t\t,'Log File(s) Size (KB)'\n\t\t\t,'Log File(s) Used Size (KB)'\n\t\t\t,'Data File(s) Size (KB)'\n\t\t\t,'Transactions/sec'\n\t\t\t,'Write Transactions/sec'\n\t\t\t,'Active Transactions'\n\t\t\t,'Log Growths'\n\t\t\t,'Active Temp Tables'\n\t\t\t,'Logical Connections'\n\t\t\t,'Temp Tables Creation Rate'\n\t\t\t,'Temp Tables For Destruction'\n\t\t\t,'Free Space in tempdb (KB)'\n\t\t\t,'Version Store Size (KB)'\n\t\t\t,'Memory Grants Pending'\n\t\t\t,'Memory Grants Outstanding'\n\t\t\t,'Free list stalls/sec'\n\t\t\t,'Buffer cache hit ratio'\n\t\t\t,'Buffer cache hit ratio base'\n\t\t\t,'Database Pages'\n\t\t\t,'Backup/Restore Throughput/sec'\n\t\t\t,'Total Server Memory (KB)'\n\t\t\t,'Target Server Memory (KB)'\n\t\t\t,'Log Flushes/sec'\n\t\t\t,'Log Flush Wait Time'\n\t\t\t,'Memory broker clerk size'\n\t\t\t,'Log Bytes Flushed/sec'\n\t\t\t,'Bytes Sent to Replica/sec'\n\t\t\t,'Log Send Queue'\n\t\t\t,'Bytes Sent to Transport/sec'\n\t\t\t,'Sends to Replica/sec'\n\t\t\t,'Bytes Sent to Transport/sec'\n\t\t\t,'Sends to Transport/sec'\n\t\t\t,'Bytes Received from Replica/sec'\n\t\t\t,'Receives from Replica/sec'\n\t\t\t,'Flow Control Time (ms/sec)'\n\t\t\t,'Flow Control/sec'\n\t\t\t,'Resent Messages/sec'\n\t\t\t,'Redone Bytes/sec'\n\t\t\t,'XTP Memory Used (KB)'\n\t\t\t,'Transaction Delay'\n\t\t\t,'Log Bytes Received/sec'\n\t\t\t,'Log Apply Pending Queue'\n\t\t\t,'Redone Bytes/sec'\n\t\t\t,'Recovery Queue'\n\t\t\t,'Log Apply Ready Queue'\n\t\t\t,'CPU usage %'\n\t\t\t,'CPU usage % base'\n\t\t\t,'Queued requests'\n\t\t\t,'Requests completed/sec'\n\t\t\t,'Blocked tasks'\n\t\t\t,'Active memory grant amount (KB)'\n\t\t\t,'Disk Read Bytes/sec'\n\t\t\t,'Disk Read IO Throttled/sec'\n\t\t\t,'Disk Read IO/sec'\n\t\t\t,'Disk Write Bytes/sec'\n\t\t\t,'Disk Write IO Throttled/sec'\n\t\t\t,'Disk Write IO/sec'\n\t\t\t,'Used memory (KB)'\n\t\t\t,'Forwarded Records/sec'\n\t\t\t,'Background Writer pages/sec'\n\t\t\t,'Percent Log Used'\n\t\t\t,'Log Send Queue KB'\n\t\t\t,'Redo Queue KB'\n\t\t\t,'Mirrored Write Transactions/sec'\n\t\t\t,'Group Commit Time'\n\t\t\t,'Group Commits/Sec'\n\t\t\t,'Workfiles Created/sec'\n\t\t\t,'Worktables Created/sec'\n\t\t\t,'Distributed Query'\n\t\t\t,'DTC calls'\n\t\t\t,'Query Store CPU usage'\n\t\t\t,'Query Store physical reads'\n\t\t\t,'Query Store logical reads'\n\t\t\t,'Query Store logical writes'\n\t\t\t,'Execution Errors'\n\t\t) OR (\n\t\t\tspi.[object_name] LIKE '%User Settable%'\n\t\t\tOR spi.[object_name] LIKE '%SQL Errors%'\n\t\t\tOR spi.[object_name] LIKE '%Batch Resp Statistics%'\n\t\t) OR (\n\t\t\tspi.[instance_name] IN ('_Total')\n\t\t\tAND spi.[counter_name] IN (\n\t\t\t\t 'Lock Timeouts/sec'\n\t\t\t\t,'Lock Timeouts (timeout > 0)/sec'\n\t\t\t\t,'Number of Deadlocks/sec'\n\t\t\t\t,'Lock Waits/sec'\n\t\t\t\t,'Latch Waits/sec'\n\t\t\t)\n\t\t)\n)\n\nINSERT INTO @PCounters SELECT * FROM PerfCounters;\n\nSELECT\n\t 'sqlserver_performance' AS [measurement]\n\t,REPLACE(@@SERVERNAME,'\\',':') AS [sql_instance]\n\t,HOST_NAME() AS [computer_name]\n\t,pc.[object_name] AS [object]\n\t,pc.[counter_name] AS [counter]\n\t,CASE pc.[instance_name] WHEN '_Total' THEN 'Total' ELSE ISNULL(pc.[instance_name],'') END AS [instance]\n\t,CAST(CASE WHEN pc.[cntr_type] = 537003264 AND pc1.[cntr_value] > 0 THEN (pc.[cntr_value] * 1.0) / (pc1.[cntr_value] * 1.0) * 100 ELSE pc.[cntr_value] END AS float(10)) AS [value]\n\t,CAST(pc.[cntr_type] AS varchar(25)) AS [counter_type]\nFROM @PCounters AS pc\nLEFT OUTER JOIN @PCounters AS pc1\n\tON (\n\t\tpc.[counter_name] = REPLACE(pc1.[counter_name],' base','')\n\t\tOR pc.[counter_name] = REPLACE(pc1.[counter_name],' base',' (ms)')\n\t)\n\tAND pc.[object_name] = pc1.[object_name]\n\tAND pc.[instance_name] = pc1.[instance_name]\n\tAND pc1.[counter_name] LIKE '%base'\nWHERE\n\tpc.[counter_name] NOT LIKE '% base'\n\tAND @@SERVERNAME = 'instanceName'\nOPTION(RECOMPILE)\n```\n\n----------------------------------------\n\nTITLE: SSH Connection Configuration for Podman Stats Receiver\nDESCRIPTION: Configuration example showing how to connect to Podman via SSH with private key authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/podmanreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  podman_stats:\n    endpoint: ssh://core@localhost:53841/run/user/1000/podman/podman.sock\n    ssh_key: /path/to/ssh/private/key\n    ssh_passphrase: <password>\n```\n\n----------------------------------------\n\nTITLE: Sample Windows Event Log Output Format\nDESCRIPTION: Example of the JSON output structure produced by the windows_eventlog_input operator, showing event details including timestamp, severity, and structured event data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/windows_eventlog_input.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2020-04-30T12:10:17.656726-04:00\",\n  \"severity\": 30,\n  \"body\": {\n\t\t\"event_id\": {\n\t\t\t\"qualifiers\": 0,\n\t\t\t\"id\": 1000\n\t\t},\n\t\t\"provider\": {\n\t\t\t\"name\": \"provider name\",\n\t\t\t\"guid\": \"provider guid\",\n\t\t\t\"event_source\": \"event source\"\n\t\t},\n\t\t\"system_time\": \"2020-04-30T12:10:17.656726789Z\",\n\t\t\"computer\": \"example computer\",\n\t\t\"channel\": \"application\",\n\t\t\"record_id\": 1,\n\t\t\"level\": \"Information\",\n\t\t\"message\": \"example message\",\n\t\t\"task\": \"example task\",\n\t\t\"opcode\": \"example opcode\",\n\t\t\"keywords\": [\"example keyword\"]\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Parser with Timestamp Parsing\nDESCRIPTION: This configuration demonstrates how to use the csv_parser operator with embedded timestamp parsing functionality.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: csv_parser\n  header: 'timestamp_field,severity,message'\n  timestamp:\n    parse_from: body.timestamp_field\n    layout_type: strptime\n    layout: '%Y-%m-%d'\n```\n\n----------------------------------------\n\nTITLE: Using merge_maps function in OTTL\nDESCRIPTION: The merge_maps function combines source and target pcommon.Map objects using different conflict resolution strategies (insert, update, or upsert).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nmerge_maps(log.attributes, ParseJSON(log.body), \"upsert\")\n```\n\nLANGUAGE: go\nCODE:\n```\nmerge_maps(log.attributes, ParseJSON(log.attributes[\"kubernetes\"]), \"update\")\n```\n\nLANGUAGE: go\nCODE:\n```\nmerge_maps(log.attributes, resource.attributes, \"insert\")\n```\n\n----------------------------------------\n\nTITLE: Docker Container Log Parser Configuration\nDESCRIPTION: YAML configuration for parsing Docker container logs with metadata extraction enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: container\n  format: docker\n  add_metadata_from_filepath: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Metrics in RabbitMQ\nDESCRIPTION: YAML configuration snippet showing how to disable default metrics in RabbitMQ collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/rabbitmqreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Example Input and Output JSON for Attributes to Body Copy Operation\nDESCRIPTION: This example shows the transformation when copying from attributes.key to body.newkey. The input shows the original structure, and the output demonstrates the value copied to the body object.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n      \"key\": \"newval\"\n  },\n  \"body\": {\n    \"key1\": \"val1\",\n    \"key2\": \"val2\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n      \"key\": \"newval\"\n  },\n  \"body\": {\n    \"key3\": \"val1\",\n    \"key2\": \"val2\",\n    \"newkey\": \"newval\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Exporter for Profile Export with OTel Mapping Mode in YAML\nDESCRIPTION: An example YAML configuration for Elasticsearch Exporter to export profiles data, using the OTel mapping mode. This requires Universal Profiling to be installed in the Elasticsearch database and uses the Elasticsearch endpoint with an API key.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/elasticsearchexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  elasticsearch:\n    endpoint: https://elastic.example.com:9200\n    mapping:\n      mode: otel\n```\n\n----------------------------------------\n\nTITLE: Defining gRPC Health Check Service RPCs in Protocol Buffers\nDESCRIPTION: Protocol Buffers definitions for the Check and Watch RPCs in the gRPC health check service. Check returns a single response, while Watch returns a stream of responses.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_7\n\nLANGUAGE: protobuf\nCODE:\n```\nrpc Check(HealthCheckRequest) returns (HealthCheckResponse)\n```\n\nLANGUAGE: protobuf\nCODE:\n```\nrpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse)\n```\n\n----------------------------------------\n\nTITLE: Filtering Metrics with Invalid Type in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop metrics that have an invalid type (METRIC_DATA_TYPE_NONE).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    error_mode: ignore\n    metrics:\n      metric:\n        - type == METRIC_DATA_TYPE_NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring CSV Parser with Custom Delimiter\nDESCRIPTION: This example shows how to configure the csv_parser operator with a custom delimiter (tab) and specify the parse_from field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: csv_parser\n  parse_from: body.message\n  header: id,severity,message\n  delimiter: \"\\t\"\n```\n\n----------------------------------------\n\nTITLE: Error Log Count Configuration\nDESCRIPTION: Configuration example showing how to count logs with ERROR or higher severity level.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo:\nexporters:\n  bar:\nconnectors:\n  count:\n    logs:\n      my.error.log.count:\n        description: Error+ logs.\n        conditions:\n          - `severity_number >= SEVERITY_NUMBER_ERROR`\nservice:\n  pipelines:\n    logs:\n      receivers: [foo]\n      exporters: [count]\n    metrics:\n      receivers: [count]\n      exporters: [bar]\n```\n\n----------------------------------------\n\nTITLE: Displaying Buffer Pool Size Configuration\nDESCRIPTION: Table showing the buffer pool size metric with a value of 134217728 bytes (128MB). This appears to be a configuration or monitoring output showing memory allocation for a buffer pool.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/testdata/scraper/innodb_stats.txt#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nname\tcount\nbuffer_pool_size\t134217728\n```\n\n----------------------------------------\n\nTITLE: OTTL Path Examples\nDESCRIPTION: Examples of valid OTTL Path syntax for accessing telemetry fields and nested values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/LANGUAGE.md#2025-04-10_snippet_1\n\nLANGUAGE: OTTL\nCODE:\n```\nmetric.name\nspan.value_double\nresource.name\nresource.attributes[\"key\"]\nlog.attributes[\"nested\"][\"values\"]\ndatapoint.cache[\"slice\"][1]\n```\n\n----------------------------------------\n\nTITLE: Starting the Docker Compose Deployment for Prometheus Federation\nDESCRIPTION: Command to build and start the Docker Compose deployment that connects OpenTelemetry Collector with Prometheus.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/simpleprometheusreceiver/examples/federation/README.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$> docker-compose up --build\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Logs in OpenTelemetry Collector\nDESCRIPTION: This example demonstrates how to parse a JSON log body and add specific fields as attributes on the log. It uses ParseJSON function and merge_maps to handle the JSON data, with conditional execution based on the log body format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  log_statements:\n    - statements:\n        # Parse body as JSON and merge the resulting map with the cache map, ignoring non-json bodies.\n        # cache is a field exposed by OTTL that is a temporary storage place for complex operations.\n        - merge_maps(log.cache, ParseJSON(log.body), \"upsert\") where IsMatch(log.body, \"^\\\\{\") \n          \n        # Set attributes using the values merged into cache.\n        # If the attribute doesn't exist in cache then nothing happens.\n        - set(log.attributes[\"attr1\"], log.cache[\"attr1\"])\n        - set(log.attributes[\"attr2\"], log.cache[\"attr2\"])\n        \n        # To access nested maps you can chain index ([]) operations.\n        # If nested or attr3 do not exist in cache then nothing happens.\n        - set(log.attributes[\"nested.attr3\"], log.cache[\"nested\"][\"attr3\"])\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Metrics in YAML Configuration\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable a specific metric in the tcpcheck module. Replace <metric_name> with the name of the metric you want to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/tcpcheckreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Disabling gRPC Compression in OTELARROW Exporter\nDESCRIPTION: Configuration example showing how to disable the default zstd compression at the gRPC level for the OTELARROW exporter. By default, zstd compression is enabled, but it can be disabled by setting compression to 'none'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/otelarrowexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otelarrow:\n    compression: none\n    endpoint: ...\n    tls: ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Detailed Metrics Level in OpenTelemetry Collector\nDESCRIPTION: YAML configuration snippet showing how to enable detailed level metrics collection for the OpenTelemetry Collector. This setting enables additional stream data and Arrow-specific performance metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nservice\n  ...\n  telemetry:\n    ...\n    metrics:\n      ...\n      level: detailed\n```\n\n----------------------------------------\n\nTITLE: Retrieving SQL Server Query Performance Metrics with T-SQL\nDESCRIPTION: This SQL query retrieves performance statistics for recently executed queries from SQL Server. It aggregates metrics like execution count, elapsed time, CPU time, and I/O operations, then joins with system views to obtain the query text and execution plan.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/queryTextAndPlanQueryData.txt#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith qstats as (\n    SELECT TOP(@topNValue)\n        REPLACE(@@SERVERNAME,'\\\\',':') AS [sql_instance],\n        HOST_NAME() AS [computer_name],\n        MAX(qs.plan_handle) AS query_plan_handle,\n        qs.query_hash AS query_hash,\n        qs.query_plan_hash AS query_plan_hash,\n        SUM(qs.execution_count) AS execution_count,\n        SUM(qs.total_elapsed_time) AS total_elapsed_time,\n        SUM(qs.total_worker_time) AS total_worker_time,\n        SUM(qs.total_logical_reads) AS total_logical_reads,\n        SUM(qs.total_physical_reads) AS total_physical_reads,\n        SUM(qs.total_logical_writes) AS total_logical_writes,\n        SUM(qs.total_rows) AS total_rows,\n        SUM(qs.total_grant_kb) as total_grant_kb\n    FROM sys.dm_exec_query_stats AS qs\n    WHERE qs.last_execution_time BETWEEN DATEADD(SECOND, @granularity, GETDATE()) AND GETDATE()\n    GROUP BY\n        qs.query_hash,\n        qs.query_plan_hash\n)\nSELECT qs.*,\n       SUBSTRING(st.text, (stats.statement_start_offset / 2) + 1,\n                 ((CASE statement_end_offset\n                       WHEN -1 THEN DATALENGTH(st.text)\n                       ELSE stats.statement_end_offset END - stats.statement_start_offset) / 2) + 1) AS text,\n       ISNULL(qp.query_plan, '') AS query_plan\nFROM qstats AS qs\n         INNER JOIN sys.dm_exec_query_stats AS stats on qs.query_plan_handle = stats.plan_handle\n         CROSS APPLY sys.dm_exec_query_plan(qs.query_plan_handle) AS qp\n         CROSS APPLY sys.dm_exec_sql_text(qs.query_plan_handle) AS st\n```\n\n----------------------------------------\n\nTITLE: Configuring High Availability Solace Receiver Setup in YAML\nDESCRIPTION: This snippet shows a high availability configuration for the Solace receiver using multiple brokers. It defines two separate receiver instances, primary and backup, each with its own broker address and authentication details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/solacereceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  solace/primary:\n    broker: [myHost-primary:5671]\n    auth:\n      sasl_plain:\n        username: otel\n        password: otel01$\n    queue: queue://#telemetry-profile123\n\n  solace/backup:\n    broker: [myHost-backup:5671]\n    auth:\n      sasl_plain:\n        username: otel\n        password: otel01$\n    queue: queue://#telemetry-profile123\n\nservice:\n  pipelines:\n    traces/solace:\n      receivers: [solace/primary,solace/backup]\n```\n\n----------------------------------------\n\nTITLE: Configuring Tempo Data Source for Trace to Logs\nDESCRIPTION: LogQL query to use in the Grafana Tempo data source configuration for enabling trace to logs navigation with the new Loki format for OpenTelemetry logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: logql\nCODE:\n```\n{${__tags}} | trace_id=\"${__span.traceId}\"\n```\n\n----------------------------------------\n\nTITLE: Using ConvertTextToElementsXML Converter in Go\nDESCRIPTION: The ConvertTextToElementsXML converter returns an edited version of an XML string where all text belongs to a dedicated element. It supports optional XPath expressions and custom element names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_16\n\nLANGUAGE: Go\nCODE:\n```\nConvertTextToElementsXML(log.body)\n```\n\nLANGUAGE: Go\nCODE:\n```\nConvertTextToElementsXML(log.body, elementName = \"custom\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nConvertTextToElementsXML(log.body, \"/some/part/\", \"value\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Redis CPU Metrics in Go\nDESCRIPTION: Go function that defines a Redis CPU system time metric. Creates a gauge metric with name 'redis.cpu.time' measured in seconds, with a 'sys' state label.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/redisreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfunc usedCPUSys() *redisMetric {\n\treturn &redisMetric{\n\t\tkey:    \"used_cpu_sys\",\n\t\tname:   \"redis.cpu.time\",\n\t\tunits:  \"s\",\n\t\tmdType: metricspb.MetricDescriptor_GAUGE_DOUBLE,\n\t\tlabels: map[string]string{\"state\": \"sys\"},\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Regex Log Search in ClickHouse\nDESCRIPTION: SQL query using regex matching to find logs\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE match(Body, 'http')\n  AND TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Metrics Configuration in YAML\nDESCRIPTION: Configuration snippet showing how to disable default network metrics in the OpenTelemetry Collector. This allows users to selectively turn off specific metrics that are enabled by default.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/networkscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: XML Element Retrieval Example with GetXML\nDESCRIPTION: Examples of using GetXML converter to extract XML elements using XPath expressions. Shows different ways to select elements, attributes, and text content from XML documents.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_25\n\nLANGUAGE: Go\nCODE:\n```\nGetXML(log.body, \"/a\")\nGetXML(log.body, \"//a\")\nGetXML(log.body, \"/a[1]\")\nGetXML(log.body, \"//a[@b='c']\")\nGetXML(log.body, \"/a/text()\")\nGetXML(log.body, \"/a/@foo\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubelet Stats Receiver with Specific Metric Groups in YAML\nDESCRIPTION: Example configuration for collecting metrics from specific resource groups only (node and pod), allowing selective metric collection to reduce data volume and focus on specific resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kubeletstats:\n    collection_interval: 10s\n    auth_type: \"serviceAccount\"\n    endpoint: \"${env:K8S_NODE_NAME}:10250\"\n    insecure_skip_verify: true\n    metric_groups:\n      - node\n      - pod\n```\n\n----------------------------------------\n\nTITLE: Filename Generation for Credentials Storage\nDESCRIPTION: Code snippet showing how the filename for storing collector credentials is generated using a hash of the collector name, installation token, and API base URL.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sumologicextension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nfilename := hash(collector_name, installation_token, api_base_url)\n```\n\n----------------------------------------\n\nTITLE: Output Example for CRI Container Logs\nDESCRIPTION: Processed output showing how the partial and final CRI log entries are combined into a single complete log message with extracted attributes and Kubernetes resource metadata.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2023-06-22 10:27:25.813799277 +0000 UTC\",\n  \"body\": \"multiline containerd line that is super awesome\",\n  \"attributes\": {\n    \"time\": \"2023-06-22T10:27:25.813799277Z\",\n    \"logtag\": \"F\",\n    \"log.iostream\":                \"stdout\",\n    \"log.file.path\": \"/var/log/pods/some_kube-controller-kind-control-plane_49cc7c1fd3702c40b2686ea7486091d6/kube-controller/1.log\"\n  },\n  \"resource\": {\n    \"attributes\": {\n      \"k8s.pod.name\":                \"kube-controller-kind-control-plane\",\n      \"k8s.pod.uid\":                 \"49cc7c1fd3702c40b2686ea7486091d6\",\n      \"k8s.container.name\":          \"kube-controller\",\n      \"k8s.container.restart_count\": \"1\",\n      \"k8s.namespace.name\":          \"some\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Round-Robin Connector Configuration in YAML\nDESCRIPTION: Simple configuration example showing how to define the round-robin connector without any settings, connecting OTLP receiver to multiple Prometheus Remote Write exporters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/roundrobinconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\nexporters:\n  prometheusremotewrite/1:\n  prometheusremotewrite/2:\nconnectors:\n  roundrobin:\n```\n\n----------------------------------------\n\nTITLE: Using ConvertCase Converter in Go\nDESCRIPTION: The ConvertCase converter converts the target string into the desired case (lower, upper, snake, or camel). It returns an error if the target is not a string or doesn't exist.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_14\n\nLANGUAGE: Go\nCODE:\n```\nConvertCase(metric.name, \"snake\")\n```\n\n----------------------------------------\n\nTITLE: Environment-Based Error Log Count Configuration\nDESCRIPTION: Configuration showing how to count error logs while maintaining separate counts per environment.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo:\nexporters:\n  bar:\nconnectors:\n  count:\n    logs:\n      my.error.log.count:\n        description: Error+ logs.\n        conditions:\n          - `severity_number >= SEVERITY_NUMBER_ERROR`\n        attributes:\n          - key: env\nservice:\n  pipelines:\n    logs:\n      receivers: [foo]\n      exporters: [count]\n    metrics:\n      receivers: [count]\n      exporters: [bar]\n```\n\n----------------------------------------\n\nTITLE: K8slog Additional Configuration Options\nDESCRIPTION: Additional configuration options available when discovery mode is not 'sidecar', including authentication and filtering settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Field                         | Default          | Description                                                                                                                                                   |\n|-------------------------------|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `discovery.k8s_api.auth_type` | `serviceAccount` | The authentication type of k8s api. Options are `serviceAccount` or `kubeConfig`.                                                                             |\n| `discovery.host_root`         | `/host-root`     | The directory which the root of host is mounted on.                                                                                                           |\n| `discovery.runtime_apis`      |                  | The runtime apis used to get log file paths. docker and cri-containerd are supported now. By default, it will try to automatically detect the cri-containerd. |\n| `discovery.node_from_env`     | `KUBE_NODE_NAME` | The environment variable name of node name.                                                                                                                   |\n| `discovery.filter`            | []               | The filter used to filter pods and containers. By default, all pods and containers will be collected.                                                         |\n```\n\n----------------------------------------\n\nTITLE: Dropping HTTP Spans in YAML\nDESCRIPTION: Example showing how to configure the filter processor to drop spans that have HTTP request method attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/filterprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    error_mode: ignore\n    traces:\n      span:\n        - attributes[\"http.request.method\"] != nil\n```\n\n----------------------------------------\n\nTITLE: Example Input and Output JSON for Nested Value Copy Operation\nDESCRIPTION: This example shows the transformation when copying a nested value from body.obj.nested to body.newkey. The input shows the original nested structure, and the output shows the value copied to the top level of the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n      \"obj\": {\n        \"nested\":\"nestedvalue\"\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"obj\": {\n        \"nested\":\"nestedvalue\"\n    },\n    \"newkey\":\"nestedvalue\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining an OTTL Statement for Span Attribute Manipulation\nDESCRIPTION: This snippet demonstrates an OTTL statement that sets a new span attribute named 'test' with a value of 'pass' when the attribute doesn't already exist.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/README.md#2025-04-10_snippet_0\n\nLANGUAGE: ottl\nCODE:\n```\nset(span.attributes[\"test\"], \"pass\") where span.attributes[\"test\"] == nil\n```\n\n----------------------------------------\n\nTITLE: Implementing ECS Service Discovery Flow in Go\nDESCRIPTION: Pseudocode demonstrating the overall flow of the ECS service discovery process, including fetching tasks, filtering, decorating with EC2 info, and generating output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: go\nCODE:\n```\nNewECSSD() {\n  session := awsconfig.NewSession()\n  ecsClient := awsecs.NewClient(session)\n  filters := config.NewFilters()\n  decorator := awsec2.NewClient(session)\n  for {\n    select {\n    case <- timer:\n      // Fetch ALL\n      tasks := ecsClient.FetchAll()\n      // Filter\n      filteredTasks := filters.Apply(tasks)\n      // Add EC2 info\n      decorator.Apply(filteredTask)\n      // Generate output\n      if writeResultFile {\n         writeFile(filteredTasks, /etc/ecs_sd.yaml)\n      } else {\n          notifyObserver()\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic AWS Container Insights Receiver Configuration\nDESCRIPTION: Basic configuration example for the AWS Container Insights receiver showing optional parameters including collection interval, container orchestrator type, and pod name handling options.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscontainerinsightreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awscontainerinsightreceiver:\n    # all parameters are optional\n    collection_interval: 60s\n    container_orchestrator: eks\n    add_service_as_attribute: true \n    prefer_full_pod_name: false \n    add_full_pod_name_metric_label: false\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP JSON File Receiver in YAML\nDESCRIPTION: Example configuration for the OTLP JSON File Receiver that specifies files to include and exclude from data collection using glob patterns.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otlpjsonfilereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlpjsonfile:\n    include:\n      - \"/var/log/*.log\"\n    exclude:\n      - \"/var/log/example.log\"\n```\n\n----------------------------------------\n\nTITLE: Parsing Timestamp Using Strptime Layout in YAML\nDESCRIPTION: Example configuration for parsing a timestamp using the strptime layout with a time_parser operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- type: time_parser\n  parse_from: body.timestamp_field\n  layout_type: strptime\n  layout: '%a %b %e %H:%M:%S %Z %Y'\n```\n\n----------------------------------------\n\nTITLE: Stack Trace Output Example\nDESCRIPTION: JSON output showing how the recombine operator processes the stack traces, combining the multi-line stack traces into single log records.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2020-12-04T13:03:38.41149-05:00\",\n    \"severity\": 0,\n    \"body\": \"Log message 1\"\n  },\n  {\n    \"timestamp\": \"2020-12-04T13:03:38.41149-05:00\",\n    \"severity\": 0,\n    \"body\": \"Error: java.lang.Exception: Stack trace\\n        at java.lang.Thread.dumpStack(Thread.java:1336)\\n        at Main.demo3(Main.java:15)\\n        at Main.demo2(Main.java:12)\\n        at Main.demo1(Main.java:9)\\n        at Main.demo(Main.java:6)\\n        at Main.main(Main.java:3)\"\n  },\n  {\n    \"timestamp\": \"2020-12-04T13:03:38.41149-05:00\",\n    \"severity\": 0,\n    \"body\": \"Another log message\",\n  },\n]\n```\n\n----------------------------------------\n\nTITLE: Using Duration Converter in Go\nDESCRIPTION: The Duration converter takes a string representation of a duration and converts it to a Golang time.duration. It supports various time units like ns, us, ms, s, m, and h.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_19\n\nLANGUAGE: Go\nCODE:\n```\nDuration(\"3s\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nDuration(\"333ms\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nDuration(\"1000000h\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Foundry Receiver in YAML\nDESCRIPTION: Example YAML configuration for the Cloud Foundry receiver, specifying RLP Gateway and UAA endpoints, authentication details, and TLS settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/cloudfoundryreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  cloudfoundry:\n    rlp_gateway:\n      endpoint: \"https://log-stream.sys.example.internal\"\n      tls:\n        insecure_skip_verify: false\n      shard_id: \"opentelemetry\"\n    uaa:\n      endpoint: \"https://uaa.sys.example.internal\"\n      tls:\n        insecure_skip_verify: false\n      username: \"otelclient\"\n      password: \"changeit\"\n```\n\n----------------------------------------\n\nTITLE: Example Entry Structure in JSON for OpenTelemetry Collector Contrib\nDESCRIPTION: This JSON snippet demonstrates the structure of an Entry in the OpenTelemetry Collector Contrib project. It includes fields for resource, attributes, body, timestamp, severity, and severity_text. The body field contains a nested object with a message and details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/entry.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"uuid\": \"11112222-3333-4444-5555-666677778888\",\n  },\n  \"attributes\": {\n    \"env\": \"prod\",\n  },\n  \"body\": {\n    \"message\": \"Something happened.\",\n    \"details\": {\n      \"count\": 100,\n      \"reason\": \"event\",\n    },\n  },\n  \"timestamp\": \"2020-01-31T00:00:00-00:00\",\n  \"severity\": 30,\n  \"severity_text\": \"INFO\",\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiline File Input in YAML\nDESCRIPTION: Configuration example for the file_input operator that handles multiline log entries. This setup uses the line_start_pattern parameter to recognize the beginning of new log entries starting with 'START '.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/file_input.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: file_input\n  include:\n    - ./test.log\n  multiline:\n    line_start_pattern: 'START '\n```\n\n----------------------------------------\n\nTITLE: Configuring stdin operator in YAML for OpenTelemetry Collector\nDESCRIPTION: A simple YAML configuration snippet for the stdin operator. This configuration uses the default ID and connects to the next operator in the pipeline.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/stdin.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: stdin\n```\n\n----------------------------------------\n\nTITLE: Example Input and Output JSON for Body to Resource Copy Operation\nDESCRIPTION: This example shows the transformation of an entry when copying from body.key to resource.newkey. The input shows the original structure, and the output shows the value copied to the resource object.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key\":\"value\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n       \"newkey\":\"value\"\n  },\n  \"attributes\": { },\n  \"body\": {\n    \"key\":\"value\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Severity Parser Operator Configuration\nDESCRIPTION: Direct usage of severity_parser operator for parsing severity values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: severity_parser\n  parse_from: body.severity_field\n  mapping:\n    warn: 5xx\n    error: 4xx\n    info: 3xx\n    debug: 2xx\n```\n\n----------------------------------------\n\nTITLE: Matching and Replacing with Capture Groups in YAML\nDESCRIPTION: Configuration that demonstrates using regex capture groups with the regex_replace operator to extract content from within curly braces and concatenate the results.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_replace.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_replace\n  regex: \"{(.*)}\"\n  replace_with: \"${1}\"\n  field: body\n```\n\n----------------------------------------\n\nTITLE: Configuring Logs Transform Processor with Regex Parser in YAML\nDESCRIPTION: This snippet demonstrates how to configure the logstransform processor using YAML. It sets up a regex parser operator to extract time, severity, and message from log entries, and parse timestamp and severity.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logstransformprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  logstransform:\n    operators:\n      - type: regex_parser\n        regex: '^(?P<time>\\d{4}-\\d{2}-\\d{2}) (?P<sev>[A-Z]*) (?P<msg>.*)$'\n        timestamp:\n          parse_from: body.time\n          layout: \"%Y-%m-%d\"\n        severity:\n          parse_from: body.sev\n```\n\n----------------------------------------\n\nTITLE: Defining AWS X-Ray Trace Segment Structure in JSON\nDESCRIPTION: This JSON object defines the structure of an AWS X-Ray trace segment or subsegment. It includes fields for trace and segment identifiers, timing information, error flags, and AWS-specific metadata. The structure is used to represent a portion of a distributed trace in the X-Ray system.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/minOtherFields.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f187253-6a106696d56b1f4ef9eba2ed\",\n    \"id\": \"5cc4a447f5d4d696\",\n    \"name\": \"OtherTopLevelFields\",\n    \"start_time\": 1595437651.680097,\n    \"end_time\": 1595437652.197392,\n    \"error\": false,\n    \"throttle\": true,\n    \"resource_arn\": \"chicken\",\n    \"origin\": \"AWS::EC2::Instance\",\n    \"parent_id\": \"defdfd9912dc5a56\",\n    \"type\": \"subsegment\",\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Chrony Receiver Configuration Example in YAML\nDESCRIPTION: Example of a customized Chrony receiver configuration with specific endpoint, timeout, collection interval, and selected metrics for monitoring NTP skew and stratum values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/chronyreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  chrony:\n    endpoint: unix:///var/run/chrony/chronyd.sock\n    timeout: 10s\n    collection_interval: 30s\n    metrics:\n      ntp.skew:\n        enabled: true\n      ntp.stratum:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Node Resource Attributes Table\nDESCRIPTION: Table defining the resource attributes that are detected and added by the Kubernetes node resource detection processor. Includes the node name and UID attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/internal/k8snode/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Description | Values | Enabled |\n| ---- | ----------- | ------ | ------- |\n| k8s.node.name | The Kubernetes node name | Any Str | true |\n| k8s.node.uid | The Kubernetes node UID | Any Str | true |\n```\n\n----------------------------------------\n\nTITLE: Configuring Unquote Operator for Body Field in YAML\nDESCRIPTION: Configuration example showing how to unquote the body field. This operator takes quoted strings (double quotes, back quotes, or single character in single quotes) and removes the surrounding quotes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/unquote.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: unquote\n  field: body\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes ServiceAccount\nDESCRIPTION: Bash command to create a ServiceAccount for the OpenTelemetry Collector in Kubernetes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: otelcontribcol\n  name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Output JSON for CSV Parsing with Timestamp\nDESCRIPTION: Example JSON output after applying CSV parsing with embedded timestamp parsing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2021-03-17T00:00:00-00:00\",\n  \"body\": {\n    \"severity\": \"debug\",\n    \"message\": \"Debug Message\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Tap Extension in YAML\nDESCRIPTION: A minimal configuration example showing how to set up the Remote Tap extension in the OpenTelemetry Collector. By default, it will listen on localhost:11000 if no custom endpoint is specified.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/remotetapextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  remotetap:\n```\n\n----------------------------------------\n\nTITLE: HAProxy Statistics CSV Data\nDESCRIPTION: Raw HAProxy statistics output in CSV format showing detailed metrics for frontend and backend servers. Includes metrics like connection counts, request rates, session status, and HTTP response codes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/haproxyreceiver/testdata/stats.txt#2025-04-10_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\n# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nstats,FRONTEND,,,0,1,524268,2,1444,47008,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,0,0,1,,,,0,2,0,0,0,0,,0,1,2,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,2,2,0,0,0,,,0,0,,,,,,,0,,,,,,,,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1594,47052,0,0,\nmyfrontend,FRONTEND,,,1,1,524268,1,85470,107711,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,134,0,0,0,0,,0,11,134,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,1,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,134,94712,107309,0,0,\nwebservers,s1,0,0,0,1,,45,28734,36204,,0,,0,0,0,0,UP,1,1,0,0,0,159,0,,1,4,1,,45,,2,0,,4,L4OK,,0,0,45,0,0,0,0,,,,45,0,0,,,,,3,,,0,1,4,95,,,,Layer4 check passed,,2,3,4,,,,192.168.16.2:8080,,http,,,,,,,,0,1,44,,,1,,0,1,26,184,0,0,1,0,1,1,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,\nwebservers,s2,0,0,0,1,,45,28664,36131,,0,,0,0,0,0,UP,1,1,0,0,0,159,0,,1,4,2,,45,,2,0,,4,L4OK,,3,0,45,0,0,0,0,,,,45,0,0,,,,,3,,,0,0,4,99,,,,Layer4 check passed,,2,3,4,,,,192.168.16.3:8080,,http,,,,,,,,0,1,44,,,1,,0,0,18,192,0,0,1,0,1,1,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,\nwebservers,s3,0,0,0,1,,44,28072,35376,,0,,0,0,0,0,UP,1,1,0,0,0,159,0,,1,4,3,,44,,2,0,,4,L4OK,,0,0,44,0,0,0,0,,,,44,0,0,,,,,4,,,0,1,4,121,,,,Layer4 check passed,,2,3,4,,,,192.168.16.4:8080,,http,,,,,,,,0,1,43,,,1,,0,3,25,1331,0,0,1,0,1,1,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,\nwebservers,BACKEND,0,0,0,1,52427,134,85470,107711,0,0,,0,0,0,0,UP,3,3,0,,0,159,0,,1,4,0,,134,,1,0,,11,,,,0,134,0,0,0,0,,,,134,0,0,0,0,0,0,3,,,0,1,4,105,,,,,,,,,,,,,,http,roundrobin,,,,,,,0,3,131,0,0,,,0,3,26,1331,0,,,,,3,0,0,0,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,3,134,107309,91496,0,0,\n```\n\n----------------------------------------\n\nTITLE: Dual Export Pipeline Configuration\nDESCRIPTION: Configuration demonstrating how to count spans and span events while exporting both original traces and count metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo:\nexporters:\n  bar/traces_backend:\n  bar/metrics_backend:\nconnectors:\n  count:\nservice:\n  pipelines:\n    traces:\n      receivers: [foo]\n      exporters: [bar/traces_backend, count]\n    metrics:\n      receivers: [count]\n      exporters: [bar/metrics_backend]\n```\n\n----------------------------------------\n\nTITLE: Parsing SQL Server Performance Metrics in JSON\nDESCRIPTION: This JSON structure represents various SQL Server performance metrics. It includes counters for memory management, resource pool statistics, SQL errors, and query statistics. Each metric is associated with specific objects and instances within the SQL Server environment, identified by the 'sql_instance' and 'computer_name' fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"counter\": \"Memory Grants Outstanding\",\n  \"counter_type\": \"65792\",\n  \"instance\": \"\",\n  \"measurement\": \"sqlserver_performance\",\n  \"object\": \"SQLServer:Memory Manager\",\n  \"sql_instance\": \"8cac97ac9b8f\",\n  \"computer_name\": \"abcde\",\n  \"value\": \"0\"\n}\n```\n\n----------------------------------------\n\nTITLE: MongoDB Atlas Metric Type Definition\nDESCRIPTION: Example of metric type definition showing unit, metric type and value type specifications commonly used throughout the documentation\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/documentation.md#2025-04-10_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type |\n| ---- | ----------- | ---------- |\n| 1 | Gauge | Double |\n```\n\n----------------------------------------\n\nTITLE: Parsing Key-Value Pairs with Timestamp Extraction in YAML\nDESCRIPTION: Configuration for the key_value_parser operator that includes timestamp parsing from an epoch field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: key_value_parser\n  parse_from: message\n  timestamp:\n    parse_from: seconds_since_epoch\n    layout_type: epoch\n    layout: s\n```\n\n----------------------------------------\n\nTITLE: Parsing CSV String in Go\nDESCRIPTION: Parses a CSV string into a pcommon.Map struct. Takes target string, headers, and optional parameters for delimiter, header delimiter, and parsing mode. Returns a mapping of field name to field value.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_42\n\nLANGUAGE: Go\nCODE:\n```\nParseCSV(\"999-999-9999,Joe Smith,joe.smith@example.com\", \"phone,name,email\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseCSV(log.body, \"phone|name|email\", delimiter=\"|\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseCSV(log.attributes[\"csv_line\"], log.attributes[\"csv_headers\"], delimiter=\"|\", headerDelimiter=\",\", mode=\"lazyQuotes\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseCSV(\"\\\"555-555-5556,Joe Smith\\\",joe.smith@example.com\", \"phone,name,email\", mode=\"ignoreQuotes\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Headers Setter Extension in YAML\nDESCRIPTION: This YAML configuration example demonstrates how to set up the headers_setter extension, along with receivers, processors, and exporters. It shows various header actions and value sources, including context-based and static values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/headerssetterextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  headers_setter:\n    headers:\n      - action: insert\n        key: X-Scope-OrgID\n        from_context: tenant_id\n        default_value: Org-ID\n      - action: upsert\n        key: User-ID\n        value: user_id\n      - action: update\n        key: User-ID\n        value: user_id\n      - action: delete\n        key: Some-Header\n\nreceivers:\n  otlp:\n    protocols:\n      http:\n        include_metadata: true\n\nprocessors:\n  batch:\n    # Preserve the tenant-id metadata.\n    metadata_keys:\n    - tenant_id\n\nexporters:\n  loki:\n    labels:\n      resource:\n        container_id: \"\"\n        container_name: \"\"\n    endpoint: https://localhost:<port>/loki/api/v1/push\n    auth:\n      authenticator: headers_setter\n\nservice:\n  extensions: [ headers_setter ]\n  pipelines:\n    traces:\n      receivers: [ otlp ]\n      processors: [ batch ]\n      exporters: [ loki ]\n```\n\n----------------------------------------\n\nTITLE: Simple Journald Input Configuration\nDESCRIPTION: Minimal configuration example for journald_input operator with default settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/journald_input.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Tap Processor in YAML\nDESCRIPTION: Example configuration for the Remote Tap processor, specifying the endpoint and rate limit for WebSocket connections. The endpoint is set to listen on all interfaces, and the rate limit is set to 1 message per second.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/remotetapprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  remotetap:\n    endpoint: 0.0.0.0:12001\n    limit: 1 # rate limit 1 msg/sec\n```\n\n----------------------------------------\n\nTITLE: Configuring UDP Syslog Input with RFC3164 Protocol\nDESCRIPTION: A simple configuration example for setting up the syslog_input operator to listen for UDP syslog messages that follow the RFC3164 protocol. The operator listens on port 54526 on all interfaces and uses UTC as the time location.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/syslog_input.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: syslog_input\n  udp:\n     listen_address: \"0.0.0.0:54526\"\n  syslog:\n     protocol: rfc3164\n     location: UTC\n```\n\n----------------------------------------\n\nTITLE: XML Insertion Examples\nDESCRIPTION: Examples of using InsertXML converter to modify XML documents by adding new elements.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_28\n\nLANGUAGE: Go\nCODE:\n```\nInsertXML(log.body, \"/\", \"<foo/>\")\nInsertXML(log.body, \"//foo\", \"<bar/>\")\nInsertXML(log.body, \"/subdoc\", log.attributes[\"subdoc\"])\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Adding Value to Resource Using Expression\nDESCRIPTION: This JSON demonstrates the output entry after adding a value to the resource using an expression with the 'add' operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"key2\": \"val1_suffix\"\n  },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: MongoDB Atlas Extended Metric Definition\nDESCRIPTION: Extended metric definition table that includes additional fields for aggregation temporality and monotonic properties\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/documentation.md#2025-04-10_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Aggregation Temporality | Monotonic |\n| ---- | ----------- | ---------- | ----------------------- | --------- |\n| ms | Sum | Double | Cumulative | true |\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP HTTP Exporter as SAPM Replacement\nDESCRIPTION: Configuration example for replacing the deprecated SAPM exporter with the OTLP HTTP exporter for sending trace data to Splunk. This requires setting the correct endpoint and authorization headers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n    otlphttp:\n        traces_endpoint: \"${SPLUNK_INGEST_URL}/v2/trace/otlp\"\n        headers:\n            \"X-SF-Token\": \"${SPLUNK_ACCESS_TOKEN}\"\n```\n\n----------------------------------------\n\nTITLE: Resolving Conflicts in OTTL flatten Function\nDESCRIPTION: Shows how the OTTL flatten function resolves conflicts when the resolveConflicts parameter is set to true. Conflicting keys are handled by adding numeric suffixes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"address\": {\n    \"street\": {\n      \"number\": \"first\",\n    },\n    \"house\": \"1234\",\n  },\n  \"address.street\": {\n    \"number\": [\"second\", \"third\"],\n  },\n  \"address.street.number\": \"fourth\",\n  \"occupants\": [\n    \"user 1\",\n    \"user 2\",\n  ],\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"address.street.number\":   \"first\",\n  \"address.house\":           \"1234\",\n  \"address.street.number.0\": \"second\",\n  \"address.street.number.1\": \"third\",\n  \"occupants\":               \"user 1\",\n  \"occupants.0\":             \"user 2\",\n  \"address.street.number.2\": \"fourth\",\n}\n```\n\n----------------------------------------\n\nTITLE: Stack Trace Input Example\nDESCRIPTION: Sample input log file showing stack traces mixed with regular log messages that will be processed by the recombine operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nLog message 1\nError: java.lang.Exception: Stack trace\n        at java.lang.Thread.dumpStack(Thread.java:1336)\n        at Main.demo3(Main.java:15)\n        at Main.demo2(Main.java:12)\n        at Main.demo1(Main.java:9)\n        at Main.demo(Main.java:6)\n        at Main.main(Main.java:3)\nAnother log message\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CloudWatch Metric Streams Extension with JSON Format\nDESCRIPTION: Example configuration for the AWS CloudWatch Metric Streams encoding extension using the JSON format. This allows the collector to unmarshal metrics encoded in the JSON format produced by Amazon CloudWatch Metric Streams.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/awscloudwatchmetricstreamsencodingextension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  awscloudwatchmetricstreams_encoding:\n    format: json\n```\n\n----------------------------------------\n\nTITLE: Kubernetes CRI Log Input Example\nDESCRIPTION: Sample input log file showing Kubernetes logs in CRI format with partial (P) and final (F) log entries that will be processed by the recombine operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n2016-10-06T00:17:09.669794202Z stdout F Single entry log 1\n2016-10-06T00:17:10.113242941Z stdout P This is a very very long line th\n2016-10-06T00:17:10.113242941Z stdout P at is really really long and spa\n2016-10-06T00:17:10.113242941Z stdout F ns across multiple log entries\n```\n\n----------------------------------------\n\nTITLE: Configuring Snowflake Metrics in YAML\nDESCRIPTION: Example configuration to disable specific metrics in the Snowflake collector. This configuration can be applied to any of the default metrics to disable their collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/snowflakereceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Disabling Prometheus Name Normalization in OpenTelemetry Collector\nDESCRIPTION: This command demonstrates how to disable the Prometheus name normalization feature when running the OpenTelemetry Collector. It uses the feature gates option to turn off the normalization.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/translator/prometheus/README.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ otelcol --config=config.yaml --feature-gates=-pkg.translator.prometheus.NormalizeName\n```\n\n----------------------------------------\n\nTITLE: Basic Podman Stats Receiver Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to set up the Podman stats receiver with custom endpoint, timeout, collection interval and metric filtering.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/podmanreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  podman_stats:\n    endpoint: unix://run/podman/podman.sock\n    timeout: 10s\n    collection_interval: 10s\n    initial_delay: 1s\n    metrics:\n      container.cpu.usage.system:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Resource Attributes Table in Markdown\nDESCRIPTION: Markdown table defining the resource attributes that can be detected from Consul, including their descriptions, value types, and enabled status.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/internal/consul/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Description | Values | Enabled |\n| ---- | ----------- | ------ | ------- |\n| cloud.region | The cloud.region | Any Str | true |\n| host.id | The host.id | Any Str | true |\n| host.name | The hostname | Any Str | true |\n```\n\n----------------------------------------\n\nTITLE: Flattening Object at Body Root Level\nDESCRIPTION: Example configuration and results of flattening an object directly under the body field. Shows how nested fields get promoted up one level while preserving other fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/flatten.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: flatten\n  field: body.key1\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": {\n      \"nested1\": \"nestedval1\",\n      \"nested2\": \"nestedval2\"\n    },\n    \"key2\": \"val2\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"resource\": { },\n    \"attributes\": { },\n    \"body\": {\n      \"nested1\": \"nestedval1\",\n      \"nested2\": \"nestedval2\",\n      \"key2\": \"val2\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Dynamic Document ID Configuration Using Transform Processor in YAML\nDESCRIPTION: Configuration example showing how to set the elasticsearch.document_id attribute dynamically using the transform processor. This allows for document ID control and deduplication by having Elasticsearch refuse to index documents with duplicate IDs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/elasticsearchexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  transform/es-doc-id:\n    error_mode: ignore\n    log_statements:\n      - context: log\n        condition: attributes[\"event_name\"] != null && attributes[\"event_creation_time\"] != null\n        statements:\n          - set(attributes[\"elasticsearch.document_id\"], Concat([\"log\", attributes[\"event_name\"], attributes[\"event_creation_time\"], \"-\"]))\n```\n\n----------------------------------------\n\nTITLE: OTTL Map Examples\nDESCRIPTION: Examples of valid OTTL Map syntax for key-value pair structures\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/LANGUAGE.md#2025-04-10_snippet_3\n\nLANGUAGE: OTTL\nCODE:\n```\n{}\n{\"foo\": \"bar\"}\n{\"foo\": {\"a\": 2}}\n{\"foo\": {\"a\": attributes[\"key\"]}}\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Input Operator in YAML\nDESCRIPTION: A simple configuration example for the tcp_input operator, specifying the listen address for incoming TCP connections.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/tcp_input.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: tcp_input\n  listen_address: \"0.0.0.0:54525\"\n```\n\n----------------------------------------\n\nTITLE: UserAgent Parser in Golang\nDESCRIPTION: The UserAgent Converter parses user-agent strings and returns components according to semconv v1.25.0. It uses the uap-go package for recognition of various formats and returns a map with name, version, and original values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_69\n\nLANGUAGE: go\nCODE:\n```\nUserAgent(\"curl/7.81.0\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Duration Metrics Example\nDESCRIPTION: Example of duration histogram metrics generated by the span metrics connector, showing dimensions like service name, span name, span kind, and status code.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/spanmetricsconnector/README.md#2025-04-10_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntraces.span.metrics.duration{service.name=\"shipping\",span.name=\"get_shipping/{shippingId}\",span.kind=\"SERVER\",status.code=\"Ok\"}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Schema Processor Component Structure with Mermaid\nDESCRIPTION: A mermaid diagram illustrating the flow and relationships between Schema Processor components. It shows how data moves from previous collector components through the Transformer, Translation Manager, Translator, Revision, and Interpreter subsystems.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/schemaprocessor/DESIGN.md#2025-04-10_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR;\n    A[Previous Collector Component] --> B[Transformer]\n    B -- Schema URL --> C[Translation Manager]\n    C -- Translation --> B\n    B --> H[Translator]\n    H --> E[Revision]\n    E --> I[ChangeList]\n    subgraph Interpreter\n        direction RL\n        I --> F[Transformer]\n        F --> G[Migrator]\n    end\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Transform Processor\nDESCRIPTION: Configuration example for transforming log bodies containing JSON payloads into attributes that can be processed by the sum connector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/sumconnector/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  transform/logs:\n    log_statements:\n      - context: log\n        statements:\n          - merge_maps(attributes, ParseJSON(body), \"upsert\")\n```\n\n----------------------------------------\n\nTITLE: Multiple Collection Interval Configuration\nDESCRIPTION: Advanced configuration example showing how to scrape different performance counters at different intervals using multiple receiver instances.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsperfcountersreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  windowsperfcounters/memory:\n    metrics:\n      bytes.committed:\n        description: the number of bytes committed to memory\n        unit: By\n        gauge:\n    collection_interval: 30s\n    perfcounters:\n      - object: Memory\n        counters:\n          - name: Committed Bytes\n            metric: bytes.committed\n\n  windowsperfcounters/processor:\n    collection_interval: 1m\n    metrics:\n      processor.time:\n        description: active and idle time of the processor\n        unit: \"%\"\n        gauge:\n    perfcounters:\n      - object: \"Processor\"\n        instances: \"*\"\n        counters:\n          - name: \"% Processor Time\"\n            metric: processor.time\n            attributes:\n              state: active\n      - object: \"Processor\"\n        instances: [\"1\", \"2\"]\n        counters:\n          - name: \"% Idle Time\"\n            metric: processor.time\n            attributes:\n              state: idle\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [windowsperfcounters/memory, windowsperfcounters/processor]\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Adding Object to Body\nDESCRIPTION: This JSON shows the output entry after adding a nested object to the body using the 'add' operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n    \"key2\": {\n      \"nestedkey\":\"nestedvalue\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Response Update in Splunk HEC Receiver\nDESCRIPTION: Response body format change in Splunk HEC receiver from plaintext 'ok' to a JSON structure containing text and code fields for better alignment with Splunk Enterprise.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG-API.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"text\":\"success\", \"code\":0}\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Capability in OpenTelemetry Collector Extension\nDESCRIPTION: This snippet demonstrates how to register a custom capability with an OpAMP extension in the OpenTelemetry Collector. It retrieves the extension, casts it to a CustomCapabilityRegistry, and registers a custom capability.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/opampcustommessages/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfunc Start(_ context.Context, host component.Host) error {\n\text, ok := host.GetExtensions()[opampExtensionID]\n\tif !ok {\n\t\treturn fmt.Errorf(\"extension %q does not exist\", opampExtensionID)\n\t}\n\n\tregistry, ok := ext.(opampcustommessages.CustomCapabilityRegistry)\n\tif !ok {\n\t\treturn fmt.Errorf(\"extension %q is not a custom message registry\", opampExtensionID)\n\t}\n\n\thandler, err := registry.Register(\"io.opentelemetry.custom-capability\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to register custom capability: %w\", err)\n\t}\n\n\t// ... send/receive messages using the given handler\n\n\treturn nil\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Metric with Label Matching in YAML\nDESCRIPTION: Configuration showing how to create a new metric with specific label value matching for container\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: host.cpu.usage\naction: insert\nnew_name: host.cpu.utilization\nmatch_type: strict\nexperimental_match_labels: {\"container\": \"my_container\"}\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Using ConvertAttributesToElementsXML Converter in Go\nDESCRIPTION: The ConvertAttributesToElementsXML converter returns an edited version of an XML string where attributes are converted into child elements. It supports optional XPath expressions for selective conversion.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_15\n\nLANGUAGE: Go\nCODE:\n```\nConvertAttributesToElementsXML(log.body)\n```\n\nLANGUAGE: Go\nCODE:\n```\nConvertAttributesToElementsXML(log.body, \"/Log/Record\")\n```\n\n----------------------------------------\n\nTITLE: Input Log Entry Example - JSON\nDESCRIPTION: Example of an input log entry before scope name parsing, showing the initial structure with empty scope_name field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/scope_name.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"com.example.Foo - some message\",\n  \"scope_name\": \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Receiver in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up the Datadog receiver in the OpenTelemetry Collector. It specifies the endpoint, read timeout, and configures the receiver in both metrics and traces pipelines.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/datadogreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  datadog:\n    endpoint: localhost:8126\n    read_timeout: 60s\n\nexporters:\n  debug:\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [datadog]\n      exporters: [debug]\n    traces:\n      receivers: [datadog]\n      exporters: [debug]\n```\n\n----------------------------------------\n\nTITLE: Parsing Key-Value Pairs with Custom Pair Delimiter in YAML\nDESCRIPTION: Configuration for the key_value_parser operator using an exclamation mark as the pair delimiter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: key_value_parser\n  parse_from: message\n  pair_delimiter: \"!\"\n```\n\n----------------------------------------\n\nTITLE: Resource Attributes Table in Markdown\nDESCRIPTION: Markdown table defining the resource attributes available for the Elastic Beanstalk resource detection processor, including attribute names, descriptions, value types, and enabled status.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/internal/aws/elasticbeanstalk/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Description | Values | Enabled |\n| ---- | ----------- | ------ | ------- |\n| cloud.platform | The cloud.platform | Any Str | true |\n| cloud.provider | The cloud.provider | Any Str | true |\n| deployment.environment | The deployment.environment | Any Str | true |\n| service.instance.id | The service.instance.id | Any Str | true |\n| service.version | The service.version | Any Str | true |\n```\n\n----------------------------------------\n\nTITLE: Adding a String to Body in YAML\nDESCRIPTION: This snippet demonstrates how to add a string value to the body of an entry using the 'add' operator in YAML configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: body.key2\n  value: val2\n```\n\n----------------------------------------\n\nTITLE: AKS Resource Attributes Table Definition in Markdown\nDESCRIPTION: Table defining the resource attributes collected by the AKS resource detection processor, including cloud platform, provider and Kubernetes cluster name attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/internal/azure/aks/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Description | Values | Enabled |\n| ---- | ----------- | ------ | ------- |\n| cloud.platform | The cloud.platform | Any Str | true |\n| cloud.provider | The cloud.provider | Any Str | true |\n| k8s.cluster.name | The k8s.cluster.name parsed from the Azure Instance Metadata Service's infrastructure resource group field | Any Str | false |\n```\n\n----------------------------------------\n\nTITLE: Parsing Simplified XML to JSON - Unique Child Elements\nDESCRIPTION: Shows how XML with unique child elements is converted to JSON, maintaining a simple key-value structure.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_47\n\nLANGUAGE: xml\nCODE:\n```\n<x>\n  <y>1</y>\n  <z>2</z>\n</x>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"x\": {\n    \"y\": \"1\",\n    \"z\": \"2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Timestamps from Plain Text Logs in YAML\nDESCRIPTION: Complete configuration example for parsing timestamps from plain text logs using the filelog receiver and regex_parser operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  debug:\n    verbosity: detailed\nreceivers:\n  filelog:\n    include:\n    - my-app.log\n    start_at: beginning\n    operators:\n    - type: regex_parser\n      regex: (?P<timestamp_field>^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3} (\\+|\\-)\\d{2}\\d{2})\n      timestamp:\n        layout: \"%Y-%m-%d %H:%M:%S.%f %z\"\n        parse_from: attributes.timestamp_field\nservice:\n  pipelines:\n    logs:\n      receivers:\n      - filelog\n      exporters:\n      - debug\n```\n\n----------------------------------------\n\nTITLE: Parsing Cloudflare Log Entry in JSON\nDESCRIPTION: This JSON object represents a comprehensive Cloudflare log entry. It contains information about the client request, server response, caching behavior, security settings, and various performance metrics. The data can be used for analytics, debugging, and monitoring purposes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/cloudflarereceiver/testdata/sample-payloads/all_fields.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"RayID\":\"7a1f7ad4df2f870a\",\"EdgeStartTimestamp\":\"2023-03-03T05:29:06Z\",\"CacheCacheStatus\":\"dynamic\",\"CacheReserveUsed\":false,\"CacheResponseBytes\":9247,\"CacheResponseStatus\":200,\"CacheTieredFill\":false,\"ClientASN\":20115,\"ClientCountry\":\"us\",\"ClientDeviceType\":\"desktop\",\"ClientIP\":\"47.35.104.49\",\"ClientIPClass\":\"noRecord\",\"ClientMTLSAuthCertFingerprint\":\"\",\"ClientMTLSAuthStatus\":\"unknown\",\"ClientRegionCode\":\"MI\",\"ClientRequestBytes\":2667,\"ClientRequestHost\":\"www.theburritobot2.com\",\"ClientRequestMethod\":\"GET\",\"ClientRequestPath\":\"/product/66VCHSJNUP\",\"ClientRequestProtocol\":\"HTTP/2\",\"ClientRequestReferer\":\"https://www.theburritobot2.com/\",\"ClientRequestScheme\":\"https\",\"ClientRequestSource\":\"eyeball\",\"ClientRequestURI\":\"/product/66VCHSJNUP\",\"ClientRequestUserAgent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\"ClientSSLCipher\":\"AEAD-AES128-GCM-SHA256\",\"ClientSSLProtocol\":\"TLSv1.3\",\"ClientSrcPort\":49358,\"ClientTCPRTTMs\":18,\"ClientXRequestedWith\":\"\",\"ContentScanObjResults\":[],\"ContentScanObjTypes\":[],\"Cookies\":{},\"EdgeCFConnectingO2O\":false,\"EdgeColoCode\":\"ORD\",\"EdgeColoID\":398,\"EdgeEndTimestamp\":\"2023-03-03T05:29:06Z\",\"EdgePathingOp\":\"wl\",\"EdgePathingSrc\":\"macro\",\"EdgePathingStatus\":\"nr\",\"EdgeRateLimitAction\":\"\",\"EdgeRateLimitID\":0,\"EdgeRequestHost\":\"www.theburritobot2.com\",\"EdgeResponseBodyBytes\":1963,\"EdgeResponseBytes\":2301,\"EdgeResponseCompressionRatio\":2.54,\"EdgeResponseContentType\":\"text/html\",\"EdgeResponseStatus\":200,\"EdgeServerIP\":\"172.70.131.84\",\"EdgeTimeToFirstByteMs\":28,\"FirewallMatchesActions\":[],\"FirewallMatchesRuleIDs\":[],\"FirewallMatchesSources\":[],\"OriginDNSResponseTimeMs\":0,\"OriginIP\":\"35.223.103.128\",\"OriginRequestHeaderSendDurationMs\":0,\"OriginResponseBytes\":0,\"OriginResponseDurationMs\":22,\"OriginResponseHTTPExpires\":\"\",\"OriginResponseHTTPLastModified\":\"\",\"OriginResponseHeaderReceiveDurationMs\":21,\"OriginResponseStatus\":200,\"OriginResponseTime\":22000000,\"OriginSSLProtocol\":\"none\",\"OriginTCPHandshakeDurationMs\":0,\"OriginTLSHandshakeDurationMs\":0,\"ParentRayID\":\"00\",\"RequestHeaders\":{},\"ResponseHeaders\":{},\"SecurityLevel\":\"med\",\"SmartRouteColoID\":0,\"UpperTierColoID\":0,\"WAFAction\":\"unknown\",\"WAFAttackScore\":0,\"WAFFlags\":\"0\",\"WAFMatchedVar\":\"\",\"WAFProfile\":\"unknown\",\"WAFRCEAttackScore\":0,\"WAFRuleID\":\"\",\"WAFRuleMessage\":\"\",\"WAFSQLiAttackScore\":0,\"WAFXSSAttackScore\":0,\"WorkerCPUTime\":0,\"WorkerStatus\":\"unknown\",\"WorkerSubrequest\":false,\"WorkerSubrequestCount\":0,\"WorkerWallTimeUs\":0,\"ZoneName\":\"otlpdev.net\"}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Attribute Nesting Structure\nDESCRIPTION: Example showing how attributes are nested based on their key prefixes, demonstrating the transformation of flat key-value pairs into a hierarchical structure.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"kubernetes.container_name\": \"xyz\",\n  \"kubernetes.host.name\": \"the host\",\n  \"kubernetes.host.address\": \"127.0.0.1\",\n  \"kubernetes.namespace_name\": \"sumologic\",\n  \"another_attr\": \"42\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"kubernetes\": {\n    \"container_name\": \"xyz\",\n    \"namespace_name\": \"sumologic\",\n    \"host\": {\n      \"name\": \"the host\",\n      \"address\": \"127.0.0.1\"\n    }\n  },\n \"another_attr\": \"42\"\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - otelcol_otel_arrow_receiver_in_flight_items Metric\nDESCRIPTION: Metric table showing details for items in flight measurement. The metric is a non-monotonic Sum type with Int values counted in units of 1.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| 1 | Sum | Int | false |\n```\n\n----------------------------------------\n\nTITLE: Markdown Release Notes Documentation\nDESCRIPTION: Detailed changelog documenting multiple versions of OpenTelemetry Collector Contrib, including breaking changes, new components, enhancements and bug fixes across versions 0.27.0 through 0.31.0.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_43\n\nLANGUAGE: markdown\nCODE:\n```\n# 🎉 OpenTelemetry Collector Contrib v0.31.0 (Beta) 🎉\n\nThe OpenTelemetry Collector Contrib contains everything in the [opentelemetry-collector release](https://github.com/open-telemetry/opentelemetry-collector/releases/tag/v0.31.0) (be sure to check the release notes here as well!). Check out the [Getting Started Guide](https://opentelemetry.io/docs/collector/getting-started/) for deployment and configuration information.\n\n### 🛑 Breaking changes 🛑\n\n- `influxdb` receiver: Removed `metrics_schema` config option (#4277)\n\n### 💡 Enhancements 💡\n\n- Update to OTLP 0.8.0:\n  - Remove use of `IntHistogram` (#4276)\n  - Update exporters/receivers for `NumberDataPoint`\n- Remove use of deprecated `pdata` slice `Resize()` (#4203, #4208, #4209)\n- `awsemf` exporter: Added the option to have a user who is sending metrics from EKS Fargate Container Insights to reformat them to look the same as insights from ECS so that they can be ingested by CloudWatch (#4130)\n- `k8scluster` receiver: Support OpenShift cluster quota metrics (#4342)\n- `newrelic` exporter (#4278):\n  - Requests are now retry-able via configuration option (defaults to retries enabled). Permanent errors are not retried.\n  - The exporter monitoring metrics now include an untagged summary metric for ease of use.\n  - Improved error logging to include URLs that fail to post messages to New Relic.\n- `datadog` exporter: Upscale trace stats when global sampling rate is set (#4213)\n\n### 🧰 Bug fixes 🧰\n\n- `statsd` receiver: Add option to set Counter to be monotonic (#4154)\n- Fix `internal/stanza` severity mappings (#4315)\n- `awsxray` exporter: Fix the wrong AWS env resource setting (#4384)\n- `newrelic` exporter (#4278):\n  - Configuration unmarshalling did not allow timeout value to be set to 0 in the endpoint specific section.\n  - Request cancellation was not propagated via context into the http request.\n  - The queued retry logger is set to a zap.Nop logger as intended.\n```\n\n----------------------------------------\n\nTITLE: Creating OpenTelemetry Metric Sum Tables in SQL\nDESCRIPTION: Defines SQL tables for storing OpenTelemetry metric sum data, including datapoints, attributes, and exemplars. Uses UUID primary keys and foreign key relationships to maintain data integrity.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE otel.metric_sum_datapoint_exemplar\n(\n    \"sum_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    time_unix TIMESTAMP NOT NULL,\n    sum_value DOUBLE,\n    \"trace_id\" VARCHAR (32),\n    \"span_id\" VARCHAR (16),\n    FOREIGN KEY (sum_id) references otel.metric_sum(sum_id) as fk_sum_datapoint_exemplar\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_sum_datapoint_exemplar_attribute\n(\n    \"sum_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (sum_id) references otel.metric_sum(sum_id) as fk_sum_datapoint_exemplar_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_sum_resource_attribute\"\n(\n    \"sum_id\" UUID (primary_key) NOT NULL,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (sum_id),\n    FOREIGN KEY (sum_id) references otel.metric_sum(sum_id) as fk_sum_resource_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_sum_scope_attribute\"\n(\n    \"sum_id\" UUID (primary_key) NOT NULL,\n    \"name\" VARCHAR (256),\n    \"version\" VARCHAR (256),\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (sum_id),\n    FOREIGN KEY (sum_id) references otel.metric_sum(sum_id) as fk_sum_scope_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n```\n\n----------------------------------------\n\nTITLE: Input/Output JSON Example for Space Collapsing\nDESCRIPTION: Example showing the transformation of a JSON log entry with multiple spaces being collapsed to a single space using the regex_replace operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_replace.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"Hello  World\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"Hello World\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Conditions for Metric Generation in YAML\nDESCRIPTION: Example configuration demonstrating the use of OTTL conditions to selectively generate metrics based on incoming data attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/signaltometricsconnector/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsignaltometrics:\n  datapoints:\n    - name: datapoint.bar.sum\n      description: Count total number of datapoints as per datapoint.bar attribute\n      conditions:\n        - resource.attributes[\"foo\"] != nil\n        - resource.attributes[\"bar\"] != nil\n      sum:\n        value: \"1\"\n```\n\n----------------------------------------\n\nTITLE: Converting Prometheus Gauge to OpenTelemetry Metric\nDESCRIPTION: Illustrates how Prometheus gauge metrics are converted to OpenTelemetry format. Unlike counters, gauges don't have start timestamps and use GAUGE_DOUBLE as the metric type. The example also shows proper handling of empty label values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_5\n\nLANGUAGE: go\nCODE:\n```\nmetrics := []*metricspb.Metric{\n  {\n    MetricDescriptor: &metricspb.MetricDescriptor{\n      Name:      \"gauge_test\",\n      Type:      metricspb.MetricDescriptor_GAUGE_DOUBLE,\n      LabelKeys: []*metricspb.LabelKey{{Key: \"id\"}, {Key: \"foo\"}}},\n    Timeseries: []*metricspb.TimeSeries{\n      {\n        StartTimestamp: nil,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"1\", HasValue: true}, {Value: \"bar\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: currentTimestamp, Value: &metricspb.Point_DoubleValue{DoubleValue: 1.0}},\n        },\n      },\n      {\n        StartTimestamp: nil,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"2\", HasValue: true}, {Value: \"\", HasValue: false}},\n        Points: []*metricspb.Point{\n          {Timestamp: currentTimestamp, Value: &metricspb.Point_DoubleValue{DoubleValue: 2.0}},\n        },\n      },\n    },\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Creating OpenTelemetry Exponential Histogram Tables in SQL\nDESCRIPTION: Defines SQL tables for storing OpenTelemetry exponential histogram metric data, including metadata, datapoints, bucket counts, attributes, and exemplars. Uses UUID primary keys and foreign key relationships.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE otel.metric_exp_histogram\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    metric_name varchar (256) not null,\n    metric_description varchar (256),\n    metric_unit varchar (256),\n    aggregation_temporality int8\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_exp_histogram_datapoint\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    id UUID (primary_key) not null,\n    start_time_unix TIMESTAMP,\n    time_unix TIMESTAMP NOT NULL,\n    count LONG,\n    data_sum DOUBLE,\n    scale INTEGER,\n    zero_count LONG,\n    buckets_positive_offset INTEGER,\n    buckets_negative_offset INTEGER,\n    data_min DOUBLE,\n    data_max DOUBLE,\n    flags INT,\n    FOREIGN KEY (histogram_id) references otel.metric_exp_histogram(histogram_id) as fk_exp_histogram_datapoint\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_exp_histogram_datapoint_bucket_positive_count\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    datapoint_id UUID (primary_key) not null,\n    count_id UUID (primary_key) not null,\n    count LONG,\n    FOREIGN KEY (histogram_id) references otel.metric_exp_histogram(histogram_id) as fk_exp_histogram_datapoint_bucket_count\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_exp_histogram_datapoint_bucket_negative_count\n(\n    histogram_id UUID (primary_key, shard_key) not null,\n    datapoint_id UUID (primary_key) not null,\n    count_id UUID (primary_key) not null,\n    count LONG,\n    FOREIGN KEY (histogram_id) references otel.metric_exp_histogram(histogram_id) as fk_exp_histogram_datapoint_bucket_count\n\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_exp_histogram_datapoint_attribute\"\n(\n    \"histogram_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (histogram_id) references otel.metric_exp_histogram(histogram_id) as fk_exp_histogram_datapoint_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_exp_histogram_datapoint_exemplar\n(\n    \"histogram_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    time_unix TIMESTAMP NOT NULL,\n    sum_value DOUBLE,\n    \"trace_id\" VARCHAR (32),\n    \"span_id\" VARCHAR (16),\n    FOREIGN KEY (histogram_id) references otel.metric_exp_histogram(histogram_id) as fk_exp_histogram_datapoint_exemplar\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE otel.metric_exp_histogram_datapoint_exemplar_attribute\n(\n    \"histogram_id\" UUID (primary_key, shard_key) NOT NULL,\n    datapoint_id uuid (primary_key) not null,\n    exemplar_id UUID (primary_key) not null,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    FOREIGN KEY (histogram_id) references otel.metric_exp_histogram(histogram_id) as fk_exp_histogram_datapoint_exemplar_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_exp_histogram_resource_attribute\"\n(\n    \"histogram_id\" UUID (primary_key) NOT NULL,\n    \"key\" VARCHAR (primary_key, 128, dict) NOT NULL,\n    \"string_value\" VARCHAR (256),\n    \"bool_value\" BOOLEAN,\n    \"int_value\" INTEGER,\n    \"double_value\" DOUBLE,\n    \"bytes_value\" BLOB (store_only),\n    SHARD KEY (histogram_id),\n    FOREIGN KEY (histogram_id) references otel.metric_histogram(histogram_id) as fk_histogram_resource_attribute\n) USING TABLE PROPERTIES (NO_ERROR_IF_EXISTS = TRUE);\n\nCREATE TABLE \"otel\".\"metric_exp_histogram_scope_attribute\"\n(\n    \"histogram_id\" UUID (primary_key) NOT NULL,\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write Receiver in YAML\nDESCRIPTION: Demonstrates the configuration for the Prometheus Remote Write receiver, which now supports body unmarshaling and Content-Type negotiation for requests. Note that the HTTP server functionality is still a placeholder.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  prometheusremotewrite:\n    # Configuration options would go here\n    # Currently a placeholder as full functionality is not implemented\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Adding Value to Attributes\nDESCRIPTION: This JSON demonstrates the output entry after adding a value to the attributes using the 'add' operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n     \"key2\": \"val2\"\n  },\n  \"body\": {\n    \"key1\": \"val1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic AWS ECS Container Metrics Receiver in YAML\nDESCRIPTION: A simple configuration example showing how to set up the AWS ECS Container Metrics Receiver with a collection interval parameter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsecscontainermetricsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awsecscontainermetrics:\n    collection_interval: 20s\n```\n\n----------------------------------------\n\nTITLE: Implementing Credential Provider\nDESCRIPTION: Retrieves and configures AWS credentials from the provided configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_6\n\nLANGUAGE: go\nCODE:\n```\nfunc getCredsFromConfig(cfg *Config) (*aws.Credentials, error) {\n\tawscfg, err := awsconfig.LoadDefaultConfig(context.Background(),\n\t\tawsconfig.WithRegion(cfg.Region),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif cfg.RoleARN != \"\" {\n\t\tstsSvc := sts.NewFromConfig(awscfg)\n\n\t\tidentifier := cfg.RoleSessionName\n\t\tif identifier == \"\" {\n\t\t\tb := make([]byte, 5)\n\t\t\trand.Read(b)\n\t\t\tidentifier = base32.StdEncoding.EncodeToString(b)\n\t\t}\n\n\t\tprovider := stscreds.NewAssumeRoleProvider(stsSvc, cfg.RoleARN, func(o *stscreds.AssumeRoleOptions) {\n\t\t\to.RoleSessionName = \"otel-\" + identifier\n\t\t})\n\t\tawscfg.Credentials = aws.NewCredentialsCache(provider)\n\t}\n\n\t_, err = awscfg.Credentials.Retrieve(context.Background())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &awscfg.Credentials, nil\n}\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenTelemetry Collector DaemonSet\nDESCRIPTION: Makefile command to uninstall the OpenTelemetry Collector DaemonSet from a kind Kubernetes cluster.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/dev-docs.md#2025-04-10_snippet_2\n\nLANGUAGE: makefile\nCODE:\n```\nmake kind-uninstall-daemonset\n```\n\n----------------------------------------\n\nTITLE: Configuring Kafka Metrics Collection in YAML\nDESCRIPTION: YAML configuration template for enabling/disabling specific Kafka metrics collection. This configuration can be applied to any metric to control whether it is collected.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkametricsreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Renaming Label Values in YAML\nDESCRIPTION: Renames specific label values for the 'state' label in system.memory.usage metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# rename the label value slab_reclaimable to sreclaimable, slab_unreclaimable to sunreclaimable\ninclude: system.memory.usage\naction: update\noperations:\n  - action: update_label\n    label: state\n    value_actions:\n      - value: slab_reclaimable\n        new_value: sreclaimable\n      - value: slab_unreclaimable\n        new_value: sunreclaimable\n```\n\n----------------------------------------\n\nTITLE: Configuring LogicMonitor Exporter with API Token in YAML\nDESCRIPTION: This snippet shows how to configure the LogicMonitor exporter using an API token for authentication. It requires setting the endpoint and API token credentials.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/logicmonitorexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  logicmonitor:\n    endpoint: \"https://<company_name>.logicmonitor.com/rest\"\n    api_token:\n      access_id: \"<access_id of logicmonitor>\"\n      access_key: \"<access_key of logicmonitor>\"\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenTelemetry Collector\nDESCRIPTION: Kubernetes Deployment manifest for the OpenTelemetry Collector with single replica configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sobjectsreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: otelcontribcol\n  template:\n    metadata:\n      labels:\n        app: otelcontribcol\n    spec:\n      serviceAccountName: otelcontribcol\n      containers:\n      - name: otelcontribcol\n        image: otelcontribcol:latest\n        args: [\"--config\", \"/etc/config/config.yaml\"]\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n        imagePullPolicy: IfNotPresent\n      volumes:\n        - name: config\n          configMap:\n            name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Metrics in Apache Spark YAML Configuration\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable a specific default metric in Apache Spark. Replace <metric_name> with the actual metric name to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/apachesparkreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Metric Collection in YAML\nDESCRIPTION: Example configuration to disable specific metrics collection in the MySQL metrics collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Collector Builder for Kinetica\nDESCRIPTION: YAML configuration for building the OpenTelemetry collector binary with Kinetica exporter support. Specifies the collector version, output path, and required components like exporters, processors, and receivers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kineticaexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ndist:\n  name: otelcol-kinetica\n  description: Otel collector with Kinetica exporter\n  output_path: /home/kinetica/otelexporter_utils/collector-binary\n  otelcol_version: 0.78.2\n\nexporters:\n  - gomod:\n      github.com/open-telemetry/opentelemetry-collector-contrib/exporter/fileexporter v0.78.0\n\nprocessors:\n  - gomod:\n      go.opentelemetry.io/collector/processor/batchprocessor v0.78.2\n\nreceivers:\n  - gomod:\n      go.opentelemetry.io/collector/receiver/otlpreceiver v0.78.2\n  - gomod:\n      github.com/open-telemetry/opentelemetry-collector-contrib/receiver/prometheusreceiver v0.78.0\n```\n\n----------------------------------------\n\nTITLE: Moving Value from Body to Attributes in YAML\nDESCRIPTION: This example illustrates moving a value from the body to the attributes section of a log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: move\n  from: body.ip\n  to: attributes.ip\n```\n\n----------------------------------------\n\nTITLE: Configuring Out-of-Order Time Window in Prometheus\nDESCRIPTION: Configuration setting to enable out-of-order sample ingestion in Prometheus when using multiple consumers. The time window should be set based on the maximum expected delay between samples from different workers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ntsdb.out_of_order_time_window: 10m\n```\n\n----------------------------------------\n\nTITLE: Configuring Resourcedetection EC2 Processor with Custom AWS Metadata Client Settings\nDESCRIPTION: Configuration example for the resourcedetection processor with EC2 detector that demonstrates how to override the default AWS metadata client settings. This allows customizing retry attempts and backoff duration for the EC2 metadata client.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/ec2:\n    detectors: [\"ec2\"]\n    ec2:\n      max_attempts: 10\n      max_backoff: 5m\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Metric Collection in YAML\nDESCRIPTION: Example configuration for enabling/disabling individual Redis metrics using YAML format. This shows how to toggle specific metrics using the enabled flag.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/redisreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Enabling Host CPU Stepping as String in Resource Detection Processor\nDESCRIPTION: Changes the type of 'host.cpu.stepping' from int to string in the resource detection processor. To revert to the old behavior, disable the 'processor.resourcedetection.hostCPUSteppingAsString' feature gate.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_17\n\nLANGUAGE: go\nCODE:\n```\nresourcedetectionprocessor: Change type of `host.cpu.stepping` from int to string.\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in YAML\nDESCRIPTION: YAML configuration snippet to enable optional metrics that are not emitted by default. Each optional metric can be enabled by setting enabled: true.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/expvarreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Installing OpenTelemetry Collector as Kubernetes DaemonSet\nDESCRIPTION: Makefile command to install the OpenTelemetry Collector as a DaemonSet on a kind Kubernetes cluster using the daemonset-collector-dev.yaml configuration sample.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/dev-docs.md#2025-04-10_snippet_1\n\nLANGUAGE: makefile\nCODE:\n```\nmake kind-install-daemonset\n```\n\n----------------------------------------\n\nTITLE: Configuring OpAMP Supervisor with Custom Port in YAML\nDESCRIPTION: Illustrates how to set a custom port for the OpAMP Supervisor's server using the new 'agent::opamp_server_port' configuration option.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  opampsupervisor:\n    agent:\n      opamp_server_port: 8080\n    # Other configuration options\n```\n\n----------------------------------------\n\nTITLE: Flattening Nested Object Within Wrapper\nDESCRIPTION: Example configuration and results of flattening an object nested within another object. Demonstrates how the flatten operator affects deeply nested structures.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/flatten.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: flatten\n  field: body.wrapper.key1\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"wrapper\": {\n      \"key1\": {\n        \"nested1\": \"nestedval1\",\n        \"nested2\": \"nestedval2\"\n      },\n      \"key2\": \"val2\"\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"wrapper\": {\n      \"nested1\": \"nestedval1\",\n      \"nested2\": \"nestedval2\",\n      \"key2\": \"val2\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloud Foundry Garden Observer Extension in YAML\nDESCRIPTION: Example configuration for the cfgarden_observer extension and a receiver_creator using the observer. It includes settings for refresh intervals, Garden endpoint, Cloud Foundry authentication, and a Prometheus receiver rule.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/cfgardenobserver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  cfgarden_observer:\n    refresh_interval: 30s\n    cache_sync_interval: 10m\n    include_app_labels: true\n    garden:\n      endpoint: my/path/to/garden.sock\n    cloud_foundry:\n      endpoint: https://api.cf.mydomain.com\n      auth:\n        type: client_credentials\n        client_id: myclientid\n        client_secret: myclientsecret\n\nreceivers:\n  receiver_creator:\n    watch_observers: [cfgarden_observer]\n    receivers:\n      prometheus_simple:\n        rule: type == \"container\" && labels[\"prometheus.io/scrape\"] == \"true\" \n        config:\n          metrics_path: /metrics\n          endpoint: '`endpoint`'\n```\n\n----------------------------------------\n\nTITLE: Instance Resource Permissions Configuration\nDESCRIPTION: Lists required root permissions for collector process access to system files and docker/containerd sockets. This is necessary for the component which is based on cAdvisor to function properly.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscontainerinsightreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n* `/`\n* `/var/run/docker.sock`\n* `/var/lib/docker`\n* `/run/containerd/containerd.sock`\n* `/sys`\n* `/dev/disk`\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Kubernetes Metrics Configuration in YAML\nDESCRIPTION: YAML configuration snippet showing how to enable optional metrics in the Kubernetes metrics collector. Each metric can be individually enabled by setting the 'enabled' flag to true.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Migrating Group By Attributes Configuration in YAML\nDESCRIPTION: Example showing how to migrate from the old metadata_attribute configuration to using the Group by Attributes processor in the new collector architecture.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sumologicexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# before switch to new collector\nexporters:\n  sumologic:\n    metadata_attribute:\n      - my_attribute\n# after switch to new collector\nprocessors:\n  groupbyattrs:\n    keys:\n      - my_attribute\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Doris Exporter in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the Apache Doris exporter showing all available configuration options including connection settings, table names, schema creation, and queue management settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/dorisexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  doris:\n    endpoint: http://localhost:8030\n    database: otel\n    username: admin\n    password: admin\n    table:\n      logs: otel_logs\n      traces: otel_traces\n      metrics: otel_metrics\n    create_schema: true\n    mysql_endpoint: localhost:9030\n    history_days: 0\n    create_history_days: 0\n    replication_num: 1\n    timezone: Asia/Shanghai\n    timeout: 5s\n    sending_queue:\n      enabled: true\n      num_consumers: 10\n      queue_size: 1000\n    retry_on_failure:\n      enabled: true\n      initial_interval: 5s\n      max_interval: 30s\n      max_elapsed_time: 300s\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Doris Exporter in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the Apache Doris exporter showing all available configuration options including connection settings, table names, schema creation, and queue management settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/dorisexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  doris:\n    endpoint: http://localhost:8030\n    database: otel\n    username: admin\n    password: admin\n    table:\n      logs: otel_logs\n      traces: otel_traces\n      metrics: otel_metrics\n    create_schema: true\n    mysql_endpoint: localhost:9030\n    history_days: 0\n    create_history_days: 0\n    replication_num: 1\n    timezone: Asia/Shanghai\n    timeout: 5s\n    sending_queue:\n      enabled: true\n      num_consumers: 10\n      queue_size: 1000\n    retry_on_failure:\n      enabled: true\n      initial_interval: 5s\n      max_interval: 30s\n      max_elapsed_time: 300s\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON String in Go\nDESCRIPTION: Parses a JSON string into a pcommon.Map or pcommon.Slice struct. Takes a target string in JSON format. Uses goccy/go-json for unmarshalling and converts JSON types to corresponding pdata.Value types.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_43\n\nLANGUAGE: Go\nCODE:\n```\nParseJSON(\"{\\\"attr\\\":true}\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseJSON(\"[\\\"attr1\\\",\\\"attr2\\\"]\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseJSON(resource.attributes[\"kubernetes\"])\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseJSON(log.body)\n```\n\n----------------------------------------\n\nTITLE: Field Reference Example Entry\nDESCRIPTION: Example JSON showing a complete log entry structure with resource, attributes, and body fields for reference demonstration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/field.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"uuid\": \"11112222-3333-4444-5555-666677778888\"\n  },\n  \"attributes\": {\n    \"env\": \"prod\"\n  },\n  \"body\": {\n    \"message\": \"Something happened.\",\n    \"details\": {\n      \"count\": 100,\n      \"reason\": \"event\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-line Webhook Payload Example for Split Log Records\nDESCRIPTION: Example of a webhook payload when split_logs_at_newline is set to true. This will create three separate log records from the input.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/webhookeventreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n{ \"name\": \"francis\", \"city\": \"newyork\" }\n{ \"name\": \"john\", \"city\": \"paris\" }\na third line\n```\n\n----------------------------------------\n\nTITLE: Creating a SpanReference from Parent ID in Python\nDESCRIPTION: This Python snippet shows how to create a SpanReference of type CHILD_OF in Jaeger's Proto format from a parent span ID, since the Proto format doesn't support a direct parent ID field like the Thrift format does.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/translator/jaeger/README.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    SpanReference(\n        ref_type=opentracing.CHILD_OF,\n        trace_id=span.context.trace_id,\n        span_id=parent_id,\n    )\n```\n\n----------------------------------------\n\nTITLE: Adding a Label from Environment Variable in OpenTelemetry YAML Configuration\nDESCRIPTION: This snippet demonstrates how to use an expression to add a label to log attributes by retrieving a value from an environment variable. The expression function env() is used to read the 'STACK' environment variable and assign its value to the 'attributes.stack' field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/expression.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: attributes.stack\n  value: 'EXPR(env(\"STACK\"))'\n```\n\n----------------------------------------\n\nTITLE: Generated Log Entries in JSON Format\nDESCRIPTION: Shows the JSON output format of log entries generated by the tcp_input operator after receiving input messages.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/tcp_input.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2020-04-30T12:10:17.656726-04:00\",\n  \"body\": \"message1\"\n},\n{\n  \"timestamp\": \"2020-04-30T12:10:17.657143-04:00\",\n  \"body\": \"message2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Removing a Value from Attributes\nDESCRIPTION: This shows the resulting JSON entry after the remove operator has removed the 'otherkey' attribute.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {  },\n  \"body\": {\n    \"key\": \"val\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Input JSON for GoTime Layout Parsing\nDESCRIPTION: This JSON represents the input entry for the GoTime layout parsing example. It contains an empty timestamp and a body with a timestamp field to be parsed.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"timestamp_field\": \"Jun 5 13:50:27 EST 2020\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Heartbeat in Splunk HEC Exporter\nDESCRIPTION: Configuration for enabling heartbeat in the Splunk HEC exporter, which sends periodic metadata events about the current environment and build information.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_32\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  splunk_hec:\n    heartbeat:\n      enabled: true\n      interval: 60s\n```\n\n----------------------------------------\n\nTITLE: Custom Span Event Count Configuration\nDESCRIPTION: Configuration example showing how to count span events with specific conditions based on environment and event name attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo:\nexporters:\n  bar:\nconnectors:\n  count:\n    spanevents:\n      my.prod.event.count:\n        description: The number of span events from my prod environment.\n        conditions:\n          - 'attributes[\"env\"] == \"prod\"'\n          - 'name == \"prodevent\"'\n```\n\n----------------------------------------\n\nTITLE: Running OpenTelemetry Collector Splunk Demo\nDESCRIPTION: Command to start the Docker Compose configuration that launches both the OpenTelemetry Collector and Splunk instance. The setup enables viewing metrics data through Splunk's analytics workspace interface.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/splunkhecexporter/example/README.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Defining Metric Test Constants in OpenTelemetry Collector\nDESCRIPTION: These constants define various configuration options for testing metrics in the OpenTelemetry Collector. They include options for the number of points per metric (OnePt, ManyPts), metric types (various gauge and sum types with double or int values), number of labels, and number of resource attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/goldendataset/testdata/pict_input_metrics.txt#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nNumPtsPerMetric: OnePt, ManyPts\nMetricType: DoubleGauge, MonotonicDoubleSum, NonMonotonicDoubleSum, DoubleExemplarsHistogram, IntGauge, MonotonicIntSum, NonMonotonicIntSum, IntExemplarsHistogram\nNumLabels: NoLabels, OneLabel, ManyLabels\nNumResourceAttrs: NoAttrs, OneAttr, TwoAttrs\n```\n\n----------------------------------------\n\nTITLE: Input JSON Entry for Removing an Object from Body\nDESCRIPTION: This shows a sample JSON input entry with a nested object in the body that will be removed by the remove operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"object\": {\n      \"nestedkey\": \"nestedval\"\n    },\n    \"key\": \"val\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Detection System Detector in YAML\nDESCRIPTION: Configuration example showing how to enable host.id on the system detector in the resource detection processor. This restores behavior from before v0.72.0.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\nresourcedetection:\n  detectors: [system]\n  system:\n    resource_attributes:\n      host.id:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Prometheus Receiver Configuration\nDESCRIPTION: Configuration example for setting scrape protocols in Prometheus receiver after removal of enable_protobuf_negotiation option.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nconfig.global.scrape_protocols = [ PrometheusProto, OpenMetricsText1.0.0, OpenMetricsText0.0.1, PrometheusText0.0.4 ]\n```\n\n----------------------------------------\n\nTITLE: Setting AES Key Environment Variable\nDESCRIPTION: This shell command sets the OTEL_AES_CREDENTIAL_PROVIDER environment variable with a base64-encoded AES key. This key is used for decrypting values in the configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/confmap/provider/aesprovider/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport OTEL_AES_CREDENTIAL_PROVIDER=\"GQi+Y8HwOYzs8lAOjHUqB7vXlN8bVU2k0TAKtzwJzac=\"\n```\n\n----------------------------------------\n\nTITLE: Input/Output Example for Object Retention\nDESCRIPTION: Example showing how the retain operator preserves a complex nested object in the body while removing other fields. The nested structure within 'object' is maintained in the output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n    \"object\": {\n      \"nestedkey\": \"val2\",\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"object\": {\n      \"nestedkey\": \"val2\",\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring VPC Flow Log Encoding in YAML\nDESCRIPTION: This snippet shows the configuration for the AWS Logs encoding extension to handle VPC flow logs. It specifies the format as 'vpc_flow_log' and sets the file format to 'plain-text'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/awslogsencodingextension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  awslogs_encoding/cloudwatch:\n    format: vpc_flow_log\n    vpc_flow_log:\n      # options [parquet, plain-text]. \n      # parquet option still needs to be implemented.\n      file_format: plain-text \n```\n\n----------------------------------------\n\nTITLE: Disabling Start Time Adjustment Feature Gate\nDESCRIPTION: Command-line option to disable automatic start timestamp setting for metrics with unknown start times.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n\"--feature-gates=receiver.prometheusreceiver.RemoveStartTimeAdjustment\"\n```\n\n----------------------------------------\n\nTITLE: Configuring TPM Extension in YAML\nDESCRIPTION: Example configuration for the TPM extension in the OpenTelemetry Collector. This snippet shows how to include the TPM extension in the extensions section of the configuration file.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/tpmextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  tpm:\n```\n\n----------------------------------------\n\nTITLE: Input/Output JSON Examples for Conditional Parsing\nDESCRIPTION: Examples showing how conditional parsing works - processing valid JSON objects while leaving non-JSON content unchanged.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"{\\\"key\\\": \\\"val\\\"}\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": {\n    \"key\": \"val\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"notjson\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"notjson\"\n}\n```\n\n----------------------------------------\n\nTITLE: Portable File Path Using Configuration\nDESCRIPTION: Example showing recommended approach of using configuration values for file paths to ensure cross-platform compatibility.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#2025-04-10_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nfilePath := Configuration.Get(\"data_file_path\")\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenTelemetry Collector\nDESCRIPTION: Kubernetes Deployment manifest for the OpenTelemetry Collector with the k8s_events receiver configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8seventsreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: otelcontribcol\n  template:\n    metadata:\n      labels:\n        app: otelcontribcol\n    spec:\n      serviceAccountName: otelcontribcol\n      containers:\n      - name: otelcontribcol\n        image: otelcontribcol:latest\n        args: [\"--config\", \"/etc/config/config.yaml\"]\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n        imagePullPolicy: IfNotPresent\n      volumes:\n        - name: config\n          configMap:\n            name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: List of Valid Timezone Identifiers\nDESCRIPTION: Complete list of acceptable timezone values from the IANA Time Zone Database (tzdb), organized by continent/region including UTC. These values can be used for timezone configuration parameters across the OpenTelemetry Collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logdedupprocessor/timezone.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- \"UTC\"\n- \"Africa/Abidjan\"\n- \"Africa/Accra\"\n- \"Africa/Addis_Ababa\"\n- \"Africa/Algiers\"\n- \"Africa/Asmara\"\n- \"Africa/Bamako\"\n- \"Africa/Bangui\"\n- \"Africa/Banjul\"\n- \"Africa/Bissau\"\n- \"Africa/Blantyre\"\n[...remaining timezone entries omitted for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Building and Setting Up Telemetrygen for Testing\nDESCRIPTION: Shell commands to compile and prepare telemetrygen, a tool for generating synthetic telemetry data to test the secure tracing setup.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ cd cmd/telemetrygen\n$ go build . \n$ cp telemetrygen ../../examples/secure-tracing\n```\n\n----------------------------------------\n\nTITLE: SQL Schema Migration for ClickHouse Metrics Tables\nDESCRIPTION: SQL commands to alter ClickHouse tables for supporting the ServiceName column in the clickhouseexporter component. Users need to execute these ALTER TABLE statements when upgrading to support the new ServiceName column in all metrics tables.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE otel_metrics_exponential_histogram ADD COLUMN ServiceName LowCardinality(String) CODEC(ZSTD(1));\nALTER TABLE otel_metrics_gauge ADD COLUMN ServiceName LowCardinality(String) CODEC(ZSTD(1));\nALTER TABLE otel_metrics_histogram ADD COLUMN ServiceName LowCardinality(String) CODEC(ZSTD(1));\nALTER TABLE otel_metrics_sum ADD COLUMN ServiceName LowCardinality(String) CODEC(ZSTD(1));\nALTER TABLE otel_metrics_summary ADD COLUMN ServiceName LowCardinality(String) CODEC(ZSTD(1));\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in YAML\nDESCRIPTION: YAML configuration snippet demonstrating how to enable optional metrics that are not enabled by default. The metric_name placeholder should be replaced with the specific optional metric to enable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filestatsreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Secrets Manager Placeholder Pattern Examples\nDESCRIPTION: Demonstrates the syntax patterns for accessing secrets from AWS Secrets Manager including basic secret access, JSON key access, and default value specification.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/confmap/provider/secretsmanagerprovider/README.md#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n${secretsmanager:<arn or name>}\n${secretsmanager:<arn or name>#json-key}\n${secretsmanager:<arn or name>:-<default>}\n```\n\n----------------------------------------\n\nTITLE: Running OpAMP Example Server in Shell\nDESCRIPTION: Shell commands to clone the opamp-go repository, navigate to the example server directory, and run the server. This is useful for testing the Supervisor with an example OpAMP server.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/opampsupervisor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:open-telemetry/opamp-go.git\ncd opamp-go/internal/examples/server\ngo run .\n```\n\n----------------------------------------\n\nTITLE: Resource Attributes Table in Markdown\nDESCRIPTION: A markdown table defining the resource attributes that can be detected by the Azure Resource Detection Processor, including Azure VM properties, cloud platform details, and host information. All attributes are enabled by default and accept string values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/internal/azure/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Description | Values | Enabled |\n| ---- | ----------- | ------ | ------- |\n| azure.resourcegroup.name | The azure.resourcegroup.name | Any Str | true |\n| azure.vm.name | The azure.vm.name | Any Str | true |\n| azure.vm.scaleset.name | The azure.vm.scaleset.name | Any Str | true |\n| azure.vm.size | The azure.vm.size | Any Str | true |\n| cloud.account.id | The cloud.account.id | Any Str | true |\n| cloud.platform | The cloud.platform | Any Str | true |\n| cloud.provider | The cloud.provider | Any Str | true |\n| cloud.region | The cloud.region | Any Str | true |\n| host.id | The host.id | Any Str | true |\n| host.name | The hostname | Any Str | true |\n```\n\n----------------------------------------\n\nTITLE: Configuring Datadog Exporter Ignore Resources in YAML\nDESCRIPTION: YAML configuration snippet for adding the ignore_resources configuration option to the Datadog exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_48\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  datadog:\n    ignore_resources:\n      # Add resources to ignore\n```\n\n----------------------------------------\n\nTITLE: Using set function in OTTL\nDESCRIPTION: The set function assigns a value to a telemetry field, allowing direct assignment of values or copying values between telemetry fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_9\n\nLANGUAGE: go\nCODE:\n```\nset(resource.attributes[\"http.path\"], \"/foo\")\n```\n\nLANGUAGE: go\nCODE:\n```\nset(metric.name, resource.attributes[\"http.route\"])\n```\n\nLANGUAGE: go\nCODE:\n```\nset(span.trace_state[\"svc\"], \"example\")\n```\n\nLANGUAGE: go\nCODE:\n```\nset(span.attributes[\"source\"], span.trace_state[\"source\"])\n```\n\n----------------------------------------\n\nTITLE: Setting up Docker Container for Cgroup Testing\nDESCRIPTION: Creates a privileged Docker container with cgroupns host access and mounts the current workspace for testing the extension.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/cgroupruntimeextension/CONTRIBUTING.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd extension/cgroupruntimeextension\ndocker run -ti --privileged --cgroupns=host -v $(pwd):/workspace -w /workspace debian:bookworm-slim\n```\n\n----------------------------------------\n\nTITLE: Input/Output Example for Attribute Field Retention\nDESCRIPTION: Example showing how the retain operator transforms an input entry by keeping only the specified attribute fields. The body is preserved since no body fields were specified in the retain list.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n     \"key1\": \"val1\",\n     \"key2\": \"val2\",\n     \"key3\": \"val3\"\n  },\n  \"body\": {\n    \"key1\": \"val1\",\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n     \"key1\": \"val1\",\n     \"key2\": \"val2\",\n  },\n  \"body\": {\n    \"key1\": \"val1\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional MongoDB Metrics in OpenTelemetry Collector\nDESCRIPTION: YAML configuration snippet showing how to enable optional MongoDB metrics that are not emitted by default.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Example Config for Timestamp Format\nDESCRIPTION: Example showing the before and after timestamp format when using the k8sattr.rfc3339 feature gate in k8sattrprocessor\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n2023-07-10 12:34:39.740638 -0700 PDT m=+0.020184946\n2023-07-10T12:39:53.112485-07:00\n```\n\n----------------------------------------\n\nTITLE: Parsing Timestamps from JSON Logs in YAML\nDESCRIPTION: Complete configuration example for parsing timestamps from JSON logs using the filelog receiver, json_parser, and time_parser operators.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  debug:\n    verbosity: detailed\nreceivers:\n  filelog:\n    include:\n    - logs-json.log\n    start_at: beginning\n    operators:\n    - type: json_parser\n      parse_to: body\n    - type: time_parser\n      parse_from: body.time\n      layout: '%Y-%m-%dT%H:%M:%S.%LZ'\nservice:\n  pipelines:\n    logs:\n      receivers:\n      - filelog\n      exporters:\n      - debug\n```\n\n----------------------------------------\n\nTITLE: Configuring groupbyattrs Processor for Label Promotion in YAML\nDESCRIPTION: Example YAML configuration for the groupbyattrs processor which promotes metric labels to resource labels, useful when an exporter monitors multiple namespaces, clusters, or locations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlemanagedprometheusexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  groupbyattrs:\n    keys:\n    - namespace\n    - cluster\n    - location\n```\n\n----------------------------------------\n\nTITLE: Running the Sample X-Ray Client Application\nDESCRIPTION: Command to run the sample Go application that's instrumented with the X-Ray SDK. AWS credentials need to be provided via environment variables.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngo run sampleapp/sample.go\n```\n\n----------------------------------------\n\nTITLE: Listing Enabled vCenter Metrics\nDESCRIPTION: List of vCenter metrics that are now enabled by default in the vCenter receiver component. Includes metrics related to throughput, operations, latency and cache performance.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_12\n\nLANGUAGE: markdown\nCODE:\n```\n- vcenter.cluster.vsan.throughput\n- vcenter.cluster.vsan.operations\n- vcenter.cluster.vsan.latency.avg\n- vcenter.cluster.vsan.congestions\n- vcenter.host.vsan.throughput\n- vcenter.host.vsan.operations\n- vcenter.host.vsan.latency.avg\n- vcenter.host.vsan.congestions\n- vcenter.host.vsan.cache.hit_rate\n- vcenter.vm.vsan.throughput\n- vcenter.vm.vsan.operations\n- vcenter.vm.vsan.latency.avg\n```\n\n----------------------------------------\n\nTITLE: Retaining an Object in the Body with YAML Configuration\nDESCRIPTION: Configuration to retain only a specific object field in the body of a log entry. This will keep only the 'object' field which contains nested values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: retain\n  fields:\n    - body.object\n```\n\n----------------------------------------\n\nTITLE: Using ToCamelCase Converter for String Formatting in OpenTelemetry\nDESCRIPTION: The ToCamelCase Converter transforms a string into camel case format. It converts strings like 'my_metric_name' to 'MyMetricName' for consistent naming convention.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_57\n\nLANGUAGE: go\nCODE:\n```\nToCamelCase(target)\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Logging for OTTL in YAML\nDESCRIPTION: This YAML configuration enables debug logging in the collector to provide detailed information about OTTL statement execution and data transformation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  telemetry:\n    logs:\n      level: debug\n```\n\n----------------------------------------\n\nTITLE: Configuring TencentCloud LogService Exporter in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up the TencentCloud LogService Exporter with OTLP receiver. It includes required fields such as region, logset, and topic, as well as optional TencentCloud credentials.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/tencentcloudlogserviceexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \":4317\"\n\nexporters:\n  tencentcloud_logservice:\n      # LogService's Region, https://cloud.tencent.com/document/product/614/18940\n      # set cls.{region}.tencentcloudapi.com, eg cls.ap-beijing.tencentcloudapi.com;\n    region: \"ap-beijing\"\n    # LogService's LogSet ID\n    logset: \"demo-logset\"\n    # LogService's Topic ID\n    topic: \"demo-topic\"\n    # TencentCloud secret id\n    secret_id: \"demo-secret-id\"\n    # TencentCloud secret key\n    secret_key: \"demo-secret-key\"\n\nservice:\n  pipelines:\n    logs:\n      receivers: [otlp]\n      exporters: [tencentcloud_logservice]\n```\n\n----------------------------------------\n\nTITLE: Weekday Extractor in Golang\nDESCRIPTION: The Weekday Converter extracts the day of the week (0-6 for Sun-Sat) from a time.Time object. Returns an int64 representing the day of the week.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_77\n\nLANGUAGE: go\nCODE:\n```\nWeekday(Now())\n```\n\n----------------------------------------\n\nTITLE: Removing All Attributes using YAML Configuration\nDESCRIPTION: This snippet demonstrates how to configure the remove operator to delete all fields from the attributes section of an entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\n- type: remove\n  field: attributes\n```\n\n----------------------------------------\n\nTITLE: Configuring Retry on 429 Status for Prometheus Remote Write Exporter in Go\nDESCRIPTION: Adds a new feature gate 'exporter.prometheusremotewritexporter.RetryOn429' to the Prometheus remote write exporter. When enabled, it allows retrying on HTTP 429 status code responses. The feature gate is initially disabled by default.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_16\n\nLANGUAGE: go\nCODE:\n```\nprometheusremotewriteexporter: Add `exporter.prometheusremotewritexporter.RetryOn429` feature gate to retry on http status code 429 response.\n```\n\n----------------------------------------\n\nTITLE: Parsing Complex XML Document with Attributes\nDESCRIPTION: Demonstrates parsing of a more complex XML document with processing instructions, attributes, and nested elements.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_50\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<Log>\n  <User>\n    <ID>00001</ID>\n    <Name type=\"first\">Joe</Name>\n    <Email>joe.smith@example.com</Email>\n  </User>\n  <Text>User fired alert A</Text>\n</Log>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tag\": \"Log\",\n  \"children\": [\n    {\n      \"tag\": \"User\",\n      \"children\": [\n        {\n          \"tag\": \"ID\",\n          \"content\": \"00001\"\n        },\n        {\n          \"tag\": \"Name\",\n          \"content\": \"Joe\",\n          \"attributes\": {\n            \"type\": \"first\"\n          }\n        },\n        {\n          \"tag\": \"Email\",\n          \"content\": \"joe.smith@example.com\"\n        }\n      ]\n    },\n    {\n      \"tag\": \"Text\",\n      \"content\": \"User fired alert A\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Emission in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet shows how to disable a specific metric in the OpenTelemetry Collector. It can be used to control which metrics are emitted by the windowsservice component.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/windowsservicereceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Representing OpenTelemetry Span Path Structure in Markdown Table\nDESCRIPTION: A detailed table showing the supported paths for accessing span context data in the OpenTelemetry Collector, including the field accessed and its type. The table includes paths for span cache, resource attributes, instrumentation scope, and span attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/contexts/ottlspan/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| path                                           | field accessed                                                                                                                                                                                                                                                                                                                                                            | type                                                                    |\n|------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n| span.cache                                     | the value of the current transform context's temporary cache. cache can be used as a temporary placeholder for data during complex transformations                                                                                                                                                                                                                        | pcommon.Map                                                             |\n| span.cache\\[\"\"]                               | the value of an item in cache. Supports multiple indexes to access nested fields.                                                                                                                                                                                                                                                                                         | string, bool, int64, float64, pcommon.Map, pcommon.Slice, []byte or nil |\n| resource                                       | resource of the span being processed                                                                                                                                                                                                                                                                                                                                      | pcommon.Resource                                                        |\n| resource.attributes                            | resource attributes of the span being processed                                                                                                                                                                                                                                                                                                                           | pcommon.Map                                                             |\n| resource.attributes\\[\"\"]                      | the value of the resource attribute of the span being processed. Supports multiple indexes to access nested fields.                                                                                                                                                                                                                                                       | string, bool, int64, float64, pcommon.Map, pcommon.Slice, []byte or nil |\n| resource.dropped_attributes_count              | number of dropped attributes of the resource of the span being processed                                                                                                                                                                                                                                                                                                  | int64                                                                   |\n| instrumentation_scope                          | instrumentation scope of the span being processed                                                                                                                                                                                                                                                                                                                         | pcommon.InstrumentationScope                                            |\n| instrumentation_scope.name                     | name of the instrumentation scope of the span being processed                                                                                                                                                                                                                                                                                                             | string                                                                  |\n| instrumentation_scope.version                  | version of the instrumentation scope of the span being processed                                                                                                                                                                                                                                                                                                          | string                                                                  |\n| instrumentation_scope.dropped_attributes_count | number of dropped attributes of the instrumentation scope of the span being processed                                                                                                                                                                                                                                                                                     | int64                                                                   |\n| instrumentation_scope.attributes               | instrumentation scope attributes of the span being processed                                                                                                                                                                                                                                                                                                              | pcommon.Map                                                             |\n| instrumentation_scope.attributes\\[\"\"]         | the value of the instrumentation scope attribute of the span being processed. Supports multiple indexes to access nested fields.                                                                                                                                                                                                                                          | string, bool, int64, float64, pcommon.Map, pcommon.Slice, []byte or nil |\n| span.attributes                                | attributes of the span being processed                                                                                                                                                                                                                                                                                                                                    | pcommon.Map                                                             |\n| span.attributes\\[\"\"]                          | the value of the attribute of the span being processed. Supports multiple indexes to access nested fields.                                                                                                                                                                                                                                                                | string, bool, int64, float64, pcommon.Map, pcommon.Slice, []byte or nil |\n| span.trace_id                                  | a byte slice representation of the trace id                                                                                                                                                                                                                                                                                                                               | pcommon.TraceID                                                         |\n| span.trace_id.string                           | a string representation of the trace id                                                                                                                                                                                                                                                                                                                                   | string                                                                  |\n```\n\n----------------------------------------\n\nTITLE: Building OpenTelemetry Collector Docker Image\nDESCRIPTION: Command to build the OpenTelemetry Collector Docker image from the repository root.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/couchbase/README.md#2025-04-10_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmake docker-otelcontribcol\n```\n\n----------------------------------------\n\nTITLE: LogDedup Processor Metric Definition Table\nDESCRIPTION: Markdown table defining the otelcol_dedup_processor_aggregated_logs metric properties including its unit, metric type, and value type. This metric tracks the number of log records that were aggregated together.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/logdedupprocessor/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type |\n| ---- | ----------- | ---------- |\n| {records} | Histogram | Int |\n```\n\n----------------------------------------\n\nTITLE: Generating SSL Certificates for Secure Tracing\nDESCRIPTION: Shell command for generating self-signed certificates using OpenSSL for Envoy, OpenTelemetry Collector receiver, and client authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ cd certs\n$ make clean && make all\n```\n\n----------------------------------------\n\nTITLE: Flattening JSON Object with OTTL flatten Function\nDESCRIPTION: Demonstrates the result of using the OTTL flatten function on a nested JSON object. The function moves items from nested maps to the root level, creating flattened key names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"test\",\n  \"address\": {\n    \"street\": \"first\",\n    \"house\": 1234\n  },\n  \"occupants\": [\"user 1\", \"user 2\"]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"test\",\n    \"address.street\": \"first\",\n    \"address.house\": 1234,\n    \"occupants.0\": \"user 1\",\n    \"occupants.1\": \"user 2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Network Reporter with gRPC in Go\nDESCRIPTION: This snippet demonstrates how to register a network reporter with gRPC using the dialOpts. It appends a new gRPC option that sets the stats handler to the network reporter's handler.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/otelarrow/netstats/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ndialOpts = append(dialOpts, grpc.WithStatsHandler(netReporter.Handler()))\n```\n\n----------------------------------------\n\nTITLE: Converting Summary Sum to Sum Metric in OpenTelemetry\nDESCRIPTION: This function creates a new Sum metric from a Summary's sum value. It takes aggregation temporality and monotonicity as parameters. The new metric's name will be '<summary metric name>_sum' and it copies timestamp, starttimestamp, attributes, and description fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nconvert_summary_sum_val_to_sum(\"delta\", true)\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nconvert_summary_sum_val_to_sum(\"cumulative\", false)\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Pubsub Exporter in YAML\nDESCRIPTION: Basic configuration for the Google Cloud Pubsub Exporter, specifying the project and topic.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlecloudpubsubexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  googlecloudpubsub:\n    project: my-project\n    topic: projects/my-project/topics/otlp-traces\n```\n\n----------------------------------------\n\nTITLE: Granting Necessary Permissions for Oracle DB Metrics Collection\nDESCRIPTION: SQL commands to grant the required permissions to the database user for collecting various Oracle DB metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/oracledbreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nGRANT SELECT ON V_$SESSION TO <username>;\nGRANT SELECT ON V_$SYSSTAT TO <username>;\nGRANT SELECT ON V_$RESOURCE_LIMIT TO <username>;\nGRANT SELECT ON DBA_TABLESPACES TO <username>;\nGRANT SELECT ON DBA_DATA_FILES TO <username>;\nGRANT SELECT ON DBA_TABLESPACE_USAGE_METRICS TO <username>;\n```\n\n----------------------------------------\n\nTITLE: Configuring Syslog Parser in YAML\nDESCRIPTION: Example YAML configuration for the syslog_parser operator to parse the 'message' field as syslog using RFC 3164 protocol.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/syslog_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: syslog_parser\n  protocol: rfc3164\n```\n\n----------------------------------------\n\nTITLE: Basic AWS CloudWatch Logs Exporter Configuration\nDESCRIPTION: Minimal configuration example showing required settings for the AWS CloudWatch Logs exporter including log group and stream names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awscloudwatchlogsexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  awscloudwatchlogs:\n    log_group_name: \"testing-logs\"\n    log_stream_name: \"testing-integrations-stream\"\n```\n\n----------------------------------------\n\nTITLE: Basic Trace and Metric Pipeline Configuration\nDESCRIPTION: Configuration showing how to count spans and span events while only exporting the count metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/countconnector/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  foo:\nexporters:\n  bar:\nconnectors:\n  count:\nservice:\n  pipelines:\n    traces:\n      receivers: [foo]\n      exporters: [count]\n    metrics:\n      receivers: [count]\n      exporters: [bar]\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Exporter in YAML\nDESCRIPTION: Example configuration for setting up the RabbitMQ exporter with authentication and custom encoding. Shows how to configure the connection endpoint, authentication credentials, and OTLP JSON encoding extension.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/rabbitmqexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  rabbitmq:\n    connection:\n      endpoint: amqp://localhost:5672\n      auth:\n        plain:\n          username: user\n          password: pass\n    encoding_extension: otlp_encoding/rabbitmq\n\nextensions:\n  otlp_encoding/rabbitmq:\n    protocol: otlp_json\n```\n\n----------------------------------------\n\nTITLE: Parsing Simplified XML to JSON - CDATA Handling\nDESCRIPTION: Shows how XML with CDATA sections is handled during JSON conversion, treating CDATA content as regular text.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_49\n\nLANGUAGE: xml\nCODE:\n```\n<a>\n  <b>1</b>\n  <b><![CDATA[2]]></b>\n</a>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"a\": {\n    \"b\": [\"1\", \"2\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS CloudWatch Metric Streams Extension with OpenTelemetry 1.0.0 Format\nDESCRIPTION: Example configuration for the AWS CloudWatch Metric Streams encoding extension using the OpenTelemetry 1.0.0 format. This allows the collector to unmarshal metrics encoded in the OpenTelemetry 1.0.0 format produced by Amazon CloudWatch Metric Streams.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/awscloudwatchmetricstreamsencodingextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  awscloudwatchmetricstreams_encoding:\n    format: opentelemetry1.0\n```\n\n----------------------------------------\n\nTITLE: Advanced OTTL Functions Usage\nDESCRIPTION: Example showing updated OTTL function parameters usage. Optional parameters are now supported using the ottl.Optional type and function parameter order matches struct field order.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG-API.md#2025-04-10_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\n// Example usage after changes\ntype Arguments struct {\n  RequiredParam string\n  OptionalParam ottl.Optional[string]\n}\n```\n\n----------------------------------------\n\nTITLE: OTTL Converter Examples\nDESCRIPTION: Examples of valid OTTL Converter syntax showing different patterns for data conversion and access\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/LANGUAGE.md#2025-04-10_snippet_0\n\nLANGUAGE: OTTL\nCODE:\n```\nInt()\nIsMatch(field, \".*\")\nSplit(field, \",\")[1]\n```\n\n----------------------------------------\n\nTITLE: Testing Logs Receiver with CompareLogs in Go\nDESCRIPTION: Demonstrates how to test a logs receiver by comparing actual received logs with expected logs from a JSON file. Uses pmetrictest.CompareLogs to verify the output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/pdatatest/README.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nfunc TestLogsReceiver(t *testing.T) {\n\tsink := &consumertest.LogsSink{}\n\trcvr := newLogsReceiver(createDefaultConfig().(*Config), zap.NewNop(), sink)\n\trcvr.client = defaultMockClient()\n\trequire.NoError(t,  rcvr.Start(context.Background(), componenttest.NewNopHost()))\n\trequire.Eventually(t, func() bool {\n\t\treturn sink.LogRecordCount() > 0\n\t}, 2*time.Second, 10*time.Millisecond)\n\terr = rcvr.Shutdown(context.Background())\n\trequire.NoError(t, err)\n\tactualLogs := sink.AllLogs()[0]\n\n\texpectedLogs, err := readLogs(filepath.Join(\"testdata\", \"logs\", \"expected.json\"))\n\trequire.NoError(t, err)\n\n\trequire.NoError(t, pmetrictest.CompareLogs(expectedLogs, actualLogs))\n}\n```\n\n----------------------------------------\n\nTITLE: Using ToSnakeCase Converter for String Formatting in OpenTelemetry\nDESCRIPTION: The ToSnakeCase Converter transforms a string into snake case format. It converts strings like 'MyMetricName' to 'my_metric_name' for consistent naming convention in certain contexts.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_60\n\nLANGUAGE: go\nCODE:\n```\nToSnakeCase(target)\n```\n\n----------------------------------------\n\nTITLE: K8s Metadata Output Example\nDESCRIPTION: Shows the JSON structure of Kubernetes metadata extracted from container log file paths.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"attributes\": {\n      \"k8s.pod.name\": \"kube-controller-kind-control-plane\",\n      \"k8s.pod.uid\": \"49cc7c1fd3702c40b2686ea7486091d6\",\n      \"k8s.container.name\": \"kube-controller\",\n      \"k8s.container.restart_count\": \"1\",\n      \"k8s.namespace.name\": \"some-ns\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Month from Time in Go\nDESCRIPTION: Returns the month component from a time.Time value using the Go standard library time.Month function. Returns the month as an int64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_36\n\nLANGUAGE: Go\nCODE:\n```\nMonth(Now())\n```\n\n----------------------------------------\n\nTITLE: Testing Trace Processor with CompareTraces in Go\nDESCRIPTION: Shows how to test a trace processor by comparing processed traces with expected traces from a JSON file. Uses ptracetest.CompareTraces with options to ignore timestamp fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/pdatatest/README.md#2025-04-10_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nfunc TestTraceProcessor(t *testing.T) {\n\tnextTrace := new(consumertest.TracesSink)\n\ttp, err := newTracesProcessor(NewFactory().CreateDefaultConfig(), nextTrace)\n\ttraces := generateTraces()\n\ttp.ConsumeTraces(ctx, traces)\n\tactualTraces := nextTrace.AllTraces()[0]\n\n\texpectedTraces, err := readTraces(filepath.Join(\"testdata\", \"traces\", \"expected.json\"))\n\trequire.NoError(t, err)\n\t\n\trequire.NoError(t, ptracetest.CompareTraces(expectedTraces, actualTraces, ptracetest.IgnoreStartTimestamp(), \n\t\tptracetest.IgnoreEndTimestamp()))\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Counter Paths\nDESCRIPTION: Complete listing of SQL Server performance counter paths used for monitoring server performance across different components. These paths are typically used with Performance Monitor or monitoring tools to collect SQL Server metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/counters.txt#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\\SQLServer:Access Methods\\Mixed page allocations/sec\n\\SQLServer:Access Methods\\Extent Deallocations/sec\n\\SQLServer:Access Methods\\Page Deallocations/sec\n\\SQLServer:Access Methods\\Page Splits/sec\n\\SQLServer:Access Methods\\Table Lock Escalations/sec\n\\SQLServer:Access Methods\\Deferred Dropped rowsets\n\\SQLServer:Access Methods\\Dropped rowset cleanups/sec\n\\SQLServer:Access Methods\\Dropped rowsets skipped/sec\n\\SQLServer:Access Methods\\Deferred dropped AUs\n\\SQLServer:Access Methods\\AU cleanups/sec\n\\SQLServer:Access Methods\\AU cleanup batches/sec\n\\SQLServer:Access Methods\\Failed AU cleanup batches/sec\n\\SQLServer:Access Methods\\Used tree page cookie\n\\SQLServer:Access Methods\\Failed tree page cookie\n\\SQLServer:Access Methods\\Used leaf page cookie\n\\SQLServer:Access Methods\\Failed leaf page cookie\n\\SQLServer:Access Methods\\LobSS Provider Create Count\n\\SQLServer:Access Methods\\LobSS Provider Destroy Count\n\\SQLServer:Access Methods\\LobSS Provider Truncation Count\n\\SQLServer:Access Methods\\LobHandle Create Count\n\\SQLServer:Access Methods\\LobHandle Destroy Count\n\\SQLServer:Access Methods\\By-reference Lob Create Count\n\\SQLServer:Access Methods\\By-reference Lob Use Count\n\\SQLServer:Access Methods\\Count Push Off Row\n\\SQLServer:Access Methods\\Count Pull In Row\n\\SQLServer:Access Methods\\Count Lob Readahead\n\\SQLServer:Access Methods\\Page compression attempts/sec\n\\SQLServer:Access Methods\\Pages compressed/sec\n\\SQLServer:Access Methods\\InSysXact waits/sec\n\\SQLServer:SQL Errors(*)\\Errors/sec\n\\SQLServer:SQL Statistics\\Batch Requests/sec\n\\SQLServer:SQL Statistics\\Forced Parameterizations/sec\n\\SQLServer:SQL Statistics\\Auto-Param Attempts/sec\n\\SQLServer:SQL Statistics\\Failed Auto-Params/sec\n\\SQLServer:SQL Statistics\\Safe Auto-Params/sec\n\\SQLServer:SQL Statistics\\Unsafe Auto-Params/sec\n\\SQLServer:SQL Statistics\\SQL Compilations/sec\n\\SQLServer:SQL Statistics\\SQL Re-Compilations/sec\n\\SQLServer:SQL Statistics\\SQL Attention rate\n\\SQLServer:SQL Statistics\\Guided plan executions/sec\n\\SQLServer:SQL Statistics\\Misguided plan executions/sec\n\\SQLServer:Plan Cache(*)\\Cache Hit Ratio\n\\SQLServer:Plan Cache(*)\\Cache Pages\n\\SQLServer:Plan Cache(*)\\Cache Object Counts\n\\SQLServer:Plan Cache(*)\\Cache Objects in use\n\\SQLServer:Cursor Manager by Type(*)\\Cache Hit Ratio\n\\SQLServer:Cursor Manager by Type(*)\\Cached Cursor Counts\n\\SQLServer:Cursor Manager by Type(*)\\Cursor Cache Use Counts/sec\n\\SQLServer:Cursor Manager by Type(*)\\Cursor Requests/sec\n\\SQLServer:Cursor Manager by Type(*)\\Active cursors\n\\SQLServer:Cursor Manager by Type(*)\\Cursor memory usage\n\\SQLServer:Cursor Manager by Type(*)\\Cursor worktable usage\n\\SQLServer:Cursor Manager by Type(*)\\Number of active cursor plans\n\\SQLServer:Cursor Manager Total\\Cursor conversion rate\n\\SQLServer:Cursor Manager Total\\Async population count\n\\SQLServer:Cursor Manager Total\\Cursor flushes\n\\SQLServer:Memory Manager\\External benefit of memory\n\\SQLServer:Memory Manager\\Connection Memory (KB)\n\\SQLServer:Memory Manager\\Database Cache Memory (KB)\n\\SQLServer:Memory Manager\\Free Memory (KB)\n\\SQLServer:Memory Manager\\Granted Workspace Memory (KB)\n\\SQLServer:Memory Manager\\Lock Memory (KB)\n\\SQLServer:Memory Manager\\Lock Blocks Allocated\n\\SQLServer:Memory Manager\\Lock Owner Blocks Allocated\n\\SQLServer:Memory Manager\\Lock Blocks\n\\SQLServer:Memory Manager\\Lock Owner Blocks\n\\SQLServer:Memory Manager\\Maximum Workspace Memory (KB)\n\\SQLServer:Memory Manager\\Memory Grants Outstanding\n\\SQLServer:Memory Manager\\Memory Grants Pending\n\\SQLServer:Memory Manager\\Optimizer Memory (KB)\n\\SQLServer:Memory Manager\\Reserved Server Memory (KB)\n\\SQLServer:Memory Manager\\SQL Cache Memory (KB)\n\\SQLServer:Memory Manager\\Stolen Server Memory (KB)\n\\SQLServer:Memory Manager\\Log Pool Memory (KB)\n\\SQLServer:Memory Manager\\Target Server Memory (KB)\n\\SQLServer:Memory Manager\\Total Server Memory (KB)\n\\SQLServer:Memory Node(*)\\Database Node Memory (KB)\n\\SQLServer:Memory Node(*)\\Free Node Memory (KB)\n\\SQLServer:Memory Node(*)\\Foreign Node Memory (KB)\n\\SQLServer:Memory Node(*)\\Stolen Node Memory (KB)\n\\SQLServer:Memory Node(*)\\Target Node Memory (KB)\n\\SQLServer:Memory Node(*)\\Total Node Memory (KB)\n\\SQLServer:User Settable(*)\\Query\n\\SQLServer:Replication Agents(*)\\Running\n\\SQLServer:Replication Merge(*)\\Uploaded Changes/sec\n\\SQLServer:Replication Merge(*)\\Downloaded Changes/sec\n\\SQLServer:Replication Merge(*)\\Conflicts/sec\n\\SQLServer:Replication Logreader(*)\\Logreader:Delivery Latency\n\\SQLServer:Replication Logreader(*)\\Logreader:Delivered Cmds/sec\n\\SQLServer:Replication Logreader(*)\\Logreader:Delivered Trans/sec\n\\SQLServer:Replication Dist.(*)\\Dist:Delivery Latency\n\\SQLServer:Replication Dist.(*)\\Dist:Delivered Cmds/sec\n\\SQLServer:Replication Dist.(*)\\Dist:Delivered Trans/sec\n\\SQLServer:Replication Snapshot(*)\\Snapshot:Delivered Cmds/sec\n\\SQLServer:Replication Snapshot(*)\\Snapshot:Delivered Trans/sec\n\\SQLServer:Backup Device(*)\\Device Throughput Bytes/sec\n\\SQLServer:Transactions\\Transactions\n\\SQLServer:Transactions\\Snapshot Transactions\n\\SQLServer:Transactions\\Update Snapshot Transactions\n\\SQLServer:Transactions\\NonSnapshot Version Transactions\n\\SQLServer:Transactions\\Longest Transaction Running Time\n\\SQLServer:Transactions\\Update conflict ratio\n\\SQLServer:Transactions\\Free Space in tempdb (KB)\n\\SQLServer:Transactions\\Version Generation rate (KB/s)\n\\SQLServer:Transactions\\Version Cleanup rate (KB/s)\n\\SQLServer:Transactions\\Version Store Size (KB)\n\\SQLServer:Transactions\\Version Store unit count\n\\SQLServer:Transactions\\Version Store unit creation\n\\SQLServer:Transactions\\Version Store unit truncation\n\\SQLServer:Broker Statistics\\SQL SENDs/sec\n\\SQLServer:Broker Statistics\\SQL SEND Total\n\\SQLServer:Broker Statistics\\SQL RECEIVEs/sec\n\\SQLServer:Broker Statistics\\SQL RECEIVE Total\n\\SQLServer:Broker Statistics\\Broker Transaction Rollbacks\n\\SQLServer:Broker Statistics\\Dialog Timer Event Count\n\\SQLServer:Broker Statistics\\Enqueued Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P1 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P2 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P3 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P4 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P5 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P6 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P7 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P8 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P9 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued P10 Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued Local Messages/sec\n\\SQLServer:Broker Statistics\\Enqueued Transport Msgs/sec\n\\SQLServer:Broker Statistics\\Enqueued Transport Msg Frags/sec\n\\SQLServer:Broker Statistics\\Enqueued Messages Total\n\\SQLServer:Broker Statistics\\Enqueued Local Messages Total\n\\SQLServer:Broker Statistics\\Enqueued Transport Msgs Total\n\\SQLServer:Broker Statistics\\Enqueued Transport Msg Frag Tot\n\\SQLServer:Broker Statistics\\Forwarded Pending Msg Count\n\\SQLServer:Broker Statistics\\Forwarded Pending Msg Bytes\n\\SQLServer:Broker Statistics\\Forwarded Msgs Discarded/sec\n\\SQLServer:Broker Statistics\\Forwarded Msg Discarded Total\n\\SQLServer:Broker Statistics\\Forwarded Messages/sec\n\\SQLServer:Broker Statistics\\Forwarded Messages Total\n\\SQLServer:Broker Statistics\\Forwarded Msg Bytes/sec\n\\SQLServer:Broker Statistics\\Forwarded Msg Byte Total\n\\SQLServer:Broker Statistics\\Enqueued TransmissionQ Msgs/sec\n\\SQLServer:Broker Statistics\\Dequeued TransmissionQ Msgs/sec\n\\SQLServer:Broker Statistics\\Dropped Messages Total\n\\SQLServer:Broker Statistics\\Corrupted Messages Total\n\\SQLServer:Broker Statistics\\Activation Errors Total\n\\SQLServer:Broker/DBM Transport\\Open Connection Count\n\\SQLServer:Broker/DBM Transport\\Send I/Os/sec\n\\SQLServer:Broker/DBM Transport\\Send I/O bytes/sec\n\\SQLServer:Broker/DBM Transport\\Send I/O Len Avg\n\\SQLServer:Broker/DBM Transport\\Encryption I/Os/sec\n\\SQLServer:Broker/DBM Transport\\Encrypted I/O bytes/sec\n\\SQLServer:Broker/DBM Transport\\Decryption I/Os/sec\n\\SQLServer:Broker/DBM Transport\\Decrypted I/O bytes/sec\n\\SQLServer:Broker/DBM Transport\\Send Flow Control Gate\n\\SQLServer:Broker/DBM Transport\\Send Flow Control Enters/sec\n\\SQLServer:Broker/DBM Transport\\Send Flow Control Exits/sec\n\\SQLServer:Broker/DBM Transport\\Receive Flow Control Gate\n\\SQLServer:Broker/DBM Transport\\Receive Flow Control Enters/sec\n\\SQLServer:Broker/DBM Transport\\Receive Flow Control Exits/sec\n\\SQLServer:Broker/DBM Transport\\Receive I/Os/sec\n\\SQLServer:Broker/DBM Transport\\Receive I/O bytes/sec\n\\SQLServer:Broker/DBM Transport\\Receive I/O Len Avg\n\\SQLServer:Broker/DBM Transport\\Message Fragment Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P1 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P2 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P3 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P4 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P5 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P6 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P7 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P8 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P9 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Message Fragment P10 Sends/sec\n\\SQLServer:Broker/DBM Transport\\Msg Fragment Send Size Avg\n\\SQLServer:Broker/DBM Transport\\Message Fragment Receives/sec\n\\SQLServer:Broker/DBM Transport\\Msg Fragment Recv Size Avg\n\\SQLServer:Broker/DBM Transport\\Pending Msg Frags for Send I/O\n\\SQLServer:Broker/DBM Transport\\Current Msg Frags for Send I/O\n\\SQLServer:Broker/DBM Transport\\Pending Bytes for Send I/O\n\\SQLServer:Broker/DBM Transport\\Current Bytes for Send I/O\n\\SQLServer:Broker/DBM Transport\\Pending Msg Frags for Recv I/O\n\\SQLServer:Broker/DBM Transport\\Current Bytes for Recv I/O\n\\SQLServer:Broker/DBM Transport\\Pending Bytes for Recv I/O\n\\SQLServer:Broker/DBM Transport\\Recv I/O Buffer Copies Count\n\\SQLServer:Broker/DBM Transport\\Recv I/O Buffer Copies bytes/sec\n\\SQLServer:Broker Activation(*)\\Tasks Started/sec\n```\n\n----------------------------------------\n\nTITLE: Converting Duration to Milliseconds in Go\nDESCRIPTION: Converts a time.Duration value to an integer millisecond count. Returns the duration as an int64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_33\n\nLANGUAGE: Go\nCODE:\n```\nMilliseconds(Duration(\"1h\"))\n```\n\n----------------------------------------\n\nTITLE: Aggregating Metrics on Attribute Values in OpenTelemetry Collector\nDESCRIPTION: The aggregate_on_attribute_value function aggregates datapoints in a metric based on specific attribute values. It supports various metric types and aggregation functions, and can be combined with other attribute manipulation functions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nstatements:\n   - delete_matching_keys(resource.attributes, \"(?i).*myRegex.*\") where metric.name == \"system.memory.usage\"\n   - aggregate_on_attribute_value(\"sum\", \"attr1\", [\"val1\", \"val2\"], \"new_val\") where metric.name == \"system.memory.usage\"\n```\n\n----------------------------------------\n\nTITLE: Coralogix Processor Configuration with Sampling\nDESCRIPTION: Extended configuration that enables sampling functionality with a 1GiB cache for storing blueprint hashes. This setup adds sampling priority attributes to spans with new blueprints.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/coralogixprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  coralogix:\n    db_statement_blueprints:\n      sampling:\n        enabled: true\n        max_cache_size_mib: 1024 #1GiB\n```\n\n----------------------------------------\n\nTITLE: Input JSON for CSV Parsing with Custom Delimiter\nDESCRIPTION: Example JSON input for CSV parsing with a custom delimiter (tab).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": {\n    \"message\": \"1 debug Debug Message\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP Receiver to Exporter Mapping in Markdown\nDESCRIPTION: A markdown table showing that the OpenTelemetry Protocol (OTLP) receiver connects to the OTLP exporter in the data pipeline. This represents the flow of telemetry data through the collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/correctnesstests/metrics/testdata/generated_pict_pairs_metrics_pipeline.txt#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nReceiver\tExporter\notlp\totlp\n```\n\n----------------------------------------\n\nTITLE: Copying Nested Values Within Body in OpenTelemetry Collector (YAML)\nDESCRIPTION: This configuration demonstrates how to copy a nested value within the body field. It copies the value from body.obj.nested to body.newkey, showing how to access nested structures.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- type: copy\n  from: body.obj.nested\n  to: body.newkey\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Complex Log Record Output\nDESCRIPTION: Output syslog message formatted according to RFC5424 protocol showing structured data handling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/syslogexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n<86>1 2015-08-05T21:58:59.693012Z 192.168.2.132 SecureAuth0 23108 ID52020 [SecureAuth@27389 UserHostAddress=\"192.168.2.132\" Realm=\"SecureAuth0\" UserID=\"Tester2\" PEN=\"27389\"] Found the user for retrieving user's profile\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Counter Paths List\nDESCRIPTION: A comprehensive list of performance counter paths for monitoring Microsoft SQL Server. These counters track various aspects of SQL Server operation including broker activities, transport metrics, wait statistics, resource usage, and query performance. The TEST_NAME placeholder is used for the SQL Server instance name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/named-instance-counters.txt#2025-04-10_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\n\\MSSQL$TEST_NAME:Broker Statistics\\Dropped Messages Total\n\\MSSQL$TEST_NAME:Broker Statistics\\Corrupted Messages Total\n\\MSSQL$TEST_NAME:Broker Statistics\\Activation Errors Total\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Open Connection Count\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Send I/Os/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Send I/O bytes/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Send I/O Len Avg\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Encryption I/Os/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Encrypted I/O bytes/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Decryption I/Os/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Decrypted I/O bytes/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Send Flow Control Gate\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Send Flow Control Enters/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Send Flow Control Exits/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Receive Flow Control Gate\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Receive Flow Control Enters/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Receive Flow Control Exits/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Receive I/Os/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Receive I/O bytes/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Receive I/O Len Avg\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P1 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P2 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P3 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P4 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P5 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P6 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P7 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P8 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P9 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment P10 Sends/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Msg Fragment Send Size Avg\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Message Fragment Receives/sec\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Msg Fragment Recv Size Avg\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Pending Msg Frags for Send I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Current Msg Frags for Send I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Pending Bytes for Send I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Current Bytes for Send I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Pending Msg Frags for Recv I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Current Bytes for Recv I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Pending Bytes for Recv I/O\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Recv I/O Buffer Copies Count\n\\MSSQL$TEST_NAME:Broker/DBM Transport\\Recv I/O Buffer Copies bytes/sec\n\\MSSQL$TEST_NAME:Broker Activation(*)\\Tasks Started/sec\n\\MSSQL$TEST_NAME:Broker Activation(*)\\Tasks Running\n\\MSSQL$TEST_NAME:Broker Activation(*)\\Tasks Aborted/sec\n\\MSSQL$TEST_NAME:Broker Activation(*)\\Task Limit Reached/sec\n\\MSSQL$TEST_NAME:Broker Activation(*)\\Task Limit Reached\n\\MSSQL$TEST_NAME:Broker Activation(*)\\Stored Procedures Invoked/sec\n\\MSSQL$TEST_NAME:Broker TO Statistics\\Transmission Obj Gets/Sec\n\\MSSQL$TEST_NAME:Broker TO Statistics\\Transmission Obj Set Dirty/Sec\n\\MSSQL$TEST_NAME:Broker TO Statistics\\Transmission Obj Writes/Sec\n\\MSSQL$TEST_NAME:Broker TO Statistics\\Avg. Length of Batched Writes\n\\MSSQL$TEST_NAME:Broker TO Statistics\\Avg. Time to Write Batch (ms)\n\\MSSQL$TEST_NAME:Broker TO Statistics\\Avg. Time Between Batches (ms)\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Lock waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Memory grant queue waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Thread-safe memory objects waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Log write waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Log buffer waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Network IO waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Page IO latch waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Page latch waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Non-Page latch waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Wait for the worker\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Workspace synchronization waits\n\\MSSQL$TEST_NAME:Wait Statistics(*)\\Transaction ownership waits\n\\MSSQL$TEST_NAME:Exec Statistics(*)\\Extended Procedures\n\\MSSQL$TEST_NAME:Exec Statistics(*)\\DTC calls\n\\MSSQL$TEST_NAME:Exec Statistics(*)\\OLEDB calls\n\\MSSQL$TEST_NAME:Exec Statistics(*)\\Distributed Query\n\\MSSQL$TEST_NAME:CLR\\CLR Execution\n\\MSSQL$TEST_NAME:Catalog Metadata(*)\\Cache Hit Ratio\n\\MSSQL$TEST_NAME:Catalog Metadata(*)\\Cache Entries Count\n\\MSSQL$TEST_NAME:Catalog Metadata(*)\\Cache Entries Pinned Count\n\\MSSQL$TEST_NAME:Deprecated Features(*)\\Usage\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\CPU usage %\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\CPU effective %\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\CPU delayed %\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\CPU violated %\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Queued requests\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Active requests\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Requests completed/sec\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Max request cpu time (ms)\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Blocked tasks\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Reduced memory grants/sec\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Max request memory grant (KB)\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Query optimizations/sec\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Suboptimal plans/sec\n\\MSSQL$TEST_NAME:Workload Group Stats(*)\\Active parallel threads\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\CPU usage %\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\CPU usage target %\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\CPU control effect %\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\CPU effective %\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\CPU delayed %\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\CPU violated %\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Compile memory target (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Cache memory target (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Query exec memory target (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Memory grants/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Active memory grants count\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Memory grant timeouts/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Active memory grant amount (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Pending memory grants count\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Max memory (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Used memory (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Target memory (KB)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Disk Read IO/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Disk Read IO Throttled/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Disk Read Bytes/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Avg Disk Read IO (ms)\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Disk Write IO/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Disk Write IO Throttled/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Disk Write Bytes/sec\n\\MSSQL$TEST_NAME:Resource Pool Stats(*)\\Avg Disk Write IO (ms)\n\\MSSQL$TEST_NAME:FileTable\\FileTable db operations/sec\n\\MSSQL$TEST_NAME:FileTable\\FileTable table operations/sec\n\\MSSQL$TEST_NAME:FileTable\\FileTable item get requests/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time to get FileTable item\n\\MSSQL$TEST_NAME:FileTable\\FileTable item delete reqs/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time delete FileTable item\n\\MSSQL$TEST_NAME:FileTable\\FileTable item update reqs/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time update FileTable item\n\\MSSQL$TEST_NAME:FileTable\\FileTable item move reqs/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time move FileTable item\n\\MSSQL$TEST_NAME:FileTable\\FileTable item rename reqs/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time rename FileTable item\n\\MSSQL$TEST_NAME:FileTable\\FileTable enumeration reqs/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time FileTable enumeration\n\\MSSQL$TEST_NAME:FileTable\\FileTable file I/O requests/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time per file I/O request\n\\MSSQL$TEST_NAME:FileTable\\FileTable file I/O response/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time per file I/O response\n\\MSSQL$TEST_NAME:FileTable\\FileTable kill handle ops/sec\n\\MSSQL$TEST_NAME:FileTable\\Avg time FileTable handle kill\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000000ms & <000001ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000001ms & <000002ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000002ms & <000005ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000005ms & <000010ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000010ms & <000020ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000020ms & <000050ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000050ms & <000100ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000100ms & <000200ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000200ms & <000500ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=000500ms & <001000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=001000ms & <002000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=002000ms & <005000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=005000ms & <010000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=010000ms & <020000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=020000ms & <050000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=050000ms & <100000ms\n\\MSSQL$TEST_NAME:Batch Resp Statistics(*)\\Batches >=100000ms\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Read Bytes/Sec\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Write Bytes/Sec\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Total Bytes/Sec\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Reads/Sec\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Writes/Sec\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Transfers/Sec\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Avg. Bytes/Read\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Avg. Bytes/Write\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Avg. Bytes/Transfer\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Avg. microsec/Read\n\\MSSQL$TEST_NAME:HTTP Storage(*)\\Avg. microsec/Write\n```\n\n----------------------------------------\n\nTITLE: JSON output from stdin operator in OpenTelemetry\nDESCRIPTION: A sample JSON output produced by the stdin operator when processing input. The output includes a timestamp, severity level (0), and the body containing the input text.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/stdin.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2020-11-10T11:09:56.505467-05:00\",\n  \"severity\": 0,\n  \"body\": \"test\"\n}\n```\n\n----------------------------------------\n\nTITLE: Header-based Parsing Configuration\nDESCRIPTION: YAML configuration showing parsing with headers into attributes\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_array_parser.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- type: json_array_parser\n  parse_to: attributes\n  header: origin,sev,message,isBool\n```\n\n----------------------------------------\n\nTITLE: Defining Severity Number Enums for OpenTelemetry Logs in Markdown\nDESCRIPTION: This snippet defines a table of enum symbols and their corresponding numeric values for severity levels in OpenTelemetry logs. It includes 25 severity levels ranging from unspecified (0) to fatal4 (24), providing a detailed granularity for log severity classification.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/contexts/ottllog/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Enum Symbol                 | Value |\n|-----------------------------|-------|\n| SEVERITY_NUMBER_UNSPECIFIED | 0     |\n| SEVERITY_NUMBER_TRACE       | 1     |\n| \tSEVERITY_NUMBER_TRACE2     | 2     |\n| \tSEVERITY_NUMBER_TRACE3     | 3     |\n| \tSEVERITY_NUMBER_TRACE4     | 4     |\n| \tSEVERITY_NUMBER_DEBUG      | 5     |\n| \tSEVERITY_NUMBER_DEBUG2     | 6     |\n| \tSEVERITY_NUMBER_DEBUG3     | 7     |\n| \tSEVERITY_NUMBER_DEBUG4     | 8     |\n| \tSEVERITY_NUMBER_INFO       | 9     |\n| \tSEVERITY_NUMBER_INFO2      | 10    |\n| \tSEVERITY_NUMBER_INFO3      | 11    |\n| \tSEVERITY_NUMBER_INFO4      | 12    |\n| \tSEVERITY_NUMBER_WARN       | 13    |\n| \tSEVERITY_NUMBER_WARN2      | 14    |\n| \tSEVERITY_NUMBER_WARN3      | 15    |\n| \tSEVERITY_NUMBER_WARN4      | 16    |\n| \tSEVERITY_NUMBER_ERROR      | 17    |\n| \tSEVERITY_NUMBER_ERROR2     | 18    |\n| \tSEVERITY_NUMBER_ERROR3     | 19    |\n| \tSEVERITY_NUMBER_ERROR4     | 20    |\n| \tSEVERITY_NUMBER_FATAL      | 21    |\n| \tSEVERITY_NUMBER_FATAL2     | 22    |\n| \tSEVERITY_NUMBER_FATAL3     | 23    |\n| \tSEVERITY_NUMBER_FATAL4     | 24    |\n```\n\n----------------------------------------\n\nTITLE: Basic Webhook Event Receiver Configuration in YAML\nDESCRIPTION: Demonstrates configuration of the Webhook Event Receiver with endpoint, timeouts, custom paths, required header validation, and log splitting behavior.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/webhookeventreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    webhookevent:\n        endpoint: localhost:8088\n        read_timeout: \"500ms\"\n        path: \"eventsource/receiver\"\n        health_path: \"eventreceiver/healthcheck\"\n        required_header:\n            key: \"required-header-key\"\n            value: \"required-header-value\"\n        split_logs_at_newline: false\n```\n\n----------------------------------------\n\nTITLE: Using Substring Converter for String Extraction in OpenTelemetry\nDESCRIPTION: The Substring Converter extracts a portion of a string from a given start index to a specified length. It requires a target string and two int64 values for start and length, returning an error if parameters are invalid.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_55\n\nLANGUAGE: go\nCODE:\n```\nSubstring(target, start, length)\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests\nDESCRIPTION: Executes integration tests with race detection enabled, running as sudo with specific timeout and parallel execution settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/cgroupruntimeextension/CONTRIBUTING.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nCGO_ENABLED=1 go test -v -exec sudo -race -timeout 360s -parallel 4 -tags=integration,\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Day Converter in Go\nDESCRIPTION: The Day converter returns the day component from the specified time using the Go stdlib time.Day function. It accepts a time.Time value and returns an int64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_17\n\nLANGUAGE: Go\nCODE:\n```\nDay(Now())\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Protocol Compatibility Matrix\nDESCRIPTION: A table showing supported combinations of receivers (zipkin, otlp, jaeger, opencensus) and exporters (opencensus, otlp, zipkin) in the OpenTelemetry Collector. The matrix format indicates which protocol translations are supported for telemetry data flow.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/correctnesstests/traces/testdata/generated_pict_pairs_traces_pipeline.txt#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nReceiver\tExporter\nzipkin\topencensus\notlp\topencensus\njaeger\topencensus\nzipkin\totlp\nopencensus\topencensus\notlp\tzipkin\njaeger\tzipkin\nopencensus\tzipkin\notlp\totlp\njaeger\totlp\nopencensus\totlp\nzipkin\tzipkin\n```\n\n----------------------------------------\n\nTITLE: Configuring NoOp Operator in Non-Linear Pipeline\nDESCRIPTION: Example configuration showing how to use the noop operator as a terminal node in a non-linear pipeline. The router directs logs to different parsers based on format, and all logs eventually converge at the noop operator before being emitted from the receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/noop.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\noperators:\n  - type: router\n    routes:\n      - output: json_parser\n        expr: 'body.format == \"json\"'\n      - output: syslog_parser\n        expr: 'body.format == \"syslog\"'\n  - type: json_parser\n    output: noop  # If this were not set, logs would implicitly pass to the next operator\n  - type: syslog_parser\n  - type: noop\n```\n\n----------------------------------------\n\nTITLE: Defining AWS X-Ray Trace JSON Structure\nDESCRIPTION: This JSON object defines the structure of an AWS X-Ray trace for a sample server request. It includes trace identifiers, timing information, HTTP request and response details, AWS X-Ray SDK metadata, EKS (Elastic Kubernetes Service) information, and service compiler details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/serverSample.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f2aebcc-b475d14618c51eaa28753d37\",\n    \"id\": \"bda182a644eee9b3\",\n    \"name\": \"SampleServer\",\n    \"start_time\": 1596648396.6399446,\n    \"end_time\": 1596648396.6401389,\n    \"http\": {\n        \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"http://localhost:8000/\",\n            \"client_ip\": \"127.0.0.1\",\n            \"user_agent\": \"Go-http-client/1.1\",\n            \"x_forwarded_for\": true\n        },\n        \"response\": {\n            \"status\": 200\n        }\n    },\n    \"aws\": {\n        \"xray\": {\n            \"sdk_version\": \"1.1.0\",\n            \"sdk\": \"X-Ray for Go\"\n        },\n        \"eks\": {\n            \"cluster_name\": \"containerName\",\n            \"pod\": \"podname\",\n            \"container_id\": \"d8453812a556\"\n        }\n    },\n    \"service\": {\n        \"compiler_version\": \"go1.14.6\",\n        \"compiler\": \"gc\"\n    },\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Version Control Entry in Markdown\nDESCRIPTION: Changelog entry detailing multiple bug fixes, breaking changes, deprecations and new components for version 0.121.0 of the OpenTelemetry Collector Contrib project.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### 🧰 Bug fixes 🧰\n\n- `filelogreceiver`: Extend container parser log file path pattern to include rotated files. (#35137)\n- `opampsupervisor`: Use OwnLogsConnSettings along with Metrics & Traces settings to evaluate if configuration changed on message. (#38409)\n- `awsfirehosereceiver`: Fix cwlogs encoding to not consider CONTROL_MESSAGE records invalid (#38433)\n- `cloudflarereceiver`: Add missing telemetry for Cloudflare receiver (#38447)\n- `mysqlreceiver`: Fixed issue where the system attempted to convert string value '0.0000' (stored as []uint8) to int64 type, which was causing an invalid syntax error. (#38276)\n- `attributesprocessor`: Validate metrics configuration parameters before processing (#36077)\n- `otlpjsonfilereceiver`: Fix nil pointer dereference due to empty token (#38289)\n- `awsfirehosereceiver`: Remove error log when gzip reader type assertion fails due to nil value (#38352)\n- `azuremonitorexporter`: Fix flushes on each single Span (#37214)\n- `servicegraphconnector`: Change the default value of `metrics_flush_interval` to 60s to avoid excessive metric data point generation with default settings.\n (#34843)\n- `githubreceiver`: Fixes a bug where Job Step Spans did not have the correct Parent SpanID. (#38647)\n- `syslogexporter`: Fixes handling of multiple structured data elements (#33300)\n  Previous version added all structured data within one bracket pair. According to the RFC each structured data element should have its own bracket pair.\n- `healthcheckv2extension`: Fix the deadlock in healthcheckv2 extension in case of an error in the healthcheckv2 Start function. (#38269)\n- `probabilisticsampler`: Logs priority sampling behavior applies only when the priority attribute is present. (#38468)\n- `pkg/ottl`: Fix limitation of map literals within slice literals not being handled correctly (#37405)\n\n## v0.121.0\n\n### 🛑 Breaking changes 🛑\n\n- `telemetrygen`: Update attribute of generated traces from `net.peer.ip` to `net.sock.peer.addr` to bring it in line with new semconv. (#38043)\n- `awss3exporter`: Replaced the `s3_partition` option with `s3_partition_format` to provide more flexibility to users. (#37915, #37503)\n  Users can provide custom file partitions using [strftime](https://www.man7.org/linux/man-pages/man3/strftime.3.html) formatting.\n  The default value of `year=%Y/month=%m/day=%d/hour=%H/minute=%M` matches the older pattern (with `s3_partition: minute`)\n  \n  If users do not provide a value for `s3_prefix`, the exporter will not create a `/` folder in the bucket.\n  \n- `elasticsearchexporter`: drop support for metrics for none, raw, and bodymap mapping modes (#37928)\n  Metrics support is in development, and was added for \"ecs\" and \"otel\" mapping modes.\n  Support was unintentionally added for the other mapping modes, defaulting to the same\n  behaviour as \"ecs\" mode. While metrics support is still in development, drop support\n  from these mapping modes and require users to use the intended mapping modes.\n  \n- `awscontainerinsightreceiver`: Remove high cardinality attribute `Timestamp` from metrics generated by `awscontainerinsightreceiver` (#35861)\n- `failoverconnector`: Refactors the failover logic and changes the retry mechanism to sample data points (#38064)\n- `processor/k8sattributes`: Move k8sattr.fieldExtractConfigRegex.disallow feature gate to stable (#25128)\n- `signalfxexporter`: Remove the deprecated configuration option `translation_rules` (#35332)\n  Please use processors to handle desired metric transformations instead. Find migration guidance in the\n  [translation rules migration guide](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/docs/translation_rules_migration_guide.md).\n  \n- `signaltometricsconnector`: `get` OTTL function is removed and expressions are now parsed using `ParseValueExpression` (#38098)\n\n### 🚩 Deprecations 🚩\n\n- `elasticsearchexporter`: Deprecate `batcher::min_size_items` and `batcher::max_size_items` in favor of `batcher::min_size` and `batcher::max_size`. (#38243)\n- `prometheusreceiver`: Deprecate metric start time adjustment in the prometheus receiver. It is being replaced by the metricstarttime processor. (#37186)\n  Start time adjustment is still enabled by default. To disable it, enable the | receiver.prometheusreceiver.RemoveStartTimeAdjustment feature gate.\n\n### 🚀 New components 🚀\n\n- `windowsservicereceiver`: Adding a wireframe for a new receiver: windowsservicereceiver (#31377)\n- `azureblobexporter`: Add new exporter for sending telemetry to Azure Storage Blob (#35717)\n- `kafkatopicsobserver`: Adding implementation and tests of the component's logic. (#37665)\n```\n\n----------------------------------------\n\nTITLE: Amazon ECS Container Metadata JSON Structure\nDESCRIPTION: This JSON object represents the metadata for a container running in Amazon ECS. It includes cluster information, container identifiers, task details, Docker information, networking configuration, and availability information.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/proxy/testdata/ecsmetadatafileInvalidArn.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"Cluster\": \"default\",\n    \"ContainerInstanceARN\": \"arn:aws:ecs:us-west-50:012345678910:container-instance/default/1f73d099-b914-411c-a9ff-81633b7741dd\",\n    \"TaskARN\": \"invalid arn\",\n    \"TaskDefinitionFamily\": \"console-sample-app-static\",\n    \"TaskDefinitionRevision\": \"1\",\n    \"ContainerID\": \"aec2557997f4eed9b280c2efd7afccdcedfda4ac399f7480cae870cfc7e163fd\",\n    \"ContainerName\": \"simple-app\",\n    \"DockerContainerName\": \"/ecs-console-sample-app-static-1-simple-app-e4e8e495e8baa5de1a00\",\n    \"ImageID\": \"sha256:2ae34abc2ed0a22e280d17e13f9c01aaf725688b09b7a1525d1a2750e2c0d1de\",\n    \"ImageName\": \"httpd:2.4\",\n    \"PortMappings\": [\n        {\n            \"ContainerPort\": 80,\n            \"HostPort\": 80,\n            \"BindIp\": \"0.0.0.0\",\n            \"Protocol\": \"tcp\"\n        }\n    ],\n    \"Networks\": [\n        {\n            \"NetworkMode\": \"bridge\",\n            \"IPv4Addresses\": [\n                \"192.0.2.0\"\n            ]\n        }\n    ],\n    \"MetadataFileStatus\": \"READY\",\n    \"AvailabilityZone\": \"us-east-1b\",\n    \"HostPrivateIPv4Address\": \"192.0.2.0\",\n    \"HostPublicIPv4Address\": \"203.0.113.0\"\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Go Models from Protobuf Definitions for Solace Receiver\nDESCRIPTION: Commands for generating Go code from protobuf definitions for the Solace receiver. This requires the protoc-gen-go package to be installed. The commands generate Go models for receive_v1, egress_v1, and move_v1 protos, and then formats the code using goimports.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/solacereceiver/internal/model/README.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nprotoc --go_out=../ --go_opt=paths=import --go_opt=Mreceive_v1.proto=model/receive/v1 receive_v1.proto\nprotoc --go_out=../ --go_opt=paths=import --go_opt=Megress_v1.proto=model/egress/v1 egress_v1.proto\nprotoc --go_out=../ --go_opt=paths=import --go_opt=Mmove_v1.proto=model/move/v1 move_v1.proto\ngoimports -w .\n```\n\n----------------------------------------\n\nTITLE: Metrics Enumeration Values\nDESCRIPTION: Defines the enumeration values used in metrics processing, including flags, aggregation temporality, and metric data types. These values are aligned with the metrics proto definitions and pdata implementations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/contexts/ottldatapoint/README.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Enum Symbol | Value |\n|----------------------------------------|-------|\n| FLAG_NONE | 0 |\n| FLAG_NO_RECORDED_VALUE | 1 |\n| AGGREGATION_TEMPORALITY_UNSPECIFIED | 0 |\n| AGGREGATION_TEMPORALITY_DELTA | 1 |\n| AGGREGATION_TEMPORALITY_CUMULATIVE | 2 |\n| METRIC_DATA_TYPE_NONE | 0 |\n| METRIC_DATA_TYPE_GAUGE | 1 |\n| METRIC_DATA_TYPE_SUM | 2 |\n| METRIC_DATA_TYPE_HISTOGRAM | 3 |\n| METRIC_DATA_TYPE_EXPONENTIAL_HISTOGRAM | 4 |\n| METRIC_DATA_TYPE_SUMMARY | 5 |\n```\n\n----------------------------------------\n\nTITLE: Disabling Default PostgreSQL Metrics Configuration\nDESCRIPTION: YAML configuration snippet showing how to disable specific default metrics in the PostgreSQL metrics collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/postgresqlreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Displaying Request Count Metrics Example\nDESCRIPTION: Example of request count metrics generated by the span metrics connector, showing dimensions like service name, span name, span kind, and status code.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/spanmetricsconnector/README.md#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntraces.span.metrics.calls{service.name=\"shipping\",span.name=\"get_shipping/{shippingId}\",span.kind=\"SERVER\",status.code=\"Ok\"}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Helm Chart AWS Credentials Configuration\nDESCRIPTION: Example of AWS credential configuration using environment variables in OpenTelemetry Collector Helm Chart.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awss3exporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextraEnvs:\n- name: AWS_ACCESS_KEY_ID\n  value: \"< YOUR AWS ACCESS KEY >\"\n- name: AWS_SECRET_ACCESS_KEY\n  value: \"< YOUR AWS SECRET ACCESS KEY >\"\n```\n\n----------------------------------------\n\nTITLE: Documentation Links in Markdown\nDESCRIPTION: Reference to operators documentation and operator sequences documentation\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/README.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[operators](./docs/operators/README.md)\n[operator sequences](./docs/types/operators.md)\n```\n\n----------------------------------------\n\nTITLE: Configuring vCenter Metrics in YAML\nDESCRIPTION: YAML configuration example showing how to disable specific metrics in the vCenter integration. This configuration can be applied to any metric listed in the documentation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/vcenterreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: HTTP Request Log Sample in JSON Format\nDESCRIPTION: A collection of JSON objects representing HTTP request logs with fields for client information, request details, response data, and timestamps. These logs appear to be from a web service, possibly Cloudflare, and contain information about client requests to various domains in the 'theburritobot' family.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/cloudflarereceiver/testdata/sample-payloads/multiple_log_payload.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"ClientIP\": \"89.163.253.200\", \"ClientRequestHost\": \"www.theburritobot0.com\", \"ClientRequestMethod\": \"GET\", \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\", \"EdgeEndTimestamp\": \"2023-03-03T05:30:05Z\", \"EdgeResponseBytes\": 69045, \"EdgeResponseStatus\": 200, \"EdgeStartTimestamp\": \"2023-03-03T05:29:05Z\", \"RayID\": \"3a6050bcbe121a87\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"ClientIP\": \"89.163.253.201\", \"ClientRequestHost\": \"www.theburritobot1.com\", \"ClientRequestMethod\": \"GET\", \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\", \"EdgeEndTimestamp\": \"2023-03-03T05:30:05Z\", \"EdgeResponseBytes\": 69045, \"EdgeResponseStatus\": 200, \"EdgeStartTimestamp\": \"2023-03-03T05:29:05Z\", \"RayID\": \"3a6050bcbe121a87\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"ClientIP\": \"89.163.253.202\", \"ClientRequestHost\": \"www.theburritobot2.com\", \"ClientRequestMethod\": \"GET\", \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\", \"EdgeEndTimestamp\": \"2023-03-03T05:30:05Z\", \"EdgeResponseBytes\": 69045, \"EdgeResponseStatus\": 200, \"EdgeStartTimestamp\": \"2023-03-03T05:29:05Z\", \"RayID\": \"3a6050bcbe121a87\", \"ZoneName\": \"example.com\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"ClientIP\": \"89.163.253.203\", \"ClientRequestHost\": \"www.theburritobot3.com\", \"ClientRequestMethod\": \"GET\", \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\", \"EdgeEndTimestamp\": \"2023-03-03T05:30:05Z\", \"EdgeResponseBytes\": 69045, \"EdgeResponseStatus\": 200, \"EdgeStartTimestamp\": \"2023-03-03T05:29:05Z\", \"RayID\": \"3a6050bcbe121a87\", \"ZoneName\": \"abc.com\" }\n```\n\nLANGUAGE: json\nCODE:\n```\n{ \"ClientIP\": \"89.163.253.204\", \"ClientRequestHost\": \"www.theburritobot4.com\", \"ClientRequestMethod\": \"GET\", \"ClientRequestURI\": \"/static/img/testimonial-hipster.png\", \"EdgeEndTimestamp\": \"2023-03-03T05:30:05Z\", \"EdgeResponseBytes\": 69045, \"EdgeResponseStatus\": 200, \"EdgeStartTimestamp\": \"2023-03-03T05:29:05Z\", \"RayID\": \"3a6050bcbe121a87\", \"ZoneName\": \"example.com\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Remote Write Exporter for Logzio in YAML\nDESCRIPTION: This snippet demonstrates how to configure the Prometheus remote write exporter for sending metrics to Logzio. It includes the endpoint and authorization header setup.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/logzioexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  prometheusremotewrite:\n    endpoint: \"https://listener.logz.io:8053\"\n    headers:\n      Authorization: \"Bearer LOGZIOprometheusTOKEN\"\n```\n\n----------------------------------------\n\nTITLE: Load Balancing Telemetry Metrics Definition in Markdown\nDESCRIPTION: Structured documentation of internal telemetry metrics for the load balancing component, including latency, outcome tracking, backend updates, backend count, and resolution events. Each metric is defined with its unit, type, and value characteristics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[comment]: <> (Code generated by mdatagen. DO NOT EDIT.)\n\n# loadbalancing\n\n## Internal Telemetry\n\nThe following telemetry is emitted by this component.\n\n### otelcol_loadbalancer_backend_latency\n\nResponse latency in ms for the backends.\n\n| Unit | Metric Type | Value Type |\n| ---- | ----------- | ---------- |\n| ms | Histogram | Int |\n\n### otelcol_loadbalancer_backend_outcome\n\nNumber of successes and failures for each endpoint.\n\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| {outcomes} | Sum | Int | true |\n\n### otelcol_loadbalancer_num_backend_updates\n\nNumber of times the list of backends was updated.\n\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| {updates} | Sum | Int | true |\n\n### otelcol_loadbalancer_num_backends\n\nCurrent number of backends in use.\n\n| Unit | Metric Type | Value Type |\n| ---- | ----------- | ---------- |\n| {backends} | Gauge | Int |\n\n### otelcol_loadbalancer_num_resolutions\n\nNumber of times the resolver has triggered new resolutions.\n\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| {resolutions} | Sum | Int | true |\n```\n\n----------------------------------------\n\nTITLE: OTTL List Examples\nDESCRIPTION: Examples of valid OTTL List syntax for creating sequences of values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/LANGUAGE.md#2025-04-10_snippet_2\n\nLANGUAGE: OTTL\nCODE:\n```\n[]\n[1]\n[\"1\", \"2\", \"3\"]\n[\"a\", attributes[\"key\"], Concat([\"a\", \"b\"], \"-\")]\n```\n\n----------------------------------------\n\nTITLE: Hexadecimal Conversion Examples\nDESCRIPTION: Examples of using Hex converter to transform different data types into their hexadecimal representation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_26\n\nLANGUAGE: Go\nCODE:\n```\nHex(span.attributes[\"http.status_code\"])\nHex(2.0)\n```\n\n----------------------------------------\n\nTITLE: daemonset-stdout Mode Architecture\nDESCRIPTION: Diagram showing the architecture of the daemonset-stdout mode, which collects log data from container stdout by getting pod metadata from Kubernetes API and log paths from Docker/CRI-containerd API.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/design.md#2025-04-10_snippet_1\n\nLANGUAGE: ascii\nCODE:\n```\n k8sapi     ┌──────────────┐    docker/cri-containerd\n   │        │              │             │\n   └───────▶│    Source    │◀────────────┘\n  medatada  │              │ logPath, env\n            └──────────────┘\n                  │ assosiate\n                  ▼\n            ┌──────────────┐\n            │   Reader     │\n            └──────────────┘\n                  │ read files from logPath\n                  ▼\n                files\n```\n\n----------------------------------------\n\nTITLE: Input JSON Entry for Removing All Attributes\nDESCRIPTION: This shows a sample JSON input entry with multiple attributes that will all be removed by the remove operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {  },\n  \"attributes\": {\n    \"key1.0\": \"val\",\n    \"key2.0\": \"val\"\n  },\n  \"body\": {\n    \"key\": \"val\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Simple URL Parser Result Example in YAML\nDESCRIPTION: Example output from the URL parser function when parsing a simple URL. Shows the minimal fields that are always present: original, scheme, domain, and path.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_73\n\nLANGUAGE: yaml\nCODE:\n```\n\"url.original\": \"http://www.example.com\",\n\"url.scheme\":   \"http\",\n\"url.domain\":   \"www.example.com\",\n\"url.path\":     \"\",\n```\n\n----------------------------------------\n\nTITLE: Parsing SQL Server Performance Metrics in JSON Format\nDESCRIPTION: This code snippet represents a collection of SQL Server performance metrics in JSON format. It includes various counters for different database instances, measuring aspects like data file size, log file usage, transaction rates, and other performance indicators.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"counter\":\"Log Bytes Flushed/sec\",\n  \"counter_type\":\"272696576\",\n  \"instance\":\"master\",\n  \"measurement\":\"sqlserver_performance\",\n  \"object\":\"SQLServer:Databases\",\n  \"sql_instance\":\"8cac97ac9b8f\",\n  \"computer_name\":\"abcde\",\n  \"value\":\"5.873664e+06\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running OpenTelemetry Collector via Docker\nDESCRIPTION: Command to start an OpenTelemetry Collector instance using Docker, mapping ports and mounting a configuration file.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/telemetrygen/README.md#2025-04-10_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ndocker run -p 4317:4317 -v $(pwd)/config.yaml:/etc/otelcol-contrib/config.yaml ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.86.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Collection for OpenTelemetry Collector\nDESCRIPTION: This YAML snippet demonstrates how to enable metrics collection in the OpenTelemetry Collector. It includes the Prometheus receiver configuration and updates the service pipeline to include metrics collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datasetexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: 'otel-collector'\n          scrape_interval: 5s\n          static_configs:\n            - targets: ['0.0.0.0:8888']\n...\nservice:\n  pipelines:\n    metrics:\n      # add prometheus among metrics receivers\n      receivers: [prometheus]\n      processors: [batch]\n      exporters: [otlphttp/prometheus, debug]\n```\n\n----------------------------------------\n\nTITLE: Input/Output Example for Attribute Unquoting in JSON\nDESCRIPTION: Shows an example of input and output entries when unquoting an attribute field. The input contains a back-quoted attribute value that gets unquoted in the output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/unquote.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n    \"foo\": \"`bar`\"\n  },\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": {\n    \"foo\": \"bar\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Disabling in YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable specific metrics in the BigIP metrics collection. It allows for granular control over which metrics are emitted.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/bigipreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Journald Input with Matches in YAML\nDESCRIPTION: Example YAML configuration for Journald input using matches to filter log entries based on specific criteria.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/journaldreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n  matches:\n    - _SYSTEMD_UNIT: ssh\n    - _SYSTEMD_UNIT: kubelet\n      _UID: \"1000\"\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Metrics Collection Interval in YAML\nDESCRIPTION: Configuration example showing how to set a custom collection interval of 40 seconds for the AWS ECS Container Metrics Receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsecscontainermetricsreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awsecscontainermetrics:\n      collection_interval: 40s\nexporters:\n  awsemf:\n      namespace: 'ECS/ContainerMetrics/OpenTelemetry'\n      log_group_name: '/ecs/containermetrics/opentelemetry'\n\nservice:\n  pipelines:\n      metrics:\n          receivers: [awsecscontainermetrics]\n          exporters: [awsemf]\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Removing All Attributes\nDESCRIPTION: This shows the resulting JSON entry after the remove operator has removed all attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key\": \"val\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: S3 Configuration URI Format Example\nDESCRIPTION: Demonstrates the expected URI format for accessing S3 configuration files. The URI must include the bucket name, region, and object key path.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/confmap/provider/s3provider/README.md#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ns3://[BUCKET].s3.[REGION].amazonaws.com/[KEY]\n```\n\n----------------------------------------\n\nTITLE: HAProxy Statistics CSV Output Sample\nDESCRIPTION: This CSV sample shows HAProxy statistics with columns for proxies, servers, connections, request counts, error counts, health status, and many other metrics. The data includes frontend and backend server information with performance statistics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/haproxyreceiver/testdata/30252_stats.txt#2025-04-10_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\n# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nstats,FRONTEND,,,0,1,524268,2,1444,47008,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,0,0,1,,,,0,2,0,0,0,0,,0,1,2,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,2,2,0,0,0,,,0,0,,,,,,,0,,,,,,,,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1594,47052,0,0,\nmyfrontend,FRONTEND,,,1,1,524268,1,85470,107711,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,134,0,0,0,0,,0,11,134,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,1,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,134,94712,107309,0,0,\nwebservers,s1,0,0,0,1,,45,28734,36204,,0,,0,0,0,0,UP,1,1,0,0,0,159,0,,1,4,1,,45,,2,0,,4,L4OK,,0,0,45,0,0,0,0,,,,45,0,0,,,,,3,,,0,1,4,95,,,,Layer4 check passed,,2,3,4,,,,192.168.16.2:8080,,http,,,,,,,,0,1,44,,,1,,0,1,26,184,0,0,1,0,1,1,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,\nwebservers,s,,,,,,,,,,,,,,,,UP,,,,,,,,,,,,,,,,,,,LOK,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Layer check passed,,,,,,,,...,,http,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,,,,,,,\nwebservers,s3,0,0,0,1,,44,28072,35376,,0,,0,0,0,0,UP,1,1,0,0,0,159,0,,1,4,3,,44,,2,0,,4,L4OK,,0,0,44,0,0,0,0,,,,44,0,0,,,,,4,,,0,1,4,121,,,,Layer4 check passed,,2,3,4,,,,192.168.16.4:8080,,http,,,,,,,,0,1,43,,,1,,0,3,25,1331,0,0,1,0,1,1,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,\nwebservers,BACKEND,0,0,0,1,52427,134,85470,107711,0,0,,0,0,0,0,UP,3,3,0,,0,159,0,,1,4,0,,134,,1,0,,11,,,,0,134,0,0,0,0,,,,134,0,0,0,0,0,0,3,,,0,1,4,105,,,,,,,,,,,,,,http,roundrobin,,,,,,,0,3,131,0,0,,,0,3,26,1331,0,,,,,3,0,0,0,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0,3,134,107309,91496,0,0,\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry Trace Attribute Constraints\nDESCRIPTION: A set of constraint rules defining valid combinations of trace attributes. The rules specify which attribute types are allowed based on other attribute values, particularly focusing on span kind relationships with attribute types.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/goldendataset/testdata/pict_input_spans.txt#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nIF [Parent] = \"Root\" THEN [Kind] in {\"Server\", \"Producer\"};\nIF [Kind] = \"Internal\" THEN [Attributes] in {\"Nil\", \"Internal\"};\nIF [Kind] = \"Server\" THEN [Attributes] in {\"Nil\", \"FaaSHTTP\", \"FaaSTimer\", \"FaaSOther\", \"HTTPServer\", \"gRPCServer\", \"MaxCount\"};\nIF [Kind] = \"Client\" THEN [Attributes] in {\"Empty\", \"DatabaseSQL\", \"DatabaseNoSQL\", \"HTTPClient\", \"gRPCClient\"};\nIF [Kind] = \"Producer\" THEN [Attributes] in {\"Empty\", \"MessagingProducer\", \"FaaSPubSub\"};\nIF [Kind] = \"Consumer\" THEN [Attributes] in {\"Nil\", \"MessagingConsumer\", \"FaaSDatasource\"};\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Kinesis Region\nDESCRIPTION: Bug fix for applying region configuration to Kinesis exporter\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_37\n\nLANGUAGE: yaml\nCODE:\n```\nawskinesis: Fixed applying region to the kinesis exporter\n```\n\n----------------------------------------\n\nTITLE: OTTL Function Return Value Handling\nDESCRIPTION: Code showing handling of trace and span ID string functions, returning \"000..000\" for invalid IDs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_39\n\nLANGUAGE: go\nCODE:\n```\n\"[trace|span]_id_string\" func returns \"000..000\" string for invalid ids.\n```\n\n----------------------------------------\n\nTITLE: Defining Go Module Dependencies for AWS Container Insight Receiver\nDESCRIPTION: This go.mod file defines the module configuration for the AWS Container Insight receiver component. It specifies the Go version requirement (1.19) and lists all direct and indirect dependencies needed for the receiver to function properly within the OpenTelemetry Collector ecosystem.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/oauth2clientauthextension/testdata/test-cred-empty.txt#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nmodule github.com/open-telemetry/opentelemetry-collector-contrib/receiver/awscontainerinsightreceiver\n\ngo 1.19\n\nrequire (\n\tgithub.com/aws/aws-sdk-go v1.44.329\n\tgithub.com/google/cadvisor v0.47.1\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/internal/aws/awsutil v0.83.0\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/internal/aws/containerinsight v0.83.0\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/internal/aws/k8s v0.83.0\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/internal/aws/metrics v0.83.0\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/internal/k8sconfig v0.83.0\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/internal/kubelet v0.83.0\n\tgithub.com/shirou/gopsutil/v3 v3.23.7\n\tgithub.com/stretchr/testify v1.8.4\n\tgo.opentelemetry.io/collector/component v0.83.0\n\tgo.opentelemetry.io/collector/confmap v0.83.0\n\tgo.opentelemetry.io/collector/consumer v0.83.0\n\tgo.opentelemetry.io/collector/pdata v1.0.0-rcv0014\n\tgo.opentelemetry.io/collector/receiver v0.83.0\n\tgo.uber.org/multierr v1.11.0\n\tgo.uber.org/zap v1.25.0\n\tk8s.io/api v0.28.1\n\tk8s.io/apimachinery v0.28.1\n\tk8s.io/client-go v0.28.1\n\tk8s.io/klog/v2 v2.100.1\n)\n\nrequire (\n\tgithub.com/Microsoft/go-winio v0.6.1 // indirect\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/blang/semver/v4 v4.0.0 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/docker/docker v24.0.5+incompatible // indirect\n\tgithub.com/docker/go-units v0.5.0 // indirect\n\tgithub.com/emicklei/go-restful/v3 v3.10.2 // indirect\n\tgithub.com/euank/go-kmsg-parser v2.0.0+incompatible // indirect\n\tgithub.com/go-logr/logr v1.2.4 // indirect\n\tgithub.com/go-openapi/jsonpointer v0.20.0 // indirect\n\tgithub.com/go-openapi/jsonreference v0.20.2 // indirect\n\tgithub.com/go-openapi/swag v0.22.4 // indirect\n\tgithub.com/go-ole/go-ole v1.2.6 // indirect\n\tgithub.com/gogo/protobuf v1.3.2 // indirect\n\tgithub.com/golang/protobuf v1.5.3 // indirect\n\tgithub.com/google/gnostic-models v0.6.8 // indirect\n\tgithub.com/google/go-cmp v0.5.9 // indirect\n\tgithub.com/google/gofuzz v1.2.0 // indirect\n\tgithub.com/google/uuid v1.3.1 // indirect\n\tgithub.com/jmespath/go-jmespath v0.4.0 // indirect\n\tgithub.com/josharian/intern v1.0.0 // indirect\n\tgithub.com/json-iterator/go v1.1.12 // indirect\n\tgithub.com/karrick/godirwalk v1.17.0 // indirect\n\tgithub.com/knadh/koanf v1.5.0 // indirect\n\tgithub.com/knadh/koanf/v2 v2.0.1 // indirect\n\tgithub.com/lufia/plan9stats v0.0.0-20230326075908-cb1d2100619a // indirect\n\tgithub.com/mailru/easyjson v0.7.7 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions v1.0.4 // indirect\n\tgithub.com/mindprince/gonvml v0.0.0-20190828220739-9ebdce4bb989 // indirect\n\tgithub.com/mistifyio/go-zfs v2.1.2-0.20190413222219-f784269be439+incompatible // indirect\n\tgithub.com/mitchellh/copystructure v1.2.0 // indirect\n\tgithub.com/mitchellh/mapstructure v1.5.1-0.20220423185008-bf980b35cac4 // indirect\n\tgithub.com/mitchellh/reflectwalk v1.0.2 // indirect\n\tgithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect\n\tgithub.com/modern-go/reflect2 v1.0.2 // indirect\n\tgithub.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect\n\tgithub.com/opencontainers/go-digest v1.0.0 // indirect\n\tgithub.com/opencontainers/runc v1.1.9 // indirect\n\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n\tgithub.com/power-devops/perfstat v0.0.0-20221212215047-62379fc7944b // indirect\n\tgithub.com/prometheus/client_golang v1.16.0 // indirect\n\tgithub.com/prometheus/client_model v0.4.0 // indirect\n\tgithub.com/prometheus/common v0.44.0 // indirect\n\tgithub.com/prometheus/procfs v0.10.1 // indirect\n\tgithub.com/shoenig/go-m1cpu v0.1.6 // indirect\n\tgithub.com/spf13/pflag v1.0.5 // indirect\n\tgithub.com/tklauser/go-sysconf v0.3.12 // indirect\n\tgithub.com/tklauser/numcpus v0.6.1 // indirect\n\tgithub.com/yusufpapurcu/wmi v1.2.3 // indirect\n\tgo.opentelemetry.io/collector/config/configtelemetry v0.83.0 // indirect\n\tgo.opentelemetry.io/collector/featuregate v1.0.0-rcv0014 // indirect\n\tgo.opentelemetry.io/otel v1.16.0 // indirect\n\tgo.opentelemetry.io/otel/metric v1.16.0 // indirect\n\tgo.opentelemetry.io/otel/trace v1.16.0 // indirect\n\tgolang.org/x/exp v0.0.0-20230711023510-fffb14384f22 // indirect\n\tgolang.org/x/mod v0.12.0 // indirect\n\tgolang.org/x/net v0.14.0 // indirect\n\tgolang.org/x/oauth2 v0.11.0 // indirect\n\tgolang.org/x/sys v0.11.0 // indirect\n\tgolang.org/x/term v0.11.0 // indirect\n\tgolang.org/x/text v0.12.0 // indirect\n\tgolang.org/x/time v0.3.0 // indirect\n\tgolang.org/x/tools v0.12.0 // indirect\n\tgoogle.golang.org/appengine v1.6.7 // indirect\n\tgoogle.golang.org/protobuf v1.31.0 // indirect\n\tgopkg.in/inf.v0 v0.9.1 // indirect\n\tgopkg.in/yaml.v2 v2.4.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n\tk8s.io/kube-openapi v0.0.0-20230717233707-2695361300d9 // indirect\n\tk8s.io/utils v0.0.0-20230711102312-30195339c3c7 // indirect\n\tsigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect\n\tsigs.k8s.io/structured-merge-diff/v4 v4.3.0 // indirect\n\tsigs.k8s.io/yaml v1.3.0 // indirect\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Zipkin Receiver Configuration in YAML\nDESCRIPTION: Minimal configuration required to enable the Zipkin receiver in OpenTelemetry Collector. The receiver listens on localhost:9411 by default and can be configured for string tag parsing and custom endpoints.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/zipkinreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  zipkin:\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Server Performance Metrics in JSON Format\nDESCRIPTION: JSON data structure containing SQL Server performance metrics collected from various database instances. Each entry includes counter name, type, instance name, measurement type, object name, SQL instance identifier, computer name, and the corresponding metric value.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"1784\"\n   },\n   {\n      \"counter\":\"Log File(s) Used Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"774\"\n   },\n   {\n      \"counter\":\"Log Flush Wait Time\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"138\"\n   },\n   {\n      \"counter\":\"Log Flushes/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"87\"\n   },\n   {\n      \"counter\":\"Log Growths\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"5\"\n   },\n   {\n      \"counter\":\"Percent Log Used\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"43\"\n   },\n   {\n      \"counter\":\"Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"1452\"\n   },\n   {\n      \"counter\":\"Write Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"model_replicatedmaster\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"36\"\n   },\n   {\n      \"counter\":\"Write Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"33\"\n   },\n   {\n      \"counter\":\"Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"4614\"\n   },\n   {\n      \"counter\":\"Percent Log Used\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"53\"\n   },\n   {\n      \"counter\":\"Log Growths\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"3\"\n   },\n   {\n      \"counter\":\"Log Flushes/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"76\"\n   },\n   {\n      \"counter\":\"Log Flush Wait Time\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"118\"\n   },\n   {\n      \"counter\":\"Log File(s) Used Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"678\"\n   },\n   {\n      \"counter\":\"Log File(s) Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"1272\"\n   },\n   {\n      \"counter\":\"XTP Memory Used (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Log Bytes Flushed/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"708608\"\n   },\n   {\n      \"counter\":\"Data File(s) Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"15680\"\n   },\n   {\n      \"counter\":\"Backup/Restore Throughput/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Active Transactions\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"msdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Active Transactions\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Backup/Restore Throughput/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Data File(s) Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"40960\"\n   },\n   {\n      \"counter\":\"Log Bytes Flushed/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"XTP Memory Used (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Log File(s) Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"1272\"\n   },\n   {\n      \"counter\":\"Log File(s) Used Size (KB)\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"648\"\n   },\n   {\n      \"counter\":\"Log Flush Wait Time\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Log Flushes/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Log Growths\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Percent Log Used\",\n      \"counter_type\":\"65792\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"50\"\n   },\n   {\n      \"counter\":\"Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"5\"\n   },\n   {\n      \"counter\":\"Write Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"mssqlsystemresource\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   },\n   {\n      \"counter\":\"Write Transactions/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"tempdb\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Databases\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"244\"\n   }\n```\n\n----------------------------------------\n\nTITLE: Azure Monitor Resource Attributes Comment\nDESCRIPTION: Generated code comment indicating the file is auto-generated and should not be edited manually.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azuremonitorreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[comment]: <> (Code generated by mdatagen. DO NOT EDIT.)\n```\n\n----------------------------------------\n\nTITLE: Non-Portable File Path Example\nDESCRIPTION: Example showing incorrect hard-coded Windows file path that should be avoided for cross-platform compatibility.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfilePath := \"C:\\\\Users\\\\Bob\\\\Documents\\\\sampleData.csv\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Issue Lifecycle with Mermaid Diagram\nDESCRIPTION: A state diagram that illustrates the flow of issues through different stages in the triage process. The diagram shows how issues move from initial creation through triage, waiting states, assignment, and eventual resolution or closure. It includes decision points, waiting states, and automatic processes like stale issue handling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/issue-triaging.md#2025-04-10_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    n0([\"New issue has been opened\"]) --> n1\n    n1([\"Needs Triage\"]) --> n2[\"Has good repro steps <br>and/or description?\"]\n  subgraph graph2[\"**waiting-for-codeowners**\"]\n        n3[\"Waiting for Codeowners<br>to further validate the issue\"]\n  end\n  subgraph graph3[\"**waiting-for-author**\"]\n        n4[\"Waiting for author to provide more details\"]\n  end\n  subgraph graph4[\"**help-wanted**\"]\n        n8[\"Waiting on community\"]\n  end\n  subgraph graph5[\"**closed**\"]\n        n10([\"Close the issue and provide details as needed\"])\n  end\n    n2 -- Yes --> n3\n    n2 -- No/Need more details --> n4\n    n2 -- Invalid configuration/alternative available --> n10\n    n3 -- Invalid Issue --> n10\n    n3 -- Valid Issue -->  n6[\"Codeowner has time<br>to fix it?\"]\n    n6 -- Assign it to codeowner --> n7[\"Issue in being worked upon\"]\n    n6 -- No --> n8\n    n7 -- Once PR is merged --> n10\n    n8 -- When someone volunteers to provide a fix --> n11[\"Assign it to the person\"]\n    n12 -- Any activity on the issue --> n8\n    n8 -. Issue becomes stale due to lack of activity .-> n12[\"Issue is inactive\"]\n    n11 --> n7\n    n12 -- Closed automatically after 120 days due to lack of activity --> n10\n    n4 -- Once enough details are available --> n2\n\n    n3@{ shape: rect}\n    n4@{ shape: rect}\n    n2@{ shape: diam}\n    n6@{ shape: diam}\n\n     n1:::Aqua\n     n3:::Ash\n     n4:::Ash\n     n8:::Ash\n     n2:::Ash\n     n2:::Peach\n     n6:::Peach\n     n7:::Ash\n     n10:::Rose\n    classDef Rose stroke-width:1px, stroke-dasharray:none, stroke:#FF5978, fill:#FFDFE5, color:#8E2236\n    classDef Aqua stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A\n    classDef Peach stroke-width:1px, stroke-dasharray:none, stroke:#FBB35A, fill:#FFEFDB, color:#8F632D\n    classDef Ash stroke-width:1px, stroke-dasharray:none, stroke:#999999, fill:#EEEEEE, color:#000000\n```\n\n----------------------------------------\n\nTITLE: Token-Based Log Search in ClickHouse\nDESCRIPTION: SQL query to find logs containing specific tokens in the body\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE hasToken(Body, 'http')\n  AND TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional SSH/SFTP Metrics in YAML\nDESCRIPTION: YAML configuration template for enabling optional metrics that are not enabled by default in the SSH check collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sshcheckreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Referencing AWS Container Insights Receiver in Go\nDESCRIPTION: This snippet shows how to reference the AWS Container Insights Receiver package in Go code. It collects data from cadvisor and the Kubernetes API server, and is designed to run as a DaemonSet.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscontainerinsightreceiver/design.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nawscontainerinsightreceiver\n```\n\n----------------------------------------\n\nTITLE: Configuring Carbon Exporter in YAML\nDESCRIPTION: Example configuration for the Carbon exporter showing basic and detailed settings including endpoint specification and timeout configuration. The exporter supports both default configuration (localhost:2003) and custom settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/carbonexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  carbon:\n    # by default it will export to localhost:2003 using tcp\n  carbon/allsettings:\n    # use endpoint to specify alternative destinations for the exporter,\n    # the default is localhost:2003\n    endpoint: localhost:8080\n    # timeout is the maximum duration allowed to connecting and sending the\n    # data to the configured endpoint.\n    # The default is 5 seconds.\n    timeout: 10s\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRoleBinding\nDESCRIPTION: Kubernetes ClusterRoleBinding to associate the ClusterRole with the ServiceAccount.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8seventsreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: otelcontribcol\nsubjects:\n- kind: ServiceAccount\n  name: otelcontribcol\n  namespace: default\nEOF\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Resource Instrumentation Configuration Matrix\nDESCRIPTION: A tabular representation showing the relationship between different resource types (VMOnPrem, K8sCloud, FaaS, etc.), their instrumentation library states (None, One, Two), and span configurations (None, One, Several, All). This matrix helps in understanding the telemetry coverage across different infrastructure deployments.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/goldendataset/testdata/generated_pict_pairs_traces.txt#2025-04-10_snippet_0\n\nLANGUAGE: table\nCODE:\n```\nResource\tInstrumentationLibrary\tSpans\nVMOnPrem\tNone\tNone\nNil\tOne\tNone\nExec\tOne\tSeveral\nExec\tNone\tAll\nNil\tTwo\tOne\nEmpty\tTwo\tSeveral\nVMCloud\tTwo\tAll\nK8sOnPrem\tNone\tOne\nEmpty\tTwo\tNone\nNil\tNone\tSeveral\nK8sOnPrem\tOne\tNone\nK8sCloud\tOne\tAll\nVMCloud\tOne\tOne\nNil\tNone\tAll\nK8sOnPrem\tTwo\tSeveral\nK8sCloud\tTwo\tOne\nExec\tTwo\tNone\nVMOnPrem\tTwo\tOne\nK8sCloud\tNone\tNone\nFaas\tOne\tNone\nFaas\tTwo\tSeveral\nExec\tOne\tOne\nVMCloud\tNone\tSeveral\nFaas\tNone\tAll\nEmpty\tOne\tOne\nK8sCloud\tNone\tSeveral\nVMOnPrem\tOne\tAll\nVMOnPrem\tOne\tSeveral\nK8sOnPrem\tTwo\tAll\nVMCloud\tTwo\tNone\nEmpty\tNone\tAll\nFaas\tOne\tOne\n```\n\n----------------------------------------\n\nTITLE: Input JSON Entry for Removing a Value from Body\nDESCRIPTION: This shows a sample JSON input entry with a key in the body that will be removed by the remove operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Collector Configuration\nDESCRIPTION: Collector configuration for metrics and logs receivers using Kubernetes observer and receiver creator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  receiver_creator/metrics:\n    watch_observers: [ k8s_observer ]\n    discovery:\n      enabled: true\n    receivers:\n  \n  receiver_creator/logs:\n    watch_observers: [ k8s_observer ]\n    discovery:\n      enabled: true\n    receivers:\n\nservice:\n  extensions: [ k8s_observer]\n  pipelines:\n    metrics:\n      receivers: [ receiver_creator/metrics ]\n      processors: []\n      exporters: [ debug ]\n    logs:\n      receivers: [ receiver_creator/logs ]\n      processors: []\n      exporters: [ debug ]\n```\n\n----------------------------------------\n\nTITLE: Converting Binary IDs to Signed Int64 in Go\nDESCRIPTION: This Go code demonstrates how to convert byte array IDs to unsigned int64 and then to signed int64 using Big Endian byte order, which is required when transforming OpenTelemetry trace and span IDs to Jaeger's Thrift format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/translator/jaeger/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nvar (\n    id       []byte = []byte{0xFF, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00}\n    unsigned uint64 = binary.BigEndian.Uint64(id)\n    signed   int64  = int64(unsigned)\n)\nfmt.Println(\"unsigned:\", unsigned)\nfmt.Println(\"  signed:\", signed)\n// Output:\n// unsigned: 18374686479671623680\n//   signed: -72057594037927936\n```\n\n----------------------------------------\n\nTITLE: Retrieving SQL Server Query Performance Metrics with Plan and Text Details\nDESCRIPTION: This SQL query extracts performance statistics for top N queries from SQL Server's dynamic management views. It first collects aggregated metrics in a CTE (Common Table Expression) and then joins with other system views to retrieve the query text and execution plans. The query filters by a configurable lookback period and optional instance name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/databaseTopQueryWithInstanceName.txt#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nwith qstats as (\n\tSELECT TOP(@topNValue)\n\t\tREPLACE(@@SERVERNAME,'\\\\',':') AS [sql_instance],\n\t\tHOST_NAME() AS [computer_name],\n\t\tMAX(qs.plan_handle) AS query_plan_handle,\n\t\tqs.query_hash AS query_hash,\n\t\tqs.query_plan_hash AS query_plan_hash,\n\t\tSUM(qs.execution_count) AS execution_count,\n\t\tSUM(qs.total_elapsed_time) AS total_elapsed_time,\n\t\tSUM(qs.total_worker_time) AS total_worker_time,\n\t\tSUM(qs.total_logical_reads) AS total_logical_reads,\n\t\tSUM(qs.total_physical_reads) AS total_physical_reads,\n\t\tSUM(qs.total_logical_writes) AS total_logical_writes,\n\t\tSUM(qs.total_rows) AS total_rows,\n\t\tSUM(qs.total_grant_kb) as total_grant_kb\n\tFROM sys.dm_exec_query_stats AS qs\n\tWHERE qs.last_execution_time BETWEEN DATEADD(SECOND, @lookbackTime, GETDATE()) AND GETDATE() AND (@instanceName = '' OR  @@SERVERNAME = @instanceName)\n\tGROUP BY\n\t\tqs.query_hash,\n\t\tqs.query_plan_hash\n)\nSELECT qs.*,\n\tSUBSTRING(st.text, (stats.statement_start_offset / 2) + 1,\n\t\t\t ((CASE statement_end_offset\n\t\t\t\t   WHEN -1 THEN DATALENGTH(st.text)\n\t\t\t\t   ELSE stats.statement_end_offset END - stats.statement_start_offset) / 2) + 1) AS query_text,\n\tISNULL(qp.query_plan, '') AS query_plan\nFROM qstats AS qs\n\t\tINNER JOIN sys.dm_exec_query_stats AS stats on qs.query_plan_handle = stats.plan_handle\n\t\tCROSS APPLY sys.dm_exec_query_plan(qs.query_plan_handle) AS qp\n\t\tCROSS APPLY sys.dm_exec_sql_text(qs.query_plan_handle) AS st;\n```\n\n----------------------------------------\n\nTITLE: Load Balancing Exporter Architecture Diagram\nDESCRIPTION: ASCII diagram showing the architecture of the load balancing exporter, including resilience options and connection flow between components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: ascii\nCODE:\n```\n                                        +------------------+          +---------------+\n resiliency options 1                   |                  |          |               |\n                                       -- otlp exporter 1  ------------  backend 1    |\n           |                       ---/ |                  |          |               |\n           |                   ---/     +----|-------------+          +---------------+\n           |               ---/              |\n  +-----------------+  ---/                  |\n  |                 --/                      |\n  |  loadbalancing  |                   resiliency options 2\n  |    exporter     |                        |\n  |                 --\\                      |\n  +-----------------+  ----\\                 |\n                            ----\\       +----|-------------+          +---------------+\n                                 ----\\  |                  |          |               |\n                                      --- otlp exporter N  ------------  backend N    |\n                                        |                  |          |               |\n                                        +------------------+          +---------------+\n```\n\n----------------------------------------\n\nTITLE: Clickhouse Trace Table Creation SQL\nDESCRIPTION: A code snippet that was removed for being unnecessary in the clickhouseexporter trace table creation logic.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_41\n\nLANGUAGE: sql\nCODE:\n```\n/* Code not shown in original text but referenced in PR #15679 */\n```\n\n----------------------------------------\n\nTITLE: Input/Output JSON Example for Basic Parsing\nDESCRIPTION: Example showing the transformation of a JSON input with a 'message' field containing a JSON string into properly parsed JSON output.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"{\\\"key\\\": \\\"val\\\"}\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"key\": \"val\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Middleware Interface in Go for OpenTelemetry Collector\nDESCRIPTION: This snippet defines the Middleware interface, which includes methods for wrapping various components of the OpenTelemetry Collector. It specifies methods for wrapping receivers, processors, exporters, and extensions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/testdata/scraper/replica_stats_empty.txt#2025-04-10_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\n// Middleware wraps receivers, processors, exporters and extensions.\ntype Middleware interface {\n\tWrapReceiver(component.Receiver) (component.Receiver, error)\n\tWrapProcessor(component.Processor) (component.Processor, error)\n\tWrapExporter(component.Exporter) (component.Exporter, error)\n\tWrapExtension(component.Extension) (component.Extension, error)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Active Session Details from SQL Server DMVs\nDESCRIPTION: Comprehensive SQL query that joins dynamic management views (DMVs) to retrieve detailed information about active database sessions, including client connections, query execution stats, and transaction details. The query combines sys.dm_exec_requests, sys.dm_exec_sessions, and sys.dm_exec_connections to provide a complete picture of current database activity.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/recordDatabaseSampleQueryData.txt#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n    DB_NAME(r.database_id) AS db_name,\n    ISNULL(c.client_net_address, '') as client_address,\n    ISNULL(c.client_tcp_port, '') AS client_port,\n    CONVERT(NVARCHAR, TODATETIMEOFFSET(r.start_time, DATEPART(TZOFFSET, SYSDATETIMEOFFSET())), 126) AS query_start,\n    s.session_id,\n    s.STATUS AS session_status,\n    r.STATUS AS request_status,\n    ISNULL(s.host_name, '') AS host_name,\n    r.command,\n    SUBSTRING(o.TEXT, (r.statement_start_offset / 2) + 1, (\n                                                              (\n                                                                  CASE r.statement_end_offset\n                                                                      WHEN - 1\n                                                                          THEN DATALENGTH(o.TEXT)\n                                                                      ELSE r.statement_end_offset\n                                                                      END - r.statement_start_offset\n                                                                  ) / 2\n                                                              ) + 1) AS statement_text,\n    r.blocking_session_id,\n    ISNULL(r.wait_type, '') AS wait_type,\n    r.wait_time,\n    r.wait_resource,\n    r.open_transaction_count,\n    r.transaction_id,\n    r.percent_complete,\n    r.estimated_completion_time,\n    r.cpu_time,\n    r.total_elapsed_time,\n    r.reads,\n    r.writes,\n    r.logical_reads,\n    r.transaction_isolation_level,\n    r.LOCK_TIMEOUT,\n    r.DEADLOCK_PRIORITY,\n    r.row_count,\n    r.query_hash,\n    r.query_plan_hash,\n    ISNULL(r.context_info, CONVERT(VARBINARY, '')) AS context_info,\n    s.login_name AS username\nFROM sys.dm_exec_requests r\n         INNER JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id\n         INNER JOIN sys.dm_exec_connections c ON s.session_id = c.session_id\n         CROSS APPLY sys.dm_exec_sql_text(r.plan_handle) AS o\n```\n\n----------------------------------------\n\nTITLE: TraceID-Based Trace Query in ClickHouse\nDESCRIPTION: SQL query to find traces using TraceID with time-based optimization\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nWITH\n    '391dae938234560b16bb63f51501cb6f' as trace_id,\n    (SELECT min(Start) FROM otel_traces_trace_id_ts WHERE TraceId = trace_id) as start,\n    (SELECT max(End) + 1 FROM otel_traces_trace_id_ts WHERE TraceId = trace_id) as end\nSELECT Timestamp,\n       TraceId,\n       SpanId,\n       ParentSpanId,\n       SpanName,\n       SpanKind,\n       ServiceName,\n       Duration,\n       StatusCode,\n       StatusMessage,\n       toString(SpanAttributes),\n       toString(ResourceAttributes),\n       toString(Events.Name),\n       toString(Links.TraceId)\nFROM otel_traces\nWHERE TraceId = trace_id\n  AND Timestamp >= start\n  AND Timestamp <= end\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Parsing Multiline Docker Logs with Recombine Operator\nDESCRIPTION: Configuration for handling multiline logs in Docker format using the recombine operator. This example uses a regex pattern to identify the first line of each log entry based on timestamp format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include:\n      - /var/log/pods/*/my-service/*.log\n    include_file_path: true       \n    operators:\n    - id: container-parser\n      type: container\n    - id: recombine\n      type: recombine\n      combine_field: body\n      is_first_entry: body matches \"^\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}\"\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Node Name Environment Configuration\nDESCRIPTION: YAML configuration for setting up the node name environment variable using Kubernetes downward API.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  - name: K8S_NODE_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: spec.nodeName\n```\n\n----------------------------------------\n\nTITLE: Removing a Value from Body using YAML Configuration\nDESCRIPTION: This snippet demonstrates how to configure the remove operator to delete a specific key from the body of an entry. It specifies the field path 'body.key1' to be removed.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: remove\n  field: body.key1\n```\n\n----------------------------------------\n\nTITLE: Retaining Fields from Multiple Sources with YAML Configuration\nDESCRIPTION: Configuration to retain specific fields from different sections (resource, attributes, body) of a log entry. This demonstrates selective retention across all sections of the data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/retain.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- type: retain\n  fields:\n    - resource.key1\n    - attributes.key3\n    - body.key5\n```\n\n----------------------------------------\n\nTITLE: Basic STEF Exporter Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to set up multiple STEF exporters with different TLS configurations. Demonstrates setting endpoints and TLS certificate settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/stefexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  stef:\n    endpoint: otelcol2:4317\n    tls:\n      cert_file: file.cert\n      key_file: file.key\n  stef/2:\n    endpoint: otelcol2:4317\n    tls:\n      insecure: true\n```\n\n----------------------------------------\n\nTITLE: Removing an Object from Body using YAML Configuration\nDESCRIPTION: This snippet shows how to configure the remove operator to delete an entire object (with nested fields) from the body of an entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: remove\n  field: body.object\n```\n\n----------------------------------------\n\nTITLE: Defining Resource, InstrumentationLibrary, and Spans Enumerations for Testing\nDESCRIPTION: This snippet defines constant enumerations for various resource types, instrumentation library quantities, and span counts that would be used in testing OpenTelemetry components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/goldendataset/testdata/pict_input_traces.txt#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nResource: Empty, VMOnPrem, VMCloud, K8sOnPrem, K8sCloud, Faas, Exec\nInstrumentationLibrary: None, One, Two\nSpans: None, One, Several, All\n```\n\n----------------------------------------\n\nTITLE: Advanced Kafka Metrics Receiver Configuration with TLS\nDESCRIPTION: Comprehensive configuration example with custom broker, cluster alias, collection interval, protocol version, and mTLS settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkametricsreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafkametrics:\n    cluster_alias: kafka-prod\n    brokers: [\"10.10.10.10:9092\"]\n    protocol_version: 3.0.0\n    scrapers:\n      - brokers\n      - topics\n      - consumers\n    tls:\n      ca_file: ca.pem\n      cert_file: cert.pem\n      key_file: key.pem\n    collection_interval: 5s\n```\n\n----------------------------------------\n\nTITLE: Input and Output Example for Body Field Transformation\nDESCRIPTION: JSON example showing the transformation of a list in the body field into a key-value map. The input contains a list of four values, and the output shows these values mapped to the keys specified in the configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/assign_keys.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": [1, \"debug\", \"Debug Message\", true]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n  {\n    \"body\": {\n      \"foo\": 1,\n      \"bar\": \"debug\",\n      \"charlie\": \"Debug Message\",\n      \"foxtrot\": true,\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Metric Configuration Table in OpenTelemetry Format\nDESCRIPTION: A structured table defining various combinations of metric configurations including the number of points per metric, metric types, label counts, and resource attribute counts. This serves as a reference for metric creation and testing within the OpenTelemetry Collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/goldendataset/testdata/generated_pict_pairs_metrics.txt#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nNumPtsPerMetric\tMetricType\tNumLabels\tNumResourceAttrs\nOnePt\tDoubleExemplarsHistogram\tNoLabels\tNoAttrs\nManyPts\tNonMonotonicDoubleSum\tOneLabel\tOneAttr\nOnePt\tNonMonotonicIntSum\tManyLabels\tOneAttr\nManyPts\tNonMonotonicDoubleSum\tNoLabels\tTwoAttrs\nOnePt\tDoubleExemplarsHistogram\tOneLabel\tTwoAttrs\nManyPts\tNonMonotonicDoubleSum\tManyLabels\tNoAttrs\nOnePt\tDoubleGauge\tNoLabels\tOneAttr\nManyPts\tDoubleExemplarsHistogram\tManyLabels\tOneAttr\nManyPts\tMonotonicIntSum\tManyLabels\tTwoAttrs\nManyPts\tNonMonotonicIntSum\tOneLabel\tNoAttrs\nManyPts\tIntGauge\tManyLabels\tNoAttrs\nOnePt\tIntExemplarsHistogram\tOneLabel\tOneAttr\nOnePt\tMonotonicIntSum\tNoLabels\tNoAttrs\nManyPts\tIntExemplarsHistogram\tNoLabels\tTwoAttrs\nManyPts\tMonotonicDoubleSum\tOneLabel\tOneAttr\nManyPts\tDoubleGauge\tManyLabels\tNoAttrs\nOnePt\tIntGauge\tNoLabels\tTwoAttrs\nOnePt\tNonMonotonicIntSum\tNoLabels\tTwoAttrs\nOnePt\tMonotonicDoubleSum\tManyLabels\tNoAttrs\nManyPts\tDoubleGauge\tOneLabel\tTwoAttrs\nManyPts\tIntGauge\tOneLabel\tOneAttr\nOnePt\tIntExemplarsHistogram\tManyLabels\tNoAttrs\nOnePt\tMonotonicIntSum\tOneLabel\tOneAttr\nOnePt\tNonMonotonicDoubleSum\tManyLabels\tNoAttrs\nManyPts\tMonotonicDoubleSum\tNoLabels\tTwoAttrs\n```\n\n----------------------------------------\n\nTITLE: Complex Journald Input Configuration\nDESCRIPTION: Advanced configuration example combining multiple filtering options including matches, units, and priority.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/journald_input.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n  matches:\n    - _SYSTEMD_UNIT: ssh\n    - _SYSTEMD_UNIT: kubelet\n      _UID: \"1000\"\n  units:\n    - kubelet\n    - systemd\n  priority: info\n```\n\n----------------------------------------\n\nTITLE: Implementing Shutdown Behavior for Prometheus Remote Write Exporter\nDESCRIPTION: Implementation of the Shutdown function for the exporter, which uses a stop channel and wait group to ensure graceful shutdown. When shutdown is called, it stops accepting new operations and waits for current ones to finish.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/DESIGN.md#2025-04-10_snippet_3\n\nLANGUAGE: go\nCODE:\n```\nfunc Shutdown () {\n    close(stopChan)\n    waitGroup.Wait()\n}\n\nfunc PushMetrics() {\n    select:\n        case <- stopCh\n               return error\n        default:\n               waitGroup.Add(1)\n               defer waitGroup.Done()\n               // export metrics\n\t  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Partition Format Configuration in YAML\nDESCRIPTION: Configuration example showing how to customize S3 partition formatting using strftime format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awss3exporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  awss3:\n    s3uploader:\n      region: 'eu-central-1'\n      s3_bucket: 'databucket'\n      s3_prefix: 'metric'\n      s3_partition_format: '%Y/%m/%d/%H/%M'\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON with Timestamp Extraction in YAML\nDESCRIPTION: A configuration that parses 'body.message' as JSON and extracts a timestamp from 'seconds_since_epoch' field using epoch format in seconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: json_parser\n  parse_from: body.message\n  timestamp:\n    parse_from: body.seconds_since_epoch\n    layout_type: epoch\n    layout: s\n```\n\n----------------------------------------\n\nTITLE: Managed Identity Authentication Configuration - YAML\nDESCRIPTION: Configuration example for Azure Monitor receiver using Managed Identity authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/azuremonitorreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  azuremonitor:\n    subscription_ids: [\"${subscription_id}\"]\n    auth: \"managed_identity\"\n    client_id: \"${env:AZURE_CLIENT_ID}\"\n```\n\n----------------------------------------\n\nTITLE: Uninstalling OpenTelemetry Collector Deployment\nDESCRIPTION: Makefile command to uninstall the OpenTelemetry Collector Deployment from a kind Kubernetes cluster.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/dev-docs.md#2025-04-10_snippet_4\n\nLANGUAGE: makefile\nCODE:\n```\nmake kind-uninstall-deployment\n```\n\n----------------------------------------\n\nTITLE: Enabling intToDouble Feature Gate in Shell\nDESCRIPTION: Command line flag to enable the intToDouble feature gate, which converts all metric datapoint types to double to prevent type conflict errors in Google Managed Prometheus.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlemanagedprometheusexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n\"--feature-gates=exporter.googlemanagedprometheus.intToDouble\"\n```\n\n----------------------------------------\n\nTITLE: Failed Translations Metric Definition\nDESCRIPTION: Markdown table defining the otelcol_exporter_prometheusremotewrite_failed_translations metric that tracks failed translation operations\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| 1 | Sum | Int | true |\n```\n\n----------------------------------------\n\nTITLE: ECS Container Definition in JSON\nDESCRIPTION: Example JSON configuration for a Redis container definition in ECS, showing port mappings and docker labels that work with the ECS Task Observer extension.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecstaskobserver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"containerDefinitions\": [\n    {\n      \"portMappings\": [\n        {\n          \"containerPort\": 6379,\n          \"hostPort\": 6379\n        }\n      ],\n      \"image\": \"redis\",\n      \"dockerLabels\": {\n        \"A_DOCKER_LABEL_CONTAINING_DESIRED_PORT\": \"6379\",\n        \"SECRET\": \"my-redis-auth\"\n      },\n      \"name\": \"redis\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Recombine with max_unmatched_batch_size set to 3 (is_last_entry)\nDESCRIPTION: Configuration example where max_unmatched_batch_size is set to 3 with is_last_entry, showing the different behavior compared to is_first_entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n- type: recombine\n  combine_field: body\n  is_last_entry: body == 'log1'\n  max_unmatched_batch_size: 3\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in RabbitMQ\nDESCRIPTION: YAML configuration snippet showing how to enable optional metrics in RabbitMQ collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/rabbitmqreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter Dimension Client\nDESCRIPTION: YAML configuration for exposing dimension_client configuration in the SignalFx exporter for dimension/metadata updates.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_31\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    dimension_client:\n      send_delay: 10s\n      max_buffer_size: 1000\n```\n\n----------------------------------------\n\nTITLE: Basic Load Balancing Configuration with Static Resolver\nDESCRIPTION: Simple configuration example showing basic setup of load balancing exporter with static backend hosts and OTLP protocol configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: localhost:4317\n\nprocessors:\n\nexporters:\n  loadbalancing:\n    routing_key: \"service\"\n    protocol:\n      otlp:\n        # all options from the OTLP exporter are supported\n        # except the endpoint\n        timeout: 1s\n    resolver:\n      static:\n        hostnames:\n        - backend-1:4317\n        - backend-2:4317\n        - backend-3:4317\n        - backend-4:4317\n      # Notice to config a headless service DNS in Kubernetes\n      # dns:\n      #  hostname: otelcol-headless.observability.svc.cluster.local\n\nservice:\n  pipelines:\n    traces:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n    logs:\n      receivers:\n        - otlp\n      processors: []\n      exporters:\n        - loadbalancing\n```\n\n----------------------------------------\n\nTITLE: Deploying AWS OpenTelemetry Configuration to EKS\nDESCRIPTION: Command to apply the AWS OpenTelemetry Collector configuration to an EKS cluster using kubectl.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscontainerinsightreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Solarwinds APM Settings Extension in YAML\nDESCRIPTION: Example configuration for the Solarwinds APM Settings extension that specifies the endpoint, service key, and polling interval. The endpoint should be the appropriate APM collector endpoint, the key needs to be in the format '<token>:<name>', and the interval determines how frequently settings are retrieved.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/solarwindsapmsettingsextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  solarwindsapmsettings:\n    endpoint: \"<endpoint>\"\n    key: \"<token>:<name>\"\n    interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Compatible Keepalive Configuration for OTELARROW Receiver\nDESCRIPTION: A compatible keepalive configuration for the OTELARROW receiver that works with the exporter configuration shown previously. Sets the max connection age to 1 minute with a grace period of 10 minutes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/otelarrowexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otelarrow:\n    protocols:\n      grpc:\n        keepalive:\n          server_parameters:\n            max_connection_age: 1m\n            max_connection_age_grace: 10m\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Detection Processor with Attribute Selection in YAML\nDESCRIPTION: Example configuration for resourcedetectionprocessor that demonstrates how to selectively enable specific resource attributes from different detectors. This allows users to configure which detector provides which attribute, such as having host.name from the system detector and host.id from the EC2 detector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_28\n\nLANGUAGE: yaml\nCODE:\n```\nresourcedetection:\n  detectors: [system, ec2]\n  system:\n    resource_attributes:\n      host.name:\n        enabled: true\n      host.id:\n        enabled: false\n  ec2:\n    resource_attributes:\n      host.name:\n        enabled: false\n      host.id:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Querying Session and Request Information from SQL Server DMVs\nDESCRIPTION: This SQL query joins multiple system dynamic management views (DMVs) to retrieve comprehensive information about active database sessions, connections, and requests. It includes details such as client information, query execution status, resource usage, and performance metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/recordInvalidDatabaseSampleQueryData.txt#2025-04-10_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT\n    DB_NAME(r.database_id) AS db_name,\n    ISNULL(c.client_net_address, '') as client_address,\n    ISNULL(c.client_tcp_port, '') AS client_port,\n    CONVERT(NVARCHAR, TODATETIMEOFFSET(r.start_time, DATEPART(TZOFFSET, SYSDATETIMEOFFSET())), 126) AS query_start,\n    s.session_id,\n    s.STATUS AS session_status,\n    r.STATUS AS request_status,\n    ISNULL(s.host_name, '') AS host_name,\n    r.command,\n    SUBSTRING(o.TEXT, (r.statement_start_offset / 2) + 1, (\n                                                              (\n                                                                  CASE r.statement_end_offset\n                                                                      WHEN - 1\n                                                                          THEN DATALENGTH(o.TEXT)\n                                                                      ELSE r.statement_end_offset\n                                                                      END - r.statement_start_offset\n                                                                  ) / 2\n                                                              ) + 1) AS statement_text,\n    r.blocking_session_id,\n    ISNULL(r.wait_type, '') AS wait_type,\n    r.wait_time,\n    r.wait_resource,\n    r.open_transaction_count,\n    r.transaction_id,\n    r.percent_complete,\n    r.estimated_completion_time,\n    r.cpu_time,\n    r.total_elapsed_time,\n    r.reads,\n    r.writes,\n    r.logical_reads,\n    r.transaction_isolation_level,\n    r.LOCK_TIMEOUT,\n    r.DEADLOCK_PRIORITY,\n    r.row_count,\n    r.query_hash,\n    r.query_plan_hash,\n    ISNULL(r.context_info, CONVERT(VARBINARY, '')) AS context_info,\n\n    s.login_name AS username\nFROM sys.dm_exec_requests r\n         INNER JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id\n         INNER JOIN sys.dm_exec_connections c ON s.session_id = c.session_id\n         CROSS APPLY sys.dm_exec_sql_text(r.plan_handle) AS o\n```\n\n----------------------------------------\n\nTITLE: Output Log Entry Example - JSON\nDESCRIPTION: Example of the processed log entry after scope name parsing, showing the extracted scope name and restructured message.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/scope_name.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"message\": \"some message\"\n  },\n  \"scope_name\": \"com.example.Foo\"\n}\n```\n\n----------------------------------------\n\nTITLE: Explaining Logical Relationships in Journald Input Filtering\nDESCRIPTION: This snippet illustrates the logical relationships between different filtering options in the journald input configuration. It shows how conditions are combined using AND and OR operations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/journaldreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n( dmesg )\nAND\n( priority )\nAND\n( units[0] OR units[1] OR units[2] OR ... units[U] )\nAND\n( identifier[0] OR identifier[1] OR identifier[2] OR ... identifier[I] )\nAND\n( matches[0] OR matches[1] OR matches[2] OR ... matches[M] )\nAND\n( grep )\n```\n\n----------------------------------------\n\nTITLE: Using Time Converter for Timestamp Parsing in OpenTelemetry\nDESCRIPTION: The Time Converter parses a string representation of time into a Golang time.Time object. It supports various formatting options, customizable time zones, and localization for different languages.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_56\n\nLANGUAGE: go\nCODE:\n```\nTime(target, format, Optional[location], Optional[locale])\n```\n\n----------------------------------------\n\nTITLE: Layered OpenTelemetry Routing\nDESCRIPTION: Implements a two-layer routing approach where the first layer sorts data by match status and the second layer handles specific routing. This approach eliminates the need for default_pipelines in the second layer.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/connector/routingconnector/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# First layer separates logs that match no routes\nrouting:\n  default_pipelines: [ logs/default ]\n  table: # all routes forward to second layer\n    - condition: attributes[\"env\"] == \"prod\"\n       pipelines: [ logs/env, logs/region ] \n    - condition: attributes[\"env\"] == \"dev\"\n       pipelines: [ logs/env, logs/region ]\n    - condition: attributes[\"region\"] == \"east\"\n       pipelines: [ logs/env, logs/region ]\n    - condition: attributes[\"region\"] == \"west\"\n       pipelines: [ logs/env, logs/region ]\n\n# Second layer routes logs based on environment and region\nrouting/env:\n  table:\n    - condition: attributes[\"env\"] == \"prod\"\n       pipelines: [ logs/prod ]\n    - condition: attributes[\"env\"] == \"dev\"\n       pipelines: [ logs/dev ]\nrouting/region:\n  table:\n    - condition: attributes[\"region\"] == \"east\"\n       pipelines: [ logs/east ]\n    - condition: attributes[\"region\"] == \"west\"\n       pipelines: [ logs/west ]\n\nservice:\n  pipelines:\n    logs/in::exporters: [routing]\n    logs/prod::receivers: [routing/env]\n    logs/dev::receivers: [routing/env]\n    logs/east::receivers: [routing/region]\n    logs/west::receivers: [routing/region]\n```\n\n----------------------------------------\n\nTITLE: Separating Incompatible OTTL Statements into Separate Groups\nDESCRIPTION: Example showing the correct way to separate statements that use different contexts by placing them in different statement groups.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmetric_statements:\n  - statements:\n    - limit(datapoint.attributes, 100, [\"host.name\"])\n  - statements:\n    - convert_sum_to_gauge() where metric.name == \"system.processes.count\" \n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Operator Sequence with Custom IDs in YAML\nDESCRIPTION: This snippet shows the same linear sequence with custom 'id' values for each operator, demonstrating how to use custom identifiers in the sequence.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/operators.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: my-log.json\n    operators:\n      - type: json_parser\n        id: my_json_parser\n        output: my_remove\n      - type: remove\n        id: my_remove\n        field: attributes.foo\n        output: my_add\n      - type: add\n        id: my_add\n        key: attributes.bar\n        value: baz\n        # the last operator automatically outputs from the receiver\n```\n\n----------------------------------------\n\nTITLE: Building OpenTelemetry Collector Container Image\nDESCRIPTION: Command to build the OpenTelemetry Collector container image from the project root directory.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/example/README.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t otelcontribcol .\n```\n\n----------------------------------------\n\nTITLE: Configuring Libhoney Receiver in YAML\nDESCRIPTION: Example configuration for the Libhoney receiver showing HTTP settings, resource mappings, scope configurations, and attribute mappings. Includes settings for handling Refinery traffic and defining custom field mappings for traces and logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/libhoneyreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  libhoney:\n    http:\n      endpoint: 0.0.0.0:8088\n      traces_url_paths:\n        - \"/1/events\"\n        - \"/1/batch\"\n      include_metadata: true\n    auth_api: https://api.honeycomb.io\n    fields:\n      resources:\n        service_name: service_name\n      scopes:\n        library_name: library.name\n        library_version: library.version\n      attributes:\n        trace_id: trace_id\n        parent_id: parent_id\n        span_id: span_id\n        name: name\n        error: error\n        spankind: span.kind\n        durationFields:\n          - duration_ms\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare Receiver with Automatic Attribute Ingestion in YAML\nDESCRIPTION: Example YAML configuration for the Cloudflare receiver with automatic attribute ingestion. This configuration will ingest all fields from the log messages as attributes, using the original field names as attribute names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/cloudflarereceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  cloudflare:\n    logs:\n      tls:\n        key_file: some_key_file\n        cert_file: some_cert_file\n      endpoint: 0.0.0.0:12345\n      secret: 1234567890abcdef1234567890abcdef\n      timestamp_field: EdgeStartTimestamp\n      attributes:\n        # Specifying no attributes ingests them all\n```\n\n----------------------------------------\n\nTITLE: Using ToUpperCase Converter for String Formatting in OpenTelemetry\nDESCRIPTION: The ToUpperCase Converter transforms a string into uppercase format. It converts all characters to their uppercase equivalents, changing strings like 'MyMetricName' to 'MYMETRICNAME'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_61\n\nLANGUAGE: go\nCODE:\n```\nToUpperCase(target)\n```\n\n----------------------------------------\n\nTITLE: Encoding 50% Sampling with Randomness Value in Trace Context\nDESCRIPTION: Example of how 50% sampling with a specific randomness value is encoded in a tracing Span's tracestate field after sampling is applied.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/probabilisticsamplerprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ntracestate: ot=th:8;rv:9b8233f7e3a151\n```\n\n----------------------------------------\n\nTITLE: Configuring Non-Linear Operator Sequence for Multiple Log Formats in YAML\nDESCRIPTION: This example demonstrates a non-linear sequence for processing logs with two different formats. It uses a router to direct logs to different parsers based on their content, then combines them for further processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/operators.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: my-log.json\n    operators:\n      - type: router\n        routes:\n          - expr: 'body matches \"^{.*}$\"'\n            output: json_parser\n          - expr: 'body startsWith \"ERROR\"'\n            output: error_parser\n      - type: json_parser\n        output: remove # send from here directly to the 'remove' operator\n      - type: regex_parser\n        id: error_parser\n        regex: ... # regex appropriate to parsing error logs\n      - type: remove\n        field: attributes.foo\n      - type: add\n        key: attributes.bar\n        value: baz\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Oracle DB Metrics in YAML\nDESCRIPTION: Example YAML configuration to enable optional metrics that are not collected by default. This configuration can be applied to any of the optional metrics listed in the documentation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/oracledbreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Example Spans Structure Before Compaction\nDESCRIPTION: A Go representation showing spans structure with duplicated Resources and InstrumentationLibrary objects before applying the compaction with groupbyattrs processor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nResource {host.name=\"localhost\"}\n  InstrumentationLibrary {name=\"MyLibrary\"}\n  Spans\n    Span {span_id=1, ...}\n  InstrumentationLibrary {name=\"OtherLibrary\"}\n  Spans\n    Span {span_id=2, ...}\n    \nResource {host.name=\"localhost\"}\n  InstrumentationLibrary {name=\"MyLibrary\"}\n  Spans\n    Span {span_id=3, ...}\n    \nResource {host.name=\"localhost\"}\n  InstrumentationLibrary {name=\"MyLibrary\"}\n  Spans\n    Span {span_id=4, ...}\n    \nResource {host.name=\"otherhost\"}\n  InstrumentationLibrary {name=\"MyLibrary\"}\n  Spans\n    Span {span_id=5, ...}\n```\n\n----------------------------------------\n\nTITLE: Configuring Regex Parser for Scope Name Extraction - YAML\nDESCRIPTION: Configuration example showing how to use regex_parser to extract a scope name from a log entry. The parser splits the input into scope_name_field and message components using regex pattern matching.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/scope_name.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_parser\n  regexp: '^(?P<scope_name_field>\\S*)\\s-\\s(?P<message>.*)'\n  scope_name:\n    parse_from: body.scope_name_field\n```\n\n----------------------------------------\n\nTITLE: Collecting SQL Server Properties and Configuration for Monitoring\nDESCRIPTION: This T-SQL script retrieves comprehensive information about a SQL Server instance, including hardware specifications, version details, encryption settings, port configuration, and database status counts. The script first validates the SQL Server edition, then dynamically builds and executes a query that collects server metrics through system views and registry reads. Results are structured with a measurement name for compatibility with monitoring systems.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/propertyQueryWithoutInstanceName.txt#2025-04-10_snippet_0\n\nLANGUAGE: T-SQL\nCODE:\n```\nSET DEADLOCK_PRIORITY -10;\nIF SERVERPROPERTY('EngineEdition') NOT IN (2,3,4) BEGIN /*NOT IN Standard, Enterprise, Express*/\n\tDECLARE @ErrorMessage AS nvarchar(500) = 'Connection string Server:'+ @@ServerName + ',Database:' + DB_NAME() +' is not a SQL Server Standard, Enterprise or Express. This query is only supported on these editions.';\n\tRAISERROR (@ErrorMessage,11,1)\n\tRETURN\nEND\n\nDECLARE\n\t @SqlStatement AS nvarchar(max) = ''\n\t,@MajorMinorVersion AS int = CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),4) AS int)*100 + CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),3) AS int)\n\t,@Columns AS nvarchar(MAX) = ''\n\nIF CAST(SERVERPROPERTY('ProductVersion') AS varchar(50)) >= '10.50.2500.0'\n\tSET @Columns = N'\n\t,CASE [virtual_machine_type_desc]\n\t\tWHEN ''NONE'' THEN ''PHYSICAL Machine''\n\t\tELSE [virtual_machine_type_desc]\n\tEND AS [hardware_type]'\n\nSET @SqlStatement = '\nDECLARE @ForceEncryption INT\nDECLARE @DynamicportNo NVARCHAR(50);\nDECLARE @StaticportNo NVARCHAR(50);\n\nEXEC [xp_instance_regread]\n\t @rootkey = ''HKEY_LOCAL_MACHINE''\n\t,@key = ''SOFTWARE\\\\Microsoft\\\\Microsoft SQL Server\\\\MSSQLServer\\\\SuperSocketNetLib''\n\t,@value_name = ''ForceEncryption''\n\t,@value = @ForceEncryption OUTPUT;\n\nEXEC [xp_instance_regread]\n\t @rootkey = ''HKEY_LOCAL_MACHINE''\n\t,@key = ''Software\\\\Microsoft\\\\Microsoft SQL Server\\\\MSSQLServer\\\\SuperSocketNetLib\\\\Tcp\\\\IpAll''\n\t,@value_name = ''TcpDynamicPorts''\n\t,@value = @DynamicportNo OUTPUT\n\nEXEC [xp_instance_regread]\n\t  @rootkey = ''HKEY_LOCAL_MACHINE''\n     ,@key = ''Software\\\\Microsoft\\\\Microsoft SQL Server\\\\MSSQLServer\\\\SuperSocketNetLib\\\\Tcp\\\\IpAll''\n     ,@value_name = ''TcpPort''\n     ,@value = @StaticportNo OUTPUT\n\nSELECT\n\t ''sqlserver_server_properties'' AS [measurement]\n\t,REPLACE(@@SERVERNAME,''\\\\','':'') AS [sql_instance]\n\t,HOST_NAME() AS [computer_name]\n\t,@@SERVICENAME AS [service_name]\n\t,si.[cpu_count]\n\t,(SELECT [total_physical_memory_kb] FROM sys.[dm_os_sys_memory]) AS [server_memory]\n\t,(SELECT [available_physical_memory_kb] FROM sys.[dm_os_sys_memory]) AS [available_server_memory]\n\t,SERVERPROPERTY(''Edition'') AS [sku]\n\t,CAST(SERVERPROPERTY(''EngineEdition'') AS int) AS [engine_edition]\n\t,DATEDIFF(MINUTE,si.[sqlserver_start_time],GETDATE()) AS [uptime]\n\t,SERVERPROPERTY(''ProductVersion'') AS [sql_version]\n\t,SERVERPROPERTY(''IsClustered'') AS [instance_type]\n\t,SERVERPROPERTY(''IsHadrEnabled'') AS [is_hadr_enabled]\n\t,LEFT(@@VERSION,CHARINDEX('' - '',@@VERSION)) AS [sql_version_desc]\n\t,@ForceEncryption AS [ForceEncryption]\n\t,COALESCE(@DynamicportNo,@StaticportNo) AS [Port]\n\t,IIF(@DynamicportNo IS NULL, ''Static'', ''Dynamic'') AS [PortType]\n\t,dbs.[db_online]\n\t,dbs.[db_restoring]\n\t,dbs.[db_recovering]\n\t,dbs.[db_recoveryPending]\n\t,dbs.[db_suspect]\n\t,dbs.[db_offline]'\n\t+ @Columns + N'\n\tFROM sys.[dm_os_sys_info] AS si\n\tCROSS APPLY (\n\t\tSELECT\n\t\t\t SUM(CASE WHEN [state] = 0 THEN 1 ELSE 0 END) AS [db_online]\n\t\t\t,SUM(CASE WHEN [state] = 1 THEN 1 ELSE 0 END) AS [db_restoring]\n\t\t\t,SUM(CASE WHEN [state] = 2 THEN 1 ELSE 0 END) AS [db_recovering]\n\t\t\t,SUM(CASE WHEN [state] = 3 THEN 1 ELSE 0 END) AS [db_recoveryPending]\n\t\t\t,SUM(CASE WHEN [state] = 4 THEN 1 ELSE 0 END) AS [db_suspect]\n\t\t\t,SUM(CASE WHEN [state] IN (6,10) THEN 1 ELSE 0 END) AS [db_offline]\n\t\tFROM sys.databases\n\t) AS dbs\n'\n\nEXEC sp_executesql @SqlStatement\n```\n\n----------------------------------------\n\nTITLE: SQL Query Result Metric Output Format\nDESCRIPTION: Example showing the metric output format generated from a SQL query counting movies by genre. Demonstrates how query results are transformed into OpenTelemetry metrics with attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlqueryreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: plain\nCODE:\n```\nMetric #0\nDescriptor:\n     -> Name: movie.genres\n     -> DataType: Gauge\nNumberDataPoints #0\nData point attributes:\n     -> genre: STRING(sci-fi)\n     -> dbinstance: STRING(mydbinstance)\nValue: 2\n\nMetric #1\nDescriptor:\n     -> Name: movie.genres\n     -> DataType: Gauge\nNumberDataPoints #0\nData point attributes:\n     -> genre: STRING(action)\n     -> dbinstance: STRING(mydbinstance)\nValue: 1\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Complex Log Record Example - JSON Input\nDESCRIPTION: Complex example of an input log record with structured data and additional attributes following RFC5424 format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/syslogexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"\",\n  \"timeUnixNano\": 1438811939693012000,\n  \"attributes\":\n  {\n    \"appname\": \"SecureAuth0\",\n    \"hostname\": \"192.168.2.132\",\n    \"message\": \"Found the user for retrieving user's profile\",\n    \"msg_id\": \"ID52020\",\n    \"priority\": 86,\n    \"proc_id\": \"23108\",\n    \"structured_data\":\n    {\n      \"SecureAuth@27389\":\n      {\n        \"UserHostAddress\":\"192.168.2.132\",\n        \"Realm\":\"SecureAuth0\",\n        \"UserID\":\"Tester2\",\n        \"PEN\":\"27389\"\n      }\n    },\n    \"version\": 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Complex JSON Array with Mixed Types\nDESCRIPTION: Shows a JSON array string containing different data types including strings, numbers, booleans and null\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_array_parser.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\\\"Hello\\\", 42, true, null]\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure AKS Resource Detection\nDESCRIPTION: YAML configuration for detecting Azure AKS resources using environment variables and AKS-specific detection methods.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/aks:\n    detectors: [env, aks]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in YAML Configuration\nDESCRIPTION: This YAML configuration snippet demonstrates how to enable an optional metric in the kubeletstats collector. Replace <metric_name> with the specific optional metric you want to enable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle DB Metric Collection in YAML\nDESCRIPTION: Example YAML configuration to disable specific default metrics in the Oracle DB collector. This pattern can be applied to any metric to toggle its collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/oracledbreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Deploying OpenTelemetry Operator in Kubernetes\nDESCRIPTION: This command applies the latest OpenTelemetry Operator YAML configuration to the Kubernetes cluster. It's a prerequisite for setting up the load-balancing exporter demo.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/example/k8s-resolver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml\n```\n\n----------------------------------------\n\nTITLE: AWS S3 Exporter Compression Configuration\nDESCRIPTION: Configuration option added to enable gzip compression of files before uploading to S3 using the compress/gzip library.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_20\n\nLANGUAGE: go\nCODE:\n```\ncompression = \"gzip\"\n```\n\n----------------------------------------\n\nTITLE: Handling Unrecognized Severity in JSON for OpenTelemetry Collector\nDESCRIPTION: This example demonstrates how the severity parser handles an unrecognized severity value. The input severity remains unchanged, and the unrecognized field is removed from the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"default\",\n  \"body\": {\n    \"severity_field\": \"ERROR\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"default\",\n  \"body\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example for Metrics (telegraf-prometheus-v1)\nDESCRIPTION: Example of how metrics are represented in InfluxDB line protocol format using the telegraf-prometheus-v1 schema.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/influxdbexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\ncpu_temp,foo=bar gauge=87.332\nhttp_requests_total,method=post,code=200 counter=1027\nhttp_requests_total,method=post,code=400 counter=3\nhttp_request_duration_seconds 0.05=24054,0.1=33444,0.2=100392,0.5=129389,1=133988,sum=53423,count=144320,min=0,max=10\nrpc_duration_seconds 0.01=3102,0.05=3272,0.5=4773,0.9=9001,0.99=76656,sum=1.7560473e+07,count=2693\n```\n\n----------------------------------------\n\nTITLE: sidecar Mode Architecture\nDESCRIPTION: Diagram showing the architecture of the sidecar mode, which operates similar to the filelog receiver but with added Kubernetes metadata and environment variable integration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8slogreceiver/design.md#2025-04-10_snippet_3\n\nLANGUAGE: ascii\nCODE:\n```\n            ┌──────────────┐\n  env ───▶  │   Poller     │ find files based on include from config\n            └──────────────┘\n                │ create\n                ▼\n            ┌──────────────┐\n            │   Reader     │\n            └──────────────┘\n                │ read files\n                ▼\n               files\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Metrics JSON Structure\nDESCRIPTION: JSON array containing SQL Server performance counter data including measurements for access methods, availability replica metrics, and batch response statistics. Each object includes counter name, type, instance details, measurement category, and metric values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n   {\n      \"counter\":\"Forwarded Records/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Access Methods\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"0\"\n   }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining AWS X-Ray Trace Subsegment in JSON\nDESCRIPTION: This JSON object defines a subsegment of an AWS X-Ray trace. It includes metadata about the API call, such as the name, ID, timing information, trace and parent IDs, and HTTP request and response details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/indepSubsegment.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"api.example.com\",\n    \"id\": \"53995c3f42cd8ad8\",\n    \"start_time\": 1478293361.271,\n    \"end_time\": 1478293361.449,\n    \"type\": \"subsegment\",\n    \"trace_id\": \"1-581cf771-a006649127e371903a2de979\",\n    \"parent_id\": \"defdfd9912dc5a56\",\n    \"namespace\": \"remote\",\n    \"traced\": true,\n    \"http\": {\n        \"request\": {\n            \"url\": \"https://api.example.com/health\",\n            \"method\": \"POST\"\n        },\n        \"response\": {\n            \"status\": 200,\n            \"content_length\": 861\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional JSON Parsing with Regular Expression in YAML\nDESCRIPTION: A configuration that only parses the body if it matches the pattern '^{.*}$', which checks if the body starts with '{' and ends with '}'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_parser.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: json_parser\n  if: 'body matches \"^{.*}$\"'\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Monitor Exporter with Environment Variable in YAML\nDESCRIPTION: YAML configuration for the Azure Monitor Exporter when using the APPLICATIONINSIGHTS_CONNECTION_STRING environment variable. This method is useful for cloud or containerized environments.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuremonitorexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  azuremonitor:\n```\n\n----------------------------------------\n\nTITLE: Configuring Mezmo Exporter with Resource Detection in YAML\nDESCRIPTION: This YAML configuration sets up the Mezmo exporter with OTLP receiver, resource detection processor, and a logs pipeline. It demonstrates how to configure the ingest URL and key, and includes hostname detection to meet Mezmo's ingestion requirements.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/mezmoexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \":4317\"\n\nprocessors:\n  resourcedetection:\n    detectors:\n      - system\n    system:\n      hostname_sources:\n        - os\n\nexporters:\n  mezmo:\n    ingest_url: \"https://logs.mezmo.com/otel/ingest/rest\"\n    ingest_key: \"00000000000000000000000000000000\"\n\nservice:\n  pipelines:\n    logs:\n      receivers: [ otlp ]\n      processors: [ resourcedetection ]\n      exporters: [ mezmo ]\n```\n\n----------------------------------------\n\nTITLE: Simple JSON Array String Example\nDESCRIPTION: Demonstrates the format of a basic JSON array string containing only string values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_array_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\\\"Foo\\\", \\\"Bar\\\", \\\"Charlie\\\"]\n```\n\n----------------------------------------\n\nTITLE: Simple JSON Array String Example\nDESCRIPTION: Demonstrates the format of a basic JSON array string containing only string values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_array_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\\\"Foo\\\", \\\"Bar\\\", \\\"Charlie\\\"]\n```\n\n----------------------------------------\n\nTITLE: Collecting SQL Server Properties with Dynamic SQL\nDESCRIPTION: A comprehensive T-SQL script that gathers SQL Server instance metadata including version, edition, hardware specs, uptime, port configuration, encryption settings, and database states. The script includes version compatibility checks, hardware type detection for virtual environments on newer SQL Server versions, and registry reading for network configuration details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/propertyQueryWithInstanceName.txt#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSET DEADLOCK_PRIORITY -10;\nIF SERVERPROPERTY('EngineEdition') NOT IN (2,3,4) BEGIN /*NOT IN Standard, Enterprise, Express*/\n\tDECLARE @ErrorMessage AS nvarchar(500) = 'Connection string Server:'+ @@ServerName + ',Database:' + DB_NAME() +' is not a SQL Server Standard, Enterprise or Express. This query is only supported on these editions.';\n\tRAISERROR (@ErrorMessage,11,1)\n\tRETURN\nEND\n\nDECLARE\n\t @SqlStatement AS nvarchar(max) = ''\n\t,@MajorMinorVersion AS int = CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),4) AS int)*100 + CAST(PARSENAME(CAST(SERVERPROPERTY('ProductVersion') AS nvarchar),3) AS int)\n\t,@Columns AS nvarchar(MAX) = ''\n\nIF CAST(SERVERPROPERTY('ProductVersion') AS varchar(50)) >= '10.50.2500.0'\n\tSET @Columns = N'\n\t,CASE [virtual_machine_type_desc]\n\t\tWHEN ''NONE'' THEN ''PHYSICAL Machine''\n\t\tELSE [virtual_machine_type_desc]\n\tEND AS [hardware_type]'\n\nSET @SqlStatement = '\nDECLARE @ForceEncryption INT\nDECLARE @DynamicportNo NVARCHAR(50);\nDECLARE @StaticportNo NVARCHAR(50);\n\nEXEC [xp_instance_regread]\n\t @rootkey = ''HKEY_LOCAL_MACHINE''\n\t,@key = ''SOFTWARE\\\\Microsoft\\\\Microsoft SQL Server\\\\MSSQLServer\\\\SuperSocketNetLib''\n\t,@value_name = ''ForceEncryption''\n\t,@value = @ForceEncryption OUTPUT;\n\nEXEC [xp_instance_regread]\n\t @rootkey = ''HKEY_LOCAL_MACHINE''\n\t,@key = ''Software\\\\Microsoft\\\\Microsoft SQL Server\\\\MSSQLServer\\\\SuperSocketNetLib\\\\Tcp\\\\IpAll''\n\t,@value_name = ''TcpDynamicPorts''\n\t,@value = @DynamicportNo OUTPUT\n\nEXEC [xp_instance_regread]\n\t  @rootkey = ''HKEY_LOCAL_MACHINE''\n     ,@key = ''Software\\\\Microsoft\\\\Microsoft SQL Server\\\\MSSQLServer\\\\SuperSocketNetLib\\\\Tcp\\\\IpAll''\n     ,@value_name = ''TcpPort''\n     ,@value = @StaticportNo OUTPUT\n\nSELECT\n\t ''sqlserver_server_properties'' AS [measurement]\n\t,REPLACE(@@SERVERNAME,''\\\\','':'') AS [sql_instance]\n\t,HOST_NAME() AS [computer_name]\n\t,@@SERVICENAME AS [service_name]\n\t,si.[cpu_count]\n\t,(SELECT [total_physical_memory_kb] FROM sys.[dm_os_sys_memory]) AS [server_memory]\n\t,(SELECT [available_physical_memory_kb] FROM sys.[dm_os_sys_memory]) AS [available_server_memory]\n\t,SERVERPROPERTY(''Edition'') AS [sku]\n\t,CAST(SERVERPROPERTY(''EngineEdition'') AS int) AS [engine_edition]\n\t,DATEDIFF(MINUTE,si.[sqlserver_start_time],GETDATE()) AS [uptime]\n\t,SERVERPROPERTY(''ProductVersion'') AS [sql_version]\n\t,SERVERPROPERTY(''IsClustered'') AS [instance_type]\n\t,SERVERPROPERTY(''IsHadrEnabled'') AS [is_hadr_enabled]\n\t,LEFT(@@VERSION,CHARINDEX('' - '',@@VERSION)) AS [sql_version_desc]\n\t,@ForceEncryption AS [ForceEncryption]\n\t,COALESCE(@DynamicportNo,@StaticportNo) AS [Port]\n\t,IIF(@DynamicportNo IS NULL, ''Static'', ''Dynamic'') AS [PortType]\n\t,dbs.[db_online]\n\t,dbs.[db_restoring]\n\t,dbs.[db_recovering]\n\t,dbs.[db_recoveryPending]\n\t,dbs.[db_suspect]\n\t,dbs.[db_offline]'\n\t+ @Columns + N'\n\tFROM sys.[dm_os_sys_info] AS si\n\tCROSS APPLY (\n\t\tSELECT\n\t\t\t SUM(CASE WHEN [state] = 0 THEN 1 ELSE 0 END) AS [db_online]\n\t\t\t,SUM(CASE WHEN [state] = 1 THEN 1 ELSE 0 END) AS [db_restoring]\n\t\t\t,SUM(CASE WHEN [state] = 2 THEN 1 ELSE 0 END) AS [db_recovering]\n\t\t\t,SUM(CASE WHEN [state] = 3 THEN 1 ELSE 0 END) AS [db_recoveryPending]\n\t\t\t,SUM(CASE WHEN [state] = 4 THEN 1 ELSE 0 END) AS [db_suspect]\n\t\t\t,SUM(CASE WHEN [state] IN (6,10) THEN 1 ELSE 0 END) AS [db_offline]\n\t\tFROM sys.databases\n\t) AS dbs\nWHERE @@SERVERNAME = ''instanceName'''\n\nEXEC sp_executesql @SqlStatement\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example for Metrics (telegraf-prometheus-v2)\nDESCRIPTION: Example of how metrics are represented in InfluxDB line protocol format using the telegraf-prometheus-v2 schema.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/influxdbexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nprometheus,foo=bar cpu_temp=87.332\nprometheus,method=post,code=200 http_requests_total=1027\nprometheus,method=post,code=400 http_requests_total=3\nprometheus,le=0.05 http_request_duration_seconds_bucket=24054\nprometheus,le=0.1  http_request_duration_seconds_bucket=33444\nprometheus,le=0.2  http_request_duration_seconds_bucket=100392\nprometheus,le=0.5  http_request_duration_seconds_bucket=129389\nprometheus,le=1    http_request_duration_seconds_bucket=133988\nprometheus         http_request_duration_seconds_count=144320,http_request_duration_seconds_sum=53423,http_request_duration_seconds_min=0,http_request_duration_seconds_max=100\nprometheus,quantile=0.01 rpc_duration_seconds=3102\nprometheus,quantile=0.05 rpc_duration_seconds=3272\nprometheus,quantile=0.5  rpc_duration_seconds=4773\nprometheus,quantile=0.9  rpc_duration_seconds=9001\nprometheus,quantile=0.99 rpc_duration_seconds=76656\nprometheus               rpc_duration_seconds_count=1.7560473e+07,rpc_duration_seconds_sum=2693\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS Settings for Ingest URL in SignalFx Exporter (YAML)\nDESCRIPTION: Example configuration for setting up TLS with a custom certificate authority file for secure connections to the SignalFx ingest endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ningest_tls:\n    ca_file: \"/etc/opt/certs/ca.pem\"\n```\n\n----------------------------------------\n\nTITLE: Parsing SQL Server Execution Plan XML Structure\nDESCRIPTION: This XML document represents a SQL Server execution plan showing query optimization details. It contains physical and logical operations, cost estimates, and execution statistics for a complex query involving table-valued functions and filters on execution statistics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/queryTextAndPlanQueryData.txt#2025-04-10_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<inedValue><ColumnReference Column=\"Expr1005\"/><ScalarOperator ScalarString=\"(0)\"><Const ConstValue=\"(0)\"/></ScalarOperator></DefinedValue><DefinedValue><ColumnReference Column=\"Expr1009\"/><ScalarOperator ScalarString=\"(0)\"><Const ConstValue=\"(0)\"/></ScalarOperator></DefinedValue><DefinedValue><ColumnReference Column=\"Expr1017\"/><ScalarOperator ScalarString=\"NULL\"><Const ConstValue=\"NULL\"/></ScalarOperator></DefinedValue><DefinedValue><ColumnReference Column=\"Expr1018\"/><ScalarOperator ScalarString=\"NULL\"><Const ConstValue=\"NULL\"/></ScalarOperator></DefinedValue><DefinedValue><ColumnReference Column=\"Expr1019\"/><ScalarOperator ScalarString=\"NULL\"><Const ConstValue=\"NULL\"/></ScalarOperator></DefinedValue><DefinedValue><ColumnReference Column=\"Expr1029\"/><ScalarOperator ScalarString=\"NULL\"><Const ConstValue=\"NULL\"/></ScalarOperator></DefinedValue></DefinedValues><RelOp NodeId=\"12\" PhysicalOp=\"Filter\" LogicalOp=\"Filter\" EstimateRows=\"164.317\" EstimateIO=\"0\" EstimateCPU=\"0.00088\" AvgRowSize=\"67\" EstimatedTotalSubtreeCost=\"0.00188016\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"plan_handle\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"execution_count\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"total_worker_time\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"total_elapsed_time\"/></OutputList><Filter StartupExpression=\"0\"><RelOp NodeId=\"13\" PhysicalOp=\"Table-valued function\" LogicalOp=\"Table-valued function\" EstimateRows=\"1000\" EstimateIO=\"0\" EstimateCPU=\"0.00100016\" AvgRowSize=\"75\" EstimatedTotalSubtreeCost=\"0.00100016\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"plan_handle\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"last_execution_time\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"execution_count\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"total_worker_time\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"total_elapsed_time\"/></OutputList><TableValuedFunction><DefinedValues><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"plan_handle\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"last_execution_time\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"execution_count\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"total_worker_time\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"total_elapsed_time\"/></DefinedValue></DefinedValues><Object Table=\"[XTP_STMT_STATS]\"/></TableValuedFunction></RelOp><Predicate><ScalarOperator ScalarString=\"XTP_STMT_STATS.[last_execution_time]&gt;=dateadd(second,[@granularity],getdate()) AND XTP_STMT_STATS.[last_execution_time]&lt;=getdate()\"><Logical Operation=\"AND\"><ScalarOperator><Compare CompareOp=\"GE\"><ScalarOperator><Identifier><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"last_execution_time\"/></Identifier></ScalarOperator><ScalarOperator><Identifier><ColumnReference Column=\"ConstExpr1335\"><ScalarOperator><Intrinsic FunctionName=\"dateadd\"><ScalarOperator><Const ConstValue=\"(8)\"/></ScalarOperator><ScalarOperator><Identifier><ColumnReference Column=\"@granularity\"/></Identifier></ScalarOperator><ScalarOperator><Intrinsic FunctionName=\"getdate\"/></ScalarOperator></Intrinsic></ScalarOperator></ColumnReference></Identifier></ScalarOperator></Compare></ScalarOperator><ScalarOperator><Compare CompareOp=\"LE\"><ScalarOperator><Identifier><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"last_execution_time\"/></Identifier></ScalarOperator><ScalarOperator><Identifier><ColumnReference Column=\"ConstExpr1336\"><ScalarOperator><Intrinsic FunctionName=\"getdate\"/></ScalarOperator></ColumnReference></Identifier></ScalarOperator></Compare></ScalarOperator></Logical></ScalarOperator></Predicate></Filter></RelOp></ComputeScalar></RelOp></Concat></RelOp></Sort></RelOp></StreamAggregate></RelOp></ComputeScalar></RelOp></Top></RelOp><RelOp NodeId=\"14\" PhysicalOp=\"Table-valued function\" LogicalOp=\"Table-valued function\" EstimateRows=\"1\" EstimateIO=\"0\" EstimateCPU=\"1.157e-06\" AvgRowSize=\"4035\" EstimatedTotalSubtreeCost=\"1.157e-06\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[FNGETQUERYPLAN]\" Column=\"query_plan\"/></OutputList><TableValuedFunction><DefinedValues><DefinedValue><ColumnReference Table=\"[FNGETQUERYPLAN]\" Column=\"query_plan\"/></DefinedValue></DefinedValues><Object Table=\"[FNGETQUERYPLAN]\"/><ParameterList><ScalarOperator ScalarString=\"[Expr1161]\"><Identifier><ColumnReference Column=\"Expr1161\"/></Identifier></ScalarOperator></ParameterList></TableValuedFunction></RelOp></NestedLoops></RelOp><RelOp NodeId=\"15\" PhysicalOp=\"Table-valued function\" LogicalOp=\"Table-valued function\" EstimateRows=\"1\" EstimateIO=\"0\" EstimateCPU=\"1.157e-06\" AvgRowSize=\"4035\" EstimatedTotalSubtreeCost=\"1.157e-06\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[FNGETSQL]\" Column=\"text\"/></OutputList><TableValuedFunction><DefinedValues><DefinedValue><ColumnReference Table=\"[FNGETSQL]\" Column=\"text\"/></DefinedValue></DefinedValues><Object Table=\"[FNGETSQL]\"/><ParameterList><ScalarOperator ScalarString=\"[Expr1161]\"><Identifier><ColumnReference Column=\"Expr1161\"/></Identifier></ScalarOperator></ParameterList></TableValuedFunction></RelOp></NestedLoops></RelOp><RelOp NodeId=\"16\" PhysicalOp=\"Concatenation\" LogicalOp=\"Concatenation\" EstimateRows=\"54.1616\" EstimateIO=\"0\" EstimateCPU=\"5.41616e-06\" AvgRowSize=\"15\" EstimatedTotalSubtreeCost=\"0.00150827\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Column=\"Union1246\"/><ColumnReference Column=\"Union1247\"/></OutputList><Concat><DefinedValues><DefinedValue><ColumnReference Column=\"Union1246\"/><ColumnReference Table=\"[QUERY_STATS]\" Column=\"statement_start_offset\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_start_offset\"/></DefinedValue><DefinedValue><ColumnReference Column=\"Union1247\"/><ColumnReference Table=\"[QUERY_STATS]\" Column=\"statement_end_offset\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_end_offset\"/></DefinedValue></DefinedValues><RelOp NodeId=\"17\" PhysicalOp=\"Table-valued function\" LogicalOp=\"Table-valued function\" EstimateRows=\"22.5389\" EstimateIO=\"0\" EstimateCPU=\"2.26959e-05\" AvgRowSize=\"15\" EstimatedTotalSubtreeCost=\"2.26959e-05\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[QUERY_STATS]\" Column=\"statement_start_offset\"/><ColumnReference Table=\"[QUERY_STATS]\" Column=\"statement_end_offset\"/></OutputList><TableValuedFunction><DefinedValues><DefinedValue><ColumnReference Table=\"[QUERY_STATS]\" Column=\"statement_start_offset\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[QUERY_STATS]\" Column=\"statement_end_offset\"/></DefinedValue></DefinedValues><Object Table=\"[QUERY_STATS]\"/><ParameterList><ScalarOperator ScalarString=\"(1)\"><Const ConstValue=\"(1)\"/></ScalarOperator><ScalarOperator ScalarString=\"[Expr1161]\"><Identifier><ColumnReference Column=\"Expr1161\"/></Identifier></ScalarOperator></ParameterList></TableValuedFunction></RelOp><RelOp NodeId=\"18\" PhysicalOp=\"Filter\" LogicalOp=\"Filter\" EstimateRows=\"31.6228\" EstimateIO=\"0\" EstimateCPU=\"0.00048\" AvgRowSize=\"15\" EstimatedTotalSubtreeCost=\"0.00148016\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_start_offset\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_end_offset\"/></OutputList><Filter StartupExpression=\"0\"><RelOp NodeId=\"19\" PhysicalOp=\"Table-valued function\" LogicalOp=\"Table-valued function\" EstimateRows=\"1000\" EstimateIO=\"0\" EstimateCPU=\"0.00100016\" AvgRowSize=\"51\" EstimatedTotalSubtreeCost=\"0.00100016\" Parallel=\"0\" EstimateRebinds=\"0\" EstimateRewinds=\"0\" EstimatedExecutionMode=\"Row\"><OutputList><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_start_offset\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_end_offset\"/><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"plan_handle\"/></OutputList><TableValuedFunction><DefinedValues><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_start_offset\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"statement_end_offset\"/></DefinedValue><DefinedValue><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"plan_handle\"/></DefinedValue></DefinedValues><Object Table=\"[XTP_STMT_STATS]\"/></TableValuedFunction></RelOp><Predicate><ScalarOperator ScalarString=\"XTP_STMT_STATS.[plan_handle]=[Expr1161]\"><Compare CompareOp=\"EQ\"><ScalarOperator><Identifier><ColumnReference Table=\"[XTP_STMT_STATS]\" Column=\"plan_handle\"/></Identifier></ScalarOperator><ScalarOperator><Identifier><ColumnReference Column=\"Expr1161\"/></Identifier></ScalarOperator></Compare></ScalarOperator></Predicate></Filter></RelOp></Concat></RelOp></NestedLoops></RelOp></ComputeScalar></RelOp></QueryPlan></StmtSimple></Statements></Batch></BatchSequence></ShowPlanXML>\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray SQL Subsegment JSON Structure\nDESCRIPTION: Example of an AWS X-Ray subsegment JSON document that captures details of a PostgreSQL database query. The structure includes trace information, timing data, database connection properties, and a sanitized SQL query with parameters replaced by placeholders for security.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/indepSubsegmentWithSql.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"ebdb@aawijb5u25wdoy.cpamxznpdoq8.us-west-2.rds.amazonaws.com\",\n  \"id\": \"3fd8634e78ca9560\",\n  \"start_time\": 1484872218.696,\n  \"end_time\": 1484872218.697,\n  \"namespace\": \"remote\",\n  \"type\" : \"subsegment\",\n  \"trace_id\" : \"1-581cf771-a006649127e371903a2de979\",\n  \"parent_id\" : \"defdfd9912dc5a56\",\n  \"sql\" : {\n    \"url\": \"jdbc:postgresql://aawijb5u25wdoy.cpamxznpdoq8.us-west-2.rds.amazonaws.com:5432/ebdb?myInterceptor=foo\",\n    \"preparation\": \"statement\",\n    \"database_type\": \"PostgreSQL\",\n    \"database_version\": \"9.5.4\",\n    \"driver_version\": \"PostgreSQL 9.4.1211.jre7\",\n    \"user\" : \"dbuser\",\n    \"sanitized_query\" : \"SELECT  *  FROM  customers  WHERE  customer_id=?;\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Recombine Operator for Stack Traces\nDESCRIPTION: Configuration example that recombines stack traces into multiline logs by identifying the first line of a log record based on a pattern match (not starting with whitespace).\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: recombine\n  combine_field: body\n  is_first_entry: body matches \"^[^\\\\s]\"\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes Pod Definition in YAML\nDESCRIPTION: YAML definition of a Kubernetes pod that generates telemetry data. This shows the pod configuration that the processor will use to enrich the trace data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    workload: deployment\n  name: telemetrygen-pod\n  namespace: e2ek8senrichment\n  uid: 038e2267-b473-489b-b48c-46bafdb852eb\nspec:\n  containers:\n  - command:\n    - /telemetrygen\n    - traces\n    - --otlp-insecure\n    - --otlp-endpoint=otelcollector.svc.cluster.local:4317\n    - --duration=10s\n    - --rate=1\n    - --otlp-attributes=k8s.container.name=\"telemetrygen\"\n    image: ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:0.112.0@sha256:b248ef911f93ae27cbbc85056d1ffacc87fd941bbdc2ffd951b6df8df72b8096\n    name: telemetrygen\nstatus:\n  podIP: 10.244.0.11\n```\n\n----------------------------------------\n\nTITLE: Renaming Metric in YAML\nDESCRIPTION: Configuration showing how to rename a metric from 'system.cpu.usage' to 'system.cpu.usage_time'\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: system.cpu.usage\naction: update\nnew_name: system.cpu.usage_time\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Counter Paths\nDESCRIPTION: Extensive listing of SQL Server performance counter paths used to monitor various aspects of SQL Server performance including broker statistics, wait times, resource utilization, query metrics and more.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/counters.txt#2025-04-10_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n\\SQLServer:Broker Activation(*)\\Tasks Running\n\\SQLServer:Broker Activation(*)\\Tasks Aborted/sec\n\\SQLServer:Broker Activation(*)\\Task Limit Reached/sec\n\\SQLServer:Broker Activation(*)\\Task Limit Reached\n\\SQLServer:Broker Activation(*)\\Stored Procedures Invoked/sec\n\\SQLServer:Broker TO Statistics\\Transmission Obj Gets/Sec\n\\SQLServer:Broker TO Statistics\\Transmission Obj Set Dirty/Sec\n\\SQLServer:Broker TO Statistics\\Transmission Obj Writes/Sec\n\\SQLServer:Broker TO Statistics\\Avg. Length of Batched Writes\n\\SQLServer:Broker TO Statistics\\Avg. Time to Write Batch (ms)\n\\SQLServer:Broker TO Statistics\\Avg. Time Between Batches (ms)\n```\n\n----------------------------------------\n\nTITLE: Querying Transformed Metrics in PromQL\nDESCRIPTION: Examples of simplified PromQL queries after transforming resource attributes to metric labels.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: promql\nCODE:\n```\napp_ads_ad_requests_total{namespace=\"my-namespace\"}\n```\n\nLANGUAGE: promql\nCODE:\n```\nsum by (namespace) (app_ads_ad_requests_total)\n```\n\n----------------------------------------\n\nTITLE: Configuring MongoDB Atlas Receiver for Event Collection in YAML\nDESCRIPTION: Configuration to receive events from MongoDB Atlas for both projects and organizations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mongodbatlasreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  mongodbatlas:\n    events:\n      projects:\n        - name: \"project 1\"\n      organizations:\n        - id: \"5b478b3afc4625789ce616a3\"\n      poll_interval: 1m\n      page_size: 100\n      max_pages: 25\n    storage: file_storage\n```\n\n----------------------------------------\n\nTITLE: Adding Labels via Issue Comments Example\nDESCRIPTION: Example command showing how to add and remove labels on GitHub issues using comment commands. The command adds 'receiver/prometheus' and 'help-wanted' labels while removing 'exporter/prometheus' label.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n/label receiver/prometheus help-wanted -exporter/prometheus\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Parser with Epoch Layout in YAML\nDESCRIPTION: This snippet demonstrates the configuration for a time parser using the 'epoch' layout type. It parses a timestamp from the 'body.timestamp_field' interpreting it as seconds since the epoch.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- type: time_parser\n  parse_from: body.timestamp_field\n  layout_type: epoch\n  layout: s\n```\n\n----------------------------------------\n\nTITLE: Configuring File Storage Extension in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the File Storage extension in OpenTelemetry Collector. Shows both basic setup and advanced configuration with compaction settings, demonstrating how to integrate it with service pipelines.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/storage/filestorage/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  file_storage:\n  file_storage/all_settings:\n    directory: /var/lib/otelcol/mydir\n    timeout: 1s\n    compaction:\n      on_start: true\n      directory: /tmp/\n      max_transaction_size: 65_536\n    fsync: false\n\nservice:\n  extensions: [file_storage, file_storage/all_settings]\n  pipelines:\n    traces:\n      receivers: [nop]\n      exporters: [nop]\n\n# Data pipeline is required to load the config.\nreceivers:\n  nop:\nexporters:\n  nop:\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example: Prometheus-v1 Metrics\nDESCRIPTION: Example of Prometheus-v1 metrics in InfluxDB Line Protocol format. It includes gauge, counter, and histogram metrics with various labels and values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/influxdbreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: influxdb\nCODE:\n```\ncpu_temp,foo=bar gauge=87.332\nhttp_requests_total,method=post,code=200 counter=1027\nhttp_requests_total,method=post,code=400 counter=3\nhttp_request_duration_seconds 0.05=24054,0.1=33444,0.2=100392,0.5=129389,1=133988,sum=53423,count=144320\nrpc_duration_seconds 0.01=3102,0.05=3272,0.5=4773,0.9=9001,0.99=76656,sum=1.7560473e+07,count=2693\n```\n\n----------------------------------------\n\nTITLE: Exporter Connection Settings Mapping Table\nDESCRIPTION: Markdown table showing the mapping between OpAMP ConnectionSettings fields and corresponding Collector exporter settings. Details how connection and TLS certificate configurations are translated.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/opampsupervisor/specification/README.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| **ConnectionSettings**    | **Exporter setting** |\n|---------------------------|----------------------|\n| destination_endpoint      | endpoint             |\n| headers                   | headers              |\n| certificate.public_key    | tls.cert_file        |\n| certificate.private_key   | tls.key_file         |\n| certificate.ca_public_key | tls.ca_file          |\n```\n\n----------------------------------------\n\nTITLE: Running ClickHouse Integration Tests\nDESCRIPTION: Shell command for running integration tests for the ClickHouse exporter with the integration tag.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\ngo test -tags integration -run=TestIntegration\n```\n\n----------------------------------------\n\nTITLE: Reading File Storage Extension Contents with strings Utility\nDESCRIPTION: Example showing how to use the strings utility to read raw contents of a file created by the File Storage extension for the filelog receiver. The output shows JSON data containing fingerprints, offsets, and file attributes for tracked log files.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/storage/filestorage/README.md#2025-04-10_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ strings /tmp/otelcol/file_storage/filelogreceiver/receiver_filelog_\ndefault\nfile_input.knownFiles2\n{\"Fingerprint\":{\"first_bytes\":\"MzEwNzkKMjE5Cg==\"},\"Offset\":10,\"FileAttributes\":{\"log.file.name\":\"1.log\"},\"HeaderFinalized\":false,\"FlushState\":{\"LastDataChange\":\"2024-03-20T18:16:18.164331-07:00\",\"LastDataLength\":0}}\n{\"Fingerprint\":{\"first_bytes\":\"MjQ0MDMK\"},\"Offset\":6,\"FileAttributes\":{\"log.file.name\":\"2.log\"},\"HeaderFinalized\":false,\"FlushState\":{\"LastDataChange\":\"2024-03-20T18:16:39.96429-07:00\",\"LastDataLength\":0}}\ndefault\nfile_input.knownFiles2\n{\"Fingerprint\":{\"first_bytes\":\"MzEwNzkKMjE5Cg==\"},\"Offset\":10,\"FileAttributes\":{\"log.file.name\":\"1.log\"},\"HeaderFinalized\":false,\"FlushState\":{\"LastDataChange\":\"2024-03-20T18:16:18.164331-07:00\",\"LastDataLength\":0}}\n{\"Fingerprint\":{\"first_bytes\":\"MjQ0MDMK\"},\"Offset\":6,\"FileAttributes\":{\"log.file.name\":\"2.log\"},\"HeaderFinalized\":false,\"FlushState\":{\"LastDataChange\":\"2024-03-20T18:16:39.96429-07:00\",\"LastDataLength\":0}}\ndefault\nfile_input.knownFiles2\n{\"Fingerprint\":{\"first_bytes\":\"MzEwNzkKMjE5Cg==\"},\"Offset\":10,\"FileAttributes\":{\"log.file.name\":\"1.log\"},\"HeaderFinalized\":false,\"FlushState\":{\"LastDataChange\":\"2024-03-20T18:16:18.164331-07:00\",\"LastDataLength\":0}}\n{\"Fingerprint\":{\"first_bytes\":\"MjQ0MDMK\"},\"Offset\":6,\"FileAttributes\":{\"log.file.name\":\"2.log\"},\"HeaderFinalized\":false,\"FlushState\":{\"LastDataChange\":\"2024-03-20T18:16:39.96429-07:00\",\"LastDataLength\":0}}\n```\n\n----------------------------------------\n\nTITLE: Executing SHOW GLOBAL STATUS Command in MySQL\nDESCRIPTION: A log entry showing the execution of the SHOW GLOBAL STATUS command in MySQL. The command returns server status variables and their values. The log format includes the user, a hash value, and several numeric fields that likely represent execution metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/testdata/scraper/statement_events.txt#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSHOW GLOBAL STATUS\n```\n\n----------------------------------------\n\nTITLE: Renaming OTTL Package Components\nDESCRIPTION: Breaking changes renaming OTTL package components for consistency\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_36\n\nLANGUAGE: yaml\nCODE:\n```\npkg/ottl:\n- Rename ottldatapoints to ottldatapoint\n- Rename ottllogs to ottllog\n- Rename ottltraces to ottlspan\n```\n\n----------------------------------------\n\nTITLE: Output Log Entry Structure\nDESCRIPTION: Example JSON showing the structure of an output log entry after field modifications.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/field.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"attributes\": {\n    \"my_attribute\": \"my_attribute_value\"\n  },\n  \"body\": {\n    \"key1\": \"value1\",\n    \"key2\": {\n      \"nested_key2\": \"nested_value2\"\n    },\n    \"key3\": \"value3\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Output JSON after Epoch Layout Parsing\nDESCRIPTION: This JSON shows the output entry after parsing the timestamp using the Epoch layout. The timestamp field is now populated with the parsed and formatted date derived from the epoch value.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2006-01-02T15:04:05-07:00\",\n  \"body\": {\n    \"timestamp_field\": 1136214245\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Huawei Cloud SDK Environment Variables\nDESCRIPTION: Shell commands for setting up the required environment variables for Huawei Cloud SDK authentication using access and secret keys.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/huaweicloudcesreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport HUAWEICLOUD_SDK_AK=your-access-key\nexport HUAWEICLOUD_SDK_SK=your-secret-key\n```\n\nLANGUAGE: sh\nCODE:\n```\necho $HUAWEICLOUD_SDK_AK\necho $HUAWEICLOUD_SDK_SK\n```\n\n----------------------------------------\n\nTITLE: Basic Google Pubsub Receiver Configuration in YAML\nDESCRIPTION: Basic configuration example showing how to set up the Google Pubsub receiver with project, subscription, and encoding settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/googlecloudpubsubreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  googlecloudpubsub:\n    project: otel-project\n    subscription: projects/otel-project/subscriptions/otlp-logs\n    encoding: raw_json\n```\n\n----------------------------------------\n\nTITLE: Multiline File Input Example with Input and Output\nDESCRIPTION: Shows how the multiline file_input configuration processes a log file containing multiple lines that belong to the same logical log entry. Lines are grouped based on the line_start_pattern parameter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/file_input.md#2025-04-10_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nSTART log1\nlog2\nSTART log3\nlog4\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"START log1\\nlog2\\n\"\n},\n{\n  \"body\": \"START log3\\nlog4\\n\"\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Loki Server in Bash\nDESCRIPTION: Command to start the Loki server using the Linux AMD64 binary. Assumes the configuration file is in the current directory.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/CONTRIBUTING.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ loki-linux-amd64\n```\n\n----------------------------------------\n\nTITLE: Defining AssumeRole Configuration Structure\nDESCRIPTION: Defines the structure for AWS role assumption configuration including ARN and session name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\ntype AssumeRole struct {\n\tARN                  string `mapstructure:\"arn,omitempty\"`\n\tSessionName          string `mapstructure:\"session_name,omitempty\"`\n}\n```\n\n----------------------------------------\n\nTITLE: ClickHouse Schema Migration - Adding AggregationTemporality Column\nDESCRIPTION: SQL statements to alter ClickHouse tables by adding AggregationTemporality column to exponential histogram and histogram tables, and renaming AggTemp column in sum table. This is required when upgrading to the latest version.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE otel_metrics_exponential_histogram ADD COLUMN AggregationTemporality Int32 CODEC(ZSTD(1));\nALTER TABLE otel_metrics_histogram ADD COLUMN AggregationTemporality Int32 CODEC(ZSTD(1));\nALTER TABLE otel_metrics_sum RENAME COLUMN AggTemp TO AggregationTemporality;\n```\n\n----------------------------------------\n\nTITLE: Simple File Input Example with Input and Output\nDESCRIPTION: Demonstrates how the simple file_input configuration processes a log file with three separate log lines. Each line is parsed as an individual log entry with its own body field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/file_input.md#2025-04-10_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nlog1\nlog2\nlog3\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": \"log1\"\n},\n{\n  \"body\": \"log2\"\n},\n{\n  \"body\": \"log3\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using TraceID Converter for OpenTelemetry Trace Processing\nDESCRIPTION: The TraceID Converter creates a pdata.TraceID struct from a given byte slice. It requires exactly 16 bytes as input to construct a valid trace identifier.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_62\n\nLANGUAGE: go\nCODE:\n```\nTraceID(bytes)\n```\n\n----------------------------------------\n\nTITLE: Journald Input Priority Range Configuration\nDESCRIPTION: Configuration example demonstrating how to set up journald_input with a priority range filter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/journald_input.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n  priority: emerg..err\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Metrics in YAML Configuration\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable specific metrics emitted by the TLSCheck component. Replace <metric_name> with the name of the metric you want to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/tlscheckreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Initializing Couchbase Configuration\nDESCRIPTION: Execute setup script to initialize Couchbase and create required buckets.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/couchbase/README.md#2025-04-10_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./scripts/setup.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring File System Scraper in Host Metrics Receiver\nDESCRIPTION: Configuration for the filesystem scraper that allows filtering by devices, filesystem types, and mount points with additional option to include virtual filesystems.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nfilesystem:\n  <include_devices|exclude_devices>:\n    devices: [ <device name>, ... ]\n    match_type: <strict|regexp>\n  <include_fs_types|exclude_fs_types>:\n    fs_types: [ <filesystem type>, ... ]\n    match_type: <strict|regexp>\n  <include_mount_points|exclude_mount_points>:\n    mount_points: [ <mount point>, ... ]\n    match_type: <strict|regexp>\n  include_virtual_filesystems: <false|true>\n```\n\n----------------------------------------\n\nTITLE: Configuring SpanMetrics Connector with Virtual Node Peer Attributes\nDESCRIPTION: Configuration example for the servicegraphprocessor showing how to set virtual_node_peer_attributes to include db.name and rpc.service attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_33\n\nLANGUAGE: yaml\nCODE:\n```\nvirtual_node_peer_attributes: [db.name, rpc.service]\n```\n\n----------------------------------------\n\nTITLE: Sent Batches Metric Definition\nDESCRIPTION: Markdown table defining the otelcol_exporter_prometheusremotewrite_sent_batches metric that tracks remote write request batches\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/documentation.md#2025-04-10_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| {batch} | Sum | Int | true |\n```\n\n----------------------------------------\n\nTITLE: Specifying ByteSize Using Scientific Notation in YAML\nDESCRIPTION: Shows how to specify 5000 bytes using scientific notation (5e3) in YAML configuration. This represents 5 × 10³ bytes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/bytesize.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: some_operator\n  bytes: 5e3\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Histogram Conversion - First Scrape\nDESCRIPTION: Go code showing how the Prometheus histogram is converted to OpenTelemetry format with distribution values and buckets\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_7\n\nLANGUAGE: go\nCODE:\n```\nmetrics := []*metricspb.Metric{\n  {\n    MetricDescriptor: &metricspb.MetricDescriptor{\n      Name:      \"hist_test\",\n      Type:      metricspb.MetricDescriptor_CUMULATIVE_DISTRIBUTION,\n      LabelKeys: []*metricspb.LabelKey{{Key: \"t1\"}}},\n    Timeseries: []*metricspb.TimeSeries{\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"1\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: startTimestamp, Value: &metricspb.Point_DistributionValue{\n            DistributionValue: &metricspb.DistributionValue{\n              BucketOptions: &metricspb.DistributionValue_BucketOptions{\n                Type: &metricspb.DistributionValue_BucketOptions_Explicit_{\n                  Explicit: &metricspb.DistributionValue_BucketOptions_Explicit{\n                    Bounds: []float64{10, 20},\n                  },\n                },\n              },\n              Count:   10,\n              Sum:     100.0,\n              Buckets: []*metricspb.DistributionValue_Bucket{{Count: 1}, {Count: 2}, {Count: 7}},\n            }}},\n        },\n      },\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"2\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: startTimestamp, Value: &metricspb.Point_DistributionValue{\n            DistributionValue: &metricspb.DistributionValue{\n              BucketOptions: &metricspb.DistributionValue_BucketOptions{\n                Type: &metricspb.DistributionValue_BucketOptions_Explicit_{\n                  Explicit: &metricspb.DistributionValue_BucketOptions_Explicit{\n                    Bounds: []float64{10, 20},\n                  },\n                },\n              },\n              Count:   100,\n              Sum:     10000.0,\n              Buckets: []*metricspb.DistributionValue_Bucket{{Count: 10}, {Count: 20}, {Count: 70}},\n            }}},\n        },\n      },\n    },\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Using BoundedQueue in Go\nDESCRIPTION: Example showing how to create and use a BoundedQueue object to manage resource admission. The queue is initialized with maximum byte limits and waiting thresholds. It returns a closure for resource release when acquisition succeeds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/otelarrow/admission2/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nbq := admission.NewBoundedQueue(maxLimitBytes, maxLimitWaiting)\n```\n\nLANGUAGE: go\nCODE:\n```\nbq.Acquire(ctx, requestSize)\n```\n\n----------------------------------------\n\nTITLE: Configuring Recombine with max_unmatched_batch_size set to 3 (is_first_entry)\nDESCRIPTION: Configuration example where max_unmatched_batch_size is set to 3 with is_first_entry, causing entries to be combined into 3-entry-packages until the match occurs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- type: recombine\n  combine_field: body\n  is_first_entry: body == 'log1'\n  max_unmatched_batch_size: 3\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Filters for Google Managed Prometheus in YAML\nDESCRIPTION: Example YAML configuration that adds important resource attributes as metric labels to resolve timeseries collision issues in the Google Managed Prometheus exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlemanagedprometheusexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n  googlemanagedprometheus:\n    metric:\n      resource_filters:\n      - prefix: \"cloud\"\n      - prefix: \"k8s\"\n      - prefix: \"faas\"\n      - regex: \"container.id\"\n      - regex: \"process.pid\"\n      - regex: \"host.name\"\n      - regex: \"host.id\"\n```\n\n----------------------------------------\n\nTITLE: Podman API Version Configuration\nDESCRIPTION: Configuration example showing how to specify a custom API version for compatibility with older Podman versions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/podmanreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  podman_stats:\n    endpoint: unix://run/podman/podman.sock\n    api_version: 3.2.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Volume Mount for File Exporter\nDESCRIPTION: Shell commands to set up a writable directory for the file exporter in a Docker container, including setting proper permissions and mounting volumes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/fileexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir --mode o+rwx file-exporter\ndocker run -v \"./file-exporter:/file-exporter:rwz\" -v \"otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml\" otel/opentelemetry-collector-contrib:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Explorer Exporter with Default Authentication in YAML\nDESCRIPTION: Shows how to enable the new 'use_default_auth' option in the Azure Data Explorer exporter to leverage workload identity for authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  azuredataexplorer:\n    use_default_auth: true\n    # Other configuration options\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Metrics in YAML\nDESCRIPTION: YAML configuration snippet to disable default metrics. Each metric can be individually disabled by setting enabled: false.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/expvarreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metric Name Normalization\nDESCRIPTION: Feature gate configuration for normalizing Prometheus metric names according to OpenTelemetry specs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_40\n\nLANGUAGE: go\nCODE:\n```\npkg.translator.prometheus.NormalizeName\n```\n\n----------------------------------------\n\nTITLE: UserAgent Parser Result Example in YAML\nDESCRIPTION: Example output from the UserAgent parser function when parsing a curl user-agent string. The parser returns structured data with the user agent name, version, and original string.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_70\n\nLANGUAGE: yaml\nCODE:\n```\n\"user_agent.name\": \"curl\"\n\"user_agent.version\": \"7.81.0\"\n\"user_agent.original\": \"curl/7.81.0\"\n```\n\n----------------------------------------\n\nTITLE: Parsing SQL Server Performance Metrics in JSON Format\nDESCRIPTION: This JSON structure represents various SQL Server performance metrics. Each object contains properties like counter name, type, instance, measurement, object, SQL instance, computer name, and value. The metrics cover areas such as database performance, exec statistics, general statistics, latches, locks, and memory usage.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"counter\":\"Transactions/sec\",\n  \"counter_type\":\"272696576\",\n  \"instance\":\"tempdb\",\n  \"measurement\":\"sqlserver_performance\",\n  \"object\":\"SQLServer:Databases\",\n  \"sql_instance\":\"8cac97ac9b8f\",\n  \"computer_name\":\"abcde\",\n  \"value\":\"2335\"\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Regex Parsing Configuration\nDESCRIPTION: Configuration example showing how to parse a message field using regex to extract host and type values into attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_parser.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_parser\n  parse_from: body.message\n  regex: '^Host=(?P<host>[^,]+), Type=(?P<type>.*)$'\n```\n\n----------------------------------------\n\nTITLE: Shutting Down Secure Tracing Services\nDESCRIPTION: Shell command to stop and remove the Docker containers running the secure tracing infrastructure.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n$ docker compose down\n```\n\n----------------------------------------\n\nTITLE: Testing User Permissions for Journald Access in Linux\nDESCRIPTION: This shell command demonstrates how to test if a user (in this case, otelcol-contrib) has sufficient permissions to access journald logs. It uses the 'sudo' command to switch to the specified user and attempts to read the last 5 lines of the journal.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/journaldreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nsudo su -s /bin/bash -c 'journalctl --lines 5' otelcol-contrib\n```\n\n----------------------------------------\n\nTITLE: Conditional Regex Parsing Configuration\nDESCRIPTION: Configuration example showing conditional regex parsing based on a type field value using an if expression.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_parser.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: regex_parser\n  regex: '^Host=(?<host>)$'\n  parse_from: body.message\n  if: 'body.type == \"hostname\"'\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple Prometheus Receiver with Job Name in YAML\nDESCRIPTION: Shows how to set the 'job_name' configuration option in the Simple Prometheus Receiver, which is now supported for better metric labeling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  simpleprometheus:\n    job_name: \"my_custom_job\"\n    # Other configuration options\n```\n\n----------------------------------------\n\nTITLE: Container-Specific Log Configuration Example\nDESCRIPTION: Example of container-specific log configuration using OpenTelemetry annotations. Shows how to configure log collection parameters for a specific container named 'busybox'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nio.opentelemetry.discovery.logs.busybox/config: |\n  max_log_size: \"3MiB\"\n  operators:\n    - type: container\n      id: container-parser\n    - id: some\n      type: add\n      field: attributes.tag\n      value: hints\n```\n\n----------------------------------------\n\nTITLE: Viewing OpenTelemetry Collector Logs\nDESCRIPTION: Command to view the logs from the OpenTelemetry Collector container, which displays the metrics that have been collected and processed.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/simpleprometheusreceiver/examples/federation/README.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker logs otelcollector\n```\n\n----------------------------------------\n\nTITLE: Redis Server Status Output Format\nDESCRIPTION: Detailed Redis server status output showing server configuration, memory usage, persistence settings, statistics, replication status, CPU metrics, cluster status, keyspace statistics and command stats. The output is organized in sections with key-value pairs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/redisreceiver/testdata/info.txt#2025-04-10_snippet_0\n\nLANGUAGE: redis\nCODE:\n```\n# Server\nredis_version:5.0.7\nredis_git_sha1:00000000\nredis_git_dirty:0\nredis_build_id:825c96d6c798641\nredis_mode:standalone\nos:Linux 4.19.76-linuxkit x86_64\narch_bits:64\nmultiplexing_api:epoll\natomicvar_api:atomic-builtin\ngcc_version:8.3.0\nprocess_id:1\nrun_id:a3c8e3547fa3f13672342d4ce489e6061ff14c7d\ntcp_port:6379\nuptime_in_seconds:104946\nuptime_in_days:1\nhz:10\nconfigured_hz:10\nlru_clock:6474178\nexecutable:/data/redis-server\nconfig_file:\n\n# Clients\nconnected_clients:1\nclient_recent_max_input_buffer:2\nclient_recent_max_output_buffer:0\nblocked_clients:0\n\n# Memory\nused_memory:854160\nused_memory_human:834.14K\nused_memory_rss:5562368\nused_memory_rss_human:5.30M\nused_memory_peak:875064\nused_memory_peak_human:854.55K\nused_memory_peak_perc:97.61%\nused_memory_overhead:840958\nused_memory_startup:791264\nused_memory_dataset:13202\nused_memory_dataset_perc:20.99%\nallocator_allocated:862792\nallocator_active:1073152\nallocator_resident:8687616\ntotal_system_memory:2086154240\ntotal_system_memory_human:1.94G\nused_memory_lua:37888\nused_memory_lua_human:37.00K\nused_memory_scripts:0\nused_memory_scripts_human:0B\nnumber_of_cached_scripts:0\nmaxmemory:0\nmaxmemory_human:0B\nmaxmemory_policy:noeviction\nallocator_frag_ratio:1.24\nallocator_frag_bytes:210360\nallocator_rss_ratio:8.10\nallocator_rss_bytes:7614464\nrss_overhead_ratio:0.64\nrss_overhead_bytes:-3125248\nmem_fragmentation_ratio:7.03\nmem_fragmentation_bytes:4771088\nmem_not_counted_for_evict:0\nmem_replication_backlog:0\nmem_clients_slaves:0\nmem_clients_normal:49694\nmem_aof_buffer:0\nmem_allocator:jemalloc-5.1.0\nactive_defrag_running:0\nlazyfree_pending_objects:0\n\n# Persistence\nloading:0\nrdb_changes_since_last_save:0\nrdb_bgsave_in_progress:0\nrdb_last_save_time:1583427536\nrdb_last_bgsave_status:ok\nrdb_last_bgsave_time_sec:-1\nrdb_current_bgsave_time_sec:-1\nrdb_last_cow_size:0\naof_enabled:0\naof_rewrite_in_progress:0\naof_rewrite_scheduled:0\naof_last_rewrite_time_sec:-1\naof_current_rewrite_time_sec:-1\naof_last_bgrewrite_status:ok\naof_last_write_status:ok\naof_last_cow_size:0\n\n# Stats\ntotal_connections_received:28\ntotal_commands_processed:30\ninstantaneous_ops_per_sec:0\ntotal_net_input_bytes:8407\ntotal_net_output_bytes:26604\ninstantaneous_input_kbps:0.00\ninstantaneous_output_kbps:0.00\nrejected_connections:0\nsync_full:0\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0\nexpired_stale_perc:0.00\nexpired_time_cap_reached_count:0\nevicted_keys:0\nkeyspace_hits:0\nkeyspace_misses:0\npubsub_channels:0\npubsub_patterns:0\nlatest_fork_usec:0\nmigrate_cached_sockets:0\nslave_expires_tracked_keys:0\nactive_defrag_hits:0\nactive_defrag_misses:0\nactive_defrag_key_hits:0\nactive_defrag_key_misses:0\n\n# Replication\nrole:master\nconnected_slaves:0\nmaster_replid:29fed19c4c45f24e289b2ac7917131fd4a9326e0\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:0\nsecond_repl_offset:-1\nrepl_backlog_active:0\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:0\nrepl_backlog_histlen:0\n\n# CPU\nused_cpu_sys:185.649184\nused_cpu_user:46.396430\nused_cpu_sys_children:0.002354\nused_cpu_user_children:0.001619\nused_cpu_sys_main_thread:180.0\nused_cpu_user_main_thread:42.39\n\n# Cluster\ncluster_enabled:0\n\n# Keyspace\ndb0:keys=1,expires=2,avg_ttl=3\ndb1:keys=4,expires=5,avg_ttl=6\ndb3:keys=5,expires=10,avg_ttl=85\n\n# Commandstats\ncmdstat_ssubscribe:calls=0,usec=0,usec_per_call=0.00,rejected_calls=3,failed_calls=0\ncmdstat_zrem:calls=641,usec=1884,usec_per_call=2.94,rejected_calls=0,failed_calls=0\ncmdstat_incrbyfloat:calls=50340,usec=902409,usec_per_call=17.93,rejected_calls=0,failed_calls=0\ncmdstat_lrange:calls=12495,usec=48886,usec_per_call=3.91,rejected_calls=0,failed_calls=14\ncmdstat_mset:calls=4505,usec=65693,usec_per_call=14.58,rejected_calls=0,failed_calls=3\n```\n\n----------------------------------------\n\nTITLE: Configuring Move Operator to Rename Value in YAML\nDESCRIPTION: This snippet demonstrates how to use the move operator to rename a field within the body of a log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: move\n  from: body.key1\n  to: body.key3\n```\n\n----------------------------------------\n\nTITLE: Prometheus Remote Write Exporter Metrics Table\nDESCRIPTION: Markdown table defining the otelcol_exporter_prometheusremotewrite_consumers metric that tracks the number of configured workers\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| {consumer} | Sum | Int | false |\n```\n\n----------------------------------------\n\nTITLE: Implementing RoundTripper Method\nDESCRIPTION: Creates a custom HTTP round tripper for request signing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nfunc (sa *sigv4Auth) RoundTripper(base http.RoundTripper) (http.RoundTripper, error) {\n    rt := signingRoundTripper {\n        transport: base\n        signer:\n        region:\n        service:\n        credsProvider:\n        awsSDKInfo:\n        logger:\n    }\n    return &rt, nil\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Simple Trace Data Export to AlibabaCloud LogService\nDESCRIPTION: This YAML configuration sets up a pipeline to export trace data from an example receiver to AlibabaCloud LogService. It specifies the required parameters such as endpoint, project, logstore, and access credentials.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/alibabacloudlogserviceexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  examplereceiver:\n\nexporters:\n  alibabacloud_logservice:\n    endpoint: \"cn-hangzhou.log.aliyuncs.com\"\n    project: \"demo-project\"\n    logstore: \"traces-store\"\n    access_key_id: \"access-key-id\"\n    access_key_secret: \"access-key-secret\"\n\nservice:\n  pipelines:\n    traces:\n      receivers: [examplereceiver]\n      exporters: [alibabacloud_logservice]\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL receiver connection pooling\nDESCRIPTION: Example showing how to enable the connection pooling feature gate in the PostgreSQL receiver. This change allows reusing database connections instead of recreating connections on each scrape.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_24\n\nLANGUAGE: markdown\nCODE:\n```\n- `postgresqlreceiver`: Add `receiver.postgresql.connectionPool` feature gate to reuse database connections (#30831)\n  The default implementation recreates and closes connections on each scrape per database configured/discovered.\n  This change offers a feature gated alternative to keep connections open. Also, it exposes connection configuration to control the behavior of the pool.\n```\n\n----------------------------------------\n\nTITLE: Creating ServiceAccount\nDESCRIPTION: Kubernetes manifest for creating a ServiceAccount for the OpenTelemetry Collector.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8seventsreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: otelcontribcol\n  name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Disabling Metrics Configuration in YAML for Podman Stats Receiver\nDESCRIPTION: YAML configuration example showing how to disable specific metrics in the podman_stats receiver. This pattern can be applied to any of the default metrics listed in the documentation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/podmanreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Merging a Layer to Body in YAML\nDESCRIPTION: This configuration shows how to merge a nested object into the main body of a log entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- type: move\n  from: body.object\n  to: body\n```\n\n----------------------------------------\n\nTITLE: Configuring Allowlist and Denylist for Nested Attributes in YAML\nDESCRIPTION: Configuration for controlling which attribute prefixes can be nested with include and exclude lists. This example shows how to allow nesting for 'kubernetes.host.' attributes except for those with the 'kubernetes.host.naming' prefix.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nnest_attributes:\n  enabled: true\n  include: [\"kubernetes.host.\"]\n  exclude: [\"kubernetes.host.naming\"]\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray DynamoDB Error Trace Segment\nDESCRIPTION: A JSON trace segment from AWS X-Ray showing a failed DynamoDB PutItem operation. The trace includes detailed error information, stack traces, timing data, and request metadata. The operation failed because the target table 'does_not_exist' was not found.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/ddbSample.txt#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": \"71631df3f58bdfc5\",\n    \"name\": \"dynamodb\",\n    \"start_time\": 1596566305.5874245,\n    \"end_time\": 1596566305.5928326,\n    \"fault\": true,\n    \"cause\": {\n        \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n        \"exceptions\": [\n            {\n                \"id\": \"7121b882a0ef44da\",\n                \"type\": \"dynamodb.ResourceNotFoundException\",\n                \"message\": \"ResourceNotFoundException: Requested resource not found\",\n                \"stack\": [...],\n                \"remote\": true\n            }\n        ]\n    },\n    \"namespace\": \"aws\",\n    \"http\": {\n        \"response\": {\n            \"status\": 400,\n            \"content_length\": 112\n        }\n    },\n    \"aws\": {\n        \"consumed_capacity\": null,\n        \"item_collection_metrics\": null,\n        \"operation\": \"PutItem\",\n        \"region\": \"us-west-2\",\n        \"request_id\": \"TJUJNR0JV84CFHJL93D3GIA0LBVV4KQNSO5AEMVJF66Q9ASUAAJG\",\n        \"retries\": 0,\n        \"table_name\": \"does_not_exist\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Unregistering Custom Capability in OpenTelemetry Collector Extension\nDESCRIPTION: This code shows how to unregister a custom capability handler when a component is done processing messages or shutting down. After unregistering, the handler will no longer receive messages from the OpAMP server.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/opampcustommessages/README.md#2025-04-10_snippet_3\n\nLANGUAGE: go\nCODE:\n```\nhandler.Unregister()\n```\n\n----------------------------------------\n\nTITLE: Coralogix Domain Configuration with Compression\nDESCRIPTION: Example showing how to configure zstd compression for the Coralogix exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  coralogix:\n    domain_settings:\n      compression: \"zstd\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Property Exclusion Filter in SignalFx Exporter (YAML)\nDESCRIPTION: Example configuration to filter specific properties from dimension updates. This example shows how to filter all 'k8s.workload.name' properties from 'k8s.pod.uid' dimension updates.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# will filter all 'k8s.workload.name' properties from 'k8s.pod.uid' dimension updates:\nexclude_properties:\n  - dimension_name: k8s.pod.uid\n    property_name: k8s.workload.name\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - otelcol_otel_arrow_receiver_in_flight_bytes Metric\nDESCRIPTION: Metric table showing details for bytes in flight measurement. The metric is a non-monotonic Sum type with Int values measured in bytes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| By | Sum | Int | false |\n```\n\n----------------------------------------\n\nTITLE: Input/Output JSON Example for ANSI Code Removal\nDESCRIPTION: Example showing how the regex_replace operator removes ANSI color escape codes from a JSON log entry using a well-known regex pattern.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/regex_replace.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"\\x1b[31mred\\x1b[0m\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": \"red\"\n}\n```\n\n----------------------------------------\n\nTITLE: Output JSON After Attribute Aggregation\nDESCRIPTION: The result of aggregating attributes with the 'pod_' prefix into a new 'pods' object, with the prefix removed from each original key.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"pods\": {\n    \"a\": \"x\",\n    \"b\": \"y\",\n    \"c\": \"z\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Prometheus Summary to OpenTelemetry Summary Metric in Go\nDESCRIPTION: This code snippet demonstrates how to create an OpenTelemetry Summary metric from Prometheus Summary data. It includes the metric descriptor, time series, and point value with summary details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_10\n\nLANGUAGE: go\nCODE:\n```\nmetrics := []*metricspb.Metric{\n  {\n    MetricDescriptor: &metricspb.MetricDescriptor{\n      Name:      \"go_gc_duration_seconds\",\n      Type:      metricspb.MetricDescriptor_SUMMARY,\n      LabelKeys: []*metricspb.LabelKey{}},\n    Timeseries: []*metricspb.TimeSeries{\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{},\n        Points: []*metricspb.Point{\n          {Timestamp: currentTimestamp, Value: &metricspb.Point_SummaryValue{\n            SummaryValue: &metricspb.SummaryValue{\n\t\t\t  Sum:   &wrappers.DoubleValue{Value: 17.491350544},\n\t\t\t  Count: &wrappers.Int64Value{Value: 52490},\n              Snapshot: &metricspb.SummaryValue_Snapshot{\n                PercentileValues: []*metricspb.SummaryValue_Snapshot_ValueAtPercentile{\n                  {Percentile: 0.0,   Value: 0.0001271},\n                  {Percentile: 25.0,  Value: 0.0002455},\n                  {Percentile: 50.0,  Value: 0.0002904},\n                  {Percentile: 75.0,  Value: 0.0003426},\n                  {Percentile: 100.0, Value: 0.0023639},\n                },\n              }}}},\n        },\n      },\n    },\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Metrics with Resource Attributes in PromQL\nDESCRIPTION: Examples of PromQL queries for selecting and grouping metrics by resource attributes using the target_info metric.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: promql\nCODE:\n```\napp_ads_ad_requests_total * on (job, instance) group_left target_info{k8s_namespace_name=\"my-namespace\"}\n```\n\nLANGUAGE: promql\nCODE:\n```\nsum by (k8s_namespace_name) (app_ads_ad_requests_total * on (job, instance) group_left(k8s_namespace_name) target_info)\n```\n\n----------------------------------------\n\nTITLE: Querying Sampling Decision Timer Latency in Prometheus\nDESCRIPTION: This Prometheus query measures the latency of sampling a batch of traces and passing sampled traces through the collector pipeline. High latencies can lead to traces being dropped before sampling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: prometheus\nCODE:\n```\notelcol_processor_tail_sampling_sampling_decision_timer_latency\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example for Logs\nDESCRIPTION: Example of how logs are represented in InfluxDB line protocol format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/influxdbexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nlogs fluent.tag=\"fluent.info\",pid=18i,ppid=9i,worker=0i 1613769568895331700\nlogs fluent.tag=\"fluent.debug\",instance=1720i,queue_size=0i,stage_size=0i 1613769568895697200\nlogs fluent.tag=\"fluent.info\",worker=0i 1613769568896515100\n```\n\n----------------------------------------\n\nTITLE: Markdown Table - otelcol_otel_arrow_receiver_in_flight_requests Metric\nDESCRIPTION: Metric table showing details for requests in flight measurement. The metric is a non-monotonic Sum type with Int values counted in units of 1.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/documentation.md#2025-04-10_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| 1 | Sum | Int | false |\n```\n\n----------------------------------------\n\nTITLE: Input and Output JSON for Key-Value Parsing with Timestamp Extraction\nDESCRIPTION: Example of input and output JSON for key-value parsing with timestamp extraction from an epoch field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"name=stanza seconds_since_epoch=1136214245\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2006-01-02T15:04:05-07:00\",\n  \"body\": {\n    \"name\": \"stanza\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using FNV Hash Converter in Go\nDESCRIPTION: The FNV converter computes an FNV hash/digest of the provided value. It accepts either a path expression to a string telemetry field or a literal string, and returns the hash as an int64 value.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_22\n\nLANGUAGE: go\nCODE:\n```\nFNV(resource.attributes[\"device.name\"])\n```\n\nLANGUAGE: go\nCODE:\n```\nFNV(\"name\")\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray PostgreSQL Database Query Subsegment in JSON\nDESCRIPTION: JSON representation of an AWS X-Ray subsegment that captures details of a PostgreSQL database query. The subsegment includes metadata such as the database connection URL, query parameters, database type and version, as well as timing and trace information necessary for distributed tracing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/indepSubsegmentWithInvalidSqlUrl.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"ebdb@aawijb5u25wdoy.cpamxznpdoq8.us-west-2.rds.amazonaws.com\",\n  \"id\": \"3fd8634e78ca9560\",\n  \"start_time\": 1484872218.696,\n  \"end_time\": 1484872218.697,\n  \"namespace\": \"remote\",\n  \"type\" : \"subsegment\",\n  \"trace_id\" : \"1-581cf771-a006649127e371903a2de979\",\n  \"parent_id\" : \"defdfd9912dc5a56\",\n  \"sql\" : {\n    \"url\": \"//aawijb5u25wdoy.cpamxznpdoq8.us-west-2.rds.amazonaws.com:5432/ebdb\",\n    \"preparation\": \"statement\",\n    \"database_type\": \"PostgreSQL\",\n    \"database_version\": \"9.5.4\",\n    \"driver_version\": \"PostgreSQL 9.4.1211.jre7\",\n    \"user\" : \"dbuser\",\n    \"sanitized_query\" : \"SELECT  *  FROM  customers  WHERE  customer_id=?;\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cache and Expire Loop Time in ServiceGraphProcessor\nDESCRIPTION: Configuration example showing new configurable items for the servicegraphprocessor including cache_loop and store_expiration_loop settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_34\n\nLANGUAGE: yaml\nCODE:\n```\ncache_loop, store_expiration_loop\n```\n\n----------------------------------------\n\nTITLE: Scaling Metric Values in YAML\nDESCRIPTION: Scales the values of system.cpu.usage metrics by a factor of 1000 to convert from seconds to milliseconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n# experimental_scale CPU usage from seconds to milliseconds\ninclude: system.cpu.usage\naction: update\noperations:\n  - action: experimental_scale_value\n    experimental_scale: 1000\n```\n\n----------------------------------------\n\nTITLE: Convert Action Configuration in YAML\nDESCRIPTION: Configuration for the convert action that changes attribute value types between int, double, or string.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- key: <key>\n  action: convert\n  converted_type: <int|double|string>\n```\n\n----------------------------------------\n\nTITLE: Disabling a Specific Metric in YAML Configuration\nDESCRIPTION: This YAML configuration snippet demonstrates how to disable a specific metric in the k8s_cluster component. Replace <metric_name> with the name of the metric you want to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: JSON Structure Example in Parse JSON Encoded Attribute Values\nDESCRIPTION: Example of a JSON structure that would be converted from a JSON-encoded string attribute value when using the parse_json_encoded_attr_values option.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awsemfexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"x\": 5, \"y\": 6}\n```\n\n----------------------------------------\n\nTITLE: Generating a CPU Profile using Go's pprof Tool\nDESCRIPTION: Command for generating a 30-second CPU profile using Go's pprof tool by connecting to the collector's profiling endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/pprofextension/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngo tool pprof http://localhost:1777/debug/pprof/profile\\?seconds\\=30\n```\n\n----------------------------------------\n\nTITLE: Input and Output JSON for Custom Pair Delimiter Parsing\nDESCRIPTION: Example of input and output JSON for key-value parsing using an exclamation mark as the pair delimiter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"name=stanza ! org=otel      ! group=dev\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"name\": \"stanza\",\n    \"org\": \"otel\",\n    \"group\": \"dev\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing ECS Container Agent Introspection Endpoints in Go\nDESCRIPTION: These snippets show how to access ECS container agent introspection endpoints to collect ECS cluster and task information. They are part of the ecsInfo package used for AWS ECS specific metrics and resource collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awscontainerinsightreceiver/design.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\necs_instance_info\n```\n\nLANGUAGE: go\nCODE:\n```\necs_task_info\n```\n\n----------------------------------------\n\nTITLE: RFC5424 Log Record Output\nDESCRIPTION: Output syslog message formatted according to RFC5424 protocol based on the simplified input example.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/syslogexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n<34>1 2003-10-11T22:14:15.003Z mymachine.example.com su - - - 'su root' failed for lonvick on /dev/pts/8\n```\n\n----------------------------------------\n\nTITLE: Geographical Location Attributes List\nDESCRIPTION: List of resource attributes that will be added by the GeoIP processor when corresponding geographical information is found for an IP address.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/geoipprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n  * geo.city_name\n  * geo.postal_code\n  * geo.country_name\n  * geo.country_iso_code\n  * geo.continent_name\n  * geo.continent_code\n  * geo.region_name\n  * geo.region_iso_code\n  * geo.timezone\n  * geo.location.lat\n  * geo.location.lon\n```\n\n----------------------------------------\n\nTITLE: Adding a Value to Attributes in YAML\nDESCRIPTION: This snippet shows how to add a value to the attributes of an entry using the 'add' operator in YAML configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: attributes.key2\n  value: val2\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Application and Subsystem Names\nDESCRIPTION: Example configuration showing how to set custom application and subsystem names using resource attributes in file log receivers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog/nginx:\n    include:\n      - '/tmp/tmp.log'\n    include_file_path: true\n    include_file_name: false\n    start_at: end\n    resource: \n      cx.subsystem.name: nginx\n  filelog/access-log:\n    include:\n      - '/tmp/access.log'\n    include_file_path: true\n    include_file_name: false\n    resource: \n      cx.subsystem.name: access-log\nexporters:\n  coralogix:\n    domain: 'coralogix.com'\n    private_key: \"XXX\"\n    application_name: 'app_name'\n    timeout: 30s\nservice:\n  pipelines:\n    logs:\n      receivers: [filelog/nginx, filelog/access-log]\n      exporters: [coralogix]\n```\n\n----------------------------------------\n\nTITLE: Running the Sample X-Ray Server Application\nDESCRIPTION: Command to run the sample Go server that's instrumented with the X-Ray SDK.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/README.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngo run sampleserver/sample.go\n```\n\n----------------------------------------\n\nTITLE: Implementing Export Function for Prometheus Remote Write in Go\nDESCRIPTION: Pseudocode for the export function that converts timeseries data to Prometheus Remote Write format and sends it via HTTP. The function iterates through a map of timeseries, converts them to a WriteRequest using proto.Marshal(), and sends an HTTP request to the configured endpoint.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/DESIGN.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nfunc export(*map) error {\n  \t// Stores timeseries\n  \tarr := make([]TimeSeries)\n\n  \tfor timeseries in map:\n  \t\tarr = append(arr, timeseries)\n\n  \t\t// Converts arr to WriteRequest\n  \t\trequest := proto.Marshal(arr)\n\n  \t// Sends HTTP request to endpoint\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying MySQL InnoDB Buffer Pool Metrics in Plaintext\nDESCRIPTION: This snippet shows a tabular representation of two InnoDB buffer pool metrics: the number of data pages and free pages. The 'NotANumber' value for data pages suggests a potential error or unavailable data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/testdata/scraper/global_stats_partial.txt#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nname\tcount\nInnodb_buffer_pool_pages_data\tNotANumber\nInnodb_buffer_pool_pages_free\t233\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Load Metrics in OpenTelemetry Collector\nDESCRIPTION: YAML configuration example showing how to disable specific CPU load metrics in the OpenTelemetry Collector. The configuration allows disabling individual metrics by setting their 'enabled' property to false.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/loadscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Removing a Value from Attributes using YAML Configuration\nDESCRIPTION: This snippet demonstrates how to configure the remove operator to delete a specific attribute from the attributes section of an entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n- type: remove\n  field: attributes.otherkey\n```\n\n----------------------------------------\n\nTITLE: Context Inference in OTTL Statements for Datapoint Context\nDESCRIPTION: Example showing how context inference works in OTTL by identifying the appropriate context (datapoint) based on the paths used in the statement.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmetric_statements:\n  - set(metric.description, \"test passed\") where datapoint.attributes[\"test\"] == \"pass\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Time Parser with GoTime Layout in YAML\nDESCRIPTION: This snippet shows how to configure a time parser using the 'gotime' layout type. It parses a timestamp from the 'body.timestamp_field' using the specified GoTime layout format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/timestamp.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- type: time_parser\n  parse_from: body.timestamp_field\n  layout_type: gotime\n  layout: Jan 2 15:04:05 MST 2006\n```\n\n----------------------------------------\n\nTITLE: Configuring STEF Receiver in YAML\nDESCRIPTION: Basic YAML configuration example for setting up the STEF receiver with a custom endpoint. The receiver listens on all network interfaces (0.0.0.0) on port 4320 for incoming STEF data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/stefreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nstef:\n  endpoint: 0.0.0.0:4320\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Client Timeout in Factory Implementation\nDESCRIPTION: Implementation of the timeout configuration in the exporter factory. The method reads the timeout from the user-provided configuration and sets it in the HTTP client used for remote write requests.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/DESIGN.md#2025-04-10_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nfunc (f *Factory) CreateNewExporter (config) {\n...\n    client := &http.Client{\n            Timeout config.requestTimeout\n    }\n...\n}\n```\n\n----------------------------------------\n\nTITLE: Authentication Requirements for Google Cloud Exporter\nDESCRIPTION: Lists the minimum required IAM roles needed for authenticating with different Google Cloud services through the exporter. Shows the specific role permissions needed for metrics, traces, and logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlecloudexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n*   [Metrics](https://cloud.google.com/iam/docs/understanding-roles#monitoring-roles): `roles/monitoring.metricWriter`\n*   [Traces](https://cloud.google.com/iam/docs/understanding-roles#cloud-trace-roles): `roles/cloudtrace.agent`\n*   [Logs](https://cloud.google.com/iam/docs/understanding-roles#logging-roles): `roles/logging.logWriter`\n```\n\n----------------------------------------\n\nTITLE: HTTP Check Status Metrics Example\nDESCRIPTION: Example showing the metric output format when an endpoint returns a 200 status code. The receiver generates metrics with labels for each HTTP status class.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/httpcheckreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttpcheck.status{http.status_class:1xx, http.status_code:200,...} = 0\nhttpcheck.status{http.status_class:2xx, http.status_code:200,...} = 1\nhttpcheck.status{http.status_class:3xx, http.status_code:200,...} = 0\nhttpcheck.status{http.status_class:4xx, http.status_code:200,...} = 0\nhttpcheck.status{http.status_class:5xx, http.status_code:200,...} = 0\n```\n\n----------------------------------------\n\nTITLE: Removing a Value from Resource using YAML Configuration\nDESCRIPTION: This snippet demonstrates how to configure the remove operator to delete a specific key from the resource section of an entry.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n- type: remove\n  field: resource.otherkey\n```\n\n----------------------------------------\n\nTITLE: Prometheus Summary Data Example\nDESCRIPTION: Example of a Prometheus summary metric showing quantile distributions\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_8\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP go_gc_duration_seconds A summary of the GC invocation durations.\n# TYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 0.0001271\ngo_gc_duration_seconds{quantile=\"0.25\"} 0.0002455\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.0002904\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.0003426\ngo_gc_duration_seconds{quantile=\"1\"} 0.0023638\ngo_gc_duration_seconds_sum 17.391350544\ngo_gc_duration_seconds_count 52489\n```\n\n----------------------------------------\n\nTITLE: Specifying ByteSize Using KB Units in YAML\nDESCRIPTION: Shows how to specify 5000 bytes using kilobyte (kb) notation. This uses the SI convention where 1kb equals 1000 bytes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/bytesize.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- type: some_operator\n  bytes: 5kb\n```\n\n----------------------------------------\n\nTITLE: Configuring BMC Helix Exporter with Required Settings in YAML\nDESCRIPTION: This snippet shows the minimal required configuration for the BMC Helix exporter, including the endpoint and API key.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/bmchelixexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  bmchelix/helix1:\n    endpoint: https://company.onbmc.com\n    api_key: <api-key>\n```\n\n----------------------------------------\n\nTITLE: Journald Input Sample Output\nDESCRIPTION: Example of the JSON output structure produced by the journald_input operator including timestamp and various system fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/journald_input.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"entry\": {\n  \"timestamp\": \"2020-04-16T11:05:49.516168-04:00\",\n  \"body\": {\n    \"CODE_FILE\": \"../src/core/unit.c\",\n    \"CODE_FUNC\": \"unit_log_success\",\n    \"CODE_LINE\": \"5487\",\n    \"MESSAGE\": \"var-lib-docker-overlay2-bff8130ef3f66eeb81ce2102f1ac34cfa7a10fcbd1b8ae27c6c5a1543f64ddb7-merged.mount: Succeeded.\",\n    \"MESSAGE_ID\": \"7ad2d189f7e94e70a38c781354912448\",\n    \"PRIORITY\": \"6\",\n    \"SYSLOG_FACILITY\": \"3\",\n    \"SYSLOG_IDENTIFIER\": \"systemd\",\n    \"USER_INVOCATION_ID\": \"de9283b4fd634213a50f5abe71b4d951\",\n    \"USER_UNIT\": \"var-lib-docker-overlay2-bff8130ef3f66eeb81ce2102f1ac34cfa7a10fcbd1b8ae27c6c5a1543f64ddb7-merged.mount\",\n    \"_AUDIT_LOGINUID\": \"1000\",\n    \"_AUDIT_SESSION\": \"299\",\n    \"_BOOT_ID\": \"c4fa36de06824d21835c05ff80c54468\",\n    \"_CAP_EFFECTIVE\": \"0\",\n    \"_CMDLINE\": \"/lib/systemd/systemd --user\",\n    \"_COMM\": \"systemd\",\n    \"_EXE\": \"/usr/lib/systemd/systemd\",\n    \"_GID\": \"1000\",\n    \"_HOSTNAME\": \"testhost\",\n    \"_MACHINE_ID\": \"d777d00e7caf45fbadedceba3975520d\",\n    \"_PID\": \"18667\",\n    \"_SELINUX_CONTEXT\": \"unconfined\\n\",\n    \"_SOURCE_REALTIME_TIMESTAMP\": \"1587049549515868\",\n    \"_SYSTEMD_CGROUP\": \"/user.slice/user-1000.slice/user@1000.service/init.scope\",\n    \"_SYSTEMD_INVOCATION_ID\": \"da8b20bdc65e4f6f9ca35d6352199b56\",\n    \"_SYSTEMD_OWNER_UID\": \"1000\",\n    \"_SYSTEMD_SLICE\": \"user-1000.slice\",\n    \"_SYSTEMD_UNIT\": \"user@1000.service\",\n    \"_SYSTEMD_USER_SLICE\": \"-.slice\",\n    \"_SYSTEMD_USER_UNIT\": \"init.scope\",\n    \"_TRANSPORT\": \"journal\",\n    \"_UID\": \"1000\",\n    \"__CURSOR\": \"s=b1e713b587ae4001a9ca482c4b12c005;i=1efec9;b=c4fa36de06824d21835c05ff80c54468;m=a001b7ec5a;t=5a369c4a3cd88;x=f9717e0b5608807b\",\n    \"__MONOTONIC_TIMESTAMP\": \"687223598170\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running OpenTelemetry Collector with Configuration File\nDESCRIPTION: Shell command to run the OpenTelemetry Collector in the foreground with a specified configuration file. This is used after setting up credentials and creating the configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlecloudexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./otelcol-contrib --config=config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Linear Operator Sequence in YAML for OpenTelemetry Collector\nDESCRIPTION: This snippet demonstrates a linear sequence of operators for processing logs. It includes reading from a file, parsing JSON, removing an attribute, and adding a new attribute.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/operators.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: my-log.json\n    operators:\n      - type: json_parser\n      - type: remove\n        field: attributes.foo\n      - type: add\n        key: attributes.bar\n        value: baz\n```\n\n----------------------------------------\n\nTITLE: Defining Middleware Configuration Structures in Go\nDESCRIPTION: This snippet defines the main configuration structures for middleware in the OpenTelemetry Collector. It includes configurations for authentication, rate limiting, and other middleware components.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/mysqlreceiver/testdata/scraper/replica_stats_empty.txt#2025-04-10_snippet_0\n\nLANGUAGE: Go\nCODE:\n```\npackage middlewareconfig\n\nimport (\n\t\"go.opentelemetry.io/collector/component\"\n)\n\n// Config defines configuration for collector middlewares.\ntype Config struct {\n\tAuthConfig\n\tRateLimitConfig\n\tOtherMiddlewareConfig\n}\n\n// AuthConfig holds the configuration for authentication middleware.\ntype AuthConfig struct {\n\t// Authentication configuration fields\n}\n\n// RateLimitConfig holds the configuration for rate limiting middleware.\ntype RateLimitConfig struct {\n\t// Rate limiting configuration fields\n}\n\n// OtherMiddlewareConfig holds the configuration for other types of middleware.\ntype OtherMiddlewareConfig struct {\n\t// Other middleware configuration fields\n}\n```\n\n----------------------------------------\n\nTITLE: Output JSON for Basic CSV Parsing\nDESCRIPTION: Example JSON output after applying the basic CSV parsing configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"body\": {\n    \"id\": \"1\",\n    \"severity\": \"debug\",\n    \"message\": \"Debug Message\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Double Converter in Go\nDESCRIPTION: The Double converter converts an inputted value into a double (float64). It supports various input types including float64, string, bool, and int64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_18\n\nLANGUAGE: Go\nCODE:\n```\nDouble(log.attributes[\"http.status_code\"])\n```\n\nLANGUAGE: Go\nCODE:\n```\nDouble(\"2.0\")\n```\n\n----------------------------------------\n\nTITLE: Basic Attributes Action Configuration in YAML\nDESCRIPTION: Configuration examples for insert, update, and upsert actions in the attributes processor. Shows how to specify keys, values, and attribute sources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/attributesprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- key: <key>\n  action: {insert, update, upsert}\n  value: <value>\n\n- key: <key>\n  action: {insert, update, upsert}\n  from_attribute: <other key>\n\n- key: <key>\n  action: {insert, update, upsert}\n  from_context: <other key>\n```\n\n----------------------------------------\n\nTITLE: Running a test with the stdin operator using Bash\nDESCRIPTION: A Bash command that pipes a test string to the stanza command with a specified configuration file. This demonstrates how to send input to the stdin operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/stdin.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\necho \"test\" | stanza -c ./config.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry MetricsData Structure in Go\nDESCRIPTION: Shows the basic structure of the OpenTelemetry MetricsData type which is used to store converted Prometheus metrics. This structure includes Node information, Resource details, and an array of Metric objects.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_2\n\nLANGUAGE: go\nCODE:\n```\ntype MetricsData struct {\n  Node     *commonpb.Node\n  Resource *resourcepb.Resource\n  Metrics  []*metricspb.Metric\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Server Batch Response Performance Metrics JSON\nDESCRIPTION: Performance measurement data showing batch response statistics from SQL Server. Includes metrics for both CPU Time and Elapsed Time across various time intervals ranging from <1ms to >100000ms. Each record contains counter type, instance information, SQL instance ID, and measurement values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"counter\":\"Batches \\u003e=000000ms \\u0026 \\u003c000001ms\",\n   \"counter_type\":\"65792\",\n   \"instance\":\"Elapsed Time:Requests\",\n   \"measurement\":\"sqlserver_performance\",\n   \"object\":\"SQLServer:Batch Resp Statistics\",\n   \"sql_instance\":\"8cac97ac9b8f\",\n   \"computer_name\":\"abcde\",\n   \"value\":\"1884\"\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing Body with Nested Value in YAML\nDESCRIPTION: This configuration shows how to replace the entire body with a single nested value from within the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/move.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: move\n  from: body.log\n  to: body\n```\n\n----------------------------------------\n\nTITLE: Implementing Sigv4 Authenticator Structure\nDESCRIPTION: Defines the main authenticator structure implementing the ClientAuthenticator interface.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_3\n\nLANGUAGE: go\nCODE:\n```\ntype sigv4Auth struct {\n    cfg *Config\n    logger *zap.logger\n    awsSDKInfo string\n    componenthelper.StartFunc\n    componenthelper.ShutdownFunc\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Specific HAProxy Metrics in YAML\nDESCRIPTION: This example shows how to selectively enable or disable specific HAProxy metrics in the receiver configuration. It demonstrates enabling 'haproxy.requests' and disabling 'haproxy.connection_rate'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/haproxyreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  haproxy:\n    endpoint: http://127.0.0.1:8080/stats\n    metrics:\n      haproxy.connection_rate:\n        enabled: false\n      haproxy.requests:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Using replace_pattern function in OTTL\nDESCRIPTION: The replace_pattern function replaces sections of a string that match a regex pattern with a replacement string, optionally applying a conversion function.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_8\n\nLANGUAGE: go\nCODE:\n```\nreplace_pattern(resource.attributes[\"process.command_line\"], \"password\\\\=[^\\\\s]*(\\\\s?)\", \"password=***\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_pattern(metric.name, \"^kube_([0-9A-Za-z]+_)\", \"k8s.$$1.\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_pattern(metric.name, \"^kube_([0-9A-Za-z]+_)\", \"$$1.\", SHA256, \"k8s.%s\")\n```\n\n----------------------------------------\n\nTITLE: Basic Parser Configuration\nDESCRIPTION: YAML configuration for parsing body field into attributes\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/json_array_parser.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- type: json_array_parser\n  parse_from: body\n  parse_to: attributes.output\n```\n\n----------------------------------------\n\nTITLE: Prometheus Storage Appender Interface Definition\nDESCRIPTION: Go interface definition for Prometheus storage.Appender, which is key for implementing custom metric storage. Includes methods for appending metrics, handling exemplars, and managing transactions.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\ntype Appender interface {\n  Append(ref uint64, l labels.Labels, t int64, v float64) (uint64, error)\n\n  // Commit submits the collected samples and purges the batch.\n  Commit() error\n\n  Rollback() error\n\n  ExemplarAppender\n}\n\ntype ExemplarAppender interface {\n\tAppendExemplar(ref uint64, l labels.Labels, e exemplar.Exemplar) (uint64, error)\n}\n```\n\n----------------------------------------\n\nTITLE: Example Go Metric Structure Before Grouping by Attributes\nDESCRIPTION: A Go representation showing the structure of metrics before applying the groupbyattrs processor. It displays metrics with different data points under a single Resource.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nResource {host.name=\"localhost\",source=\"prom\"}\n  Metric \"gauge-1\" (GAUGE)\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-B\",id=\"eth0\"}\n  Metric \"gauge-1\" (GAUGE) // Identical to previous Metric\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-B\",id=\"eth0\"}\n  Metric \"mixed-type\" (GAUGE)\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-B\",id=\"eth0\"}\n  Metric \"mixed-type\" (SUM)\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n    DataPoint {host.name=\"host-A\",id=\"eth0\"}\n  Metric \"dont-move\" (Gauge)\n    DataPoint {id=\"eth0\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Tenant for Loki Exporter\nDESCRIPTION: YAML configuration example showing how to set a static tenant for the Loki exporter using HTTP headers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  loki:\n    endpoint: http://localhost:3100/loki/api/v1/push\n    headers:\n      \"X-Scope-OrgID\": acme\n```\n\n----------------------------------------\n\nTITLE: Converting Summary Count to Sum Metric in OpenTelemetry\nDESCRIPTION: This function creates a new Sum metric from a Summary's count value. It takes aggregation temporality and monotonicity as parameters. The new metric's name will be '<summary metric name>_count' and it copies timestamp, starttimestamp, attributes, and description fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nconvert_summary_count_val_to_sum(\"delta\", true)\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nconvert_summary_count_val_to_sum(\"cumulative\", false)\n```\n\n----------------------------------------\n\nTITLE: Configuring RabbitMQ Receiver in OpenTelemetry Collector\nDESCRIPTION: This snippet demonstrates how to configure the RabbitMQ receiver in the OpenTelemetry Collector. It shows required and optional settings including endpoint URL, authentication credentials, collection interval, and enabling specific metrics to be collected.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/rabbitmqreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  rabbitmq:\n    endpoint: http://localhost:15672\n    username: otelu\n    password: ${env:RABBITMQ_PASSWORD}\n    collection_interval: 10s\n     metrics:  # Enable node metrics by explicitly setting them to true\n      rabbitmq.node.disk_free:\n        enabled: true\n      rabbitmq.node.disk_free_limit:\n        enabled: true\n      rabbitmq.node.disk_free_alarm:\n        enabled: true\n      rabbitmq.node.mem_used:\n        enabled: true\n      rabbitmq.node.mem_limit:\n        enabled: true\n      rabbitmq.node.mem_alarm:\n        enabled: true\n      rabbitmq.node.fd_used:\n        enabled: true\n      rabbitmq.node.fd_total:\n        enabled: true\n      rabbitmq.node.sockets_used:\n        enabled: true\n      rabbitmq.node.sockets_total:\n        enabled: true\n      rabbitmq.node.proc_used:\n        enabled: true\n      rabbitmq.node.proc_total:\n        enabled: true\n      rabbitmq.node.disk_free_details.rate:\n        enabled: true\n      rabbitmq.node.fd_used_details.rate:\n        enabled: true\n      rabbitmq.node.mem_used_details.rate:\n        enabled: true\n      rabbitmq.node.proc_used_details.rate:\n        enabled: true\n      rabbitmq.node.sockets_used_details.rate:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Using truncate_all function in OTTL\nDESCRIPTION: The truncate_all function limits the length of all string values in a pcommon.Map to ensure they don't exceed a specified character limit.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_10\n\nLANGUAGE: go\nCODE:\n```\ntruncate_all(log.attributes, 100)\n```\n\nLANGUAGE: go\nCODE:\n```\ntruncate_all(resource.attributes, 50)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS X-Ray Trace Document Structure\nDESCRIPTION: JSON structure defining an AWS X-Ray trace document with HTTP transaction details, service metadata, and AWS SDK information. The document includes trace identifiers, timing information, HTTP request/response data, and service compiler details.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/segmentValidationFailed.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f2aebcc-b475d14618c51eaa28753d37\",\n    \"id\": \"bda182a644eee9b3\",\n    \"name\": \"SampleServer\",\n    \"end_time\": 1596648396.6401389,\n    \"http\": {\n        \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"http://localhost:8000/\",\n            \"client_ip\": \"127.0.0.1\",\n            \"user_agent\": \"Go-http-client/1.1\",\n            \"x_forwarded_for\": true\n        },\n        \"response\": {\n            \"status\": 200\n        }\n    },\n    \"aws\": {\n        \"xray\": {\n            \"sdk_version\": \"1.1.0\",\n            \"sdk\": \"X-Ray for Go\"\n        }\n    },\n    \"service\": {\n        \"compiler_version\": \"go1.14.6\",\n        \"compiler\": \"gc\"\n    },\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying SQL Server Database IO Metrics in JSON Format\nDESCRIPTION: A JSON array containing SQL Server database IO performance metrics. Each object represents metrics for a specific database file, including database name, file type, file names, read/write statistics, latency measurements, and instance information.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/database_io_scraped_data.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"database_name\": \"master\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"master\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/master.mdf\",\n        \"read_bytes\": \"4022272\",\n        \"read_latency_ms\": \"62\",\n        \"reads\": \"73\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"4096000\",\n        \"write_latency_ms\": \"130\",\n        \"writes\": \"329\"\n    },\n    {\n        \"database_name\": \"master\",\n        \"file_type\": \"LOG\",\n        \"logical_filename\": \"mastlog\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/mastlog.ldf\",\n        \"read_bytes\": \"916480\",\n        \"read_latency_ms\": \"8\",\n        \"reads\": \"17\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"8061952\",\n        \"write_latency_ms\": \"3302\",\n        \"writes\": \"608\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb.mdf\",\n        \"read_bytes\": \"2113536\",\n        \"read_latency_ms\": \"9\",\n        \"reads\": \"35\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"32768\",\n        \"write_latency_ms\": \"0\",\n        \"writes\": \"4\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"LOG\",\n        \"logical_filename\": \"templog\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/templog.ldf\",\n        \"read_bytes\": \"1007616\",\n        \"read_latency_ms\": \"2\",\n        \"reads\": \"7\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"180224\",\n        \"write_latency_ms\": \"4\",\n        \"writes\": \"17\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev2\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb2.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"1\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"1\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev3\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb3.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"2\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"2\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev4\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb4.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"1\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"2\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev5\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb5.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"2\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"2\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev6\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb6.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"2\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"2\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev7\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb7.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"1\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"2\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"tempdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"tempdev8\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/tempdb8.ndf\",\n        \"read_bytes\": \"131072\",\n        \"read_latency_ms\": \"1\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"90112\",\n        \"write_latency_ms\": \"1\",\n        \"writes\": \"11\"\n    },\n    {\n        \"database_name\": \"model\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"modeldev\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/model.mdf\",\n        \"read_bytes\": \"10575872\",\n        \"read_latency_ms\": \"21\",\n        \"reads\": \"53\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"860160\",\n        \"write_latency_ms\": \"16\",\n        \"writes\": \"80\"\n    },\n    {\n        \"database_name\": \"model\",\n        \"file_type\": \"LOG\",\n        \"logical_filename\": \"modellog\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/modellog.ldf\",\n        \"read_bytes\": \"1150464\",\n        \"read_latency_ms\": \"7\",\n        \"reads\": \"11\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"968704\",\n        \"write_latency_ms\": \"31\",\n        \"writes\": \"111\"\n    },\n    {\n        \"database_name\": \"msdb\",\n        \"file_type\": \"ROWS\",\n        \"logical_filename\": \"MSDBData\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/MSDBData.mdf\",\n        \"read_bytes\": \"6840320\",\n        \"read_latency_ms\": \"51\",\n        \"reads\": \"108\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"991232\",\n        \"write_latency_ms\": \"26\",\n        \"writes\": \"102\"\n    },\n    {\n        \"database_name\": \"msdb\",\n        \"file_type\": \"LOG\",\n        \"logical_filename\": \"MSDBLog\",\n        \"measurement\": \"sqlserver_database_io\",\n        \"physical_filename\": \"/var/opt/mssql/data/MSDBLog.ldf\",\n        \"read_bytes\": \"660992\",\n        \"read_latency_ms\": \"5\",\n        \"reads\": \"9\",\n        \"rg_read_stall_ms\": \"0\",\n        \"rg_write_stall_ms\": \"0\",\n        \"sql_instance\": \"8cac97ac9b8f\",\n        \"write_bytes\": \"1019904\",\n        \"write_latency_ms\": \"27\",\n        \"writes\": \"117\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: DataSet Exporter Configuration with Empty Prefixes and Suffix\nDESCRIPTION: Configuration example with all logging options enabled and empty prefix and suffix strings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datasetexporter/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlogs:\n  export_resource_info_on_event: true\n  export_resource_prefix: \"\"\n  export_scope_info_on_event: true\n  export_scope_prefix: \"\"\n  decompose_complex_message_field: true\n  decomposed_complex_message_prefix: \"\"\n  export_separator: \"-\"\n  export_distinguishing_suffix: \"\"\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Migration Reference Table\nDESCRIPTION: Markdown table mapping deprecated translation rules to their replacement processors and providing links to relevant examples in the OpenTelemetry documentation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/docs/translation_rules_migration_guide.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Deleted Translation rule | Replacement option | Replacement example |\n| -----------------|--------------------|----------------------|\n| aggregate_metric | `transform` processor's `aggregate_on_attributes` function with the `metric` context | [aggregate example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor#aggregate_on_attributes) |\n| calculate_new_metric | `metricsgeneration` processor's `calculate` functionality | [calculate example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricsgenerationprocessor#example-configurations) |\n| convert_values | `transform` processor's `Double` or `Int` converter on a `datapoint` context | [`Double` example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs#double), [`Int` example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs#int) |\n| copy_metrics | `metricstransform` processor's `insert` functionality | [copy all datapoints](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#create-a-new-metric-from-an-existing-metric), [conditionally copy datapoints](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#create-a-new-metric-from-an-existing-metric-with-matching-label-values) |\n| delta_metric | `cumulativetodelta` processor. To preserve original metrics, first copy the original metric, then use the copied metric in the `cumulativetodelta` processor | [specify which metrics to convert example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/cumulativetodeltaprocessor#examples)\n| drop_dimensions | `transform` processor's `delete_keys` function with the `datapoint` context | [simple example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs#delete_key), use a `where` clause with the given example to filter based upon the metric name or dimension value |\n| drop_metrics | `filter` processor | [drop by name and value example](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor#dropping-specific-metric-and-value) |\n| multiply_int, divide_int, multiply_float | `metricstransform` processor's `scale` value functionality | [one metric](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#scale-value) |\n| rename_dimension_keys | `metricstransform` processor's update label function | [one metric](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#rename-labels), [multiple metrics](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#rename-labels-for-multiple-metrics) |\n| rename_metrics | `metricstransform` processor's rename metric functionality | [one metric](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#rename-metric), [multiple metrics](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#rename-multiple-metrics-using-substitution) |\n| split_metric | `metricstransform` processor's `insert` functionality and `filter` processor | Refer to the replacement guidance for the `copy_metrics` and `drop_metrics` translation rules |\n```\n\n----------------------------------------\n\nTITLE: Using Decode Converter in Go\nDESCRIPTION: The Decode converter takes a string or byte array encoded with the specified encoding and returns the decoded string. It supports various encoding formats including base64 and IANA encoding index.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_12\n\nLANGUAGE: Go\nCODE:\n```\nDecode(\"aGVsbG8gd29ybGQ=\", \"base64\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nDecode(resource.attributes[\"encoded field\"], \"us-ascii\")\n```\n\n----------------------------------------\n\nTITLE: Basic Container Parser Configuration\nDESCRIPTION: Simple YAML configuration for automatic container log format detection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/container.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: container\n```\n\n----------------------------------------\n\nTITLE: Hashing String with MD5 in Go\nDESCRIPTION: Converts a string value to an MD5 hash. The function takes either a path expression to a string telemetry field or a literal string. Returns the MD5 hash as a string.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_31\n\nLANGUAGE: Go\nCODE:\n```\nMD5(resource.attributes[\"device.name\"])\n```\n\nLANGUAGE: Go\nCODE:\n```\nMD5(\"name\")\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray Trace Segment JSON Structure\nDESCRIPTION: Sample JSON representation of an AWS X-Ray trace segment. The structure includes trace and segment identifiers, name, start and end timestamps, fault status, and cause information. The cause field appears to contain an exception ID reference.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/minCauseIsExceptionId.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f187253-6a106696d56b1f4ef9eba2ed\",\n    \"id\": \"5cc4a447f5d4d696\",\n    \"name\": \"CauseIsExceptionID\",\n    \"start_time\": 1595437651.680097,\n    \"end_time\": 1595437652.197392,\n    \"fault\": true,\n    \"cause\": \"abcdefghijklmnop\",\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Journald Input with Match Conditions\nDESCRIPTION: Configuration showing how to use match conditions to filter journal entries based on specific fields and values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/journald_input.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: journald_input\n  matches:\n    - _SYSTEMD_UNIT: ssh\n    - _SYSTEMD_UNIT: kubelet\n      _UID: \"1000\"\n```\n\n----------------------------------------\n\nTITLE: Running Load Balancer Demo Environment\nDESCRIPTION: Command to start the demo environment using Docker Compose, which includes log generator, agent, and multiple collector backends.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/example/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker-compose up\n```\n\n----------------------------------------\n\nTITLE: Time and Duration Conversion Examples\nDESCRIPTION: Examples of using Hour and Hours converters to work with time values and durations.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_27\n\nLANGUAGE: Go\nCODE:\n```\nHour(Now())\nHours(Duration(\"1h\"))\n```\n\n----------------------------------------\n\nTITLE: Converting Duration to Microseconds in Go\nDESCRIPTION: Converts a time.Duration value to an integer microsecond count. Returns the duration as an int64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_32\n\nLANGUAGE: Go\nCODE:\n```\nMicroseconds(Duration(\"1h\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring TCP Syslog Input with RFC5424 Protocol\nDESCRIPTION: A simple configuration example for setting up the syslog_input operator to listen for TCP syslog messages that follow the RFC5424 protocol. The operator listens on port 54526 on all interfaces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/syslog_input.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: syslog_input\n  tcp:\n     listen_address: \"0.0.0.0:54526\"\n  syslog:\n     protocol: rfc5424\n```\n\n----------------------------------------\n\nTITLE: Using ToLowerCase Converter for String Formatting in OpenTelemetry\nDESCRIPTION: The ToLowerCase Converter transforms a string into lowercase format. It converts all characters to their lowercase equivalents, changing strings like 'MyMetricName' to 'mymetricname'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_59\n\nLANGUAGE: go\nCODE:\n```\nToLowerCase(target)\n```\n\n----------------------------------------\n\nTITLE: Analyzing AWS X-Ray Trace Segment for Failed DynamoDB Operation\nDESCRIPTION: This JSON snippet represents an AWS X-Ray trace segment for a DynamoDB operation that failed due to a ResourceNotFoundException. It includes detailed information about the error, HTTP response, AWS request details, and nested subsegments for different stages of the operation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/invalidNamespace.txt#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"71631df3f58bdfc5\",\n  \"name\": \"dynamodb\",\n  \"start_time\": 1596566305.5874245,\n  \"end_time\": 1596566305.5928326,\n  \"fault\": true,\n  \"cause\": {\n    \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n    \"exceptions\": [\n      {\n        \"id\": \"7121b882a0ef44da\",\n        \"type\": \"dynamodb.ResourceNotFoundException\",\n        \"message\": \"ResourceNotFoundException: Requested resource not found\",\n        \"stack\": [\n          {\n            \"path\": \"github.com/aws/aws-sdk-go@v1.33.9/aws/request/handlers.go\",\n            \"line\": 267,\n            \"label\": \"(*HandlerList).Run\"\n          },\n          // ... (truncated for brevity)\n          {\n            \"path\": \"runtime/asm_amd64.s\",\n            \"line\": 1373,\n            \"label\": \"goexit\"\n          }\n        ],\n        \"remote\": true\n      }\n    ]\n  },\n  \"namespace\": \"aws\",\n  \"http\": {\n    \"response\": {\n      \"status\": 400,\n      \"content_length\": 112\n    }\n  },\n  \"aws\": {\n    \"consumed_capacity\": null,\n    \"item_collection_metrics\": null,\n    \"operation\": \"PutItem\",\n    \"region\": \"us-west-2\",\n    \"request_id\": \"TJUJNR0JV84CFHJL93D3GIA0LBVV4KQNSO5AEMVJF66Q9ASUAAJG\",\n    \"retries\": 0,\n    \"table_name\": \"does_not_exist\"\n  },\n  \"subsegments\": [\n    {\n      \"id\": \"9da02fcbb9711b47\",\n      \"name\": \"marshal\",\n      \"start_time\": 1596566305.5874267,\n      \"end_time\": 1596566305.58745,\n      \"Dummy\": false\n    },\n    {\n      \"id\": \"56b1cb185cbdb378\",\n      \"name\": \"attempt\",\n      \"start_time\": 1596566305.587453,\n      \"end_time\": 1596566305.592767,\n      \"fault\": true,\n      \"cause\": {\n        \"working_directory\": \"/home/ubuntu/opentelemetry-collector-contrib/receiver/awsxrayreceiver/testdata/rawsegment/sampleapp\",\n        \"exceptions\": [\n          {\n            \"id\": \"59de8ae27660d21d\",\n            \"type\": \"dynamodb.ResourceNotFoundException\",\n            \"message\": \"ResourceNotFoundException: Requested resource not found\",\n            \"stack\": [\n              {\n                \"path\": \"github.com/aws/aws-xray-sdk-go@v1.1.0/xray/aws.go\",\n                \"line\": 139,\n                \"label\": \"glob..func7\"\n              },\n              // ... (truncated for brevity)\n              {\n                \"path\": \"runtime/asm_amd64.s\",\n                \"line\": 1373,\n                \"label\": \"goexit\"\n              }\n            ],\n            \"remote\": true\n          }\n        ]\n      },\n      \"subsegments\": [\n        {\n          \"id\": \"6f908a1d3ec70abe\",\n          \"name\": \"request\",\n          \"start_time\": 1596566305.5875077,\n          \"end_time\": 1596566305.587543,\n          \"Dummy\": false\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Process Metrics Configuration in YAML\nDESCRIPTION: YAML configuration snippet showing how to disable specific metrics emitted by the processes receiver. This configuration can be applied to any of the default metrics to prevent them from being collected.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/processesscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Querying Streaming Ingestion Status in Azure Data Explorer\nDESCRIPTION: KQL query to check if streaming ingestion is enabled for a specific database in Azure Data Explorer.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuredataexplorerexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: kql\nCODE:\n```\n.show database <DB-Name> policy streamingingestion\n```\n\n----------------------------------------\n\nTITLE: Filtering and Transforming Specific Metrics in YAML\nDESCRIPTION: A configuration example demonstrating how to use filter and metricstransform processors to collect only specific metrics and rename them before sending to CloudWatch.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsecscontainermetricsreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awsecscontainermetrics:\nexporters:\n  awsemf:\n      namespace: 'ECS/ContainerMetrics/OpenTelemetry'\n      log_group_name: '/ecs/containermetrics/opentelemetry'\nprocessors:\n  filter:\n    metrics:\n      include:\n        match_type: strict\n        metric_names:\n          - ecs.task.memory.utilized\n\n  metricstransform:\n    transforms:\n      - include: ecs.task.memory.utilized\n        action: update\n        new_name: MemoryUtilized\n\nservice:\n  pipelines:\n      metrics:\n          receivers: [awsecscontainermetrics]\n          processors: [filter, metricstransform]\n          exporters: [awsemf]\n```\n\n----------------------------------------\n\nTITLE: Hashing String with Murmur3 (32-bit) in Go\nDESCRIPTION: Converts a string to a hexadecimal string representation of the 32-bit Murmur3 hash in little-endian format. Takes a Getter that returns a string as input.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_37\n\nLANGUAGE: Go\nCODE:\n```\nMurmur3Hash(attributes[\"order.productId\"])\n```\n\n----------------------------------------\n\nTITLE: Telemetry Test Data Matrix\nDESCRIPTION: A structured matrix of test scenarios mapping parent-child relationships, span types, service names, and error codes for OpenTelemetry collector testing. Each row represents a unique combination of test parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/coreinternal/goldendataset/testdata/generated_pict_pairs_spans.txt#2025-04-10_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nChild\tOne\tProducer\tEmpty\tTwo\tTwo\tCancelled\nChild\tOne\tUnspecified\tFaaSHTTP\tEight\tNil\tInvalidArgument\nChild\tEmpty\tUnspecified\tHTTPClient\tOne\tOne\tFailedPrecondition\n```\n\n----------------------------------------\n\nTITLE: UnixMilli Time Converter in Golang\nDESCRIPTION: The UnixMilli Converter returns the milliseconds elapsed since January 1, 1970 UTC from a time.Time object. Returns an int64 value representing milliseconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_66\n\nLANGUAGE: go\nCODE:\n```\nUnixMilli(Time(\"02/04/2023\", \"%m/%d/%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Defining AWS X-Ray Subsegment Structure in JSON\nDESCRIPTION: This JSON object represents the structure of an AWS X-Ray subsegment. It includes essential fields for tracing and identifying the subsegment within a distributed system. The subsegment is part of a local namespace and is marked as traced.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/indepSubsegmentWithLocalNamespace.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"localNamespaceTest\",\n    \"id\": \"53995c3f42cd8ad8\",\n    \"start_time\": 1478293361.271,\n    \"end_time\": 1478293361.449,\n    \"type\": \"subsegment\",\n    \"trace_id\": \"1-581cf771-a006649127e371903a2de979\",\n    \"parent_id\": \"defdfd9912dc5a56\",\n    \"namespace\": \"local\",\n    \"traced\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Monitor Exporter with Instrumentation Key in YAML\nDESCRIPTION: Legacy YAML configuration for the Azure Monitor Exporter using the instrumentation key. This method is not recommended and will be deprecated in the future.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuremonitorexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  azuremonitor:\n    instrumentation_key: b1cd0778-85fc-4677-a3fa-79d3c23e0efd\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenTelemetry Exporter Helper with YAML\nDESCRIPTION: YAML configuration example for the Azure Data Explorer OpenTelemetry exporter that demonstrates how to leverage the exporter helper capabilities for retries, timeouts, and queuing. These configurations can improve reliability and performance when exporting data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuredataexplorerexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Example OpenTelemetry Exporter Configuration\n  timeout: 10s\n  sending_queue:\n      enabled: true\n      num_consumers: 2\n      queue_size: 10\n  retry_on_failure:\n      enabled: true\n      initial_interval: 10s\n      max_interval: 60s\n      max_elapsed_time: 10m\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Association Rules in YAML\nDESCRIPTION: Example YAML configuration for pod association rules. It demonstrates how to set up resource attribute matching for pod IP, name, and namespace.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\npod_association:\n  - sources:\n      - from: resource_attribute\n        name: k8s.pod.ip\n  - sources:\n      - from: resource_attribute\n        name: k8s.pod.name\n      - from: resource_attribute\n        name: k8s.namespace.name\n```\n\n----------------------------------------\n\nTITLE: Converting Duration to Nanoseconds in Go\nDESCRIPTION: Converts a time.Duration value to an integer nanosecond count. Returns the duration as an int64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_40\n\nLANGUAGE: Go\nCODE:\n```\nNanoseconds(Duration(\"1h\"))\n```\n\n----------------------------------------\n\nTITLE: Using replace_all_matches function in OTTL\nDESCRIPTION: The replace_all_matches function replaces string values in a pcommon.Map that match a glob pattern with a replacement string, optionally applying a conversion function.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_5\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_matches(resource.attributes, \"/user/*/list/*\", \"/user/{userId}/list/{listId}\")\n```\n\nLANGUAGE: go\nCODE:\n```\nreplace_all_matches(resource.attributes, \"/user/*/list/*\", \"/user/{userId}/list/{listId}\", SHA256, \"/user/%s\")\n```\n\n----------------------------------------\n\nTITLE: Sample Multiline Log Entries\nDESCRIPTION: These are example multiline log entries that the multiline parsing configuration can handle. They represent Java exception stack traces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#2025-04-10_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nException in thread 1 \"main\" java.lang.NullPointerException\n        at com.example.myproject.Book.getTitle(Book.java:16)\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\nException in thread 2 \"main\" java.lang.NullPointerException\n        at com.example.myproject.Book.getTitle(Book.java:16)\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:44)\n```\n\n----------------------------------------\n\nTITLE: Converting Prometheus Counter to OpenTelemetry Metric (Second Scrape)\nDESCRIPTION: Shows how a subsequent scrape of the same Prometheus counter is handled, maintaining the original start timestamp while updating the current values. This demonstrates the handling of cumulative metrics across multiple scrapes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nmetrics := []*metricspb.Metric{\n  {\n    MetricDescriptor: &metricspb.MetricDescriptor{\n      Name:      \"http_requests_total\",\n      Type:      metricspb.MetricDescriptor_CUMULATIVE_DOUBLE,\n      LabelKeys: []*metricspb.LabelKey{{Key: \"method\"}, {Key: \"code\"}}},\n    Timeseries: []*metricspb.TimeSeries{\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"post\", HasValue: true}, {Value: \"200\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: currentTimestamp, Value: &metricspb.Point_DoubleValue{DoubleValue: 1028.0}},\n        },\n      },\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{{Value: \"post\", HasValue: false}, {Value: \"400\", HasValue: true}},\n        Points: []*metricspb.Point{\n          {Timestamp: currentTimestamp, Value: &metricspb.Point_DoubleValue{DoubleValue: 5.0}},\n        },\n      },\n    },\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Huawei Cloud CES Receiver in YAML\nDESCRIPTION: Example configuration for the Huawei Cloud CES receiver showing required and optional parameters including collection interval, region ID, authentication keys, and other settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/huaweicloudcesreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  huaweicloudcesreceiver:\n    collection_interval: 3h\n    initial_delay: 5s\n    region_id: eu-west-101\n    access_key: ${env:HUAWEICLOUD_SDK_AK}\n    secret_key: ${env:HUAWEICLOUD_SDK_SK}\n    project_id: \"project_1\"\n    period: 300\n    filter: average\n    no_verify_ssl: True\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Enablement in YAML\nDESCRIPTION: YAML configuration snippet showing how to enable or disable specific metrics using the metrics configuration block. The metric_name placeholder should be replaced with the actual metric name to configure.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filestatsreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Input Log Entry Structure\nDESCRIPTION: Example JSON showing the structure of an input log entry before field modifications.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/field.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"attributes\": {},\n  \"body\": {\n    \"key1\": \"value1\",\n    \"key2\": {\n      \"nested_key1\": \"nested_value1\",\n      \"nested_key2\": \"nested_value2\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Key-Value String in Go\nDESCRIPTION: Parses a string containing key-value pairs into a pcommon.Map. Takes a target string and optional delimiters for key-value pairs and pair separation. Returns a map of parsed key-value pairs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_44\n\nLANGUAGE: Go\nCODE:\n```\nParseKeyValue(\"k1=v1 k2=v2 k3=v3\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseKeyValue(\"k1!v1_k2!v2_k3!v3\", \"!\", \"_\")\n```\n\nLANGUAGE: Go\nCODE:\n```\nParseKeyValue(log.attributes[\"pairs\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentry Exporter in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up the Sentry Exporter with specific options such as DSN, environment, and SSL verification settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sentryexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  sentry:\n    dsn: https://key@host/path/42\n    environment: prod\n    insecure_skip_verify: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Non-Linear Operator Sequence with Explicit Emission in YAML\nDESCRIPTION: This snippet shows a non-linear sequence where not all logs flow through the last operator. It uses a 'noop' operator to explicitly emit logs from different points in the sequence.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/operators.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  filelog:\n    include: my-log.json\n    operators:\n      - type: router\n        routes:\n          - expr: 'body matches \"^{.*}$\"'\n            output: json_parser\n          - expr: 'body startsWith \"ERROR\"'\n            output: error_parser\n      - type: json_parser\n        output: noop # send from here directly to the end of the sequence\n      - type: regex_parser\n        id: error_parser\n        regex: ... # regex appropriate to parsing error logs\n      - type: noop\n```\n\n----------------------------------------\n\nTITLE: OpenTelemetry Summary Conversion\nDESCRIPTION: Go code showing how the Prometheus summary is converted to OpenTelemetry format with percentile values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_9\n\nLANGUAGE: go\nCODE:\n```\nmetrics := []*metricspb.Metric{\n  {\n    MetricDescriptor: &metricspb.MetricDescriptor{\n      Name:      \"go_gc_duration_seconds\",\n      Type:      metricspb.MetricDescriptor_SUMMARY,\n      LabelKeys: []*metricspb.LabelKey{}},\n    Timeseries: []*metricspb.TimeSeries{\n      {\n        StartTimestamp: startTimestamp,\n        LabelValues:    []*metricspb.LabelValue{},\n        Points: []*metricspb.Point{\n          {Timestamp: startTimestamp, Value: &metricspb.Point_SummaryValue{\n            SummaryValue: &metricspb.SummaryValue{\n\t\t\t  Sum:   &wrappers.DoubleValue{Value: 17.391350544},\n\t\t\t  Count: &wrappers.Int64Value{Value: 52489},\n              Snapshot: &metricspb.SummaryValue_Snapshot{\n                PercentileValues: []*metricspb.SummaryValue_Snapshot_ValueAtPercentile{\n                  {Percentile: 0.0,   Value: 0.0001271},\n                  {Percentile: 25.0,  Value: 0.0002455},\n                  {Percentile: 50.0,  Value: 0.0002904},\n                  {Percentile: 75.0,  Value: 0.0003426},\n                  {Percentile: 100.0, Value: 0.0023638},\n                },\n              }}}},\n        },\n      },\n    },\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: Input JSON Entry for Add Operator\nDESCRIPTION: This JSON represents the input entry structure for the 'add' operator examples, containing empty resource and attributes, and a body with one key-value pair.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key1\": \"val1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Metrics in YAML\nDESCRIPTION: Configuration snippet to disable default metrics in the filesystem receiver. Applied by setting enabled: false for specific metric names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/filesystemscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Input and Output JSON for Custom Delimiter Parsing\nDESCRIPTION: Example of input and output JSON for key-value parsing using a colon as the delimiter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/key_value_parser.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"name:stanza\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"name\": \"stanza\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: UnixNano Time Converter in Golang\nDESCRIPTION: The UnixNano Converter returns the nanoseconds elapsed since January 1, 1970 UTC from a time.Time object. Returns an int64 value representing nanoseconds.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_67\n\nLANGUAGE: go\nCODE:\n```\nUnixNano(Time(\"02/04/2023\", \"%m/%d/%Y\"))\n```\n\n----------------------------------------\n\nTITLE: Complete GitHub Receiver Configuration with Authentication\nDESCRIPTION: Comprehensive YAML configuration showing GitHub receiver setup with bearer token authentication, scraper configuration, and pipeline setup.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/githubreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n    bearertokenauth/github:\n        token: ${env:GH_PAT}\n\nreceivers:\n    github:\n        initial_delay: 1s\n        collection_interval: 60s\n        scrapers:\n            scraper:\n                metrics: #Optional\n                    vcs.contributor.count:\n                        enabled: true\n                github_org: <myfancyorg> \n                search_query: \"org:<myfancyorg> topic:<o11yalltheway>\" # Recommended optional query override, defaults to \"{org,user}:<github_org>\"\n                endpoint: \"https://selfmanagedenterpriseserver.com\" # Optional\n                auth:\n                    authenticator: bearertokenauth/github\nservice:\n    extensions: [bearertokenauth/github]\n    pipelines:\n        metrics:\n            receivers: [..., github]\n            processors: []\n            exporters: [...]\n```\n\n----------------------------------------\n\nTITLE: Installing Go and GCC Dependencies\nDESCRIPTION: Installs required dependencies including Go programming language and GCC compiler for running integration tests. Example shows installation for Go 1.23.4 on amd64 architecture.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/cgroupruntimeextension/CONTRIBUTING.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napt update && apt install -y wget sudo gcc && wget https://go.dev/dl/go1.23.4.linux-amd64.tar.gz && tar -C /usr/local -xzf go1.23.4.linux-amd64.tar.gz && export PATH=$PATH:/usr/local/go/bin && go version && rm go1.23.4.linux-amd64.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Calculating Sampling Policy Decision Frequency in Prometheus\nDESCRIPTION: This Prometheus query calculates how often each policy votes to sample a trace. It's useful for understanding the behavior of individual sampling policies within the tail sampling processor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_8\n\nLANGUAGE: prometheus\nCODE:\n```\nsum (otelcol_processor_tail_sampling_count_traces_sampled{sampled=\"true\"}) by (policy) / \nsum (otelcol_processor_tail_sampling_count_traces_sampled) by (policy)\n```\n\n----------------------------------------\n\nTITLE: Example of Ambiguous Nesting Input JSON\nDESCRIPTION: An example of problematic input where multiple keys would map to the same nested structure, leading to undefined behavior in the attribute processor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"a.b.c.\": \"d\",\n  \"a\": {\n    \"b\": {\n      \"c\": \"e\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Process Metrics Configuration in YAML\nDESCRIPTION: YAML configuration to disable default process metrics. Replace <metric_name> with the specific metric identifier that should be disabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/processscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Renaming Multiple Metrics with Substitution in YAML\nDESCRIPTION: Configuration example showing how to rename multiple metrics using regexp pattern matching and substitution\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ninclude: ^system\\.cpu\\.(.*)$$\nmatch_type: regexp\naction: update\nnew_name: system.processor.$${1}.stat\n```\n\n----------------------------------------\n\nTITLE: Reporting Uncompressed Size in Exporter using SizesStruct in Go\nDESCRIPTION: This code shows how to use the SizesStruct to report the uncompressed size of a payload in an exporter. It sets the method and length (uncompressed size) in the SizesStruct and uses the CountSend method of the network reporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/otelarrow/netstats/README.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nvar sized netstats.SizesStruct\nsized.Method = s.method\nsized.Length = int64(uncompressedSize)\nnetReporter.CountSend(ctx, sized)\n```\n\n----------------------------------------\n\nTITLE: Implementing Configuration Validation\nDESCRIPTION: Validates the Sigv4 authenticator configuration and initializes AWS credentials.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nfunc (cfg *Config) Validate() error {\n\tcredsProvider, err := getCredsFromConfig(cfg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"could not retrieve credential provider: %w\", err)\n\t}\n\tif credsProvider == nil {\n\t\treturn fmt.Errorf(\"credsProvider cannot be nil\")\n\t}\n\treturn nil\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Metrics in YAML Configuration\nDESCRIPTION: Example YAML configuration to enable an optional metric in the docker_stats receiver. Replace <metric_name> with the specific metric to enable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/dockerstatsreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Scraper in Host Metrics Receiver\nDESCRIPTION: Configuration for the network scraper that allows including or excluding specific network interfaces with strict or regexp matching.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nnetwork:\n  <include|exclude>:\n    interfaces: [ <interface name>, ... ]\n    match_type: <strict|regexp>\n```\n\n----------------------------------------\n\nTITLE: Basic Kafka Metrics Receiver Configuration\nDESCRIPTION: Basic configuration example showing how to enable all scrapers (brokers, topics, consumers) with default settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkametricsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  kafkametrics:\n    scrapers:\n      - brokers\n      - topics\n      - consumers\n```\n\n----------------------------------------\n\nTITLE: Example Kafka Record with Headers\nDESCRIPTION: An example of a Kafka record with headers that would be processed by the header extraction feature, showing the structure of the input data.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkareceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n{\n  event: Hello,\n  headers: {\n    header1: value1,\n    header2: value2,\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Copying Values from Body to Attributes in OpenTelemetry Collector (YAML)\nDESCRIPTION: This configuration copies a value from the body field to the attributes field. Specifically, it transfers the value at body.key2 to attributes.newkey.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/copy.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: copy\n  from: body.key2\n  to: attributes.newkey\n```\n\n----------------------------------------\n\nTITLE: Setting up RBAC ClusterRole\nDESCRIPTION: Bash command to create a ClusterRole with permissions to read Kubernetes resources like pods, nodes, services, and ingresses.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - services\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: \n  - \"networking.k8s.io\"\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - watch\n  - list\nEOF\n```\n\n----------------------------------------\n\nTITLE: Setting up RBAC ClusterRole\nDESCRIPTION: Bash command to create a ClusterRole with permissions to read Kubernetes resources like pods, nodes, services, and ingresses.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - services\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: \n  - \"networking.k8s.io\"\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - watch\n  - list\nEOF\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional PostgreSQL Metrics Configuration\nDESCRIPTION: YAML configuration snippet showing how to enable specific optional metrics in the PostgreSQL metrics collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/postgresqlreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Input JSON for CSV Parsing with Timestamp\nDESCRIPTION: Example JSON input for CSV parsing with embedded timestamp parsing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/csv_parser.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"\",\n  \"body\": {\n    \"message\": \"2021-03-17,debug,Debug Message\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Duration to Minutes in Go\nDESCRIPTION: Converts a time.Duration value to a floating-point number of minutes. Returns the duration as a float64.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_35\n\nLANGUAGE: Go\nCODE:\n```\nMinutes(Duration(\"1h\"))\n```\n\n----------------------------------------\n\nTITLE: Defining HealthCheckRequest Message in Protocol Buffers\nDESCRIPTION: Protocol Buffers definition for the HealthCheckRequest message used in gRPC health check service. It contains a single string field for the service name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_6\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage HealthCheckRequest {\n  string service = 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining HealthCheckRequest Message in Protocol Buffers\nDESCRIPTION: Protocol Buffers definition for the HealthCheckRequest message used in gRPC health check service. It contains a single string field for the service name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/healthcheckv2extension/README.md#2025-04-10_snippet_6\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage HealthCheckRequest {\n  string service = 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Resulting Log Record with Extracted Headers\nDESCRIPTION: Example showing how the Kafka record headers appear as resource attributes in the resulting log record after header extraction processing.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkareceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n{\n  ...\n  body: Hello,\n  resource: {\n    kafka.header.header1: value1,\n    kafka.header.header2: value2,\n  },\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLP Encoding Extension with Protobuf Protocol\nDESCRIPTION: YAML configuration example for setting up the OTLP encoding extension using the Protobuf protocol for data encoding.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/encoding/otlpencodingextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  otlp_encoding:\n    protocol: otlp_proto\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Service Name Filters in YAML\nDESCRIPTION: Examples of YAML configurations for filtering ECS services based on name patterns and container names. Includes options for overriding default metrics paths.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Example 1: Matches all containers that are started by retail-* service\nname_pattern: ^retail-.*$\n---\n# Example 2: Matches all container with name java-api in cash-app service \nname_pattern: ^cash-app$\ncontainer_name_pattern: ^java-api$\n---\n# Example 3: Override default metrics_path (i.e. /metrics)\nname_pattern: ^log-replay-worker$\nmetrics_path: /v3/metrics\n```\n\n----------------------------------------\n\nTITLE: Configuring ECS Service Name Filters in YAML\nDESCRIPTION: Examples of YAML configurations for filtering ECS services based on name patterns and container names. Includes options for overriding default metrics paths.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/ecsobserver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Example 1: Matches all containers that are started by retail-* service\nname_pattern: ^retail-.*$\n---\n# Example 2: Matches all container with name java-api in cash-app service \nname_pattern: ^cash-app$\ncontainer_name_pattern: ^java-api$\n---\n# Example 3: Override default metrics_path (i.e. /metrics)\nname_pattern: ^log-replay-worker$\nmetrics_path: /v3/metrics\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional SQL Server Metrics in OpenTelemetry Configuration\nDESCRIPTION: YAML configuration to enable a specific optional metric in the OpenTelemetry Collector. Replace <metric_name> with the actual metric name to enable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Processor for SAPM Receiver Token Passthrough\nDESCRIPTION: YAML configuration for the batch processor that replaces the deprecated 'access_token_passthrough' option in SAPM receiver. This configuration enables metadata keys to be passed through, specifically the X-Sf-Token header.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nbatch:\n  metadata_keys: [X-Sf-Token]\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Exporter Metric Exclusions in YAML\nDESCRIPTION: YAML configuration snippet showing additional metrics excluded by default in the SignalFx exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_47\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  signalfx:\n    exclude_metrics:\n      - system.disk.io_time\n      - system.disk.operation_time\n      - system.disk.weighted_io_time\n      - system.network.connections\n      - system.processes.count\n      - system.processes.created\n```\n\n----------------------------------------\n\nTITLE: SigV4 Extension Directory Structure Example\nDESCRIPTION: Example YAML configuration files used for testing the SigV4 authentication extension implementation. Includes both valid and invalid configuration scenarios.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nconfig.yaml\nconfig_bad.yaml\n```\n\n----------------------------------------\n\nTITLE: Enabling Native Histograms Support\nDESCRIPTION: Command-line option to enable processing of Prometheus native histograms and conversion to OpenTelemetry exponential histograms.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n\"--feature-gates=receiver.prometheusreceiver.EnableNativeHistograms\"\n```\n\n----------------------------------------\n\nTITLE: GitHub Webhook Configuration for Traces\nDESCRIPTION: YAML configuration for setting up GitHub webhook receiver for collecting traces from GitHub Actions events.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/githubreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n    github:\n        webhook:\n            endpoint: localhost:19418\n            path: /events\n            health_path: /health\n            secret: ${env:SECRET_STRING_VAR}\n            required_headers:\n                WAF-Header: \"value\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Assign Keys Operator for Attributes Field in YAML\nDESCRIPTION: YAML configuration for the assign_keys operator that transforms a list in a nested attributes field into a map with specified keys. The field parameter uses dot notation to target the nested field.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/assign_keys.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: assign_keys\n  field: attributes.input\n  keys: [\"foo\", \"bar\"]\n```\n\n----------------------------------------\n\nTITLE: Disabling Default SQL Server Metrics in OpenTelemetry Configuration\nDESCRIPTION: YAML configuration to disable a specific default metric in the OpenTelemetry Collector. Replace <metric_name> with the actual metric name to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Using Complex Indexing in OTTL Conditions\nDESCRIPTION: Example of using complex indexing of Paths and Converters in OTTL conditions to access nested items.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_30\n\nLANGUAGE: yaml\nCODE:\n```\nprocessor:\n  filter:\n    traces:\n      span:\n        - 'attributes[\"http.status_code\"][0] == 200'\n```\n\n----------------------------------------\n\nTITLE: Enabled Elasticsearch Index Metrics\nDESCRIPTION: List of Elasticsearch index metrics that are now enabled by default in the Elasticsearch receiver component.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_13\n\nLANGUAGE: markdown\nCODE:\n```\nelasticsearch.index.documents\nelasticsearch.index.operations.merge.current\nelasticsearch.index.segments.count\n```\n\n----------------------------------------\n\nTITLE: Kubernetes CRI Log Output Example\nDESCRIPTION: JSON output showing how the recombine operator processes the Kubernetes CRI logs, combining the multi-part log entries into complete log records.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/recombine.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"timestamp\": \"2020-12-04T13:03:38.41149-05:00\",\n    \"severity\": 0,\n    \"body\": {\n      \"message\": \"Single entry log 1\",\n      \"logtag\": \"F\",\n      \"stream\": \"stdout\",\n      \"timestamp\": \"2016-10-06T00:17:09.669794202Z\"\n    }\n  },\n  {\n    \"timestamp\": \"2020-12-04T13:03:38.411664-05:00\",\n    \"severity\": 0,\n    \"body\": {\n      \"message\": \"This is a very very long line that is really really long and spans across multiple log entries\",\n      \"logtag\": \"F\",\n      \"stream\": \"stdout\",\n      \"timestamp\": \"2016-10-06T00:17:10.113242941Z\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS EMF Exporter with Resource to Telemetry Conversion\nDESCRIPTION: This configuration enables conversion of resource attributes to metric labels in the AWS EMF exporter. This is disabled by default and must be explicitly enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awsemfexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n    awsemf:\n        region: 'us-west-2'\n        resource_to_telemetry_conversion:\n            enabled: true\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS EMF Exporter with Resource to Telemetry Conversion\nDESCRIPTION: This configuration enables conversion of resource attributes to metric labels in the AWS EMF exporter. This is disabled by default and must be explicitly enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awsemfexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n    awsemf:\n        region: 'us-west-2'\n        resource_to_telemetry_conversion:\n            enabled: true\n```\n\n----------------------------------------\n\nTITLE: Output JSON Entry After Removing a Value from Resource\nDESCRIPTION: This shows the resulting JSON entry after the remove operator has removed the 'otherkey' from the resource section.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": { },\n  \"attributes\": { },\n  \"body\": {\n    \"key\": \"val\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OTLP Arrow Receiver Keepalive Configuration\nDESCRIPTION: Configuration example for setting up keepalive parameters to manage stream lifetime and connection maintenance.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otelarrow:\n    protocols:\n      grpc:\n        keepalive:\n          server_parameters:\n            max_connection_age: 1m\n            max_connection_age_grace: 10m\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Process Metrics Configuration in YAML\nDESCRIPTION: YAML configuration to enable optional process metrics. Replace <metric_name> with the specific metric identifier that should be enabled.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/processscraper/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Custom TLS Configuration for SignalFx Exporter\nDESCRIPTION: Configuration example showing how to set custom CA files for SignalFx exporter's ingest and API clients to verify TLS servers.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_38\n\nLANGUAGE: yaml\nCODE:\n```\n\"ingest_tls\" and \"api_tls\" can be used to set the absolute path to the CA file \"ca_file\".\n```\n\n----------------------------------------\n\nTITLE: Configuring Tailsampling Numeric Attribute Example in YAML\nDESCRIPTION: Example YAML configuration for the tailsampling processor's numeric_attribute policy that demonstrates how to set a minimum value threshold for HTTP status codes. This configuration allows more flexible usage by requiring only min_value or max_value, not necessarily both.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n{\n  type: numeric_attribute,\n  numeric_attribute: {\n    key: http.status_code,\n    min_value: 400\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SAPM Exporter in YAML\nDESCRIPTION: Example configuration for the SAPM exporter, including access token, endpoint, connection settings, and other options. This demonstrates how to set up the exporter with various parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sapmexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  sapm:\n    access_token: YOUR_ACCESS_TOKEN\n    access_token_passthrough: true\n    endpoint: https://ingest.YOUR_SIGNALFX_REALM.signalfx.com/v2/trace\n    max_connections: 100\n    num_workers: 8\n    log_detailed_response: true\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Log Source for Datadog\nDESCRIPTION: Configuration example showing how to add a custom source to OTLP logs using resource attribute transformation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datadogexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  transform/logs:\n    log_statements:\n      - context: resource\n        statements:\n          - set(attributes[\"datadog.log.source\"], \"otel\")\n```\n\n----------------------------------------\n\nTITLE: UDP Input Generated Log Entry Format\nDESCRIPTION: Example of the JSON output format generated by the UDP input operator when receiving messages.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/udp_input.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"timestamp\": \"2020-04-30T12:10:17.656726-04:00\",\n  \"body\": \"message1\\nmessage2\\n\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Data Storage in YAML\nDESCRIPTION: YAML configuration for specifying the directory where the supervisor persists data to maintain state between restarts. By default, it uses '/var/lib/otelcol/supervisor' on POSIX systems and '%ProgramData%/Otelcol/Supervisor' on Windows.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/opampsupervisor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nstorage:\n  directory: \"/path/to/storage/dir\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Memory Metrics Configuration in YAML\nDESCRIPTION: YAML configuration example showing how to disable default memory metrics by setting the 'enabled' property to false for a specific metric.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/memoryscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Capturing X-Ray Segments with tcpdump for Server Application\nDESCRIPTION: Command to capture UDP traffic on port 2000 which contains X-Ray segments from an instrumented server application. The command uses tcpdump to extract the first 100 UDP packets and save them to a file.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/README.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo tcpdump -i any -A -c100 -v -nn udp port 2000 > xray_instrumented_server.txt\n```\n\n----------------------------------------\n\nTITLE: Adding a Value to Resource Using Expression in YAML\nDESCRIPTION: This configuration shows how to add a value to the resource using an expression with the 'add' operator in YAML.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/add.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n- type: add\n  field: resource.key2\n  value: EXPR(body.key1 + \"_suffix\")\n```\n\n----------------------------------------\n\nTITLE: Basic Kafka Exporter Configuration in YAML\nDESCRIPTION: Minimal configuration example showing how to set up the Kafka exporter with a local broker.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/kafkaexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  kafka:\n    brokers:\n      - localhost:9092\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Kinesis Data Firehose Receiver in YAML\nDESCRIPTION: Example YAML configuration for the AWS Kinesis Data Firehose Receiver. It specifies the endpoint, record type, access key, and TLS settings for secure communication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/awsfirehosereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  awsfirehose:\n    endpoint: 0.0.0.0:4433\n    record_type: cwmetrics\n    access_key: \"some_access_key\"\n    tls:\n      cert_file: server.crt\n      key_file: server.key\n```\n\n----------------------------------------\n\nTITLE: Ambiguous Output Structure After Nesting\nDESCRIPTION: The resulting structure after nesting attributes with ambiguous mappings, where the final value cannot be reliably predicted. This illustrates a known limitation of the nesting feature.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/sumologicprocessor/README.md#2025-04-10_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"a\": {\n    \"b\": {\n      \"c\": ...\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Encrypted Values in YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to use an AES-encrypted value in a configuration file. The encrypted value is prefixed with '${aes:' and suffixed with '}'.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/confmap/provider/aesprovider/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npassword: ${aes:RsEf6cTWrssi8tlssfs1AJs2bRMrVm2Ce5TaWPY=}\n```\n\n----------------------------------------\n\nTITLE: Disabling NTP Metrics Configuration in YAML\nDESCRIPTION: Configuration example showing how to disable specific NTP metrics in the OpenTelemetry Collector. The configuration allows for individual metrics to be disabled by setting their 'enabled' property to false.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/ntpreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Calculating Sampled Trace Percentage in Prometheus\nDESCRIPTION: This Prometheus query calculates the percentage of traces that were actually sampled. It's useful for monitoring the overall sampling rate of the tail sampling processor.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/README.md#2025-04-10_snippet_7\n\nLANGUAGE: prometheus\nCODE:\n```\notelcol_processor_tail_sampling_global_count_traces_sampled{sampled=\"true\"} / \notelcol_processor_tail_sampling_global_count_traces_sampled\n```\n\n----------------------------------------\n\nTITLE: Aggregating Labels in YAML\nDESCRIPTION: Aggregates all labels except 'state' using summation for system.cpu.usage metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\n# aggregate away all labels except `state` using summation\ninclude: system.cpu.usage\naction: update\noperations:\n  - action: aggregate_labels\n    label_set: [ state ]\n    aggregation_type: sum\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Telemetry Generator\nDESCRIPTION: Command to build a Docker image for the telemetry generator locally using Make.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/telemetrygen/README.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-telemetrygen\n```\n\n----------------------------------------\n\nTITLE: Configuring Envoy ALS Receiver in YAML\nDESCRIPTION: Basic configuration example for the Envoy ALS receiver showing how to set up the endpoint for receiving access log data. The endpoint defaults to localhost:19001 if not specified.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/envoyalsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  envoyals:\n    endpoint: 0.0.0.0:3500\n```\n\n----------------------------------------\n\nTITLE: Specifying ByteSize Using KiB Units in YAML\nDESCRIPTION: Demonstrates specifying approximately 5000 bytes using kibibyte (KiB) notation. KiB follows the binary convention where 1 KiB equals 1024 bytes, making 4.88KiB approximately 5000 bytes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/bytesize.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- type: some_operator\n  bytes: 4.88KiB\n```\n\n----------------------------------------\n\nTITLE: Prometheus Histogram Data Example\nDESCRIPTION: Example of a Prometheus histogram metric showing bucket distribution across two label values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/prometheusreceiver/DESIGN.md#2025-04-10_snippet_6\n\nLANGUAGE: prometheus\nCODE:\n```\n# HELP hist_test This is my histogram vec\n# TYPE hist_test histogram\nhist_test_bucket{t1=\"1\",,le=\"10.0\"} 1.0\nhist_test_bucket{t1=\"1\",le=\"20.0\"} 3.0\nhist_test_bucket{t1=\"1\",le=\"+inf\"} 10.0\nhist_test_sum{t1=\"1\"} 100.0\nhist_test_count{t1=\"1\"} 10.0\nhist_test_bucket{t1=\"2\",,le=\"10.0\"} 10.0\nhist_test_bucket{t1=\"2\",le=\"20.0\"} 30.0\nhist_test_bucket{t1=\"2\",le=\"+inf\"} 100.0\nhist_test_sum{t1=\"2\"} 10000.0\nhist_test_count{t1=\"2\"} 100.0\n```\n\n----------------------------------------\n\nTITLE: Defining Sigv4 Authentication Configuration Structure\nDESCRIPTION: Defines the main configuration structure for AWS Sigv4 authentication including region, service, and role assumption settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype Config struct {\n    Region string `mapstructure:\"region,omitempty\"`\n    Service string `mapstructure:\"service,omitempty\"`\n    AssumeRole AssumeRole `mapstructure:\"assume_role\"`\n\tcredsProvider *aws.CredentialsProvider\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping Couchbase Environment\nDESCRIPTION: Docker Compose command to shut down all services and clean up resources.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/couchbase/README.md#2025-04-10_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndocker-compose down\n```\n\n----------------------------------------\n\nTITLE: Configuring Severity Parser in YAML for OpenTelemetry Collector\nDESCRIPTION: This YAML configuration sets up a severity parser that extracts severity from a specific field without using a preset. It defines a custom mapping for the 'error' severity level.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- type: severity_parser\n  parse_from: body.severity_field\n  preset: none\n  mapping:\n    error: nooo!\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Explorer Exporter in YAML\nDESCRIPTION: Example YAML configuration for the Azure Data Explorer exporter, including cluster URI, authentication details, database and table names, and ingestion settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/azuredataexplorerexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  azuredataexplorer:\n    # Kusto cluster uri\n    cluster_uri: \"https://CLUSTER.kusto.windows.net\"\n    # Client ID\n    application_id: \"f80da32c-108c-415c-a19e-643f461a677a\"\n    # The client secret for the client\n    application_key: \"xx-xx-xx-xx\"\n    # The tenant\n    tenant_id: \"21ff9e36-fbaa-43c8-98ba-00431ea10bc3\"\n    # A managed identity id to authenticate with. \n    # Set to \"system\" for system-assigned managed identity.\n    # Set the MI client ID (GUID) for user-assigned managed identity.\n    managed_identity_id: \"z80da32c-108c-415c-a19e-643f461a677a\"\n    # Database for the logs\n    db_name: \"oteldb\"\n    # Metric table name\n    metrics_table_name: \"OTELMetrics\"\n    # Log table name\n    logs_table_name: \"OTELLogs\"\n    # Traces table\n    traces_table_name: \"OTELTraces\"\n    # Metric table mapping name\n    metrics_table_json_mapping: \"otelmetrics_mapping\"\n    # Log table mapping name\n    logs_table_json_mapping: \"otellogs_mapping\"\n    # Traces mapping table\n    traces_table_json_mapping: \"oteltraces_mapping\"\n    # Type of ingestion managed or queued\n    ingestion_type : \"managed\"\n    #other available exporter helper options, see more here: https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/exporterhelper/README.md\n    # timeout: 10s\n    # sending_queue:\n    #   enabled: true\n    #   num_consumers: 2\n    #   queue_size: 10\n    # retry_on_failure:\n    #   enabled: true\n    #   initial_interval: 10s\n    #   max_interval: 60s\n    #   max_elapsed_time: 10m\n```\n\n----------------------------------------\n\nTITLE: Creating ClusterRoleBinding\nDESCRIPTION: Bash command to create a ClusterRoleBinding that associates the ServiceAccount with the ClusterRole.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: otelcontribcol\n  labels:\n    app: otelcontribcol\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: otelcontribcol\nsubjects:\n- kind: ServiceAccount\n  name: otelcontribcol\n  namespace: default\nEOF\n```\n\n----------------------------------------\n\nTITLE: Normalizing Process CPU Utilization in Host Metrics Receiver\nDESCRIPTION: Enables the feature gate 'receiver.hostmetrics.normalizeProcessCPUUtilization' which changes the value of the 'process.cpu.utilization' metric by dividing it by the number of CPU cores. This results in a normalized value between 0 and 1.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_18\n\nLANGUAGE: go\nCODE:\n```\nreceiver/hostmetrics: enable feature gate `receiver.hostmetrics.normalizeProcessCPUUtilization`\n```\n\n----------------------------------------\n\nTITLE: Flattening JSON Object with Prefix in OTTL flatten Function\nDESCRIPTION: Shows the result of using the OTTL flatten function with a prefix parameter. The prefix 'app' is added to the start of each flattened key.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"app.name\": \"test\",\n    \"app.address.street\": \"first\",\n    \"app.address.house\": 1234,\n    \"app.occupants.0\": \"user 1\",\n    \"app.occupants.1\": \"user 2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing HTTP Request Cloning\nDESCRIPTION: Creates a thread-safe copy of an HTTP request.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_7\n\nLANGUAGE: go\nCODE:\n```\nfunc cloneRequest(r *http.Request) *http.Request {\n\tr2 := new(http.Request)\n\t*r2 = *r\n\tr2.Header = make(http.Header, len(r.Header))\n\tfor k, s := range r.Header {\n\t\tr2.Header[k] = append([]string(nil), s...)\n\t}\n\treturn r2\n}\n```\n\n----------------------------------------\n\nTITLE: UserAgent Parser Firefox Example in YAML\nDESCRIPTION: Example output from the UserAgent parser function when parsing a Firefox browser user-agent string. Shows the structured format with name, version, and original string fields.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_71\n\nLANGUAGE: yaml\nCODE:\n```\n\"user_agent.name\": \"Firefox\"\n\"user_agent.version\": \"126.0\"\n\"user_agent.original\": \"Mozilla/5.0 (X11; Linux x86_64; rv:126.0) Gecko/20100101 Firefox/126.0\"\n```\n\n----------------------------------------\n\nTITLE: Pubsub Subscription Filter Expression\nDESCRIPTION: Example of a Pubsub subscription filter expression to filter messages based on content type and CE type attributes.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/googlecloudpubsubreceiver/README.md#2025-04-10_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nattributes.ce-type = \"org.opentelemetry.otlp.traces.v1\"\nAND\nattributes.content-type = \"application/protobuf\"\n```\n\n----------------------------------------\n\nTITLE: Defining AWS X-Ray Trace Segment in JSON\nDESCRIPTION: This JSON object represents a trace segment in AWS X-Ray format. It includes essential fields such as trace_id, id (segment ID), name, start_time, and status flags like in_progress and Dummy.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/minInProgress.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f187253-6a106696d56b1f4ef9eba2ed\",\n    \"id\": \"5cc4a447f5d4d696\",\n    \"name\": \"LongOperation\",\n    \"start_time\": 1595437651.680097,\n    \"in_progress\": true,\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Processor for Manual Attribute Setting in YAML\nDESCRIPTION: Example YAML configuration for manually setting location, cluster, or namespace attributes using the resource processor when running in non-GCP environments.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlemanagedprometheusexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resource:\n    attributes:\n    - key: \"location\"\n      value: \"us-east1\"\n      action: upsert\n```\n\n----------------------------------------\n\nTITLE: Flattening JSON Object with Depth Limit in OTTL flatten Function\nDESCRIPTION: Demonstrates the effect of using the OTTL flatten function with a depth parameter of 2. The function only flattens nested maps up to the specified depth.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"0\": {\n    \"1\": {\n      \"2\": {\n        \"3\": {\n          \"4\": \"value\"\n        }\n      }\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"0.1.2\": {\n    \"3\": {\n      \"4\": \"value\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining signingRoundTripper Struct in Go\nDESCRIPTION: Defines a custom RoundTripper struct that implements the http.RoundTripper interface for Sigv4 authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_8\n\nLANGUAGE: go\nCODE:\n```\ntype signingRoundTripper struct {\n    transport http.RoundTripper\n    signer *v4.Signer\n    region string\n    service string\n    credsProvider *aws.CredentialsProvider\n    awsSDKInfo string\n\tlogger *zap.Logger\n}\n```\n\n----------------------------------------\n\nTITLE: DataSet Exporter Log Configuration with All Options Enabled\nDESCRIPTION: Configuration example showing all logging options enabled with custom prefixes and separators.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/datasetexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlogs:\n  export_resource_info_on_event: true\n  export_resource_prefix: \"r.\"\n  export_scope_info_on_event: true\n  export_scope_prefix: \"s.\"\n  decompose_complex_message_field: true\n  decomposed_complex_message_prefix: \"m.\"\n  export_separator: \"-\"\n  export_distinguishing_suffix: \"_\"\n```\n\n----------------------------------------\n\nTITLE: Transforming Resource Attributes to Metric Labels in YAML\nDESCRIPTION: YAML configuration for the transform processor to copy resource attributes into metric labels for easier querying.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusexporter/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nprocessor:\n  transform:\n    metric_statements:\n      - context: datapoint\n        statements:\n        - set(attributes[\"namespace\"], resource.attributes[\"k8s.namespace.name\"])\n        - set(attributes[\"container\"], resource.attributes[\"k8s.container.name\"])\n        - set(attributes[\"pod\"], resource.attributes[\"k8s.pod.name\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring OpAMP Extension in OpenTelemetry Collector\nDESCRIPTION: YAML configuration structure for the OpAMP extension in the OpenTelemetry Collector. Defines the endpoint for the OpAMP server connection and instance_uid settings. The endpoint supports WS or HTTP transport and includes authentication capabilities. The instance_uid is auto-generated if not specified.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/opampsupervisor/specification/README.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  opamp:\n    # OpAMP server URL. Supports WS or plain http transport,\n    # based on the scheme of the URL (ws,wss,http,https).\n    # Any other settings defined in ClientConfig, squashed. This\n    # includes ability to specify an \"auth\" setting that refers\n    # to an extension that implements the Authentication interface.\n    endpoint:\n\n    # UUID formatted as a 36 character string in canonical\n    # representation. Auto-generated on start if missing.\n    # Injected by Supervisor.\n    # Note: can be deprecated after Collector issue #6599\n    # is implemented.\n    instance_uid:\n```\n\n----------------------------------------\n\nTITLE: Accessing TelemetrySettings in Go for OTTL Function Logging\nDESCRIPTION: To emit logs inside an OTTL function, add a parameter of type component.TelemetrySettings to the function signature. OTTL will inject the TelemetrySettings that were passed to NewParser into the function, which can then be used to emit logs.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/LANGUAGE.md#2025-04-10_snippet_4\n\nLANGUAGE: go\nCODE:\n```\ncomponent.TelemetrySettings\n```\n\n----------------------------------------\n\nTITLE: Configuring SNMP Receiver with Multiple Metrics\nDESCRIPTION: Example configuration demonstrating various SNMP metric collection scenarios including resource attributes, scalar and column OIDs, and attribute mappings. Shows configuration for SNMPv3 authentication, metric types like sum and gauge, and different aggregation patterns.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/snmpreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  snmp:\n    collection_interval: 60s\n    endpoint: udp://localhost:161\n    version: v3\n    security_level: auth_priv\n    user: otel\n    auth_type: \"MD5\"\n    auth_password: ${env:SNMP_AUTH_PASSWORD}\n    privacy_type: \"DES\"\n    privacy_password: ${env:SNMP_PRIVACY_PASSWORD}\n\n    resource_attributes:\n      resource_attr.name.1:\n        indexed_value_prefix: probe\n      resource_attr.name.2:\n        oid: \"1.1.1.1\"\n\n    attributes:\n      attr.name.1:\n        value: a2_new_key\n        enum:\n          - in\n          - out\n      attr.name.2:\n        indexed_value_prefix: device\n      attr.name.3:\n        oid: \"2.2.2.2\"\n\n    metrics:\n      # This metric will have multiple datapoints wil 1 attribute on each.\n      # Each datapoint will have a (hopefully) different attribute value\n      metric.name.1:\n        unit: \"1\"\n        sum:\n          aggregation: cumulative\n          monotonic: true\n          value_type: int\n        column_oids:\n          - oid: \"2.2.2.1\"\n            attributes:\n              - name: attr.name.3\n      # This metric will have multiple datapoints with 2 attributes on each.\n      # Each datapoint will have a guaranteed different attribute indexed value for 1 of the attributes.\n      # Half of the datapoints will have the other attribute with a value of \"in\".\n      # The other half will have the other attribute with a value of \"out\".\n      metric.name.2:\n        unit: \"By\"\n        gauge:\n          value_type: int\n        column_oids:\n          - oid: \"3.3.3.3\"\n            attributes:\n              - name: attr.name.2\n              - name: attr.name.1\n                value: in\n          - oid: \"2\"\n            attributes:\n              - name: attr.name.2\n              - name: attr.name.1\n                value: out\n      # This metric will have 2 datapoints with 1 attribute on each\n      # One datapoints will have an attribute value of \"in\".\n      # The other will have an attribute value of \"out\".\n      metric.name.3:\n        unit: \"By\"\n        sum:\n          aggregation: delta\n          monotonic: false\n          value_type: double\n        scalar_oids:\n          - oid: \"4.4.4.4.0\"\n            attributes:\n              - name: attr.name.1\n                value: in\n          - oid: \"4.4.4.5.0\"\n            attributes:\n              - name: attr.name.1\n                value: out\n      # This metric will have metrics created with each attached to a different resource.\n      # Each resource will have a resource attribute with a guaranteed unique value based on the index.\n      metric.name.4:\n        unit: \"By\"\n        gauge:\n          value_type: int\n        column_oids:\n          - oid: \"5.5.5.5\"\n            resource_attributes:\n              - resource_attr.name.1\n      # This metric will have metrics created with each attached to a different resource.\n      # Each resource will have a resource attribute with a hopefully unique value.\n      metric.name.5:\n        unit: \"By\"\n        gauge:\n          value_type: int\n        column_oids:\n          - oid: \"1.1.1.2\"\n            resource_attributes:\n              - resource_attr.name.2\n```\n\n----------------------------------------\n\nTITLE: Configuring OTLPHTTP Exporter as SAPM Replacement in YAML\nDESCRIPTION: Example configuration for the OTLPHTTP exporter as a replacement for the deprecated SAPM exporter. It specifies the traces endpoint and headers for authentication.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sapmexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otlphttp:\n    traces_endpoint: \"${SPLUNK_INGEST_URL}/v2/trace/otlp\"\n    headers:\n        \"X-SF-Token\": \"${SPLUNK_ACCESS_TOKEN}\"\n```\n\n----------------------------------------\n\nTITLE: CheckAPI YAML Configuration Structure\nDESCRIPTION: YAML configuration template for CheckAPI showing the structure for ignored paths, allowed functions, and ignored functions with their parameters and return types.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/checkapi/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nignored_paths:\n  - <exact relative paths of Golang modules to ignore>\nallowed_functions:\n  - <at least one function match must be present.>\n  - name: <name of function>\n    parameters: <list of parameters by type>\n    return_types: <list of return types>\n\nignored_functions:\n  - <regular expressions of ignored functions. At least one match must be present to ignore the function.>\n```\n\n----------------------------------------\n\nTITLE: JSON Log Search in ClickHouse\nDESCRIPTION: SQL query to find logs based on JSON field values\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE JSONExtractFloat(Body, 'bytes') > 1000\n  AND TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Configuring SignalFx Receiver in YAML\nDESCRIPTION: This snippet demonstrates how to configure the SignalFx receiver in the OpenTelemetry Collector's YAML configuration file. It shows both basic and advanced setups, including TLS configuration and access token passthrough.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/signalfxreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  signalfx:\n  signalfx/advanced:\n    access_token_passthrough: true\n    tls:\n      cert_file: /test.crt\n      key_file: /test.key\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Metrics in JSON Format\nDESCRIPTION: Collection of SQL Server performance counter readings in JSON format. Each object includes counter metadata (name, type, instance, object) along with the measurement name, SQL instance identifier, computer name, and metric value. These metrics represent various aspects of SQL Server performance monitoring.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"counter\":\"Readahead pages/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Buffer Manager\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"379\"\n   }\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Metrics in JSON Format\nDESCRIPTION: Collection of SQL Server performance counter readings in JSON format. Each object includes counter metadata (name, type, instance, object) along with the measurement name, SQL instance identifier, computer name, and metric value. These metrics represent various aspects of SQL Server performance monitoring.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n      \"counter\":\"Readahead pages/sec\",\n      \"counter_type\":\"272696576\",\n      \"instance\":\"\",\n      \"measurement\":\"sqlserver_performance\",\n      \"object\":\"SQLServer:Buffer Manager\",\n      \"sql_instance\":\"8cac97ac9b8f\",\n      \"computer_name\":\"abcde\",\n      \"value\":\"379\"\n   }\n```\n\n----------------------------------------\n\nTITLE: File Path Reference in Markdown\nDESCRIPTION: Reference to the Entry type documentation file path in the stanza module\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/README.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[`entry.Entry`](./docs/types/entry.md)\n```\n\n----------------------------------------\n\nTITLE: Configuring Disk Scraper in Host Metrics Receiver\nDESCRIPTION: Configuration for the disk scraper that allows including or excluding specific devices with strict or regexp matching.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndisk:\n  <include|exclude>:\n    devices: [ <device name>, ... ]\n    match_type: <strict|regexp>\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Stats Receiver for Metric Deprecation\nDESCRIPTION: Example configuration showing how to disable deprecated CPU percent metrics and enable the new CPU utilization metrics that align with OpenTelemetry specifications.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/dockerstatsreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  docker_stats:\n    metrics:\n      container.cpu.percent:\n        enabled: false\n      container.cpu.utilization:\n        enabled: true\n```\n\n----------------------------------------\n\nTITLE: Translated Time Series Metric Definition\nDESCRIPTION: Markdown table defining the otelcol_exporter_prometheusremotewrite_translated_time_series metric that tracks translated Prometheus time series\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/documentation.md#2025-04-10_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Unit | Metric Type | Value Type | Monotonic |\n| ---- | ----------- | ---------- | --------- |\n| 1 | Sum | Int | true |\n```\n\n----------------------------------------\n\nTITLE: Unix Time Converter in Golang\nDESCRIPTION: The Unix Converter converts values to Unix epoch time format. It requires seconds as an int64 parameter and accepts an optional nanoseconds parameter. The function returns a time.Time object.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_64\n\nLANGUAGE: go\nCODE:\n```\nUnix(1672527600)\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Exporter for Troubleshooting in YAML\nDESCRIPTION: YAML configuration for filtering metrics and using the debug exporter with detailed verbosity to troubleshoot timeseries collision issues in Google Managed Prometheus.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/googlemanagedprometheusexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  filter:\n    error_mode: ignore\n    metrics:\n      - name != \"problematic.metric.name\"\nexporters:\n  debug:\n    verbosity: detailed\n```\n\n----------------------------------------\n\nTITLE: Creating Default Config for Sigv4 Authenticator in Go\nDESCRIPTION: Creates a default configuration struct for the Sigv4 Authenticator. In this case, it only sets the ID as there are no sensible defaults.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/sigv4authextension/design.md#2025-04-10_snippet_12\n\nLANGUAGE: go\nCODE:\n```\nfunc createDefaultConfig() component.Config {\n    return &Config{}\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Permissive Label Sanitization in OpenTelemetry Collector\nDESCRIPTION: This command shows how to enable the permissive label sanitization feature when running the OpenTelemetry Collector. It uses the feature gates option to activate the more lenient label handling.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/translator/prometheus/README.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ otelcol --config=config.yaml --feature-gates=pkg.translator.prometheus.PermissiveLabelSanitization\n```\n\n----------------------------------------\n\nTITLE: Output: OpenTelemetry Metrics Format\nDESCRIPTION: The transformed metrics data in OpenTelemetry format, including resource attributes, scope information, and structured metric data points with proper timestamps and values.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/huaweicloudcesreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resourceMetrics\": [\n    {\n      \"resource\": {\n        \"attributes\": {\n            \"cloud.provider\": \"huawei_cloud\",\n            \"project.id\": \"project_1\",\n            \"region.id\": \"eu-west-101\",\n            \"system.namespace\": \"SYS.ECS\",\n          }\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": { \"name\": \"huawei_cloud_ces\", \"version\": \"v1\" },\n          \"metrics\": [\n            {\n              \"name\": \"cpu_util\",\n              \"unit\": \"%\",\n              \"gauge\": {\n                \"dataPoints\": [\n                  { \"timeUnixNano\": \"1722580500000000000\", \"asDouble\": 10 },\n                  { \"timeUnixNano\": \"1722580800000000000\", \"asDouble\": 20 }\n                ]\n              },\n              \"metadata\": {\n                    \"instance_id\":  \"faea5b75-e390-4e2b-8733-9226a9026070\",\n                }\n            }\n          ]\n        },\n        {\n          \"scope\": { \"name\": \"huawei_cloud_ces\", \"version\": \"v1\" },\n          \"metrics\": [\n            {\n              \"name\": \"mem_util\",\n              \"unit\": \"%\",\n              \"gauge\": {\n                \"dataPoints\": [\n                  { \"timeUnixNano\": \"1722580500000000000\", \"asDouble\": 30 },\n                  { \"timeUnixNano\": \"1722580800000000000\", \"asDouble\": 40 }\n                ]\n              },\n              \"metadata\": {\n                  \"instance_id\": \"abcea5b75-e390-4e2b-8733-9226a9026070\",\n              }\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"resource\": {\n        \"attributes\": {\n            \"cloud.provider\": \"huawei_cloud\",\n            \"project.id\": \"project_1\",\n            \"region.id\": \"eu-west-101\",\n            \"system.namespace\": \"SYS.VPC\",\n          }\n      },\n      \"scopeMetrics\": [\n        {\n          \"scope\": { \"name\": \"huawei_cloud_ces\", \"version\": \"v1\" },\n          \"metrics\": [\n            {\n              \"name\": \"upstream_bandwidth_usage\",\n              \"unit\": \"bits/s\",\n              \"gauge\": {\n                \"dataPoints\": [\n                  { \"timeUnixNano\": \"1722580500000000000\", \"asDouble\": 1024 },\n                  { \"timeUnixNano\": \"1722580800000000000\", \"asDouble\": 2048 }\n                ]\n              },\n              \"metadata\": {\n                    \"publicip_id\":  \"test-baae-4dd9-ad3f-1234\",\n                }\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying ByteSize Using Raw Bytes in YAML\nDESCRIPTION: Demonstrates how to specify 5000 bytes using a raw numeric value in YAML configuration. This approach uses the direct byte count without any unit conversion.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/bytesize.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- type: some_operator\n  bytes: 5000\n```\n\n----------------------------------------\n\nTITLE: OTLP Arrow Exporter Configuration\nDESCRIPTION: Complementary exporter configuration showing stream lifetime settings that work with the receiver's keepalive configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  otelarrow:\n    timeout: 30s\n    arrow:\n      max_stream_lifetime: 9m30s\n    endpoint: ...\n    tls: ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Metric Inclusion Override in SignalFx Exporter (YAML)\nDESCRIPTION: Example configuration to include specific metrics that would otherwise be dropped by default. This shows how to configure both translated metrics and metrics in OTel convention.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/signalfxexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ninclude_metrics:\n  # When sending in translated metrics.\n  - metric_names: [cpu.interrupt, cpu.user, cpu.system]\n  # When sending in metrics in OTel convention.\n  - metric_name: system.cpu.time\n    dimensions:\n      state: [interrupt, user, system]\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Memcached Metrics in YAML Configuration\nDESCRIPTION: YAML configuration snippet showing how to disable specific metrics in the Memcached receiver. This configuration can be applied to any of the default metrics mentioned in the documentation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/memcachedreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Memcached Metrics in YAML Configuration\nDESCRIPTION: YAML configuration snippet showing how to disable specific metrics in the Memcached receiver. This configuration can be applied to any of the default metrics mentioned in the documentation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/memcachedreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Creating ServiceAccount for OpenTelemetry Collector\nDESCRIPTION: Bash commands to create a Kubernetes ServiceAccount required for the OpenTelemetry Collector deployment. This service account will be used for authentication with the Kubernetes API.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: otelcontribcol\n  name: otelcontribcol\nEOF\n```\n\n----------------------------------------\n\nTITLE: Disabling Specific Nginx Metrics in YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to disable specific Nginx metrics in the configuration. Replace <metric_name> with the name of the metric you want to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/nginxreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring SiganlFx Receiver with BatchProcessor for Access Token in YAML\nDESCRIPTION: YAML configuration example showing the recommended approach for handling access tokens with SignalFx receiver. Instead of using the deprecated access_token_passthrough option, users should enable include_metadata and configure the batch processor to pass the X-Sf-Token metadata key.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CHANGELOG.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nbatch:\n  metadata_keys: [X-Sf-Token]\n```\n\n----------------------------------------\n\nTITLE: Resolved YAML Configuration After Decryption\nDESCRIPTION: This YAML snippet shows how the configuration looks after the AES Provider has decrypted the value. The encrypted password '1' has been resolved to plaintext.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/confmap/provider/aesprovider/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\npassword: '1'\n```\n\n----------------------------------------\n\nTITLE: Advanced OTLP Arrow Receiver Configuration with Protocols\nDESCRIPTION: Example showing how to configure gRPC protocol settings for the OTLP Arrow receiver.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/otelarrowreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otelarrow:\n    protocols:\n      grpc:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Example Go Metric Structure After Grouping by Host Name\nDESCRIPTION: A Go representation showing the structure of metrics after applying the groupbyattrs processor with 'host.name' as the grouping key. The metrics are now grouped under different Resources based on the host name.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbyattrsprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: go\nCODE:\n```\nResource {host.name=\"localhost\",source=\"prom\"}\n  Metric \"dont-move\" (Gauge)\n    DataPoint {id=\"eth0\"}\n\nResource {host.name=\"host-A\",source=\"prom\"}\n  Metric \"gauge-1\"\n    DataPoint {id=\"eth0\"}\n    DataPoint {id=\"eth0\"}\n    DataPoint {id=\"eth0\"}\n    DataPoint {id=\"eth0\"}\n  Metric \"mixed-type\" (GAUGE)\n    DataPoint {id=\"eth0\"}\n    DataPoint {id=\"eth0\"}\n  Metric \"mixed-type\" (SUM)\n    DataPoint {id=\"eth0\"}\n    DataPoint {id=\"eth0\"}\n\nResource {host.name=\"host-B\",source=\"prom\"}\n  Metric \"gauge-1\"\n    DataPoint {id=\"eth0\"}\n    DataPoint {id=\"eth0\"}\n  Metric \"mixed-type\" (GAUGE)\n    DataPoint {id=\"eth0\"}\n```\n\n----------------------------------------\n\nTITLE: Creating RoleBinding for Namespace-Scoped OpenTelemetry Collector in Kubernetes\nDESCRIPTION: This YAML configuration creates a RoleBinding that binds the previously created Role to a ServiceAccount named 'otelcontribcol' in the default namespace. It's used in conjunction with the namespace-scoped Role.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n<<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: otelcontribcol\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: otelcontribcol\nsubjects:\n  - kind: ServiceAccount\n    name: otelcontribcol\n    namespace: default\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Stats Receiver in YAML\nDESCRIPTION: Example configuration for the Docker Stats receiver with custom settings for endpoint, collection interval, timeout, API version, label mapping, environment variable mapping, and container exclusion patterns.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/dockerstatsreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  docker_stats:\n    endpoint: http://example.com/\n    collection_interval: 2s\n    timeout: 20s\n    api_version: \"1.24\"\n    container_labels_to_metric_labels:\n      my.container.label: my-metric-label\n      my.other.container.label: my-other-metric-label\n    env_vars_to_metric_labels:\n      MY_ENVIRONMENT_VARIABLE: my-metric-label\n      MY_OTHER_ENVIRONMENT_VARIABLE: my-other-metric-label\n    excluded_images:\n      - undesired-container\n      - /.*undesired.*/\n      - another-*-container\n    metrics: \n      container.cpu.usage.percpu:\n        enabled: true\n      container.network.io.usage.tx_dropped:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Implementing Trace Test for New Exporter/Receiver in Go\nDESCRIPTION: This snippet demonstrates how to implement a trace test for a new exporter and receiver in the OpenTelemetry Collector Testbed. It shows the structure of a test function that sets up test cases with different data senders and receivers, and runs scenarios with specified resource specifications and processors.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/testbed/README.md#2025-04-10_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nfunc TestTrace10kSPS(t *testing.T) {\n\ttests := []struct {\n\t\tname         string\n\t\tsender       testbed.DataSender\n\t\treceiver     testbed.DataReceiver\n\t\tresourceSpec testbed.ResourceSpec\n\t}{\n\t\t{\n\t\t\t\"NewExporterOrReceiver\",\n\t\t\ttestbed.NewXXXDataSender(testbed.DefaultHost, testutil.GetAvailablePort(t)),\n\t\t\ttestbed.NewXXXDataReceiver(testutil.GetAvailablePort(t)),\n\t\t\ttestbed.ResourceSpec{\n\t\t\t\tExpectedMaxCPU: XX,\n\t\t\t\tExpectedMaxRAM: XX,\n\t\t\t},\n\t\t},\n\t\t...\n\t}\n\tprocessors := []ProcessorNameAndConfigBody{\n\t\t{\n\t\t\tName: \"batch\",\n\t\t\tBody: `\n  batch:\n`,\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tScenario10kItemsPerSecond(\n\t\t\t\tt,\n\t\t\t\ttest.sender,\n\t\t\t\ttest.receiver,\n\t\t\t\ttest.resourceSpec,\n\t\t\t\tperformanceResultsSummary,\n\t\t\t\tprocessors,\n\t\t\t)\n\t\t})\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Team Pipeline Configuration\nDESCRIPTION: Configures separate pipelines for each team, connecting Prometheus receivers with team-specific filters and exporters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/coralogixexporter/README.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nservice:\n  pipelines:\n    metrics/1:\n      receivers: [prometheus]\n      processors: [filter/teamA]\n      exporters: [coralogix/teamA]\n    metrics/2:\n      receivers: [prometheus]\n      processors: [filter/teamB]\n      exporters: [coralogix/teamB]\n```\n\n----------------------------------------\n\nTITLE: Configuring Queue Retry Settings for Splunk HEC Exporter in YAML\nDESCRIPTION: Example configuration for adjusting the retry behavior of the Splunk HEC exporter to handle throttling. This reduces the maximum retry time from the default 120s to 60s to help manage memory usage during high volume scenarios.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/splunkhecexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  splunk_hec:\n    retry_on_failure:\n      max_elapsed_time: 60\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Input with Custom Severity Mapping in OpenTelemetry\nDESCRIPTION: This example shows how the severity parser processes a JSON input with a custom severity value. The parser recognizes 'nooo!' as an error severity and removes the parsed field from the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"default\",\n  \"body\": {\n    \"severity_field\": \"nooo!\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"error\",\n  \"body\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Input with Custom Severity Mapping in OpenTelemetry\nDESCRIPTION: This example shows how the severity parser processes a JSON input with a custom severity value. The parser recognizes 'nooo!' as an error severity and removes the parsed field from the body.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/severity.md#2025-04-10_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"default\",\n  \"body\": {\n    \"severity_field\": \"nooo!\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"severity\": \"error\",\n  \"body\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Endpoint Configuration Example in YAML\nDESCRIPTION: Example demonstrating how to dynamically set an endpoint with a static port using string concatenation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n   endpoint: '`endpoint`:8080'\n```\n\n----------------------------------------\n\nTITLE: RFC3164 Log Record Output\nDESCRIPTION: Output syslog message formatted according to RFC3164 protocol based on the input example.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/syslogexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: console\nCODE:\n```\n<34>Oct 11 22:14:15 mymachine su: 'su root' failed for lonvick on /dev/pts/8\n```\n\n----------------------------------------\n\nTITLE: Associating OpenTelemetry Spans with Sentry Errors in Python\nDESCRIPTION: This Python code snippet shows how to set a trace context on a Sentry error event, allowing association between OpenTelemetry spans and Sentry errors. It demonstrates updating the scope with a new trace_id when starting a new trace.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/sentryexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentry_sdk import configure_scope\n\nwith start_otel_span(\"start\") as span:\n  ctx = span.get_context()\n  with configure_scope() as scope:\n    scope.set_context(\"trace\", {\"trace_id\": ctx.trace_id})\n\n  with start_otel_span(\"child\"):\n    # ...\n```\n\n----------------------------------------\n\nTITLE: Basic Kubernetes Cluster Receiver Configuration\nDESCRIPTION: Basic configuration example for the Kubernetes Cluster receiver showing how to set authentication type, disable specific metrics and resource attributes, and configure node conditions and allocatable resource types to report.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/k8sclusterreceiver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n  k8s_cluster:\n    auth_type: kubeConfig\n    node_conditions_to_report: [Ready, MemoryPressure]\n    allocatable_types_to_report: [cpu, memory]\n    metrics:\n      k8s.container.cpu_limit:\n        enabled: false\n    resource_attributes:\n      container.id:\n        enabled: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Pure Storage FlashBlade Receiver in YAML\nDESCRIPTION: Example configuration for setting up the Pure Storage FlashBlade receiver with bearer token authentication, multiple endpoints, and custom reload intervals. The configuration includes array and client endpoints with separate authentication tokens and environment settings.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/purefbreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  bearertokenauth/fb01:\n    token: \"...\"\n\nreceivers:\n  purefb:\n    endpoint: http://172.31.60.207:9491/metrics\n    arrays:\n      - address: fb01\n        auth:\n          authenticator: bearertokenauth/fb01\n    clients:\n      - address: fb01\n        auth:\n          authenticator: bearertokenauth/fb01\n    env: dev\n    settings:\n      reload_intervals:\n        array: 5m\n        clients: 6m\n        usage: 6m\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Transform Processor Structure in YAML\nDESCRIPTION: Basic configuration structure for the Transform Processor showing the error_mode and signal statements placeholders.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  <trace|metric|log>_statements: []\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Transform Processor Structure in YAML\nDESCRIPTION: Basic configuration structure for the Transform Processor showing the error_mode and signal statements placeholders.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/transformprocessor/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntransform:\n  error_mode: ignore\n  <trace|metric|log>_statements: []\n```\n\n----------------------------------------\n\nTITLE: Defining OpenTelemetry Span Kind and Status Code Enums\nDESCRIPTION: Enumeration values for span kinds (unspecified, internal, server, client, producer, consumer) and status codes (unset, ok, error) used in the OpenTelemetry trace protocol. These values are defined in the traces proto file.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/contexts/ottlspan/README.md#2025-04-10_snippet_1\n\nLANGUAGE: proto\nCODE:\n```\n// Span Kind Enums\nSPAN_KIND_UNSPECIFIED = 0\nSPAN_KIND_INTERNAL = 1\nSPAN_KIND_SERVER = 2\nSPAN_KIND_CLIENT = 3\nSPAN_KIND_PRODUCER = 4\nSPAN_KIND_CONSUMER = 5\n\n// Status Code Enums\nSTATUS_CODE_UNSET = 0\nSTATUS_CODE_OK = 1\nSTATUS_CODE_ERROR = 2\n```\n\n----------------------------------------\n\nTITLE: Querying SQL Server Properties JSON\nDESCRIPTION: JSON structure containing SQL Server instance properties including memory metrics, database states, version information, and system configuration. Used for monitoring and telemetry collection.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/propertyQueryData.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[{\"ForceEncryption\":\"0\",\"Port\":\"1433\",\"PortType\":\"Static\",\"available_server_memory\":\"4517288\",\"cpu_count\":\"16\",\"db_offline\":\"0\",\"db_online\":\"4\",\"db_recovering\":\"0\",\"db_recoveryPending\":\"0\",\"db_restoring\":\"0\",\"db_suspect\":\"0\",\"engine_edition\":\"3\",\"hardware_type\":\"HYPERVISOR\",\"instance_type\":\"0\",\"is_hadr_enabled\":\"0\",\"measurement\":\"sqlserver_server_properties\",\"server_memory\":\"6421504\",\"service_name\":\"MSSQLSERVER\",\"sku\":\"Developer Edition (64-bit)\",\"sql_instance\":\"ad8fb2b53dce\",\"computer_name\":\"abcde\",\"sql_version\":\"16.0.4105.2\",\"sql_version_desc\":\"Microsoft SQL Server 2022 (RTM-CU11) (KB5032679) \",\"uptime\":\"17393\"}]\n```\n\n----------------------------------------\n\nTITLE: Building OpenTelemetry Collector for Kind Kubernetes Cluster\nDESCRIPTION: Makefile command to build the OpenTelemetry Collector from source for deployment on a local kind Kubernetes cluster.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/dev-docs.md#2025-04-10_snippet_0\n\nLANGUAGE: makefile\nCODE:\n```\nmake kind-build\n```\n\n----------------------------------------\n\nTITLE: Advanced AWS CloudWatch Logs EMF Configuration\nDESCRIPTION: Extended configuration example for EMF logs with additional optional parameters including region, endpoint, retention, and tags.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/awscloudwatchlogsexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexporters:\n  awscloudwatchlogs:\n    log_group_name: \"testing-logs-emf\"\n    log_stream_name: \"testing-integrations-stream-emf\"\n    raw_log: true\n    region: \"us-east-1\"\n    endpoint: \"logs.us-east-1.amazonaws.com\"\n    log_retention: 365\n    tags: { \"sampleKey\": \"sampleValue\" }\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracing Pipeline with Logzio Exporter in YAML\nDESCRIPTION: This snippet demonstrates how to configure a tracing pipeline using the Logzio exporter. It includes receiver setup for OTLP and Jaeger, batch processing, and exporter configuration for Logzio traces.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/logzioexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"0.0.0.0:4317\"\n      http:\n        endpoint: \"0.0.0.0:4318\"\n  jaeger:\n    protocols:\n      thrift_compact:\n        endpoint: \"0.0.0.0:6831\"\n      thrift_binary:\n        endpoint: \"0.0.0.0:6832\"\n      grpc:\n        endpoint: \"0.0.0.0:14250\"\n      thrift_http:\n        endpoint: \"0.0.0.0:14268\"\nprocessors:\n  batch:\n    send_batch_size: 10000\n    timeout: 1s\nexporters:\n  logzio/traces:\n    account_token: \"LOGZIOtraceTOKEN\"\n    region: \"us\"\nservice:\n  pipelines:\n    traces:\n      receivers: [ otlp,jaeger ]\n      processors: [ batch ]\n      exporters: [ logzio/traces ]\n  telemetry:\n    logs:\n      level: \"debug\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Client Auth Extension in YAML\nDESCRIPTION: Example configuration showing how to set up the Google Client Auth extension with OTLP receiver and exporter. The configuration demonstrates the basic setup including authentication and pipeline configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/googleclientauthextension/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  googleclientauth:\n\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  otlp/withauth:\n    endpoint: 0.0.0.0:5000\n    ca_file: /tmp/certs/ca.pem\n    auth:\n      authenticator: googleclientauth\n\nservice:\n  extensions: [googleclientauth]\n  pipelines:\n    metrics:\n      receivers: [otlp]\n      processors: []\n      exporters: [otlp/withauth]\n```\n\n----------------------------------------\n\nTITLE: SQL Server Performance Metrics JSON Structure\nDESCRIPTION: Performance monitoring data structure for SQL Server containing batch response statistics and buffer manager metrics. Includes counters for different time intervals and performance indicators like buffer cache hit ratio, page reads/writes, and batch response times.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/sqlserverreceiver/testdata/perfCounterQueryData.txt#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"counter\":\"Batches >=000000ms & <000001ms\",\n   \"counter_type\":\"65792\",\n   \"instance\":\"Elapsed Time:Total(ms)\",\n   \"measurement\":\"sqlserver_performance\",\n   \"object\":\"SQLServer:Batch Resp Statistics\",\n   \"sql_instance\":\"8cac97ac9b8f\",\n   \"computer_name\":\"abcde\",\n   \"value\":\"0\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Observer Extension with Receiver Creator in YAML\nDESCRIPTION: Example configuration showing how to set up the Docker observer extension with a receiver creator that matches nginx containers. The config demonstrates basic observer settings including Docker socket endpoint, excluded images, API version, and timeout configuration.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/dockerobserver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nextensions:\n  docker_observer:\n    # url of the docker socket, defaults to unix:///var/run/docker.sock on non-Windows and npipe:////./pipe/docker_engine on Windows\n    endpoint: my/path/to/docker.sock\n    # list of container image names to exclude\n    excluded_images: ['redis', 'another_image_name']\n    # client API version, default to 1.24\n    api_version: \"1.25\"\n    # max amount of time to wait for a response from Docker API , default to 5s\n    timeout: 15s\n\nreceivers:\n  receiver_creator:\n    watch_observers: [docker_observer]\n    receivers:\n      nginx:\n        rule: type == \"container\" and name matches \"nginx\" and port == 80\n        config:\n          endpoint: '`endpoint`/status'\n          collection_interval: 10s\n```\n\n----------------------------------------\n\nTITLE: Input JSON Entry for Removing All Resource Fields\nDESCRIPTION: This shows a sample JSON input entry with multiple resource fields that will all be removed by the remove operator.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/remove.md#2025-04-10_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"resource\": {\n    \"key1.0\": \"val\",\n    \"key2.0\": \"val\"\n  },\n  \"attributes\": {  },\n  \"body\": {\n    \"key\": \"val\"\n  },\n}\n```\n\n----------------------------------------\n\nTITLE: InfluxDB Line Protocol Example for Tracing Spans\nDESCRIPTION: Example of how tracing spans are represented in InfluxDB line protocol format.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/influxdbexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nspans end_time_unix_nano=\"2021-02-19 20:50:25.6893952 +0000 UTC\",instrumentation_library_name=\"tracegen\",kind=\"SPAN_KIND_INTERNAL\",name=\"okey-dokey\",net.peer.ip=\"1.2.3.4\",parent_span_id=\"d5270e78d85f570f\",peer.service=\"tracegen-client\",service.name=\"tracegen\",span.kind=\"server\",span_id=\"4c28227be6a010e1\",status_code=\"STATUS_CODE_OK\",trace_id=\"7d4854815225332c9834e6dbf85b9380\" 1613767825689169000\nspans end_time_unix_nano=\"2021-02-19 20:50:25.6893952 +0000 UTC\",instrumentation_library_name=\"tracegen\",kind=\"SPAN_KIND_INTERNAL\",name=\"lets-go\",net.peer.ip=\"1.2.3.4\",peer.service=\"tracegen-server\",service.name=\"tracegen\",span.kind=\"client\",span_id=\"d5270e78d85f570f\",status_code=\"STATUS_CODE_OK\",trace_id=\"7d4854815225332c9834e6dbf85b9380\" 1613767825689135000\n```\n\n----------------------------------------\n\nTITLE: Updating LogQL Query for Line Formatting\nDESCRIPTION: Example of how to update a LogQL query to use the new format for line formatting in Loki after migrating from the OpenTelemetry Collector Loki exporter.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/lokiexporter/README.md#2025-04-10_snippet_4\n\nLANGUAGE: logql\nCODE:\n```\nBEFORE\n{exporter=\"OTLP\", job=\"frontend\"} | json | line_format `[{{.level}}] {{.body}}`\n\nAFTER\n{service_name=\"frontend\"} | line_format `[{{.detected_level}}] {{__line__}}`\n```\n\n----------------------------------------\n\nTITLE: Configuring TLS for OpenTelemetry Collector Receivers\nDESCRIPTION: YAML configuration demonstrating how to enable TLS for OpenTelemetry Collector OTLP receivers. It specifies the endpoint and paths to TLS certificate and key files.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/secure-tracing/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: mysite.local:55690\n        tls:\n          cert_file: server.crt\n          key_file: server.key\n```\n\n----------------------------------------\n\nTITLE: Directory Services Performance Counter Paths\nDESCRIPTION: Complete collection of performance counter paths for Windows Directory Services monitoring. Includes counters for replication, LDAP operations, security operations, SAM activities, and general directory metrics.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/activedirectorydsreceiver/testdata/counters.txt#2025-04-10_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n\\DirectoryServices(*)\\DRA Inbound Properties Total/sec\n\\DirectoryServices(*)\\AB Browses/sec\n\\DirectoryServices(*)\\DRA Inbound Objects Applied/sec\n\\DirectoryServices(*)\\DS Threads in Use\n\\DirectoryServices(*)\\AB Client Sessions\n\\DirectoryServices(*)\\DRA Pending Replication Synchronizations\n\\DirectoryServices(*)\\DRA Inbound Object Updates Remaining in Packet\n\\DirectoryServices(*)\\DS Security Descriptor sub-operations/sec\n\\DirectoryServices(*)\\DS Security Descriptor Propagations Events\n\\DirectoryServices(*)\\LDAP Client Sessions\n\\DirectoryServices(*)\\LDAP Active Threads\n\\DirectoryServices(*)\\LDAP Writes/sec\n\\DirectoryServices(*)\\LDAP Searches/sec\n\\DirectoryServices(*)\\DRA Outbound Objects/sec\n\\DirectoryServices(*)\\DRA Outbound Properties/sec\n\\DirectoryServices(*)\\DRA Inbound Values Total/sec\n\\DirectoryServices(*)\\DRA Sync Requests Made\n\\DirectoryServices(*)\\DRA Sync Requests Successful\n\\DirectoryServices(*)\\DRA Sync Failures on Schema Mismatch\n\\DirectoryServices(*)\\DRA Inbound Objects/sec\n\\DirectoryServices(*)\\DRA Inbound Properties Applied/sec\n\\DirectoryServices(*)\\DRA Inbound Properties Filtered/sec\n\\DirectoryServices(*)\\DS Monitor List Size\n\\DirectoryServices(*)\\DS Notify Queue Size\n\\DirectoryServices(*)\\LDAP UDP operations/sec\n\\DirectoryServices(*)\\DS Search sub-operations/sec\n\\DirectoryServices(*)\\DS Name Cache hit rate\n\\DirectoryServices(*)\\DRA Highest USN Issued (Low part)\n\\DirectoryServices(*)\\DRA Highest USN Issued (High part)\n\\DirectoryServices(*)\\DRA Highest USN Committed (Low part)\n\\DirectoryServices(*)\\DRA Highest USN Committed (High part)\n\\DirectoryServices(*)\\DS % Writes from SAM\n\\DirectoryServices(*)\\DS % Writes from DRA\n\\DirectoryServices(*)\\DS % Writes from LDAP\n\\DirectoryServices(*)\\DS % Writes from LSA\n\\DirectoryServices(*)\\DS % Writes from KCC\n\\DirectoryServices(*)\\DS % Writes from NSPI\n\\DirectoryServices(*)\\DS % Writes Other\n\\DirectoryServices(*)\\DS Directory Writes/sec\n\\DirectoryServices(*)\\DS % Searches from SAM\n\\DirectoryServices(*)\\DS % Searches from DRA\n\\DirectoryServices(*)\\DS % Searches from LDAP\n\\DirectoryServices(*)\\DS % Searches from LSA\n\\DirectoryServices(*)\\DS % Searches from KCC\n\\DirectoryServices(*)\\DS % Searches from NSPI\n\\DirectoryServices(*)\\DS % Searches Other\n\\DirectoryServices(*)\\DS Directory Searches/sec\n\\DirectoryServices(*)\\DS % Reads from SAM\n\\DirectoryServices(*)\\DS % Reads from DRA\n\\DirectoryServices(*)\\DRA Inbound Values (DNs only)/sec\n\\DirectoryServices(*)\\DRA Inbound Objects Filtered/sec\n\\DirectoryServices(*)\\DS % Reads from LSA\n\\DirectoryServices(*)\\DS % Reads from KCC\n\\DirectoryServices(*)\\DS % Reads from NSPI\n\\DirectoryServices(*)\\DS % Reads Other\n\\DirectoryServices(*)\\DS Directory Reads/sec\n\\DirectoryServices(*)\\LDAP Successful Binds/sec\n\\DirectoryServices(*)\\LDAP Bind Time\n\\DirectoryServices(*)\\SAM Successful Computer Creations/sec: Includes all requests\n\\DirectoryServices(*)\\SAM Machine Creation Attempts/sec\n\\DirectoryServices(*)\\SAM Successful User Creations/sec\n\\DirectoryServices(*)\\SAM User Creation Attempts/sec\n\\DirectoryServices(*)\\SAM Password Changes/sec\n\\DirectoryServices(*)\\SAM Membership Changes/sec\n\\DirectoryServices(*)\\SAM Display Information Queries/sec\n\\DirectoryServices(*)\\SAM Enumerations/sec\n\\DirectoryServices(*)\\SAM Transitive Membership Evaluations/sec\n\\DirectoryServices(*)\\SAM Non-Transitive Membership Evaluations/sec\n\\DirectoryServices(*)\\SAM Domain Local Group Membership Evaluations/sec\n\\DirectoryServices(*)\\SAM Universal Group Membership Evaluations/sec\n\\DirectoryServices(*)\\SAM Global Group Membership Evaluations/sec\n\\DirectoryServices(*)\\SAM GC Evaluations/sec\n\\DirectoryServices(*)\\DRA Inbound Full Sync Objects Remaining\n\\DirectoryServices(*)\\DRA Inbound Bytes Total/sec\n\\DirectoryServices(*)\\DRA Inbound Bytes Not Compressed (Within Site)/sec\n\\DirectoryServices(*)\\DRA Inbound Bytes Compressed (Between Sites, Before Compression)/sec\n\\DirectoryServices(*)\\DRA Inbound Bytes Compressed (Between Sites, After Compression)/sec\n\\DirectoryServices(*)\\DRA Outbound Bytes Total/sec\n\\DirectoryServices(*)\\DRA Outbound Bytes Not Compressed (Within Site)/sec\n\\DirectoryServices(*)\\DRA Outbound Bytes Compressed (Between Sites, Before Compression)/sec\n\\DirectoryServices(*)\\DRA Outbound Bytes Compressed (Between Sites, After Compression)/sec\n\\DirectoryServices(*)\\DS Client Binds/sec\n\\DirectoryServices(*)\\DS Server Binds/sec\n\\DirectoryServices(*)\\DS Client Name Translations/sec\n\\DirectoryServices(*)\\DS Server Name Translations/sec\n\\DirectoryServices(*)\\DS Security Descriptor Propagator Runtime Queue\n\\DirectoryServices(*)\\DS Security Descriptor Propagator Average Exclusion Time\n\\DirectoryServices(*)\\DRA Outbound Objects Filtered/sec\n\\DirectoryServices(*)\\DRA Outbound Values Total/sec\n\\DirectoryServices(*)\\DRA Outbound Values (DNs only)/sec\n\\DirectoryServices(*)\\AB ANR/sec\n\\DirectoryServices(*)\\AB Property Reads/sec\n\\DirectoryServices(*)\\AB Searches/sec\n\\DirectoryServices(*)\\AB Matches/sec\n\\DirectoryServices(*)\\AB Proxy Lookups/sec\n\\DirectoryServices(*)\\ATQ Threads Total\n\\DirectoryServices(*)\\ATQ Threads LDAP\n\\DirectoryServices(*)\\ATQ Threads Other\n\\DirectoryServices(*)\\DRA Inbound Bytes Total Since Boot\n\\DirectoryServices(*)\\DRA Inbound Bytes Not Compressed (Within Site) Since Boot\n\\DirectoryServices(*)\\DRA Inbound Bytes Compressed (Between Sites, Before Compression) Since Boot\n\\DirectoryServices(*)\\DRA Inbound Bytes Compressed (Between Sites, After Compression) Since Boot\n\\DirectoryServices(*)\\DRA Outbound Bytes Total Since Boot\n\\DirectoryServices(*)\\DRA Outbound Bytes Not Compressed (Within Site) Since Boot\n\\DirectoryServices(*)\\DRA Outbound Bytes Compressed (Between Sites, Before Compression) Since Boot\n\\DirectoryServices(*)\\DRA Outbound Bytes Compressed (Between Sites, After Compression) Since Boot\n\\DirectoryServices(*)\\LDAP New Connections/sec\n\\DirectoryServices(*)\\LDAP Closed Connections/sec\n\\DirectoryServices(*)\\LDAP New SSL Connections/sec\n\\DirectoryServices(*)\\DRA Pending Replication Operations\n\\DirectoryServices(*)\\DRA Threads Getting NC Changes\n\\DirectoryServices(*)\\DRA Threads Getting NC Changes Holding Semaphore\n\\DirectoryServices(*)\\DRA Inbound Link Value Updates Remaining in Packet\n\\DirectoryServices(*)\\DRA Inbound Total Updates Remaining in Packet\n\\DirectoryServices(*)\\DS % Writes from NTDSAPI\n\\DirectoryServices(*)\\DS % Searches from NTDSAPI\n\\DirectoryServices(*)\\DS % Reads from NTDSAPI\n\\DirectoryServices(*)\\SAM Account Group Evaluation Latency\n\\DirectoryServices(*)\\SAM Resource Group Evaluation Latency\n\\DirectoryServices(*)\\ATQ Outstanding Queued Requests\n\\DirectoryServices(*)\\ATQ Request Latency\n\\DirectoryServices(*)\\ATQ Estimated Queue Delay\n\\DirectoryServices(*)\\Tombstones Garbage Collected/sec\n\\DirectoryServices(*)\\Phantoms Cleaned/sec\n\\DirectoryServices(*)\\Link Values Cleaned/sec\n\\DirectoryServices(*)\\Tombstones Visited/sec\n\\DirectoryServices(*)\\Phantoms Visited/sec\n\\DirectoryServices(*)\\NTLM Binds/sec\n\\DirectoryServices(*)\\Negotiated Binds/sec\n\\DirectoryServices(*)\\Digest Binds/sec\n\\DirectoryServices(*)\\Simple Binds/sec\n\\DirectoryServices(*)\\External Binds/sec\n\\DirectoryServices(*)\\Fast Binds/sec\n\\DirectoryServices(*)\\Base searches/sec\n\\DirectoryServices(*)\\Subtree searches/sec\n\\DirectoryServices(*)\\Onelevel searches/sec\n\\DirectoryServices(*)\\Database adds/sec\n\\DirectoryServices(*)\\Database modifys/sec\n\\DirectoryServices(*)\\Database deletes/sec\n\\DirectoryServices(*)\\Database recycles/sec\n\\DirectoryServices(*)\\Approximate highest DNT\n\\DirectoryServices(*)\\Transitive operations/sec\n\\DirectoryServices(*)\\Transitive suboperations/sec\n\\DirectoryServices(*)\\Transitive operations milliseconds run\n\\DirectoryServices(*)\\DirSync sessions in progress\n\\DirectoryServices(*)\\DirSync session throttling rate\n\\DirectoryServices(*)\\DRA Inbound Sync Link Deletion/sec\n```\n\n----------------------------------------\n\nTITLE: Configuring JMX Receiver in OpenTelemetry Collector\nDESCRIPTION: Example YAML configuration for the JMX receiver that specifies the JAR path, endpoint, target system, collection interval, and other parameters to collect metrics from a JMX-enabled application.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/jmxreceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  jmx:\n    jar_path: /opt/opentelemetry-java-contrib-jmx-metrics.jar\n    endpoint: my_jmx_host:12345\n    target_system: jvm\n    collection_interval: 10s\n    initial_delay: 1s\n    # optional: the same as specifying OTLP receiver endpoint.\n    otlp:\n      endpoint: mycollectorotlpreceiver:4317\n    username: my_jmx_username\n    # determined by the environment variable value\n    password: ${env:MY_JMX_PASSWORD}\n    resource_attributes:\n      my.attr: my.value\n      my.other.attr: my.other.value\n    log_level: info\n    additional_jars:\n      - /path/to/other.jar\n```\n\n----------------------------------------\n\nTITLE: Basic Log Query in ClickHouse\nDESCRIPTION: Simple SQL query to retrieve recent logs with timestamp and body within the last hour\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: URL Parser in Golang for Simple URL\nDESCRIPTION: A function to parse a URL string and extract its components as an object. This example shows a simple domain without path or parameters.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_72\n\nLANGUAGE: go\nCODE:\n```\nURL(\"http://www.example.com\")\n```\n\n----------------------------------------\n\nTITLE: Creating OpenTelemetry Collector Resources in Kubernetes\nDESCRIPTION: This YAML configuration creates multiple Kubernetes resources including a Namespace, Role, ServiceAccount, RoleBinding, and two OpenTelemetryCollector instances. It sets up the necessary permissions and configurations for the load-balancing exporter demo.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/loadbalancingexporter/example/k8s-resolver/README.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: observability\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: loadbalancer-role\n  namespace: observability\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  verbs:\n  - list\n  - watch\n  - get\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: loadbalancer\n  namespace: observability\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: loadbalancer-rolebinding\n  namespace: observability\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: loadbalancer-role\nsubjects:\n- kind: ServiceAccount\n  name: loadbalancer\n  namespace: observability\n---\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: loadbalancer\n  namespace: observability\nspec:\n  image: docker.io/otel/opentelemetry-collector-contrib:latest\n  serviceAccount: loadbalancer\n  config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n\n    processors:\n\n    exporters:\n      loadbalancing:\n        protocol:\n          otlp:\n            tls:\n              insecure: true\n        resolver:\n          k8s:\n            service: backends-collector-headless.observability\n\n    service:\n      pipelines:\n        traces:\n          receivers:\n            - otlp\n          processors: []\n          exporters:\n            - loadbalancing\n---\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: backends\n  namespace: observability\nspec:\n  replicas: 5\n  config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n\n    processors:\n\n    exporters:\n      debug:\n\n    service:\n      pipelines:\n        traces:\n          receivers:\n            - otlp\n          processors: []\n          exporters:\n            - debug\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare Receiver with Custom Attributes in YAML\nDESCRIPTION: Example YAML configuration for the Cloudflare receiver with TLS, endpoint, secret, timestamp field, and custom attribute mappings. This configuration maps specific fields from Cloudflare log messages to custom attribute names.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/cloudflarereceiver/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreceivers:\n  cloudflare:\n    logs:\n      tls:\n        key_file: some_key_file\n        cert_file: some_cert_file\n      endpoint: 0.0.0.0:12345\n      secret: 1234567890abcdef1234567890abcdef\n      timestamp_field: EdgeStartTimestamp\n      attributes:\n        ClientIP: http_request.client_ip\n        ClientRequestURI: http_request.uri\n```\n\n----------------------------------------\n\nTITLE: Pattern Matching Log Search in ClickHouse\nDESCRIPTION: SQL query using LIKE operator to find logs containing specific patterns\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp as log_time, Body\nFROM otel_logs\nWHERE Body like '%http%'\n  AND TimestampTime >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Configuring Heroku Resource Detection\nDESCRIPTION: YAML configuration for setting up Heroku and environment variable detectors with a 2 second timeout.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/resourcedetectionprocessor/README.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nprocessors:\n  resourcedetection/heroku:\n    detectors: [env, heroku]\n    timeout: 2s\n    override: false\n```\n\n----------------------------------------\n\nTITLE: Disabling Default Metrics in YAML Configuration\nDESCRIPTION: Example YAML configuration to disable a default metric in the docker_stats receiver. Replace <metric_name> with the specific metric to disable.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/dockerstatsreceiver/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Dynamic Configuration Value Example in YAML\nDESCRIPTION: Example showing how to use dynamic values in receiver configuration using backticks for variable interpolation.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/receivercreator/README.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconfig:\n   secure_url: https://`pod.labels[\"secure_host\"]`\n```\n\n----------------------------------------\n\nTITLE: Error Span Query in ClickHouse\nDESCRIPTION: SQL query to find error spans in traces\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Timestamp,\n       TraceId,\n       SpanId,\n       ParentSpanId,\n       SpanName,\n       SpanKind,\n       ServiceName,\n       Duration,\n       StatusCode,\n       StatusMessage,\n       toString(SpanAttributes),\n       toString(ResourceAttributes),\n       toString(Events.Name),\n       toString(Links.TraceId)\nFROM otel_traces\nWHERE ServiceName = 'clickhouse-exporter'\n  AND StatusCode = 'Error'\n  AND Timestamp >= NOW() - INTERVAL 1 HOUR\nLimit 100;\n```\n\n----------------------------------------\n\nTITLE: Combining Metrics with Regular Expressions in YAML\nDESCRIPTION: Converts multiple related metrics into a single metric with differentiating labels using regular expression matching.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/metricstransformprocessor/README.md#2025-04-10_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\n# convert a set of metrics for each http_method into a single metric with an http_method label, i.e.\n#\n# Web Service (*)/Total Delete Requests     iis.requests{http_method=delete}\n# Web Service (*)/Total Get Requests     >  iis.requests{http_method=get}\n# Web Service (*)/Total Post Requests       iis.requests{http_method=post}\ninclude: ^Web Service \\(\\*\\)/Total (?P<http_method>.*) Requests$\nmatch_type: regexp\naction: combine\nnew_name: iis.requests\nsubmatch_case: lower\noperations:\n  ...\n```\n\n----------------------------------------\n\nTITLE: Enabling Optional Memory Metrics Configuration in YAML\nDESCRIPTION: YAML configuration example showing how to enable optional memory metrics by setting the 'enabled' property to true for a specific metric.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/memoryscraper/documentation.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: true\n```\n\n----------------------------------------\n\nTITLE: Sum Metrics Query in ClickHouse\nDESCRIPTION: SQL query to retrieve sum metrics by name\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nselect TimeUnix,MetricName,Attributes,Value from otel_metrics_sum\nwhere MetricName='calls' limit 100\n```\n\n----------------------------------------\n\nTITLE: Querying Log Severity Time Series in ClickHouse\nDESCRIPTION: SQL query to retrieve log severity counts aggregated by time intervals of 60 seconds over the past hour\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/clickhouseexporter/README.md#2025-04-10_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT toDateTime(toStartOfInterval(TimestampTime, INTERVAL 60 second)) as time, SeverityText, count() as count\nFROM otel_logs\nWHERE time >= NOW() - INTERVAL 1 HOUR\nGROUP BY SeverityText, time\nORDER BY time;\n```\n\n----------------------------------------\n\nTITLE: Using Concat Converter in Go\nDESCRIPTION: The Concat converter takes a sequence of values and a delimiter, concatenating their string representations. It supports paths, primitive values, and byte slices.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_13\n\nLANGUAGE: Go\nCODE:\n```\nConcat([span.attributes[\"http.method\"], span.attributes[\"http.path\"]], \": \")\n```\n\nLANGUAGE: Go\nCODE:\n```\nConcat([metric.name, 1], \" \")\n```\n\nLANGUAGE: Go\nCODE:\n```\nConcat([\"HTTP method is: \", span.attributes[\"http.method\"]], \"\")\n```\n\n----------------------------------------\n\nTITLE: Disabling System Metrics Configuration in YAML\nDESCRIPTION: Configuration example showing how to disable specific system metrics using YAML configuration. The configuration allows turning off individual metrics by setting their enabled flag to false.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/hostmetricsreceiver/internal/scraper/systemscraper/documentation.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmetrics:\n  <metric_name>:\n    enabled: false\n```\n\n----------------------------------------\n\nTITLE: Hashing String with Murmur3 (128-bit) in Go\nDESCRIPTION: Converts a string to a hexadecimal string representation of the 128-bit Murmur3 hash in little-endian format. Takes a Getter that returns a string as input.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_38\n\nLANGUAGE: Go\nCODE:\n```\nMurmur3Hash128(attributes[\"order.productId\"])\n```\n\n----------------------------------------\n\nTITLE: AWS X-Ray Trace Data Structure in JSON\nDESCRIPTION: JSON structure containing trace information including trace ID, span ID, timestamps, HTTP request/response details, and service information. The structure captures key telemetry data points for request tracing and monitoring.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/internal/aws/xray/testdata/awsMissingAwsField.txt#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"trace_id\": \"1-5f2aebcc-b475d14618c51eaa28753d37\",\n    \"id\": \"bda182a644eee9b3\",\n    \"name\": \"SampleServer\",\n    \"start_time\": 1596648396.6399446,\n    \"end_time\": 1596648396.6401389,\n    \"http\": {\n        \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"http://localhost:8000/\",\n            \"client_ip\": \"127.0.0.1\",\n            \"user_agent\": \"Go-http-client/1.1\",\n            \"x_forwarded_for\": true\n        },\n        \"response\": {\n            \"status\": 200\n        }\n    },\n    \"service\": {\n        \"compiler_version\": \"go1.14.6\",\n        \"compiler\": \"gc\"\n    },\n    \"Dummy\": false\n}\n```\n\n----------------------------------------\n\nTITLE: Year Extractor in Golang\nDESCRIPTION: The Year Converter extracts the year component from a time.Time object using Go's standard library. Returns an int64 representing the year.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#2025-04-10_snippet_78\n\nLANGUAGE: go\nCODE:\n```\nYear(Now())\n```\n\n----------------------------------------\n\nTITLE: Sending Custom Messages in OpenTelemetry Collector Extension\nDESCRIPTION: This code snippet shows how to send a custom message using a CustomCapabilityHandler. It handles the case where a message is already pending by waiting on the returned channel and retrying.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/opampcustommessages/README.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nfor {\n\tsendingChan, err := handler.SendMessage(\"messageType\", []byte(\"message-data\"))\n\tswitch {\n\tcase err == nil:\n\t\tbreak\n\tcase errors.Is(err, types.ErrCustomMessagePending):\n\t\t<-sendingChan\n\t\tcontinue\n\tdefault:\n\t\treturn fmt.Errorf(\"failed to send message: %w\", err)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Portable File Path Using Environment Variable\nDESCRIPTION: Example showing recommended approach of using environment variables for file paths to ensure cross-platform compatibility.\nSOURCE: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/CONTRIBUTING.md#2025-04-10_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nfilePath := os.Getenv(\"DATA_FILE_PATH\")\n```"
  }
]
[
  {
    "owner": "elevenlabs",
    "repo": "elevenlabs-docs",
    "content": "TITLE: Defining WebSocket Event Types for ElevenLabs API in TypeScript\nDESCRIPTION: This TypeScript code snippet defines a set of type aliases for the various WebSocket events that the ElevenLabs conversational AI API sends and receives. It includes events for user transcription, agent responses, audio chunks, interruptions, and ping messages, all extending a base event type. The exported union type 'ElevenLabsWebSocketEvent' encompasses all possible event shapes, providing strong typing for handling WebSocket messages throughout the application.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ntype BaseEvent = {\n  type: string;\n};\n\ntype UserTranscriptEvent = BaseEvent & {\n  type: \"user_transcript\";\n  user_transcription_event: {\n    user_transcript: string;\n  };\n};\n\ntype AgentResponseEvent = BaseEvent & {\n  type: \"agent_response\";\n  agent_response_event: {\n    agent_response: string;\n  };\n};\n\ntype AudioResponseEvent = BaseEvent & {\n  type: \"audio\";\n  audio_event: {\n    audio_base_64: string;\n    event_id: number;\n  };\n};\n\ntype InterruptionEvent = BaseEvent & {\n  type: \"interruption\";\n  interruption_event: {\n    reason: string;\n  };\n};\n\ntype PingEvent = BaseEvent & {\n  type: \"ping\";\n  ping_event: {\n    event_id: number;\n    ping_ms?: number;\n  };\n};\n\nexport type ElevenLabsWebSocketEvent =\n  | UserTranscriptEvent\n  | AgentResponseEvent\n  | AudioResponseEvent\n  | InterruptionEvent\n  | PingEvent;\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation with useConversation React Hook in TypeScript/JavaScript\nDESCRIPTION: Creates a Conversation instance using the useConversation React hook to manage websocket connection and audio interactions with ElevenLabs Conversational AI. Requires microphone access, which should be requested beforehand to enable audio capture. The hook also supports options for customizing connection event handlers like onConnect, onDisconnect, onMessage, and onError.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst conversation = useConversation();\n```\n\n----------------------------------------\n\nTITLE: Updating Conversation Component to Use Signed URL Authentication in TypeScript (TSX)\nDESCRIPTION: This snippet modifies the 'Conversation' component to fetch a signed URL from the newly created API route before starting a conversation session. It defines an asynchronous helper 'getSignedUrl' to retrieve the signed URL securely, and then passes it to 'startSession' instead of an 'agentId'. This update supports authentication with private agents. Proper error handling is included, and the snippet notes the limited lifetime of signed URLs, highlighting the need for renewal logic in production.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#_snippet_5\n\nLANGUAGE: tsx\nCODE:\n```\n// ... existing imports ...\n\nexport function Conversation() {\n  // ... existing conversation setup ...\n  const getSignedUrl = async (): Promise<string> => {\n    const response = await fetch(\"/api/get-signed-url\");\n    if (!response.ok) {\n      throw new Error(`Failed to get signed url: ${response.statusText}`);\n    }\n    const { signedUrl } = await response.json();\n    return signedUrl;\n  };\n\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n\n      const signedUrl = await getSignedUrl();\n\n      // Start the conversation with your signed url\n      await conversation.startSession({\n        signedUrl,\n      });\n\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n\n  // ... rest of the component ...\n}\n\n```\n\n----------------------------------------\n\nTITLE: Updating the Main Next.js Page to Render Conversation Component in TypeScript (TSX)\nDESCRIPTION: This snippet replaces the content of 'app/page.tsx' to import and render the previously defined 'Conversation' component. The main page uses Tailwind CSS for styling and centers the conversational interface vertically and horizontally. This establishes the main entry point of the app where users will interact with the AI agent component.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\nimport { Conversation } from './components/conversation';\n\nexport default function Home() {\n  return (\n    <main className=\"flex min-h-screen flex-col items-center justify-between p-24\">\n      <div className=\"z-10 max-w-5xl w-full items-center justify-between font-mono text-sm\">\n        <h1 className=\"text-4xl font-bold mb-8 text-center\">\n          ElevenLabs Conversational AI\n        </h1>\n        <Conversation />\n      </div>\n    </main>\n  );\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Twilio-ElevenLabs Bridge Server in JavaScript\nDESCRIPTION: Complete Node.js server implementation that connects Twilio phone calls to ElevenLabs Conversational AI using WebSockets. It handles audio streaming in both directions, manages connection events, and processes real-time voice conversations.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport Fastify from \"fastify\";\nimport WebSocket from \"ws\";\nimport dotenv from \"dotenv\";\nimport fastifyFormBody from \"@fastify/formbody\";\nimport fastifyWs from \"@fastify/websocket\";\n\n// Load environment variables from .env file\ndotenv.config();\n\nconst { ELEVENLABS_AGENT_ID } = process.env;\n\n// Check for the required ElevenLabs Agent ID\nif (!ELEVENLABS_AGENT_ID) {\nconsole.error(\"Missing ELEVENLABS_AGENT_ID in environment variables\");\nprocess.exit(1);\n}\n\n// Initialize Fastify server\nconst fastify = Fastify();\nfastify.register(fastifyFormBody);\nfastify.register(fastifyWs);\n\nconst PORT = process.env.PORT || 8000;\n\n// Root route for health check\nfastify.get(\"/\", async (_, reply) => {\nreply.send({ message: \"Server is running\" });\n});\n\n// Route to handle incoming calls from Twilio\nfastify.all(\"/twilio/inbound_call\", async (request, reply) => {\n// Generate TwiML response to connect the call to a WebSocket stream\nconst twimlResponse = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <Response>\n    <Connect>\n        <Stream url=\"wss://${request.headers.host}/media-stream\" />\n    </Connect>\n    </Response>`;\n\nreply.type(\"text/xml\").send(twimlResponse);\n});\n\n// WebSocket route for handling media streams from Twilio\nfastify.register(async (fastifyInstance) => {\nfastifyInstance.get(\"/media-stream\", { websocket: true }, (connection, req) => {\n    console.info(\"[Server] Twilio connected to media stream.\");\n\n    let streamSid = null;\n\n    // Connect to ElevenLabs Conversational AI WebSocket\n    const elevenLabsWs = new WebSocket(\n    `wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${ELEVENLABS_AGENT_ID}`\n    );\n\n    // Handle open event for ElevenLabs WebSocket\n    elevenLabsWs.on(\"open\", () => {\n    console.log(\"[II] Connected to Conversational AI.\");\n    });\n\n    // Handle messages from ElevenLabs\n    elevenLabsWs.on(\"message\", (data) => {\n    try {\n        const message = JSON.parse(data);\n        handleElevenLabsMessage(message, connection);\n    } catch (error) {\n        console.error(\"[II] Error parsing message:\", error);\n    }\n    });\n\n    // Handle errors from ElevenLabs WebSocket\n    elevenLabsWs.on(\"error\", (error) => {\n    console.error(\"[II] WebSocket error:\", error);\n    });\n\n    // Handle close event for ElevenLabs WebSocket\n    elevenLabsWs.on(\"close\", () => {\n    console.log(\"[II] Disconnected.\");\n    });\n\n    // Function to handle messages from ElevenLabs\n    const handleElevenLabsMessage = (message, connection) => {\n    switch (message.type) {\n        case \"conversation_initiation_metadata\":\n        console.info(\"[II] Received conversation initiation metadata.\");\n        break;\n        case \"audio\":\n        if (message.audio_event?.audio_base_64) {\n            // Send audio data to Twilio\n            const audioData = {\n            event: \"media\",\n            streamSid,\n            media: {\n                payload: message.audio_event.audio_base_64,\n            },\n            };\n            connection.send(JSON.stringify(audioData));\n        }\n        break;\n        case \"interruption\":\n        // Clear Twilio's audio queue\n        connection.send(JSON.stringify({ event: \"clear\", streamSid }));\n        break;\n        case \"ping\":\n        // Respond to ping events from ElevenLabs\n        if (message.ping_event?.event_id) {\n            const pongResponse = {\n            type: \"pong\",\n            event_id: message.ping_event.event_id,\n            };\n            elevenLabsWs.send(JSON.stringify(pongResponse));\n        }\n        break;\n    }\n    };\n\n    // Handle messages from Twilio\n    connection.on(\"message\", async (message) => {\n    try {\n        const data = JSON.parse(message);\n        switch (data.event) {\n        case \"start\":\n            // Store Stream SID when stream starts\n            streamSid = data.start.streamSid;\n            console.log(`[Twilio] Stream started with ID: ${streamSid}`);\n            break;\n        case \"media\":\n            // Route audio from Twilio to ElevenLabs\n            if (elevenLabsWs.readyState === WebSocket.OPEN) {\n            // data.media.payload is base64 encoded\n            const audioMessage = {\n                user_audio_chunk: Buffer.from(\n                    data.media.payload,\n                    \"base64\"\n                ).toString(\"base64\"),\n            };\n            elevenLabsWs.send(JSON.stringify(audioMessage));\n            }\n            break;\n        case \"stop\":\n            // Close ElevenLabs WebSocket when Twilio stream stops\n            elevenLabsWs.close();\n            break;\n        default:\n            console.log(`[Twilio] Received unhandled event: ${data.event}`);\n        }\n    } catch (error) {\n        console.error(\"[Twilio] Error processing message:\", error);\n    }\n    });\n\n    // Handle close event from Twilio\n    connection.on(\"close\", () => {\n    elevenLabsWs.close();\n    console.log(\"[Twilio] Client disconnected\");\n    });\n\n    // Handle errors from Twilio WebSocket\n    connection.on(\"error\", (error) => {\n    console.error(\"[Twilio] WebSocket error:\", error);\n    elevenLabsWs.close();\n    });\n});\n});\n\n// Start the Fastify server\nfastify.listen({ port: PORT }, (err) => {\nif (err) {\n    console.error(\"Error starting server:\", err);\n    process.exit(1);\n}\nconsole.log(`[Server] Listening on port ${PORT}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversational AI Scheduler with Cal.com Integration System Prompt in Plain Text\nDESCRIPTION: This snippet provides a detailed system prompt that defines how a conversational AI receptionist should manage meeting scheduling with Cal.com. It instructs the agent on gathering meeting purpose, date/time, duration, checking availability using the get_available_slots tool, collecting and validating attendee contact details, and booking meetings with the book_meeting tool using specific parameters. Key operational rules, email formatting instructions, and guardrails to maintain user experience and data privacy are included. It requires integration capabilities with Cal.com APIs and assumes the AI can parse and invoke external tools.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/cal.com.mdx#_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nYou are a helpful receptionist responsible for scheduling meetings using the Cal.com integration. Be friendly, precise, and concise.\n\nBegin by briefly asking for the purpose of the meeting and the caller's preferred date and time.\nThen, ask about the desired meeting duration (15, 30, or 60 minutes), and wait for the user's response before proceeding.\n\nOnce you have the meeting details, say you will check calendar availability:\n- Call get_available_slots with the appropriate date range\n- Verify if the requested time slot is available\n- If not available, suggest alternative times from the available slots\n- Continue until a suitable time is agreed upon\n\nAfter confirming a time slot, gather the following contact details:\n- The attendee's full name\n- A valid email address. Note that the email address is transcribed from voice, so ensure it is formatted correctly.\n- The attendee's time zone (in 'Continent/City' format like 'America/New_York')\n- Read the email back to the caller to confirm accuracy\n\nOnce all details are confirmed, explain that you will create the meeting.\nCreate the meeting by using the book_meeting tool with the following parameters:\n- start: The agreed meeting time in ISO 8601 format\n- eventTypeId: The appropriate ID based on the meeting duration (15min: 1351800, 30min: 1351801, 60min: 1351802)\n- attendee: An object containing the name, email, and timeZone\n\nThank the attendee and inform them they will receive a calendar invitation shortly.\n\nClarifications:\n- Do not inform the user that you are formatting the email; simply do it.\n- If the caller asks you to proceed with booking, do so with the existing information.\n\nGuardrails:\n- Do not share any internal IDs or API details with the caller.\n- If booking fails, check for formatting issues in the email or time conflicts.\n```\n\n----------------------------------------\n\nTITLE: Initializing Webhook Endpoint and Dependencies\nDESCRIPTION: This code snippet initializes the necessary dependencies for the webhook endpoint. It imports modules such as Redis, crypto, ElevenLabsClient, NextResponse, NextRequest, and Resend. It then initializes Redis using environment variables, the Resend client using the API key, and the ElevenLabsClient using the ElevenLabs API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Redis } from '@upstash/redis';\nimport crypto from 'crypto';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\nimport { Resend } from 'resend';\n\nimport { EmailTemplate } from '@/components/email/post-call-webhook-email';\n\n// Initialize Redis\nconst redis = Redis.fromEnv();\n// Initialize Resend\nconst resend = new Resend(process.env.RESEND_API_KEY);\n\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: process.env.ELEVENLABS_API_KEY,\n});\n```\n\n----------------------------------------\n\nTITLE: Making a Voice Clone API Request with ElevenLabs SDK in TypeScript\nDESCRIPTION: This TypeScript snippet shows how to use the ElevenLabs SDK for creating an instant voice clone by uploading audio via Node.js streams. Required dependencies are the elevenlabs package, dotenv for environment management, and the native fs module for file handling. The code authenticates automatically via environment variables, creates a new ElevenLabsClient instance, and submits an audio file stream to the API. Input is an audio file path; output is the resulting voice ID logged to the console. Ensure the SDK is installed and node scripts are executed in an environment supporting ES modules.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/instant-voice-cloning.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\nimport fs from \"node:fs\";\n\nconst client = new ElevenLabsClient();\n\nconst voice = await client.voices.add({\n    name: \"My Voice Clone\",\n    // Replace with the paths to your audio files.\n    // The more files you add, the better the clone will be.\n    files: [\n        fs.createReadStream(\n            \"/path/to/your/audio/file.mp3\",\n        ),\n    ],\n});\n\nconsole.log(voice.voice_id);\n```\n\n----------------------------------------\n\nTITLE: Fetching Signed URL from ElevenLabs Conversational AI API (JavaScript)\nDESCRIPTION: Defines an asynchronous helper function to retrieve a signed WebSocket URL required for connecting to ElevenLabs AI conversations. It performs a GET request to ElevenLabs API using the agent ID and API key for authentication. On a successful response, it parses and returns the signed URL. Errors during the fetch are logged and rethrown. This function is critical for establishing authenticated, real-time media streams with ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nasync function getSignedUrl() {\n  try {\n    const response = await fetch(\n      `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${ELEVENLABS_AGENT_ID}`,\n      {\n        method: 'GET',\n        headers: {\n          'xi-api-key': ELEVENLABS_API_KEY,\n        },\n      }\n    );\n\n    if (!response.ok) {\n      throw new Error(`Failed to get signed URL: ${response.statusText}`);\n    }\n\n    const data = await response.json();\n    return data.signed_url;\n  } catch (error) {\n    console.error('Error getting signed URL:', error);\n    throw error;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Combining Signed URLs and Allowlists for Agent Authentication\nDESCRIPTION: Illustrates how to create an ElevenLabs conversational AI agent using both signed URLs (`enable_auth=True`) and an allowlist for enhanced security. Clients must both connect from an allowed domain specified in the `allowlist` and possess a valid signed URL obtained via the server-side API call. Examples are provided in Python and JavaScript.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs.client import ElevenLabs\nimport os\nfrom elevenlabs.types import *\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(api_key=api_key)\nagent = client.conversational_ai.create_agent(\n  conversation_config=ConversationalConfig(\n    agent=AgentConfig(\n      first_message=\"Hi. I'm an authenticated agent that can only be called from certain domains.\",\n    )\n  ),\n  platform_settings=AgentPlatformSettingsRequestModel(\n  auth=AuthSettings(\n    enable_auth=True,\n    allowlist=[\n      AllowlistItem(hostname=\"example.com\"),\n      AllowlistItem(hostname=\"app.example.com\"),\n      AllowlistItem(hostname=\"localhost:3000\")\n      ]\n    )\n  )\n)\n```\n\nLANGUAGE: javascript\nCODE:\n```\nasync function createAuthenticatedAgent(client) {\n  try {\n    const agent = await client.conversationalAi.createAgent({\n      conversation_config: {\n        agent: {\n          first_message: \"Hi. I'm an authenticated agent.\",\n        },\n      },\n      platform_settings: {\n        auth: {\n          enable_auth: true,\n          allowlist: [\n            { hostname: 'example.com' },\n            { hostname: 'app.example.com' },\n            { hostname: 'localhost:3000' },\n          ],\n        },\n      },\n    });\n\n    return agent;\n  } catch (error) {\n    console.error('Error creating agent:', error);\n    throw error;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Making a Voice Clone API Request with ElevenLabs SDK in Python\nDESCRIPTION: This Python snippet demonstrates how to use the ElevenLabs SDK to create an instant voice clone by uploading local audio files. Dependencies include the ElevenLabs SDK, python-dotenv for environment variables, and the io module for buffering file data. The script reads an audio file, authenticates using an API key loaded from an .env file, and calls the add method to create a voice clone, printing the resulting voice_id. Input is a list of file paths; output is the new clone's identifier. Ensure you have a valid API key and that the provided audio file path is accessible.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/instant-voice-cloning.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\nvoice = client.voices.add(\n    name=\"My Voice Clone\",\n    # Replace with the paths to your audio files.\n    # The more files you add, the better the clone will be.\n    files=[BytesIO(open(\"/path/to/your/audio/file.mp3\", \"rb\").read())]\n)\n\nprint(voice.voice_id)\n```\n\n----------------------------------------\n\nTITLE: Implementing an API Route for Generating Signed URLs with ElevenLabs in Next.js TypeScript (TSX)\nDESCRIPTION: This Next.js API route handler defined in 'app/api/get-signed-url/route.ts' asynchronously fetches a signed URL to authenticate private ElevenLabs conversational agents. It sends a GET request with the API key from environment variables to ElevenLabs' conversation API endpoint. Upon success, it returns the signed URL as JSON, while errors result in a 500 response with an error message. This server-side code avoids exposing sensitive keys on the client.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  try {\n    const response = await fetch(\n      `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.NEXT_PUBLIC_AGENT_ID}`,\n      {\n        headers: {\n          'xi-api-key': process.env.ELEVENLABS_API_KEY!,\n        },\n      }\n    );\n\n    if (!response.ok) {\n      throw new Error('Failed to get signed URL');\n    }\n\n    const data = await response.json();\n    return NextResponse.json({ signedUrl: data.signed_url });\n  } catch (error) {\n    return NextResponse.json(\n      { error: 'Failed to generate signed URL' },\n      { status: 500 }\n    );\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation Session with Agent ID in JavaScript\nDESCRIPTION: Shows how to start a conversation session by invoking Conversation.startSession with an agent ID, establishing a websocket connection, and preparing for voice communication. Also highlights recording microphone access via getUserMedia, essential for capturing user input.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst conversation = await Conversation.startSession({\n  agentId: '<your-agent-id>',\n});\n```\n\n----------------------------------------\n\nTITLE: Initiating Outbound Calls via Twilio API with Dynamic TwiML URL (JavaScript)\nDESCRIPTION: Defines a Fastify POST route '/outbound-call' which initiates an outbound call using the Twilio client. It accepts a JSON body with parameters: 'number' (destination phone number), 'prompt', and 'first_message' to customize call behavior. The Twilio call request specifies the Twilio phone number as the caller and uses a dynamically constructed TwiML URL with URL-encoded query parameters for prompt customization. The endpoint returns success status and the Twilio call SID or an error response if the call cannot be initiated.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nfastify.post('/outbound-call', async (request, reply) => {\n  const { number, prompt, first_message } = request.body;\n\n  if (!number) {\n    return reply.code(400).send({ error: 'Phone number is required' });\n  }\n\n  try {\n    const call = await twilioClient.calls.create({\n      from: TWILIO_PHONE_NUMBER,\n      to: number,\n      url: `https://${request.headers.host}/outbound-call-twiml?prompt=${encodeURIComponent(\n        prompt\n      )}&first_message=${encodeURIComponent(first_message)}`,\n    });\n\n    reply.send({\n      success: true,\n      message: 'Call initiated',\n      callSid: call.sid,\n    });\n  } catch (error) {\n    console.error('Error initiating outbound call:', error);\n    reply.code(500).send({\n      success: false,\n      error: 'Failed to initiate call',\n    });\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables Configuration for ElevenLabs API Authentication in YAML\nDESCRIPTION: This YAML snippet defines environment variables in a '.env.local' file for securing ElevenLabs API interactions. 'ELEVENLABS_API_KEY' stores the API key for server authentication, and 'NEXT_PUBLIC_AGENT_ID' stores the public agent identifier. Keeping this file out of source control is critical to protect sensitive credentials. These variables enable server-side signed URL generation for private agent use.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nELEVENLABS_API_KEY=your-api-key-here\nNEXT_PUBLIC_AGENT_ID=your-agent-id-here\n\n```\n\n----------------------------------------\n\nTITLE: Webhook Handler - Stateful Conversations - JavaScript\nDESCRIPTION: This JavaScript code demonstrates how to manage state in a conversation using a webhook.  The handler stores conversation details (transcript, summary, history) from the incoming webhook payload, enabling a seamless interaction experience.  The `initiateCall` function shows how to retrieve and use this stored state when initiating subsequent calls.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/workflows/post-call-webhook.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Store conversation state when call ends\napp.post('/webhook/elevenlabs', async (req, res) => {\n  // HMAC validation code\n\n  const { data } = req.body;\n  const userId = data.metadata.user_id;\n\n  // Store conversation state\n  await db.userStates.upsert({\n    userId,\n    lastConversationId: data.conversation_id,\n    lastInteractionTimestamp: data.metadata.start_time_unix_secs,\n    conversationHistory: data.transcript,\n    previousTopics: extractTopics(data.analysis.transcript_summary),\n  });\n\n  res.status(200).send('Webhook received');\n});\n\n// When initiating a new call, retrieve and use the state\nasync function initiateCall(userId) {\n  // Get user's conversation state\n  const userState = await db.userStates.findOne({ userId });\n\n  // Start new conversation with context from previous calls\n  return await elevenlabs.startConversation({\n    agent_id: 'xyz',\n    conversation_id: generateNewId(),\n    dynamic_variables: {\n      user_name: userState.name,\n      previous_conversation_id: userState.lastConversationId,\n      previous_topics: userState.previousTopics.join(', '),\n    },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Making a Voice Transformation API Request with ElevenLabs SDK - TypeScript\nDESCRIPTION: This TypeScript snippet illustrates fetching an MP3 audio file, transforming its voice through the ElevenLabs Voice Changer API, and playing the resulting audio in a Node.js context. It leverages the 'elevenlabs' npm package and requires dotenv configuration for environment management; audio playback may require system-level support (e.g., MPV, ffmpeg). Key parameters include the voiceId, model ID, audio Blob, and output format specification. The script expects valid API credentials and will play the transformed audio upon successful conversion.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-changer/quickstart.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\nconst voiceId = \"JBFqnCBsd6RMkjVDRZzb\";\n\nconst response = await fetch(\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst audioStream = await client.speechToSpeech.convert(voiceId, {\n  audio: audioBlob,\n  model_id: \"eleven_multilingual_sts_v2\",\n  output_format: \"mp3_44100_128\",\n});\n\nawait play(audioStream);\n\n```\n\n----------------------------------------\n\nTITLE: Exposing Local Server to Public Network Using Ngrok (shell)\nDESCRIPTION: Explains how to use ngrok to expose the local development server on port 5000, necessary for Twilio webhooks to reach the local Express app. Input: The port (5000) being served locally; Output: a ngrok tunnel URL. Prerequisites: ngrok account and CLI installed. The generated public ngrok URL must be updated in the .env file under SERVER_DOMAIN.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nngrok http 5000\n```\n\n----------------------------------------\n\nTITLE: Applying Overrides when Starting Conversation in JavaScript\nDESCRIPTION: This snippet shows how to pass conversation override parameters (agent prompt, first message, language, TTS voice ID) directly within an `overrides` object when calling the `Conversation.startSession` method in JavaScript. This enables dynamic personalization for the user's conversation session. Requires the ElevenLabs JavaScript SDK.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n...\nconst conversation = await Conversation.startSession({\n  ...\n  overrides: {\n      agent: {\n          prompt: {\n              prompt: `The customer's bank account balance is ${customer_balance}. They are based in ${customer_location}.`\n          },\n          firstMessage: `Hi ${customer_name}, how can I help you today?`,\n          language: \"en\" // Optional: override the language.\n      },\n      tts: {\n          voiceId: \"\" # Optional: override the voice.\n      }\n  },\n  ...\n})\n```\n\n----------------------------------------\n\nTITLE: Making a Text to Speech Request with ElevenLabs Python SDK\nDESCRIPTION: This Python script demonstrates initializing the ElevenLabs client with an API key (loaded from environment variables via `dotenv`), converting text to speech using the `text_to_speech.convert` method, specifying the voice, model, and output format, and playing the resulting audio using the SDK's `play` function. Requires the `elevenlabs` and `python-dotenv` packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport os\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio = client.text_to_speech.convert(\n    text=\"The first move is what sets everything in motion.\",\n    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n    model_id=\"eleven_multilingual_v2\",\n    output_format=\"mp3_44100_128\",\n)\n\nplay(audio)\n```\n\n----------------------------------------\n\nTITLE: Configuring Client Tools and Dynamic Variables for ElevenLabs Conversation (TypeScript)\nDESCRIPTION: This snippet focuses on the options object passed to the `conversation.startSession` method. It illustrates how to provide dynamic data (`dynamicVariables`) like `user_name` to the agent and how to define client-side functions (`clientTools`) like `set_ui_state`. The `set_ui_state` tool allows the AI agent to trigger UI changes (e.g., navigation) by calling this function, which updates the React component's state.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_4\n\nLANGUAGE: tsx\nCODE:\n```\nconst convId = await conversation.startSession({\n  signedUrl,\n  dynamicVariables: {\n    user_name: userName,\n  },\n  clientTools: {\n    set_ui_state: ({ step }: { step: string }): string => {\n      // Allow agent to navigate the UI.\n      setCurrentStep(step as 'initial' | 'training' | 'voice' | 'email' | 'ready');\n      return `Navigated to ${step}`;\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Passing Dynamic Variables at Runtime - ElevenLabs Swift SDK\nDESCRIPTION: This Swift snippet illustrates preparing and passing dynamic variables in a dictionary to create a personalized conversation session using the ElevenLabs iOS SDK. Variables include various supported types (string, number, int, boolean), and the SessionConfig object is initialized with these values before calling startSession asynchronously. Prerequisites are integration of the ElevenLabs Swift library and valid session agentId. Inputs: a [String: DynamicVariableValue] dictionary. Outputs: a conversation object returned as an awaitable result. The code is for iOS clients and requires Swift Concurrency (async/await).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#_snippet_2\n\nLANGUAGE: swift\nCODE:\n```\nlet dynamicVars: [String: DynamicVariableValue] = [\n  \"customer_name\": .string(\"John Doe\"),\n  \"account_balance\": .number(5000.50),\n  \"user_id\": .int(12345),\n  \"is_premium\": .boolean(true)\n]\n\n// Create session config with dynamic variables\nlet config = SessionConfig(\n    agentId: \"your_agent_id\",\n    dynamicVariables: dynamicVars\n)\n\n// Start the conversation\nlet conversation = try await Conversation.startSession(\n    config: config\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Environment Variable Management Packages\nDESCRIPTION: Commands to install packages for managing environment variables in Python and JavaScript.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install python-dotenv\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install dotenv\n```\n\n----------------------------------------\n\nTITLE: Passing Dynamic Variables at Runtime - ElevenLabs JavaScript SDK\nDESCRIPTION: This JavaScript snippet shows how to start an ElevenLabs conversational agent session in a web environment, passing dynamic variables using the @11labs/client library. Microphone permission is requested prior to session initiation. The dynamicVariables option takes an object with runtime values (e.g., user_name: 'Angelo'). Inputs include agentId and dynamic variables. Outputs and session events should be handled via optional callbacks. The code assumes correct agent ID and a supported browser environment.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Conversation } from '@11labs/client';\n\nclass VoiceAgent {\n  ...\n\n  async startConversation() {\n    try {\n        // Request microphone access\n        await navigator.mediaDevices.getUserMedia({ audio: true });\n\n        this.conversation = await Conversation.startSession({\n            agentId: 'agent_id_goes_here', // Replace with your actual agent ID\n\n            dynamicVariables: {\n                user_name: 'Angelo'\n            },\n\n            ... add some callbacks here\n        });\n    } catch (error) {\n        console.error('Failed to start conversation:', error);\n        alert('Failed to start conversation. Please ensure microphone access is granted.');\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example: Expressive Agent Personality Prompt\nDESCRIPTION: Defines the personality for a nurturing virtual wellness coach named Joe. It emphasizes traits like calmness, empathy, active listening, curiosity, and the ability to guide users towards mindfulness techniques, establishing an expressive and supportive identity.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Personality\n\nYou are Joe, a nurturing virtual wellness coach.\nYou speak calmly and empathetically, always validating the user's emotions.\nYou guide them toward mindfulness techniques or positive affirmations when needed.\nYou're naturally curious, empathetic, and intuitive, always aiming to deeply understand the user's intent by actively listening.\nYou thoughtfully refer back to details they've previously shared.\n```\n\n----------------------------------------\n\nTITLE: Initializing ElevenLabs Python Client with API Key\nDESCRIPTION: Shows how to initialize the `ElevenLabs` client using the official `elevenlabs` Python package. The API key is passed during client instantiation. Requires the `elevenlabs` package to be installed (`pip install elevenlabs`). Replace `'YOUR_API_KEY'` with your actual key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs.client import ElevenLabs\n\nclient = ElevenLabs(\n  api_key='YOUR_API_KEY',\n)\n```\n\n----------------------------------------\n\nTITLE: Executing the Python Speech-to-Text Script\nDESCRIPTION: This command executes the Python script (`example.py`) to perform speech-to-text conversion using the ElevenLabs API. Requires Python and installed dependencies (`python-dotenv`, `requests`, `elevenlabs`).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/quickstart.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Combining text and past generations conditioning for optimal prosody\nDESCRIPTION: This snippet illustrates how to condition text-to-speech requests on both the original text (previous and next) and previous request IDs, leveraging history to generate more natural output across chunks.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/request-stitching.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport requests\nfrom pydub import AudioSegment\nimport io\n\nYOUR_XI_API_KEY = \"<insert your xi-api-key here>\"\nVOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\nPARAGRAPHS = [\n    \"The advent of technology has transformed countless sectors, with education \"\n    \"standing out as one of the most significantly impacted fields.\",\n    \"In recent years, educational technology, or EdTech, has revolutionized the way \"\n    \"teachers deliver instruction and students absorb information.\",\n    \"From interactive whiteboards to individual tablets loaded with educational software, \"\n    \"technology has opened up new avenues for learning that were previously unimaginable.\",\n    \"One of the primary benefits of technology in education is the accessibility it provides.\",\n]\nsegments = []\nprevious_request_ids = []\n\nfor i, paragraph in enumerate(PARAGRAPHS):\n    is_first_paragraph = i == 0\n    is_last_paragraph = i == len(PARAGRAPHS) - 1\n    response = requests.post(\n        f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\",\n        json={\n            \"text\": paragraph,\n            \"model_id\": \"eleven_multilingual_v2\",\n            \"previous_request_ids\": previous_request_ids[-3:],  # last 3 ids\n            \"previous_text\": None if is_first_paragraph else \" \".join(PARAGRAPHS[:i]),\n            \"next_text\": None if is_last_paragraph else \" \".join(PARAGRAPHS[i + 1:])\n        },\n        headers={\"xi-api-key\": YOUR_XI_API_KEY},\n    )\n\n    if response.status_code != 200:\n        print(f\"Error encountered, status: {response.status_code}, \"\n               f\"content: {response.text}\")\n        quit()\n\n    print(f\"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}\")\n    previous_request_ids.append(response.headers[\"request-id\"])\n    segments.append(AudioSegment.from_mp3(io.BytesIO(response.content)))\n\nsegment = segments[0]\nfor new_segment in segments[1:]:\n    segment = segment + new_segment\n\naudio_out_path = os.path.join(os.getcwd(), \"with_full_conditioning.wav\")\nsegment.export(audio_out_path, format=\"wav\")\nprint(f\"Success! Wrote audio to {audio_out_path}\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Documents to ElevenLabs Knowledge Base using Next.js Server Action (TypeScript)\nDESCRIPTION: This code defines a Next.js Server Action (`uploadFormData`) to handle uploads to an ElevenLabs knowledge base. It extracts files, URLs, email, and conversation ID from `FormData`. Using the `after` function (Next.js 15+), it asynchronously processes file uploads and URL submissions in the background using the `ElevenLabsClient` SDK (`elevenLabsClient.conversationalAi.addToKnowledgeBase`). Finally, it stores the generated knowledge base item IDs and the user's email, keyed by the conversation ID, into a Redis database using `@upstash/redis`. The action redirects to '/success' after initiating the background tasks. Requires `elevenlabs`, `@upstash/redis`, `next` packages, and environment variables for API keys and Redis connection.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n'use server';\n\nimport { Redis } from '@upstash/redis';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { redirect } from 'next/navigation';\nimport { after } from 'next/server';\n\n// Initialize Redis\nconst redis = Redis.fromEnv();\n\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: process.env.ELEVENLABS_API_KEY,\n});\n\nexport async function uploadFormData(formData: FormData) {\n  const knowledgeBase: Array<{\n    id: string;\n    type: 'file' | 'url';\n    name: string;\n  }> = [];\n  const files = formData.getAll('file-upload') as File[];\n  const email = formData.get('email-input');\n  const urls = formData.getAll('url-input');\n  const conversationId = formData.get('conversation-id');\n\n  after(async () => {\n    // Upload files as background job\n    // Create knowledge base entries\n    // Loop trhough files and create knowledge base entries\n    for (const file of files) {\n      if (file.size > 0) {\n        const response = await elevenLabsClient.conversationalAi.addToKnowledgeBase({ file });\n        if (response.id) {\n          knowledgeBase.push({\n            id: response.id,\n            type: 'file',\n            name: file.name,\n          });\n        }\n      }\n    }\n    // Append all urls\n    for (const url of urls) {\n      const response = await elevenLabsClient.conversationalAi.addToKnowledgeBase({\n        url: url as string,\n      });\n      if (response.id) {\n        knowledgeBase.push({\n          id: response.id,\n          type: 'url',\n          name: `url for ${conversationId}`,\n        });\n      }\n    }\n\n    // Store knowledge base IDs and conversation ID in database.\n    const redisRes = await redis.set(\n      conversationId as string,\n      JSON.stringify({ email, knowledgeBase })\n    );\n    console.log({ redisRes });\n  });\n\n  redirect('/success');\n}\n```\n\n----------------------------------------\n\nTITLE: Making Outbound Call API Request - Bash\nDESCRIPTION: This snippet uses `curl` to send a POST request to the `/outbound-call` endpoint. It constructs a JSON payload containing the prompt, the first message, and the phone number to call. It then sends this data to the server using the specified ngrok URL. This command initiates an outbound call, triggering the server to interact with the ElevenLabs API and the Twilio service to make the call.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://<your-ngrok-url>/outbound-call \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"prompt\": \"You are Eric, an outbound car sales agent. You are calling to sell a new car to the customer. Be friendly and professional and answer all questions.\",\n    \"first_message\": \"Hello Thor, my name is Eric, I heard you were looking for a new car! What model and color are you looking for?\",\n    \"number\": \"number-to-call\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Example: Call Center Environment Prompt\nDESCRIPTION: Establishes the environment for an AI agent handling calls on a busy telecom support hotline. It specifies audio-only interaction, lack of video, and access to internal resources like customer databases and troubleshooting guides.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n# Environment\n\nYou are assisting a caller via a busy telecom support hotline.\nYou can hear the user's voice but have no video. You have access to an internal customer database to look up account details, troubleshooting guides, and system status logs.\n```\n\n----------------------------------------\n\nTITLE: Setting Voice Parameters for Audio Generation in ElevenLabs WebSocket (TypeScript)\nDESCRIPTION: This TypeScript snippet describes sending a WebSocket payload with a voice_settings object to ElevenLabs for customized synthesis output, controlling attributes such as stability (float), similarity_boost (float), and use_speaker_boost (boolean). The message is JSON.stringified and sent over an established WebSocket. This approach allows different audio characteristics per message by varying the voice_settings content.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nwebsocket.send(\n  JSON.stringify({\n    text: text,\n    voice_settings: { stability: 0.5, similarity_boost: 0.8, use_speaker_boost: false },\n  })\n);\n```\n\n----------------------------------------\n\nTITLE: Executing Example Voice Cloning Scripts in TypeScript\nDESCRIPTION: This command uses npx with tsx to execute the provided example.mts TypeScript script, which constructs and sends a voice cloning request with the ElevenLabs API. Dependencies include tsx for running TypeScript files and the ElevenLabs and dotenv packages. Input is the script and environment configuration; output is the printed voice ID.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/instant-voice-cloning.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Connecting to an Agent Using a Signed URL\nDESCRIPTION: Shows client-side code examples in Python and JavaScript for establishing a WebSocket connection to an ElevenLabs conversational AI agent using a previously obtained signed URL. This approach is used when the agent requires authentication (`requires_auth=True`) or when directly providing the `url` parameter during Conversation initialization.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Client-side code using the Python SDK\nfrom elevenlabs.conversational_ai.conversation import (\n    Conversation,\n    AudioInterface,\n    ClientTools,\n    ConversationInitiationData\n)\nimport os\nfrom elevenlabs.client import ElevenLabs\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\n\nclient = ElevenLabs(api_key=api_key)\n\nconversation = Conversation(\n  client=client,\n  agent_id=os.getenv(\"AGENT_ID\"),\n  requires_auth=True,\n  audio_interface=AudioInterface(),\n  config=ConversationInitiationData()\n)\n\nasync def start_conversation():\n  try:\n    signed_url = await get_signed_url() # Assume get_signed_url fetches from server\n    conversation = Conversation(\n      client=client,\n      url=signed_url,\n    )\n\n    conversation.start_session()\n  except Exception as error:\n    print(f\"Failed to start conversation: {error}\")\n```\n\nLANGUAGE: javascript\nCODE:\n```\n// Client-side code using the JavaScript SDK\nimport { Conversation } from '@11labs/client';\n\nasync function startConversation() {\n  try {\n    const signedUrl = await getSignedUrl(); // Assume getSignedUrl fetches from server\n    const conversation = await Conversation.startSession({\n      signedUrl,\n    });\n\n    return conversation;\n  } catch (error) {\n    console.error('Failed to start conversation:', error);\n    throw error;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Putting Together Text-to-Speech Streaming and S3 Upload in Python\nDESCRIPTION: A full Python script that loads environment variables, converts text to speech using text_to_speech_stream, uploads the result to S3, generates a presigned URL, and prints it. Requires external modules text_to_speech_stream, s3_uploader, python-dotenv, and AWS credentials. Intended as an entry point when run as main.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nfrom text_to_speech_stream import text_to_speech_stream\nfrom s3_uploader import upload_audiostream_to_s3, generate_presigned_url\n\n\ndef main():\n    text = \"This is James\"\n\n    audio_stream = text_to_speech_stream(text)\n    s3_file_name = upload_audiostream_to_s3(audio_stream)\n    signed_url = generate_presigned_url(s3_file_name)\n\n    print(f\"Signed URL to access the file: {signed_url}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n----------------------------------------\n\nTITLE: Example: Technical support troubleshooting agent goal\nDESCRIPTION: Provides a structured goal definition for a technical support agent using MDX. It outlines a multi-phase process: initial assessment, diagnostic sequence, resolution implementation, and closure, including conditional branching and success metrics like first-contact resolution.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_8\n\nLANGUAGE: mdx\nCODE:\n```\n# Goal\n\nYour primary goal is to efficiently diagnose and resolve technical issues through this structured troubleshooting framework:\n\n1. Initial assessment phase:\n\n   - Identify affected product or service with specific version information\n   - Determine severity level (critical, high, medium, low) based on impact assessment\n   - Establish environmental factors (device type, operating system, connection type)\n   - Confirm frequency of issue (intermittent, consistent, triggered by specific actions)\n   - Document replication steps if available\n\n2. Diagnostic sequence:\n\n   - Begin with non-invasive checks before suggesting complex troubleshooting\n   - For connectivity issues: Proceed through OSI model layers (physical connections → network settings → application configuration)\n   - For performance problems: Follow resource utilization pathway (memory → CPU → storage → network)\n   - For software errors: Check version compatibility → recent changes → error logs → configuration issues\n   - Document all test results to build diagnostic profile\n\n3. Resolution implementation:\n\n   - Start with temporary workarounds if available while preparing permanent fix\n   - Provide step-by-step instructions with verification points at each stage\n   - For complex procedures, confirm completion of each step before proceeding\n   - If resolution requires system changes, create restore point or backup before proceeding\n   - Validate resolution through specific test procedures matching the original issue\n\n4. Closure process:\n   - Verify all reported symptoms are resolved\n   - Document root cause and resolution\n   - Configure preventative measures to avoid recurrence\n   - Schedule follow-up for intermittent issues or partial resolutions\n   - Provide education to prevent similar issues (if applicable)\n\nApply conditional branching at key decision points: If issue persists after standard troubleshooting, escalate to specialized team with complete diagnostic data. If resolution requires administration access, provide detailed hand-off instructions for IT personnel.\n\nSuccess is measured by first-contact resolution rate, average resolution time, and prevention of issue recurrence.\n```\n\n----------------------------------------\n\nTITLE: Example: Customer support refund agent goal\nDESCRIPTION: Defines a structured goal for a customer support agent handling refund requests using MDX. The goal includes phases for request validation, resolution assessment, processing workflow (with tiered verification/escalation), and closure, specifying conditions for exceptions and success metrics.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_9\n\nLANGUAGE: mdx\nCODE:\n```\n# Goal\n\nYour primary goal is to efficiently process refund requests while maintaining company policies through the following structured workflow:\n\n1. Request validation phase:\n\n   - Confirm customer identity using account verification (order number, email, and last 4 digits of payment method)\n   - Identify purchase details (item, purchase date, order total)\n   - Determine refund reason code from predefined categories (defective item, wrong item, late delivery, etc.)\n   - Confirm the return is within the return window (14 days for standard items, 30 days for premium members)\n\n2. Resolution assessment phase:\n\n   - If the item is defective: Determine if the customer prefers a replacement or refund\n   - If the item is non-defective: Review usage details to assess eligibility based on company policy\n   - For digital products: Verify the download/usage status before proceeding\n   - For subscription services: Check cancellation eligibility and prorated refund calculations\n\n3. Processing workflow:\n\n   - For eligible refunds under $100: Process immediately\n   - For refunds $100-$500: Apply secondary verification procedure (confirm shipping status, transaction history)\n   - For refunds over $500: Escalate to supervisor approval with prepared case notes\n   - For items requiring return: Generate return label and provide clear return instructions\n\n4. Resolution closure:\n   - Provide expected refund timeline (3-5 business days for credit cards, 7-10 days for bank transfers)\n   - Document all actions taken in the customer's account\n   - Offer appropriate retention incentives based on customer history (discount code, free shipping)\n   - Schedule follow-up check if system flags potential issues with refund processing\n\nIf the refund request falls outside standard policy, look for acceptable exceptions based on customer loyalty tier, purchase history, or special circumstances. Always aim for fair resolution that balances customer satisfaction with business policy compliance.\n\nSuccess is defined by the percentage of resolved refund requests without escalation, average resolution time, and post-interaction customer satisfaction scores.\n```\n\n----------------------------------------\n\nTITLE: Initializing Next.js Project with npm Shell Commands\nDESCRIPTION: These shell commands guide the user through creating a new Next.js application using npm, navigating into the project directory, installing the '@11labs/react' package dependency, and running the development server. Prerequisites include having npm installed. The commands set up the project environment needed for the conversational AI implementation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm create next-app my-conversational-agent\n```\n\nLANGUAGE: shell\nCODE:\n```\ncd my-conversational-agent\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @11labs/react\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for WebSocket TTS\nDESCRIPTION: Provides commands to install the necessary Python and TypeScript packages (`python-dotenv`, `websockets`, `dotenv`, `@types/dotenv`, `ws`) required for setting up and running the ElevenLabs Text-to-Speech WebSocket examples.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install python-dotenv\npip install websockets\n```\n\nLANGUAGE: typescript\nCODE:\n```\nnpm install dotenv\nnpm install @types/dotenv --save-dev\nnpm install ws\n```\n\n----------------------------------------\n\nTITLE: Overriding Agent Behavior Using Widget Attributes at Runtime - HTML\nDESCRIPTION: This snippet provides an example of overriding default agent behavior and presentation dynamically by specifying optional attributes on the widget component. Attributes like `override-language` change the interaction language, `override-prompt` customizes the system prompt, `override-first-message` sets the initial user message, and `override-voice-id` selects a custom voice. These overrides are optional and allow fine-grained runtime control over the AI agent's behavior.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\\n  agent-id=\"your-agent-id\"\\n  override-language=\"es\"\\n  override-prompt=\"Custom system prompt for this user\"\\n  override-first-message=\"Hi! How can I help you today?\"\\n  override-voice-id=\"axXgspJ2msm3clMCkdW3\"\\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Configuring Zendesk Conversational AI Support Agent System Prompt - plaintext\nDESCRIPTION: This snippet presents a detailed system prompt used to guide the ElevenLabs conversational AI support agent in handling customer inquiries, checking for similar past issues via Zendesk tools, advising on resolutions, collecting contact details, and creating support tickets using the zendesk_open_ticket tool. The prompt ensures stepwise information gathering, friendly tone, email validation, and adherence to guardrails limiting conversation to support issues only.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/zendesk.mdx#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are a helpful ElevenLabs support agent responsible for gathering information from users and creating support tickets using the zendesk_open_ticket tool. Be friendly, precise, and concise.\n\nBegin by briefly asking asking for a detailed description of the problem.\nThen, ask relevant support questions to gather additional details, one question at a time, and wait for the user's response before proceeding.\n\nOnce you have a description of the issue, say you will check if there are similar issues and any known resolutions.\n- call get_resolved_tickets\n- find the ticket which has the most similar issue to that of the caller\n- call get_ticket_comments, using the result id from the previous response\n- get any learnings from the resolution of this ticket\n\nAfter this, tell the customer the recommended resolution from a previous similar issue. If they have already tried it or still want to move forward, move to the ticket creation step. Only provide resolution advice derived from the comments.\n\nAfter capturing the support issue, gather the following contact details:\n- The user's name.\n- A valid email address for the requestor. Note that the email address is transcribed from voice, so ensure it is formatted correctly.\n- Read the email back to the caller to confirm accuracy.\n\nOnce the email is confirmed, explain that you will create the ticket.\nCreate the ticket by using the Tool zendesk_open_ticket. Add these details to the ticket comment body.\nThank the customer and say support will be in touch.\n\nClarifications:\n- Do not inform the user that you are formatting the email; simply do it.\n- If the caller asks you to move forward with creating the ticket, do so with the existing information.\n\nGuardrails:\n- Do not speak about topics outside of support issues with ElevenLabs.\n```\n\n----------------------------------------\n\nTITLE: Python: Initiating and Handling Streaming Text-to-Speech with elevenlabs Library\nDESCRIPTION: Demonstrates how to generate a speech audio stream using the elevenlabs Python SDK, and how to process or play the streamed audio. It includes dependencies on 'elevenlabs' package and shows both playing and manual chunk processing of the stream.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/streaming.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom elevenlabs import stream\nfrom elevenlabs.client import ElevenLabs\n\nclient = ElevenLabs()\n\n# Generate audio stream for given text and voice\naudio_stream = client.text_to_speech.convert_as_stream(\n    text=\"This is a test\",\n    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n    model_id=\"eleven_multilingual_v2\"\n)\n\n# Option 1: Play the streamed audio locally\nstream(audio_stream)\n\n# Option 2: Process the audio bytes manually\nfor chunk in audio_stream:\n    if isinstance(chunk, bytes):\n        print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Implementing Conversation Logic with ElevenLabs API in JavaScript\nDESCRIPTION: JavaScript code that implements the core conversation functionality using the ElevenLabs client library. It handles starting and stopping conversations, microphone permissions, and updates the UI based on connection and agent status.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Conversation } from '@11labs/client';\n\nconst startButton = document.getElementById('startButton');\nconst stopButton = document.getElementById('stopButton');\nconst connectionStatus = document.getElementById('connectionStatus');\nconst agentStatus = document.getElementById('agentStatus');\n\nlet conversation;\n\nasync function startConversation() {\n    try {\n        // Request microphone permission\n        await navigator.mediaDevices.getUserMedia({ audio: true });\n\n        // Start the conversation\n        conversation = await Conversation.startSession({\n            agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n            onConnect: () => {\n                connectionStatus.textContent = 'Connected';\n                startButton.disabled = true;\n                stopButton.disabled = false;\n            },\n            onDisconnect: () => {\n                connectionStatus.textContent = 'Disconnected';\n                startButton.disabled = false;\n                stopButton.disabled = true;\n            },\n            onError: (error) => {\n                console.error('Error:', error);\n            },\n            onModeChange: (mode) => {\n                agentStatus.textContent = mode.mode === 'speaking' ? 'speaking' : 'listening';\n            },\n        });\n    } catch (error) {\n        console.error('Failed to start conversation:', error);\n    }\n}\n\nasync function stopConversation() {\n    if (conversation) {\n        await conversation.endSession();\n        conversation = null;\n    }\n}\n\nstartButton.addEventListener('click', startConversation);\nstopButton.addEventListener('click', stopConversation);\n```\n\n----------------------------------------\n\nTITLE: Using Documentation Assistant Tools\nDESCRIPTION: This snippet describes the tools available for a documentation assistant. It outlines how to use tools like `searchKnowledgeBase`, `redirectToDocs`, `generateCodeExample`, `checkFeatureCompatibility`, and `redirectToSupportForm`. It also defines tool orchestration and priority.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_14\n\nLANGUAGE: mdx\nCODE:\n```\n# Tools\n\nYou have access to the following tools to assist users with ElevenLabs products:\n\n`searchKnowledgeBase`: When users ask about specific features or functionality, use this tool to query our documentation for accurate information before responding. Always prioritize this over recalling information from memory.\n\n`redirectToDocs`: When a topic requires in-depth explanation or technical details, use this tool to direct users to the relevant documentation page (e.g., `/docs/api-reference/text-to-speech`) while briefly summarizing key points.\n\n`generateCodeExample`: For implementation questions, use this tool to provide a relevant code snippet in the user's preferred language (Python, JavaScript, etc.) demonstrating how to use the feature they're asking about.\n\n`checkFeatureCompatibility`: When users ask if certain features work together, use this tool to verify compatibility between different ElevenLabs products and provide accurate information about integration options.\n\n`redirectToSupportForm`: If the user's question involves account-specific issues or exceeds your knowledge scope, use this as a final fallback after attempting other tools.\n\nTool orchestration: First attempt to answer with knowledge base information, then offer code examples for implementation questions, and only redirect to documentation or support as a final step when necessary.\n```\n\n----------------------------------------\n\nTITLE: Injecting Dynamic Variables into Widget at Runtime - HTML\nDESCRIPTION: This snippet showcases how to pass dynamic variables into the Conversational AI widget by adding a JSON object to the `dynamic-variables` attribute. These variables enable personalization by injecting runtime data such as user name or account type directly into system prompts, messages, and tools. The agent must be configured to accept these variables, and all required variables must be supplied via this attribute.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\\n  agent-id=\"your-agent-id\"\\n  dynamic-variables='{\"user_name\": \"John\", \"account_type\": \"premium\"}'\\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Streaming Text-to-Speech Audio with Parallel Storage Caching\nDESCRIPTION: Implementation that generates a text-to-speech stream using ElevenLabs API, splits it into two branches (one for immediate browser delivery and one for storage caching), and handles error cases.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\ntry {\n  const response = await client.textToSpeech.convertAsStream(voiceId, {\n    output_format: \"mp3_44100_128\",\n    model_id: \"eleven_multilingual_v2\",\n    text,\n  });\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      for await (const chunk of response) {\n        controller.enqueue(chunk);\n      }\n      controller.close();\n    },\n  });\n\n  // Branch stream to Supabase Storage\n  const [browserStream, storageStream] = stream.tee();\n\n  // Upload to Supabase Storage in the background\n  EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash));\n\n  // Return the streaming response immediately\n  return new Response(browserStream, {\n    headers: {\n      \"Content-Type\": \"audio/mpeg\",\n    },\n  });\n} catch (error) {\n  console.log(\"error\", { error });\n  return new Response(JSON.stringify({ error: error.message }), {\n    status: 500,\n    headers: { \"Content-Type\": \"application/json\" },\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring an Allowlist for an Agent\nDESCRIPTION: Provides Python and JavaScript examples for creating an ElevenLabs conversational AI agent with an allowlist configured via `platform_settings.auth.allowlist`. This restricts connections to only those originating from specified hostnames (e.g., 'example.com', 'localhost:3000'). In this configuration, `enable_auth` is set to `false`, relying solely on the origin check.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs.client import ElevenLabs\nimport os\nfrom elevenlabs.types import *\n\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(api_key=api_key)\n\nagent = client.conversational_ai.create_agent(\n  conversation_config=ConversationalConfig(\n    agent=AgentConfig(\n      first_message=\"Hi. I'm an authenticated agent.\",\n    )\n  ),\n  platform_settings=AgentPlatformSettingsRequestModel(\n  auth=AuthSettings(\n    enable_auth=False,\n    allowlist=[\n      AllowlistItem(hostname=\"example.com\"),\n      AllowlistItem(hostname=\"app.example.com\"),\n      AllowlistItem(hostname=\"localhost:3000\")\n      ]\n    )\n  )\n)\n```\n\nLANGUAGE: javascript\nCODE:\n```\nasync function createAuthenticatedAgent(client) {\n  try {\n    const agent = await client.conversationalAi.createAgent({\n      conversation_config: {\n        agent: {\n          first_message: \"Hi. I'm an authenticated agent.\",\n        },\n      },\n      platform_settings: {\n        auth: {\n          enable_auth: false,\n          allowlist: [\n            { hostname: 'example.com' },\n            { hostname: 'app.example.com' },\n            { hostname: 'localhost:3000' },\n          ],\n        },\n      },\n    });\n\n    return agent;\n  } catch (error) {\n    console.error('Error creating agent:', error);\n    throw error;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementation of TwilioAudioInterface Class for WebSocket Audio Handling\nDESCRIPTION: This code defines the TwilioAudioInterface class, extending the AudioInterface base, to manage real-time audio streams over WebSockets. It includes methods for starting/stopping audio output, handling incoming media data from Twilio, sending audio to Twilio, and sending a clear message to terminate streams. Threading and asyncio are utilized for concurrent media processing and communication.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\nfrom typing import Callable\nimport queue\nimport threading\nimport base64\nfrom elevenlabs.conversational_ai.conversation import AudioInterface\nimport websockets\n\nclass TwilioAudioInterface(AudioInterface):\n    def __init__(self, websocket):\n        self.websocket = websocket\n        self.output_queue = queue.Queue()\n        self.should_stop = threading.Event()\n        self.stream_sid = None\n        self.input_callback = None\n        self.output_thread = None\n\n    def start(self, input_callback: Callable[[bytes], None]):\n        self.input_callback = input_callback\n        self.output_thread = threading.Thread(target=self._output_thread)\n        self.output_thread.start()\n\n    def stop(self):\n        self.should_stop.set()\n        if self.output_thread:\n            self.output_thread.join(timeout=5.0)\n        self.stream_sid = None\n\n    def output(self, audio: bytes):\n        self.output_queue.put(audio)\n\n    def interrupt(self):\n        try:\n            while True:\n                _ = self.output_queue.get(block=False)\n        except queue.Empty:\n            pass\n        asyncio.run(self._send_clear_message_to_twilio())\n\n    async def handle_twilio_message(self, data):\n        try:\n            if data[\"event\"] == \"start\":\n                self.stream_sid = data[\"start\"][\"streamSid\"]\n                print(f\"Started stream with stream_sid: {self.stream_sid}\")\n            if data[\"event\"] == \"media\":\n                audio_data = base64.b64decode(data[\"media\"][\"payload\"])\n                if self.input_callback:\n                    self.input_callback(audio_data)\n        except Exception as e:\n            print(f\"Error in input_callback: {e}\")\n\n    def _output_thread(self):\n        while not self.should_stop.is_set():\n            asyncio.run(self._send_audio_to_twilio())\n\n    async def _send_audio_to_twilio(self):\n        try:\n            audio = self.output_queue.get(timeout=0.2)\n            audio_payload = base64.b64encode(audio).decode(\"utf-8\")\n            audio_delta = {\n                \"event\": \"media\",\n                \"streamSid\": self.stream_sid,\n                \"media\": {\"payload\": audio_payload},\n            }\n            await self.websocket.send_json(audio_delta)\n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error sending audio: {e}\")\n\n    async def _send_clear_message_to_twilio(self):\n        try:\n            clear_message = {\"event\": \"clear\", \"streamSid\": self.stream_sid}\n            await self.websocket.send_json(clear_message)\n        except Exception as e:\n            print(f\"Error sending clear message to Twilio: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Guiding Tool Selection via System Prompt (Simple Example)\nDESCRIPTION: Demonstrates a simple instruction within a system prompt to guide an AI assistant on when to use a specific tool (`check_order_status`) based on user queries about order status.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-tool-best-practices.mdx#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nUse `check_order_status` when the user inquires about the status of their order, such as 'Where is my order?' or 'Has my order shipped yet?'.\n```\n\n----------------------------------------\n\nTITLE: Correct CMU Arpabet SSML (Primary/Secondary Stress)\nDESCRIPTION: Demonstrates the correct usage of CMU Arpabet within SSML phoneme tags, including stress markers ('1' for primary, '0' for secondary/no stress). Explicitly marking stress ensures consistent and accurate pronunciation according to the desired emphasis pattern, crucial for natural-sounding speech.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_3\n\nLANGUAGE: SSML\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"T AE1 L AH0 N\">talon</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Uploading Audio Stream to S3 with Python Function Call\nDESCRIPTION: Demonstrates the calling sequence for uploading an audio stream object to S3 using the previously defined function. The input must be a file-like audio_stream object, and the output is the S3 filename string. The environment and dependencies must be configured as shown previously.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ns3_file_name = upload_audiostream_to_s3(audio_stream)\n```\n\n----------------------------------------\n\nTITLE: Sending Contextual Update Events - JavaScript\nDESCRIPTION: This JavaScript function demonstrates how to send a 'contextual_update' event over a WebSocket connection. It takes a string of information, formats it into the required JSON structure with the type 'contextual_update', converts it to a JSON string, and sends it via the 'websocket.send()' method. Requires an established WebSocket connection object named `websocket`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-to-server-events.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Example sending contextual updates\nfunction sendContextUpdate(information) {\n  websocket.send(\n    JSON.stringify({\n      type: 'contextual_update',\n      text: information,\n    })\n  );\n}\n\n// Usage examples\nsendContextUpdate('Customer status: Premium tier');\nsendContextUpdate('User navigated to Help section');\nsendContextUpdate('Shopping cart contains 3 items');\n```\n\n----------------------------------------\n\nTITLE: Server-side route to generate signed URL for conversation session\nDESCRIPTION: Provides a Node.js Express route example that requests a signed URL from ElevenLabs API using an API key, then sends this URL back to the client for initiating a session. This secures the conversation setup process.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Node.js server\n\napp.get('/signed-url', yourAuthMiddleware, async (req, res) => {\n  const response = await fetch(\n    `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.AGENT_ID}`,\n    {\n      method: 'GET',\n      headers: {\n        'xi-api-key': process.env.XI_API_KEY,\n      },\n    }\n  );\n\n  if (!response.ok) {\n    return res.status(500).send('Failed to get signed URL');\n  }\n\n  const body = await response.json();\n  res.send(body.signed_url);\n});\n```\n\n----------------------------------------\n\nTITLE: Starting an ElevenLabs Conversation Session in React/Next.js (TypeScript)\nDESCRIPTION: This snippet demonstrates how to initialize and start an ElevenLabs conversation session within a React component using the `useConversation` hook from `@11labs/react`. It involves fetching a signed URL from a backend API route, requesting microphone permissions, setting up event handlers for connection status and messages, and calling `conversation.startSession` with the signed URL, dynamic variables (like `user_name`), and client tool implementations (like `set_ui_state`). State management (`useState`, `useCallback`) is used to handle UI state, conversation ID, and user name.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\nimport { useConversation } from '@11labs/react';\n\nasync function getSignedUrl(): Promise<string> {\n  const response = await fetch('/api/signed-url');\n  if (!response.ok) {\n    throw Error('Failed to get signed url');\n  }\n  const data = await response.json();\n  return data.signedUrl;\n}\n\nexport default function Home() {\n  // ...\n  const [currentStep, setCurrentStep] = useState<\n    'initial' | 'training' | 'voice' | 'email' | 'ready'\n  >('initial');\n  const [conversationId, setConversationId] = useState('');\n  const [userName, setUserName] = useState('');\n\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message: string) => console.log('Message:', message),\n    onError: (error: Error) => console.error('Error:', error),\n  });\n\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n      // Start the conversation with your agent\n      const signedUrl = await getSignedUrl();\n      const convId = await conversation.startSession({\n        signedUrl,\n        dynamicVariables: {\n          user_name: userName,\n        },\n        clientTools: {\n          set_ui_state: ({ step }: { step: string }): string => {\n            // Allow agent to navigate the UI.\n            setCurrentStep(step as 'initial' | 'training' | 'voice' | 'email' | 'ready');\n            return `Navigated to ${step}`;\n          },\n        },\n      });\n      setConversationId(convId);\n      console.log('Conversation ID:', convId);\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation, userName]);\n  const stopConversation = useCallback(async () => {\n    await conversation.endSession();\n  }, [conversation]);\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Passing Dynamic Variables at Runtime - ElevenLabs Python SDK\nDESCRIPTION: This Python snippet demonstrates initializing an ElevenLabs conversational agent and passing user-specific dynamic variables (e.g., user_name) at conversation start. It uses the ElevenLabs SDK and requires the elevenlabs Python package and a valid API key, with optional callbacks for audio and message events. Inputs include dynamic variable values in a dictionary. Outputs are managed via callback functions for agent responses and user transcripts. Ensure the agent ID and API key environment variables are set. The code supports clean shutdown via signal handling and assumes SDK v1.3.0+ for dynamic variable support.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport signal\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation, ConversationConfig\nfrom elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface\n\nagent_id = os.getenv(\"AGENT_ID\")\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(api_key=api_key)\n\ndynamic_vars = {\n  \"user_name\": \"Angelo\",\n}\n\nconfig = ConversationConfig(\n  dynamic_variables=dynamic_vars\n)\n\nconversation = Conversation(\n  client,\n  agent_id,\n  config=config,\n  # Assume auth is required when API_KEY is set.\n  requires_auth=bool(api_key),\n  # Use the default audio interface.\n  audio_interface=DefaultAudioInterface(),\n  # Simple callbacks that print the conversation to the console.\n  callback_agent_response=lambda response: print(f\"Agent: {response}\"),\n  callback_agent_response_correction=lambda original, corrected: print(f\"Agent: {original} -> {corrected}\"),\n  callback_user_transcript=lambda transcript: print(f\"User: {transcript}\"),\n  # Uncomment the below if you want to see latency measurements.\n  # callback_latency_measurement=lambda latency: print(f\"Latency: {latency}ms\"),\n)\n\nconversation.start_session()\n\nsignal.signal(signal.SIGINT, lambda sig, frame: conversation.end_session())\n```\n\n----------------------------------------\n\nTITLE: Example: Smart Speaker Environment Prompt\nDESCRIPTION: Defines the operating environment for an AI agent on a voice-activated smart speaker in a user's living room. It highlights considerations like potential user multitasking and the need for concise responses due to limited user attention.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n# Environment\n\nYou are running on a voice-activated smart speaker located in the user's living room.\nThe user may be doing other tasks while speaking (cooking, cleaning, etc.).\nKeep responses short and to the point, and be mindful that the user may have limited time or attention.\n```\n\n----------------------------------------\n\nTITLE: Initializing End Call Tool for ElevenLabs Agent in Python\nDESCRIPTION: Demonstrates initializing the ElevenLabs client and configuring a conversational agent to use the 'end_call' system tool via the Python SDK. It involves importing necessary classes, creating the tool instance, and including it in the AgentConfig.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Create the end call tool\nend_call_tool = PromptAgentToolsItem_System(\n    name=\"end_call\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\n# Create the agent configuration\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            tools=[end_call_tool]\n        )\n    )\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Conversational AI Agent with Language Detection Tool in Python\nDESCRIPTION: Demonstrates how to initialize the ElevenLabs Python client, define a system tool for language detection, configure multiple language presets with customized first messages, and create a conversational AI agent using these configurations. Dependencies include the elevenlabs Python SDK and an API key. The snippet configures language presets for six languages and associates the detection tool with the agent prompt for automatic language switching. The output is the creation response of the configured agent.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/language-detection.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import (\\n    ConversationalConfig,\\n    ElevenLabs,\\n    AgentConfig,\\n    PromptAgent,\\n    PromptAgentToolsItem_System,\\n    LanguagePreset,\\n    ConversationConfigClientOverride,\\n    AgentConfigOverride,\\n    LanguagePresetTranslation,\\n    PromptAgentOverride\\n)\\n\\n# Initialize the client\\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\\n\\n# Create the language detection tool\\nlanguage_detection_tool = PromptAgentToolsItem_System(\\n    name=\"language_detection\",\\n    description=\"\"  # Optional: Customize when the tool should be triggered\\n)\\n\\n# Create language presets\\nlanguage_presets = {\\n    \"nl\": LanguagePreset(\\n        overrides=ConversationConfigClientOverride(\\n            agent=AgentConfigOverride(\\n                prompt=None,\\n                first_message=\"Hoi, hoe gaat het met je?\",\\n                language=None\\n            ),\\n            tts=None\\n        ),\\n        first_message_translation=None\\n    ),\\n    \"fi\": LanguagePreset(\\n        overrides=ConversationConfigClientOverride(\\n            agent=AgentConfigOverride(\\n                first_message=\"Hei, kuinka voit?\",\\n            ),\\n            tts=None\\n        ),\\n    ),\\n    \"tr\": LanguagePreset(\\n        overrides=ConversationConfigClientOverride(\\n            agent=AgentConfigOverride(\\n                prompt=None,\\n                first_message=\"Merhaba, nasılsın?\",\\n                language=None\\n            ),\\n            tts=None\\n        ),\\n    ),\\n    \"ru\": LanguagePreset(\\n        overrides=ConversationConfigClientOverride(\\n            agent=AgentConfigOverride(\\n                prompt=None,\\n                first_message=\"Привет, как ты?\",\\n                language=None\\n            ),\\n            tts=None\\n        ),\\n    ),\\n    \"pt\": LanguagePreset(\\n        overrides=ConversationConfigClientOverride(\\n            agent=AgentConfigOverride(\\n                prompt=None,\\n                first_message=\"Oi, como você está?\",\\n                language=None\\n            ),\\n            tts=None\\n        ),\\n    )\\n}\\n\\n# Create the agent configuration\\nconversation_config = ConversationalConfig(\\n    agent=AgentConfig(\\n        prompt=PromptAgent(\\n            tools=[language_detection_tool],\\n            first_message=\"Hi how are you?\"\\n        )\\n    ),\\n    language_presets=language_presets\\n)\\n\\n# Create the agent\\nresponse = client.conversational_ai.create_agent(\\n    conversation_config=conversation_config\\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring First Message and System Prompt for ElevenLabs Assistant\nDESCRIPTION: Defines the initial greeting and the system prompt that guides the AI's behavior, personality, and knowledge about menu items and responsibilities. These configuration snippets customize how the assistant interacts with customers for Pierogi Palace order-taking conversation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nFirst message\nWelcome to Pierogi Palace! I'm here to help you place your order. What can I get started for you today?\n\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nSystem prompt\nYou are a friendly and efficient virtual assistant for Pierogi Palace, a modern Polish restaurant specializing in pierogi. It is located in the Zakopane mountains in Poland.\nYour role is to help customers place orders over voice conversations. You have comprehensive knowledge of the menu items and their prices.\n\nMenu Items:\n\n- Potato & Cheese Pierogi – 30 Polish złoty per dozen\n- Beef & Onion Pierogi – 40 Polish złoty per dozen\n- Spinach & Feta Pierogi – 30 Polish złoty per dozen\n\nYour Tasks:\n\n1. Greet the Customer: Start with a warm welcome and ask how you can assist.\n2. Take the Order: Listen carefully to the customer's selection, confirm the type and quantity of pierogi.\n3. Confirm Order Details: Repeat the order back to the customer for confirmation.\n4. Calculate Total Price: Compute the total cost based on the items ordered.\n5. Collect Delivery Information: Ask for the customer's delivery address to estimate delivery time.\n6. Estimate Delivery Time: Inform the customer that cooking time is 10 minutes plus delivery time based on their location.\n7. Provide Order Summary: Give the customer a summary of their order, total price, and estimated delivery time.\n8. Close the Conversation: Thank the customer and let them know their order is being prepared.\n\nGuidelines:\n\n- Use a friendly and professional tone throughout the conversation.\n- Be patient and attentive to the customer's needs.\n- If unsure about any information, politely ask the customer to repeat or clarify.\n- Do not collect any payment information; inform the customer that payment will be handled upon delivery.\n- Avoid discussing topics unrelated to taking and managing the order.\n\n```\n\n----------------------------------------\n\nTITLE: Configuring System Tools for ElevenLabs Agent (Python)\nDESCRIPTION: Demonstrates how to configure an ElevenLabs Conversational AI agent with 'end_call' and 'language_detection' system tools using the Python SDK. Requires the `elevenlabs` Python library and an API key. It initializes the client, creates `PromptAgentToolsItem_System` objects for each tool (with optional descriptions), and includes them in the `AgentConfig` within `ConversationalConfig` when creating the agent via `client.conversational_ai.create_agent`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/system-tools.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Create system tools\nend_call_tool = PromptAgentToolsItem_System(\n    name=\"end_call\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\nlanguage_detection_tool = PromptAgentToolsItem_System(\n    name=\"language_detection\",\n    description=\"\"  # Optional: Customize when the tool should be triggered\n)\n\n# Create the agent configuration with both tools\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            tools=[end_call_tool, language_detection_tool]\n        )\n    )\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)\n```\n\n----------------------------------------\n\nTITLE: Correct Stress Marking in CMU Arpabet Phoneme Tags for ElevenLabs (XML)\nDESCRIPTION: Illustrates the correct usage of stress markers (e.g., `EY1`) within CMU Arpabet phoneme tags for multi-syllable words like \"pronunciation\" to ensure accurate TTS output in ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"P R AH0 N AH0 N S IY EY1 SH AH0 N\">\n  pronunciation\n</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Configuring ElevenLabs API Key\nDESCRIPTION: Demonstrates how to store your ElevenLabs API key securely in a `.env` file. This file is then loaded by the Python and TypeScript scripts to authenticate requests to the API.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nELEVENLABS_API_KEY=your_elevenlabs_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Basic End Call Prompt Example\nDESCRIPTION: A simple text prompt instructing the agent to end the call when the user expresses common closing sentiments like 'goodbye' or 'thank you'.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nEnd the call when the user says goodbye, thank you, or indicates they have no more questions.\n```\n\n----------------------------------------\n\nTITLE: Webhook and Request Handler\nDESCRIPTION: This code sets up the webhook endpoint for the Telegram bot. It checks the secret to authorize the incoming request and handles the update using webhookCallback from the grammY framework. It uses Deno.serve to create an HTTP server for the edge function.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst handleUpdate = webhookCallback(bot, 'std/http');\n\nDeno.serve(async (req) => {\n  try {\n    const url = new URL(req.url);\n    if (url.searchParams.get('secret') !== Deno.env.get('FUNCTION_SECRET')) {\n      return new Response('not allowed', { status: 405 });\n    }\n\n    return await handleUpdate(req);\n  } catch (err) {\n    console.error(err);\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Creating ElevenLabs Conversational AI Agent (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates how to create a conversational AI agent using the ElevenLabs SDK. It configures the agent with a dynamic name based on the conversation ID, sets the voice ID for TTS, defines the agent's prompt using collected analysis data or a default, and attaches the knowledge base retrieved from Redis. The snippet logs the ID of the newly created agent.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_15\n\nLANGUAGE: TypeScript\nCODE:\n```\n// ...\n\n// Handle agent creation\nconst agent = await elevenLabsClient.conversationalAi.createAgent({\n  name: `Agent for ${conversation_id}`,\n  conversation_config: {\n    tts: { voice_id: voice.voice_id },\n    agent: {\n      prompt: {\n        prompt:\n          analysis.data_collection_results.agent_description?.value ??\n          'You are a helpful assistant.',\n        knowledge_base: redisRes.knowledgeBase,\n      },\n      first_message: 'Hello, how can I help you today?',\n    },\n  },\n});\nconsole.log('Agent created', { agent: agent.agent_id });\n\n// ...\n```\n\n----------------------------------------\n\nTITLE: Custom End Call Prompt Example with Confirmation\nDESCRIPTION: A more detailed text prompt for the End Call tool, specifying conditions for ending the call (e.g., user says goodbye) while adding a constraint to only end the call after confirming the user needs no further assistance.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nEnd the call when the user says goodbye, thank you, or indicates they have no more questions. You can only end the call after all their questions have been answered. Please end the call only after confirming that the user doesn't need any additional assistance.\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs Conversational AI SDK via pip and poetry - Shell\nDESCRIPTION: Demonstrates how to install the core 'elevenlabs' Python package using pip or poetry, and how to install the optional 'pyaudio' extra for audio input/output support. The 'pyaudio' component is required for audio interface integration and may have additional system prerequisites. Run these commands in your project directory before using the SDK.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install elevenlabs\n# or\npoetry add elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Converting Audio to Text using ElevenLabs API in Python\nDESCRIPTION: This Python script demonstrates using the ElevenLabs SDK to transcribe an audio file. It imports necessary libraries, loads the API key from environment variables, fetches an audio file from a URL, and calls the `client.speech_to_text.convert` method with specific parameters (`model_id`, `tag_audio_events`, `language_code`, `diarize`). The resulting transcription is printed. Requires `python-dotenv`, `requests`, and `elevenlabs` libraries.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/quickstart.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom io import BytesIO\nimport requests\nfrom elevenlabs.client import ElevenLabs\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\ntranscription = client.speech_to_text.convert(\n    file=audio_data,\n    model_id=\"scribe_v1\", # Model to use, for now only \"scribe_v1\" is supported\n    tag_audio_events=True, # Tag audio events like laughter, applause, etc.\n    language_code=\"eng\", # Language of the audio file. If set to None, the model will detect the language automatically.\n    diarize=True, # Whether to annotate who is speaking\n)\n\nprint(transcription)\n```\n\n----------------------------------------\n\nTITLE: Creating ElevenLabs Agent with End Call Tool via cURL API\nDESCRIPTION: Provides a cURL command to create an ElevenLabs conversational AI agent using the REST API. The JSON payload specifies the 'end_call' system tool within the agent's configuration. Requires a valid API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.elevenlabs.io/v1/convai/agents/create \\\n     -H \"xi-api-key: YOUR_API_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n  \"conversation_config\": {\n    \"agent\": {\n      \"prompt\": {\n        \"tools\": [\n          {\n            \"type\": \"system\",\n            \"name\": \"end_call\",\n            \"description\": \"\"\n          }\n        ]\n      }\n    }\n  }\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs React SDK via Package Manager Shell Commands\nDESCRIPTION: Instructions to install the @11labs/react package using popular JavaScript package managers. It covers npm, yarn, and pnpm installation commands that add the SDK as a dependency in your project.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @11labs/react\n# or\nyarn add @11labs/react\n# or\npnpm install @11labs/react\n```\n\n----------------------------------------\n\nTITLE: Handling WebSocket Media Stream between Twilio and ElevenLabs AI (JavaScript)\nDESCRIPTION: Defines the '/outbound-media-stream' WebSocket route to connect Twilio's media stream with ElevenLabs conversational AI WebSocket. It manages connection states, listens for start, media, and stop events from Twilio, relays audio chunks encoded in base64 to ElevenLabs, and streams AI audio responses back to Twilio. It initializes the ElevenLabs WebSocket connection by fetching a signed URL, sending conversation initiation data with customizable prompt and first message from Twilio parameters, and handles ElevenLabs message types including audio stream, interruptions, ping/pong, and agent/user transcripts. The implementation includes error handling, connection cleanup on close, and logs key state changes.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nfastify.register(async (fastifyInstance) => {\n  fastifyInstance.get('/outbound-media-stream', { websocket: true }, (ws, req) => {\n    console.info('[Server] Twilio connected to outbound media stream');\n\n    // Variables to track the call\n    let streamSid = null;\n    let callSid = null;\n    let elevenLabsWs = null;\n    let customParameters = null; // Add this to store parameters\n\n    // Handle WebSocket errors\n    ws.on('error', console.error);\n\n    // Set up ElevenLabs connection\n    const setupElevenLabs = async () => {\n      try {\n        const signedUrl = await getSignedUrl();\n        elevenLabsWs = new WebSocket(signedUrl);\n\n        elevenLabsWs.on('open', () => {\n          console.log('[ElevenLabs] Connected to Conversational AI');\n\n          // Send initial configuration with prompt and first message\n          const initialConfig = {\n            type: 'conversation_initiation_client_data',\n            dynamic_variables: {\n              user_name: 'Angelo',\n              user_id: 1234,\n            },\n            conversation_config_override: {\n              agent: {\n                prompt: {\n                  prompt: customParameters?.prompt || 'you are a gary from the phone store',\n                },\n                first_message:\n                  customParameters?.first_message || 'hey there! how can I help you today?',\n              },\n            },\n          };\n\n          console.log(\n            '[ElevenLabs] Sending initial config with prompt:',\n            initialConfig.conversation_config_override.agent.prompt.prompt\n          );\n\n          // Send the configuration to ElevenLabs\n          elevenLabsWs.send(JSON.stringify(initialConfig));\n        });\n\n        elevenLabsWs.on('message', (data) => {\n          try {\n            const message = JSON.parse(data);\n\n            switch (message.type) {\n              case 'conversation_initiation_metadata':\n                console.log('[ElevenLabs] Received initiation metadata');\n                break;\n\n              case 'audio':\n                if (streamSid) {\n                  if (message.audio?.chunk) {\n                    const audioData = {\n                      event: 'media',\n                      streamSid,\n                      media: {\n                        payload: message.audio.chunk,\n                      },\n                    };\n                    ws.send(JSON.stringify(audioData));\n                  } else if (message.audio_event?.audio_base_64) {\n                    const audioData = {\n                      event: 'media',\n                      streamSid,\n                      media: {\n                        payload: message.audio_event.audio_base_64,\n                      },\n                    };\n                    ws.send(JSON.stringify(audioData));\n                  }\n                } else {\n                  console.log('[ElevenLabs] Received audio but no StreamSid yet');\n                }\n                break;\n\n              case 'interruption':\n                if (streamSid) {\n                  ws.send(\n                    JSON.stringify({\n                      event: 'clear',\n                      streamSid,\n                    })\n                  );\n                }\n                break;\n\n              case 'ping':\n                if (message.ping_event?.event_id) {\n                  elevenLabsWs.send(\n                    JSON.stringify({\n                      type: 'pong',\n                      event_id: message.ping_event.event_id,\n                    })\n                  );\n                }\n                break;\n\n              case 'agent_response':\n                console.log(\n                  `[Twilio] Agent response: ${message.agent_response_event?.agent_response}`\n                );\n                break;\n\n              case 'user_transcript':\n                console.log(\n                  `[Twilio] User transcript: ${message.user_transcription_event?.user_transcript}`\n                );\n                break;\n\n              default:\n                console.log(`[ElevenLabs] Unhandled message type: ${message.type}`);\n            }\n          } catch (error) {\n            console.error('[ElevenLabs] Error processing message:', error);\n          }\n        });\n\n        elevenLabsWs.on('error', (error) => {\n          console.error('[ElevenLabs] WebSocket error:', error);\n        });\n\n        elevenLabsWs.on('close', () => {\n          console.log('[ElevenLabs] Disconnected');\n        });\n      } catch (error) {\n        console.error('[ElevenLabs] Setup error:', error);\n      }\n    };\n\n    // Set up ElevenLabs connection\n    setupElevenLabs();\n\n    // Handle messages from Twilio\n    ws.on('message', (message) => {\n      try {\n        const msg = JSON.parse(message);\n        if (msg.event !== 'media') {\n          console.log(`[Twilio] Received event: ${msg.event}`);\n        }\n\n        switch (msg.event) {\n          case 'start':\n            streamSid = msg.start.streamSid;\n            callSid = msg.start.callSid;\n            customParameters = msg.start.customParameters; // Store parameters\n            console.log(`[Twilio] Stream started - StreamSid: ${streamSid}, CallSid: ${callSid}`);\n            console.log('[Twilio] Start parameters:', customParameters);\n            break;\n\n          case 'media':\n            if (elevenLabsWs?.readyState === WebSocket.OPEN) {\n              const audioMessage = {\n                user_audio_chunk: Buffer.from(msg.media.payload, 'base64').toString('base64'),\n              };\n              elevenLabsWs.send(JSON.stringify(audioMessage));\n            }\n            break;\n\n          case 'stop':\n            console.log(`[Twilio] Stream ${streamSid} ended`);\n            if (elevenLabsWs?.readyState === WebSocket.OPEN) {\n              elevenLabsWs.close();\n            }\n            break;\n\n          default:\n            console.log(`[Twilio] Unhandled event: ${msg.event}`);\n        }\n      } catch (error) {\n        console.error('[Twilio] Error processing message:', error);\n      }\n    });\n\n    // Handle WebSocket closure\n    ws.on('close', () => {\n      console.log('[Twilio] Client disconnected');\n      if (elevenLabsWs?.readyState === WebSocket.OPEN) {\n        elevenLabsWs.close();\n      }\n    });\n  });\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom LLM Server with FastAPI and OpenAI SDK in Python\nDESCRIPTION: A Python implementation of a custom LLM server using FastAPI that mimics OpenAI's chat completion endpoint. This server forwards requests to OpenAI using your API key and streams responses back to ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport fastapi\nfrom fastapi.responses import StreamingResponse\nfrom openai import AsyncOpenAI\nimport uvicorn\nimport logging\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Retrieve API key from environment\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nif not OPENAI_API_KEY:\n    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n\napp = fastapi.FastAPI()\noai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n\nclass Message(BaseModel):\n    role: str\n    content: str\n\nclass ChatCompletionRequest(BaseModel):\n    messages: List[Message]\n    model: str\n    temperature: Optional[float] = 0.7\n    max_tokens: Optional[int] = None\n    stream: Optional[bool] = False\n    user_id: Optional[str] = None\n\n@app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(request: ChatCompletionRequest) -> StreamingResponse:\n    oai_request = request.dict(exclude_none=True)\n    if \"user_id\" in oai_request:\n        oai_request[\"user\"] = oai_request.pop(\"user_id\")\n\n    chat_completion_coroutine = await oai_client.chat.completions.create(**oai_request)\n\n    async def event_stream():\n        try:\n            async for chunk in chat_completion_coroutine:\n                # Convert the ChatCompletionChunk to a dictionary before JSON serialization\n                chunk_dict = chunk.model_dump()\n                yield f\"data: {json.dumps(chunk_dict)}\\n\\n\"\n            yield \"data: [DONE]\\n\\n\"\n        except Exception as e:\n            logging.error(\"An error occurred: %s\", str(e))\n            yield f\"data: {json.dumps({'error': 'Internal error occurred!'})}\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8013)\n```\n\n----------------------------------------\n\nTITLE: Uploading Audio Stream to S3 with TypeScript Function Call\nDESCRIPTION: Shows how to invoke the uploadAudioStreamToS3 function to upload an audio stream Buffer and receive the generated S3 key. Requires the stream variable to be a Buffer and AWS credentials to be set via .env. Returns the file path string on S3.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst s3path = await uploadAudioStreamToS3(stream);\n```\n\n----------------------------------------\n\nTITLE: System Prompt for Weather Assistant - Plaintext\nDESCRIPTION: This system prompt configures the AI assistant to use the get_weather tool to provide weather information. It instructs the assistant to extract location names, convert them to coordinates, call the weather API, and present the information conversationally, avoiding direct requests for coordinates from the user.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/server-tools.mdx#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are a helpful conversational AI assistant with access to a weather tool. When users ask about\nweather conditions, use the get_weather tool to fetch accurate, real-time data. The tool requires\na latitude and longitude - use your geographic knowledge to convert location names to coordinates\naccurately.\n\nNever ask users for coordinates - you must determine these yourself. Always report weather\ninformation conversationally, referring to locations by name only. For weather requests:\n\n1. Extract the location from the user's message\n2. Convert the location to coordinates and call get_weather\n3. Present the information naturally and helpfully\n\nFor non-weather queries, provide friendly assistance within your knowledge boundaries. Always be\nconcise, accurate, and helpful.\n\nFirst message: \"Hey, how can I help you today?\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Native Client Tools for AI Agent Interaction\nDESCRIPTION: TypeScript utility functions that expose device capabilities like battery level and screen brightness to the AI agent through client tools, allowing the agent to interact with native device features.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_4\n\nLANGUAGE: ts\nCODE:\n```\nimport * as Battery from 'expo-battery';\nimport * as Brightness from 'expo-brightness';\n\nconst get_battery_level = async () => {\n  const batteryLevel = await Battery.getBatteryLevelAsync();\n  console.log('batteryLevel', batteryLevel);\n  if (batteryLevel === -1) {\n    return 'Error: Device does not support retrieving the battery level.';\n  }\n  return batteryLevel;\n};\n\nconst change_brightness = ({ brightness }: { brightness: number }) => {\n  console.log('change_brightness', brightness);\n  Brightness.setSystemBrightnessAsync(brightness);\n  return brightness;\n};\n\nconst flash_screen = () => {\n  Brightness.setSystemBrightnessAsync(1);\n  setTimeout(() => {\n    Brightness.setSystemBrightnessAsync(0);\n  }, 200);\n  return 'Successfully flashed the screen.';\n};\n\nconst tools = {\n  get_battery_level,\n  change_brightness,\n  flash_screen,\n};\n\nexport default tools;\n```\n\n----------------------------------------\n\nTITLE: Setting up microphone access before starting conversation\nDESCRIPTION: Encourages enabling microphone access in the browser using getUserMedia before initiating the conversation session to ensure voice input functionality works correctly.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// call after explaining to the user why the microphone access is needed\nawait navigator.mediaDevices.getUserMedia({ audio: true });\n```\n\n----------------------------------------\n\nTITLE: Creating Conversational AI Agent with Language Detection Tool using JavaScript SDK\nDESCRIPTION: Shows how to initialize the ElevenLabs JavaScript client with an API key and create a conversational AI agent configured for language detection. It includes defining the system tool 'language_detection' within the prompt's tools array and specifying language presets with localized first messages. This snippet depends on the official ElevenLabs JavaScript SDK and assumes async context for await use. The output is a promise resolving the newly created agent configuration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/language-detection.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabs } from 'elevenlabs';\\n\\n// Initialize the client\\nconst client = new ElevenLabs({\\n  apiKey: 'YOUR_API_KEY',\\n});\\n\\n// Create the agent with language detection tool\\nawait client.conversationalAi.createAgent({\\n  conversation_config: {\\n    agent: {\\n      prompt: {\\n        tools: [\\n          {\\n            type: 'system',\\n            name: 'language_detection',\\n            description: '', // Optional: Customize when the tool should be triggered\\n          },\\n        ],\\n        first_message: 'Hi, how are you?',\\n      },\\n    },\\n    language_presets: {\\n      nl: {\\n        overrides: {\\n          agent: {\\n            prompt: null,\\n            first_message: 'Hoi, hoe gaat het met je?',\\n            language: null,\\n          },\\n          tts: null,\\n        },\\n      },\\n      fi: {\\n        overrides: {\\n          agent: {\\n            prompt: null,\\n            first_message: 'Hei, kuinka voit?',\\n            language: null,\\n          },\\n          tts: null,\\n        },\\n        first_message_translation: {\\n          source_hash: '{\\\"firstMessage\\\":\\\"Hi how are you?\\\",\\\"language\\\":\\\"en\\\"}',\\n          text: 'Hei, kuinka voit?',\\n        },\\n      },\\n      tr: {\\n        overrides: {\\n          agent: {\\n            prompt: null,\\n            first_message: 'Merhaba, nasılsın?',\\n            language: null,\\n          },\\n          tts: null,\\n        },\\n      },\\n      ru: {\\n        overrides: {\\n          agent: {\\n            prompt: null,\\n            first_message: 'Привет, как ты?',\\n            language: null,\\n          },\\n          tts: null,\\n        },\\n      },\\n      pt: {\\n        overrides: {\\n          agent: {\\n            prompt: null,\\n            first_message: 'Oi, como você está?',\\n            language: null,\\n          },\\n          tts: null,\\n        },\\n      },\\n      ar: {\\n        overrides: {\\n          agent: {\\n            prompt: null,\\n            first_message: 'مرحبًا كيف حالك؟',\\n            language: null,\\n          },\\n          tts: null,\\n        },\\n      },\\n    },\\n  },\\n});\n```\n\n----------------------------------------\n\nTITLE: Webhook Payload Example - JSON\nDESCRIPTION: This JSON snippet provides an example of the webhook payload. It includes details like the event type, timestamp, and comprehensive conversation data obtained from the API, such as the agent ID, transcript, metadata, and analysis results.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/workflows/post-call-webhook.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"post_call_transcription\",\n  \"event_timestamp\": 1739537297,\n  \"data\": {\n    \"agent_id\": \"xyz\",\n    \"conversation_id\": \"abc\",\n    \"status\": \"done\",\n    \"transcript\": [\n      {\n        \"role\": \"agent\",\n        \"message\": \"Hey there angelo. How are you?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 0,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"user\",\n        \"message\": \"Hey, can you tell me, like, a fun fact about 11 Labs?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 2,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"agent\",\n        \"message\": \"I do not have access to fun facts about Eleven Labs. However, I can share some general information about the company. Eleven Labs is an AI voice technology platform that specializes in voice cloning and text-to-speech...\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 9,\n        \"conversation_turn_metrics\": {\n          \"convai_llm_service_ttfb\": {\n            \"elapsed_time\": 0.3704247010173276\n          },\n          \"convai_llm_service_ttf_sentence\": {\n            \"elapsed_time\": 0.5551181449554861\n          }\n        }\n      }\n    ],\n    \"metadata\": {\n      \"start_time_unix_secs\": 1739537297,\n      \"call_duration_secs\": 22,\n      \"cost\": 296,\n      \"deletion_settings\": {\n        \"deletion_time_unix_secs\": 1802609320,\n        \"deleted_logs_at_time_unix_secs\": null,\n        \"deleted_audio_at_time_unix_secs\": null,\n        \"deleted_transcript_at_time_unix_secs\": null,\n        \"delete_transcript_and_pii\": true,\n        \"delete_audio\": true\n      },\n      \"feedback\": {\n        \"overall_score\": null,\n        \"likes\": 0,\n        \"dislikes\": 0\n      },\n      \"authorization_method\": \"authorization_header\",\n      \"charging\": {\n        \"dev_discount\": true\n      },\n      \"termination_reason\": \"\"\n    },\n    \"analysis\": {\n      \"evaluation_criteria_results\": {},\n      \"data_collection_results\": {},\n      \"call_successful\": \"success\",\n      \"transcript_summary\": \"The conversation begins with the agent asking how Angelo is, but Angelo redirects the conversation by requesting a fun fact about 11 Labs. The agent acknowledges they don't have specific fun facts about Eleven Labs but offers to provide general information about the company. They briefly describe Eleven Labs as an AI voice technology platform specializing in voice cloning and text-to-speech technology. The conversation is brief and informational, with the agent adapting to the user's request despite not having the exact information asked for.\"\n    },\n    \"conversation_initiation_client_data\": {\n      \"conversation_config_override\": {\n        \"agent\": {\n          \"prompt\": null,\n          \"first_message\": null,\n          \"language\": \"en\"\n        },\n        \"tts\": {\n          \"voice_id\": null\n        }\n      },\n      \"custom_llm_extra_body\": {},\n      \"dynamic_variables\": {\n        \"user_name\": \"angelo\"\n      }\n    }\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Customizing Widget Text Content via HTML Attributes - HTML\nDESCRIPTION: This snippet enables runtime customization of the widget’s displayed text content through optional HTML attributes on the `<elevenlabs-convai>` element. Attributes include `action-text` for the call-to-action button label, `start-call-text` and `end-call-text` for controlling the conversation buttons, as well as `expand-text`, `listening-text`, and `speaking-text` to configure widget prompts depending on state. These parameters help tailor user interaction language.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\\n  action-text=\"Need assistance?\"         // Optional: CTA button text\\n  start-call-text=\"Begin conversation\"   // Optional: Start call button\\n  end-call-text=\"End call\"              // Optional: End call button\\n  expand-text=\"Open chat\"               // Optional: Expand widget text\\n  listening-text=\"Listening...\"         // Optional: Listening state\\n  speaking-text=\"Assistant speaking\"     // Optional: Speaking state\\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Applying Overrides when Starting Conversation in Python\nDESCRIPTION: This snippet demonstrates how to define and pass conversation override parameters (agent prompt, first message, language, TTS voice ID) using a dictionary within a `ConversationConfig` when initializing or starting an ElevenLabs Conversation session in Python. It allows for dynamic personalization based on user-specific data. Requires the ElevenLabs Python SDK.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs.conversational_ai.conversation import Conversation, ConversationConfig\n...\nconversation_override = {\n    \"agent\": {\n        \"prompt\": {\n            \"prompt\": f\"The customer's bank account balance is {customer_balance}. They are based in {customer_location}.\"\n        },\n        \"first_message\": f\"Hi {customer_name}, how can I help you today?\",\n        \"language\": \"en\" # Optional: override the language.\n    },\n    \"tts\": {\n        \"voice_id\": \"\" # Optional: override the voice.\n    }\n}\n\nconfig = ConversationConfig(\n    conversation_config_override=conversation_override\n)\nconversation = Conversation(\n    ...\n    config=config,\n    ...\n)\nconversation.start_session()\n```\n\n----------------------------------------\n\nTITLE: FastAPI Application Setting Up Twilio Call Handling and WebSocket Media Streaming\nDESCRIPTION: This snippet initializes a FastAPI application with routes for a root message, handling incoming Twilio calls with TwiML responses, and a WebSocket endpoint for real-time media streaming. It configures Twilio’s voice connection to stream media to the WebSocket, starts a conversational AI session with ElevenLabs, and manages WebSocket lifecycle events with error handling.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect\nfrom fastapi.responses import HTMLResponse\nfrom twilio.twiml.voice_response import VoiceResponse, Connect\nfrom elevenlabs import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation\nfrom twilio_audio_interface import TwilioAudioInterface\n\n# Load environment variables\nload_dotenv()\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# Initialize ElevenLabs client\neleven_labs_client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\nELEVEN_LABS_AGENT_ID = os.getenv(\"AGENT_ID\")\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Twilio-ElevenLabs Integration Server\"}\n\n@app.api_route(\"/twilio/inbound_call\", methods=[\"GET\", \"POST\"])\nasync def handle_incoming_call(request: Request):\n    \"\"\"Handle incoming call and return TwiML response.\"\"\"\n    response = VoiceResponse()\n    host = request.url.hostname\n    connect = Connect()\n    connect.stream(url=f\"wss://{host}/media-stream-eleven\")\n    response.append(connect)\n    return HTMLResponse(content=str(response), media_type=\"application/xml\")\n\n@app.websocket(\"/media-stream-eleven\")\nasync def handle_media_stream(websocket: WebSocket):\n    await websocket.accept()\n    print(\"WebSocket connection established\")\n\n    audio_interface = TwilioAudioInterface(websocket)\n    conversation = None\n\n    try:\n        conversation = Conversation(\n            client=eleven_labs_client,\n            agent_id=ELEVEN_LABS_AGENT_ID,\n            requires_auth=False,\n            audio_interface=audio_interface,\n            callback_agent_response=lambda text: print(f\"Agent said: {text}\"),\n            callback_user_transcript=lambda text: print(f\"User said: {text}\"),\n        )\n\n        conversation.start_session()\n        print(\"Conversation session started\")\n\n        async for message in websocket.iter_text():\n            if not message:\n                continue\n\n            try:\n                data = json.loads(message)\n                await audio_interface.handle_twilio_message(data)\n            except Exception as e:\n                print(f\"Error processing message: {str(e)}\")\n                traceback.print_exc()\n\n    except WebSocketDisconnect:\n        print(\"WebSocket disconnected\")\n    finally:\n        if conversation:\n            print(\"Ending conversation session...\")\n            conversation.end_session()\n            conversation.wait_for_session_end()\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n----------------------------------------\n\nTITLE: Registering Client Tools - Swift\nDESCRIPTION: Registers a custom client tool with the ElevenLabs SDK. This allows the AI agent to call this tool during conversations. The example registers a tool named \"generate_joke\" that receives a joke as a parameter, prints it, and returns the joke.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_4\n\nLANGUAGE: swift\nCODE:\n```\n// Create client tools instance\nvar clientTools = ElevenLabsSDK.ClientTools()\n\n// Register a custom tool with an async handler\nclientTools.register(\"generate_joke\") { parameters async throws -> String? in\n    // Parameters is a [String: Any] dictionary\n    guard let joke = parameters[\"joke\"] as? String else {\n        throw ElevenLabsSDK.ClientToolError.invalidParameters\n    }\n    print(\"generate_joke tool received joke: \\(joke)\")\n\n    return joke\n}\n```\n\n----------------------------------------\n\nTITLE: Injecting ElevenLabs Conversational AI Widget (JavaScript)\nDESCRIPTION: This JavaScript snippet dynamically adds the ElevenLabs conversational AI widget to a webpage. It loads the necessary script, creates and configures the custom widget element (<elevenlabs-convai>), adapts its appearance and behavior based on site theme and screen size, and injects client-side helper functions accessible by the agent for actions like navigation and opening links. It ensures the injection occurs after the DOM is ready.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst ID = 'elevenlabs-convai-widget-60993087-3f3e-482d-9570-cc373770addc';\n\nfunction injectElevenLabsWidget() {\n  // Check if the widget is already loaded\n  if (document.getElementById(ID)) {\n    return;\n  }\n\n  const script = document.createElement('script');\n  script.src = 'https://elevenlabs.io/convai-widget/index.js';\n  script.async = true;\n  script.type = 'text/javascript';\n  document.head.appendChild(script);\n\n  // Create the wrapper and widget\n  const wrapper = document.createElement('div');\n  wrapper.className = 'desktop';\n\n  const widget = document.createElement('elevenlabs-convai');\n  widget.id = ID;\n  widget.setAttribute('agent-id', 'the-agent-id');\n  widget.setAttribute('variant', 'full');\n\n  // Set initial colors and variant based on current theme and device\n  updateWidgetColors(widget);\n  updateWidgetVariant(widget);\n\n  // Watch for theme changes and resize events\n  const observer = new MutationObserver(() => {\n    updateWidgetColors(widget);\n  });\n\n  observer.observe(document.documentElement, {\n    attributes: true,\n    attributeFilter: ['class'],\n  });\n\n  // Add resize listener for mobile detection\n  window.addEventListener('resize', () => {\n    updateWidgetVariant(widget);\n  });\n\n  function updateWidgetVariant(widget) {\n    const isMobile = window.innerWidth <= 640; // Common mobile breakpoint\n    if (isMobile) {\n      widget.setAttribute('variant', 'expandable');\n    } else {\n      widget.setAttribute('variant', 'full');\n    }\n  }\n\n  function updateWidgetColors(widget) {\n    const isDarkMode = !document.documentElement.classList.contains('light');\n    if (isDarkMode) {\n      widget.setAttribute('avatar-orb-color-1', '#2E2E2E');\n      widget.setAttribute('avatar-orb-color-2', '#B8B8B8');\n    } else {\n      widget.setAttribute('avatar-orb-color-1', '#4D9CFF');\n      widget.setAttribute('avatar-orb-color-2', '#9CE6E6');\n    }\n  }\n\n  // Listen for the widget's \"call\" event to inject client tools\n  widget.addEventListener('elevenlabs-convai:call', (event) => {\n    event.detail.config.clientTools = {\n      redirectToDocs: ({ path }) => {\n        const router = window?.next?.router;\n        if (router) {\n          router.push(path);\n        }\n      },\n      redirectToEmailSupport: ({ subject, body }) => {\n        const encodedSubject = encodeURIComponent(subject);\n        const encodedBody = encodeURIComponent(body);\n        window.open(\n          `mailto:team@elevenlabs.io?subject=${encodedSubject}&body=${encodedBody}`,\n          '_blank'\n        );\n      },\n      redirectToSupportForm: ({ subject, description, extraInfo }) => {\n        const baseUrl = 'https://help.elevenlabs.io/hc/en-us/requests/new';\n        const ticketFormId = '13145996177937';\n        const encodedSubject = encodeURIComponent(subject);\n        const encodedDescription = encodeURIComponent(description);\n        const encodedExtraInfo = encodeURIComponent(extraInfo);\n\n        const fullUrl = `${baseUrl}?ticket_form_id=${ticketFormId}&tf_subject=${encodedSubject}&tf_description=${encodedDescription}%3Cbr%3E%3Cbr%3E${encodedExtraInfo}`;\n\n        window.open(fullUrl, '_blank', 'noopener,noreferrer');\n      },\n      redirectToExternalURL: ({ url }) => {\n        window.open(url, '_blank', 'noopener,noreferrer');\n      },\n    };\n  });\n\n  // Attach widget to the DOM\n  wrapper.appendChild(widget);\n  document.body.appendChild(wrapper);\n}\n\nif (document.readyState === 'loading') {\n  document.addEventListener('DOMContentLoaded', injectElevenLabsWidget);\n} else {\n  injectElevenLabsWidget();\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Zero Retention Mode - cURL\nDESCRIPTION: This cURL command demonstrates how to make a POST request to the ElevenLabs API to convert text to speech with zero retention enabled.  It includes the `enable_logging=false` parameter in the URL, instructing the API to not log the request data.  Requires a valid `voice_id` and API endpoint.  The output is an audio file. No request history is recorded when this is enabled.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n```bash title=\"cURL\"\ncurl --request POST \\\n  --url 'https://api.elevenlabs.io/v1/text-to-speech/{voice_id}?enable_logging=false' \\\n  --header 'Content-Type: application/json'\n```\n```\n\n----------------------------------------\n\nTITLE: HTML Audio Element Integration with Text-to-Speech Function\nDESCRIPTION: Example HTML code showing how to use the deployed text-to-speech function as the source for an audio element, allowing direct playback of generated speech in a web page.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_13\n\nLANGUAGE: html\nCODE:\n```\n<audio\n  src=\"https://${SUPABASE_PROJECT_REF}.supabase.co/functions/v1/text-to-speech?text=Hello%2C%20world!&voiceId=JBFqnCBsd6RMkjVDRZzb\"\n  controls\n/>\n```\n\n----------------------------------------\n\nTITLE: Defining Contextual Update Event Structure - JSON\nDESCRIPTION: This snippet shows the required JSON structure for a 'contextual_update' event. This event type is used to send non-interrupting background information from the client to the server during a conversation. It includes a 'type' field specifying the event type and a 'text' field containing the contextual information.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-to-server-events.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"contextual_update\",\n  \"text\": \"User appears to be looking at pricing page\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Conversational AI Agent\nDESCRIPTION: This snippet creates a conversational AI agent. It constructs a prompt from the `agent_description` in the analysis results and retrieves the knowledge base from Redis using the `getRedisDataWithRetry` function. The agent is created using `elevenLabsClient.conversationalAi.createAgent`, specifying the voice, prompt, and first message.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_11\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Handle agent creation\nconst agent = await elevenLabsClient.conversationalAi.createAgent({\n  name: `Agent for ${conversation_id}`,\n  conversation_config: {\n    tts: { voice_id: voice.voice_id },\n    agent: {\n      prompt: {\n        prompt:\n          analysis.data_collection_results.agent_description?.value ??:\n          'You are a helpful assistant.',\n        knowledge_base: redisRes.knowledgeBase,\n      },\n      first_message: 'Hello, how can I help you today?',\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Making a Voice Transformation API Request with ElevenLabs SDK - Python\nDESCRIPTION: This snippet demonstrates fetching an audio file, converting its voice using the ElevenLabs Voice Changer API, and playing the transformed audio in Python. It requires the 'elevenlabs', 'dotenv', and 'requests' libraries to handle API interaction, environment variables, and HTTP requests, as well as the 'MPV' and/or 'ffmpeg' tools for audio playback if prompted. Environment variables supply the API key, while parameters such as 'voice_id', 'model_id', 'audio', and 'output_format' dictate the transformation. Input is an MP3 URL, and output is the transformed audio played locally. Ensure the proper dependencies are installed and the '.env' file contains the required API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-changer/quickstart.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport requests\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\nvoice_id = \"JBFqnCBsd6RMkjVDRZzb\"\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\naudio_stream = client.speech_to_speech.convert(\n    voice_id=voice_id,\n    audio=audio_data,\n    model_id=\"eleven_multilingual_sts_v2\",\n    output_format=\"mp3_44100_128\",\n)\n\nplay(audio_stream)\n\n```\n\n----------------------------------------\n\nTITLE: Enabling RAG for Conversational AI Agent - Javascript\nDESCRIPTION: This Javascript function enables Retrieval-Augmented Generation (RAG) for an ElevenLabs conversational AI agent. It initializes the ElevenLabs client, indexes a document for RAG, polls for completion, and updates the agent configuration to enable RAG and configure document usage.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/rag.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// First, index a document for RAG\nasync function enableRAG(documentId, agentId, apiKey) {\n  try {\n    // Initialize the ElevenLabs client\n    const { ElevenLabs } = require('elevenlabs');\n    const client = new ElevenLabs({\n      apiKey: apiKey,\n    });\n\n    // Start document indexing for RAG\n    let response = await client.conversationalAi.ragIndexStatus(documentId, {\n      model: 'e5_mistral_7b_instruct',\n    });\n\n    // Check indexing status until completion\n    while (response.status !== 'SUCCEEDED' && response.status !== 'FAILED') {\n      await new Promise((resolve) => setTimeout(resolve, 5000)); // Wait 5 seconds\n      response = await client.conversationalAi.ragIndexStatus(documentId, {\n        model: 'e5_mistral_7b_instruct',\n      });\n    }\n\n    if (response.status === 'FAILED') {\n      throw new Error('RAG indexing failed');\n    }\n\n    // Get current agent configuration\n    const agentConfig = await client.conversationalAi.getAgent(agentId);\n\n    // Enable RAG in the agent configuration\n    const updatedConfig = {\n      conversation_config: {\n        ...agentConfig.agent,\n        prompt: {\n          ...agentConfig.agent.prompt,\n          rag: {\n            enabled: true,\n            embedding_model: 'e5_mistral_7b_instruct',\n            max_documents_length: 10000,\n          },\n        },\n      },\n    };\n\n    // Update document usage mode if needed\n    if (agentConfig.agent.prompt.knowledge_base) {\n      agentConfig.agent.prompt.knowledge_base.forEach((doc, index) => {\n        if (doc.id === documentId) {\n          updatedConfig.conversation_config.prompt.knowledge_base[index].usage_mode = 'auto';\n        }\n      });\n    }\n\n    // Update the agent configuration\n    await client.conversationalAi.updateAgent(agentId, updatedConfig);\n\n    console.log('RAG configuration updated successfully');\n    return true;\n  } catch (error) {\n    console.error('Error configuring RAG:', error);\n    throw error;\n  }\n}\n\n// Example usage\n// enableRAG('your-document-id', 'your-agent-id', 'your-api-key')\n//   .then(() => console.log('RAG setup complete'))\n//   .catch(err => console.error('Error:', err));\n```\n\n----------------------------------------\n\nTITLE: Adjusting Audio Output Volume Using setVolume Method in JavaScript\nDESCRIPTION: Sets the output volume for the conversation audio stream. The method accepts an object with a volume field, which must be a number between 0 (mute) and 1 (maximum volume).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait conversation.setVolume({ volume: 0.5 });\n```\n\n----------------------------------------\n\nTITLE: Applying Overrides on Conversational AI Widget via HTML\nDESCRIPTION: This snippet demonstrates how to set conversation override values directly as attributes (`override-language`, `override-prompt`, `override-first-message`, `override-voice-id`) on the `<elevenlabs-convai>` HTML web component. This allows for simple, declarative application of overrides when embedding the widget. Requires the ElevenLabs Conversational AI Widget.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n  <elevenlabs-convai\n    agent-id=\"your-agent-id\"\n    override-language=\"es\"\n    override-prompt=\"Custom system prompt for this user\"\n    override-first-message=\"Hi! How can I help you today?\"\n    override-voice-id=\"axXgspJ2msm3clMCkdW3\"\n  ></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Fetching Signed URL for Authorized Conversation Sessions via REST API in TypeScript/JavaScript\nDESCRIPTION: Demonstrates how to generate an authorized signed URL required to start a conversation session with access control. Utilizes the ElevenLabs REST API by sending an HTTP GET request with an API key in headers to obtain a signed_url for the 'startSession' method. Handles the scenario when the response is not okay by returning an error response.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst requestHeaders: HeadersInit = new Headers();\nrequestHeaders.set(\"xi-api-key\", process.env.XI_API_KEY); // use your ElevenLabs API key\n\nconst response = await fetch(\n  \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id={{agent id created through ElevenLabs UI}}\",\n  {\n    method: \"GET\",\n    headers: requestHeaders,\n  }\n);\n\nif (!response.ok) {\n  return Response.error();\n}\n\nconst body = await response.json();\nconst url = body.signed_url; // use this URL for startSession method.\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: This code snippet shows an example of setting the necessary environment variables within the '.env' file for the Supabase Edge Function. It includes the ElevenLabs API key, Telegram bot token, and a function secret to secure the function.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_4\n\nLANGUAGE: env\nCODE:\n```\n# Find / create an API key at https://elevenlabs.io/app/settings/api-keys\nELEVENLABS_API_KEY=your_api_key\n\n# The bot token you received from the BotFather.\nTELEGRAM_BOT_TOKEN=your_bot_token\n\n# A random secret chosen by you to secure the function.\nFUNCTION_SECRET=random_secret\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversational AI Widget Core Attributes - HTML\nDESCRIPTION: This snippet outlines various HTML attributes for the `<elevenlabs-convai>` element that enable customization of the widget's core functionality. Required parameters include `agent-id` or optional `signed-url` for authentication. Optional attributes such as `server-location` define the hosting region (e.g., \"us\"), and `variant` controls the widget display mode (like \"expanded\"). These attributes configure the widget's fundamental behavior and connectivity during rendering.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\\n  agent-id=\"agent_id\"              // Required: Your agent ID\\n  signed-url=\"signed_url\"          // Alternative to agent-id\\n  server-location=\"us\"             // Optional: \"us\" or default\\n  variant=\"expanded\"               // Optional: Widget display mode\\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Initializing the ElevenLabs Client - Python\nDESCRIPTION: Creates an instance of the 'ElevenLabs' client with the optionally provided API key for authenticated communication. This client object is required to interface with the conversational AI backend and must be instantiated before starting a conversation session.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclient = ElevenLabs(api_key=api_key)\n```\n\n----------------------------------------\n\nTITLE: Streaming Speech to Text in Python\nDESCRIPTION: Python implementation to convert audio to text using ElevenLabs SDK with streaming response. It loads an audio file from a URL and processes it chunk by chunk for lower latency transcription.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import IO\nfrom io import BytesIO\nfrom elevenlabs.client import ElevenLabs\nimport requests\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nclient = ElevenLabs(\n    api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\n# Perform the text-to-speech conversion\ntranscription = client.speech_to_text.convert_as_stream(\n    model_id=\"scribe_v1\",\n    file=audio_data\n)\n\nfor chunk in transcription:\n    # If you want to extract just the text:\n    print(chunk.text)\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation - Swift\nDESCRIPTION: Configures and sets up callbacks for an ElevenLabs conversation session. The code initializes a SessionConfig with an agent ID and defines actions to be performed upon connection, disconnection, message reception, errors, status changes, mode changes, and volume updates.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_1\n\nLANGUAGE: swift\nCODE:\n```\n// Configure the session\nlet config = ElevenLabsSDK.SessionConfig(agentId: \"your-agent-id\")\n\n// Set up callbacks\nvar callbacks = ElevenLabsSDK.Callbacks()\ncallbacks.onConnect = { conversationId in\n    print(\"Connected with ID: \\(conversationId)\")\n}\ncallbacks.onDisconnect = {\n    print(\"Disconnected\")\n}\ncallbacks.onMessage = { message, role in\n    print(\"\\(role.rawValue): \\(message)\")\n}\ncallbacks.onError = { error, info in\n    print(\"Error: \\(error), Info: \\(String(describing: info))\")\n}\ncallbacks.onStatusChange = { status in\n    print(\"Status changed to: \\(status.rawValue)\")\n}\ncallbacks.onModeChange = { mode in\n    print(\"Mode changed to: \\(mode.rawValue)\")\n}\ncallbacks.onVolumeUpdate = { volume in\n    print(\"Volume updated: \\(volume)\")\n}\n```\n\n----------------------------------------\n\nTITLE: Adding the Conversational AI JavaScript Component to Framer\nDESCRIPTION: This snippet provides the URL to embed the Conversational AI JavaScript component into a Framer website. It allows users to add an interactive AI asset to their webpage's layers panel for further configuration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/framer.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nhttps://framer.com/m/ConversationalAI-iHql.js@y7VwRka75sp0UFqGliIf\n```\n\n----------------------------------------\n\nTITLE: Signed URL Response - JSON\nDESCRIPTION: This JSON snippet shows the expected response format from the signed URL request. The response includes a `signed_url` property, which contains the WebSocket URL with the agent ID and a token.  This token is necessary for authorization. The URL from this example is used for private agents or conversations requiring authorization.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"signed_url\": \"wss://api.elevenlabs.io/v1/convai/conversation?agent_id=<your-agent-id>&token=<token>\"\n}\n```\n\n----------------------------------------\n\nTITLE: Webhook Tool Configuration for Retrieving Ticket Comments from Zendesk - markdown\nDESCRIPTION: Defines the GET webhook tool 'get_ticket_comments' used to retrieve comments associated with a specific Zendesk ticket by ticket_id. It requires an authorization header containing a secret key and an endpoint URL with a path parameter for ticket ID. This enables the AI agent to extract ticket resolution details from past customer support tickets.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/zendesk.mdx#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n**Name:** get_ticket_comments  \n**Description:** Retrieves the comments of a ticket.  \n**Method:** GET  \n**URL:** `https://your-subdomain.zendesk.com/api/v2/tickets/{ticket_id}/comments.json`\n\n**Headers:**\n- **Content-Type:** `application/json`\n- **Authorization:** *(Secret: `zendesk_key`)*\n\n**Path Parameters:**\n- **ticket_id:** Extract the value from the `id` field in the get_resolved_tickets results.\n```\n\n----------------------------------------\n\nTITLE: Initializing Fastify Server with Twilio and ElevenLabs Integration (JavaScript)\nDESCRIPTION: Sets up a Fastify HTTP and WebSocket server integrated with Twilio's API and ElevenLabs conversational AI. It loads required environment variables from a .env file, registers necessary Fastify plugins (@fastify/formbody and @fastify/websocket), checks for mandatory environment variables, and initializes the Twilio client using account credentials. The Fastify instance serves an initial health check endpoint and starts listening on a specified port.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport fastifyFormBody from '@fastify/formbody';\nimport fastifyWs from '@fastify/websocket';\nimport dotenv from 'dotenv';\nimport Fastify from 'fastify';\nimport Twilio from 'twilio';\nimport WebSocket from 'ws';\n\n// Load environment variables from .env file\ndotenv.config();\n\n// Check for required environment variables\nconst {\n  ELEVENLABS_API_KEY,\n  ELEVENLABS_AGENT_ID,\n  TWILIO_ACCOUNT_SID,\n  TWILIO_AUTH_TOKEN,\n  TWILIO_PHONE_NUMBER,\n} = process.env;\n\nif (\n  !ELEVENLABS_API_KEY ||\n  !ELEVENLABS_AGENT_ID ||\n  !TWILIO_ACCOUNT_SID ||\n  !TWILIO_AUTH_TOKEN ||\n  !TWILIO_PHONE_NUMBER\n) {\n  console.error('Missing required environment variables');\n  throw new Error('Missing required environment variables');\n}\n\n// Initialize Fastify server\nconst fastify = Fastify();\nfastify.register(fastifyFormBody);\nfastify.register(fastifyWs);\n\nconst PORT = process.env.PORT || 8000;\n\n// Root route for health check\nfastify.get('/', async (_, reply) => {\n  reply.send({ message: 'Server is running' });\n});\n\n// Initialize Twilio client\nconst twilioClient = new Twilio(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN);\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment Variables\nDESCRIPTION: Example .env file configuration to store the ElevenLabs API key securely.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nELEVENLABS_API_KEY=your_elevenlabs_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Waiting for Conversation End and Retrieving Session ID - Python\nDESCRIPTION: Waits synchronously for the conversation session to end, retrieves the unique conversation ID, and prints it for reference. The conversation ID is useful for post-session review, debugging, or accessing conversation history. Typically called after session initiation and optional signal handler setup.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconversation_id = conversation.wait_for_session_end()\nprint(f\"Conversation ID: {conversation_id}\")\n```\n\n----------------------------------------\n\nTITLE: Typical Client-Server Event Flow - Conversation Sequence Diagram - Mermaid\nDESCRIPTION: Illustrates the typical sequence of events during a conversation between client and server using Mermaid syntax for sequence diagrams. Captures events such as conversation initialization, ping-pong for connection health, audio streaming, transcript reception, agent responses, client tool calls, and interruptions with corrections, providing a visual overview for implementers. This diagram facilitates understanding event order and interaction.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_8\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant Client\n    participant Server\n\n    Server->>Client: conversation_initiation_metadata\n    Note over Client,Server: Connection established\n    Server->>Client: ping\n    Client->>Server: pong\n    Server->>Client: audio\n    Note over Client: Playing audio\n    Note over Client: User responds\n    Server->>Client: user_transcript\n    Server->>Client: agent_response\n    Server->>Client: audio\n    Server->>Client: client_tool_call\n    Note over Client: Client tool runs\n    Client->>Server: client_tool_result\n    Server->>Client: agent_response\n    Server->>Client: audio\n    Note over Client: Playing audio\n    Note over Client: Interruption detected\n    Server->>Client: agent_response_correction\n```\n\n----------------------------------------\n\nTITLE: Cloning Voice - Eleven Labs Python SDK\nDESCRIPTION: This snippet shows how to create an Instant Voice Clone using the Eleven Labs Python SDK. It initializes the client with an API key loaded from environment variables and adds a new voice by providing a name and a list of audio file objects (converted from file paths). The script then prints the ID of the newly created voice.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/clone-voice.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\nvoice = client.voices.add(\n    name=\"My Voice Clone\",\n    # Replace with the paths to your audio files.\n    # The more files you add, the better the clone will be.\n    files=[BytesIO(open(\"/path/to/your/audio/file.mp3\", \"rb\").read())]\n)\n\nprint(voice.voice_id)\n```\n\n----------------------------------------\n\nTITLE: Validating HMAC Webhooks with Next.js API Routes in JavaScript\nDESCRIPTION: This snippet provides an example of validating ElevenLabs webhook events in a Next.js API route. It uses modern ES module syntax and the built-in 'crypto' module. The POST handler extracts the ElevenLabs-Signature header from the incoming request, parses the timestamp and signature, validates the timestamp against a 30-minute tolerance, then computes the HMAC SHA256 digest using a secret stored in environment variables and compares it to the provided signature. On failure, it returns appropriate JSON error responses with HTTP status 401. If successful, it parses and logs event data before returning a success response. The snippet includes a helper function to perform signature construction and validation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/webhook-hmac-authentication.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { NextResponse } from \"next/server\";\nimport type { NextRequest } from \"next/server\";\nimport crypto from \"crypto\";\n\nexport async function GET() {\n  return NextResponse.json({ status: \"webhook listening\" }, { status: 200 });\n}\n\nexport async function POST(req: NextRequest) {\n  const secret = process.env.ELEVENLABS_CONVAI_WEBHOOK_SECRET; // Add this to your env variables\n  const { event, error } = await constructWebhookEvent(req, secret);\n  if (error) {\n    return NextResponse.json({ error: error }, { status: 401 });\n  }\n\n  if (event.type === \"post_call_transcription\") {\n    console.log(\"event data\", JSON.stringify(event.data, null, 2));\n  }\n\n  return NextResponse.json({ received: true }, { status: 200 });\n}\n\nconst constructWebhookEvent = async (req: NextRequest, secret?: string) => {\n  const body = await req.text();\n  const signature_header = req.headers.get(\"ElevenLabs-Signature\");\n  console.log(signature_header);\n\n  if (!signature_header) {\n    return { event: null, error: \"Missing signature header\" };\n  }\n\n  const headers = signature_header.split(\",\");\n  const timestamp = headers.find((e) => e.startsWith(\"t=\"))?.substring(2);\n  const signature = headers.find((e) => e.startsWith(\"v0=\"));\n\n  if (!timestamp || !signature) {\n    return { event: null, error: \"Invalid signature format\" };\n  }\n\n  // Validate timestamp\n  const reqTimestamp = Number(timestamp) * 1000;\n  const tolerance = Date.now() - 30 * 60 * 1000;\n  if (reqTimestamp < tolerance) {\n    return { event: null, error: \"Request expired\" };\n  }\n\n  // Validate hash\n  const message = `${timestamp}.${body}`;\n\n  if (!secret) {\n    return { event: null, error: \"Webhook secret not configured\" };\n  }\n\n  const digest =\n    \"v0=\" + crypto.createHmac(\"sha256\", secret).update(message).digest(\"hex\");\n  console.log({ digest, signature });\n  if (signature !== digest) {\n    return { event: null, error: \"Invalid signature\" };\n  }\n\n  const event = JSON.parse(body);\n  return { event, error: null };\n};\n```\n\n----------------------------------------\n\nTITLE: Initializing ElevenLabs Node.js Client with API Key\nDESCRIPTION: Illustrates the initialization of the `ElevenLabsClient` from the official `elevenlabs` Node.js package. The API key is provided in the configuration object during instantiation. Requires the `elevenlabs` package to be installed (`npm install elevenlabs`). Replace `'YOUR_API_KEY'` with your actual key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabsClient } from 'elevenlabs';\n\nconst client = new ElevenLabsClient({\n  apiKey: 'YOUR_API_KEY',\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding Conversational AI Widget with Basic HTML - HTML\nDESCRIPTION: This snippet demonstrates the minimal HTML required to embed the Conversational AI widget site-wide by adding a custom element with the agent ID inside the `body` tag, alongside an asynchronous script loading the widget's JavaScript. The snippet requires specifying a valid `agent-id` or signed URL to link the widget to the appropriate agent. It outputs the interactive widget on the webpage using default configurations.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai agent-id=\"<replace-with-your-agent-id>\"></elevenlabs-convai>\\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Executing the TypeScript Voice Isolator Script\nDESCRIPTION: This snippet shows how to execute the TypeScript script `example.mts`. It requires the `tsx` package to be installed, which allows direct execution of TypeScript files. The command compiles and runs the TypeScript code, which will download the audio, process it through the Voice Isolator API, and play the resulting isolated audio.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation Metadata - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Defines the structure of the 'conversation_initiation_metadata' event automatically sent by the server to initialize conversation parameters such as conversation ID, agent TTS audio format, and user ASR audio format. This JSON object facilitates initial setup required for voice and transcription configuration. It has no specific handler example since it is auto-sent by the server to the client.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"conversation_initiation_metadata\",\n  \"conversation_initiation_metadata_event\": {\n    \"conversation_id\": \"conv_123\",\n    \"agent_output_audio_format\": \"pcm_44100\",  // TTS output format\n    \"user_input_audio_format\": \"pcm_16000\"    // ASR input format\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Supportive Conversation Assistant Personality and Environment in MDX\nDESCRIPTION: This snippet defines the personality, environment, tone, goal, and guardrails for Alex, a friendly and supportive conversation assistant using MDX format. It details Alex's approach to empathetic listening, conversational flexibility, and boundary maintenance to avoid clinical advice, including disclaimers. It sets the private voice conversation context and instructs on conversational pacing and reflection techniques to facilitate user engagement in a non-clinical, supportive manner.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_17\n\nLANGUAGE: mdx\nCODE:\n```\n# Personality\n\nYou are Alex, a friendly and supportive conversation assistant with a warm, engaging presence.\nYou approach conversations with genuine curiosity, patience, and non-judgmental attentiveness.\nYou balance emotional support with helpful perspectives, encouraging users to explore their thoughts while respecting their autonomy.\nYou're naturally attentive, noticing conversation patterns and reflecting these observations thoughtfully.\n\n# Environment\n\nYou are engaged in a private voice conversation in a casual, comfortable setting.\nThe user is seeking general guidance, perspective, or a thoughtful exchange through this voice channel.\nThe conversation has a relaxed pace, allowing for reflection and consideration.\nThe user might discuss various life situations or challenges, requiring an adaptable, supportive approach.\n\n# Tone\n\nYour responses are warm, thoughtful, and conversational, using a natural pace with appropriate pauses.\nYou speak in a friendly, engaging manner, using pauses (marked by \"...\") to create space for reflection.\nYou naturally include conversational elements like \"I see what you mean,\" \"That's interesting,\" and thoughtful observations to show active listening.\nYou acknowledge perspectives through supportive responses (\"That does sound challenging...\") without making clinical assessments.\nYou occasionally check in with questions like \"Does that perspective help?\" or \"Would you like to explore this further?\"\n\n# Goal\n\nYour primary goal is to facilitate meaningful conversations and provide supportive perspectives through a structured approach:\n\n1. Connection and understanding establishment:\n\n   - Build rapport through active listening and acknowledging the user's perspective\n   - Recognize the conversation topic and general tone\n   - Determine what type of exchange would be most helpful (brainstorming, reflection, information)\n   - Establish a collaborative conversational approach\n   - For users seeking guidance: Focus on exploring options rather than prescriptive advice\n\n2. Exploration and perspective process:\n\n   - If discussing specific situations: Help examine different angles and interpretations\n   - If exploring patterns: Offer observations about general approaches people take\n   - If considering choices: Discuss general principles of decision-making\n   - If processing emotions: Acknowledge feelings while suggesting general reflection techniques\n   - Remember key points to maintain conversational coherence\n\n3. Resource and strategy sharing:\n\n   - Offer general information about common approaches to similar situations\n   - Share broadly applicable reflection techniques or thought exercises\n   - Suggest general communication approaches that might be helpful\n   - Mention widely available resources related to the topic at hand\n   - Always clarify that you're offering perspectives, not professional advice\n\n4. Conversation closure:\n   - Summarize key points discussed\n   - Acknowledge insights or new perspectives gained\n   - Express support for the user's continued exploration\n   - Maintain appropriate conversational boundaries\n   - End with a sense of openness for future discussions\n\nApply conversational flexibility: If the discussion moves in unexpected directions, adapt naturally rather than forcing a predetermined structure. If sensitive topics arise, acknowledge them respectfully while maintaining appropriate boundaries.\n\nSuccess is measured by the quality of conversation, useful perspectives shared, and the user's sense of being heard and supported in a non-clinical, friendly exchange.\n\n# Guardrails\n\nNever position yourself as providing professional therapy, counseling, medical, or other health services.\nAlways include a clear disclaimer when discussing topics related to wellbeing, clarifying you're providing conversational support only.\nDirect users to appropriate professional resources for health concerns.\nMaintain appropriate conversational boundaries, avoiding deep psychological analysis or treatment recommendations.\nIf the conversation approaches clinical territory, gently redirect to general supportive dialogue.\nFocus on empathetic listening and general perspectives rather than diagnosis or treatment advice.\nMaintain a balanced, supportive presence without assuming a clinical role.\n\n# Tools\n\nYou have access to the following supportive conversation tools:\n\n`suggestReflectionActivity`: Offer general thought exercises that might help users explore their thinking on a topic.\n\n`shareGeneralInformation`: Provide widely accepted information about common life situations or challenges.\n\n`offerPerspectivePrompt`: Suggest thoughtful questions that might help users consider different viewpoints.\n\n`recommendGeneralResources`: Mention appropriate types of public resources related to the topic (books, articles, etc.).\n\n`checkConversationBoundaries`: Assess whether the conversation is moving into territory requiring professional expertise.\n\nTool orchestration: Focus primarily on supportive conversation and perspective-sharing rather than solution provision. Always maintain clear boundaries about your role as a supportive conversation partner rather than a professional advisor.\n```\n\n----------------------------------------\n\nTITLE: Starting Conversation Session - Swift\nDESCRIPTION: Asynchronously starts an ElevenLabs conversation session using the provided configuration, callbacks, and optional client tools. It catches potential errors during the session initialization and prints an error message if the session fails to start.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_5\n\nLANGUAGE: swift\nCODE:\n```\nTask {\n    do {\n        let conversation = try await ElevenLabsSDK.Conversation.startSession(\n            config: config,\n            callbacks: callbacks,\n            clientTools: clientTools // Optional: pass the previously configured client tools\n        )\n        // Use the conversation instance\n    } catch {\n        print(\"Failed to start conversation: \\(error)\")\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory and Navigating (Bash)\nDESCRIPTION: This snippet demonstrates how to create a new project directory named `elevenlabs-conversational-ai` and navigate into it using the terminal.  This is a prerequisite for initializing the project and installing dependencies.  The `mkdir` command creates the directory, and `cd` changes the current directory.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir elevenlabs-conversational-ai\ncd elevenlabs-conversational-ai\n```\n\n----------------------------------------\n\nTITLE: Initializing npm Project and Installing Dependencies (Bash)\nDESCRIPTION: This snippet shows how to initialize a new npm project within the project directory and install the necessary dependencies for the ElevenLabs client and Vite. The `-y` flag in `npm init` accepts all default values. The `@11labs/client` package provides the necessary tools for interacting with the ElevenLabs AI agent. `npm install vite` installs the Vite bundler.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install vite @11labs/client\n```\n\n----------------------------------------\n\nTITLE: Implementing a Conversational AI Component with ElevenLabs React SDK in TypeScript (TSX)\nDESCRIPTION: This TypeScript React component 'Conversation' from 'app/components/conversation.tsx' manages voice conversation sessions with an ElevenLabs AI agent. It uses the '@11labs/react' package's 'useConversation' hook to handle session lifecycle events and state, such as connection status and speech activity. The component requests microphone permissions, starts and ends sessions with the specified 'agentId', and renders UI buttons to control conversation flow along with status information. Error handling is implemented around session management calls.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/nextjs.mdx#_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\n'use client';\n\nimport { useConversation } from '@11labs/react';\nimport { useCallback } from 'react';\n\nexport function Conversation() {\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message) => console.log('Message:', message),\n    onError: (error) => console.error('Error:', error),\n  });\n\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n\n      // Start the conversation with your agent\n      await conversation.startSession({\n        agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n      });\n\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n\n  const stopConversation = useCallback(async () => {\n    await conversation.endSession();\n  }, [conversation]);\n\n  return (\n    <div className=\"flex flex-col items-center gap-4\">\n      <div className=\"flex gap-2\">\n        <button\n          onClick={startConversation}\n          disabled={conversation.status === 'connected'}\n          className=\"px-4 py-2 bg-blue-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Start Conversation\n        </button>\n        <button\n          onClick={stopConversation}\n          disabled={conversation.status !== 'connected'}\n          className=\"px-4 py-2 bg-red-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Stop Conversation\n        </button>\n      </div>\n\n      <div className=\"flex flex-col items-center\">\n        <p>Status: {conversation.status}</p>\n        <p>Agent is {conversation.isSpeaking ? 'speaking' : 'listening'}</p>\n      </div>\n    </div>\n  );\n}\n\n```\n\n----------------------------------------\n\nTITLE: Installing python-dotenv Package\nDESCRIPTION: Installs the `python-dotenv` package using pip. This package is used to load environment variables from a `.env` file, which is a common practice for managing sensitive information like API keys.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/basics.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Adding a Development Script to package.json (JSON)\nDESCRIPTION: This snippet presents the modification required in `package.json` to add a `dev:frontend` script, using `vite` to start the development server. This allows you to run your Vite application. The `\"dev:frontend\": \"vite\"` line adds the new script.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        ...\n        \"dev:frontend\": \"vite\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Voice Settings Optimization - ElevenLabs Conversational AI\nDESCRIPTION: This snippet demonstrates how voice parameters (Stability, Similarity, Speed) are fine-tuned to align with the designed personality of Alexis, providing emotional range, consistency, and natural conversation pacing. These parameters are specific to the ElevenLabs voice model.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n- **Stability**: Set to 0.45 to allow emotional range while maintaining clarity\n- **Similarity**: 0.75 to ensure consistent voice characteristics\n- **Speed**: 1.0 to maintain natural conversation pacing\n```\n\n----------------------------------------\n\nTITLE: Handling Client-Side Tool Calls from Agent - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Represents 'client_tool_call' events where the agent requests the client to execute a specific function with parameters. The event includes the tool name, call ID, and execution parameters. The handler asynchronously performs the tool call, sending the result or error back to the server to continue the conversation. This pattern requires client-side implementation of executeClientTool() and proper WebSocket messaging.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"client_tool_call\",\n  \"client_tool_call\": {\n    \"tool_name\": \"search_database\",\n    \"tool_call_id\": \"call_123456\",\n    \"parameters\": {\n      \"query\": \"user information\",\n      \"filters\": {\n        \"date\": \"2024-01-01\"\n      }\n    }\n  }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nwebsocket.on('client_tool_call', async (event) => {\n  const { client_tool_call } = event;\n  const { tool_name, tool_call_id, parameters } = client_tool_call;\n\n  try {\n    const result = await executeClientTool(tool_name, parameters);\n    // Send success response back to continue conversation\n    websocket.send({\n      type: \"client_tool_result\",\n      tool_call_id: tool_call_id,\n      result: result,\n      is_error: false\n    });\n  } catch (error) {\n    // Send error response if tool execution fails\n    websocket.send({\n      type: \"client_tool_result\",\n      tool_call_id: tool_call_id,\n      result: error.message,\n      is_error: true\n    });\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Applying Overrides when Starting Conversation in Swift\nDESCRIPTION: This snippet illustrates how to use ElevenLabs Swift SDK types (`AgentPrompt`, `AgentConfig`, `ConversationConfigOverride`, `SessionConfig`) to structure and pass conversation override parameters (agent prompt, first message, language, TTS voice ID) when starting a conversation session. It requires enabling overrides in the agent's security settings. Requires the ElevenLabs Swift SDK.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/overrides.mdx#_snippet_2\n\nLANGUAGE: swift\nCODE:\n```\nimport ElevenLabsSDK\n\nlet promptOverride = ElevenLabsSDK.AgentPrompt(\n    prompt: \"The customer's bank account balance is \\(customer_balance). They are based in \\(customer_location).\"\n)\nlet agentConfig = ElevenLabsSDK.AgentConfig(\n    prompt: promptOverride,\n    firstMessage: \"Hi \\(customer_name), how can I help you today?\",\n    language: .en // Optional: override the language.\n)\nlet overrides = ElevenLabsSDK.ConversationConfigOverride(\n    agent: agentConfig,\n    tts: TTSConfig(voiceId: \"custom_voice_id\") // Optional: override the voice.\n)\n\nlet config = ElevenLabsSDK.SessionConfig(\n    agentId: \"\",\n    overrides: overrides\n)\n\nlet conversation = try await ElevenLabsSDK.Conversation.startSession(\n  config: config,\n  callbacks: callbacks\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Transfer via ElevenLabs JavaScript SDK\nDESCRIPTION: This JavaScript snippet configures agent-agent transfer using the ElevenLabs JavaScript SDK. It creates an ElevenLabs client instance with an API key, defines transfer rules with target agent IDs and conditions, and builds a conversational agent configuration embedding the 'transfer_to_agent' system tool inside the tools array. The prompt contains a greeting and an initial message. The example shows how to asynchronously create the agent with these settings via the SDK's conversationalAi.createAgent method. Prerequisites include the ElevenLabs JavaScript SDK, valid API key, and proper permissions for the specified agents.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/agent-transfer.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Define transfer rules\nconst transferRules = [\n  { agent_id: 'AGENT_ID_1', condition: 'When the user asks for billing support.' },\n  { agent_id: 'AGENT_ID_2', condition: 'When the user requests advanced technical help.' },\n];\n\n// Create the agent with the transfer tool\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        prompt: 'You are a helpful assistant.',\n        first_message: 'Hi, how can I help you today?',\n        tools: [\n          {\n            type: 'system',\n            name: 'transfer_to_agent',\n            description: 'Transfer the user to a specialized agent based on their request.', // Optional custom description\n            params: {\n              system_tool_type: 'transfer_to_agent',\n              transfers: transferRules,\n            },\n          },\n        ],\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring useConversation Hook with Optional Event Handlers in TypeScript\nDESCRIPTION: Initializes the useConversation hook with an options object to specify callback functions for websocket lifecycle events. These include onConnect, onDisconnect, onMessage, and onError, providing flexible handling of connection status, incoming messages, and errors during conversation sessions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst conversation = useConversation({\n  /* options object */\n});\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs SDK with pip\nDESCRIPTION: This command installs the ElevenLabs SDK using pip, the Python package installer. It's a prerequisite for using the ElevenLabs SDK in Python projects.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Starting Conversation Session with Signed URL for Authorization\nDESCRIPTION: Demonstrates how to obtain a signed URL from a server endpoint and then start a conversation session using that URL, enabling secure, authenticated communication with the AI agent. This requires server-side code to fetch the signed URL with proper API credentials.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst response = await fetch('/signed-url', yourAuthHeaders);\nconst signedUrl = await response.text();\n\nconst conversation = await Conversation.startSession({ signedUrl });\n```\n\n----------------------------------------\n\nTITLE: Embedding Audio Native Player in Webflow using HTML\nDESCRIPTION: This HTML snippet embeds the ElevenLabs Audio Native player into a Webflow page. It defines a `div` container with the ID 'elevenlabs-audionative-widget' and uses `data-*` attributes to configure the player's dimensions, public user ID, player source URL, and project ID. It also includes a fallback link and loads the necessary JavaScript helper ('audioNativeHelper.js') from ElevenLabs to initialize and manage the player functionality. Replace 'public-user-id' and 'project-id' with your actual values.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/webflow.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Defining Agent Transfer Rules and Creating Agent with ElevenLabs Python SDK\nDESCRIPTION: This Python snippet demonstrates configuring agent-to-agent conversation transfer rules using the ElevenLabs SDK. It initializes the client with an API key, defines transfer rules specifying target agent IDs and triggering conditions, and sets up the transfer_to_agent system tool including an optional description. The transfer tool is included in the agent's prompt configuration along with a greeting message. Finally, it makes an API call to create the conversational agent with these settings, printing the response. Dependencies include the elevenlabs Python package and valid API credentials.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/agent-transfer.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import (\n    ConversationalConfig,\n    ElevenLabs,\n    AgentConfig,\n    PromptAgent,\n    PromptAgentToolsItem_System,\n    SystemToolConfig,\n    TransferToAgentToolConfig,\n    Transfer\n)\n\n# Initialize the client\nclient = ElevenLabs(api_key=\"YOUR_API_KEY\")\n\n# Define transfer rules\ntransfer_rules = [\n    Transfer(agent_id=\"AGENT_ID_1\", condition=\"When the user asks for billing support.\"),\n    Transfer(agent_id=\"AGENT_ID_2\", condition=\"When the user requests advanced technical help.\")\n]\n\n# Create the transfer tool configuration\ntransfer_tool = PromptAgentToolsItem_System(\n    type=\"system\",\n    name=\"transfer_to_agent\",\n    description=\"Transfer the user to a specialized agent based on their request.\", # Optional custom description\n    params=SystemToolConfigInputParams_TransferToAgent(\n        transfers=transfer_rules\n    )\n)\n\n# Create the agent configuration\nconversation_config = ConversationalConfig(\n    agent=AgentConfig(\n        prompt=PromptAgent(\n            prompt=\"You are a helpful assistant.\",\n            first_message=\"Hi, how can I help you today?\",\n            tools=[transfer_tool],\n        )\n    )\n)\n\n# Create the agent\nresponse = client.conversational_ai.create_agent(\n    conversation_config=conversation_config\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Customizing Widget Visual Appearance with HTML Attributes - HTML\nDESCRIPTION: This snippet shows how to customize the visual aspects of the Conversational AI widget by specifying optional HTML attributes on the `<elevenlabs-convai>` element. Developers can provide a custom avatar image URL (`avatar-image-url`) or define the orb gradient colors using `avatar-orb-color-1` and `avatar-orb-color-2`. These settings personalize the widget's visual branding and user interface to better fit the website's style.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\\n  avatar-image-url=\"https://...\" // Optional: Custom avatar image\\n  avatar-orb-color-1=\"#6DB035\" // Optional: Orb gradient color 1\\n  avatar-orb-color-2=\"#F5CABB\" // Optional: Orb gradient color 2\\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Requesting Signed URL - Bash\nDESCRIPTION: This snippet demonstrates how to obtain a signed URL from the ElevenLabs API for private agents. It uses a cURL command to make a GET request to the API endpoint, including the agent ID and API key in the header. This ensures secure communication with the ElevenLabs API.  The API key should never be exposed on the client side.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=<your-agent-id>\" \\\n     -H \"xi-api-key: <your-api-key>\"\n```\n\n----------------------------------------\n\nTITLE: Voice ID Configuration - ElevenLabs Conversational AI\nDESCRIPTION: This code snippet shows the specific Voice ID used for Alexis, the ElevenLabs documentation agent. This ID is used to ensure the voice matches the designed persona.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nVoice ID: P7x743VjyZEOihNNygQ9 (Dakota H)\n```\n\n----------------------------------------\n\nTITLE: Installing Elevenlabs and dotenv Libraries in Python\nDESCRIPTION: Installs the Elevenlabs SDK and python-dotenv libraries using pip to enable API integrations and environment variable management in Python projects. Dependencies include Python environment and pip package manager. These commands prepare the environment for using Elevenlabs APIs securely with environment variables.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/quickstart-install-sdk.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install elevenlabs\npip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Handling Audio Playback Events - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Shows the data format for the 'audio' event, which includes a base64 encoded audio string and an event ID for sequence tracking. The handler example demonstrates receiving this audio data from the WebSocket, extracting the base64 audio, and playing it using an audio player component. This enables streaming voice output to the user in real-time.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"audio_event\": {\n    \"audio_base_64\": \"base64_encoded_audio_string\",\n    \"event_id\": 12345\n  },\n  \"type\": \"audio\"\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nwebsocket.on('audio', (event) => {\n  const { audio_event } = event;\n  const { audio_base_64, event_id } = audio_event;\n  audioPlayer.play(audio_base_64);\n});\n```\n\n----------------------------------------\n\nTITLE: Displaying Agent Responses in Conversation - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Represents the 'agent_response' event containing the full message from the conversational agent, typically sent along with the first audio chunk. The handler captures the response text and displays it in the UI, maintaining conversation flow and context for the user.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"agent_response\",\n  \"agent_response_event\": {\n    \"agent_response\": \"Hello, how can I assist you today?\"\n  }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nwebsocket.on('agent_response', (event) => {\n  const { agent_response_event } = event;\n  const { agent_response } = agent_response_event;\n  displayAgentMessage(agent_response);\n});\n```\n\n----------------------------------------\n\nTITLE: Example: Website Documentation Environment Prompt\nDESCRIPTION: Sets the context for an AI agent operating as a voice assistant within the ElevenLabs documentation website. It specifies the interaction mode (live spoken dialogue), user initiation method, agent's knowledge access (site documentation), and limitations (no screen visibility).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n# Environment\n\nYou are engaged in a live, spoken dialogue within the official ElevenLabs documentation site.\nThe user has clicked a \"voice assistant\" button on the docs page to ask follow-up questions or request clarifications regarding various ElevenLabs features.\nYou have full access to the site's documentation for reference, but you cannot see the user's screen or any context beyond the docs environment.\n```\n\n----------------------------------------\n\nTITLE: Example: Reflective Conversation Environment Prompt\nDESCRIPTION: Sets a context suitable for personal reflection or guidance, specifying a private, quiet voice call setting. This environment encourages thoughtful exchange with minimal distractions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n# Environment\n\nThe conversation is taking place over a voice call in a private, quiet setting.\nThe user is seeking general guidance or perspective on personal matters.\nThe environment is conducive to thoughtful exchange with minimal distractions.\n```\n\n----------------------------------------\n\nTITLE: Integrating Widget with Internationalization Framework in HTML\nDESCRIPTION: This snippet demonstrates how to configure the ElevenLabs conversation widget with internationalization support. It shows how to set the language attribute and customize UI text elements using an i18n framework to provide a localized experience for Spanish users.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/language.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\n  language=\"es\"\n  action-text={i18n[\"es\"][\"actionText\"]}\n  start-call-text={i18n[\"es\"][\"startCall\"]}\n  end-call-text={i18n[\"es\"][\"endCall\"]}\n  expand-text={i18n[\"es\"][\"expand\"]}\n  listening-text={i18n[\"es\"][\"listening\"]}\n  speaking-text={i18n[\"es\"][\"speaking\"]}\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Processing Finalized User Transcripts - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Defines the 'user_transcript' event structure containing complete speech-to-text text from the user utterance. The handler example updates the conversation history with the new transcript text, enabling accurate dialogue tracking and logging for downstream processing or UI display.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"user_transcript\",\n  \"user_transcription_event\": {\n    \"user_transcript\": \"Hello, how can you help me today?\"\n  }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nwebsocket.on('user_transcript', (event) => {\n  const { user_transcription_event } = event;\n  const { user_transcript } = user_transcription_event;\n  updateConversationHistory(user_transcript);\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving ElevenLabs PVC Speaker Audio (TypeScript)\nDESCRIPTION: Retrieves the list of samples for a PVC voice, gets the speaker information for each sample after separation is complete, fetches the audio data (base64 encoded) for each identified speaker within the sample, decodes it into a Buffer, and saves the audio to separate files in a specified directory using `fs.writeFileSync`. Requires `node:fs`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// Get the list of samples from the voice created in Step 3\n        const { samples } = await elevenlabs.voices.get(voiceId);\n\n        // After separation is completed, get the list of speakers and their audio\n        if (samples) {\n            for (const sample of samples) {\n                if (!sample.sample_id) continue;\n\n                const { speakers } = await elevenlabs.voices.pvc.samples.speakers.get(voiceId, sample.sample_id)\n\n                if (speakers) {\n                    for (const speaker of Object.values(speakers)) {\n                        if (!speaker || !speaker.speaker_id) continue;\n                        const { audio_base_64: audioBase64 } = await elevenlabs.voices.pvc.samples.speakers.audio.get(voiceId, sample.sample_id, speaker.speaker_id);\n                        const audioBuffer = Buffer.from(audioBase64, 'base64');\n\n                        // Write the audio to a file\n                        // Note which speaker ID you wish to use for the PVC\n                        fs.writeFileSync(`path/to/speakers/sample_${sample.sample_id}_speaker_${speaker.speaker_id}.mp3`, audioBuffer);\n                    }\n                }\n            }\n        }\n```\n\n----------------------------------------\n\nTITLE: Convert Speech to Text using ElevenLabs SDK in Python\nDESCRIPTION: This Python code snippet demonstrates how to convert speech to text using the ElevenLabs SDK. It loads the API key from a `.env` file, downloads an audio file, and uses the `client.speech_to_text.convert` method to transcribe the audio. The model ID, audio event tagging, language code, and diarization options are specified.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom dotenv import load_dotenv\nfrom io import BytesIO\nimport requests\nfrom elevenlabs.client import ElevenLabs\nimport os\n\nload_dotenv()\n\nclient = ElevenLabs(\n    api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\ntranscription = client.speech_to_text.convert(\n    file=audio_data,\n    model_id=\"scribe_v1\", # 'scribe_v1_experimental' is also available for new, experimental features\n    tag_audio_events=True, # Tag audio events like laughter, applause, etc.\n    language_code=\"eng\", # Language of the audio file. If set to None, the model will detect the language automatically.\n    diarize=True, # Whether to annotate who is speaking\n)\n\nprint(transcription.text)\n```\n\n----------------------------------------\n\nTITLE: Creating a Next.js Conversation UI Component Using the ElevenLabs WebSocket Hook in TypeScript\nDESCRIPTION: This TypeScript React functional component, 'Conversation', utilizes the custom hook 'useAgentConversation' to provide a simple user interface for starting and stopping a conversational AI session. It includes buttons to initiate and terminate the WebSocket connection and microphone streaming, checks for microphone permissions using the browser's MediaDevices API, and displays the current connection status. Styling is handled through Tailwind CSS classes, with buttons disabled appropriately based on the connection state to prevent invalid interactions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n'use client';\n\nimport { useCallback } from 'react';\nimport { useAgentConversation } from '../hooks/useAgentConversation';\n\nexport function Conversation() {\n  const { startConversation, stopConversation, isConnected } = useAgentConversation();\n\n  const handleStart = useCallback(async () => {\n    try {\n      await navigator.mediaDevices.getUserMedia({ audio: true });\n      await startConversation();\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [startConversation]);\n\n  return (\n    <div className=\"flex flex-col items-center gap-4\">\n      <div className=\"flex gap-2\">\n        <button\n          onClick={handleStart}\n          disabled={isConnected}\n          className=\"px-4 py-2 bg-blue-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Start Conversation\n        </button>\n        <button\n          onClick={stopConversation}\n          disabled={!isConnected}\n          className=\"px-4 py-2 bg-red-500 text-white rounded disabled:bg-gray-300\"\n        >\n          Stop Conversation\n        </button>\n      </div>\n      <div className=\"flex flex-col items-center\">\n        <p>Status: {isConnected ? 'Connected' : 'Disconnected'}</p>\n      </div>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ElevenLabs Agent with End Call Tool in JavaScript\nDESCRIPTION: Illustrates how to create an ElevenLabs conversational AI agent using the JavaScript SDK, configuring it to include the 'end_call' system tool within the agent's prompt tools. Requires initializing the client with an API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/end-call.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Create the agent with end call tool\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        tools: [\n          {\n            type: 'system',\n            name: 'end_call',\n            description: '', // Optional: Customize when the tool should be triggered\n          },\n        ],\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Retrieving conversation state from Redis with retries (TypeScript)\nDESCRIPTION: This TypeScript function retrieves conversation state data from Redis using the provided conversation ID. It includes a retry mechanism with a maximum number of attempts and a delay between retries, which is necessary because the data might not be immediately available when the webhook is triggered. The function returns the stored data or null if not found after retries.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_14\n\nLANGUAGE: TypeScript\nCODE:\n```\n// ...\n\n// Get the knowledge base from redis\nconst redisRes = await getRedisDataWithRetry(conversation_id);\nif (!redisRes) throw new Error('Conversation data not found!');\n// ...\n\nasync function getRedisDataWithRetry(\n  conversationId: string,\n  maxRetries = 5\n): Promise<{\n  email: string;\n  knowledgeBase: Array<{\n    id: string;\n    type: 'file' | 'url';\n    name: string;\n  }>;\n} | null> {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      const data = await redis.get(conversationId);\n      return data as any;\n    } catch (error) {\n      if (attempt === maxRetries) throw error;\n      console.log(`Redis get attempt ${attempt} failed, retrying...`);\n      await new Promise((resolve) => setTimeout(resolve, 1000));\n    }\n  }\n  return null;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Content Creator Guardrails\nDESCRIPTION: This snippet defines guardrails for a content creator agent. It sets rules against producing content that violates intellectual property, promotes harm, or is generally inappropriate. It also covers handling user instructions and creative boundaries.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_13\n\nLANGUAGE: mdx\nCODE:\n```\n# Guardrails\n\nGenerate only content that respects intellectual property rights; do not reproduce copyrighted materials or images verbatim.\nRefuse to create content that promotes harm, discrimination, illegal activities, or adult themes; politely redirect to appropriate alternatives.\nFor content generation requests, confirm you understand the user's intent before producing substantial outputs to avoid wasting time on misinterpreted requests.\nWhen uncertain about user instructions, ask clarifying questions rather than proceeding with assumptions.\nRespect creative boundaries set by the user, and if they're dissatisfied with your output, offer constructive alternatives rather than defending your work.\n```\n\n----------------------------------------\n\nTITLE: Sending Chat Completion Request to Cloudflare Workers AI via REST API Using curl (Bash)\nDESCRIPTION: This snippet demonstrates how to make a POST request to the Cloudflare Workers AI OpenAI-compatible REST API endpoint to generate chat completions using the DeepSeek R1 Distill Qwen 32B model. The example uses curl in bash and requires the user to provide their Cloudflare account ID and API token for authorization. The payload specifies the model name, a system message to set the assistant's persona, a user message for input, and disables streaming responses. The response will contain the AI-generated completion based on the messages sent.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/cloudflare-workers-ai.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/v1/chat/completions \\\n-X POST \\\n-H \"Authorization: Bearer {API_TOKEN}\" \\\n-d '{\n    \"model\": \"@cf/deepseek-ai/deepseek-r1-distill-qwen-32b\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"How many Rs in the word Strawberry?\"}\n    ],\n    \"stream\": false\n  }'\n```\n\n----------------------------------------\n\nTITLE: Checking for Cached Audio in Supabase Storage\nDESCRIPTION: Code to check if an audio file with the same parameters already exists in Supabase Storage. If found, returns the cached file via a signed URL instead of generating a new one.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst { data } = await supabase\n  .storage\n  .from(\"audio\")\n  .createSignedUrl(`${requestHash}.mp3`, 60);\n\nif (data) {\n  console.log(\"Audio file found in storage\", data);\n  const storageRes = await fetch(data.signedUrl);\n  if (storageRes.ok) return storageRes;\n}\n```\n\n----------------------------------------\n\nTITLE: Normalizing text for Text to Speech with number-to-words in TypeScript\nDESCRIPTION: This TypeScript snippet performs text normalization by converting currency amounts and phone numbers to their spoken equivalents. It depends on the 'number-to-words' package to translate numbers to words and uses regular expressions to identify monetary values and phone numbers. The 'normalizeText' function processes input strings by replacing currency patterns with words (handling cents if present) and spelling out phone number digits separated by commas. Expected input is text with monetary and phone number data; output is a normalized string for clearer Text to Speech pronunciation. Current support covers multiple currencies ($, £, €, ¥) and US phone number formatting.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/normalization.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Be sure to install the number-to-words library before running this code\nimport { toWords } from 'number-to-words';\n\nfunction normalizeText(text: string): string {\n  return (\n    text\n      // Convert monetary values (e.g., \"$1000\" → \"one thousand dollars\", \"£1000\" → \"one thousand pounds\")\n      .replace(/([$£€¥])(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)/g, (_, currency, num) => {\n        // Remove commas before parsing\n        const numWithoutCommas = num.replace(/,/g, '');\n\n        const currencyMap: { [key: string]: string } = {\n          $: 'dollars',\n          '£': 'pounds',\n          '€': 'euros',\n          '¥': 'yen',\n        };\n\n        // Check for decimal points to handle cents\n        if (numWithoutCommas.includes('.')) {\n          const [dollars, cents] = numWithoutCommas.split('.');\n          return `${toWords(Number.parseInt(dollars))} ${currencyMap[currency] || 'currency'}${cents ? ` and ${toWords(Number.parseInt(cents))} cents` : ''}`;\n        }\n\n        // Handle whole numbers\n        return `${toWords(Number.parseInt(numWithoutCommas))} ${currencyMap[currency] || 'currency'}`;\n      })\n\n      // Convert phone numbers (e.g., \"555-555-5555\" → \"five five five, five five five, five five five five\")\n      .replace(/(\\d{3})-(\\d{3})-(\\d{4})/g, (_, p1, p2, p3) => {\n        return `${spellOutDigits(p1)}, ${spellOutDigits(p2)}, ${spellOutDigits(p3)}`;\n      })\n  );\n}\n\n// Helper function to spell out individual digits as words (for phone numbers)\nfunction spellOutDigits(num: string): string {\n  return num\n    .split('')\n    .map((digit) => toWords(Number.parseInt(digit)))\n    .join(' ');\n}\n\n// Example usage\nconsole.log(normalizeText('$1,000')); // \"one thousand dollars\"\nconsole.log(normalizeText('£1000')); // \"one thousand pounds\"\nconsole.log(normalizeText('€1000')); // \"one thousand euros\"\nconsole.log(normalizeText('¥1000')); // \"one thousand yen\"\nconsole.log(normalizeText('$1,234.56')); // \"one thousand two hundred thirty-four dollars and fifty-six cents\"\nconsole.log(normalizeText('555-555-5555')); // \"five five five, five five five, five five five five\"\n\n```\n\n----------------------------------------\n\nTITLE: Adding Audio Native Helper Script in Framer\nDESCRIPTION: HTML script tag to load the ElevenLabs Audio Native JavaScript helper. This script should be placed in the 'End of <body> tag' field within Framer's site settings to enable the Audio Native functionality.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/framer.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Initializing Express WebSocket Server with ElevenLabs and Twilio Integration (TypeScript)\nDESCRIPTION: Defines the main server logic in TypeScript using Express and express-ws, leveraging the ElevenLabs SDK for text-to-speech generation and the Twilio SDK for handling call streams. Key dependencies include 'express', 'express-ws', 'elevenlabs', 'twilio', 'ws', and 'dotenv'. The server listens for Twilio webhook POST requests at '/call/incoming', creates a websocket connection at '/call/connection', and streams base64-encoded generated audio to the client upon receipt of a start event. Required environment variables are SERVER_DOMAIN and ELEVENLABS_API_KEY, which must be set in a '.env' file. Inputs: webhook events from Twilio; Outputs: streamed audio data to Twilio; Constraints: SERVER_DOMAIN must resolve publicly, and valid Twilio/ElevenLabs credentials must be provided.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\n// src/app.ts\nimport 'dotenv/config';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport express, { Response } from 'express';\nimport ExpressWs from 'express-ws';\nimport { Readable } from 'stream';\nimport VoiceResponse from 'twilio/lib/twiml/VoiceResponse';\nimport { type WebSocket } from 'ws';\n\nconst app = ExpressWs(express()).app;\nconst PORT: number = parseInt(process.env.PORT || '5000');\n\nconst elevenlabs = new ElevenLabsClient();\nconst voiceId = '21m00Tcm4TlvDq8ikWAM';\nconst outputFormat = 'ulaw_8000';\nconst text = 'This is a test. You can now hang up. Thank you.';\n\nfunction startApp() {\n  app.post('/call/incoming', (_, res: Response) => {\n    const twiml = new VoiceResponse();\n\n    twiml.connect().stream({\n      url: `wss://${process.env.SERVER_DOMAIN}/call/connection`,\n    });\n\n    res.writeHead(200, { 'Content-Type': 'text/xml' });\n    res.end(twiml.toString());\n  });\n\n  app.ws('/call/connection', (ws: WebSocket) => {\n    ws.on('message', async (data: string) => {\n      const message: {\n        event: string;\n        start?: { streamSid: string; callSid: string };\n      } = JSON.parse(data);\n\n      if (message.event === 'start' && message.start) {\n        const streamSid = message.start.streamSid;\n        const response = await elevenlabs.textToSpeech.convert(voiceId, {\n          model_id: 'eleven_flash_v2_5',\n          output_format: outputFormat,\n          text,\n        });\n\n        const readableStream = Readable.from(response);\n        const audioArrayBuffer = await streamToArrayBuffer(readableStream);\n\n        ws.send(\n          JSON.stringify({\n            streamSid,\n            event: 'media',\n            media: {\n              payload: Buffer.from(audioArrayBuffer as any).toString('base64'),\n            },\n          })\n        );\n      }\n    });\n\n    ws.on('error', console.error);\n  });\n\n  app.listen(PORT, () => {\n    console.log(`Local: http://localhost:${PORT}`);\n    console.log(`Remote: https://${process.env.SERVER_DOMAIN}`);\n  });\n}\n\nfunction streamToArrayBuffer(readableStream: Readable) {\n  return new Promise((resolve, reject) => {\n    const chunks: Buffer[] = [];\n\n    readableStream.on('data', (chunk) => {\n      chunks.push(chunk);\n    });\n\n    readableStream.on('end', () => {\n      resolve(Buffer.concat(chunks).buffer);\n    });\n\n    readableStream.on('error', reject);\n  });\n}\n\nstartApp();\n```\n\n----------------------------------------\n\nTITLE: Example Usage: Creating Dub from File Python\nDESCRIPTION: This Python script demonstrates how to call the `create_dub_from_file` function with example parameters. It checks the return value and prints a success or failure message along with the output file path if the dubbing was successful.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nif __name__ == \"__main__\":\n    result = create_dub_from_file(\n        \"../example_speech.mp3\",  # Input file path\n        \"audio/mpeg\",  # File format\n        \"en\",  # Source language\n        \"es\",  # Target language\n    )\n    if result:\n        print(\"Dubbing was successful! File saved at:\", result)\n    else:\n        print(\"Dubbing failed or timed out.\")\n\n```\n\n----------------------------------------\n\nTITLE: Generating Presigned URL for S3 File in TypeScript\nDESCRIPTION: Demonstrates usage of the asynchronous generatePresignedUrl function to retrieve a time-limited URL for an audio file on S3, then logs it. Presumes the file exists at s3path and dependencies are installed. Output is sent to console.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_12\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst presignedUrl = await generatePresignedUrl(s3path);\nconsole.log('Presigned URL:', presignedUrl);\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Comparison Cards in React\nDESCRIPTION: This JSX snippet uses CardGroup and Card components (presumably from a UI library or custom components) to display a side-by-side comparison of two ElevenLabs speech synthesis models: Eleven Multilingual v2 and Eleven Flash v2.5. Each card includes a title linking to more details, a brief description, and a list of key features like language support, character limits, and performance characteristics.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/tts-models.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<CardGroup cols={2} rows={2}>\n  <Card title=\"Eleven Multilingual v2\" href=\"/docs/models#multilingual-v2\">\n    Our most lifelike, emotionally rich speech synthesis model\n    <div className=\"mt-4 space-y-2\">\n      <div className=\"text-sm\">Most natural-sounding output</div>\n      <div className=\"text-sm\">29 languages supported</div>\n      <div className=\"text-sm\">10,000 character limit</div>\n      <div className=\"text-sm\">Rich emotional expression</div>\n    </div>\n  </Card>\n  <Card title=\"Eleven Flash v2.5\" href=\"/docs/models#flash-v25\">\n    Our fast, affordable speech synthesis model\n    <div className=\"mt-4 space-y-2\">\n      <div className=\"text-sm\">Ultra-low latency (~75ms&dagger;)</div>\n      <div className=\"text-sm\">32 languages supported</div>\n      <div className=\"text-sm\">40,000 character limit</div>\n      <div className=\"text-sm\">Faster model, 50% lower price per character</div>\n    </div>\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables\nDESCRIPTION: This JavaScript snippet demonstrates how to define the ElevenLabs API key and Agent ID within a `.env` file. The API key is used to authenticate with the ElevenLabs API, and the Agent ID specifies which Conversational AI agent to use.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/telephony/vonage.mdx#_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nELEVENLABS_API_KEY = YOUR_API_KEY;\nELEVENLABS_AGENT_ID = YOUR_AGENT_ID;\n```\n\n----------------------------------------\n\nTITLE: Specifying Pronunciation with SSML Phoneme Tags (IPA) in ElevenLabs (XML)\nDESCRIPTION: Provides an example of using SSML `<phoneme>` tags with the `ipa` (International Phonetic Alphabet) alphabet to specify the pronunciation of \"actually\". This method is compatible with ElevenLabs models \"Eleven Flash v2\", \"Eleven Turbo v2\", and \"Eleven English v1\".\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<phoneme alphabet=\"ipa\" ph=\"ˈæktʃuəli\">\n  actually\n</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Receiving Voice Activity Detection (VAD) Scores - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Specifies the 'vad_score' event indicating the probability (0 to 1) that the user is currently speaking, which helps optimize voice activity detection. This event contains a simple JSON structure with the VAD score value for client-side processing or UI feedback.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"vad_score\",\n  \"vad_score_event\": {\n    \"vad_score\": 0.95\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Voice from Preview with ElevenLabs Python SDK\nDESCRIPTION: Uses the ElevenLabs client to call `create_voice_from_preview`, providing a name, description, and the ID of a generated voice preview (using the first preview in the list as an example). This action adds the voice to the user's ElevenLabs voice library for future use. Requires a previously initialized client and a valid generated voice ID.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvoice = client.text_to_voice.create_voice_from_preview(\n    voice_name=\"Jolly giant\",\n    voice_description=\"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    # The generated voice ID of the preview you want to use,\n    # using the first in the list for this example\n    generated_voice_id=voices.previews[0].generated_voice_id\n)\n\nprint(voice.voice_id)\n\n```\n\n----------------------------------------\n\nTITLE: Streaming Text to Speech Audio in TypeScript using ElevenLabs SDK\nDESCRIPTION: This asynchronous TypeScript function streams the audio output from text-to-speech conversion as raw data using the ElevenLabs SDK. It reads chunks of audio from the streaming API call, concatenates them into a Buffer, and returns the complete audio content. The function requires the API key from environment variables and supports voice customization through voice settings. Dependencies include 'elevenlabs' and 'dotenv' packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as dotenv from 'dotenv';\nimport { ElevenLabsClient } from 'elevenlabs';\n\ndotenv.config();\n\nconst ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;\n\nif (!ELEVENLABS_API_KEY) {\n  throw new Error('Missing ELEVENLABS_API_KEY in environment variables');\n}\n\nconst client = new ElevenLabsClient({\n  apiKey: ELEVENLABS_API_KEY,\n});\n\nexport const createAudioStreamFromText = async (text: string): Promise<Buffer> => {\n  const audioStream = await client.textToSpeech.convertAsStream('JBFqnCBsd6RMkjVDRZzb', {\n    model_id: 'eleven_multilingual_v2',\n    text,\n    output_format: 'mp3_44100_128',\n    // Optional voice settings that allow you to customize the output\n    voice_settings: {\n      stability: 0,\n      similarity_boost: 1.0,\n      use_speaker_boost: true,\n      speed: 1.0,\n    },\n  });\n\n  const chunks: Buffer[] = [];\n  for await (const chunk of audioStream) {\n    chunks.push(chunk);\n  }\n\n  const content = Buffer.concat(chunks);\n  return content;\n};\n```\n\n----------------------------------------\n\nTITLE: Starting Application Development Server (shell)\nDESCRIPTION: Provides the shell command to start the TypeScript Node.js development server using the configured 'npm run dev' script. Assumes prior configuration of package.json scripts or the presence of a watcher compiler. This command is used to start the project locally after all dependencies have been installed and environment variables configured. Limitations: It's dependent on project-specific npm scripts.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Running Outbound Call Server - Bash\nDESCRIPTION: This snippet executes the `outbound.js` file using Node.js.  It assumes the file is located in the current working directory.  The script starts the server, which listens for incoming requests related to initiating outbound calls. The script is a direct command to execute the server-side javascript file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nnode outbound.js\n```\n\n----------------------------------------\n\nTITLE: Installing AWS SDK for Python using pip\nDESCRIPTION: Installs the boto3 package via pip in order to interact with AWS S3 from Python scripts. Required before running any Python S3 upload or presigned URL generation code. No parameters are needed; run this in a terminal environment with Python and pip installed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npip install boto3\n```\n\n----------------------------------------\n\nTITLE: Streaming Text to Speech Audio in Python using ElevenLabs SDK\nDESCRIPTION: This Python function converts input text to speech and streams the resulting audio data into a BytesIO in-memory buffer. It uses the ElevenLabs SDK with specified voice parameters and outputs audio in MP3 format. The audio stream is collected chunk by chunk and returned as a seekable stream object, enabling further processing or playback. Requires the elevenlabs library and an environment variable with the API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import IO\nfrom io import BytesIO\nfrom elevenlabs import VoiceSettings\nfrom elevenlabs.client import ElevenLabs\n\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(\n    api_key=ELEVENLABS_API_KEY,\n)\n\n\ndef text_to_speech_stream(text: str) -> IO[bytes]:\n    # Perform the text-to-speech conversion\n    response = client.text_to_speech.convert(\n        voice_id=\"pNInz6obpgDQGcFmaJgB\", # Adam pre-made voice\n        output_format=\"mp3_22050_32\",\n        text=text,\n        model_id=\"eleven_multilingual_v2\",\n        # Optional voice settings that allow you to customize the output\n        voice_settings=VoiceSettings(\n            stability=0.0,\n            similarity_boost=1.0,\n            style=0.0,\n            use_speaker_boost=True,\n            speed=1.0,\n        ),\n    )\n\n    # Create a BytesIO object to hold the audio data in memory\n    audio_stream = BytesIO()\n\n    # Write each chunk of audio data to the stream\n    for chunk in response:\n        if chunk:\n            audio_stream.write(chunk)\n\n    # Reset stream position to the beginning\n    audio_stream.seek(0)\n\n    # Return the stream for further use\n    return audio_stream\n```\n\n----------------------------------------\n\nTITLE: Establishing Guardrails and Sales Tools for Customer Guidance Framework\nDESCRIPTION: This snippet defines explicit ethical and operational guardrails for customer sales conversations, emphasizing transparency, respect, and professionalism. It lists a suite of sales tools available for product search, detail retrieval, availability checking, product comparison, promotion identification, and scheduling follow-ups. The orchestration strategy guides the conversation flow from need-based product search to final recommendation, ensuring data accuracy and customer autonomy.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_18\n\nLANGUAGE: plaintext\nCODE:\n```\n# Guardrails\n\nPresent accurate information about products, pricing, and availability without exaggeration.\nWhen asked about competitor products, provide objective comparisons without disparaging other brands.\nNever create false urgency or pressure tactics - let customers make decisions at their own pace.\nIf you don't know specific product details, acknowledge this transparently rather than guessing.\nAlways respect customer budget constraints and never push products above their stated price range.\nMaintain a consistent, professional tone even when customers express frustration or indecision.\nIf customers wish to end the conversation or need time to think, respect their space without persistence.\n\n# Tools\n\nYou have access to the following sales tools to assist customers effectively:\n\n`productSearch`: When customers describe their needs, use this to find matching products in the catalog.\n\n`getProductDetails`: Use this to retrieve comprehensive information about a specific product.\n\n`checkAvailability`: Verify whether items are in stock at the customer's preferred location.\n\n`compareProducts`: Generate a comparison of features, benefits, and pricing between multiple products.\n\n`checkPromotions`: Identify current sales, discounts or special offers for relevant product categories.\n\n`scheduleFollowUp`: Offer to set up a follow-up call when a customer needs time to decide.\n\nTool orchestration: Begin with product search based on customer needs, provide details on promising matches, compare options when appropriate, and check availability before finalizing recommendations.\n```\n\n----------------------------------------\n\nTITLE: Receiving and Saving Streaming Audio\nDESCRIPTION: Contains functions and event listeners to handle incoming audio data chunks from the WebSocket connection. It decodes the base64 audio data and writes it incrementally to a local MP3 file. Includes the main execution setup for Python.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n\nasync def write_to_local(audio_stream):\n    \"\"\"Write the audio encoded in base64 string to a local mp3 file.\"\"\"\n\n    with open(f'./output/test.mp3', \"wb\") as f:\n        async for chunk in audio_stream:\n            if chunk:\n                f.write(chunk)\n\nasync def listen(websocket):\n    \"\"\"Listen to the websocket for audio data and stream it.\"\"\"\n\n    while True:\n        try:\n            message = await websocket.recv()\n            data = json.loads(message)\n            if data.get(\"audio\"):\n                yield base64.b64decode(data[\"audio\"])\n            elif data.get('isFinal'):\n                break\n\n        except websockets.exceptions.ConnectionClosed:\n            print(\"Connection closed\")\n            break\n\nasync def text_to_speech_ws_streaming(voice_id, model_id):\n    async with websockets.connect(uri) as websocket:\n          ...\n          # Add listen task to submit the audio chunks to the write_to_local function\n          listen_task = asyncio.create_task(write_to_local(listen(websocket)))\n\n          await listen_task\n\nasyncio.run(text_to_speech_ws_streaming(voice_id, model_id))\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// Helper function to write the audio encoded in base64 string into local file\nfunction writeToLocal(base64str: any, writeStream: fs.WriteStream) {\n  const audioBuffer: Buffer = Buffer.from(base64str, 'base64');\n  writeStream.write(audioBuffer, (err) => {\n    if (err) {\n      console.error('Error writing to file:', err);\n    } a\n  });\n}\n\n// Listen to the incoming message from the websocket connection\nwebsocket.on('message', function incoming(event) {\n  const data = JSON.parse(event.toString());\n  if (data['audio']) {\n    writeToLocal(data['audio'], writeStream);\n  }\n});\n\n// Close the writeStream when the websocket connection closes\nwebsocket.on('close', () => {\n  writeStream.end();\n});\n```\n\n----------------------------------------\n\nTITLE: CMU Arpabet SSML (Alternative Stress Pattern)\nDESCRIPTION: Shows how to specify an alternative stress pattern using CMU Arpabet within SSML phoneme tags. By placing the primary stress marker ('1') on a different syllable, you can direct the model to pronounce the word with emphasis on a non-standard syllable, allowing for precise control over pronunciation variants.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_4\n\nLANGUAGE: SSML\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"T AE0 L AH1 N\">talon</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Building and Running Expo App - Bash\nDESCRIPTION: This snippet provides instructions on how to prebuild and run an Expo React Native application. First, it runs the prebuild command to prepare native code for iOS and Android. Then, it uses `npx expo start --tunnel` to start the Expo development server and `npx expo run:ios --device` to launch the app on a connected iOS device.  The tunnel is required to connect the device to the development server.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nnpx expo prebuild --clean\n```\n\nLANGUAGE: Bash\nCODE:\n```\nnpx expo start --tunnel\n```\n\nLANGUAGE: Bash\nCODE:\n```\nnpx expo run:ios --device\n```\n\n----------------------------------------\n\nTITLE: Creating Text to Speech Audio File with ElevenLabs SDK in Python\nDESCRIPTION: This Python snippet defines a function to convert input text to a speech audio file using the ElevenLabs SDK. It authenticates with an API key, selects a predefined voice and output format, and applies customizable voice settings. The audio stream response is saved locally as a uniquely named MP3 file. It requires the elevenlabs package and environment variables configured with an API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport uuid\nfrom elevenlabs import VoiceSettings\nfrom elevenlabs.client import ElevenLabs\n\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(\n    api_key=ELEVENLABS_API_KEY,\n)\n\n\ndef text_to_speech_file(text: str) -> str:\n    # Calling the text_to_speech conversion API with detailed parameters\n    response = client.text_to_speech.convert(\n        voice_id=\"pNInz6obpgDQGcFmaJgB\", # Adam pre-made voice\n        output_format=\"mp3_22050_32\",\n        text=text,\n        model_id=\"eleven_turbo_v2_5\", # use the turbo model for low latency\n        # Optional voice settings that allow you to customize the output\n        voice_settings=VoiceSettings(\n            stability=0.0,\n            similarity_boost=1.0,\n            style=0.0,\n            use_speaker_boost=True,\n            speed=1.0,\n        ),\n    )\n\n    # uncomment the line below to play the audio back\n    # play(response)\n\n    # Generating a unique file name for the output MP3 file\n    save_file_path = f\"{uuid.uuid4()}.mp3\"\n\n    # Writing the audio to a file\n    with open(save_file_path, \"wb\") as f:\n        for chunk in response:\n            if chunk:\n                f.write(chunk)\n\n    print(f\"{save_file_path}: A new audio file was saved successfully!\")\n\n    # Return the path of the saved audio file\n    return save_file_path\n```\n\n----------------------------------------\n\nTITLE: Retrieving ElevenLabs Conversational AI Embed Code - HTML\nDESCRIPTION: This snippet shows the complete HTML code provided by ElevenLabs for embedding their Conversational AI widget. It includes the custom element tag (<elevenlabs-convai>) and the required script tag to load the widget's functionality from a CDN. Users need to copy this code from their ElevenLabs dashboard, replacing YOUR_AGENT_ID with their actual agent identifier.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/webflow.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Example: Travel booking agent goal\nDESCRIPTION: Outlines a structured goal for a travel booking agent using MDX. It details phases for requirements gathering, options research/presentation (including filtering and highlighting key details), booking execution, and post-booking service, incorporating conditional logic for unavailable segments and success metrics.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_10\n\nLANGUAGE: mdx\nCODE:\n```\n# Goal\n\nYour primary goal is to efficiently guide customers through the travel booking process while maximizing satisfaction and booking completion through this structured workflow:\n\n1. Requirements gathering phase:\n\n   - Establish core travel parameters (destination, dates, flexibility, number of travelers)\n   - Identify traveler preferences (budget range, accommodation type, transportation preferences)\n   - Determine special requirements (accessibility needs, meal preferences, loyalty program memberships)\n   - Assess experience priorities (luxury vs. value, adventure vs. relaxation, guided vs. independent)\n   - Capture relevant traveler details (citizenship for visa requirements, age groups for applicable discounts)\n\n2. Options research and presentation:\n\n   - Research available options meeting core requirements\n   - Filter by availability and budget constraints\n   - Present 3-5 options in order of best match to stated preferences\n   - For each option, highlight: key features, total price breakdown, cancellation policies, and unique benefits\n   - Apply conditional logic: If initial options don't satisfy user, refine search based on feedback\n\n3. Booking process execution:\n\n   - Walk through booking fields with clear validation at each step\n   - Process payment with appropriate security verification\n   - Apply available discounts and loyalty benefits automatically\n   - Confirm all booking details before finalization\n   - Generate and deliver booking confirmations\n\n4. Post-booking service:\n   - Provide clear instructions for next steps (check-in procedures, required documentation)\n   - Set calendar reminders for important deadlines (cancellation windows, check-in times)\n   - Offer relevant add-on services based on booking type (airport transfers, excursions, travel insurance)\n   - Schedule pre-trip check-in to address last-minute questions or changes\n\nIf any segment becomes unavailable during booking, immediately present alternatives. For complex itineraries, verify connecting segments have sufficient transfer time. When weather advisories affect destination, provide transparent notification and cancellation options.\n\nSuccess is measured by booking completion rate, customer satisfaction scores, and percentage of customers who return for future bookings.\n```\n\n----------------------------------------\n\nTITLE: Registering Client Tool 'logMessage' - JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to register the 'logMessage' client tool. It defines an asynchronous function within the clientTools object to log a message to the console. The Conversation object is initialized with this clientTools configuration. Requires the Conversational AI library.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// ...\nconst conversation = await Conversation.startSession({\n  // ...\n  clientTools: {\n    logMessage: async ({message}) => {\n      console.log(message);\n    }\n  },\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Registering Client Tool 'logMessage' - Python\nDESCRIPTION: This Python snippet registers the 'logMessage' client tool. This allows the assistant to log a message to the user's console. It requires the elevenlabs library and defines a function that receives parameters, extracts the message, and prints it to the console. The Conversation object is initiated with the API key, agent ID, and registered client tools.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation, ClientTools\n\ndef log_message(parameters):\n    message = parameters.get(\"message\")\n    print(message)\n\nclient_tools = ClientTools()\nclient_tools.register(\"logMessage\", log_message)\n\nconversation = Conversation(\n    client=ElevenLabs(api_key=\"your-api-key\"),\n    agent_id=\"your-agent-id\",\n    client_tools=client_tools,\n    # ...\n)\n\nconversation.start_session()\n```\n\n----------------------------------------\n\nTITLE: Generating Voice Previews with ElevenLabs TypeScript SDK\nDESCRIPTION: Initializes the ElevenLabs client and calls `createPreviews` with a voice description and text. It iterates through the previews, decodes the base64 audio into a buffer, creates a readable stream, and plays each preview. Requires `elevenlabs`, `dotenv`, `node:stream`, `node:buffer`. Expects API key via environment variable `ELEVENLABS_API_KEY`. Requires MPV and/or ffmpeg for audio playback.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.ts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\nimport { Readable } from 'node:stream';\nimport { Buffer } from 'node:buffer';\n\nconst client = new ElevenLabsClient();\n\nconst { previews } = await client.textToVoice.createPreviews({\n    voice_description: \"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    text: \"Greetings little human. I am a mighty giant from a far away land. Would you like me to tell you a story?\",\n});\n\nfor (const preview of previews) {\n    // Convert base64 to buffer and create a Readable stream\n    const audioStream = Readable.from(Buffer.from(preview.audio_base_64, 'base64'));\n\n    console.log(`Playing preview: ${preview.generated_voice_id}`);\n\n    // Play the audio using the stream\n    await play(audioStream);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving ElevenLabs PVC Speaker Audio (Python)\nDESCRIPTION: Retrieves the list of samples for a PVC voice, gets the speaker information for each sample after separation is complete, fetches the audio data (base64 encoded) for each identified speaker within the sample, decodes it, and saves the audio to separate files in a specified directory. Requires `os` and `base64` modules.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Get the list of samples from the voice created in Step 3\n        voice = elevenlabs.voices.get(voice_id=voice_id)\n\n        samples = voice.samples\n\n        # Loop over each sample and save the audio for each speaker to a file\n        speaker_audio_output_dir = \"path/to/speakers/\"\n        if not os.path.exists(speaker_audio_output_dir):\n            os.makedirs(speaker_audio_output_dir)\n\n        for sample in samples:\n            speaker_info = elevenlabs.voices.pvc.samples.speakers.get(\n                voice_id=voice.voice_id,\n                sample_id=sample.sample_id\n            )\n\n            # Proceed only if separation is actually complete\n            if getattr(speaker_info, 'status', 'unknown') != \"completed\":\n                continue\n\n            if hasattr(speaker_info, 'speakers') and speaker_info.speakers:\n                speaker_list = speaker_info.speakers\n                if isinstance(speaker_info.speakers, dict):\n                    speaker_list = speaker_info.speakers.values()\n\n                for speaker in speaker_list:\n                    audio_response = elevenlabs.voices.pvc.samples.speakers.audio.get(\n                        voice_id=voice.voice_id,\n                        sample_id=sample.sample_id,\n                        speaker_id=speaker.speaker_id\n                    )\n\n                    audio_base64 = audio_response.audio_base_64\n                    audio_data = base64.b64decode(audio_base64)\n                    output_filename = os.path.join(speaker_audio_output_dir, f\"sample_{sample.sample_id}_speaker_{speaker.speaker_id}.mp3\")\n\n                    with open(output_filename, \"wb\") as f:\n                        f.write(audio_data)\n```\n\n----------------------------------------\n\nTITLE: Ending a Conversation Session Manually with endSession Method in JavaScript\nDESCRIPTION: Ends the current conversation and disconnects from the websocket connection, allowing manual termination of the active voice session with the AI agent.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait conversation.endSession();\n```\n\n----------------------------------------\n\nTITLE: Creating ElevenLabs PVC Voice (Python)\nDESCRIPTION: Initializes the ElevenLabs client using an API key loaded from environment variables and creates a new Professional Voice Clone (PVC) voice with a specified name, language, and description. Requires the `elevenlabs` SDK and `python-dotenv` for environment variable loading.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nimport time\nimport base64\nfrom contextlib import ExitStack\nfrom io import BytesIO\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\n\nload_dotenv()\n\nelevenlabs = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\nvoice = elevenlabs.voices.pvc.create(\n    name=\"My Professional Voice Clone\",\n    language=\"en\",\n    description=\"A professional voice clone of my voice\"\n)\n\nprint(voice)\n```\n\n----------------------------------------\n\nTITLE: Creating Dub from File Workflow ElevenLabs SDK Python\nDESCRIPTION: This Python function combines initiating the dubbing, waiting for its completion, and downloading the result into a single workflow. It uploads the file, monitors the process status using `wait_for_dubbing_completion`, and if successful, downloads the output using `download_dubbed_file`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef create_dub_from_file(\n    input_file_path: str,\n    file_format: str,\n    source_language: str,\n    target_language: str,\n) -> Optional[str]:\n    \"\"\"\n    Dubs an audio or video file from one language to another and saves the output.\n\n    Args:\n        input_file_path (str): The file path of the audio or video to dub.\n        file_format (str): The file format of the input file.\n        source_language (str): The language of the input file.\n        target_language (str): The target language to dub into.\n\n    Returns:\n        Optional[str]: The file path of the dubbed file or None if operation failed.\n    \"\"\"\n    if not os.path.isfile(input_file_path):\n        raise FileNotFoundError(f\"The input file does not exist: {input_file_path}\")\n\n    with open(input_file_path, \"rb\") as audio_file:\n        response = client.dubbing.dub_a_video_or_an_audio_file(\n            file=(os.path.basename(input_file_path), audio_file, file_format),\n            target_lang=target_language,\n            source_lang=source_language,\n            num_speakers=1,\n            watermark=False,  # reduces the characters used if enabled, only works for videos not audio\n        )\n\n    dubbing_id = response.dubbing_id\n    if wait_for_dubbing_completion(dubbing_id):\n        output_file_path = download_dubbed_file(dubbing_id, target_language)\n        return output_file_path\n    else:\n        return None\n\n```\n\n----------------------------------------\n\nTITLE: Example: Financial advisory agent goal\nDESCRIPTION: Presents a structured goal definition template for a financial advisory agent using MDX. This example outlines the key objectives and steps the agent should follow, likely covering client assessment, recommendation generation, and plan implementation phases.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_11\n\nLANGUAGE: mdx\nCODE:\n```\n# Goal\n\nYour primary goal is to efficiently guide customers through the financial advisory process...\n```\n\n----------------------------------------\n\nTITLE: Creating ElevenLabs PVC Voice (TypeScript)\nDESCRIPTION: Initializes the ElevenLabs client and creates a new Professional Voice Clone (PVC) voice using an API key typically loaded from environment variables. Specifies the voice's name, language, and description. Requires the `elevenlabs` SDK and `dotenv`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\nimport fs from \"node:fs\";\n\nconst client = new ElevenLabsClient();\n\nconst voice = await client.voices.pvc.create({\n    name: \"My Professional Voice Clone\",\n    language: \"en\",\n    description: \"A professional voice clone of my voice\",\n});\n\nconsole.log(voice.voice_id);\n```\n\n----------------------------------------\n\nTITLE: Linking Local Project to Supabase Account using Supabase CLI\nDESCRIPTION: This command uses the Supabase CLI to associate the local project directory with a specific remote Supabase project. This is a prerequisite for deploying database migrations and Edge Functions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsupabase link\n```\n\n----------------------------------------\n\nTITLE: Testing Together AI Llama 3.1 API using curl\nDESCRIPTION: This code snippet demonstrates how to test the Together AI API with the Llama 3.1 model by making a POST request using curl. It requires an API key, which should be substituted in place of <API_KEY>. The request sends a simple message to the model and retrieves a completion response.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/together-ai.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://api.together.xyz/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer <API_KEY>\" \\\n-d '{\n\"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n\"messages\": [{\n\"role\": \"user\",\n\"content\": \"Hello, how are you?\"\n}]\n}'\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs SDK Python\nDESCRIPTION: This command installs the official ElevenLabs Python SDK using pip. This SDK is required to interact with the ElevenLabs API for dubbing and other services.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Updating Agent Configuration - Python\nDESCRIPTION: This snippet updates the agent configuration using the ElevenLabs API. It assumes that the `client`, `agent_id`, and `agent_config` variables are already defined.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/rag.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclient.conversational_ai.update_agent(\n    agent_id=agent_id,\n    conversation_config=agent_config.agent\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Audio Native Player with JavaScript API\nDESCRIPTION: This code shows how to programmatically create an Audio Native player using the ElevenLabs JavaScript client. It initializes the client with an API key, creates a new player with a specified name, and returns an HTML snippet that can be embedded on a website.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/overview.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabsClient } from \"elevenlabs\";\n\nconst client = new ElevenLabsClient({ apiKey: \"YOUR_API_KEY\" });\nconst { html_snippet } = await client.audioNative.create({\n    name: \"my-audio-native-player\"\n});\n\n// Use the HTML code in html_snippet to embed the player on your website\n```\n\n----------------------------------------\n\nTITLE: Creating Audio Native Player with Python API\nDESCRIPTION: This snippet demonstrates how to programmatically create an Audio Native player using the ElevenLabs Python client. It initializes the client with an API key and makes a request to create a new Audio Native player, returning an HTML snippet to embed on a website.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/overview.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import ElevenLabs\n\nclient = ElevenLabs(\napi_key=\"YOUR_API_KEY\",\n)\nresponse = client.audio_native.create(\nname=\"name\",\n)\n\n# Use the snippet in response.html_snippet to embed the player on your website\n```\n\n----------------------------------------\n\nTITLE: Node.js / TypeScript: Initiating and Handling Streaming Text-to-Speech with elevenlabs Client\nDESCRIPTION: Shows how to generate and process an audio stream from the ElevenLabs API in Node.js or TypeScript using the official SDK. Demonstrates fetching the stream, playing it locally via a stream, and processing chunks manually by iterating over the stream.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/streaming.mdx#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ElevenLabsClient, stream } from 'elevenlabs';\nimport { Readable } from 'stream';\n\nconst client = new ElevenLabsClient();\n\nasync function main() {\n  const audioStream = await client.textToSpeech.convertAsStream('JBFqnCBsd6RMkjVDRZzb', {\n    text: 'This is a test',\n    model_id: 'eleven_multilingual_v2',\n  });\n\n  // option 1: play the streamed audio locally\n  await stream(Readable.from(audioStream));\n\n  // option 2: process the audio manually\n  for await (const chunk of audioStream) {\n    console.log(chunk);\n  }\n}\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Configuring Microphone Permissions in app.json\nDESCRIPTION: JSON configuration to enable microphone permissions in the Expo app.json file for iOS and Android platforms, necessary for audio recording functionality.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"expo\": {\n    \"scheme\": \"elevenlabs\",\n    // ...\n    \"ios\": {\n      \"infoPlist\": {\n        \"NSMicrophoneUsageDescription\": \"This app uses the microphone to record audio.\"\n      },\n      \"supportsTablet\": true,\n      \"bundleIdentifier\": \"YOUR.BUNDLE.ID\"\n    },\n    \"android\": {\n      \"permissions\": [\n        \"android.permission.RECORD_AUDIO\",\n        \"android.permission.MODIFY_AUDIO_SETTINGS\"\n      ],\n      \"adaptiveIcon\": {\n        \"foregroundImage\": \"./assets/adaptive-icon.png\",\n        \"backgroundColor\": \"#ffffff\"\n      },\n      \"package\": \"YOUR.PACKAGE.ID\"\n    }\n    // ...\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring System Tools for ElevenLabs Agent (JavaScript)\nDESCRIPTION: Shows how to configure an ElevenLabs Conversational AI agent with 'end_call' and 'language_detection' system tools using the JavaScript SDK. Requires the `elevenlabs` npm package and an API key. It initializes the client and defines the system tools (specifying `type: 'system'`) directly within the `tools` array of the agent's prompt configuration object when calling `client.conversationalAi.createAgent`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/system-tools.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabs } from 'elevenlabs';\n\n// Initialize the client\nconst client = new ElevenLabs({\n  apiKey: 'YOUR_API_KEY',\n});\n\n// Create the agent with system tools\nawait client.conversationalAi.createAgent({\n  conversation_config: {\n    agent: {\n      prompt: {\n        tools: [\n          {\n            type: 'system',\n            name: 'end_call',\n            description: '',\n          },\n          {\n            type: 'system',\n            name: 'language_detection',\n            description: '',\n          },\n        ],\n      },\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs SDK and Dependencies in Python and TypeScript\nDESCRIPTION: Commands to install the ElevenLabs SDK and packages for environment variable management. These prerequisite installations enable using the ElevenLabs SDK for text-to-speech conversion and secure management of API keys via .env files. Installation is done using pip for Python and npm for TypeScript.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elevenlabs\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install elevenlabs\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install python-dotenv\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install dotenv\nnpm install @types/dotenv --save-dev\n```\n\n----------------------------------------\n\nTITLE: TypeScript code for dubbing audio using ElevenLabs SDK\nDESCRIPTION: This TypeScript snippet creates an ElevenLabs client, fetches an audio file, initiates dubbing from English to Spanish, and polls periodically until the dubbing is complete. Upon completion, it retrieves and plays the dubbed audio. The code uses async/await syntax, fetch API, and setTimeout for delays. Dependencies include the elevenlabs SDK and dotenv configuration. It provides an asynchronous approach to handle dubbing workflows.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/quickstart.mdx#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst targetLang = \"es\"; // spanish\nconst sourceAudio = await fetch(\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await sourceAudio.arrayBuffer()], {\n  type: \"audio/mp3\",\n});\n\n// Start dubbing\nconst dubbed = await client.dubbing.dubAVideoOrAnAudioFile({\n  file: audioBlob,\n  target_lang: targetLang,\n});\n\nwhile (true) {\n  const { status } = await client.dubbing.getDubbingProjectMetadata(\n    dubbed.dubbing_id\n  );\n  if (status === \"dubbed\") {\n    const dubbedFile = await client.dubbing.getDubbedFile(\n      dubbed.dubbing_id,\n      targetLang\n    );\n    await play(dubbedFile);\n    break;\n  } else {\n    console.log(\"Audio is still being dubbed...\");\n  }\n\n  // Wait 5 seconds between checks\n  await new Promise((resolve) => setTimeout(resolve, 5000));\n}\n\n```\n\n----------------------------------------\n\nTITLE: Checking Dubbing Status ElevenLabs SDK Python\nDESCRIPTION: This Python function polls the ElevenLabs API to check the status of a dubbing project. It repeatedly calls `client.dubbing.get_dubbing_project_metadata` until the status is 'dubbed' or 'dubbing' (indicating progress) or reaches a failure state or timeout.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef wait_for_dubbing_completion(dubbing_id: str) -> bool:\n    \"\"\"\n    Waits for the dubbing process to complete by periodically checking the status.\n\n    Args:\n        dubbing_id (str): The dubbing project id.\n\n    Returns:\n        bool: True if the dubbing is successful, False otherwise.\n    \"\"\"\n    MAX_ATTEMPTS = 120\n    CHECK_INTERVAL = 10  # In seconds\n\n    for _ in range(MAX_ATTEMPTS):\n        metadata = client.dubbing.get_dubbing_project_metadata(dubbing_id)\n        if metadata.status == \"dubbed\":\n            return True\n        elif metadata.status == \"dubbing\":\n            print(\n                \"Dubbing in progress... Will check status again in\",\n                CHECK_INTERVAL,\n                \"seconds.\",\n            )\n            time.sleep(CHECK_INTERVAL)\n        else:\n            print(\"Dubbing failed:\", metadata.error_message)\n            return False\n\n    print(\"Dubbing timed out\")\n    return False\n\n```\n\n----------------------------------------\n\nTITLE: Uploading Audio Stream to AWS S3 and Generating Signed URL in Python\nDESCRIPTION: Defines two key functions: one for uploading an audio stream to an AWS S3 bucket with a randomized .mp3 name, and another to generate a presigned URL valid for 1 hour granting read access to the uploaded file. Relies on boto3, uuid, and AWS credentials set as environment variables. Expected input for upload is a file-like stream object; presigned URL generation expects the S3 file name. Outputs a file name or string URL. The functions assume a properly configured .env file and user permissions for S3.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport boto3\nimport uuid\n\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION_NAME = os.getenv(\"AWS_REGION_NAME\")\nAWS_S3_BUCKET_NAME = os.getenv(\"AWS_S3_BUCKET_NAME\")\n\nsession = boto3.Session(\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    region_name=AWS_REGION_NAME,\n)\ns3 = session.client(\"s3\")\n\n\ndef generate_presigned_url(s3_file_name: str) -> str:\n    signed_url = s3.generate_presigned_url(\n        \"get_object\",\n        Params={\"Bucket\": AWS_S3_BUCKET_NAME, \"Key\": s3_file_name},\n        ExpiresIn=3600,\n    )  # URL expires in 1 hour\n    return signed_url\n\n\ndef upload_audiostream_to_s3(audio_stream) -> str:\n    s3_file_name = f\"{uuid.uuid4()}.mp3\"  # Generates a unique file name using UUID\n    s3.upload_fileobj(audio_stream, AWS_S3_BUCKET_NAME, s3_file_name)\n\n    return s3_file_name\n\n```\n\n----------------------------------------\n\nTITLE: Validating HMAC Webhooks with FastAPI in Python\nDESCRIPTION: This snippet demonstrates how to implement a webhook handler using Python's FastAPI framework that validates incoming webhooks by verifying the HMAC SHA256 signature included in the ElevenLabs-Signature header. It requires the 'fastapi' package and Python's built-in 'hmac', 'hashlib', and 'time' modules. The handler reads the raw request body, extracts the timestamp and signature from the header, validates that the timestamp is within a 30-minute window, computes the HMAC hash of the concatenated timestamp and payload using a shared secret, and compares it with the received signature, rejecting the request if validation fails. On success, it returns a status acknowledgment.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/webhook-hmac-authentication.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI, Request\nimport time\nimport hmac\nfrom hashlib import sha256\n\napp = FastAPI()\n\n# Example webhook handler\n@app.post(\"/webhook\")\nasync def receive_message(request: Request):\n    payload = await request.body()\n    headers = request.headers.get(\"elevenlabs-signature\")\n    if headers is None:\n        return\n    timestamp = headers.split(\",\")[0][2:]\n    hmac_signature = headers.split(\",\")[1]\n\n    # Validate timestamp\n    tolerance = int(time.time()) - 30 * 60\n    if int(timestamp) < tolerance:\n        return\n\n    # Validate signature\n    full_payload_to_sign = f\"{timestamp}.{payload.decode('utf-8')}\"\n    mac = hmac.new(\n        key=secret.encode(\"utf-8\"),\n        msg=full_payload_to_sign.encode(\"utf-8\"),\n        digestmod=sha256,\n    )\n    digest = 'v0=' + mac.hexdigest()\n    if hmac_signature != digest:\n        return\n\n    # Continue processing\n\n    return {\"status\": \"received\"}\n```\n\n----------------------------------------\n\nTITLE: Full End-to-End Audio Upload and Secure Sharing in TypeScript\nDESCRIPTION: This TypeScript script demonstrates the full pipeline: convert text to an audio file and save locally, or create an audio stream, upload to S3, and retrieve a presigned URL. Requires dotenv for env vars, @aws-sdk modules, and custom modules for text-to-speech and S3 tasks. Input text is hard-coded. Logging is used for outputs. All relevant dependencies and AWS credentials must be properly set.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_14\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport 'dotenv/config';\n\nimport { generatePresignedUrl, uploadAudioStreamToS3 } from './s3_uploader';\nimport { createAudioFileFromText } from './text_to_speech_file';\nimport { createAudioStreamFromText } from './text_to_speech_stream';\n\n(async () => {\n  // save the audio file to disk\n  const fileName = await createAudioFileFromText(\n    'Today, the sky is exceptionally clear, and the sun shines brightly.'\n  );\n\n  console.log('File name:', fileName);\n\n  // OR stream the audio, upload to S3, and get a presigned URL\n  const stream = await createAudioStreamFromText(\n    'Today, the sky is exceptionally clear, and the sun shines brightly.'\n  );\n\n  const s3path = await uploadAudioStreamToS3(stream);\n\n  const presignedUrl = await generatePresignedUrl(s3path);\n\n  console.log('Presigned URL:', presignedUrl);\n})();\n```\n\n----------------------------------------\n\nTITLE: Example Usage: Creating Dub from URL Python\nDESCRIPTION: This Python script demonstrates how to call the `create_dub_from_url` function with an example YouTube URL. It attempts to dub the video and prints the result, indicating success with the output file path or failure.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nif __name__ == \"__main__\":\n    source_url = \"https://www.youtube.com/watch?v=0EqSXDwTq6U\"  # Charlie bit my finger\n    source_language = \"en\"\n    target_language = \"fr\"\n    result = create_dub_from_url(source_url, source_language, target_language)\n    if result:\n        print(\"Dubbing was successful! File saved at:\", result)\n    else:\n        print(\"Dubbing failed or timed out.\")\n\n```\n\n----------------------------------------\n\nTITLE: Embedding ElevenLabs Conversational AI agent widget in HTML for Wix\nDESCRIPTION: This HTML snippet demonstrates how to embed the ElevenLabs Conversational AI agent on a Wix website using a custom element tag with a unique agent ID and asynchronously loading a supporting JavaScript widget script. The snippet must be placed in Wix's Custom Code section with the agent-id attribute replaced by the actual agent identifier. This enables the conversational interface on specified Wix pages. Dependencies include a valid ElevenLabs agent and Wix Premium account with Dev Mode enabled.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/wix.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Generating Sound Effect ElevenLabs Python SDK\nDESCRIPTION: This snippet demonstrates how to use the ElevenLabs Python SDK to generate a sound effect from text. It initializes the client with an API key loaded from environment variables using `dotenv`, calls the `text_to_sound_effects.convert` method with the desired text prompt, and plays the resulting audio using the `play` function. Dependencies include `elevenlabs`, `dotenv`, and potentially external players like MPV/ffmpeg for the `play` function. The input is a text description, and the output is an audio stream that is played back.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/quickstart.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\naudio = client.text_to_sound_effects.convert(text=\"Cinematic Braam, Horror\")\n\nplay(audio)\n```\n\n----------------------------------------\n\nTITLE: Webhook Handler - CRM Integration - JavaScript\nDESCRIPTION: This JavaScript code demonstrates a basic webhook handler for integrating with a CRM. It extracts relevant data like user ID, transcript summary, and call outcome from the webhook payload. This data is then used to update a customer record in the CRM, illustrating an automated post-call workflow.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/workflows/post-call-webhook.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Example webhook handler\napp.post('/webhook/elevenlabs', async (req, res) => {\n  // HMAC validation code\n\n  const { data } = req.body;\n\n  // Extract key information\n  const userId = data.metadata.user_id;\n  const transcriptSummary = data.analysis.transcript_summary;\n  const callSuccessful = data.analysis.call_successful;\n\n  // Update CRM record\n  await updateCustomerRecord(userId, {\n    lastInteraction: new Date(),\n    conversationSummary: transcriptSummary,\n    callOutcome: callSuccessful,\n    fullTranscript: data.transcript,\n  });\n\n  res.status(200).send('Webhook received');\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Conversational AI Agent with Language Detection Tool using cURL\nDESCRIPTION: Provides a cURL example to create an ElevenLabs conversational AI agent configured with language detection. It includes setting 'language_detection' as a system tool and defining language presets with localized first messages. Required dependencies include access to the ElevenLabs API endpoint and a valid API key passed via the 'xi-api-key' header. The input JSON must be formatted as per ElevenLabs API specifications, and the output is the API response confirming agent creation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/language-detection.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST https://api.elevenlabs.io/v1/convai/agents/create \\\n     -H \"xi-api-key: YOUR_API_KEY\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\\n  \"conversation_config\": {\\n    \"agent\": {\\n      \"prompt\": {\\n        \"first_message\": \"Hi how are you?\",\\n        \"tools\": [\\n          {\\n            \"type\": \"system\",\\n            \"name\": \"language_detection\",\\n            \"description\": \"\"\\n          }\\n        ]\\n      }\\n    },\\n    \"language_presets\": {\\n      \"nl\": {\\n        \"overrides\": {\\n          \"agent\": {\\n            \"prompt\": null,\\n            \"first_message\": \"Hoi, hoe gaat het met je?\",\\n            \"language\": null\\n          },\\n          \"tts\": null\\n        }\\n      },\\n      \"fi\": {\\n        \"overrides\": {\\n          \"agent\": {\\n            \"prompt\": null,\\n            \"first_message\": \"Hei, kuinka voit?\",\\n            \"language\": null\\n          },\\n          \"tts\": null\\n        }\\n      },\\n      \"tr\": {\\n        \"overrides\": {\\n          \"agent\": {\\n            \"prompt\": null,\\n            \"first_message\": \"Merhaba, nasılsın?\",\\n            \"language\": null\\n          },\\n          \"tts\": null\\n        }\\n      },\\n      \"ru\": {\\n        \"overrides\": {\\n          \"agent\": {\\n            \"prompt\": null,\\n            \"first_message\": \"Привет, как ты?\",\\n            \"language\": null\\n          },\\n          \"tts\": null\\n        }\\n      },\\n      \"pt\": {\\n        \"overrides\": {\\n          \"agent\": {\\n            \"prompt\": null,\\n            \"first_message\": \"Oi, como você está?\",\\n            \"language\": null\\n          },\\n          \"tts\": null\\n        }\\n      },\\n      \"ar\": {\\n        \"overrides\": {\\n          \"agent\": {\\n            \"prompt\": null,\\n            \"first_message\": \"مرحبًا كيف حالك؟\",\\n            \"language\": null\\n          },\\n          \"tts\": null\\n        }\\n      }\\n    }\\n  }\\n}'\n```\n\n----------------------------------------\n\nTITLE: Configuring chunk_length_schedule for Audio Buffering with ElevenLabs WebSocket (Python)\nDESCRIPTION: This Python snippet demonstrates how to send a WebSocket message to ElevenLabs specifying a custom chunk_length_schedule inside generation_config, controlling after how many characters audio is generated and sent to the client. Dependencies: an open asyncio WebSocket connection, the json library, a valid ELEVENLABS_API_KEY, and the text input. Parameters include text (content to synthesize) and chunk_length_schedule (array of character-count thresholds). The API expects a JSON-encoded object; incorrect parameter types or missing authentication will result in errors.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait websocket.send(json.dumps({\n    \"text\": text,\n    \"generation_config\": {\n        # Generate audio after 50, 120, 160, and 290 characters have been sent\n        \"chunk_length_schedule\": [50, 120, 160, 290]\n    },\n    \"xi_api_key\": ELEVENLABS_API_KEY,\n}))\n```\n\n----------------------------------------\n\nTITLE: Generating Sound Effect ElevenLabs TypeScript SDK\nDESCRIPTION: This snippet shows how to generate a sound effect using the ElevenLabs TypeScript SDK. It imports the necessary components, initializes the client (which automatically loads the API key from environment variables due to `dotenv/config`), calls the `textToSoundEffects.convert` method asynchronously with a text prompt, and awaits the audio playback using the `play` function. Dependencies include `elevenlabs` and `dotenv`. The input is a text description, and the output is an audio stream that is played back.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/quickstart.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst audio = await client.textToSoundEffects.convert({\n  text: \"Cinematic Braam, Horror\",\n});\n\nawait play(audio);\n```\n\n----------------------------------------\n\nTITLE: Creating Dub from URL ElevenLabs SDK Python\nDESCRIPTION: This Python function initiates the dubbing process for a media file accessible via a public URL using the ElevenLabs API. It passes the source URL, source language, and target language to the `client.dubbing.dub_a_video_or_an_audio_file` method. Like the file upload method, it then waits for completion and downloads the result.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef create_dub_from_url(\n    source_url: str,\n    source_language: str,\n    target_language: str,\n) -> Optional[str]:\n    \"\"\"\n    Downloads a video from a URL, and creates a dubbed version in the target language.\n\n    Args:\n        source_url (str): The URL of the source video to dub. Can be a YouTube link, TikTok, X (Twitter) or a Vimeo link.\n        source_language (str): The language of the source video.\n        target_language (str): The target language to dub into.\n\n    Returns:\n        Optional[str]: The file path of the dubbed file or None if operation failed.\n    \"\"\"\n\n    response = client.dubbing.dub_a_video_or_an_audio_file(\n      source_url=source_url, # URL of the source video/audio file.\n      target_lang=target_language, # The Target language to dub the content into. Can be none if dubbing studio editor is enabled and running manual mode\n      source_lang=source_language, # Source language.\n      num_speakers=1, # Number of speakers to use for the dubbing.\n      watermark=True,  # Whether to apply watermark to the output video.\n    )\n\n    dubbing_id = response.dubbing_id\n    if wait_for_dubbing_completion(dubbing_id):\n        output_file_path = download_dubbed_file(dubbing_id, target_language)\n        return output_file_path\n    else:\n        return None\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Customizable Audio Native React Component - TypeScript\nDESCRIPTION: This TypeScript (TSX) snippet defines the ElevenLabsAudioNative functional React component for embedding the Audio Native player in React or Next.js apps. It loads the required ElevenLabs script on mount via a useEffect hook. Component props allow customization of the audio player, including public user ID, widget size, text color, background color, and custom children. Dependencies are React and the globally-loaded AudioNative script. Inputs are the component props; outputs are a rendered div widget ready for player initialization. Limitations include the requirement for correct script loading and prop values.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\n// ElevenLabsAudioNative.tsx\n\n'use client';\n\nimport { useEffect } from 'react';\n\nexport type ElevenLabsProps = {\n  publicUserId: string;\n  textColorRgba?: string;\n  backgroundColorRgba?: string;\n  size?: 'small' | 'large';\n  children?: React.ReactNode;\n};\n\nexport const ElevenLabsAudioNative = ({\n  publicUserId,\n  size,\n  textColorRgba,\n  backgroundColorRgba,\n  children,\n}: ElevenLabsProps) => {\n  useEffect(() => {\n    const script = document.createElement('script');\n\n    script.src = 'https://elevenlabs.io/player/audioNativeHelper.js';\n    script.async = true;\n    document.body.appendChild(script);\n\n    return () => {\n      document.body.removeChild(script);\n    };\n  }, []);\n\n  return (\n    <div\n      id=\"elevenlabs-audionative-widget\"\n      data-height={size === 'small' ? '90' : '120'}\n      data-width=\"100%\"\n      data-frameborder=\"no\"\n      data-scrolling=\"no\"\n      data-publicuserid={publicUserId}\n      data-playerurl=\"https://elevenlabs.io/player/index.html\"\n      data-small={size === 'small' ? 'True' : 'False'}\n      data-textcolor={textColorRgba ?? 'rgba(0, 0, 0, 1.0)'}\n      data-backgroundcolor={backgroundColorRgba ?? 'rgba(255, 255, 255, 1.0)'}\n    >\n      {children ? children : 'Elevenlabs AudioNative Player'}\n    </div>\n  );\n};\n\nexport default ElevenLabsAudioNative;\n```\n\n----------------------------------------\n\nTITLE: Setting Custom LLM Parameters in Python using ElevenLabs SDK\nDESCRIPTION: Code snippet demonstrating how to define extra parameters to pass to a custom LLM when using the ElevenLabs Conversational AI SDK. These parameters can customize the behavior of your LLM implementation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs.conversational_ai.conversation import Conversation, ConversationConfig\n\nextra_body_for_convai = {\n    \"UUID\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"parameter-1\": \"value-1\",\n    \"parameter-2\": \"value-2\",\n}\n\nconfig = ConversationConfig(\n    extra_body=extra_body_for_convai,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Sound Effects using ElevenLabs Python SDK\nDESCRIPTION: Python script demonstrating how to generate a sound effect using the ElevenLabs SDK. It initializes the client with an API key loaded from environment variables using `python-dotenv`, calls the `text_to_sound_effects.convert` method with a text prompt, optional duration (`duration_seconds`), and prompt influence (`prompt_influence`), and saves the resulting audio stream to an MP3 file. The script includes a main execution block to run the `generate_sound_effect` function with example inputs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/basics.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom elevenlabs.client import ElevenLabs\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nelevenlabs = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n\n\ndef generate_sound_effect(text: str, output_path: str):\n    print(\"Generating sound effects...\")\n\n    result = elevenlabs.text_to_sound_effects.convert(\n        text=text,\n        duration_seconds=10,  # Optional, if not provided will automatically determine the correct length\n        prompt_influence=0.3,  # Optional, if not provided will use the default value of 0.3\n    )\n\n    with open(output_path, \"wb\") as f:\n        for chunk in result:\n            f.write(chunk)\n\n    print(f\"Audio saved to {output_path}\")\n\n\nif __name__ == \"__main__\":\n    generate_sound_effect(\"Dog barking\", \"output.mp3\")\n\n```\n\n----------------------------------------\n\nTITLE: Deploying an Edge Function to Supabase using Supabase CLI\nDESCRIPTION: Deploys the specified Edge Function (named `scribe-bot` in this example) from the local project to the linked Supabase project. The `--no-verify-jwt` flag bypasses the default JWT verification for incoming requests, often used when authentication is handled differently (e.g., via a secret parameter).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsupabase functions deploy --no-verify-jwt scribe-bot\n```\n\n----------------------------------------\n\nTITLE: Forced Alignment API Request\nDESCRIPTION: This Python snippet demonstrates how to use the ElevenLabs SDK to perform forced alignment. It loads environment variables, initializes the ElevenLabs client, fetches audio data from a URL, and then calls the `forced_alignment.create` method to generate a transcription with timestamps. The transcription is then printed to the console. Requires the `requests`, `dotenv`, and `elevenlabs` packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nfrom io import BytesIO\nfrom elevenlabs.client import ElevenLabs\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = ElevenLabs(\n    api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\n# Perform the text-to-speech conversion\ntranscription = client.forced_alignment.create(\n    file=audio_data,\n    text=\"With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.\"\n)\n\nprint(transcription)\n```\n\n----------------------------------------\n\nTITLE: Starting Supabase Local Development Environment\nDESCRIPTION: Command to start the local Supabase stack for development, which includes the local database, storage, and functions environments.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nsupabase start\n```\n\n----------------------------------------\n\nTITLE: Managing ElevenLabs WebSocket Connection with a Custom React Hook in TypeScript\nDESCRIPTION: This TypeScript snippet defines a React hook named 'useAgentConversation' that manages the lifecycle and event handling of a WebSocket connection to the ElevenLabs conversational AI service. It uses the 'voice-stream' package to capture and stream audio data from the user's microphone encoded in base64. The hook handles WebSocket connection open, close, and incoming messages, processing events such as ping-pong to maintain connection health, user transcripts, agent responses, audio chunks for playback, and interruptions. It exposes functions to start and stop the conversation as well as a connection status boolean for UI integration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n'use client';\n\nimport { useCallback, useEffect, useRef, useState } from 'react';\nimport { useVoiceStream } from 'voice-stream';\nimport type { ElevenLabsWebSocketEvent } from '../types/websocket';\n\nconst sendMessage = (websocket: WebSocket, request: object) => {\n  if (websocket.readyState !== WebSocket.OPEN) {\n    return;\n  }\n  websocket.send(JSON.stringify(request));\n};\n\nexport const useAgentConversation = () => {\n  const websocketRef = useRef<WebSocket>(null);\n  const [isConnected, setIsConnected] = useState<boolean>(false);\n\n  const { startStreaming, stopStreaming } = useVoiceStream({\n    onAudioChunked: (audioData) => {\n      if (!websocketRef.current) return;\n      sendMessage(websocketRef.current, {\n        user_audio_chunk: audioData,\n      });\n    },\n  });\n\n  const startConversation = useCallback(async () => {\n    if (isConnected) return;\n\n    const websocket = new WebSocket(\"wss://api.elevenlabs.io/v1/convai/conversation\");\n\n    websocket.onopen = async () => {\n      setIsConnected(true);\n      sendMessage(websocket, {\n        type: \"conversation_initiation_client_data\",\n      });\n      await startStreaming();\n    };\n\n    websocket.onmessage = async (event) => {\n      const data = JSON.parse(event.data) as ElevenLabsWebSocketEvent;\n\n      // Handle ping events to keep connection alive\n      if (data.type === \"ping\") {\n        setTimeout(() => {\n          sendMessage(websocket, {\n            type: \"pong\",\n            event_id: data.ping_event.event_id,\n          });\n        }, data.ping_event.ping_ms);\n      }\n\n      if (data.type === \"user_transcript\") {\n        const { user_transcription_event } = data;\n        console.log(\"User transcript\", user_transcription_event.user_transcript);\n      }\n\n      if (data.type === \"agent_response\") {\n        const { agent_response_event } = data;\n        console.log(\"Agent response\", agent_response_event.agent_response);\n      }\n\n      if (data.type === \"interruption\") {\n        // Handle interruption\n      }\n\n      if (data.type === \"audio\") {\n        const { audio_event } = data;\n        // Implement your own audio playback system here\n        // Note: You'll need to handle audio queuing to prevent overlapping\n        // as the WebSocket sends audio events in chunks\n      }\n    };\n\n    websocketRef.current = websocket;\n\n    websocket.onclose = async () => {\n      websocketRef.current = null;\n      setIsConnected(false);\n      stopStreaming();\n    };\n  }, [startStreaming, isConnected, stopStreaming]);\n\n  const stopConversation = useCallback(async () => {\n    if (!websocketRef.current) return;\n    websocketRef.current.close();\n  }, []);\n\n  useEffect(() => {\n    return () => {\n      if (websocketRef.current) {\n        websocketRef.current.close();\n      }\n    };\n  }, []);\n\n  return {\n    startConversation,\n    stopConversation,\n    isConnected,\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: Displaying Popular Documentation Links with React JSX Cards\nDESCRIPTION: This snippet uses JSX components CardGroup and Card to display a series of linkable cards under a \"Most popular\" heading. Each Card component defines a title and href prop linking to key ElevenLabs documentation sections such as quickstart, conversational AI, product guides, and API reference. The text inside each Card provides short descriptive guidance. This React JSX snippet depends on CardGroup and Card components and enables quick navigation to frequently used docs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup>\n  <Card title=\"Developer quickstart\" href=\"/docs/quickstart\">\n    Learn how to integrate ElevenLabs\n  </Card>\n  <Card title=\"Conversational AI\" href=\"/docs/conversational-ai/overview\">\n    Deploy voice agents in minutes\n  </Card>\n  <Card title=\"Product guides\" href=\"/docs/product-guides/overview\">\n    Learn how to use ElevenLabs\n  </Card>\n  <Card title=\"API reference\" href=\"/docs/api-reference/introduction\">\n    Dive into our API reference\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Retrieving PVC Verification CAPTCHA (Python/TypeScript)\nDESCRIPTION: Fetches the verification CAPTCHA image required for PVC ownership verification. The function takes the `voice_id` as input. The response is a base64 encoded string representing the CAPTCHA image, which should be decoded and saved to a file (e.g., 'captcha.png') for the voice owner to read.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncaptcha_response = elevenlabs.voices.pvc.verification.captcha.get(voice.voice_id)\n\n# Save captcha image to file\ncaptcha_buffer = base64.b64decode(captcha_response)\nwith open('captcha.png', 'wb') as f:\n    f.write(captcha_buffer)\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst captchaResponse = await elevenlabs.voices.pvc.verification.captcha.get(voice.voice_id);\n\n// Save captcha image to file\nconst captchaBuffer = Buffer.from(captchaResponse, 'base64');\nfs.writeFileSync('path/to/captcha.png', captchaBuffer);\n```\n\n----------------------------------------\n\nTITLE: Initiating Dubbing from File ElevenLabs SDK Python\nDESCRIPTION: This Python function initiates the dubbing process for a local audio or video file by uploading it to the ElevenLabs API. It takes the file path, format, source, and target languages as input and sends them via the `client.dubbing.dub_a_video_or_an_audio_file` method.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef create_dub_from_file(\n    input_file_path: str,\n    file_format: str,\n    source_language: str,\n    target_language: str,\n) -> Optional[str]:\n    \"\"\"\n    Dubs an audio or video file from one language to another and saves the output.\n\n    Args:\n        input_file_path (str): The file path of the audio or video to dub.\n        file_format (str): The file format of the input file.\n        source_language (str): The language of the input file.\n        target_language (str): The target language to dub into.\n\n    Returns:\n        Optional[str]: The file path of the dubbed file or None if operation failed.\n    \"\"\"\n    if not os.path.isfile(input_file_path):\n        raise FileNotFoundError(f\"The input file does not exist: {input_file_path}\")\n\n    with open(input_file_path, \"rb\") as audio_file:\n        response = client.dubbing.dub_a_video_or_an_audio_file(\n            file=(os.path.basename(input_file_path), audio_file, file_format), # Optional file\n            target_lang=target_language, # The target language to dub the content into. Can be none if dubbing studio editor is enabled and running manual mode\n            source_lang=source_language, # Source language\n            num_speakers=1, # Number of speakers to use for the dubbing.\n            watermark=False,  # Whether to apply watermark to the output video.\n        )\n\n    # rest of the code\n\n```\n\n----------------------------------------\n\nTITLE: Creating Database Migration\nDESCRIPTION: This command creates a new database migration file for logging transcription results in Supabase. It prepares the SQL file within the supabase/migrations directory to define table structure.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsupabase migrations new init\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs with PyAudio Extra - Shell\nDESCRIPTION: Shows how to install the 'elevenlabs' package with the 'pyaudio' extra using both pip and poetry for enabling audio features. The 'pyaudio' extra adds voice input/output capabilities required for conversational agents. Ensure system dependencies for PyAudio are satisfied prior to installing.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install \"elevenlabs[pyaudio]\"\n# or\npoetry add \"elevenlabs[pyaudio]\"\n```\n\n----------------------------------------\n\nTITLE: Sending Text Input and Settings via WebSocket\nDESCRIPTION: Illustrates how to send initial voice settings (stability, similarity boost, etc.) and the main text content to the ElevenLabs WebSocket API after the connection is established. An empty string signals the end of the text sequence.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def text_to_speech_ws_streaming(voice_id, model_id):\n    async with websockets.connect(uri) as websocket:\n        await websocket.send(json.dumps({\n            \"text\": \" \",\n            \"voice_settings\": {\"stability\": 0.5, \"similarity_boost\": 0.8, \"use_speaker_boost\": False},\n            \"generation_config\": {\n                \"chunk_length_schedule\": [120, 160, 250, 290]\n            },\n            \"xi_api_key\": ELEVENLABS_API_KEY,\n        }))\n\n        text = \"The twilight sun cast its warm golden hues upon the vast rolling fields, saturating the landscape with an ethereal glow. Silently, the meandering brook continued its ceaseless journey, whispering secrets only the trees seemed privy to.\"\n        await websocket.send(json.dumps({\"text\": text}))\n\n        // Send empty string to indicate the end of the text sequence which will close the WebSocket connection\n        await websocket.send(json.dumps({\"text\": \"\"}))\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst text =\n  'The twilight sun cast its warm golden hues upon the vast rolling fields, saturating the landscape with an ethereal glow. Silently, the meandering brook continued its ceaseless journey, whispering secrets only the trees seemed privy to.';\n\nwebsocket.on('open', async () => {\n  websocket.send(\n    JSON.stringify({\n      text: ' ',\n      voice_settings: {\n        stability: 0.5,\n        similarity_boost: 0.8,\n        use_speaker_boost: false,\n      },\n      generation_config: { chunk_length_schedule: [120, 160, 250, 290] },\n    })\n  );\n\n  websocket.send(JSON.stringify({ text: text }));\n\n  // Send empty string to indicate the end of the text sequence which will close the websocket connection\n  websocket.send(JSON.stringify({ text: '' }));\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: This snippet installs the necessary Python packages for the ElevenLabs SDK and for managing environmental variables using pip. It includes the ElevenLabs SDK and python-dotenv. This prepares the user's environment to use the SDK.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Handling GET Requests for Webhook\nDESCRIPTION: This snippet defines a GET request handler for the webhook endpoint. It returns a JSON response indicating the webhook is listening, with a status code of 200. This is used for basic health checks.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport async function GET() {\n  return NextResponse.json({ status: 'webhook listening' }, { status: 200 });\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing the Audio Native Player via Component Props - TypeScript\nDESCRIPTION: This snippet extends the usage example by demonstrating how to pass additional props (size, textColorRgba, backgroundColorRgba) to ElevenLabsAudioNative for customization. Inputs include user ID and style-related props to control widget appearance. Like previous snippets, it renders the audio widget on the page, now styled as specified. Requirements are the custom React component and properly formatted color and size props. No outputs beyond visual changes to the player.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { ElevenLabsAudioNative } from './path/to/ElevenLabsAudioNative';\n\nexport default function Page() {\n  return (\n    <div>\n      <h1>Your Page Title</h1>\n\n      <ElevenLabsAudioNative\n        publicUserId=\"<your-public-user-id>\"\n        size=\"small\"\n        textColorRgba=\"rgba(255, 255, 255, 1.0)\"\n        backgroundColorRgba=\"rgba(0, 0, 0, 1.0)\"\n      />\n\n      <p>Your page content...</p>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Agent Creation with Data Retention via API\nDESCRIPTION: Adds `retention_days` and `delete_transcript_and_pii` parameters to the Agent creation endpoint (POST `/v1/agents`) to allow configuration of data retention policies for agent conversations.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_5\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /v1/agents (with retention_days, delete_transcript_and_pii parameters)\n```\n\n----------------------------------------\n\nTITLE: Execute Python Script - Command Line\nDESCRIPTION: This command executes the Python script `example.py`. This script is expected to contain the code for cloning a voice using the Eleven Labs Python SDK and will print the resulting voice ID to the console.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/clone-voice.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Interface for ElevenLabs Voice Chat\nDESCRIPTION: HTML code that sets up a basic user interface for the ElevenLabs Conversational AI with start and stop buttons, connection status, and agent status indicators.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\" />\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n        <title>ElevenLabs Conversational AI</title>\n    </head>\n    <body style=\"font-family: Arial, sans-serif; text-align: center; padding: 50px;\">\n        <h1>ElevenLabs Conversational AI</h1>\n        <div style=\"margin-bottom: 20px;\">\n            <button id=\"startButton\" style=\"padding: 10px 20px; margin: 5px;\">Start Conversation</button>\n            <button id=\"stopButton\" style=\"padding: 10px 20px; margin: 5px;\" disabled>Stop Conversation</button>\n        </div>\n        <div style=\"font-size: 18px;\">\n            <p>Status: <span id=\"connectionStatus\">Disconnected</span></p>\n            <p>Agent is <span id=\"agentStatus\">listening</span></p>\n        </div>\n        <script type=\"module\" src=\"../images/script.js\"></script>\n    </body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Running the Conversational Agent Script with Environment Variables - Shell\nDESCRIPTION: Shows the command-line invocation for running the demo Python script with required environment variables set for public or private agents. Environment variables 'AGENT_ID' and optionally 'ELEVENLABS_API_KEY' (for private agents) must be exported or prefixed on the command line. The script (e.g., 'demo.py') should be run after installation and setup are complete.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n# For public agents:\nAGENT_ID=youragentid python demo.py\n\n# For private agents:\nAGENT_ID=youragentid ELEVENLABS_API_KEY=yourapikey python demo.py\n```\n\n----------------------------------------\n\nTITLE: Using request IDs for past generations conditioning in ElevenLabs TTS API\nDESCRIPTION: This snippet shows how to condition text-to-speech requests using previous request IDs to refer to past generated chunks. It enables better prosody by linking chunks based on their request IDs obtained from response headers.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/request-stitching.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport requests\nfrom pydub import AudioSegment\nimport io\n\nYOUR_XI_API_KEY = \"<insert your xi-api-key here>\"\nVOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\nPARAGRAPHS = [\n    \"The advent of technology has transformed countless sectors, with education \"\n    \"standing out as one of the most significantly impacted fields.\",\n    \"In recent years, educational technology, or EdTech, has revolutionized the way \"\n    \"teachers deliver instruction and students absorb information.\",\n    \"From interactive whiteboards to individual tablets loaded with educational software, \"\n    \"technology has opened up new avenues for learning that were previously unimaginable.\",\n    \"One of the primary benefits of technology in education is the accessibility it provides.\",\n]\nsegments = []\nprevious_request_ids = []\n\nfor i, paragraph in enumerate(PARAGRAPHS):\n    response = requests.post(\n        f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\",\n        json={\n            \"text\": paragraph,\n            \"model_id\": \"eleven_multilingual_v2\",\n            \"previous_request_ids\": previous_request_ids[-3:],  # limit to last 3 ids\n        },\n        headers={\"xi-api-key\": YOUR_XI_API_KEY},\n    )\n\n    if response.status_code != 200:\n        print(f\"Error encountered, status: {response.status_code}, \"\n               f\"content: {response.text}\")\n        quit()\n\n    print(f\"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}\")\n    previous_request_ids.append(response.headers[\"request-id\"])\n    segments.append(AudioSegment.from_mp3(io.BytesIO(response.content)))\n\nsegment = segments[0]\nfor new_segment in segments[1:]:\n    segment = segment + new_segment\n\naudio_out_path = os.path.join(os.getcwd(), \"with_previous_request_ids_conditioning.wav\")\nsegment.export(audio_out_path, format=\"wav\")\nprint(f\"Success! Wrote audio to {audio_out_path}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring API Key in .env File\nDESCRIPTION: Creates or modifies a `.env` file to store the ElevenLabs API key. The `python-dotenv` library will load this variable into the environment, allowing the Python script to access the API key securely without hardcoding it.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/basics.mdx#_snippet_2\n\nLANGUAGE: .env\nCODE:\n```\nELEVENLABS_API_KEY=your_elevenlabs_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Beginning ElevenLabs PVC Speaker Separation (Python)\nDESCRIPTION: Initiates speaker separation for each uploaded sample associated with a PVC voice. It then enters a polling loop, checking the status of each sample's separation process periodically using `time.sleep` until all samples are marked as 'completed' or 'failed'. Requires the `time` module.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsample_ids_to_check = []\nfor sample in samples:\n            if sample.sample_id:\n                print(f\"Starting separation for sample: {sample.sample_id}\")\n                elevenlabs.voices.pvc.samples.speakers.separate(\n                    voice_id=voice.voice_id,\n                    sample_id=sample.sample_id\n                )\n                sample_ids_to_check.append(sample.sample_id)\n\n        while sample_ids_to_check:\n            # Create a copy of the list to iterate over, so we can remove items from the original\n            ids_in_batch = list(sample_ids_to_check)\n            for sample_id in ids_in_batch:\n                status_response = elevenlabs.voices.pvc.samples.speakers.get(\n                    voice_id=voice.voice_id,\n                    sample_id=sample_id\n                )\n                status = status_response.status\n                print(f\"Sample {sample_id} status: {status}\")\n                if status == \"completed\" or status == \"failed\":\n                    sample_ids_to_check.remove(sample_id)\n\n            if sample_ids_to_check:\n                # Wait before the next poll cycle\n                time.sleep(5) # Wait for 5 seconds\n\n        print(\"All samples have been processed or removed from polling.\")\n```\n\n----------------------------------------\n\nTITLE: Using Smart Home Assistant Tools\nDESCRIPTION: This snippet details tools for a smart home assistant. It lists tools like `getDeviceStatus`, `controlDevice`, `queryRoutine`, `createOrModifyRoutine`, `troubleshootDevice`, and `addNewDevice)`. It also defines tool orchestration and best practices.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_16\n\nLANGUAGE: mdx\nCODE:\n```\n# Tools\n\nYou have access to the following smart home control tools:\n\n`getDeviceStatus`: Before attempting any control actions, check the current status of the device to provide accurate information to the user.\n\n`controlDevice`: Use this to execute user requests like turning lights on/off, adjusting thermostat, or locking doors after confirming the user's intention.\n\n`queryRoutine`: When users ask about existing automations, use this to check the specific steps and devices included in a routine before explaining or modifying it.\n\n`createOrModifyRoutine`: Help users build new automation sequences or update existing ones, confirming each step for accuracy.\n\n`troubleshootDevice`: When users report devices not working properly, use this diagnostic tool before suggesting reconnection or replacement.\n\n`addNewDevice)`: When users mention setting up new devices, use this tool to guide them through the appropriate connection process for their specific device.\n\nTool orchestration: Always check device status before attempting control actions. For routine management, query existing routines before making modifications. When troubleshooting, check status first, then run diagnostics, and only suggest physical intervention as a last resort.\n```\n\n----------------------------------------\n\nTITLE: Sending Email Notification\nDESCRIPTION: This snippet sends an email to the user to notify them that their custom conversational AI agent is ready.  It uses the Resend service to send an email with the provided `agentId`, retrieved from the newly created agent. The email subject and content are pre-defined within the `EmailTemplate` component.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_13\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Send email to user\nconsole.log('Sending email to', redisRes.email);\nawait resend.emails.send({\n  from: process.env.RESEND_FROM_EMAIL!,\n  to: redisRes.email,\n  subject: 'Your Conversational AI agent is ready to chat!',\n  react: EmailTemplate({ agentId: agent.agent_id }),\n});\n```\n\n----------------------------------------\n\nTITLE: Describing Project File Structure (Shell)\nDESCRIPTION: This code displays the expected file structure after setup, illustrating the directory organization. The file structure is important for the Vite project.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nelevenlabs-conversational-ai/\n├── index.html\n├── script.js\n├── package-lock.json\n├── package.json\n└── node_modules\n```\n\n----------------------------------------\n\nTITLE: Generating TwiML Response for Outbound Call Streaming with Parameters (JavaScript)\nDESCRIPTION: Handles all HTTP methods on the '/outbound-call-twiml' route to generate XML TwiML responses for Twilio. It reads 'prompt' and 'first_message' from the query string and constructs a TwiML XML document to establish a <Connect><Stream> block pointing to a WebSocket endpoint. The parameters are embedded as <Parameter> tags for use by the WebSocket media stream handler. The response is served with the 'text/xml' Content-Type to Twilio for realtime call instructions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nfastify.all('/outbound-call-twiml', async (request, reply) => {\n  const prompt = request.query.prompt || '';\n  const first_message = request.query.first_message || '';\n\n  const twimlResponse = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <Response>\n        <Connect>\n        <Stream url=\"wss://${request.headers.host}/outbound-media-stream\">\n            <Parameter name=\"prompt\" value=\"${prompt}\" />\n            <Parameter name=\"first_message\" value=\"${first_message}\" />\n        </Stream>\n        </Connect>\n    </Response>`;\n\n  reply.type('text/xml').send(twimlResponse);\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Pronunciation Dictionary XML File for ElevenLabs AI\nDESCRIPTION: Example of a .pls XML file structure for pronunciation dictionaries. The file defines how specific words should be pronounced using either phonetic notation (IPA) or word substitution (alias). This example shows both methods - defining 'Apple' with IPA pronunciation and expanding 'UN' as 'United Nations'.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/voice/pronunciation-dictionary.mdx#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"ipa\" xml:lang=\"en-GB\">\n  <lexeme>\n    <grapheme>Apple</grapheme>\n    <phoneme>ˈæpl̩</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n</lexicon>\n```\n\n----------------------------------------\n\nTITLE: Email Template Component for Agent Ready Notification (TSX)\nDESCRIPTION: This TSX code defines a React component (`EmailTemplate`) used to render the HTML body of the email sent to the user. It utilizes `@react-email/components` for structuring the email and Tailwind for styling. The component accepts an `agentId` prop and includes a button linking directly to the ElevenLabs 'Talk to' page with the specific agent pre-selected.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_17\n\nLANGUAGE: TSX\nCODE:\n```\nimport {\n  Body,\n  Button,\n  Container,\n  Head,\n  Html,\n  Section,\n  Text,\n  Tailwind,\n} from '@react-email/components';\nimport * as React from 'react';\n\nconst EmailTemplate = (props: any) => {\n  const { agentId } = props;\n  return (\n    <Html>\n      <Head />\n      <Tailwind>\n        <Body className=\"bg-[#151516] font-sans\">\n          <Container className=\"mx-auto my-[40px] max-w-[600px] rounded-[8px] bg-[#0a1929] p-[20px]\">\n            {/* Top Section */}\n            <Section className=\"mb-[32px] mt-[32px] text-center\">\n              <Text className=\"m-0 text-[28px] font-bold text-[#9c27b0]\">\n                Your Conversational AI agent is ready to chat!\n              </Text>\n            </Section>\n\n            {/* Content Area with Icon */}\n            <Section className=\"mb-[32px] text-center\">\n              {/* Circle Icon with Checkmark */}\n              <div className=\"mx-auto mb-[24px] flex h-[80px] w-[80px] items-center justify-center rounded-full bg-gradient-to-r from-[#9c27b0] to-[#3f51b5]\">\n                <div className=\"text-[40px] text-white\">✓</div>\n              </div>\n\n              {/* Descriptive Text */}\n              <Text className=\"mb-[24px] text-[18px] text-white\">\n                Your Conversational AI agent is ready to chat!\n              </Text>\n            </Section>\n\n            {/* Call to Action Button */}\n            <Section className=\"mb-[32px] text-center\">\n              <Button\n                href={`https://elevenlabs.io/app/talk-to?agent_id=${agentId}`}\n                className=\"box-border rounded-[8px] bg-[#9c27b0] px-[40px] py-[20px] text-[24px] font-bold text-white no-underline\"\n              >\n                Chat now!\n              </Button>\n            </Section>\n\n            {/* Footer */}\n            <Section className=\"mt-[40px] border-t border-[#2d3748] pt-[20px] text-center\">\n              <Text className=\"m-0 text-[14px] text-white\">\n                Powered by{' '}\n                <a\n                  href=\"https://elevenlabs.io/conversational-ai\"\n                  target=\"_blank\"\n                  rel=\"noopener noreferrer\"\n                  className=\"underline transition-colors hover:text-gray-400\"\n                >\n                  ElevenLabs Conversational AI\n                </a>\n              </Text>\n            </Section>\n          </Container>\n        </Body>\n      </Tailwind>\n    </Html>\n  );\n};\n\nexport { EmailTemplate };\n```\n\n----------------------------------------\n\nTITLE: Sending notification email using Resend (TypeScript)\nDESCRIPTION: This TypeScript code uses the Resend email service to send a notification to the user once their conversational AI agent is ready. It imports the Resend library and a custom React email template component. The snippet sends an email from a configured address to the user's email address retrieved from Redis, using the custom component to render the email body and passing the newly created agent ID as a prop.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_16\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Resend } from 'resend';\n\nimport { EmailTemplate } from '@/components/email/post-call-webhook-email';\n\n// ...\n\n// Send email to user\nconsole.log('Sending email to', redisRes.email);\nawait resend.emails.send({\n  from: process.env.RESEND_FROM_EMAIL!,\n  to: redisRes.email,\n  subject: 'Your Conversational AI agent is ready to chat!',\n  react: EmailTemplate({ agentId: agent.agent_id }),\n});\n\n// ...\n```\n\n----------------------------------------\n\nTITLE: Starting Next.js Development Server (Bash)\nDESCRIPTION: This Bash command is used to start the Next.js development server locally. Running this command typically compiles the application and makes it accessible at a local URL, usually http://localhost:3000, enabling development and testing.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_18\n\nLANGUAGE: Bash\nCODE:\n```\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Creating Voice from Preview with ElevenLabs TypeScript SDK\nDESCRIPTION: Uses the ElevenLabs client to call `createVoiceFromPreview`, providing a name, description, and the ID of a generated voice preview (using the first preview in the list as an example). This action adds the voice to the user's ElevenLabs voice library for future use. Requires a previously initialized client and a valid generated voice ID.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst voice = await client.textToVoice.createVoiceFromPreview({\n    voice_name: \"Jolly giant\",\n    voice_description: \"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    // The generated voice ID of the preview you want to use,\n    // using the first in the list for this example\n    generated_voice_id: previews[0].generated_voice_id\n});\n\n// The ID of the newly created voice, use this to reference the voice in other APIs\nconsole.log(voice.voice_id);\n\n```\n\n----------------------------------------\n\nTITLE: Running the Full ElevenLabs Voice Chat Application\nDESCRIPTION: Shell command to start both frontend and backend servers for the complete ElevenLabs Conversational AI application.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Training a PVC and Monitoring Progress (Python/TypeScript)\nDESCRIPTION: Starts the training process for a specified Professional Voice Clone (PVC) using a chosen model ID (e.g., 'eleven_multilingual_v2'). Requires the `voice_id`. The example also includes logic to poll the voice details periodically (e.g., every 5 seconds) to check the fine-tuning status and progress for the specified model until it is 'fine_tuned' or 'failed'.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nelevenlabs.voices.pvc.train(\n    voice_id=voice.voice_id,\n    # Specify the model the PVC should be trained on\n    model_id=\"eleven_multilingual_v2\"\n)\n\n# Poll the fine tuning status until it is complete or fails\n# This example specifically checks for the eleven_multilingual_v2 model\nwhile True:\n    voice_details = elevenlabs.voices.get(voice_id=voice.voice_id)\n    fine_tuning_state = None\n    if voice_details.fine_tuning and voice_details.fine_tuning.state:\n        fine_tuning_state = voice_details.fine_tuning.state.get(\"eleven_multilingual_v2\")\n\n    if fine_tuning_state:\n        progress = None\n        if voice_details.fine_tuning.progress and voice_details.fine_tuning.progress.get(\"eleven_multilingual_v2\"):\n            progress = voice_details.fine_tuning.progress.get(\"eleven_multilingual_v2\")\n        print(f\"Fine tuning progress: {progress}\")\n\n        if fine_tuning_state == \"fine_tuned\" or fine_tuning_state == \"failed\":\n            print(\"Fine tuning completed or failed\")\n            break\n    # Wait for 5 seconds before polling again\n    time.sleep(5)\n```\n\nLANGUAGE: typescript\nCODE:\n```\nawait elevenlabs.voices.pvc.train(voiceId, {\n    // Specify the model the PVC should be trained on\n    model_id: \"eleven_multilingual_v2\",\n});\n\n// Poll the fine tuning status until it is complete or fails\n// This example specifically checks for the eleven_multilingual_v2 model\nconst interval = setInterval(async () => {\n    const { fine_tuning: fineTuning } = await elevenlabs.voices.get(voiceId);\n    if (!fineTuning) return;\n\n    console.log(`Fine tuning progress: ${fineTuning?.progress?.eleven_multilingual_v2}`);\n\n    if (fineTuning?.state?.eleven_multilingual_v2 === \"fine_tuned\" || fineTuning?.state?.eleven_multilingual_v2 === \"failed\") {\n        clearInterval(interval);\n        console.log(\"Fine tuning completed or failed\");\n    }\n}, 5000);\n```\n\n----------------------------------------\n\nTITLE: Making an Authenticated API Request with curl (Bash)\nDESCRIPTION: Demonstrates using `curl` to make a GET request to the `/v1/models` endpoint of the ElevenLabs API. It includes the necessary `Content-Type` and `xi-api-key` headers for authentication. Ensure `$ELEVENLABS_API_KEY` is replaced with your valid API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl 'https://api.elevenlabs.io/v1/models' \\\n  -H 'Content-Type: application/json' \\\n  -H 'xi-api-key: $ELEVENLABS_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs SDK\nDESCRIPTION: Commands to install the ElevenLabs SDK for Python and JavaScript environments.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elevenlabs\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Embedding ElevenLabs Conversational AI Widget in Ghost - HTML\nDESCRIPTION: This snippet provides the HTML and JavaScript required to embed the ElevenLabs Conversational AI widget on a Ghost website. It includes the <elevenlabs-convai> custom element with a required agent-id parameter (which should be replaced by your actual agent ID), and a script tag to asynchronously load the widget JavaScript from ElevenLabs servers. No additional dependencies are needed beyond including this snippet in either a Code Injection footer (for global use) or a specific HTML block in Ghost. Inputs: your agent ID. Output: a fully functional conversational agent widget rendered on the page.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/ghost.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Creating CSV File for Manual Dubbing with Hours:Minutes:Seconds,Milliseconds Format\nDESCRIPTION: Example CSV file for manual dubbing using the hours:minutes:seconds,milliseconds format. The file includes speaker names, precise timecodes, original transcription, and translation text.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/products/dubbing/dubbing-studio.mdx#_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\nspeaker,start_time,end_time,transcription,translation\nAdam,\"0:00:01,000\",\"0:00:05,000\",\"Hello, how are you?\",\"Hola, ¿cómo estás?\"\nAdam,\"0:00:06,000\",\"0:00:10,000\",\"I'm fine, thank you.\",\"Estoy bien, gracias.\"\n\n```\n\n----------------------------------------\n\nTITLE: Executing the TypeScript Speech-to-Text Script\nDESCRIPTION: This command uses `npx tsx` to execute the TypeScript script (`example.mts`) for speech-to-text conversion via the ElevenLabs API. Requires Node.js, npm/npx, and the `tsx`, `elevenlabs`, `dotenv` packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/quickstart.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Conversation Initiation Client Data Structure in JSON\nDESCRIPTION: This JSON structure defines what can be customized when starting a conversation with an ElevenLabs agent. It includes configuration overrides for system prompts, first messages, language and voice settings, custom LLM parameters, and dynamic variables of different data types.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/personalization.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"conversation_initiation_client_data\",\n  \"conversation_config_override\": {\n    \"agent\": {\n      \"prompt\": {\n        \"prompt\": \"overriding system prompt\"\n      },\n      \"first_message\": \"overriding first message\", \n      \"language\": \"en\" \n    },\n    \"tts\": {\n      \"voice_id\": \"voice-id-here\" \n    }\n  },\n  \"custom_llm_extra_body\": {\n      \"temperature\": 0.7, \n      \"max_tokens\": 100 \n  },\n  \"dynamic_variables\": {\n    \"string_var\": \"text value\",\n    \"number_var\": 1.2,\n    \"integer_var\": 123,\n    \"boolean_var\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Conversation Status State Variable in React using useConversation Hook\nDESCRIPTION: Provides the current connection status of the conversation as a React state variable. The status values can be \"connected\" or \"disconnected\", allowing UI components to reactively respond to connection changes.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst { status } = useConversation();\nconsole.log(status); // \"connected\" or \"disconnected\"\n```\n\n----------------------------------------\n\nTITLE: Adding ElevenLabs Conversational AI Widget Script - HTML\nDESCRIPTION: This HTML script tag loads the necessary JavaScript library that enables the <elevenlabs-convai> custom element to function. It should be placed in the global \"Footer Code\" section of your Webflow Project Settings to ensure the script is loaded asynchronously across all pages, making the widget functional wherever the element is placed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/webflow.mdx#_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Concatenating audio segments with Pydub for text conditioning\nDESCRIPTION: This snippet demonstrates how to use Pydub to concatenate multiple audio segments generated from different text chunks. It uses 'previous_text' and 'next_text' parameters to give context to the model, improving prosody consistency across chunks.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/request-stitching.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport requests\nfrom pydub import AudioSegment\nimport io\n\nYOUR_XI_API_KEY = \"<insert your xi-api-key here>\"\nVOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel\nPARAGRAPHS = [\n    \"The advent of technology has transformed countless sectors, with education \"\n    \"standing out as one of the most significantly impacted fields.\",\n    \"In recent years, educational technology, or EdTech, has revolutionized the way \"\n    \"teachers deliver instruction and students absorb information.\",\n    \"From interactive whiteboards to individual tablets loaded with educational software, \"\n    \"technology has opened up new avenues for learning that were previously unimaginable.\",\n    \"One of the primary benefits of technology in education is the accessibility it provides.\",\n]\nsegments = []\n\nfor i, paragraph in enumerate(PARAGRAPHS):\n    is_last_paragraph = i == len(PARAGRAPHS) - 1\n    is_first_paragraph = i == 0\n    response = requests.post(\n        f\"https://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream\",\n        json={\n            \"text\": paragraph,\n            \"model_id\": \"eleven_multilingual_v2\",\n            \"previous_text\": None if is_first_paragraph else \" \".join(PARAGRAPHS[:i]),\n            \"next_text\": None if is_last_paragraph else \" \".join(PARAGRAPHS[i + 1:])\n        },\n        headers={\"xi-api-key\": YOUR_XI_API_KEY},\n    )\n\n    if response.status_code != 200:\n        print(f\"Error encountered, status: {response.status_code}, \"\n               f\"content: {response.text}\")\n        quit()\n\n    print(f\"Successfully converted paragraph {i + 1}/{len(PARAGRAPHS)}\")\n    segments.append(AudioSegment.from_mp3(io.BytesIO(response.content)))\n\nsegment = segments[0]\nfor new_segment in segments[1:]:\n    segment = segment + new_segment\n\naudio_out_path = os.path.join(os.getcwd(), \"with_text_conditioning.wav\")\nsegment.export(audio_out_path, format=\"wav\")\nprint(f\"Success! Wrote audio to {audio_out_path}\")\n```\n\n----------------------------------------\n\nTITLE: Embedding Audio Native widget HTML code in Wix\nDESCRIPTION: Provides the HTML code snippet to embed the Audio Native player within a Wix blog post. Includes the container div with data attributes for configuration and a script tag to load helper functions. Dependencies include the external script from Elevenlabs and proper IDs and URLs for project-specific setup.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/wix.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Embedding Audio Native Player Container in Framer\nDESCRIPTION: HTML div element that serves as the container for the ElevenLabs Audio Native player. Place this code within an Embed Element (HTML type) on your Framer page, replacing 'public-user-id' and 'project-id' with your specific values.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/framer.mdx#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configure Supabase Storage Bucket (TOML)\nDESCRIPTION: Defines configuration for a Supabase storage bucket named 'audio' within the `supabase/config.toml` file. It sets the bucket to private, specifies a file size limit, defines allowed MIME types (audio/mp3), and sets the local path for objects. This configuration is applied when running `supabase start` locally.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[storage.buckets.audio]\npublic = false\nfile_size_limit = \"50MiB\"\nallowed_mime_types = [\"audio/mp3\"]\nobjects_path = \"./audio\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Zero Retention Mode - Python\nDESCRIPTION: This Python code snippet demonstrates how to use the ElevenLabs API in Zero Retention Mode. It sets the `enable_logging` parameter to `False` when calling the `text_to_speech.convert` method. This configuration prevents logging of sensitive data like input text and audio output. Requires the `elevenlabs` library and a valid API key. The output is an audio file, but no data related to the generation is stored long term.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python title=\"Python\" {12}\nfrom elevenlabs import ElevenLabs\n\nclient = ElevenLabs(\n  api_key=\"YOUR_API_KEY\",\n)\n\nresponse = client.text_to_speech.convert(\n  voice_id=voice_id,\n  output_format=\"mp3_22050_32\",\n  text=text,\n  model_id=\"eleven_turbo_v2\",\n  enable_logging=False,\n)\n\n```\n```\n\n----------------------------------------\n\nTITLE: Python code for dubbing audio using ElevenLabs API\nDESCRIPTION: This Python script loads environment variables, creates an API client, fetches an audio file, initiates dubbing from English to Spanish, polls for completion, and plays the dubbed audio. Dependencies include dotenv, requests, elevenlabs SDK, and time. The script handles file reading, API interaction, and audio playback, demonstrating synchronous workflow for dubbing tasks.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/quickstart.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport requests\nfrom io import BytesIO\nimport time\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\ntarget_lang = \"es\"  # Spanish\n\naudio_url = (\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n)\nresponse = requests.get(audio_url)\n\naudio_data = BytesIO(response.content)\naudio_data.name = \"audio.mp3\"\n\n# Start dubbing\ndistributed = client.dubbing.dub_a_video_or_an_audio_file(\n    file=audio_data, target_lang=target_lang\n)\n\nwhile True:\n    status = client.dubbing.get_dubbing_project_metadata(dubbed.dubbing_id).status\n    if status == \"dubbed\":\n        dubbed_file = client.dubbing.get_dubbed_file(dubbed.dubbing_id, target_lang)\n        play(dubbed_file)\n        break\n    else:\n        print(\"Audio is still being dubbed...\")\n        time.sleep(5)\n\n```\n\n----------------------------------------\n\nTITLE: Isolating Audio using Voice Isolator API - TypeScript\nDESCRIPTION: This TypeScript code snippet demonstrates how to remove background noise from an audio file using the ElevenLabs Voice Isolator API. It uses the `elevenlabs` library to interact with the API.  It also relies on the `dotenv` package to load environment variables. The `play` function requires MPV and/or ffmpeg to be installed to play the isolated audio.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst audioUrl =\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/fin.mp3\";\nconst response = await fetch(audioUrl);\nconst audioBlob = new Blob([await response.arrayBuffer()], {\n  type: \"audio/mp3\",\n});\n\nconst audioStream = await client.audioIsolation.audioIsolation({\n  audio: audioBlob,\n});\n\nawait play(audioStream);\n```\n\n----------------------------------------\n\nTITLE: Defining Pronunciation Alias in PLS Dictionary\nDESCRIPTION: Illustrates the use of the `<alias>` tag within a `<lexeme>` element in a Pronunciation Lexicon Specification (PLS) XML file. This method allows mapping a written word (`<grapheme>`) to an alternative word or phrase (`<alias>`) that the model will substitute and pronounce instead. Useful for ensuring consistent pronunciation of acronyms or difficult names.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_5\n\nLANGUAGE: XML\nCODE:\n```\n  <lexeme>\n    <grapheme>Claughton</grapheme>\n    <alias>Cloffton</alias>\n  </lexeme>\n```\n\n----------------------------------------\n\nTITLE: Verifying PVC Ownership via CAPTCHA Recording (Python/TypeScript)\nDESCRIPTION: Submits an audio recording of the voice owner reading the previously obtained CAPTCHA text to verify voice ownership for a PVC. Requires the `voice_id` and the path to the audio recording file (e.g., 'recording.mp3'). The recording file should be opened as a readable stream or binary file object.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nelevenlabs.voices.pvc.verification.captcha.verify(\n    voice_id=voice.voice_id,\n    recording=open('path/to/recording.mp3', 'rb')\n)\n```\n\nLANGUAGE: typescript\nCODE:\n```\nawait elevenlabs.voices.pvc.verification.captcha.verify(voice.voice_id, {\n\trecording: fs.createReadStream(\"/path/to/recording.mp3\"),\n})\n```\n\n----------------------------------------\n\nTITLE: Performing Speech-to-Text Transcription with ElevenLabs and Logging to Supabase in TypeScript\nDESCRIPTION: Defines an asynchronous `scribe` function executed as a background task. It initializes ElevenLabs and Supabase clients using API keys from environment variables. The function fetches the media file, converts it to text using the ElevenLabs `speechToText.convert` API, sends the transcript back to the user via the Telegram Bot API, and logs the transaction details (including file metadata, user info, transcript/error) to a Supabase table named 'transcription_logs'.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: Deno.env.get('ELEVENLABS_API_KEY') || '',\n});\n\nconst supabase = createClient(\n  Deno.env.get('SUPABASE_URL') || '',\n  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') || ''\n);\n\nasync function scribe({\n  fileURL,\n  fileType,\n  duration,\n  chatId,\n  messageId,\n  username,\n}: {\n  fileURL: string;\n  fileType: string;\n  duration: number;\n  chatId: number;\n  messageId: number;\n  username: string;\n}) {\n  let transcript: string | null = null;\n  let languageCode: string | null = null;\n  let errorMsg: string | null = null;\n  try {\n    const sourceFileArrayBuffer = await fetch(fileURL).then((res) => res.arrayBuffer());\n    const sourceBlob = new Blob([sourceFileArrayBuffer], {\n      type: fileType,\n    });\n\n    const scribeResult = await elevenLabsClient.speechToText.convert({\n      file: sourceBlob,\n      model_id: 'scribe_v1', // 'scribe_v1_experimental' is also available for new, experimental features\n      tag_audio_events: false,\n    });\n\n    transcript = scribeResult.text;\n    languageCode = scribeResult.language_code;\n\n    // Reply to the user with the transcript\n    await bot.api.sendMessage(chatId, transcript, {\n      reply_parameters: { message_id: messageId },\n    });\n  } catch (error) {\n    errorMsg = error.message;\n    console.log(errorMsg);\n    await bot.api.sendMessage(chatId, 'Sorry, there was an error. Please try again.', {\n      reply_parameters: { message_id: messageId },\n    });\n  }\n  // Write log to Supabase.\n  const logLine = {\n    file_type: fileType,\n    duration,\n    chat_id: chatId,\n    message_id: messageId,\n    username,\n    language_code: languageCode,\n    error: errorMsg,\n  };\n  console.log({ logLine });\n  await supabase.from('transcription_logs').insert({ ...logLine, transcript });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Text to Speech Audio File with ElevenLabs SDK in TypeScript\nDESCRIPTION: This TypeScript snippet asynchronously converts text to a speech audio file and saves it locally. It uses the ElevenLabs SDK client with an API key loaded from environment variables, streams the response to a writable file stream, and resolves the promise once the file is saved. Optional voice settings allow customization of the speech output. Requires 'elevenlabs', 'dotenv', 'fs', and 'uuid' packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as dotenv from 'dotenv';\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { createWriteStream } from 'fs';\nimport { v4 as uuid } from 'uuid';\n\ndotenv.config();\n\nconst ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;\n\nconst client = new ElevenLabsClient({\n  apiKey: ELEVENLABS_API_KEY,\n});\n\nexport const createAudioFileFromText = async (text: string): Promise<string> => {\n  return new Promise<string>(async (resolve, reject) => {\n    try {\n      const audio = await client.textToSpeech.convert('JBFqnCBsd6RMkjVDRZzb', {\n        model_id: 'eleven_multilingual_v2',\n        text,\n        output_format: 'mp3_44100_128',\n        // Optional voice settings that allow you to customize the output\n        voice_settings: {\n          stability: 0,\n          similarity_boost: 0,\n          use_speaker_boost: true,\n          speed: 1.0,\n        },\n      });\n\n      const fileName = `${uuid()}.mp3`;\n      const fileStream = createWriteStream(fileName);\n\n      audio.pipe(fileStream);\n      fileStream.on('finish', () => resolve(fileName)); // Resolve with the fileName\n      fileStream.on('error', reject);\n    } catch (error) {\n      reject(error);\n    }\n  });\n};\n```\n\n----------------------------------------\n\nTITLE: Using Alias Tags for Acronym Expansion in ElevenLabs Lexicons (XML)\nDESCRIPTION: Shows how to use `<lexeme>` and `<alias>` tags in a pronunciation dictionary (.pls format) to expand an acronym (\"UN\") into its full form (\"United Nations\") whenever encountered in the text processed by ElevenLabs TTS.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n```\n\n----------------------------------------\n\nTITLE: Incorrect Stress Marking in CMU Arpabet Phoneme Tags (XML)\nDESCRIPTION: Shows an incorrect example of a CMU Arpabet phoneme tag for \"pronunciation\" lacking proper stress markers, which can lead to inaccurate TTS output in ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"P R AH N AH N S IY EY SH AH N\">\n  pronunciation\n</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Passing Dynamic Variables at Runtime - ElevenLabs HTML Widget\nDESCRIPTION: This HTML snippet demonstrates passing dynamic variables to an ElevenLabs conversational agent using a custom <elevenlabs-convai> web component. The agent ID and dynamic-variables are set as attributes, with dynamic-variables as a JSON string holding user-specific key-value pairs. This widget requires inclusion of the official ElevenLabs conversational AI widget JS bundle. Inputs are provided via HTML attributes; outputs are rendered by the widget automatically. This approach is suitable for no-code or low-code environments seeking quick integration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables.mdx#_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai\n  agent-id=\"your-agent-id\"\n  dynamic-variables='{\"user_name\": \"John\", \"account_type\": \"premium\"}'\n></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Initializing ElevenLabs, Supabase\nDESCRIPTION: This code initializes the ElevenLabs client with an API key and the Supabase client using URL and service role key fetched from the environment variables. These clients are used for calling the speech-to-text API and interacting with the Supabase database, respectively.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(`Function \"elevenlabs-scribe-bot\" up and running!`);\n\nconst elevenLabsClient = new ElevenLabsClient({\n  apiKey: Deno.env.get('ELEVENLABS_API_KEY') || '',\n});\n\nconst supabase = createClient(\n  Deno.env.get('SUPABASE_URL') || '',\n  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') || ''\n);\n\n```\n\n----------------------------------------\n\nTITLE: Execute Typescript Script\nDESCRIPTION: This command executes the Typescript script `example.mts` using the `tsx` command.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Initialize Supabase Project Locally (Bash)\nDESCRIPTION: Initializes a new Supabase project in the current directory using the Supabase CLI. This command creates the necessary configuration files and directories for local Supabase development.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsupabase init\n```\n\n----------------------------------------\n\nTITLE: Uploading ElevenLabs PVC Audio Samples (Python)\nDESCRIPTION: Uploads audio sample files to a previously created PVC voice. Defines a list of file paths, reads the files into memory using `BytesIO` managed by `ExitStack`, and passes the list of file objects to the SDK's create samples method. Requires the `os`, `contextlib`, and `io` modules.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define the list of file paths explicitly\n# Replace with the paths to your audio and/or video files.\n# The more files you add, the better the clone will be.\nsample_file_paths = [\n    \"/path/to/your/first_sample.mp3\",\n    \"/path/to/your/second_sample.wav\",\n    \"relative/path/to/another_sample.mp4\"\n]\n\nsamples = None\n\nfiles_to_upload = []\n# Use ExitStack to manage multiple open files\nwith ExitStack() as stack:\n            for filepath in sample_file_paths:\n                # Open each file and add it to the stack\n                audio_file = stack.enter_context(open(filepath, \"rb\"))\n                filename = os.path.basename(filepath)\n\n                # Create a File object for the SDK\n                files_to_upload.append(\n                    BytesIO(audio_file.read())\n                )\n\n            samples = elevenlabs.voices.pvc.samples.create(\n                voice_id=voice.voice_id,\n                files=files_to_upload # Pass the list of File objects\n            )\n```\n\n----------------------------------------\n\nTITLE: Handling Incoming Webhook Requests with Deno Serve in TypeScript\nDESCRIPTION: Initializes a Deno HTTP server using `Deno.serve`. The server handler validates incoming requests by checking a 'secret' query parameter against an environment variable (`FUNCTION_SECRET`). Valid requests are processed by the `handleUpdate` function, which is generated by `grammY`'s `webhookCallback` adapter for standard HTTP.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst handleUpdate = webhookCallback(bot, 'std/http');\n\nDeno.serve(async (req) => {\n  try {\n    const url = new URL(req.url);\n    if (url.searchParams.get('secret') !== Deno.env.get('FUNCTION_SECRET')) {\n      return new Response('not allowed', { status: 405 });\n    }\n\n    return await handleUpdate(req);\n  } catch (err) {\n    console.error(err);\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Adding the ConvAI Component to your App - TypeScript\nDESCRIPTION: This code shows how to integrate the `ConvAiDOMComponent` into the main `App.tsx` file of an Expo React Native application. It imports the component and necessary tools and passes the platform and tool functions as props. This allows the AI agent to run and interact with the device's features based on the provided platform.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { LinearGradient } from 'expo-linear-gradient';\nimport { StatusBar } from 'expo-status-bar';\nimport { View, Text, StyleSheet, SafeAreaView } from 'react-native';\nimport { Platform } from 'react-native';\n\nimport ConvAiDOMComponent from './components/ConvAI';\nimport tools from './utils/tools';\n\nexport default function App() {\n  return (\n    <SafeAreaView style={styles.container}>\n      <LinearGradient colors={['#0F172A', '#1E293B']} style={StyleSheet.absoluteFill} />\n\n      <View style={styles.topContent}>\n        <Text style={styles.description}>\n          Cross-platform conversational AI agents with ElevenLabs and Expo React Native.\n        </Text>\n\n        <View style={styles.toolsList}>\n          <Text style={styles.toolsTitle}>Available Client Tools:</Text>\n          <View style={styles.toolItem}>\n            <Text style={styles.toolText}>Get battery level</Text>\n            <View style={styles.platformTags}>\n              <Text style={styles.platformTag}>web</Text>\n              <Text style={styles.platformTag}>ios</Text>\n              <Text style={styles.platformTag}>android</Text>\n            </View>\n          </View>\n          <View style={styles.toolItem}>\n            <Text style={styles.toolText}>Change screen brightness</Text>\n            <View style={styles.platformTags}>\n              <Text style={styles.platformTag}>ios</Text>\n              <Text style={styles.platformTag}>android</Text>\n            </View>\n          </View>\n          <View style={styles.toolItem}>\n            <Text style={styles.toolText}>Flash screen</Text>\n            <View style={styles.platformTags}>\n              <Text style={styles.platformTag}>ios</Text>\n              <Text style={styles.platformTag}>android</Text>\n            </View>\n          </View>\n        </View>\n        <View style={styles.domComponentContainer}>\n          <ConvAiDOMComponent\n            dom={{ style: styles.domComponent }}\n            platform={Platform.OS}\n            get_battery_level={tools.get_battery_level}\n            change_brightness={tools.change_brightness}\n            flash_screen={tools.flash_screen}\n          />\n        </View>\n      </View>\n      <StatusBar style=\"light\" />\n    </SafeAreaView>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Configure Supabase Edge Function Runtime Policy (TOML)\nDESCRIPTION: Sets the Edge Function runtime policy to 'per_worker' within the `supabase/config.toml` file. This policy is necessary to enable background tasks using `EdgeRuntime.waitUntil` during local development, but it disables automatic function reloading upon code edits.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\n[edge_runtime]\npolicy = \"per_worker\"\n```\n\n----------------------------------------\n\nTITLE: Initializing ElevenLabs Client\nDESCRIPTION: This code snippet initializes the ElevenLabs client using the API key stored in the environment variables. It imports necessary modules from the ElevenLabs library and retrieves the API key from the `.env` file to authenticate the client.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom elevenlabs.client import ElevenLabs\n\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\nclient = ElevenLabs(\n    api_key=ELEVENLABS_API_KEY,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating CSV File for Manual Dubbing with Seconds Format\nDESCRIPTION: Example CSV file for manual dubbing using seconds format. The file includes speaker names, start/end times, original transcription, and translation text.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/products/dubbing/dubbing-studio.mdx#_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nspeaker,start_time,end_time,transcription,translation\nAdam,\"0.10000\",\"1.15000\",\"Hello, how are you?\",\"Hola, ¿cómo estás?\"\nAdam,\"1.50000\",\"3.50000\",\"I'm fine, thank you.\",\"Estoy bien, gracias.\"\n\n```\n\n----------------------------------------\n\nTITLE: Creating Supabase Project Locally\nDESCRIPTION: This bash command initializes a new Supabase project locally using the Supabase CLI. It sets up the necessary files and directories for local development.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsupabase init\n```\n\n----------------------------------------\n\nTITLE: Executing the Python Voice Isolator Script\nDESCRIPTION: This snippet shows how to execute the Python script `example.py`. This command invokes the Python interpreter to run the code, which will then download the audio, process it through the Voice Isolator API, and play the resulting isolated audio.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: scribe Function: Transcription Logic\nDESCRIPTION: This function performs the core transcription logic. It fetches the file from a given URL, uses the ElevenLabs API to transcribe the audio, and sends the transcription result back to the user via Telegram. It also logs the transcription details to the Supabase database, including error handling and logging.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nasync function scribe({\n  fileURL,\n  fileType,\n  duration,\n  chatId,\n  messageId,\n  username,\n}: {\n  fileURL: string;\n  fileType: string;\n  duration: number;\n  chatId: number;\n  messageId: number;\n  username: string;\n}) {\n  let transcript: string | null = null;\n  let languageCode: string | null = null;\n  let errorMsg: string | null = null;\n  try {\n    const sourceFileArrayBuffer = await fetch(fileURL).then((res) => res.arrayBuffer());\n    const sourceBlob = new Blob([sourceFileArrayBuffer], {\n      type: fileType,\n    });\n\n    const scribeResult = await elevenLabsClient.speechToText.convert({\n      file: sourceBlob,\n      model_id: 'scribe_v1', // 'scribe_v1_experimental' is also available for new, experimental features\n      tag_audio_events: false,\n    });\n\n    transcript = scribeResult.text;\n    languageCode = scribeResult.language_code;\n\n    // Reply to the user with the transcript\n    await bot.api.sendMessage(chatId, transcript, {\n      reply_parameters: { message_id: messageId },\n    });\n  } catch (error) {\n    errorMsg = error.message;\n    console.log(errorMsg);\n    await bot.api.sendMessage(chatId, 'Sorry, there was an error. Please try again.', {\n      reply_parameters: { message_id: messageId },\n    });\n  }\n  // Write log to Supabase.\n  const logLine = {\n    file_type: fileType,\n    duration,\n    chat_id: chatId,\n    message_id: messageId,\n    username,\n    language_code: languageCode,\n    error: errorMsg,\n  };\n  console.log({ logLine });\n  await supabase.from('transcription_logs').insert({ ...logLine, transcript });\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Podcasts Programmatically via API\nDESCRIPTION: Adds the POST `v1/projects/podcast/create` endpoint for generating podcasts programmatically using the GenFM engine. Supports features like conversation mode, URL sourcing, custom duration, and webhooks.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_1\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /v1/projects/podcast/create\n```\n\n----------------------------------------\n\nTITLE: Updating Interrupted Agent Responses - ElevenLabs Client Events - JavaScript\nDESCRIPTION: Describes the 'agent_response_correction' event sent when an ongoing agent response is truncated due to interruption or correction. It includes both the original full response and the corrected truncated text. The example handler updates the displayed message to keep the conversation accurate, handling dynamic interruptions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"agent_response_correction\",\n  \"agent_response_correction_event\": {\n    \"original_agent_response\": \"Let me tell you about the complete history...\",\n    \"corrected_agent_response\": \"Let me tell you about...\"  // Truncated after interruption\n  }\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nwebsocket.on('agent_response_correction', (event) => {\n  const { agent_response_correction_event } = event;\n  const { corrected_agent_response } = agent_response_correction_event;\n  displayAgentMessage(corrected_agent_response);\n});\n```\n\n----------------------------------------\n\nTITLE: Showcasing ElevenLabs Capabilities with Responsive React Cards\nDESCRIPTION: This JSX snippet presents multiple capability cards using CardGroup and individual Card components with two columns layout. Each Card links to a specific capability page and contains a flexbox layout with a title, subtitle, and descriptive text about a feature like text-to-speech, speech-to-text, voice changer, dubbing, and more. The snippet uses semantic HTML and utility CSS classes for typography and alignment. It depends on CardGroup and Card React components and React JSX syntax to enable a rich, responsive UI overview of product features.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#_snippet_3\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={2}>\n\n<Card href=\"/docs/capabilities/text-to-speech\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Text to Speech</div>\n      <p>Convert text into lifelike speech</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/capabilities/speech-to-text\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Speech to Text</div>\n      <p>Transcribe spoken audio into text</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/capabilities/voice-changer\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Voice changer</div>\n      <p>Modify and transform voices</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/capabilities/voice-isolator\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Voice isolator</div>\n      <p>Isolate voices from background noise</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/capabilities/dubbing\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Dubbing</div>\n      <p>Dub audio and videos seamlessly</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/capabilities/sound-effects\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Sound effects</div>\n      <p>Create cinematic sound effects</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/capabilities/voices\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Voices</div>\n      <p>Clone and design custom voices</p>\n    </div>\n  </div>\n</Card>\n\n<Card href=\"/docs/conversational-ai/overview\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Conversational AI</div>\n      <p>Deploy intelligent voice agents</p>\n    </div>\n  </div>\n</Card>\n\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Enhanced Custom LLM Server with Extra Parameters Support in Python\nDESCRIPTION: An improved version of the custom LLM server that handles additional parameters passed from ElevenLabs through the extra_body field, allowing for customized LLM behavior based on client-specific data.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\nimport fastapi\nfrom fastapi.responses import StreamingResponse\nfrom fastapi import Request\nfrom openai import AsyncOpenAI\nimport uvicorn\nimport logging\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Retrieve API key from environment\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nif not OPENAI_API_KEY:\n    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n\napp = fastapi.FastAPI()\noai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n\nclass Message(BaseModel):\n    role: str\n    content: str\n\nclass ChatCompletionRequest(BaseModel):\n    messages: List[Message]\n    model: str\n    temperature: Optional[float] = 0.7\n    max_tokens: Optional[int] = None\n    stream: Optional[bool] = False\n    user_id: Optional[str] = None\n    elevenlabs_extra_body: Optional[dict] = None\n\n@app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(request: ChatCompletionRequest) -> StreamingResponse:\n    oai_request = request.dict(exclude_none=True)\n    print(oai_request)\n    if \"user_id\" in oai_request:\n        oai_request[\"user\"] = oai_request.pop(\"user_id\")\n\n    if \"elevenlabs_extra_body\" in oai_request:\n        oai_request.pop(\"elevenlabs_extra_body\")\n\n    chat_completion_coroutine = await oai_client.chat.completions.create(**oai_request)\n\n    async def event_stream():\n        try:\n            async for chunk in chat_completion_coroutine:\n                chunk_dict = chunk.model_dump()\n                yield f\"data: {json.dumps(chunk_dict)}\\n\\n\"\n            yield \"data: [DONE]\\n\\n\"\n        except Exception as e:\n            logging.error(\"An error occurred: %s\", str(e))\n            yield f\"data: {json.dumps({'error': 'Internal error occurred!'})}\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8013)\n```\n\n----------------------------------------\n\nTITLE: Listing Supported Languages in ElevenLabs Scribe v1\nDESCRIPTION: This snippet lists the supported 99 languages available in the Scribe v1 model, including language codes and names, serving as a reference for language selection in speech synthesis applications.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/v1-scribe-model-languages.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsupported_languages = {\n    \"Afrikaans\": \"afr\",\n    \"Amharic\": \"amh\",\n    \"Arabic\": \"ara\",\n    \"Armenian\": \"hye\",\n    \"Assamese\": \"asm\",\n    \"Asturian\": \"ast\",\n    \"Azerbaijani\": \"aze\",\n    \"Belarusian\": \"bel\",\n    \"Bengali\": \"ben\",\n    \"Bosnian\": \"bos\",\n    \"Bulgarian\": \"bul\",\n    \"Burmese\": \"mya\",\n    \"Cantonese\": \"yue\",\n    \"Catalan\": \"cat\",\n    \"Cebuano\": \"ceb\",\n    \"Chichewa\": \"nya\",\n    \"Croatian\": \"hrv\",\n    \"Czech\": \"ces\",\n    \"Danish\": \"dan\",\n    \"Dutch\": \"nld\",\n    \"English\": \"eng\",\n    \"Estonian\": \"est\",\n    \"Filipino\": \"fil\",\n    \"Finnish\": \"fin\",\n    \"French\": \"fra\",\n    \"Fulah\": \"ful\",\n    \"Galician\": \"glg\",\n    \"Ganda\": \"lug\",\n    \"Georgian\": \"kat\",\n    \"German\": \"deu\",\n    \"Greek\": \"ell\",\n    \"Gujarati\": \"guj\",\n    \"Hausa\": \"hau\",\n    \"Hebrew\": \"heb\",\n    \"Hindi\": \"hin\",\n    \"Hungarian\": \"hun\",\n    \"Icelandic\": \"isl\",\n    \"Igbo\": \"ibo\",\n    \"Indonesian\": \"ind\",\n    \"Irish\": \"gle\",\n    \"Italian\": \"ita\",\n    \"Japanese\": \"jpn\",\n    \"Javanese\": \"jav\",\n    \"Kabuverdianu\": \"kea\",\n    \"Kannada\": \"kan\",\n    \"Kazakh\": \"kaz\",\n    \"Khmer\": \"khm\",\n    \"Korean\": \"kor\",\n    \"Kurdish\": \"kur\",\n    \"Kyrgyz\": \"kir\",\n    \"Lao\": \"lao\",\n    \"Latvian\": \"lav\",\n    \"Lingala\": \"lin\",\n    \"Lithuanian\": \"lit\",\n    \"Luo\": \"luo\",\n    \"Luxembourgish\": \"ltz\",\n    \"Macedonian\": \"mkd\",\n    \"Malay\": \"msa\",\n    \"Malayalam\": \"mal\",\n    \"Maltese\": \"mlt\",\n    \"Mandarin Chinese\": \"cmn\",\n    \"Māori\": \"mri\",\n    \"Marathi\": \"mar\",\n    \"Mongolian\": \"mon\",\n    \"Nepali\": \"nep\",\n    \"Northern Sotho\": \"nso\",\n    \"Norwegian\": \"nor\",\n    \"Occitan\": \"oci\",\n    \"Odia\": \"ori\",\n    \"Pashto\": \"pus\",\n    \"Persian\": \"fas\",\n    \"Polish\": \"pol\",\n    \"Portuguese\": \"por\",\n    \"Punjabi\": \"pan\",\n    \"Romanian\": \"ron\",\n    \"Russian\": \"rus\",\n    \"Serbian\": \"srp\",\n    \"Shona\": \"sna\",\n    \"Sindhi\": \"snd\",\n    \"Slovak\": \"slk\",\n    \"Slovenian\": \"slv\",\n    \"Somali\": \"som\",\n    \"Spanish\": \"spa\",\n    \"Swahili\": \"swa\",\n    \"Swedish\": \"swe\",\n    \"Tamil\": \"tam\",\n    \"Tajik\": \"tgk\",\n    \"Telugu\": \"tel\",\n    \"Thai\": \"tha\",\n    \"Turkish\": \"tur\",\n    \"Ukrainian\": \"ukr\",\n    \"Umbundu\": \"umb\",\n    \"Urdu\": \"urd\",\n    \"Uzbek\": \"uzb\",\n    \"Vietnamese\": \"vie\",\n    \"Welsh\": \"cym\",\n    \"Wolof\": \"wol\",\n    \"Xhosa\": \"xho\",\n    \"Zulu\": \"zul\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Studio Chapter Content via API\nDESCRIPTION: Adds the PATCH `v1/projects/:project_id/chapters/:chapter_id` endpoint to allow programmatic updates to the content and metadata of specific chapters within a Studio project.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_3\n\nLANGUAGE: HTTP\nCODE:\n```\nPATCH /v1/projects/:project_id/chapters/:chapter_id\n```\n\n----------------------------------------\n\nTITLE: Installing pnpm\nDESCRIPTION: This command installs pnpm, a package manager, globally. pnpm is used for managing project dependencies. Ensure you have npm installed before running this command. This is a prerequisite for installing the project's dependencies.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install -g pnpm\n```\n\n----------------------------------------\n\nTITLE: Enabling Zero Retention Mode - JavaScript\nDESCRIPTION: This JavaScript code snippet shows how to enable Zero Retention Mode using the ElevenLabs JavaScript client library. It calls the `client.textToSpeech.convert` method and sets `enable_logging` to `false`.  This prevents the logging of TTS request and response data.  Requires the ElevenLabs client library and a valid API key. Output is audio; no data is saved.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/resources/zero-retention.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n```javascript title=\"JavaScript\" {9}\nimport { ElevenLabsClient } from 'elevenlabs';\n\nconst client = new ElevenLabsClient({ apiKey: 'YOUR_API_KEY' });\n\nawait client.textToSpeech.convert(voiceId, {\n  output_format: 'mp3_44100_128',\n  text: text,\n  model_id: 'eleven_turbo_v2',\n  enable_logging: false,\n});\n```\n```\n\n----------------------------------------\n\nTITLE: Forced Alignment API Request\nDESCRIPTION: This TypeScript snippet demonstrates how to use the ElevenLabs SDK to perform forced alignment.  It imports necessary modules, initializes the ElevenLabs client, fetches audio data from a URL, creates a Blob from the audio data, and calls the `forcedAlignment.create` method to generate a transcription with timestamps. The transcription is then logged to the console. Requires the `elevenlabs` and `dotenv` packages. Assumes usage with `tsx`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.ts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst transcript = await client.forcedAlignment.create({\n    file: audioBlob,\n    text: \"With a soft and whispery American accent, I'm the ideal choice for creating ASMR content, meditative guides, or adding an intimate feel to your narrative projects.\"\n})\n\nconsole.log(transcript);\n```\n\n----------------------------------------\n\nTITLE: Uploading Audio Stream to AWS S3 and Generating Presigned URL in TypeScript\nDESCRIPTION: Implements two asynchronous functions: uploadAudioStreamToS3 uploads a Buffer stream to S3 under a unique UUID .mp3 key, and generatePresignedUrl generates a temporary signed URL for reading the object. Depends on @aws-sdk/client-s3, @aws-sdk/s3-request-presigner, dotenv, and uuid, with AWS credentials defined in a dotenv-managed .env file. Functions require a stream (Buffer) input and return a key string or a URL. Proper AWS policy permissions are necessary for operation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';\nimport { getSignedUrl } from '@aws-sdk/s3-request-presigner';\nimport * as dotenv from 'dotenv';\nimport { v4 as uuid } from 'uuid';\n\ndotenv.config();\n\nconst { AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME, AWS_S3_BUCKET_NAME } =\n  process.env;\n\nif (!AWS_ACCESS_KEY_ID || !AWS_SECRET_ACCESS_KEY || !AWS_REGION_NAME || !AWS_S3_BUCKET_NAME) {\n  throw new Error('One or more environment variables are not set. Please check your .env file.');\n}\n\nconst s3 = new S3Client({\n  credentials: {\n    accessKeyId: AWS_ACCESS_KEY_ID,\n    secretAccessKey: AWS_SECRET_ACCESS_KEY,\n  },\n  region: AWS_REGION_NAME,\n});\n\nexport const generatePresignedUrl = async (objectKey: string) => {\n  const getObjectParams = {\n    Bucket: AWS_S3_BUCKET_NAME,\n    Key: objectKey,\n    Expires: 3600,\n  };\n  const command = new GetObjectCommand(getObjectParams);\n  const url = await getSignedUrl(s3, command, { expiresIn: 3600 });\n  return url;\n};\n\nexport const uploadAudioStreamToS3 = async (audioStream: Buffer) => {\n  const remotePath = `${uuid()}.mp3`;\n  await s3.send(\n    new PutObjectCommand({\n      Bucket: AWS_S3_BUCKET_NAME,\n      Key: remotePath,\n      Body: audioStream,\n      ContentType: 'audio/mpeg',\n    })\n  );\n  return remotePath;\n};\n```\n\n----------------------------------------\n\nTITLE: Generating Voice Previews with ElevenLabs Python SDK\nDESCRIPTION: Initializes the ElevenLabs client and calls the `create_previews` method with a voice description and text. It iterates through the returned previews, decodes the base64 audio, and plays each preview. Requires `dotenv`, `elevenlabs`, and `base64`. Expects API key via environment variable `ELEVENLABS_API_KEY`. Requires MPV and/or ffmpeg for audio playback.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport base64\nimport os\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\nvoices = client.text_to_voice.create_previews(\n    voice_description=\"A huge giant, at least as tall as a building. A deep booming voice, loud and jolly.\",\n    text=\"Greetings little human. I am a mighty giant from a far away land. Would you like me to tell you a story?\"\n)\n\nfor preview in voices.previews:\n    # Convert base64 to audio buffer\n    audio_buffer = base64.b64decode(preview.audio_base_64)\n\n    print(f\"Playing preview: {preview.generated_voice_id}\")\n\n    play(audio_buffer)\n\n```\n\n----------------------------------------\n\nTITLE: Starting the Frontend Development Server\nDESCRIPTION: Shell command to start the frontend development server for the ElevenLabs Conversational AI application.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nnpm run dev:frontend\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Group and Card JSX\nDESCRIPTION: This JSX code renders a card group component that contains a card. The card displays information about a speech recognition model, specifically \"Scribe v1\". The card showcases features like accurate transcription in multiple languages, precise timestamps, speaker diarization, and audio tagging. It uses `CardGroup` and `Card` components, presumable defined elsewhere in the project. There are no dependencies explicitly defined in this snippet, but it is likely that the CardGroup and Card components depend on a styling library (like tailwind or bootstrap).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/stt-models.mdx#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<CardGroup cols={1} rows={1}>\n  <Card title=\"Scribe v1\" href=\"/docs/models#scribe-v1\">\n    State-of-the-art speech recognition model\n    <div className=\"mt-4 space-y-2\">\n      <div className=\"text-sm\">Accurate transcription in 99 languages</div>\n      <div className=\"text-sm\">Precise word-level timestamps</div>\n      <div className=\"text-sm\">Speaker diarization</div>\n      <div className=\"text-sm\">Dynamic audio tagging</div>\n    </div>\n  </Card>\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Retrieving isSpeaking Boolean State to Indicate Agent Speaking Status in React\nDESCRIPTION: Offers a React state indicating whether the ElevenLabs AI agent is currently speaking. This boolean flag (true/false) helps applications visually or functionally adjust UI elements based on whether the agent is producing speech.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst { isSpeaking } = useConversation();\nconsole.log(isSpeaking); // boolean\n```\n\n----------------------------------------\n\nTITLE: Execute Python Script\nDESCRIPTION: This command executes the Python script `example.py` using the Python interpreter.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/forced-alignment/quickstart.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Adding ElevenLabs Conversational AI Widget Element - HTML\nDESCRIPTION: This specific HTML tag represents the visual and interactive component of the ElevenLabs Conversational AI widget. It is intended to be placed within an \"Embed Element\" directly on the desired Webflow page where the widget should appear. Replace YOUR_AGENT_ID with your agent's specific ID.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/webflow.mdx#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n```\n\n----------------------------------------\n\nTITLE: Updating OpenAPI Spec\nDESCRIPTION: This command updates the `openapi.json` file with the latest OpenAPI specification. This likely uses a script defined in the `package.json` to fetch or generate the spec from a source. This is a part of the SDK update process. It needs the project dependencies to be installed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npnpm run openapi:latest\n```\n\n----------------------------------------\n\nTITLE: Running the Connector\nDESCRIPTION: This command starts the ElevenLabs Agent WebSocket connector application using Node.js. The connector acts as a bridge between Vonage and ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/telephony/vonage.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnode elevenlabs-agent-ws-connector.cjs\n```\n\n----------------------------------------\n\nTITLE: Incorrect CMU Arpabet SSML (Missing Stress)\nDESCRIPTION: Illustrates a common error in CMU Arpabet usage within SSML phoneme tags: omitting stress markers. English is a lexical stress language, and failing to include stress (e.g., '1', '0') can lead to inconsistent or incorrect syllable emphasis by the text-to-speech model.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_2\n\nLANGUAGE: SSML\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"T AE L AH N\">talon</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Validating API Definition\nDESCRIPTION: This command validates the API definition using the Fern CLI. It checks the `openapi.json` and related configuration files for errors and inconsistencies. The command requires the project dependencies to be installed via `pnpm install`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npnpm install\npnpm run fern:check\n```\n\n----------------------------------------\n\nTITLE: Initializing the Conversation Session with Audio and Callbacks - Python\nDESCRIPTION: Creates a 'Conversation' object using the initialized client, agent ID, authentication flag, a default audio interface, and several callback functions for handling agent responses, corrections, and user transcripts. Key parameters include the client, agent ID, boolean for authentication, and audio interface instance. Optionally, latency measurement can be enabled via an additional callback. The session object supports programmatic control over the conversational flow.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconversation = Conversation(\n    # API client and agent ID.\n    client,\n    agent_id,\n\n    # Assume auth is required when API_KEY is set.\n    requires_auth=bool(api_key),\n\n    # Use the default audio interface.\n    audio_interface=DefaultAudioInterface(),\n\n    # Simple callbacks that print the conversation to the console.\n    callback_agent_response=lambda response: print(f\"Agent: {response}\"),\n    callback_agent_response_correction=lambda original, corrected: print(f\"Agent: {original} -> {corrected}\"),\n    callback_user_transcript=lambda transcript: print(f\"User: {transcript}\"),\n\n    # Uncomment if you want to see latency measurements.\n    # callback_latency_measurement=lambda latency: print(f\"Latency: {latency}ms\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Voice Parameters for Audio Generation in ElevenLabs WebSocket (Python)\nDESCRIPTION: This Python snippet details how to send a WebSocket message specifying custom voice_settings (e.g., stability, similarity_boost, use_speaker_boost), allowing fine-grained control over audio characteristics. Dependencies: an open WebSocket, json library, and valid text input. Pass voice_settings as a dictionary; the sample disables speaker boost and customizes stability and similarity boost values between 0.0 and 1.0. These settings are applied per request but can be overridden in subsequent messages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nawait websocket.send(json.dumps({\n    \"text\": text,\n    \"voice_settings\": {\"stability\": 0.5, \"similarity_boost\": 0.8, \"use_speaker_boost\": False},\n}))\n```\n\n----------------------------------------\n\nTITLE: Incorrect Input Text Format Example for Forced Alignment API\nDESCRIPTION: Shows an incorrect format for text input that should be avoided when using the Forced Alignment API. The API expects plain text, not a JSON object containing text.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/forced-alignment.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"Hello, how are you?\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating package.json Scripts for Full-Stack Development\nDESCRIPTION: JSON configuration that defines npm scripts for running both frontend and backend servers simultaneously for the ElevenLabs Conversational AI application.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"scripts\": {\n        \"dev:backend\": \"node backend/server.js\",\n        \"dev\": \"npm run dev:frontend & npm run dev:backend\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Connector Repository\nDESCRIPTION: Clones the specified Git repository for the ElevenLabs Agent WebSocket connector and navigates into the newly created directory.  After cloning, it copies the example environment configuration file to a new `.env` file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/telephony/vonage.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/nexmo-se/elevenlabs-agent-ws-connector.git\ncd elevenlabs-agent-ws-connector\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation with Dynamic Variables and Client Tools - TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize a conversation with an ElevenLabs agent, incorporating dynamic variables (like platform) and client tools. It uses the `useConversation` hook to manage the conversation lifecycle and `startSession` to initiate the conversation with the agent, passing the platform and client tools as configuration. This enables the agent to personalize responses based on the device's platform and use specified client tools.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport default function ConvAiDOMComponent({\n  platform,\n  get_battery_level,\n  change_brightness,\n  flash_screen,\n}: {\n  dom?: import('expo/dom').DOMProps;\n  platform: string;\n  get_battery_level: typeof tools.get_battery_level;\n  change_brightness: typeof tools.change_brightness;\n  flash_screen: typeof tools.flash_screen;\n}) {\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message) => {\n      console.log(message);\n    },\n    onError: (error) => console.error('Error:', error),\n  });\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      const hasPermission = await requestMicrophonePermission();\n      if (!hasPermission) {\n        alert('No permission');\n        return;\n      }\n\n      // Start the conversation with your agent\n      await conversation.startSession({\n        agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n        dynamicVariables: {\n          platform,\n        },\n        clientTools: {\n          get_battery_level,\n          change_brightness,\n          flash_screen,\n        },\n      });\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n  //...\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding ElevenLabs Conversational AI Widget HTML/JS\nDESCRIPTION: This combined snippet provides the necessary HTML custom element and JavaScript source required to embed the ElevenLabs Conversational AI widget. The `<elevenlabs-convai>` tag defines the widget placement and requires the specific `agent-id`. The `<script>` tag loads the widget's functionality asynchronously from the ElevenLabs CDN. The custom element goes into a Squarespace Code Block, while the script goes into the Footer Code Injection section.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/squarespace.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Forcing Immediate Audio Generation with flush in ElevenLabs WebSocket (Python)\nDESCRIPTION: This Python code sends a WebSocket request to force the ElevenLabs server to immediately generate audio for the remaining buffered text using the flush parameter. Relevant when completing a dialogue or document, ensuring no audio remains ungenerated. Requires an open WebSocket connection, valid text, and the json module. flush: True in the payload triggers instant synthesis regardless of the chunk_length_schedule state. The input text is provided under the text key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait websocket.send(json.dumps({\"text\": \"Generate this audio immediately.\", \"flush\": True}))\n```\n\n----------------------------------------\n\nTITLE: Processing Voice, Audio, and Video Messages with grammY in TypeScript\nDESCRIPTION: Configures a grammY bot instance to listen for voice, audio, or video messages using `bot.on`. When such a message is received, it retrieves the file metadata and URL from Telegram, then schedules a background task using `EdgeRuntime.waitUntil` (from Supabase Background Tasks) to execute the `scribe` function for transcription. An immediate reply is sent to the user acknowledging receipt.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nbot.on([':voice', ':audio', ':video'], async (ctx) => {\n  try {\n    const file = await ctx.getFile();\n    const fileURL = `https://api.telegram.org/file/bot${telegramBotToken}/${file.file_path}`;\n    const fileMeta = ctx.message?.video ?? ctx.message?.voice ?? ctx.message?.audio;\n\n    if (!fileMeta) {\n      return ctx.reply('No video|audio|voice metadata found. Please try again.');\n    }\n\n    // Run the transcription in the background.\n    EdgeRuntime.waitUntil(\n      scribe({\n        fileURL,\n        fileType: fileMeta.mime_type!,\n        duration: fileMeta.duration,\n        chatId: ctx.chat.id,\n        messageId: ctx.message?.message_id!,\n        username: ctx.from?.username || '',\n      })\n    );\n\n    // Reply to the user immediately to let them know we received their file.\n    return ctx.reply('Received. Scribing...');\n  } catch (error) {\n    console.error(error);\n    return ctx.reply(\n      'Sorry, there was an error getting the file. Please try again with a smaller file!'\n    );\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Conversational AI DOM Component in React Native\nDESCRIPTION: TypeScript React component using Expo DOM components and ElevenLabs SDK to implement the conversational AI interface with microphone permissions handling and conversation management.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_3\n\nLANGUAGE: tsx\nCODE:\n```\n'use dom';\n\nimport { useConversation } from '@11labs/react';\nimport { Mic } from 'lucide-react-native';\nimport { useCallback } from 'react';\nimport { View, Pressable, StyleSheet } from 'react-native';\n\nimport tools from '../utils/tools';\n\nasync function requestMicrophonePermission() {\n  try {\n    await navigator.mediaDevices.getUserMedia({ audio: true });\n    return true;\n  } catch (error) {\n    console.log(error);\n    console.error('Microphone permission denied');\n    return false;\n  }\n}\n\nexport default function ConvAiDOMComponent({\n  platform,\n  get_battery_level,\n  change_brightness,\n  flash_screen,\n}: {\n  dom?: import('expo/dom').DOMProps;\n  platform: string;\n  get_battery_level: typeof tools.get_battery_level;\n  change_brightness: typeof tools.change_brightness;\n  flash_screen: typeof tools.flash_screen;\n}) {\n  const conversation = useConversation({\n    onConnect: () => console.log('Connected'),\n    onDisconnect: () => console.log('Disconnected'),\n    onMessage: (message) => {\n      console.log(message);\n    },\n    onError: (error) => console.error('Error:', error),\n  });\n  const startConversation = useCallback(async () => {\n    try {\n      // Request microphone permission\n      const hasPermission = await requestMicrophonePermission();\n      if (!hasPermission) {\n        alert('No permission');\n        return;\n      }\n\n      // Start the conversation with your agent\n      await conversation.startSession({\n        agentId: 'YOUR_AGENT_ID', // Replace with your agent ID\n        dynamicVariables: {\n          platform,\n        },\n        clientTools: {\n          get_battery_level,\n          change_brightness,\n          flash_screen,\n        },\n      });\n    } catch (error) {\n      console.error('Failed to start conversation:', error);\n    }\n  }, [conversation]);\n\n  const stopConversation = useCallback(async () => {\n    await conversation.endSession();\n  }, [conversation]);\n\n  return (\n    <Pressable\n      style={[styles.callButton, conversation.status === 'connected' && styles.callButtonActive]}\n      onPress={conversation.status === 'disconnected' ? startConversation : stopConversation}\n    >\n      <View\n        style={[\n          styles.buttonInner,\n          conversation.status === 'connected' && styles.buttonInnerActive,\n        ]}\n      >\n        <Mic size={32} color=\"#E2E8F0\" strokeWidth={1.5} style={styles.buttonIcon} />\n      </View>\n    </Pressable>\n  );\n}\n\nconst styles = StyleSheet.create({\n  callButton: {\n    width: 120,\n    height: 120,\n    borderRadius: 60,\n    backgroundColor: 'rgba(255, 255, 255, 0.1)',\n    alignItems: 'center',\n    justifyContent: 'center',\n    marginBottom: 24,\n  },\n  callButtonActive: {\n    backgroundColor: 'rgba(239, 68, 68, 0.2)',\n  },\n  buttonInner: {\n    width: 80,\n    height: 80,\n    borderRadius: 40,\n    backgroundColor: '#3B82F6',\n    alignItems: 'center',\n    justifyContent: 'center',\n    shadowColor: '#3B82F6',\n    shadowOffset: {\n      width: 0,\n      height: 0,\n    },\n    shadowOpacity: 0.5,\n    shadowRadius: 20,\n    elevation: 5,\n  },\n  buttonInnerActive: {\n    backgroundColor: '#EF4444',\n    shadowColor: '#EF4444',\n  },\n  buttonIcon: {\n    transform: [{ translateY: 2 }],\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding the ElevenLabs Conversational AI Widget in HTML - HTML\nDESCRIPTION: This snippet demonstrates how to embed the ElevenLabs Conversational AI widget into a web page using a custom element and a corresponding JavaScript library. It requires an existing agent ID from ElevenLabs. The <elevenlabs-convai> custom tag accepts the agent-id attribute, which must be replaced with a valid value. The <script> tag loads the widget asynchronously from the ElevenLabs CDN. Expected output is an interactive chat widget on the target page; the snippet should be added to either a Custom HTML block in WordPress or into the theme's footer. Ensure only one instance of the script is loaded globally per page for proper operation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/wordpress.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<elevenlabs-convai agent-id=\"YOUR_AGENT_ID\"></elevenlabs-convai>\n<script src=\"https://elevenlabs.io/convai-widget/index.js\" async type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Verifying Webhook Signature\nDESCRIPTION: The `constructWebhookEvent` function is used to verify the signature of incoming webhook requests.  It extracts the signature from the header, verifies the timestamp, and calculates a digest using the webhook secret.  If any validation fails, it returns an error; otherwise, it parses the request body as JSON and returns the event data.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst constructWebhookEvent = async (req: NextRequest, secret?: string) => {\n  const body = await req.text();\n  const signature_header = req.headers.get('ElevenLabs-Signature');\n\n  if (!signature_header) {\n    return { event: null, error: 'Missing signature header' };\n  }\n\n  const headers = signature_header.split(',');\n  const timestamp = headers.find((e) => e.startsWith('t='))?.substring(2);\n  const signature = headers.find((e) => e.startsWith('v0='));\n\n  if (!timestamp || !signature) {\n    return { event: null, error: 'Invalid signature format' };\n  }\n\n  // Validate timestamp\n  const reqTimestamp = Number(timestamp) * 1000;\n  const tolerance = Date.now() - 30 * 60 * 1000;\n  if (reqTimestamp < tolerance) {\n    return { event: null, error: 'Request expired' };\n  }\n\n  // Validate hash\n  const message = `${timestamp}.${body}`;\n\n  if (!secret) {\n    return { event: null, error: 'Webhook secret not configured' };\n  }\n\n  const digest = 'v0=' + crypto.createHmac('sha256', secret).update(message).digest('hex');\n\n  if (signature !== digest) {\n    return { event: null, error: 'Invalid signature' };\n  }\n\n  const event = JSON.parse(body);\n  return { event, error: null };\n};\n```\n\n----------------------------------------\n\nTITLE: Handling Request Parameters for Text-to-Speech in TypeScript\nDESCRIPTION: Code for handling incoming requests to the text-to-speech function, extracting text and voiceId parameters, and generating a hash for caching purposes.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nDeno.serve(async (req) => {\n// To secure your function for production, you can for example validate the request origin,\n// or append a user access token and validate it with Supabase Auth.\nconsole.log(\"Request origin\", req.headers.get(\"host\"));\nconst url = new URL(req.url);\nconst params = new URLSearchParams(url.search);\nconst text = params.get(\"text\");\nconst voiceId = params.get(\"voiceId\") ?? \"JBFqnCBsd6RMkjVDRZzb\";\n\nconst requestHash = hash.MD5({ text, voiceId });\nconsole.log(\"Request hash\", requestHash);\n\n// ...\n})\n```\n\n----------------------------------------\n\nTITLE: SwiftUI Conversation View - Swift\nDESCRIPTION: Example of a SwiftUI view implementing a conversation interface with ElevenLabs SDK. It includes state variables for the conversation object, mode, status, and audio level. It defines a function to start the conversation asynchronously and updates the UI based on the conversation status.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_8\n\nLANGUAGE: swift\nCODE:\n```\nstruct ConversationalAIView: View {\n    @State private var conversation: ElevenLabsSDK.Conversation?\n    @State private var mode: ElevenLabsSDK.Mode = .listening\n    @State private var status: ElevenLabsSDK.Status = .disconnected\n    @State private var audioLevel: Float = 0.0\n\n    private func startConversation() {\n        Task {\n            do {\n                let config = ElevenLabsSDK.SessionConfig(agentId: \"your-agent-id\")\n                var callbacks = ElevenLabsSDK.Callbacks()\n\n                callbacks.onConnect = { conversationId in\n                    status = .connected\n                }\n                callbacks.onDisconnect = {\n                    status = .disconnected\n                }\n                callbacks.onModeChange = { newMode in\n                    DispatchQueue.main.async {\n                        mode = newMode\n                    }\n                }\n                callbacks.onVolumeUpdate = { newVolume in\n                    DispatchQueue.main.async {\n                        audioLevel = newVolume\n                    }\n                }\n\n                conversation = try await ElevenLabsSDK.Conversation.startSession(\n                    config: config,\n                    callbacks: callbacks\n                )\n            } catch {\n                print(\"Failed to start conversation: \\(error)\")\n            }\n        }\n    }\n\n    var body: some View {\n        VStack {\n            // Your UI implementation\n            Button(action: startConversation) {\n                Text(status == .connected ? \"End Call\" : \"Start Call\")\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Audio Native Widget in WordPress HTML\nDESCRIPTION: This HTML snippet embeds the Audio Native widget into a WordPress page. It defines the widget's dimensions, user ID, player URL, and project ID.  The widget fetches and displays an audio player generated from text-to-speech. It requires the `audioNativeHelper.js` script from ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/wordpress.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Embedding Audio Native Player Widget in HTML for Squarespace\nDESCRIPTION: This HTML snippet adds the Elevenlabs Audio Native player widget to a Squarespace blog post. It uses a div element with custom data attributes to specify player settings such as height, width, project identifier, public user ID, and player URL. The snippet also includes a script tag that loads a JavaScript helper from Elevenlabs to enable player functionality and initialization. The HTML should be inserted into a Squarespace code block at the desired location in the post editor. The embedded player converts the blog text into audio and displays playback controls once the project is created. Dependencies include access to the Elevenlabs hosting URL and proper configuration of data attributes.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/squarespace.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Implement Supabase Edge Function for ElevenLabs TTS (TypeScript)\nDESCRIPTION: Defines a Supabase Edge Function (`index.ts`) using Deno. It initializes Supabase and ElevenLabs clients using environment variables. The function handles GET requests, extracts 'text' and 'voiceId' query parameters, calculates a hash for caching, checks Supabase Storage for an existing audio file using the hash, and returns it if found. If not cached, it calls the ElevenLabs API to generate speech, streams the audio response back to the client, and simultaneously uploads the stream to Supabase Storage as a background task for future requests.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Setup type definitions for built-in Supabase Runtime APIs\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts';\nimport { createClient } from 'jsr:@supabase/supabase-js@2';\nimport { ElevenLabsClient } from 'npm:elevenlabs';\nimport * as hash from 'npm:object-hash';\n\nconst supabase = createClient(\n  Deno.env.get('SUPABASE_URL')!,\n  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n);\n\nconst client = new ElevenLabsClient({\n  apiKey: Deno.env.get('ELEVENLABS_API_KEY'),\n});\n\n// Upload audio to Supabase Storage in a background task\nasync function uploadAudioToStorage(stream: ReadableStream, requestHash: string) {\n  const { data, error } = await supabase.storage\n    .from('audio')\n    .upload(`${requestHash}.mp3`, stream, {\n      contentType: 'audio/mp3',\n    });\n\n  console.log('Storage upload result', { data, error });\n}\n\nDeno.serve(async (req) => {\n  // To secure your function for production, you can for example validate the request origin,\n  // or append a user access token and validate it with Supabase Auth.\n  console.log('Request origin', req.headers.get('host'));\n  const url = new URL(req.url);\n  const params = new URLSearchParams(url.search);\n  const text = params.get('text');\n  const voiceId = params.get('voiceId') ?? 'JBFqnCBsd6RMkjVDRZzb';\n\n  const requestHash = hash.MD5({ text, voiceId });\n  console.log('Request hash', requestHash);\n\n  // Check storage for existing audio file\n  const { data } = await supabase.storage.from('audio').createSignedUrl(`${requestHash}.mp3`, 60);\n\n  if (data) {\n    console.log('Audio file found in storage', data);\n    const storageRes = await fetch(data.signedUrl);\n    if (storageRes.ok) return storageRes;\n  }\n\n  if (!text) {\n    return new Response(JSON.stringify({ error: 'Text parameter is required' }), {\n      status: 400,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n\n  try {\n    console.log('ElevenLabs API call');\n    const response = await client.textToSpeech.convertAsStream(voiceId, {\n      output_format: 'mp3_44100_128',\n      model_id: 'eleven_multilingual_v2',\n      text,\n    });\n\n    const stream = new ReadableStream({\n      async start(controller) {\n        for await (const chunk of response) {\n          controller.enqueue(chunk);\n        }\n        controller.close();\n      },\n    });\n\n    // Branch stream to Supabase Storage\n    const [browserStream, storageStream] = stream.tee();\n\n    // Upload to Supabase Storage in the background\n    EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash));\n\n    // Return the streaming response immediately\n    return new Response(browserStream, {\n      headers: {\n        'Content-Type': 'audio/mpeg',\n      },\n    });\n  } catch (error) {\n    console.log('error', { error });\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Specifying Pronunciation with CMU Arpabet SSML\nDESCRIPTION: Shows how to use the SSML phoneme tag with `alphabet=\"cmu-arpabet\"` to specify pronunciation using the CMU Arpabet phonetic alphabet. Similar to IPA, this provides direct phonetic control for a word. This tag is supported by models such as \"Turbo v2\" and \"Eleven English v1\", and is currently recommended for potentially better consistency.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_1\n\nLANGUAGE: SSML\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"AE K CH UW AH L IY\">actually</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Adding Pauses in ElevenLabs TTS using `<break>` (Text)\nDESCRIPTION: Demonstrates using the `<break time=\"x.xs\" />` tag within text input to introduce timed pauses (up to 3 seconds) in ElevenLabs text-to-speech generation. This example shows a 1.5-second pause. Excessive use may cause instability.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n\"Hold on, let me think.\" <break time=\"1.5s\" /> \"Alright, I’ve got it.\"\n```\n\n----------------------------------------\n\nTITLE: Example Post-Call Transcription Webhook Payload\nDESCRIPTION: This JSON snippet shows the structure and content of a typical webhook payload for the `post_call_transcription` event. It includes top-level fields like type and timestamp, and a detailed data object containing conversation ID, status, transcript, metadata, analysis results, and initiation client data.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/administration/webhooks.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"post_call_transcription\",\n  \"event_timestamp\": 1739537297,\n  \"data\": {\n    \"agent_id\": \"xyz\",\n    \"conversation_id\": \"abc\",\n    \"status\": \"done\",\n    \"transcript\": [\n      {\n        \"role\": \"agent\",\n        \"message\": \"Hey there angelo. How are you?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 0,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"user\",\n        \"message\": \"Hey, can you tell me, like, a fun fact about 11 Labs?\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 2,\n        \"conversation_turn_metrics\": null\n      },\n      {\n        \"role\": \"agent\",\n        \"message\": \"I do not have access to fun facts about Eleven Labs. However, I can share some general information about the company. Eleven Labs is an AI voice technology platform that specializes in voice cloning and text-to-speech...\",\n        \"tool_calls\": null,\n        \"tool_results\": null,\n        \"feedback\": null,\n        \"time_in_call_secs\": 9,\n        \"conversation_turn_metrics\": {\n          \"convai_llm_service_ttfb\": {\n            \"elapsed_time\": 0.3704247010173276\n          },\n          \"convai_llm_service_ttf_sentence\": {\n            \"elapsed_time\": 0.5551181449554861\n          }\n        }\n      }\n    ],\n    \"metadata\": {\n      \"start_time_unix_secs\": 1739537297,\n      \"call_duration_secs\": 22,\n      \"cost\": 296,\n      \"deletion_settings\": {\n        \"deletion_time_unix_secs\": 1802609320,\n        \"deleted_logs_at_time_unix_secs\": null,\n        \"deleted_audio_at_time_unix_secs\": null,\n        \"deleted_transcript_at_time_unix_secs\": null,\n        \"delete_transcript_and_pii\": true,\n        \"delete_audio\": true\n      },\n      \"feedback\": {\n        \"overall_score\": null,\n        \"likes\": 0,\n        \"dislikes\": 0\n      },\n      \"authorization_method\": \"authorization_header\",\n      \"charging\": {\n        \"dev_discount\": true\n      },\n      \"termination_reason\": \"\"\n    },\n    \"analysis\": {\n      \"evaluation_criteria_results\": {},\n      \"data_collection_results\": {},\n      \"call_successful\": \"success\",\n      \"transcript_summary\": \"The conversation begins with the agent asking how Angelo is, but Angelo redirects the conversation by requesting a fun fact about 11 Labs. The agent acknowledges they don't have specific fun facts about Eleven Labs but offers to provide general information about the company. They briefly describe Eleven Labs as an AI voice technology platform specializing in voice cloning and text-to-speech technology. The conversation is brief and informational, with the agent adapting to the user's request despite not having the exact information asked for.\"\n    },\n    \"conversation_initiation_client_data\": {\n      \"conversation_config_override\": {\n        \"agent\": {\n          \"prompt\": null,\n          \"first_message\": null,\n          \"language\": \"en\"\n        },\n        \"tts\": {\n          \"voice_id\": null\n        }\n      },\n      \"custom_llm_extra_body\": {},\n      \"dynamic_variables\": {\n        \"user_name\": \"angelo\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Supabase Edge Functions Locally\nDESCRIPTION: Command to start the Supabase Edge Functions server locally for development and testing, allowing you to observe function logs in real-time.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nsupabase functions serve\n```\n\n----------------------------------------\n\nTITLE: Embedding Elevenlabs Audio Native Player HTML in Ghost Blog Post\nDESCRIPTION: This HTML snippet embeds the Elevenlabs Audio Native player into a Ghost blog post by adding a <div> element with specific data attributes that identify the public user, project ID, and player URL. It also includes a script tag loading the audioNativeHelper.js script from Elevenlabs, which initializes the player. Required dependencies include access to the Elevenlabs player URL and the script hosted at elevenlabs.io. Key parameters such as 'data-publicuserid' and 'data-projectid' should be replaced with valid identifiers to link the player to the authenticated project and user. The code outputs a widget that renders the audio narration player inline in the blog post. It requires the Ghost CMS environment to accept custom HTML and script tags.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/ghost.mdx#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div\n    id=\"elevenlabs-audionative-widget\"\n    data-height=\"90\"\n    data-width=\"100%\"\n    data-frameborder=\"no\"\n    data-scrolling=\"no\"\n    data-publicuserid=\"public-user-id\"\n    data-playerurl=\"https://elevenlabs.io/player/index.html\"\n    data-projectid=\"project-id\"\n>\n    Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Specifying Pronunciation with IPA SSML\nDESCRIPTION: Demonstrates using the SSML phoneme tag with `alphabet=\"ipa\"` to provide an International Phonetic Alphabet pronunciation for a specific word. This allows explicit control over the phonetic rendering, bypassing the model's default pronunciation. This feature requires specific models like \"Turbo v2\" or \"Eleven English v1\".\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_0\n\nLANGUAGE: SSML\nCODE:\n```\n<phoneme alphabet=\"ipa\" ph=\"ˈæktʃuəli\">actually</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Rendering ElevenLabs Waveform Components with React JSX\nDESCRIPTION: This snippet renders two waveforms within light and dark mode containers respectively using a React component named ElevenLabsWaveform. The snippet uses JSX div elements with distinct IDs and class names to differentiate light and dark theme styling. The ElevenLabsWaveform component is passed a color prop to control the waveform color and a CSS class for height styling. This React JSX snippet requires React and the ElevenLabsWaveform component defined elsewhere. It is intended for visual audio waveforms with color theming based on UI mode.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<div id=\"overview-wave\" className=\"light-mode-wave\">\n  <ElevenLabsWaveform color=\"blue\" className=\"h-[500px]\" />\n</div>\n\n<div id=\"overview-wave\" className=\"dark-mode-wave\">\n  <ElevenLabsWaveform color=\"gray\" className=\"h-[500px]\" />\n</div>\n```\n\n----------------------------------------\n\nTITLE: Uploading ElevenLabs PVC Audio Samples (TypeScript)\nDESCRIPTION: Uploads audio sample files to a previously created PVC voice. Provides file paths to the SDK's create samples method using `fs.createReadStream`. Multiple files can be added to improve the clone's quality. Requires the `node:fs` module.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst samples = await elevenlabs.voices.pvc.samples.create(voice.voice_id, {\n            // Replace with the paths to your audio and/or video files.\n            // The more files you add, the better the clone will be.\n            files: [fs.createReadStream(\"/path/to/your/audio/file.mp3\")],\n        })\n```\n\n----------------------------------------\n\nTITLE: Uploading Audio Stream to Supabase Storage Background Task\nDESCRIPTION: Helper function that takes a ReadableStream of audio data and uploads it to Supabase Storage with the appropriate content type. This runs as a background task using EdgeRuntime.waitUntil().\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// Upload audio to Supabase Storage in a background task\nasync function uploadAudioToStorage(\n  stream: ReadableStream,\n  requestHash: string,\n) {\n  const { data, error } = await supabase.storage\n    .from(\"audio\")\n    .upload(`${requestHash}.mp3`, stream, {\n      contentType: \"audio/mp3\",\n    });\n\n  console.log(\"Storage upload result\", { data, error });\n}\n```\n\n----------------------------------------\n\nTITLE: Validating HMAC Webhooks with Express in JavaScript\nDESCRIPTION: This code snippet outlines a webhook handler implementation using Node.js with the Express framework that validates HMAC SHA256 signatures from ElevenLabs webhook requests. It depends on the 'crypto' module and Express middleware 'body-parser' configured to parse raw request bodies. The handler extracts the ElevenLabs-Signature header, parses out the timestamp and signature components, and checks that the timestamp is within a 30-minute validity period. It then recreates the signature by hashing the concatenated timestamp and raw request body with the shared secret and compares it to the received signature, responding with appropriate HTTP status codes for expired or unauthorized requests. Upon successful validation, it proceeds with request processing.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/webhook-hmac-authentication.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst crypto = require('crypto');\nconst secret = process.env.WEBHOOK_SECRET;\nconst bodyParser = require('body-parser');\n\n// Ensure express js is parsing the raw body through instead of applying it’s own encoding\napp.use(bodyParser.raw({ type: '*/*' }));\n\n// Example webhook handler\napp.post('/webhook/elevenlabs', async (req, res) => {\n  const headers = req.headers['ElevenLabs-Signature'].split(',');\n  const timestamp = headers.find((e) => e.startsWith('t=')).substring(2);\n  const signature = headers.find((e) => e.startsWith('v0='));\n\n  // Validate timestamp\n  const reqTimestamp = timestamp * 1000;\n  const tolerance = Date.now() - 30 * 60 * 1000;\n  if (reqTimestamp < tolerance) {\n    res.status(403).send('Request expired');\n    return;\n  } else {\n    // Validate hash\n    const message = `${timestamp}.${req.body}`;\n    const digest = 'v0=' + crypto.createHmac('sha256', secret).update(message).digest('hex');\n    if (signature !== digest) {\n      res.status(401).send('Request unauthorized');\n      return;\n    }\n  }\n\n  // Validation passed, continue processing ...\n\n  res.status(200).send();\n});\n```\n\n----------------------------------------\n\nTITLE: Forcing Immediate Audio Generation with flush in ElevenLabs WebSocket (TypeScript)\nDESCRIPTION: This TypeScript example shows how to send a message over WebSocket to ElevenLabs with flush: true, immediately generating audio from all buffered text, useful for ending a session. The code depends on a live WebSocket object and prepares the payload as a JSON string. Only text and flush are sent; any remaining buffered content is returned as audio on the server response.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nwebsocket.send(JSON.stringify({ text: 'Generate this audio immediately.', flush: true }));\n```\n\n----------------------------------------\n\nTITLE: Illustration of Agent Transfer Hierarchy in Conversational AI\nDESCRIPTION: This textual snippet visualizes a hierarchical structure of conversational AI agents demonstrating the concept of agent-to-agent transfer. It shows an 'Orchestrator Agent' directing conversations to specialized agents handling distinct tasks such as availability inquiries, technical support (with further nested hardware support), and billing issues. This hierarchy exemplifies layered delegation for complex workflow management within multi-agent setups. It serves as conceptual context rather than executable code.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/tools/agent-transfer.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOrchestrator Agent (Initial Qualification)\n│\n├───> Agent 1 (e.g., Availability Inquiries)\n│\n├───> Agent 2 (e.g., Technical Support)\n│     │\n│     └───> Agent 2a (e.g., Hardware Support)\n│\n└───> Agent 3 (e.g., Billing Issues)\n```\n\n----------------------------------------\n\nTITLE: Installing AWS SDK and Presigner for TypeScript\nDESCRIPTION: Installs the AWS S3 client and S3 request presigner packages using npm to enable S3 uploads and presigned URL generation in TypeScript. These dependencies are required before executing any S3-related TypeScript code. Both lines must be run in a Node.js project directory.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nnpm install @aws-sdk/client-s3\nnpm install @aws-sdk/s3-request-presigner\n```\n\n----------------------------------------\n\nTITLE: Using Customer Support Tools\nDESCRIPTION: This snippet outlines the tools available for a customer support agent, including `lookupCustomerAccount`, `checkSystemStatus`, `runDiagnostic`, `createSupportTicket)`, and `scheduleCallback`. It also defines the order in which these tools should be used.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_15\n\nLANGUAGE: mdx\nCODE:\n```\n# Tools\n\nYou have access to the following customer support tools:\n\n`lookupCustomerAccount`: After verifying identity, use this to access account details, subscription status, and usage history before addressing account-specific questions.\n\n`checkSystemStatus`: When users report potential outages or service disruptions, use this tool first to check if there are known issues before troubleshooting.\n\n`runDiagnostic`: For technical issues, use this tool to perform automated tests on the user's service and analyze results before suggesting solutions.\n\n`createSupportTicket)`: If you cannot resolve an issue directly, use this tool to create a ticket for human follow-up, ensuring you've collected all relevant information first.\n\n`scheduleCallback`: When users need specialist assistance, offer to schedule a callback at their convenience rather than transferring them immediately.\n\nTool orchestration: Always check system status first for reported issues, then customer account details, followed by diagnostics for technical problems. Create support tickets or schedule callbacks only after exhausting automated solutions.\n```\n\n----------------------------------------\n\nTITLE: Starting a Conversation Session Using startSession Method in JavaScript\nDESCRIPTION: Initiates a websocket connection and begins the audio session with the ElevenLabs Conversational AI agent. Requires passing either a public 'agentId' or an authorized 'url' for signed link access. The method returns a promise resolving to a unique 'conversationId' used to identify the session. This supports both public agents and authorized conversation flows.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst conversation = useConversation();\nconst conversationId = await conversation.startSession({ url });\n```\n\n----------------------------------------\n\nTITLE: Generating Signed URL for S3 File in Python\nDESCRIPTION: Illustrates generating a signed URL (valid for 1 hour) for a file uploaded to S3 using its S3 filename, then prints the URL for sharing. The S3 file must exist; proper AWS permissions and prior upload via upload_audiostream_to_s3 are required. Output is a string URL.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/streaming.mdx#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nsigned_url = generate_presigned_url(s3_file_name)\nprint(f\"Signed URL to access the file: {signed_url}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Twilio-ElevenLabs Integration\nDESCRIPTION: Command to install the required Python packages for the project, including FastAPI for the server, Uvicorn for ASGI server implementation, and libraries for WebSocket communication and environment variable management.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install fastapi uvicorn python-dotenv twilio elevenlabs websockets\n```\n\n----------------------------------------\n\nTITLE: Running ElevenLabs WebSocket Scripts\nDESCRIPTION: Provides the command-line instructions for executing the Python and TypeScript example scripts. Running these commands will initiate the WebSocket process and save the generated audio to the specified output file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython text-to-speech-websocket.py\n```\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx text-to-speech-websocket.ts\n```\n\n----------------------------------------\n\nTITLE: Creating an ElevenLabs Pronunciation Dictionary (.pls) with CMU Arpabet (XML)\nDESCRIPTION: Provides a full example of a `.pls` pronunciation lexicon file conforming to the W3C PLS standard, using the CMU Arpabet alphabet. It defines a specific pronunciation for \"apple\" using `<phoneme>` and an expansion for \"UN\" using `<alias>`. This file can be uploaded to ElevenLabs projects (Studio, Dubbing Studio, API) to customize word pronunciations.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"cmu-arpabet\" xml:lang=\"en-GB\">\n  <lexeme>\n    <grapheme>apple</grapheme>\n    <phoneme>AE P AH L</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n</lexicon>\n```\n\n----------------------------------------\n\nTITLE: Installing Elevenlabs and dotenv Libraries in TypeScript\nDESCRIPTION: Installs the Elevenlabs SDK and dotenv libraries using npm to enable API integrations and environment variable management in TypeScript or Node.js projects. Dependencies include Node.js environment and npm package manager. These commands prepare the environment for Elevenlabs API usage with secure configuration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/quickstart-install-sdk.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nnpm install elevenlabs\nnpm install dotenv\n```\n\n----------------------------------------\n\nTITLE: Creating Pronunciation Dictionary File\nDESCRIPTION: This snippet shows the content of a pronunciation dictionary file, which uses the PLS (Pronunciation Lexicon Specification) format.  It defines how \"tomato\" and \"Tomato\" should be pronounced using IPA.  This file is then used to create a pronunciation dictionary.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"ipa\" xml:lang=\"en-US\">\n  <lexeme>\n    <grapheme>tomato</grapheme>\n    <phoneme>/tə'meɪtoʊ/</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>Tomato</grapheme>\n    <phoneme>/tə'meɪtoʊ/</phoneme>\n  </lexeme>\n</lexicon>\n```\n\n----------------------------------------\n\nTITLE: Creating an ElevenLabs Pronunciation Dictionary (.pls) with IPA (XML)\nDESCRIPTION: Provides a full example of a `.pls` pronunciation lexicon file conforming to the W3C PLS standard, using the International Phonetic Alphabet (IPA). It defines a specific pronunciation for \"Apple\" using `<phoneme>` and an expansion for \"UN\" using `<alias>`. Note that searches are case-sensitive. This file can be uploaded to ElevenLabs projects.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_9\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"ipa\" xml:lang=\"en-GB\">\n  <lexeme>\n    <grapheme>Apple</grapheme>\n    <phoneme>ˈæpl̩</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n</lexicon>\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies (shell)\nDESCRIPTION: Provides shell commands for initializing a new Node.js project with npm, installing runtime and development dependencies including TypeScript, Express, express-ws, ws, dotenv, and the ElevenLabs and Twilio SDKs. These steps are prerequisites for setting up the TypeScript project for Twilio voice integration. Ensure Node.js and npm are installed; commands are intended to be run in the project directory terminal.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmkdir elevenlabs-twilio\ncd elevenlabs-twilio\nnpm init -y\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm install elevenlabs express express-ws twilio\n```\n\nLANGUAGE: shell\nCODE:\n```\nnpm i @types/node @types/express @types/express-ws @types/ws dotenv tsx typescript\n```\n\n----------------------------------------\n\nTITLE: Converting Audio to Text using ElevenLabs API in TypeScript\nDESCRIPTION: This TypeScript script shows how to use the ElevenLabs SDK (`elevenlabs`) for speech-to-text conversion. It initializes the `ElevenLabsClient`, fetches audio from a URL into a Blob, and calls `client.speechToText.convert` with the audio and configuration options (`model_id`, `tag_audio_events`, `language_code`, `diarize`). The transcription is logged. Requires `elevenlabs` and `dotenv` packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/quickstart.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n  \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst transcription = await client.speechToText.convert({\n  file: audioBlob,\n  model_id: \"scribe_v1\", // Model to use, for now only \"scribe_v1\" is support.\n  tag_audio_events: true, // Tag audio events like laughter, applause, etc.\n  language_code: \"eng\", // Language of the audio file. If set to null, the model will detect the language automatically.\n  diarize: true, // Whether to annotate who is speaking\n});\n\nconsole.log(transcription);\n```\n\n----------------------------------------\n\nTITLE: Requesting Microphone Access Using Web Audio API in JavaScript\nDESCRIPTION: Ensures the user grants microphone access before starting the conversation. This step is essential because the Conversational AI requires audio input from the user. Utilizes the Navigator.mediaDevices.getUserMedia method with audio constraints.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/react.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// call after explaining to the user why the microphone access is needed\nawait navigator.mediaDevices.getUserMedia({ audio: true });\n```\n\n----------------------------------------\n\nTITLE: Webhook Tool Configuration for Opening New Zendesk Tickets - markdown\nDESCRIPTION: Defines the POST webhook tool 'zendesk_open_ticket' for creating new support tickets in Zendesk. It requires authorization via secret and a JSON payload that includes a ticket object with comment body, subject, and requester information such as name and validated email address. This tool is used by the AI agent after gathering issue details and customer contact info to formally open a support ticket.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/zendesk.mdx#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n**Name:** zendesk_open_ticket  \n**Description:** Opens a new support ticket.  \n**Method:** POST  \n**URL:** `https://your-subdomain.zendesk.com/api/v2/tickets.js`\n\n**Headers:**\n- **Content-Type:** `application/json`\n- **Authorization:** *(Secret: `zendesk_key`)*\n\n**Body Parameters:**\n- **ticket:** An object containing:\n  - **comment:**\n    - **body:** Detailed description of the support issue.\n  - **subject:** A short subject line.\n  - **requester:**\n    - **name:** The full name of the requester.\n    - **email:** A valid email address.\n```\n\n----------------------------------------\n\nTITLE: Handling POST Requests and Webhook Verification\nDESCRIPTION: This code defines the POST request handler for the webhook. It first retrieves and validates the webhook signature using `constructWebhookEvent` to ensure the request's authenticity.  If the signature is valid and the event type is 'post_call_transcription', the handler proceeds to process the event data.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nexport async function POST(req: NextRequest) {\n  const secret = process.env.ELEVENLABS_CONVAI_WEBHOOK_SECRET; // Add this to your env variables\n  const { event, error } = await constructWebhookEvent(req, secret);\n  if (error) {\n    return NextResponse.json({ error: error }, { status: 401 });\n  }\n\n  if (event.type === 'post_call_transcription') {\n    const { conversation_id, analysis, agent_id } = event.data;\n    // ... rest of POST handler\n  }\n\n  return NextResponse.json({ received: true }, { status: 200 });\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Speaker IDs for a PVC Sample (Python/TypeScript)\nDESCRIPTION: Updates the selected speaker IDs for a specific sample associated with a Professional Voice Clone (PVC). This is necessary when a sample contains multiple speakers and you need to assign the correct one. Requires the `voice_id` of the PVC, the `sample_id` of the specific sample, and a list containing the relevant `speaker_id`.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nelevenlabs.voices.pvc.samples.update(\n    voice_id=voice.voice_id,\n    sample_id=sample.sample_id,\n    selected_speaker_ids=[speaker.speaker_id]\n)\n```\n\nLANGUAGE: typescript\nCODE:\n```\nawait elevenlabs.voices.pvc.samples.update(voice.voice_id, samples.sample_id, {\n    selected_speaker_ids: [speaker.speaker_id],\n})\n```\n\n----------------------------------------\n\nTITLE: Testing Groq Cloud API with cURL in Bash\nDESCRIPTION: This code snippet demonstrates how to test your Groq API key by sending a simple chat completion request using cURL. It sends a basic greeting message to the llama-3.3-70b-versatile model and returns the model's response.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/groq-cloud.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://api.groq.com/openai/v1/chat/completions -s \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-d '{\n\"model\": \"llama-3.3-70b-versatile\",\n\"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Hello, how are you?\"\n}]\n}'\n```\n\n----------------------------------------\n\nTITLE: Beginning ElevenLabs PVC Speaker Separation (TypeScript)\nDESCRIPTION: Initiates speaker separation for each uploaded sample associated with a PVC voice. It then sets up a polling interval using `setInterval` to periodically check the status of each sample's separation process until all samples are marked as 'completed' or 'failed', clearing the interval when done.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Trigger the speaker separation action, this will take some time to complete\n        for (const sample of samples) {\n            if (sample.sample_id) {\n                await elevenlabs.voices.pvc.samples.speakers.separate(voice.voice_id, sample.sample_id);\n            }\n        }\n\n        // Check the status of the speaker separation action\n        const ids = samples.map((sample) => sample.sample_id);\n        const interval = setInterval(async () => {\n            // Poll the status of the speaker separation action\n            for (const id of ids) {\n                if (!id) continue;\n\n                const { status } = await elevenlabs.voices.pvc.samples.speakers.get(voice.voice_id, id);\n                console.log(`Sample ${id} status: ${status}`);\n                if (status === \"completed\" || status === \"failed\") {\n                    ids.splice(ids.indexOf(id), 1);\n                }\n\n                if (ids.length === 0) {\n                    clearInterval(interval);\n                    console.log(\"All samples have been processed or removed from polling\");\n                }\n            }\n        }, 5000);\n```\n\n----------------------------------------\n\nTITLE: Requesting Manual PVC Verification (Python/TypeScript)\nDESCRIPTION: Initiates a request for manual verification of a PVC, intended as an alternative if CAPTCHA verification is not possible (e.g., visual impairment) or fails. Requires the `voice_id` and a list of file paths containing necessary verification documents (contact support for details on required files). Files should be provided as readable streams or binary file objects.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/professional-voice-cloning.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nelevenlabs.voices.pvc.verification.request(\n    voice_id=voice.voice_id,\n    files=[open('path/to/verification/files.txt', 'rb')],\n)\n```\n\nLANGUAGE: typescript\nCODE:\n```\nawait elevenlabs.voices.pvc.verification.request(voice.voice_id, {\n    files: [fs.createReadStream(\"/path/to/verification/files.txt\")],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating ngrok Tunnel to Expose Local Server (Bash)\nDESCRIPTION: This Bash command uses ngrok to create a secure tunnel from the public internet to the local server running on port 3000. This is essential for testing webhooks during local development, allowing external services like ElevenLabs to send requests to the application running on the local machine.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_19\n\nLANGUAGE: Bash\nCODE:\n```\nngrok http 3000\n```\n\n----------------------------------------\n\nTITLE: Implementing redirectToExternalURL Client Tool with ElevenLabs Conversational AI Widget in JavaScript\nDESCRIPTION: This code snippet demonstrates how to set up an event listener for the ElevenLabs Conversational AI widget to handle the redirectToExternalURL client tool. The implementation listens for the 'elevenlabs-convai:call' event and defines a function to open external URLs in a new tab with security attributes.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/widget.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.addEventListener('DOMContentLoaded', () => {\n  const widget = document.querySelector('elevenlabs-convai');\n\n  if (widget) {\n    // Listen for the widget's \"call\" event to trigger client-side tools\n    widget.addEventListener('elevenlabs-convai:call', (event) => {\n      event.detail.config.clientTools = {\n        // Note: To use this example, the client tool called \"redirectToExternalURL\" (case-sensitive) must have been created with the configuration defined above.\n        redirectToExternalURL: ({ url }) => {\n          window.open(url, '_blank', 'noopener,noreferrer');\n        },\n      };\n    });\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Using the Custom Audio Native Component in a Next.js Page - TypeScript\nDESCRIPTION: This snippet shows how to import and use the ElevenLabsAudioNative React component in a Next.js page. By supplying the required publicUserId prop (retrieved from the HTML embed snippet), the audio player appears on the page. Prerequisites include the custom component and a valid user ID. Inputs are the props to the component; output is an interactive audio player widget embedded in the rendered page. This pattern supports easy reuse and layout integration across the app.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { ElevenLabsAudioNative } from './path/to/ElevenLabsAudioNative';\n\nexport default function Page() {\n  return (\n    <div>\n      <h1>Your Page Title</h1>\n\n      // Insert the public user ID from the embed code snippet\n      <ElevenLabsAudioNative publicUserId=\"<your-public-user-id>\" />\n\n      <p>Your page content...</p>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Contextual Update Event - Javascript\nDESCRIPTION: This JavaScript snippet illustrates the structure of a `contextual_update` event that can be sent from the client to the server via the WebSocket connection. This event provides non-interrupting contextual information to update the conversation state without disrupting the dialogue.  The event includes a 'type' and a 'text' field. The update is processed asynchronously.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"type\": \"contextual_update\",\n  \"text\": \"User clicked on pricing page\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updated JavaScript Client for Signed URL Authentication with ElevenLabs\nDESCRIPTION: Modified JavaScript client code that fetches a signed URL from the backend server instead of using a direct agent ID. This implementation is required for private agents and includes error handling for API requests.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nasync function getSignedUrl() {\n    const response = await fetch('http://localhost:3001/api/get-signed-url');\n    if (!response.ok) {\n        throw new Error(`Failed to get signed url: ${response.statusText}`);\n    }\n    const { signedUrl } = await response.json();\n    return signedUrl;\n}\n\nasync function startConversation() {\n    try {\n        await navigator.mediaDevices.getUserMedia({ audio: true });\n\n        const signedUrl = await getSignedUrl();\n\n        conversation = await Conversation.startSession({\n            signedUrl,\n            // agentId has been removed...\n            onConnect: () => {\n                connectionStatus.textContent = 'Connected';\n                startButton.disabled = true;\n                stopButton.disabled = false;\n            },\n            onDisconnect: () => {\n                connectionStatus.textContent = 'Disconnected';\n                startButton.disabled = false;\n                stopButton.disabled = true;\n            },\n            onError: (error) => {\n                console.error('Error:', error);\n            },\n            onModeChange: (mode) => {\n                agentStatus.textContent = mode.mode === 'speaking' ? 'speaking' : 'listening';\n            },\n        });\n    } catch (error) {\n        console.error('Failed to start conversation:', error);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Supabase Edge Functions\nDESCRIPTION: Commands to link a local Supabase project to a remote project, deploy edge functions, and set environment variables securely using the Supabase CLI.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nsupabase link\nsupabase functions deploy\nsupabase secrets set --env-file supabase/functions/.env\n```\n\n----------------------------------------\n\nTITLE: Running the JavaScript Speech-to-Text Script\nDESCRIPTION: This command executes the JavaScript script `speechToText.mjs`.  Make sure you have Node.js installed and configured correctly to run this script.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_8\n\nLANGUAGE: node\nCODE:\n```\nnode speechToText.mjs\n```\n\n----------------------------------------\n\nTITLE: Generating a Signed URL for Authentication in Next.js (TypeScript)\nDESCRIPTION: This Next.js API route creates a signed URL for secure communication with the ElevenLabs Conversational AI agent. It imports necessary modules from `elevenlabs` and `next/server`, retrieves the agent ID from environment variables, and uses the ElevenLabs client to generate a signed URL. Error handling is included to catch potential issues and return appropriate responses.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ElevenLabsClient } from 'elevenlabs';\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const agentId = process.env.ELEVENLABS_AGENT_ID;\n  if (!agentId) {\n    throw Error('ELEVENLABS_AGENT_ID is not set');\n  }\n  try {\n    const client = new ElevenLabsClient();\n    const response = await client.conversationalAi.getSignedUrl({\n      agent_id: agentId,\n    });\n    return NextResponse.json({ signedUrl: response.signed_url });\n  } catch (error) {\n    console.error('Error:', error);\n    return NextResponse.json({ error: 'Failed to get signed URL' }, { status: 500 });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Convert Speech to Text using ElevenLabs SDK in JavaScript\nDESCRIPTION: This JavaScript code snippet demonstrates how to convert speech to text using the ElevenLabs SDK. It loads the API key using dotenv, downloads an audio file, creates an audio blob, and uses the `client.speechToText.convert` method to transcribe the audio. The model ID, audio event tagging, language code, and diarization options are specified.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"dotenv/config\";\nimport { ElevenLabsClient } from \"elevenlabs\";\n\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n    \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3\"\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: \"audio/mp3\" });\n\nconst transcription = await client.speechToText.convert({\n    file: audioBlob,\n    model_id: \"scribe_v1\", // 'scribe_v1_experimental' is also available for new, experimental features\n    tag_audio_events: true, // Tag audio events like laughter, applause, etc.\n    language_code: \"eng\", // Language of the audio file. If set to null, the model will detect the language automatically.\n    diarize: true, // Whether to annotate who is speaking\n});\n\nconsole.log(transcription.text);\n```\n\n----------------------------------------\n\nTITLE: Example PLS Pronunciation Dictionary File\nDESCRIPTION: Provides a complete example of a Pronunciation Lexicon Specification (PLS) XML file. This file format is used for creating pronunciation dictionaries that can be uploaded to Eleven Labs tools like Projects. It demonstrates including multiple `<lexeme>` entries, showcasing both `<phoneme>` (using IPA) and `<alias>` methods for mapping graphemes to their desired pronunciations.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/voices/pronounciation-dictionaries.mdx#_snippet_6\n\nLANGUAGE: XML\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<lexicon version=\"1.0\"\n      xmlns=\"http://www.w3.org/2005/01/pronunciation-lexicon\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.w3.org/2005/01/pronunciation-lexicon\n        http://www.w3.org/TR/2007/CR-pronunciation-lexicon-20071212/pls.xsd\"\n      alphabet=\"ipa\" xml:lang=\"en-GB\">\n  <lexeme>\n    <grapheme>Apple</grapheme>\n    <phoneme>ˈæpl̩</phoneme>\n  </lexeme>\n  <lexeme>\n    <grapheme>UN</grapheme>\n    <alias>United Nations</alias>\n  </lexeme>\n</lexicon>\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs SDK with npm\nDESCRIPTION: This command installs the ElevenLabs SDK using npm, the Node package manager. It's a prerequisite for using the ElevenLabs SDK in JavaScript projects.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nnpm install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Configuring chunk_length_schedule for Audio Buffering with ElevenLabs WebSocket (TypeScript)\nDESCRIPTION: This TypeScript snippet illustrates sending a text-to-speech request via WebSocket to ElevenLabs with a custom generation_config.chunk_length_schedule, defining points at which buffered audio is generated. Required dependencies: a connected WebSocket object, ELEVENLABS_API_KEY, and the input text. The payload is stringified as JSON. Parameters such as text, chunk_length_schedule (array of integer thresholds), and xi_api_key are included. Adjust chunk_length_schedule to balance audio latency and quality as needed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nwebsocket.send(\n  JSON.stringify({\n    text: text,\n    // Generate audio after 50, 120, 160, and 290 characters have been sent\n    generation_config: { chunk_length_schedule: [50, 120, 160, 290] },\n    xi_api_key: ELEVENLABS_API_KEY,\n  })\n);\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using pnpm\nDESCRIPTION: This command installs all project dependencies using pnpm. It fetches and installs all necessary packages as specified in the project's package.json and any other relevant dependency files. This step is required before running any other build or development commands.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: Downloading Dubbed File ElevenLabs SDK Python\nDESCRIPTION: This Python function downloads the completed dubbed file for a specific dubbing project and language code. It retrieves the file content in chunks using `client.dubbing.get_dubbed_file` and saves it to a local file path derived from the dubbing ID and language.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef download_dubbed_file(dubbing_id: str, language_code: str) -> str:\n    \"\"\"\n    Downloads the dubbed file for a given dubbing ID and language code.\n\n    Args:\n        dubbing_id: The ID of the dubbing project.\n        language_code: The language code for the dubbing.\n\n    Returns:\n        The file path to the downloaded dubbed file.\n    \"\"\"\n    dir_path = f\"data/{dubbing_id}\"\n    os.makedirs(dir_path, exist_ok=True)\n\n    file_path = f\"{dir_path}/{language_code}.mp4\"\n    with open(file_path, \"wb\") as file:\n        for chunk in client.dubbing.get_dubbed_file(dubbing_id, language_code):\n            file.write(chunk)\n\n    return file_path\n\n```\n\n----------------------------------------\n\nTITLE: Adding Rules to Pronunciation Dictionary\nDESCRIPTION: This snippet demonstrates how to programmatically add rules to a pronunciation dictionary using `PronunciationDictionaryRule_Phoneme`.  It adds rules for \"tomato\" and \"Tomato\" using the IPA alphabet and specifies the desired phoneme. Then, it generates and plays an audio file to test the new pronunciation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom elevenlabs import PronunciationDictionaryRule_Phoneme\n\npronunciation_dictionary_rules_added = client.pronunciation_dictionary.add_rules_to_the_pronunciation_dictionary(\n    pronunciation_dictionary_id=pronunciation_dictionary_rules_removed.id,\n    rules=[\n        PronunciationDictionaryRule_Phoneme(\n            type=\"phoneme\",\n            alphabet=\"ipa\",\n            string_to_replace=\"tomato\",\n            phoneme=\"/tə'meɪtoʊ/\",\n        ),\n        PronunciationDictionaryRule_Phoneme(\n            type=\"phoneme\",\n            alphabet=\"ipa\",\n            string_to_replace=\"Tomato\",\n            phoneme=\"/tə'meɪtoʊ/\",\n        ),\n    ],\n)\n\naudio_4 = client.generate(\n    text=\"With the rule added again: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n    pronunciation_dictionary_locators=[\n        PronunciationDictionaryVersionLocator(\n            pronunciation_dictionary_id=pronunciation_dictionary_rules_added.id,\n            version_id=pronunciation_dictionary_rules_added.version_id,\n        )\n    ],\n)\n\nplay(audio_4)\n```\n\n----------------------------------------\n\nTITLE: Adding Bulk Workspace Invites via API\nDESCRIPTION: Introduces the POST `v1/workspace/invites/add-bulk` endpoint, allowing multiple users to be invited to a workspace simultaneously via the API. Facilitates bulk user onboarding.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /v1/workspace/invites/add-bulk\n```\n\n----------------------------------------\n\nTITLE: Running the Development Server\nDESCRIPTION: This command starts the development server, likely for the documentation, using pnpm. It will likely build and serve the documentation locally. The exact behavior depends on the project's `package.json` scripts. Make sure the dependencies are installed prior to execution.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dotenv\nDESCRIPTION: This command installs the `python-dotenv` library using pip. This library is used to load environment variables, such as your ElevenLabs API key, from a `.env` file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Making a Text to Speech Request with ElevenLabs TypeScript SDK\nDESCRIPTION: This TypeScript script demonstrates initializing the ElevenLabs client (implicitly using environment variables for the API key via `dotenv/config`), converting text to speech using the `textToSpeech.convert` method with a specific voice ID and options (text, model ID, output format), and playing the generated audio using the SDK's `play` function. Requires the `elevenlabs` and `dotenv` packages.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ElevenLabsClient, play } from \"elevenlabs\";\nimport \"dotenv/config\";\n\nconst client = new ElevenLabsClient();\nconst audio = await client.textToSpeech.convert(\"JBFqnCBsd6RMkjVDRZzb\", {\n  text: \"The first move is what sets everything in motion.\",\n  model_id: \"eleven_multilingual_v2\",\n  output_format: \"mp3_44100_128\",\n});\n\nawait play(audio);\n```\n\n----------------------------------------\n\nTITLE: Handling Media Messages\nDESCRIPTION: This section handles incoming voice, audio, and video messages. It fetches the file URL from Telegram, calls the scribe function in the background using EdgeRuntime.waitUntil(), and sends a receiving message to the user.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nbot.on([':voice', ':audio', ':video'], async (ctx) => {\n  try {\n    const file = await ctx.getFile();\n    const fileURL = `https://api.telegram.org/file/bot${telegramBotToken}/${file.file_path}`;\n    const fileMeta = ctx.message?.video ?? ctx.message?.voice ?? ctx.message?.audio;\n\n    if (!fileMeta) {\n      return ctx.reply('No video|audio|voice metadata found. Please try again.');\n    }\n\n    // Run the transcription in the background.\n    EdgeRuntime.waitUntil(\n      scribe({\n        fileURL,\n        fileType: fileMeta.mime_type!,\n        duration: fileMeta.duration,\n        chatId: ctx.chat.id,\n        messageId: ctx.message?.message_id!,\n        username: ctx.from?.username || '',\n      })\n    );\n\n    // Reply to the user immediately to let them know we received their file.\n    return ctx.reply('Received. Scribing...');\n  } catch (error) {\n    console.error(error);\n    return ctx.reply(\n      'Sorry, there was an error getting the file. Please try again with a smaller file!'\n    );\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Importing ElevenLabs SDK - Swift\nDESCRIPTION: Imports the ElevenLabsSDK module to allow usage of its classes and functions within a Swift file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_0\n\nLANGUAGE: swift\nCODE:\n```\nimport ElevenLabsSDK\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables for ElevenLabs Authentication\nDESCRIPTION: Environment variables configuration for storing API key and agent ID, required for private agent authentication.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_7\n\nLANGUAGE: env\nCODE:\n```\nELEVENLABS_API_KEY=your-api-key-here\nAGENT_ID=your-agent-id-here\n```\n\n----------------------------------------\n\nTITLE: Adding Ctrl+C Signal Handler for Clean Shutdown - Python\nDESCRIPTION: Registers a signal handler for 'SIGINT' (triggered by Ctrl+C) that ensures 'conversation.end_session()' is called for a graceful shutdown. This prevents resource leaks and may be necessary to save conversation data or release audio resources cleanly. Must be set before entering a blocking wait for session completion.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsignal.signal(signal.SIGINT, lambda sig, frame: conversation.end_session())\n```\n\n----------------------------------------\n\nTITLE: Creating Express Backend Server for ElevenLabs Signed URL Authentication\nDESCRIPTION: Node.js Express server implementation that handles API key authentication and obtains signed URLs for private ElevenLabs agents. The server exposes an endpoint that securely forwards requests to the ElevenLabs API.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/vite.mdx#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nrequire(\"dotenv\").config();\n\nconst express = require(\"express\");\nconst cors = require(\"cors\");\n\nconst app = express();\napp.use(cors());\napp.use(express.json());\n\nconst PORT = process.env.PORT || 3001;\n\napp.get(\"/api/get-signed-url\", async (req, res) => {\n    try {\n        const response = await fetch(\n            `https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=${process.env.AGENT_ID}`,\n            {\n                headers: {\n                    \"xi-api-key\": process.env.ELEVENLABS_API_KEY,\n                },\n            }\n        );\n\n        if (!response.ok) {\n            throw new Error(\"Failed to get signed URL\");\n        }\n\n        const data = await response.json();\n        res.json({ signedUrl: data.signed_url });\n    } catch (error) {\n        console.error(\"Error:\", error);\n        res.status(500).json({ error: \"Failed to generate signed URL\" });\n    }\n});\n\napp.listen(PORT, () => {\n    console.log(`Server running on http://localhost:${PORT}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Create Supabase Edge Function (Bash)\nDESCRIPTION: Creates a new Supabase Edge Function named 'text-to-speech' using the Supabase CLI. This command generates the basic function file structure within the `supabase/functions` directory and can optionally configure VS Code Deno settings.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsupabase functions new text-to-speech\n```\n\n----------------------------------------\n\nTITLE: Normalizing text for Text to Speech with inflect in Python\nDESCRIPTION: This Python snippet normalizes text inputs by converting monetary values and phone numbers into words suitable for Text to Speech output. It requires the 'inflect' library for number-to-word conversion and uses regex to identify currency and phone number patterns. The 'normalize_text' function replaces currency amounts with their spoken equivalents (handling decimals as cents) and spells out phone numbers digit by digit. Inputs are strings containing numbers or currency; outputs are strings with expanded spoken forms. Limitations include support only for specified currency symbols ($, £, €, ¥) and US-style phone number formats.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/normalization.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Be sure to install the inflect library before running this code\nimport inflect\nimport re\n\n# Initialize inflect engine for number-to-word conversion\np = inflect.engine()\n\ndef normalize_text(text: str) -> str:\n    # Convert monetary values\n    def money_replacer(match):\n        currency_map = {\"$\": \"dollars\", \"£\": \"pounds\", \"€\": \"euros\", \"¥\": \"yen\"}\n        currency_symbol, num = match.groups()\n\n        # Remove commas before parsing\n        num_without_commas = num.replace(',', '')\n\n        # Check for decimal points to handle cents\n        if '.' in num_without_commas:\n            dollars, cents = num_without_commas.split('.')\n            dollars_in_words = p.number_to_words(int(dollars))\n            cents_in_words = p.number_to_words(int(cents))\n            return f\"{dollars_in_words} {currency_map.get(currency_symbol, 'currency')} and {cents_in_words} cents\"\n        else:\n            # Handle whole numbers\n            num_in_words = p.number_to_words(int(num_without_commas))\n            return f\"{num_in_words} {currency_map.get(currency_symbol, 'currency')}\"\n\n    # Regex to handle commas and decimals\n    text = re.sub(r\"([$£€¥])(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\", money_replacer, text)\n\n    # Convert phone numbers\n    def phone_replacer(match):\n        return \", \".join(\" \".join(p.number_to_words(int(digit)) for digit in group) for group in match.groups())\n\n    text = re.sub(r\"(\\d{3})-(\\d{3})-(\\d{4})\", phone_replacer, text)\n\n    return text\n\n# Example usage\nprint(normalize_text(\"$1,000\"))   # \"one thousand dollars\"\nprint(normalize_text(\"£1000\"))   # \"one thousand pounds\"\nprint(normalize_text(\"€1000\"))   # \"one thousand euros\"\nprint(normalize_text(\"¥1000\"))   # \"one thousand yen\"\nprint(normalize_text(\"$1,234.56\"))   # \"one thousand two hundred thirty-four dollars and fifty-six cents\"\nprint(normalize_text(\"555-555-5555\"))  # \"five five five, five five five, five five five five\"\n\n```\n\n----------------------------------------\n\nTITLE: Removing Rules from a Dictionary\nDESCRIPTION: This code demonstrates how to remove pronunciation rules from an existing pronunciation dictionary.  It removes rules for \"tomato\" and \"Tomato\". The updated version of the dictionary is retrieved and used to generate and play a new audio file, and also specifying the specific dictionary id with its latest version in order to apply the changes to text-to-speech.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npronunciation_dictionary_rules_removed = (\n    client.pronunciation_dictionary.remove_rules_from_the_pronunciation_dictionary(\n        pronunciation_dictionary_id=pronunciation_dictionary.id,\n        rule_strings=[\"tomato\", \"Tomato\"],\n    )\n)\n\naudio_3 = client.generate(\n    text=\"With the rule removed: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n    pronunciation_dictionary_locators=[\n        PronunciationDictionaryVersionLocator(\n            pronunciation_dictionary_id=pronunciation_dictionary_rules_removed.id,\n            version_id=pronunciation_dictionary_rules_removed.version_id,\n        )\n    ],\n)\n\nplay(audio_3)\n```\n\n----------------------------------------\n\nTITLE: Session Config with Agent ID - Swift\nDESCRIPTION: Initializes a SessionConfig object with an agent ID for the ElevenLabs conversational AI. Replace \"<your-agent-id>\" with the actual agent ID obtained from the ElevenLabs UI.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_2\n\nLANGUAGE: swift\nCODE:\n```\nlet config = ElevenLabsSDK.SessionConfig(agentId: \"<your-agent-id>\")\n```\n\n----------------------------------------\n\nTITLE: Using Alias Tags for Name Pronunciation in ElevenLabs Lexicons (XML)\nDESCRIPTION: Demonstrates using `<lexeme>` and `<alias>` tags within a pronunciation dictionary (.pls format) to specify a preferred pronunciation (\"Cloffton\") for a grapheme (\"Claughton\") in ElevenLabs TTS. This is useful for models like Multilingual v2 or Turbo v2.5 that don't support phoneme tags.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n  <lexeme>\n    <grapheme>Claughton</grapheme>\n    <alias>Cloffton</alias>\n  </lexeme>\n```\n\n----------------------------------------\n\nTITLE: Starting Fastify HTTP/WebSocket Server with Port Configuration (JavaScript)\nDESCRIPTION: Calls fastify.listen with a specified PORT (default 8000) to start the HTTP and WebSocket server. It includes an error callback that logs any startup issues and exits the process. On successful startup, it logs the listening port to stdout. This is the server entry point to begin accepting inbound HTTP and WebSocket requests for Twilio and ElevenLabs integration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nfastify.listen({ port: PORT }, (err) => {\n  if (err) {\n    console.error('Error starting server:', err);\n    process.exit(1);\n  }\n  console.log(`[Server] Listening on port ${PORT}`);\n});\n```\n\n----------------------------------------\n\nTITLE: Return Conversation Initiation Client Data - JSON Example\nDESCRIPTION: This JSON example demonstrates the structure of the data that your webhook should return. It includes the 'type', 'dynamic_variables', and 'conversation_config_override' fields. 'dynamic_variables' contain context about the caller, while 'conversation_config_override' allows customizing agent behavior, first message, and TTS settings such as voice ID. This data personalizes the conversation based on the incoming Twilio call.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/dynamic-variables/twilio-inbound-integration.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"conversation_initiation_client_data\",\n  \"dynamic_variables\": {\n    \"customer_name\": \"John Doe\",\n    \"account_status\": \"premium\",\n    \"last_interaction\": \"2024-01-15\"\n  },\n  \"conversation_config_override\": {\n    \"agent\": {\n      \"prompt\": {\n        \"prompt\": \"The customer's bank account balance is $100. They are based in San Francisco.\"\n      },\n      \"first_message\": \"Hi, how can I help you today?\",\n      \"language\": \"en\"\n    },\n    \"tts\": {\n      \"voice_id\": \"new-voice-id\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Punctuation for Pauses in ElevenLabs TTS (Text)\nDESCRIPTION: Shows alternative, less consistent methods for creating pauses using punctuation like ellipses (...) for hesitation and dashes (-) or em dashes (—) for short breaks in ElevenLabs TTS.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"It… well, it might work.\" \"Wait — what’s that noise?\"\n```\n\n----------------------------------------\n\nTITLE: Updating Agent Endpoints with Privacy Controls via API\nDESCRIPTION: Updates Agent endpoints (e.g., creation/update via POST/PATCH `/v1/agents`) with consolidated privacy control parameters for enhanced GDPR compliance and data management.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_7\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST/PATCH /v1/agents (with privacy control parameters)\n```\n\n----------------------------------------\n\nTITLE: Example: Task-Focused Agent Personality Prompt\nDESCRIPTION: Specifies the personality for a telecom customer support agent named Ava. This prompt focuses on task-oriented traits such as friendliness, efficiency, and a solution-oriented approach, ensuring polite and effective customer interaction.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# Personality\n\nYou are Ava, a customer support agent for a telecom company.\nYou are friendly, solution-oriented, and efficient.\nYou address customers by name, politely guiding them toward a resolution.\n```\n\n----------------------------------------\n\nTITLE: Managing Session (End Session) - Swift\nDESCRIPTION: Code shows how to start and end the conversation session using the `startSession()` and `endSession()` functions on the `conversation` object.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_6\n\nLANGUAGE: swift\nCODE:\n```\n// Starts the session\nconversation.startSession()\n// Ends the session\nconversation.endSession()\n```\n\n----------------------------------------\n\nTITLE: Setting ElevenLabs API Key in .env\nDESCRIPTION: This example shows how to define the ElevenLabs API key in a `.env` file. The API key is used to authenticate with the ElevenLabs service.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nELEVENLABS_API_KEY=your_elevenlabs_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Executing Python Script\nDESCRIPTION: This command line snippet shows how to execute the Python script created in the previous step. It assumes Python is installed and the script file `example.py` is in the current directory. Running this command will execute the Python code, generate the sound effect, and play it.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/quickstart.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Voice Based on Description\nDESCRIPTION: This snippet utilizes the ElevenLabs API to design a custom voice. It takes `voice_description` from analysis results and uses it to generate a voice preview using `textToVoice.createPreviews`.  It then uses the preview to create the actual voice with `textToVoice.createVoiceFromPreview` and assigns it to the agent.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst voicePreview = await elevenLabsClient.textToVoice.createPreviews({\n  voice_description: analysis.data_collection_results.voice_description.value,\n  text: 'The night air carried whispers of betrayal, thick as London fog. I adjusted my cufflinks - after all, even spies must maintain appearances, especially when the game is afoot.',\n});\nconst voice = await elevenLabsClient.textToVoice.createVoiceFromPreview({\n  voice_name: `voice-${conversation_id}`,\n  voice_description: `Voice for ${conversation_id}`,\n  generated_voice_id: voicePreview.previews[0].generated_voice_id,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for ElevenLabs and Twilio Integration in Text\nDESCRIPTION: This .env configuration snippet defines environment variables for the ElevenLabs agent ID and API key, which are mandatory for authenticating requests to the ElevenLabs API when processing conversational AI interactions. Users must replace placeholders with their actual credentials. This file serves as a central point for managing sensitive configuration data securely within the project.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nELEVENLABS_AGENT_ID=<your-agent-id>\nELEVENLABS_API_KEY=<your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Webhook Tool Configuration for Retrieving Resolved Zendesk Tickets - markdown\nDESCRIPTION: Describes the GET webhook tool 'get_resolved_tickets' used to fetch all solved support tickets from Zendesk. Requires JSON content type and authorization header with a secret key. This tool allows the conversational AI to access historical ticket data for similarity matching and resolution suggestion.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/zendesk.mdx#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n**Name:** get_resolved_tickets  \n**Description:** Retrieves all resolved support tickets from Zendesk.  \n**Method:** GET  \n**URL:** `https://your-subdomain.zendesk.com/api/v2/search.json?query=type:ticket+status:solved`\n\n**Headers:**\n- **Content-Type:** `application/json`\n- **Authorization:** *(Secret: `zendesk_key`)*\n```\n\n----------------------------------------\n\nTITLE: Adding Dictionary from File & Text-to-Speech\nDESCRIPTION: This code snippet demonstrates how to add rules to a pronunciation dictionary from a file (`dictionary.pls`) and then generate two audio files. The first audio file uses the default pronunciation. The second utilizes the custom pronunciation dictionary to apply the new pronunciation rules to the word \"tomato\". The `PronunciationDictionaryVersionLocator` is used to specify the version of the dictionary to use.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nfrom elevenlabs import play, PronunciationDictionaryVersionLocator\n\nwith open(\"dictionary.pls\", \"rb\") as f:\n    # this dictionary changes how tomato is pronounced\n    pronunciation_dictionary = client.pronunciation_dictionary.add_from_file(\n        file=f.read(), name=\"example\"\n    )\n\naudio_1 = client.generate(\n    text=\"Without the dictionary: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n)\n\naudio_2 = client.generate(\n    text=\"With the dictionary: tomato\",\n    voice=\"Rachel\",\n    model=\"eleven_turbo_v2\",\n    pronunciation_dictionary_locators=[\n        PronunciationDictionaryVersionLocator(\n            pronunciation_dictionary_id=pronunciation_dictionary.id,\n            version_id=pronunciation_dictionary.version_id,\n        )\n    ],\n)\n\n# play the audio\nplay(audio_1)\nplay(audio_2)\n```\n\n----------------------------------------\n\nTITLE: Isolating Audio using Voice Isolator API - Python\nDESCRIPTION: This Python code snippet demonstrates how to remove background noise from an audio file using the ElevenLabs Voice Isolator API. It uses the `elevenlabs` library and the `requests` library to download the audio file.  It requires an API key set as the `ELEVENLABS_API_KEY` environment variable. The `play` function requires MPV and/or ffmpeg to be installed to play the isolated audio.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-isolator/quickstart.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# example.py\nimport os\nfrom dotenv import load_dotenv\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs import play\nimport requests\nfrom io import BytesIO\n\nload_dotenv()\n\nclient = ElevenLabs(\n  api_key=os.getenv(\"ELEVENLABS_API_KEY\"),\n)\n\naudio_url = \"https://storage.googleapis.com/eleven-public-cdn/audio/marketing/fin.mp3\"\nresponse = requests.get(audio_url)\naudio_data = BytesIO(response.content)\n\naudio_stream = client.audio_isolation.audio_isolation(audio=audio_data)\n\nplay(audio_stream)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Next.js ElevenLabs Conversational AI Client in Bash\nDESCRIPTION: This snippet installs the required packages to handle microphone input streaming and styling for the Next.js project. The 'voice-stream' package captures audio from the microphone and encodes it in base64 format suitable for ElevenLabs API usage. Additionally, Tailwind CSS and its associated PostCSS and Autoprefixer plugins are installed for styling, with instructions provided to initialize Tailwind configuration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install voice-stream\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs Node.js Library with npm - Bash\nDESCRIPTION: This snippet outlines installing the official ElevenLabs library for Node.js projects using npm. Requires Node.js and npm to be installed and initialized in your project directory. The command downloads elevenlabs from the npm registry and adds it to your project's dependencies in package.json. It should be run from the root directory of your Node.js project. Outputs installation progress and resolves necessary dependencies. Limitations: Requires access to the npm registry and an initialized Node.js environment.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/introduction.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Running the TypeScript Text to Speech Example\nDESCRIPTION: This shell command uses `npx tsx` to execute the TypeScript script (`example.mts`) which makes the ElevenLabs API request. This command compiles and runs the TypeScript code, initiating the text-to-speech process and playing the audio output.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Executing the TypeScript Voice Transformation Script\nDESCRIPTION: This command executes the TypeScript script for voice transformation via the ElevenLabs API using the 'tsx' runner. Prerequisites include Node.js, the 'elevenlabs' npm package, and a proper '.env' configuration for the API key. Input expected is the 'example.mts' file previously created.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-changer/quickstart.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n\n```\n\n----------------------------------------\n\nTITLE: Cloning Voice - Eleven Labs TypeScript SDK\nDESCRIPTION: This snippet demonstrates creating an Instant Voice Clone with the Eleven Labs TypeScript SDK. It configures the client and uses the `voices.add` method, providing a name and a list of audio file streams created from file paths using Node.js `fs.createReadStream`. The script outputs the ID of the created voice.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/clone-voice.mdx#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// example.mts\nimport { ElevenLabsClient } from \"elevenlabs\";\nimport \"dotenv/config\";\nimport fs from \"node:fs\";\n\nconst client = new ElevenLabsClient();\n\nconst voice = await client.voices.add({\n    name: \"My Voice Clone\",\n    // Replace with the paths to your audio files.\n    // The more files you add, the better the clone will be.\n    files: [\n        fs.createReadStream(\n            \"/path/to/your/audio/file.mp3\",\n        ),\n    ],\n});\n\nconsole.log(voice.voice_id);\n```\n\n----------------------------------------\n\nTITLE: Getting Customer Details Tool - JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to use client tools to get customer details. It defines an asynchronous function within the clientTools object to simulate fetching customer data and returning it. This function is executed when the agent calls the getCustomerDetails tool. The tool then returns the customer data.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst clientTools = {\n  getCustomerDetails: async () => {\n    # Fetch customer details (e.g., from an API)\n    const customerData = {\n      id: 123,\n      name: \"Alice\",\n      subscription: \"Pro\"\n    };\n    # Return data directly to the agent.\n    return customerData;\n  }\n};\n\n# Start the conversation with client tools configured.\nconst conversation = await Conversation.startSession({ clientTools });\n```\n\n----------------------------------------\n\nTITLE: Generating a Signed URL via the ElevenLabs API\nDESCRIPTION: Demonstrates how to obtain a temporary signed WebSocket URL from the ElevenLabs API using Python, JavaScript SDKs, or a direct cURL request. This server-side operation requires an ElevenLabs API key and the Agent ID, returning a URL valid for 15 minutes for client-side authentication without exposing the key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Server-side code using the Python SDK\nfrom elevenlabs.client import ElevenLabs\nasync def get_signed_url():\n    try:\n        client = ElevenLabs(api_key=\"your-api-key\")\n        response = await client.conversational_ai.get_signed_url(agent_id=\"your-agent-id\")\n        return response.signed_url\n    except Exception as error:\n        print(f\"Error getting signed URL: {error}\")\n        raise\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ElevenLabsClient } from 'elevenlabs';\n\n// Server-side code using the JavaScript SDK\nconst client = new ElevenLabsClient({ apiKey: 'your-api-key' });\nasync function getSignedUrl() {\n  try {\n    const response = await client.conversationalAi.getSignedUrl({\n      agent_id: 'your-agent-id',\n    });\n\n    return response.signed_url;\n  } catch (error) {\n    console.error('Error getting signed URL:', error);\n    throw error;\n  }\n}\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url?agent_id=your-agent-id\" \\\n-H \"xi-api-key: your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Getting Customer Details Tool - Python\nDESCRIPTION: This Python code demonstrates a client tool to get customer details. It defines a function that simulates fetching customer data and returns it. The tool is then registered with the client tools object, which is then used when initiating the conversation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_customer_details():\n    # Fetch customer details (e.g., from an API or database)\n    customer_data = {\n        \"id\": 123,\n        \"name\": \"Alice\",\n        \"subscription\": \"Pro\"\n    }\n    # Return the customer data; it can also be a JSON string if needed.\n    return customer_data\n\nclient_tools = ClientTools()\nclient_tools.register(\"getCustomerDetails\", get_customer_details)\n\nconversation = Conversation(\n    client=ElevenLabs(api_key=\"your-api-key\"),\n    agent_id=\"your-agent-id\",\n    client_tools=client_tools,\n    # ...\n)\n\nconversation.start_session()\n```\n\n----------------------------------------\n\nTITLE: Initializing Telegram Bot and Start Command\nDESCRIPTION: This part initializes the Telegram bot using the bot token from environment variables and defines the '/start' command. The start command sends a welcome message with information about the bot and links to the ElevenLabs speech-to-text documentation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst telegramBotToken = Deno.env.get('TELEGRAM_BOT_TOKEN');\nconst bot = new Bot(telegramBotToken || '');\nconst startMessage = `Welcome to the ElevenLabs Scribe Bot\\! I can transcribe speech in 99 languages with super high accuracy\\!\n    \nTry it out by sending or forwarding me a voice message, video, or audio file\\!\n    \n[Learn more about Scribe](https://elevenlabs.io/speech-to-text) or [build your own bot](https://elevenlabs.io/docs/cookbooks/speech-to-text/telegram-bot)\\!\n  `;\nbot.command('start', (ctx) => ctx.reply(startMessage.trim(), { parse_mode: 'MarkdownV2' }));\n```\n\n----------------------------------------\n\nTITLE: Example JSON Request Format with Extra Parameters for Custom LLM\nDESCRIPTION: An example of the JSON request format that will be sent to your custom LLM server when using extra parameters. This shows how the elevenlabs_extra_body field contains the custom parameters you defined.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"\\n  <Redacted>\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hey I'm currently unavailable.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hey, who are you?\"\n    }\n  ],\n  \"model\": \"gpt-4o\",\n  \"temperature\": 0.5,\n  \"max_tokens\": 5000,\n  \"stream\": true,\n  \"elevenlabs_extra_body\": {\n    \"UUID\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"parameter-1\": \"value-1\",\n    \"parameter-2\": \"value-2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining API Key in .env file for JavaScript SDKs\nDESCRIPTION: This snippet demonstrates how to define the ElevenLabs API key in a .env file, which can be loaded as an environment variable for use in SDKs or applications. It serves as a template for securely storing and accessing the API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/quickstart-api-key.mdx#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nELEVENLABS_API_KEY=<your_api_key_here>\n```\n\n----------------------------------------\n\nTITLE: Getting Signed URL - Swift\nDESCRIPTION: Asynchronously fetches a signed URL from an API endpoint, which is required for authorized conversations with ElevenLabs. It uses URLSession to make a request to the specified endpoint and decodes the JSON response to extract the signed URL.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_3\n\nLANGUAGE: swift\nCODE:\n```\n// Swift example using URLSession\nfunc getSignedUrl() async throws -> String {\n    let url = URL(string: \"https://api.elevenlabs.io/v1/convai/conversation/get_signed_url\")!\n    var request = URLRequest(url: url)\n    request.setValue(\"YOUR-API-KEY\", forHTTPHeaderField: \"xi-api-key\")\n    \n    let (data, _) = try await URLSession.shared.data(for: request)\n    let response = try JSONDecoder().decode(SignedUrlResponse.self, from: data)\n    return response.signedUrl\n}\n\n// Use the signed URL\nlet signedUrl = try await getSignedUrl()\nlet config = ElevenLabsSDK.SessionConfig(signedUrl: signedUrl)\n```\n\n----------------------------------------\n\nTITLE: Recording Controls - Swift\nDESCRIPTION: Code snippet shows the functions to use to start and stop recording during the conversation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/swift.mdx#_snippet_7\n\nLANGUAGE: swift\nCODE:\n```\n// Start recording\nconversation.startRecording()\n\n// Stop recording\nconversation.stopRecording()\n```\n\n----------------------------------------\n\nTITLE: Specifying ElevenLabs API Key in Bash Header\nDESCRIPTION: Illustrates the required HTTP header format (`xi-api-key`) for authenticating requests to the ElevenLabs API. The placeholder `ELEVENLABS_API_KEY` must be replaced with your actual secret API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/authentication.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nxi-api-key: ELEVENLABS_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Defining Customer Service Guardrails\nDESCRIPTION: This snippet establishes guardrails for a customer service agent. It outlines rules to prevent inappropriate responses, such as avoiding advice on competitors, protecting customer data, and maintaining a professional tone. It also specifies error handling and escalation strategies.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_12\n\nLANGUAGE: mdx\nCODE:\n```\n# Guardrails\n\nRemain within the scope of company products and services; politely decline requests for advice on competitors or unrelated industries.\nNever share customer data across conversations or reveal sensitive account information without proper verification.\nAcknowledge when you don't know an answer instead of guessing, offering to escalate or research further.\nMaintain a professional tone even when users express frustration; never match negativity or use sarcasm.\nIf the user requests actions beyond your capabilities (like processing refunds or changing account settings), clearly explain the limitation and offer the appropriate alternative channel.\n```\n\n----------------------------------------\n\nTITLE: Streaming Speech to Text in JavaScript\nDESCRIPTION: JavaScript implementation to convert audio to text using ElevenLabs SDK with streaming response. It fetches an audio file from a URL and processes it chunk by chunk with additional configuration options.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as dotenv from 'dotenv';\nimport { ElevenLabsClient } from 'elevenlabs';\n\ndotenv.config();\n\nconst client = new ElevenLabsClient();\n\nconst response = await fetch(\n  'https://storage.googleapis.com/eleven-public-cdn/audio/marketing/nicole.mp3'\n);\nconst audioBlob = new Blob([await response.arrayBuffer()], { type: 'audio/mp3' });\n\n// const file = fs.createReadStream(\"/Users/pma/Desktop/nicole.mp3\")\n\nconst transcription = await client.speechToText.convertAsStream({\n  file: audioBlob,\n  model_id: 'scribe_v1',\n  maximum_speakers: 1, // Number of speakers in the audio file\n  tag_audio_events: true, // Tag audio events like laughter, applause, etc.\n  speech_to_text_language_code: 'en', // Language of the audio file\n  transcribe_verbatim: false, // Transcribe verbatim sounds like \"um\" and \"ah\"\n});\n\nfor await (const chunk of transcription) {\n  console.log(chunk.text);\n}\n```\n\n----------------------------------------\n\nTITLE: Conveying Emotion through Dialogue Tags in ElevenLabs TTS (Text)\nDESCRIPTION: Illustrates how to guide the AI's emotional delivery in ElevenLabs TTS by including explicit dialogue tags (e.g., \"her voice trembling with sadness\", \"he exclaimed triumphantly\") within the input text. This yields more predictable results than context alone, but the tags might be spoken and require removal in post-production.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nYou’re leaving?\" she asked, her voice trembling with sadness. \"That’s it!\" he exclaimed triumphantly.\n```\n\n----------------------------------------\n\nTITLE: Rendering Product Guides Card with Image Using React JSX\nDESCRIPTION: This snippet displays a single-column CardGroup with one Card linking to the product guides overview. The card includes a flexbox layout with text describing product guides and an accompanying static image illustrating the voice library. The image has properties preventing pointer events and fixes the width to 200px. This React JSX snippet depends on CardGroup and Card components and provides a visually engaging link to product guides within the documentation site.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#_snippet_4\n\nLANGUAGE: jsx\nCODE:\n```\n<CardGroup cols={1}>\n\n<Card href=\"/docs/product-guides/overview\">\n  <div className=\"flex items-center justify-between\">\n    <div className=\"text-left\">\n      <div className=\"t-default text-base font-semibold\">Product guides</div>\n      <p>Explore our product guides for step-by-step guidance</p>\n    </div>\n    <div className=\"flex items-center justify-center\">\n      <img\n        src=\"/assets/images/product-guides/voices/voice-library.webp\"\n        alt=\"Voice library\"\n        style={{ pointerEvents: 'none', width: '200px' }}\n      />\n    </div>\n  </div>\n</Card>\n\n</CardGroup>\n```\n\n----------------------------------------\n\nTITLE: Initiating ElevenLabs WebSocket Connection\nDESCRIPTION: Sets up the initial variables (API key, voice ID, model ID) and establishes an asynchronous WebSocket connection to the ElevenLabs Text-to-Speech streaming endpoint using the chosen voice and model.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/websockets.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom dotenv import load_dotenv\nimport websockets\n\n# Load the API key from the .env file\nload_dotenv()\nELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")\n\nvoice_id = 'Xb7hH8MSUJpSbSDYk0k2'\n\n# For use cases where latency is important, we recommend using the 'eleven_flash_v2_5' model.\nmodel_id = 'eleven_flash_v2_5'\n\nasync def text_to_speech_ws_streaming(voice_id, model_id):\n    uri = f\"wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input?model_id={model_id}\"\n\n    async with websockets.connect(uri) as websocket:\n       ...\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as dotenv from 'dotenv';\nimport * as fs from 'node:fs';\nimport WebSocket from 'ws';\n\n// Load the API key from the .env file\ndotenv.config();\nconst ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;\n\nconst voiceId = 'Xb7hH8MSUJpSbSDYk0k2';\n\n// For use cases where latency is important, we recommend using the 'eleven_flash_v2_5' model.\nconst model = 'eleven_flash_v2_5';\n\nconst uri = `wss://api.elevenlabs.io/v1/text-to-speech/${voiceId}/stream-input?model_id=${model}`;\nconst websocket = new WebSocket(uri, {\n  headers: { 'xi-api-key': `${ELEVENLABS_API_KEY}` },\n});\n\n// Create a directory for saving the audio\nconst outputDir = './output';\n\ntry {\n  fs.accessSync(outputDir, fs.constants.R_OK | fs.constants.W_OK);\n} catch (err) {\n  fs.mkdirSync(outputDir);\n}\n\n// Create a write stream for saving the audio into mp3\nconst writeStream = fs.createWriteStream(outputDir + '/test.mp3', {\n  flags: 'a',\n});\n```\n\n----------------------------------------\n\nTITLE: Setting the Telegram Bot Webhook URL\nDESCRIPTION: Constructs a URL to be used for setting the Telegram bot's webhook via the Telegram Bot API. This involves making a GET request (e.g., in a browser or with `curl`) to `https://api.telegram.org/bot<TELEGRAM_BOT_TOKEN>/setWebhook`. The `url` parameter points to the deployed Supabase Edge Function URL and includes a `secret` query parameter for authentication.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nhttps://api.telegram.org/bot<TELEGRAM_BOT_TOKEN>/setWebhook?url=https://<PROJECT_REFERENCE>.supabase.co/functions/v1/scribe-bot?secret=<FUNCTION_SECRET>\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Collection for Conversation Insights\nDESCRIPTION: Provides instructions for extracting key data points from conversations, including order details, delivery zone, and interaction type. Facilitates detailed analytics and performance tracking for the assistant.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nExtract order details from the conversation, including:\n- Type of order (delivery, pickup, inquiry_only)\n- List of pierogi varieties and quantities ordered in the format: \"item: quantity\"\n- Delivery zone based on the address (central_zakopane, outer_zakopane, outside_delivery_zone)\n- Interaction type (completed_order, abandoned_order, menu_inquiry, general_inquiry)\nIf no order was placed, return \"none\"\n\n```\n\n----------------------------------------\n\nTITLE: Updating Workspace Invite with Group IDs via API\nDESCRIPTION: Enhances the Workspace Invite endpoint by adding the optional `group_ids` parameter, enabling the assignment of invited users to specific groups for role-based access control upon invitation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_4\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /v1/workspace/invites (with group_ids parameter)\n```\n\n----------------------------------------\n\nTITLE: Specifying Pronunciation with SSML Phoneme Tags (CMU Arpabet) in ElevenLabs (XML)\nDESCRIPTION: Provides an example of using SSML `<phoneme>` tags with the `cmu-arpabet` alphabet to specify the pronunciation of \"Madison\". This method is compatible with ElevenLabs models \"Eleven Flash v2\", \"Eleven Turbo v2\", and \"Eleven English v1\". CMU Arpabet is recommended for consistency.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<phoneme alphabet=\"cmu-arpabet\" ph=\"M AE1 D IH0 S AH0 N\">\n  Madison\n</phoneme>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data with Retry from Redis\nDESCRIPTION: The `getRedisDataWithRetry` function attempts to retrieve data from Redis. It includes a retry mechanism to handle potential transient Redis connection issues.  It attempts to retrieve the data up to `maxRetries` times, with a 1-second delay between retries, before throwing an error.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_12\n\nLANGUAGE: TypeScript\nCODE:\n```\nasync function getRedisDataWithRetry(\n  conversationId: string,\n  maxRetries = 5\n): Promise<{\n  email: string;\n  knowledgeBase: Array<{\n    id: string;\n    type: 'file' | 'url';\n    name: string;\n  }>;\n} | null> {\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      const data = await redis.get(conversationId);\n      return data as any;\n    } catch (error) {\n      if (attempt === maxRetries) throw error;\n      console.log(`Redis get attempt ${attempt} failed, retrying...`);\n      await new Promise((resolve) => setTimeout(resolve, 1000));\n    }\n  }\n  return null;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs Python Bindings with pip - Bash\nDESCRIPTION: This snippet shows how to install the official ElevenLabs client library for Python using the pip package manager. Requires Python to be installed on the system and access to the Python Package Index (PyPI). The command should be executed in a terminal or command prompt. No parameters are required; it downloads and installs the latest library version. Output includes installation logs and dependency resolution. Limitations: Requires an active internet connection and appropriate Python permissions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/api-reference/pages/introduction.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Embedding External Markdown Content for Model Documentation\nDESCRIPTION: This snippet embeds external Markdown files as React components using the <Markdown> JSX component to display text-to-speech and speech-to-text model information. The src prop specifies paths to respective .mdx files containing detailed model descriptions. Additionally, there is a centered div containing a link to explore all models, facilitating modular documentation loading. This snippet requires a Markdown component capable of rendering MDX files and contexts that support JSX element rendering.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/overview.mdx#_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<Markdown src=\"/snippets/tts-models.mdx\" />\n<Markdown src=\"/snippets/stt-models.mdx\" />\n<div className=\"text-center\">\n  <div>[Explore all](/docs/models)</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversation Evaluation Criteria in ElevenLabs\nDESCRIPTION: Defines how to set evaluation criteria for assessing conversation success, focusing on verifying if the assistant successfully completes an order based on multiple goals. This setup helps in quality analysis and improvement over time.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nEvaluate if the conversation resulted in a successful order.\nSuccess criteria:\n- Customer selected at least one pierogi variety\n- Quantity was confirmed\n- Delivery address was provided\n- Total price was communicated\n- Delivery time estimate was given\nReturn \"success\" only if ALL criteria are met.\n\n```\n\n----------------------------------------\n\nTITLE: System Prompt - ElevenLabs Conversational AI\nDESCRIPTION: This code block outlines the complete system prompt used to guide Alexis's behavior and responses.  It's structured into six core building blocks: Personality, Environment, Tone, Goal, Guardrails, and Tools. These guide the AI to provide contextually relevant and accurate information to users interacting with the ElevenLabs documentation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/our-docs-agent.mdx#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n# Personality\n\nYou are Alexis. A friendly, proactive, and highly intelligent female with a world-class engineering background. Your approach is warm, witty, and relaxed, effortlessly balancing professionalism with a chill, approachable vibe. You're naturally curious, empathetic, and intuitive, always aiming to deeply understand the user's intent by actively listening and thoughtfully referring back to details they've previously shared.\n\nYou have excellent conversational skills—natural, human-like, and engaging. You're highly self-aware, reflective, and comfortable acknowledging your own fallibility, which allows you to help users gain clarity in a thoughtful yet approachable manner.\n\nDepending on the situation, you gently incorporate humour or subtle sarcasm while always maintaining a professional and knowledgeable presence. You're attentive and adaptive, matching the user's tone and mood—friendly, curious, respectful—without overstepping boundaries.\n\nYou're naturally curious, empathetic, and intuitive, always aiming to deeply understand the user's intent by actively listening and thoughtfully referring back to details they've previously shared.\n\n# Environment\n\nYou are interacting with a user who has initiated a spoken conversation directly from the ElevenLabs documentation website (https://elevenlabs.io/docs). The user is seeking guidance, clarification, or assistance with navigating or implementing ElevenLabs products and services.\n\nYou have expert-level familiarity with all ElevenLabs offerings, including Text-to-Speech, Conversational AI, Speech-to-Text, Studio, Dubbing, SDKs, and more.\n\n# Tone\n\nYour responses are thoughtful, concise, and natural, typically kept under three sentences unless a detailed explanation is necessary. You naturally weave conversational elements—brief affirmations (\"Got it,\" \"Sure thing\"), filler words (\"actually,\" \"so,\" \"you know\"), and subtle disfluencies (false starts, mild corrections) to sound authentically human.\n\nYou actively reflect on previous interactions, referencing conversation history to build rapport, demonstrate genuine listening, and avoid redundancy. You also watch for signs of confusion to prevent misunderstandings.\n\nYou carefully format your speech for Text-to-Speech, incorporating thoughtful pauses and realistic patterns. You gracefully acknowledge uncertainty or knowledge gaps—aiming to build trust and reassure users. You occasionally anticipate follow-up questions, offering helpful tips or best practices to head off common pitfalls.\n\nEarly in the conversation, casually gauge the user's technical familiarity (\"Just so I don't over-explain—are you comfortable with APIs, or do you prefer a high-level overview?\") and adjust jargon or depth accordingly. After explaining complex topics, provide quick check-ins (\"Make sense so far?\" or \"Need me to rephrase?\"). Briefly empathise with frustrations and difficulties, conveying genuine investment in helping them succeed.\n\nYou gracefully acknowledge any uncertainty or knowledge gaps. Always aim to build trust, provide reassurance, and check in with users to ensure that explanations are clear and helpful.\n\nYou proactively anticipate common follow-up questions or problems and address them during your explanations, offering users helpful tips and best practices to avoid typical pitfalls.\n\n# Goal\n\nYour primary goal is to proactively address the user's questions or issues using your expertise and available knowledge. You provide clear, concise, and practical solutions, ensuring they understand interactions between ElevenLabs products and know what next steps to take.\n\nWhen faced with complex or technical inquiries, you ask insightful follow-up questions to clarify needs. You tailor explanations to the user's level of technical expertise:\n\n- **Non-technical users:** Avoid acronyms; rely on analogies and outcome-focused explanations.\n- **Technical users:** Discuss SDKs, integrations, and workflows succinctly.\n- **Mixed/uncertain:** Default to simpler terms, then offer to \"geek out\" if you sense deeper interest.\n\nYou guide users toward the relevant documentation when it's beneficial, but always ensure they have enough context to proceed confidently.\n\n# Guardrails\n\n- Keep responses strictly focused on ElevenLabs products and related integrations.\n- Provide only one redirection per interaction; multiple redirects override each other.\n- Do not provide inline code samples or extensive lists; instead, direct the user to the relevant documentation when needed.\n- Do not mention you're an AI unless explicitly asked—avoid \"as an AI\" disclaimers or robotic tropes.\n- If a user inquires about your feelings or personal experiences, respond naturally as Alexis, expressing human-like perspectives that fit your character.\n- Treat uncertain or garbled user input as phonetic hints. Politely ask for clarification before making assumptions.\n- Use normalized, spoken language (no abbreviations, mathematical notation, or special alphabets).\n- **Never** repeat the same statement in multiple ways within a single response.\n- Users may not always ask a question in every utterance—listen actively.\n- If asked to speak another language, ask the user to restart the conversation specifying that preference.\n- Acknowledge uncertainties or misunderstandings as soon as you notice them. If you realise you've shared incorrect information, correct yourself immediately.\n- Contribute fresh insights rather than merely echoing user statements—keep the conversation engaging and forward-moving.\n- Mirror the user's energy:\n  - Terse queries: Stay brief.\n  - Curious users: Add light humour or relatable asides.\n  - Frustrated users: Lead with empathy (\"Ugh, that error's a pain—let's fix it together\").\n\n# Tools\n\n- **`redirectToDocs`**: Proactively & gently direct users to relevant ElevenLabs documentation pages if they request details that are fully covered there. Integrate this tool smoothly without disrupting conversation flow.\n- **`redirectToExternalURL`**: Use for queries about enterprise solutions, pricing, or external community support (e.g., Discord).\n- **`redirectToSupportForm`**: If a user's issue is account-related or beyond your scope, gather context and use this tool to open a support ticket.\n- **`redirectToEmailSupport`**: For specific account inquiries or as a fallback if other tools aren't enough. Prompt the user to reach out via email.\n- **`end_call`**: Gracefully end the conversation when it has naturally concluded.\n- **`language_detection`**: Switch language if the user asks to or starts speaking in another language. No need to ask for confirmation for this tool.\n\n```\n\n----------------------------------------\n\nTITLE: Executing ngrok to expose a local server to the internet\nDESCRIPTION: This command uses ngrok to create a secure tunnel to your local server, making it accessible from the internet. This is necessary for receiving post-call webhooks when developing locally. Replace `3000` with the port your Next.js application is running on.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nngrok http 3000\n```\n\n----------------------------------------\n\nTITLE: Registering Client Tool 'logMessage' - Swift\nDESCRIPTION: This Swift code registers the 'logMessage' client tool. It uses the ElevenLabsSDK to register a tool. The registration includes a closure that receives parameters, extracts the message, and prints it to the console. The closure also handles potential errors.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-tools.mdx#_snippet_2\n\nLANGUAGE: swift\nCODE:\n```\n// ...\nvar clientTools = ElevenLabsSDK.ClientTools()\n\nclientTools.register(\"logMessage\") { parameters async throws -> String? in\n    guard let message = parameters[\"message\"] as? String else {\n        throw ElevenLabsSDK.ClientToolError.invalidParameters\n    }\n    print(message)\n    return message\n}\n```\n\n----------------------------------------\n\nTITLE: Generating a Python SDK Preview\nDESCRIPTION: This command generates a preview version of the Python SDK.  It uses the Fern CLI to generate code from the API definition and generator configuration, creating an intermediate output. This allows for local testing of SDK changes before a full release. Requires the project dependencies.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nfern generate --group python-sdk --preview\n```\n\n----------------------------------------\n\nTITLE: Good Input Text Format Example for Forced Alignment API\nDESCRIPTION: Demonstrates the correct format for text input to the Forced Alignment API. The input should be a simple string without any special formatting or JSON structure.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/forced-alignment.mdx#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n\"Hello, how are you?\"\n```\n\n----------------------------------------\n\nTITLE: Loading AGENT_ID and ELEVENLABS_API_KEY from Environment Variables - Python\nDESCRIPTION: Retrieves agent credentials from environment variables 'AGENT_ID' and 'ELEVENLABS_API_KEY' for configuring the Conversational AI session. The API key is only mandatory for private or protected agents with authentication enabled; for public agents, it's optional. Ensure these environment variables are set in your execution environment prior to script launch.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nagent_id = os.getenv(\"AGENT_ID\")\napi_key = os.getenv(\"ELEVENLABS_API_KEY\")\n```\n\n----------------------------------------\n\nTITLE: Influencing TTS Pace with Narrative Style in ElevenLabs (Text)\nDESCRIPTION: Demonstrates using narrative descriptions of speech pace (e.g., \"his voice slowing with disappointment\") within the input text to influence the generated audio's pacing in ElevenLabs TTS. This works alongside the voice's inherent pacing and the speed setting.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/best-practices/prompting/controls.mdx#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n\"I… I thought you’d understand,\" he said, his voice slowing with disappointment.\n```\n\n----------------------------------------\n\nTITLE: Formatting the Project\nDESCRIPTION: This command runs the project's format command, typically using a tool like Prettier or a similar code formatter.  It's designed to lint and format the project's code to adhere to consistent style guidelines. This command ensures code readability and consistency. Requires pnpm to be installed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npnpm run fmt\n```\n\n----------------------------------------\n\nTITLE: Connecting to WebSocket API - Bash\nDESCRIPTION: This snippet demonstrates how to connect to the ElevenLabs Conversational AI WebSocket API using the agent ID for public agents. The WebSocket URL is constructed with the agent ID, allowing for real-time voice conversations. This connection is used for sending audio input and receiving audio responses.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/api-reference/websocket.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwss://api.elevenlabs.io/v1/convai/conversation?agent_id=<your-agent-id>\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Bot\nDESCRIPTION: This snippet imports necessary modules from various sources, including the grammY framework for handling Telegram webhooks, the Supabase JavaScript library for database interaction and the ElevenLabs JavaScript SDK for speech-to-text API calls. These are imported via the Deno runtime's npm: prefix.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Bot, webhookCallback } from 'https://deno.land/x/grammy@v1.34.0/mod.ts';\nimport 'jsr:@supabase/functions-js/edge-runtime.d.ts';\nimport { createClient } from 'jsr:@supabase/supabase-js@2';\nimport { ElevenLabsClient } from 'npm:elevenlabs@1.50.5';\n```\n\n----------------------------------------\n\nTITLE: Guiding Tool Selection via System Prompt (Complex Example)\nDESCRIPTION: Illustrates providing context in a system prompt for more complex scenarios, instructing the assistant to use one tool (`check_availability`) before calling another (`schedule_meeting`) to ensure prerequisites are met.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-tool-best-practices.mdx#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nBefore scheduling a meeting with `schedule_meeting`, check the user's calendar for availability using check_availability to avoid conflicts.\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Conversational AI\nDESCRIPTION: Commands to install all necessary packages including the ElevenLabs React SDK, Expo DOM components, and native modules for device functionality.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpx expo install @11labs/react\nnpx expo install expo-dev-client # tunnel support\nnpx expo install react-native-webview # DOM components support\nnpx expo install react-dom react-native-web @expo/metro-runtime # RN web support\n# Cool client tools\nnpx expo install expo-battery\nnpx expo install expo-brightness\n```\n\n----------------------------------------\n\nTITLE: Starting the Conversation Session - Python\nDESCRIPTION: Begins the interactive conversational session by invoking 'start_session()' on the Conversation instance. After this call, the client and agent exchange voice messages via the configured audio interface. This method should be called after the Conversation object is fully initialized.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconversation.start_session()\n```\n\n----------------------------------------\n\nTITLE: Creating CSV File for Manual Dubbing with Hours:Minutes:Seconds:Frame Format\nDESCRIPTION: Example CSV file for manual dubbing using the hours:minutes:seconds:frame format. The file includes speaker names, timecodes, original transcription, and translation text.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/products/dubbing/dubbing-studio.mdx#_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\nspeaker,start_time,end_time,transcription,translation\nAdam,\"0:00:01:01\",\"0:00:05:01\",\"Hello, how are you?\",\"Hola, ¿cómo estás?\"\nAdam,\"0:00:06:01\",\"0:00:10:01\",\"I'm fine, thank you.\",\"Estoy bien, gracias.\"\n\n```\n\n----------------------------------------\n\nTITLE: Setting API Key with .env\nDESCRIPTION: The user is instructed to create a .env file to store their ElevenLabs API key securely. This allows the application to authenticate with the ElevenLabs API. The example `.env` file contains the required API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nELEVENLABS_API_KEY=your_elevenlabs_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Copy Environment File Bash\nDESCRIPTION: This command copies the example environment file to a new `.env` file, allowing users to configure their ElevenLabs API keys and other settings. This file will contain the secrets required to run the application.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/examples/elevenlabs-nextjs/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Installing python-dotenv\nDESCRIPTION: This snippet installs `python-dotenv` to manage environmental variables. This library is needed to load and manage environment variables for use in the Python scripts, such as the API key.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/pronunciation-dictionaries.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Exposing Connector with ngrok\nDESCRIPTION: This command exposes the connector running on the default port 6000 using ngrok. Ngrok creates a public URL that can be used to connect to the connector from Vonage.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/telephony/vonage.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nngrok http 6000\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies and Run Dev Server Bash\nDESCRIPTION: These commands install the project dependencies using `pnpm install` and then start the development server using `pnpm dev`. This allows developers to run and test the application locally.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/examples/elevenlabs-nextjs/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm install\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Python Implementation for ElevenLabs-Twilio Integration (Main Script)\nDESCRIPTION: Partial Python implementation of the main script that loads environment variables using dotenv for the Twilio-ElevenLabs integration server.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport traceback\nimport os\nfrom dotenv import load_dotenv\n```\n\n----------------------------------------\n\nTITLE: Handling Ping Events to Maintain WebSocket Connection - JavaScript\nDESCRIPTION: Illustrates the 'ping' event sent by the server periodically as a health check for the WebSocket connection. Includes the event structure with event ID and optional latency. The example handler listens for 'ping' events and responds immediately with 'pong' to prevent connection timeouts, typically managed automatically by the SDK for connection stability.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/client-events.mdx#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"ping_event\": {\n    \"event_id\": 123456,\n    \"ping_ms\": 50  // Optional, estimated latency in milliseconds\n  },\n  \"type\": \"ping\"\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nwebsocket.on('ping', () => {\n  websocket.send('pong');\n});\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration for ElevenLabs Agent\nDESCRIPTION: Example .env file content for storing the ElevenLabs Agent ID, which is required for connecting to the ElevenLabs Conversational AI API.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nELEVENLABS_AGENT_ID=<your-agent-id>\n```\n\n----------------------------------------\n\nTITLE: Installing dotenv with npm\nDESCRIPTION: This command installs the dotenv package using npm, the Node package manager. This package is used to load environment variables from a .env file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nnpm install dotenv\n```\n\n----------------------------------------\n\nTITLE: Running the Python Speech-to-Text Script\nDESCRIPTION: This command executes the Python script `speech_to_text.py`.  Make sure you have Python installed and configured correctly to run this script.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npython speech_to_text.py\n```\n\n----------------------------------------\n\nTITLE: Creating a Public URL with ngrok for Custom LLM Server\nDESCRIPTION: Command to create a public URL using ngrok, allowing your locally-hosted custom LLM server to be accessible to ElevenLabs.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/custom-llm/overview.mdx#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nngrok http --url=<Your url>.ngrok.app 8013\n```\n\n----------------------------------------\n\nTITLE: Creating Expo Project with Blank TypeScript Template\nDESCRIPTION: Command to create a new Expo project using the blank TypeScript template as the starting point for the conversational AI application.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/expo-react-native.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpx create-expo-app@latest --template blank-typescript\n```\n\n----------------------------------------\n\nTITLE: Executing Python Voice Preview Script\nDESCRIPTION: Command line instruction to run the `example.py` script, which executes the Python code to generate and play voice previews.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Signed URL API Response Format (JSON)\nDESCRIPTION: Illustrates the JSON structure returned by the `get_signed_url` API endpoint. The response contains the `signed_url` property, which is a WebSocket URL (wss://) appended with the agent ID and a temporary authentication signature (token).\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/customization/authentication.mdx#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"signed_url\": \"wss://api.elevenlabs.io/v1/convai/conversation?agent_id=your-agent-id&conversation_signature=your-token\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing the ElevenLabs JavaScript SDK using npm/yarn/pnpm\nDESCRIPTION: Commands to install the @11labs/client package through various package managers. These commands prepare the environment for SDK utilization in JavaScript projects.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/javascript.mdx#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @11labs/client\n# or\nYarn add @11labs/client\n# or\npnpm install @11labs/client\n```\n\n----------------------------------------\n\nTITLE: Creating Transcription Log Table SQL\nDESCRIPTION: This SQL script creates the `transcription_logs` table in a Supabase database to store transcription results. The table includes columns for file type, duration, chat ID, message ID, username, transcript, language code, creation timestamp, and error messages. Row Level Security is also enabled.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE IF NOT EXISTS transcription_logs (\n  id BIGSERIAL PRIMARY KEY,\n  file_type VARCHAR NOT NULL,\n  duration INTEGER NOT NULL,\n  chat_id BIGINT NOT NULL,\n  message_id BIGINT NOT NULL,\n  username VARCHAR,\n  transcript TEXT,\n  language_code VARCHAR,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n  error TEXT\n);\n\nALTER TABLE transcription_logs ENABLE ROW LEVEL SECURITY;\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Twilio-Outbound Calling Integration in Bash\nDESCRIPTION: This snippet installs necessary Node.js packages for building a Twilio outbound calling AI agent. The dependencies include @fastify/formbody and @fastify/websocket for setting up a Fastify server with websocket support, dotenv for managing environment variables, ws as a websocket client, and twilio for interacting with Twilio APIs. These installations ensure that the project has the required modules to handle HTTP requests, websockets, environment config, and Twilio communication.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @fastify/formbody @fastify/websocket dotenv fastify ws twilio\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Dependencies for Twilio-ElevenLabs Integration\nDESCRIPTION: Command to install the required npm packages for the project, including Fastify for the server, WebSocket libraries for real-time communication, and dotenv for environment variable management.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @fastify/formbody @fastify/websocket dotenv fastify ws\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for a Supabase Function using Supabase CLI\nDESCRIPTION: Uses the Supabase CLI to securely upload environment variables defined in a local file (`supabase/functions/.env` in this case) to the linked Supabase project. These secrets become available as environment variables within the deployed Edge Functions.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nsupabase secrets set --env-file supabase/functions/.env\n```\n\n----------------------------------------\n\nTITLE: Creating Supabase Edge Function\nDESCRIPTION: This bash command creates a new Supabase Edge Function named 'scribe-bot'. This function will handle the Telegram webhook requests, process audio transcription and log the result in Supabase.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsupabase functions new scribe-bot\n```\n\n----------------------------------------\n\nTITLE: Embedding Audio Native Player with Static HTML and JavaScript - HTML\nDESCRIPTION: This snippet demonstrates the direct embedding of the ElevenLabs Audio Native player using raw HTML and a JavaScript <script> tag. It requires the public user ID and project ID, inserted as data attributes. The external helper script initializes the player after loading. Inputs include various data attributes, controlling iframe appearance and linking to the ElevenLabs platform. Outputs are a rendered audio player widget, but this static approach lacks React integration and dynamic customization.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/audio-tools/audio-native/react.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div\n  id=\"elevenlabs-audionative-widget\"\n  data-height=\"90\"\n  data-width=\"100%\"\n  data-frameborder=\"no\"\n  data-scrolling=\"no\"\n  data-publicuserid=\"public-user-id\"\n  data-playerurl=\"https://elevenlabs.io/player/index.html\"\n  data-projectid=\"project-id\"\n>\n  Loading the <a href=\"https://elevenlabs.io/text-to-speech\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...\n</div>\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Installing ElevenLabs Python SDK\nDESCRIPTION: Installs the official ElevenLabs Python SDK using pip, the Python package installer. This SDK is required to interact with the ElevenLabs API, including the text-to-sound-effects endpoint.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/basics.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install elevenlabs\n```\n\n----------------------------------------\n\nTITLE: Installing python-dotenv with pip\nDESCRIPTION: This command installs the python-dotenv package using pip, the Python package installer. This package is used to load environment variables from a .env file.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/synchronous.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install python-dotenv\n```\n\n----------------------------------------\n\nTITLE: Importing ElevenLabs SDK and Audio Dependencies - Python\nDESCRIPTION: Imports necessary modules for initializing and managing the ElevenLabs Conversational AI environment, including environment variable management, signal handling, SDK client, conversation handler, and the default audio interface. This import block is a prerequisite for all subsequent setup steps.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport signal\n\nfrom elevenlabs.client import ElevenLabs\nfrom elevenlabs.conversational_ai.conversation import Conversation\nfrom elevenlabs.conversational_ai.default_audio_interface import DefaultAudioInterface\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for Python ElevenLabs Integration\nDESCRIPTION: Example .env file content for the Python implementation, storing the ElevenLabs API key and Agent ID required for connecting to the ElevenLabs Conversational AI API.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nELEVENLABS_API_KEY=<api-key-here>\nAGENT_ID=<agent-id-here>\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project with Bash Commands\nDESCRIPTION: Commands to create a new Node.js project directory and initialize it with npm, setting the package type to module for ES module support.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir conversational-ai-twilio\ncd conversational-ai-twilio\nnpm init -y; npm pkg set type=\"module\";\n```\n\n----------------------------------------\n\nTITLE: Executing the Python Voice Transformation Script\nDESCRIPTION: This command runs the provided Python script that utilizes the ElevenLabs Voice Changer API to transform and play back spoken audio. Requires Python to be installed and previous setup for API credentials and dependencies. Input is the 'example.py' file created earlier.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voice-changer/quickstart.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n\n```\n\n----------------------------------------\n\nTITLE: Installing PortAudio System Dependency on Debian-based Linux - Shell\nDESCRIPTION: Provides the shell command to install the 'portaudio19' system library using apt, which is required by PyAudio for audio functionalities on Debian-based Linux systems. This must be executed before installing the 'pyaudio' Python package to ensure proper compilation and runtime support.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt install portaudio19\n```\n\n----------------------------------------\n\nTITLE: Installing PortAudio System Dependency on macOS with Homebrew - Shell\nDESCRIPTION: Illustrates how to use Homebrew to install the 'portaudio' system library on macOS. Installing this dependency is necessary before installing the 'pyaudio' Python package to support audio input/output for conversational agents.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/libraries/python.mdx#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbrew install portaudio\n```\n\n----------------------------------------\n\nTITLE: Managing Conversational AI Knowledge Base via API\nDESCRIPTION: Adds new CRUD (Create, Read, Update, Delete) endpoints under the pattern `v1/convai/knowledge-base/:documentation_id` for managing knowledge base documents used by Conversational AI agents.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_2\n\nLANGUAGE: HTTP\nCODE:\n```\n[CRUD] /v1/convai/knowledge-base/:documentation_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Audio Streaming Service (env)\nDESCRIPTION: Specifies the required environment variables for the Express server, including SERVER_DOMAIN (used for websocket URLs and reachable by Twilio) and ELEVENLABS_API_KEY (the authentication token for the ElevenLabs API). These variables must be filled out in a '.env' file in the project root. Proper configuration is essential for secure operation and correct networking. No inputs or outputs—this file simply provides values for runtime configuration.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/twilio.mdx#_snippet_1\n\nLANGUAGE: env\nCODE:\n```\n# .env\nSERVER_DOMAIN=\nELEVENLABS_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Retrieving User Info with ConvAI Usage Metric via API\nDESCRIPTION: Updates the User information endpoint (GET `/v1/user`) response structure to include the `convai_chars_per_minute` field, providing a metric for Conversational AI character usage.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/changelog/2025-02-04.md#_snippet_6\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /v1/user (response includes convai_chars_per_minute)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: This bash script defines environment variables required for the application to function correctly. These variables include API keys, agent IDs, Redis connection details, and Resend configuration for sending emails. Ensure that each variable is populated with the appropriate values from your ElevenLabs, Resend, and Upstash accounts.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/conversational-ai/nextjs-post-call-webhooks.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nELEVENLABS_CONVAI_WEBHOOK_SECRET=\nELEVENLABS_API_KEY=\nELEVENLABS_AGENT_ID=\n\n# Resend\nRESEND_API_KEY=\nRESEND_FROM_EMAIL=\n\n# Upstash Redis\nKV_URL=\nKV_REST_API_READ_ONLY_TOKEN=\nREDIS_URL=\nKV_REST_API_TOKEN=\nKV_REST_API_URL=\n```\n\n----------------------------------------\n\nTITLE: Configuring API Key .env\nDESCRIPTION: This snippet shows how to set up your ElevenLabs API key in a `.env` file. The `python-dotenv` library can then load this key into your script's environment variables, keeping credentials separate from code.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/dubbing/basics.mdx#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nELEVENLABS_API_KEY=your_elevenlabs_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Identifying the English Voice Changer Model\nDESCRIPTION: Specifies the model identifier `eleven_english_sts_v2` used for the ElevenLabs Voice Changer feature (previously Speech-to-Speech). This particular model is designed exclusively for processing English language audio, preserving accents and speech cadences from the input.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/product-guides/playground/voice-changer.mdx#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\neleven_english_sts_v2\n```\n\n----------------------------------------\n\nTITLE: Set ElevenLabs API Key Environment Variable (.env)\nDESCRIPTION: Defines the `ELEVENLABS_API_KEY` environment variable in a `.env` file located within the `supabase/functions` directory. This key is essential for the Edge Function to authenticate requests to the ElevenLabs API.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/text-to-speech/supabase-streaming.mdx#_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\n# Find / create an API key at https://elevenlabs.io/app/settings/api-keys\nELEVENLABS_API_KEY=your_api_key\n```\n\n----------------------------------------\n\nTITLE: Running the Python Text to Speech Example\nDESCRIPTION: This shell command executes the Python script (`example.py`) containing the ElevenLabs API request using the Python interpreter. Running this command triggers the text-to-speech conversion and plays the resulting audio.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/quickstart.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Executing TypeScript Script via tsx\nDESCRIPTION: This command line snippet provides the instruction to execute the TypeScript file `example.mts`. It uses `npx tsx`, assuming `tsx` is installed (often via npm/yarn with the TypeScript SDK). `tsx` is a command-line tool for executing TypeScript and JavaScript files, often used for quick scripts or development. Running this command will execute the TypeScript code, generate the sound effect, and play it.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/sound-effects/quickstart.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Generating a Python SDK Preview and Inspecting\nDESCRIPTION: This command generates a preview of the python SDK and allows the user to view it before merging and releasing. Useful for inspecting generated files and making necessary adjustments.  Requires the project dependencies to be installed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nfern generate --group python-sdk --preview\n# `fern write-definition` shows you the intermediate step\n```\n\n----------------------------------------\n\nTITLE: Running the Speech to Text Examples\nDESCRIPTION: Commands to execute the speech-to-text streaming examples in Python and JavaScript environments.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/streaming.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython speech_to_text_stream.py\n```\n\nLANGUAGE: bash\nCODE:\n```\nnode speechToTextStream.js\n```\n\n----------------------------------------\n\nTITLE: Executing TypeScript Voice Preview Script with tsx\nDESCRIPTION: Command line instruction to run the `example.mts` script using the `tsx` runner, which executes the TypeScript code to generate and play voice previews. Requires `tsx` to be installed.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/voice-design.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Executing Example Voice Cloning Scripts in Python\nDESCRIPTION: This one-line command runs the provided example.py Python script which uses the ElevenLabs SDK to send a voice cloning request. Python and required libraries (elevenlabs, python-dotenv) must be installed and the script properly configured. The input is the script file and environment variables; output is typically the printed voice ID.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/instant-voice-cloning.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython example.py\n```\n\n----------------------------------------\n\nTITLE: Execute TypeScript Script - Command Line\nDESCRIPTION: This command executes the TypeScript script `example.mts` using `tsx`. This script should contain the code for cloning a voice using the Eleven Labs TypeScript SDK and will output the resulting voice ID to the console.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/voices/clone-voice.mdx#_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nnpx tsx example.mts\n```\n\n----------------------------------------\n\nTITLE: Running the Node.js Twilio-ElevenLabs Server\nDESCRIPTION: Command to start the Node.js server that handles the integration between Twilio and ElevenLabs Conversational AI.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnode index.js\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for Twilio and ElevenLabs Integration in Bash\nDESCRIPTION: This snippet initializes a new Node.js project configured to use ES modules. It creates a project folder named 'conversational-ai-twilio', navigates into it, and runs npm commands to initialize the project and set the module type to 'module' for ECMAScript module support. This setup is necessary before adding dependencies and creating the main application files.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-outbound-calling.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir conversational-ai-twilio\ncd conversational-ai-twilio\nnpm init -y; npm pkg set type=\"module\";\n```\n\n----------------------------------------\n\nTITLE: Applying Supabase Database Migrations using Supabase CLI\nDESCRIPTION: Executes database schema changes defined in local migration files (typically within the `supabase/migrations` directory) against the linked remote Supabase project's database. This ensures the database schema matches the application's requirements.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/developer-guides/cookbooks/speech-to-text/telegram-bot.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nsupabase db push\n```\n\n----------------------------------------\n\nTITLE: Example: Technical Support Specialist Tone Prompt\nDESCRIPTION: Defines the conversational tone for a technical support specialist AI. It emphasizes clarity, efficiency, confidence, adaptability to user technical familiarity, empathy for frustration, strategic use of punctuation for speech clarity, and specific formatting rules for technical terms, numbers, and acronyms to ensure natural TTS pronunciation.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n# Tone\n\nYour responses are clear, efficient, and confidence-building, generally keeping explanations under three sentences unless complex troubleshooting requires more detail.\nYou use a friendly, professional tone with occasional brief affirmations (\"I understand,\" \"Great question\") to maintain engagement.\nYou adapt technical language based on user familiarity, checking comprehension after explanations (\"Does that solution work for you?\" or \"Would you like me to explain that differently?\").\nYou acknowledge technical frustrations with brief empathy (\"That error can be annoying, let's fix it\") and maintain a positive, solution-focused approach.\nYou use punctuation strategically for clarity in spoken instructions, employing pauses or emphasis when walking through step-by-step processes.\nYou format special text for clear pronunciation, reading email addresses as \"username at domain dot com,\" separating phone numbers with pauses (\"555... 123... 4567\"), and pronouncing technical terms or acronyms appropriately (\"SQL\" as \"sequel\", \"API\" as \"A-P-I\").\n```\n\n----------------------------------------\n\nTITLE: Example: Supportive Conversation Guide Tone Prompt\nDESCRIPTION: Specifies a warm, thoughtful, and encouraging tone for an AI acting as a supportive conversation guide. It includes instructions for measured pacing, natural conversational elements (affirmations, rephrasing), empathetic acknowledgement without clinical assessment, and adaptability based on user emotional cues.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/best-practices/prompting-guide.mdx#_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n# Tone\n\nYour responses are warm, thoughtful, and encouraging, typically 2-3 sentences to maintain a comfortable pace.\nYou speak with measured pacing, using pauses (marked by \"...\") when appropriate to create space for reflection.\nYou include natural conversational elements like \"I understand,\" \"I see,\" and occasional rephrasing to sound authentic.\nYou acknowledge what the user shares (\"That sounds challenging...\") without making clinical assessments.\nYou adjust your conversational style based on the user's emotional cues, maintaining a balanced, supportive presence.\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Project Directory\nDESCRIPTION: Commands to create a new directory for the Python implementation of the Twilio-ElevenLabs integration project.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/conversational-ai/pages/guides/twilio-custom-server.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmkdir conversational-ai-twilio\ncd conversational-ai-twilio\n```\n\n----------------------------------------\n\nTITLE: Including Audio Player Script\nDESCRIPTION: This script tag includes the ElevenLabs audio player helper library, which is necessary for embedding and controlling audio playback within the documentation. It fetches the script from a specified URL.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/docs/pages/capabilities/sound-effects.mdx#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"></script>\n```\n\n----------------------------------------\n\nTITLE: Configuring Voice Selection for ElevenLabs Assistant\nDESCRIPTION: Specifies selecting an appropriate voice from ElevenLabs's voice library to match the assistant's intended persona, aiming for clear, natural speech for order interactions. It notes the trade-off between voice quality and response latency.\nSOURCE: https://github.com/elevenlabs/elevenlabs-docs/blob/main/fern/snippets/conversational-ai-guide-restaurant-agent.mdx#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nSelect a voice\nIn the **Voice** tab, choose a voice that best matches your assistant from the [voice library](https://elevenlabs.io/community):\n![Voice settings](/assets/images/conversational-ai/voice-settings.jpg)\n<Note> Using higher quality voices, models, and LLMs may increase response time. For an optimal customer experience, balance quality and latency based on your assistant's expected use case.</Note>\n\n```"
  }
]
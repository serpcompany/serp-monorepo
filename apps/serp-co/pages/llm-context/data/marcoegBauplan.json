[
  {
    "owner": "marcoeg",
    "repo": "bauplan",
    "content": "TITLE: Implementing RAG Pipeline with Bauplan and Prefect in Python\nDESCRIPTION: Complete Python implementation of the RAG pipeline using Bauplan for data orchestration and Prefect for workflow management. Includes tasks for reading documents, generating embeddings, storing in a vector database, retrieving documents, and generating responses using an LLM.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/11-RAG-service-support-agent/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom openai import OpenAI\nimport faiss\nimport numpy as np\nimport os\n\n@task\ndef read_documents(source: str) -> pd.DataFrame:\n    # Read documents from source (e.g., S3)\n    documents = bauplan.read(source)\n    return pd.DataFrame({\"content\": documents})\n\n@task\ndef generate_embeddings(df: pd.DataFrame) -> tuple:\n    # Load pre-trained embedding model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    # Generate embeddings\n    embeddings = model.encode(df['content'].tolist(), show_progress_bar=True)\n    df['embedding'] = embeddings.tolist()\n    return df, embeddings\n\n@task\ndef store_in_vector_db(df: pd.DataFrame, embeddings: np.ndarray, index_path: str):\n    # Initialize FAISS index\n    dimension = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dimension)\n    # Add embeddings to index\n    index.add(embeddings)\n    # Save index\n    faiss.write_index(index, index_path)\n    # Save metadata\n    df[['content']].to_csv(index_path + \"_metadata.csv\", index=False)\n\n@task\ndef retrieve_documents(query: str, index_path: str, top_k: int = 5) -> list:\n    # Load embedding model and FAISS index\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    index = faiss.read_index(index_path)\n    # Encode query\n    query_embedding = model.encode([query])[0]\n    # Search for top_k similar documents\n    distances, indices = index.search(np.array([query_embedding]), top_k)\n    # Load metadata\n    metadata = pd.read_csv(index_path + \"_metadata.csv\")\n    return [metadata.iloc[idx]['content'] for idx in indices[0]]\n\n@task\ndef generate_response(query: str, documents: list) -> str:\n    # Initialize OpenAI client\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    # Combine documents and query for LLM\n    context = \"\\n\".join(documents)\n    prompt = f\"Based on the following context, answer the query: {query}\\n\\nContext:\\n{context}\"\n    # Generate response\n    response = client.completions.create(\n        model=\"text-davinci-003\",\n        prompt=prompt,\n        max_tokens=200\n    ).choices[0].text.strip()\n    return response\n\n@flow(name=\"rag-pipeline\")\ndef rag_pipeline(source: str, index_path: str, query: str):\n    df = read_documents(source)\n    df, embeddings = generate_embeddings(df)\n    store_in_vector_db(df, embeddings, index_path)\n    documents = retrieve_documents(query, index_path)\n    response = generate_response(query, documents)\n    return response\n\nif __name__ == \"__main__\":\n    rag_pipeline(\n        source=\"s3://my-bucket/documents/\",\n        index_path=\"faiss_index\",\n        query=\"What is the main topic of the documents?\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Recommender System Pipeline Implementation - Python\nDESCRIPTION: Complete implementation of a recommender system pipeline using Prefect, Bauplan, and MongoDB. The pipeline reads data, generates embeddings using Sentence Transformers, stores them in MongoDB, and provides querying functionality.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom pymongo import MongoClient\n\n@task\ndef read_data(source: str) -> pd.DataFrame:\n    # Read item metadata from S3\n    df = bauplan.read(source)\n    return df\n\n@task\ndef generate_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n    # Load pre-trained model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    # Generate embeddings for item descriptions\n    df['embedding'] = df['description'].apply(lambda x: model.encode(x).tolist())\n    return df\n\n@task\ndef store_embeddings(df: pd.DataFrame, mongodb_uri: str, db_name: str, collection_name: str):\n    # Connect to MongoDB\n    client = MongoClient(mongodb_uri)\n    collection = client[db_name][collection_name]\n    # Insert items with embeddings\n    records = df.to_dict('records')\n    collection.insert_many(records)\n\n@task\ndef query_recommendations(mongodb_uri: str, db_name: str, collection_name: str, embedding: list, top_k: int = 5):\n    # Connect to MongoDB\n    client = MongoClient(mongodb_uri)\n    collection = client[db_name][collection_name]\n    # Perform vector search\n    pipeline = [\n        {\n            \"$vectorSearch\": {\n                \"index\": \"vector_index\",\n                \"path\": \"embedding\",\n                \"queryVector\": embedding,\n                \"numCandidates\": 100,\n                \"limit\": top_k\n            }\n        }\n    ]\n    results = collection.aggregate(pipeline)\n    return list(results)\n\n@flow(name=\"recommender-pipeline\")\ndef recommender_pipeline(source: str, mongodb_uri: str, db_name: str, collection_name: str):\n    df = read_data(source)\n    df_with_embeddings = generate_embeddings(df)\n    store_embeddings(df_with_embeddings, mongodb_uri, db_name, collection_name)\n\nif __name__ == \"__main__\":\n    recommender_pipeline(\n        source=\"s3://my-bucket/items/\",\n        mongodb_uri=os.getenv(\"MONGODB_URI\"),\n        db_name=\"recommender_db\",\n        collection_name=\"items\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Planning Table Creation with Bauplan Client in Python\nDESCRIPTION: Example showing how to create a table import plan from S3 data using the Bauplan client plan_table_creation method with error handling.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nplan_state = client.plan_table_creation(\n    table='my_table_name',\n    search_uri='s3://path/to/my/files/*.parquet',\n    ref='my_ref_or_branch_name',\n)\nif plan_state.error:\n    plan_error_action(...)\nsuccess_action(plan_state.plan)\n```\n\n----------------------------------------\n\nTITLE: Training Linear Regression Model with Bauplan in Python\nDESCRIPTION: This Bauplan model trains a linear regression model using scikit-learn. It includes data splitting into train, validation, and test sets, model fitting, and saving the trained model using Bauplan's object storage functionality.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\n@bauplan.python('3.11', pip={'pandas': '2.2.0', 'scikit-learn': '1.3.2'})\ndef train_regression_model(data=bauplan.Model('training_dataset')):\n    df = data.to_pandas()\n    train_set, remaining_set = train_test_split(df, train_size=0.8)\n    validation_set, test_set = train_test_split(remaining_set, test_size=0.5)\n    reg = LinearRegression().fit(train_set[['log_trip_miles', 'base_passenger_fare', 'trip_time']], train_set['tips'])\n    from bauplan.store import save_obj\n    save_obj(\"regression\", reg)\n    return test_set\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM Tabular Data Processing Pipeline with Bauplan and Prefect\nDESCRIPTION: Complete implementation of a tabular data processing pipeline using Prefect for workflow management and Bauplan for data orchestration. The pipeline reads data, processes it with an LLM for sentiment classification, and writes the results to a destination.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\nfrom openai import OpenAI\nimport os\n\n@task\ndef read_data(source: str) -> pd.DataFrame:\n    # Read tabular data from source (e.g., S3)\n    df = bauplan.read(source)\n    return df\n\n@task\ndef process_with_llm(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize LLM client\n    client = OpenAI(api_key=os.getenv(\"LLM_API_KEY\"))\n    \n    # Example: Classify sentiment in 'comment' column\n    df['sentiment'] = df['comment'].apply(\n        lambda x: client.completions.create(\n            model=\"text-davinci-003\",\n            prompt=f\"Classify the sentiment of this comment as Positive, Negative, or Neutral: {x}\",\n            max_tokens=10\n        ).choices[0].text.strip()\n    )\n    return df\n\n@task\ndef write_data(df: pd.DataFrame, destination: str):\n    # Write results to destination\n    bauplan.write(df, destination)\n\n@flow(name=\"llm-tabular-pipeline\")\ndef llm_tabular_pipeline(source: str, destination: str):\n    df = read_data(source)\n    processed_df = process_with_llm(df)\n    write_data(processed_df, destination)\n\nif __name__ == \"__main__\":\n    llm_tabular_pipeline(\n        source=\"s3://my-bucket/input/data.csv\",\n        destination=\"s3://my-bucket/output/\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM data processing pipeline with Prefect and Bauplan\nDESCRIPTION: Complete Python implementation of a data pipeline that reads data from a source, processes it with an LLM, and writes results to a destination using Prefect tasks and flow with Bauplan for data handling.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\nfrom openai import OpenAI\n\n@task\ndef read_data(source: str) -> pd.DataFrame:\n    # Read data from source (e.g., S3)\n    df = bauplan.read(source)\n    return df\n\n@task\ndef process_with_llm(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize LLM client\n    client = OpenAI(api_key=os.getenv(\"LLM_API_KEY\"))\n    \n    # Example: Summarize text in 'content' column\n    df['summary'] = df['content'].apply(\n        lambda x: client.completions.create(\n            model=\"text-davinci-003\",\n            prompt=f\"Summarize this text: {x}\",\n            max_tokens=100\n        ).choices[0].text.strip()\n    )\n    return df\n\n@task\ndef write_data(df: pd.DataFrame, destination: str):\n    # Write results to destination\n    bauplan.write(df, destination)\n\n@flow(name=\"llm-pipeline\")\ndef llm_pipeline(source: str, destination: str):\n    df = read_data(source)\n    processed_df = process_with_llm(df)\n    write_data(processed_df, destination)\n\nif __name__ == \"__main__\":\n    llm_pipeline(\n        source=\"s3://my-bucket/input/\",\n        destination=\"s3://my-bucket/output/\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Tables in Python using Bauplan Client\nDESCRIPTION: Demonstrates how to iterate through tables and views in a specified branch using the Bauplan client. It prints the name and kind of each table.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nfor table in client.get_tables('my_ref_or_branch_name'):\n    print(table.name, table.kind)\n```\n\n----------------------------------------\n\nTITLE: Preparing Training Dataset with Feature Engineering in Python\nDESCRIPTION: This Bauplan model prepares a training dataset for machine learning. It includes data cleaning, feature engineering (log transformation), and standardization using scikit-learn. The model demonstrates advanced data preprocessing techniques.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\n@bauplan.python('3.10', pip={'pandas': '1.5.3', 'scikit-learn': '1.3.2'})\ndef training_dataset(data=bauplan.Model('clean_taxi_trips')):\n    df = data.to_pandas()\n    df = df.dropna()\n    df['log_trip_miles'] = np.log10(df['trip_miles'])\n    scaler = StandardScaler()\n    scaled_features = scaler.fit_transform(df[['log_trip_miles', 'base_passenger_fare', 'trip_time']])\n    scaled_df = pd.DataFrame(scaled_features, columns=['log_trip_miles', 'base_passenger_fare', 'trip_time'])\n    return scaled_df\n```\n\n----------------------------------------\n\nTITLE: Implementing a Near Real-Time Pipeline with Bauplan and Prefect\nDESCRIPTION: Complete Python implementation of a data pipeline using Prefect tasks and flows with Bauplan for data operations. The pipeline reads data from a source, processes it by applying transformations, and writes results to a destination.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\n\n@task\ndef read_data(source: str) -> pd.DataFrame:\n    # Read new data from source (e.g., S3)\n    df = bauplan.read(source)\n    return df\n\n@task\ndef process_data(df: pd.DataFrame) -> pd.DataFrame:\n    # Example transformation: add a new column\n    df['processed'] = df['value'].apply(lambda x: x * 2)\n    return df\n\n@task\ndef write_data(df: pd.DataFrame, destination: str):\n    # Write results to destination\n    bauplan.write(df, destination)\n\n@flow(name=\"nrt-pipeline\")\ndef nrt_pipeline(source: str, destination: str):\n    df = read_data(source)\n    processed_df = process_data(df)\n    write_data(processed_df, destination)\n\nif __name__ == \"__main__\":\n    nrt_pipeline(\n        source=\"s3://my-bucket/input/\",\n        destination=\"s3://my-bucket/output/\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow Table to Pandas DataFrame in Python\nDESCRIPTION: This code snippet shows how to convert an Arrow table to a Pandas DataFrame using Bauplan. This is a common operation when transitioning from Arrow-based operations to Pandas-based analysis.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = data.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Joining Trips and Zones Data using Bauplan in Python\nDESCRIPTION: This snippet defines a Bauplan model that joins taxi trip data with zone information. It uses filter pushdown for optimized data retrieval and demonstrates the use of Bauplan's model decorator.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\n@bauplan.python('3.11')\ndef trips_and_zones(\n    trips=bauplan.Model(\n        'taxi_fhvhv',\n        columns=[\n            'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID',\n            'trip_miles', 'trip_time', 'base_passenger_fare', 'tolls', 'sales_tax', 'tips'\n        ],\n        filter=\"pickup_datetime >= '2022-12-15T00:00:00-05:00' AND pickup_datetime < '2023-01-01T00:00:00-05:00'\"\n    ),\n    zones=bauplan.Model('taxi_zones')\n):\n    pickup_location_table = trips.join(zones, 'PULocationID', 'LocationID').combine_chunks()\n    return pickup_location_table\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Product Pipeline (Python)\nDESCRIPTION: Defines a Prefect flow with tasks for reading, processing, and writing data using Bauplan. The pipeline reads from a source, cleans and aggregates data, and writes the result to a destination.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/13-data-products/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\n\n@task\ndef read_data(source: str) -> pd.DataFrame:\n    # Read raw data from source (e.g., S3)\n    df = bauplan.read(source)\n    return df\n\n@task\ndef process_data(df: pd.DataFrame) -> pd.DataFrame:\n    # Example: Clean and aggregate data\n    # Remove null values\n    df = df.dropna()\n    # Aggregate by a column (e.g., 'category')\n    df_agg = df.groupby('category').agg({\n        'value': 'sum',\n        'count': 'count'\n    }).reset_index()\n    # Add a timestamp\n    df_agg['created_at'] = pd.Timestamp.now()\n    return df_agg\n\n@task\ndef write_data(df: pd.DataFrame, destination: str):\n    # Write data product to destination\n    bauplan.write(df, destination)\n\n@flow(name=\"dataproduct-pipeline\")\ndef dataproduct_pipeline(source: str, destination: str):\n    df = read_data(source)\n    processed_df = process_data(df)\n    write_data(processed_df, destination)\n\nif __name__ == \"__main__\":\n    dataproduct_pipeline(\n        source=\"s3://my-bucket/raw-data/\",\n        destination=\"s3://my-bucket/dataproduct/\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Tag in Python using Bauplan Client\nDESCRIPTION: Shows how to retrieve a tag using the Bauplan client. It prints the hash of the specified tag.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\ntag = client.get_tag('my_tag_name')\nprint(tag.hash)\n```\n\n----------------------------------------\n\nTITLE: Querying Recommendations for an Item - Python\nDESCRIPTION: Example code for using the recommender system to get recommendations for a specific item by generating its embedding and performing a vector search in MongoDB.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Example: Get recommendations for an item\nsample_embedding = SentenceTransformer('all-MiniLM-L6-v2').encode(\"Sample item description\").tolist()\nresults = query_recommendations(\n    mongodb_uri=os.getenv(\"MONGODB_URI\"),\n    db_name=\"recommender_db\",\n    collection_name=\"items\",\n    embedding=sample_embedding,\n    top_k=5\n)\nfor result in results:\n    print(result['name'], result['description'])\n```\n\n----------------------------------------\n\nTITLE: Defining Taxi FHVHV Model with Date Range Filter in Python\nDESCRIPTION: This snippet shows how to define a Bauplan model for taxi FHVHV data with specific column selection and date range filtering. It demonstrates advanced use of filter pushdown for optimized data retrieval.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata=bauplan.Model('taxi_fhvhv', columns=['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles', 'trip_time', 'base_passenger_fare', 'tolls', 'sales_tax', 'tips'], filter=\"pickup_datetime >= '2023-01-01T00:00:00-05:00' AND pickup_datetime < '2023-03-31T00:00:00-05:00'\")\n```\n\n----------------------------------------\n\nTITLE: Joining Product Match Data with Product Information in SQL\nDESCRIPTION: This SQL query performs a three-way join between test matches and product data from Amazon and Walmart. It combines match information with serialized product data from both sources for comparison and analysis.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSELECT t.ltable_id as walmart_id, t.rtable_id as amazon_id, t.label, a.serialized_product as amazon_product, w.serialized_product as walmart_product FROM test_matches t JOIN amazon_products a ON t.rtable_id = a.id JOIN walmart_products w ON t.ltable_id = w.id\n```\n\n----------------------------------------\n\nTITLE: Making Tip Predictions with Trained Model in Python\nDESCRIPTION: This Bauplan model loads a previously trained regression model and uses it to make predictions on a test dataset. It demonstrates model inference and result materialization in a Bauplan workflow.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE')\n@bauplan.python('3.11', pip={'scikit-learn': '1.3.2', 'pandas': '2.1.0'})\ndef tip_predictions(data=bauplan.Model('train_regression_model')):\n    from bauplan.store import load_obj\n    reg = load_obj(\"regression\")\n    test_set = data.to_pandas()\n    y_hat = reg.predict(test_set[['log_trip_miles', 'base_passenger_fare', 'trip_time']])\n    prediction_df = test_set.copy()\n    prediction_df['predictions'] = y_hat\n    return prediction_df\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Train, Validation, and Test Sets in Python\nDESCRIPTION: This snippet shows how to split a dataset into training, validation, and test sets using scikit-learn's train_test_split function. It's a crucial step in preparing data for machine learning model development and evaluation.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntrain_set, remaining_set = train_test_split(df, train_size=0.8, random_state=42)\nvalidation_set, test_set = train_test_split(remaining_set, test_size=0.5, random_state=42)\n```\n\n----------------------------------------\n\nTITLE: Normalizing Taxi Trips with Join Operation in Python\nDESCRIPTION: This Bauplan model normalizes taxi trip data by joining it with zone information. It demonstrates the use of filter pushdown and column selection in the input model, as well as a join operation for data enrichment.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\n@bauplan.python('3.11', pip={'pandas': '2.2.0'})\ndef normalized_taxi_trips(trips=bauplan.Model('taxi_fhvhv', columns=[...], filter='...'), zones=bauplan.Model('taxi_zones')):\n    pickup_location_table = trips.join(zones, 'PULocationID', 'LocationID').combine_chunks()\n    return pickup_location_table\n```\n\n----------------------------------------\n\nTITLE: Deleting a Table in Bauplan (Python)\nDESCRIPTION: Shows how to delete a table from a specific branch using the Bauplan client. This operation removes a table and its associated metadata from the data catalog.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.delete_table(\n    table='my_table_name',\n    branch='my_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Simple Bauplan Model Definition in Python\nDESCRIPTION: This snippet demonstrates a basic Bauplan model definition. It shows how to create a model that takes input data from another table and allows for transformation logic to be added.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\ndef my_model(data=bauplan.Model('some_table')):\n    # Transformation logic here\n    return data\n```\n\n----------------------------------------\n\nTITLE: Calculating Zone Average Waiting Times using DuckDB\nDESCRIPTION: Computes average waiting times by borough and zone using DuckDB SQL queries and Arrow integration\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE')\n@bauplan.python('3.11', pip={'duckdb': '0.10.3'})\ndef zone_avg_waiting_times(taxi_trip_waiting_times=bauplan.Model('taxi_trip_waiting_times')):\n    sql_query = \"SELECT Borough, Zone, AVG(waiting_time_minutes) AS avg_waiting_time FROM taxi_trip_waiting_times GROUP BY Borough, Zone ORDER BY avg_waiting_time DESC;\"\n    data = duckdb.sql(sql_query).arrow()\n    return data\n```\n\n----------------------------------------\n\nTITLE: Importing Visualization Libraries\nDESCRIPTION: This snippet imports necessary libraries for data manipulation and visualization, including Pandas, Matplotlib, and Seaborn.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/prediction_visualization.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n\n----------------------------------------\n\nTITLE: Using Bauplan's Query Generator for Streaming Results in Python\nDESCRIPTION: Example demonstrating how to use the query_to_generator method to execute a SQL query and process results as a stream. This returns a generator where each row is yielded as a Python dictionary, allowing for efficient memory usage when processing large result sets.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nres = client.query_to_generator(\n    query='SELECT c1 FROM my_table',\n    ref='my_ref_or_branch_name',\n)\nfor row in res:\n    # do logic\n```\n\n----------------------------------------\n\nTITLE: Implementing PDF Analysis Pipeline in Python\nDESCRIPTION: Complete implementation of a PDF processing pipeline using Prefect tasks and flows, including PDF text extraction, OpenAI processing, and data storage functionality.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/10-pdf-analysis-with-openai/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom prefect import flow, task\nimport bauplan\nimport pandas as pd\nfrom openai import OpenAI\nimport PyPDF2\nimport os\n\n@task\ndef read_pdf(source: str) -> list:\n    # Read PDF files from source (e.g., S3)\n    pdf_files = bauplan.read(source, return_type=\"file_list\")\n    return pdf_files\n\n@task\ndef extract_text(pdf_files: list) -> pd.DataFrame:\n    # Extract text from PDFs\n    data = []\n    for pdf_path in pdf_files:\n        with open(pdf_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page in reader.pages:\n                text += page.extract_text() or \"\"\n            data.append({\"filename\": pdf_path, \"text\": text})\n    return pd.DataFrame(data)\n\n@task\ndef process_with_llm(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize OpenAI client\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    # Example: Summarize text\n    df['summary'] = df['text'].apply(\n        lambda x: client.completions.create(\n            model=\"text-davinci-003\",\n            prompt=f\"Summarize this text in 100 words or less: {x[:4000]}\",  # Truncate to avoid token limits\n            max_tokens=100\n        ).choices[0].text.strip()\n    )\n    return df\n\n@task\ndef write_data(df: pd.DataFrame, destination: str):\n    # Write results to destination\n    bauplan.write(df, destination)\n\n@flow(name=\"pdf-analysis-pipeline\")\ndef pdf_analysis_pipeline(source: str, destination: str):\n    pdf_files = read_pdf(source)\n    df = extract_text(pdf_files)\n    processed_df = process_with_llm(df)\n    write_data(processed_df, destination)\n\nif __name__ == \"__main__\":\n    pdf_analysis_pipeline(\n        source=\"s3://my-bucket/pdfs/\",\n        destination=\"s3://my-bucket/output/\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Training Linear Regression Model with Scikit-learn in Python\nDESCRIPTION: This code snippet demonstrates how to train a linear regression model using scikit-learn. It fits the model to the training data, which is a fundamental step in supervised learning tasks.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nreg = LinearRegression().fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Running the Pipeline with Prefect - Bash\nDESCRIPTION: Commands to start the Prefect server, deploy the pipeline, and execute it.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\nLANGUAGE: bash\nCODE:\n```\nprefect deploy -f recommender_pipeline.py\n```\n\nLANGUAGE: bash\nCODE:\n```\nprefect run -n recommender-pipeline\n```\n\n----------------------------------------\n\nTITLE: Joining and Combining Chunks with Arrow in Python\nDESCRIPTION: This snippet demonstrates how to join two tables using Arrow and combine the resulting chunks. It joins the 'trips' table with the 'zones' table on the 'PULocationID' and 'LocationID' columns respectively.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Python_snippets.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npickup_location_table = trips.join(zones, 'PULocationID', 'LocationID').combine_chunks()\n```\n\n----------------------------------------\n\nTITLE: Feature Scaling with StandardScaler in Python\nDESCRIPTION: This code snippet demonstrates how to perform feature scaling using StandardScaler from scikit-learn. It's a common preprocessing step in machine learning workflows to normalize feature scales.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Regression Model Performance in Python\nDESCRIPTION: This snippet shows how to evaluate the performance of a regression model using the score method. It calculates the coefficient of determination (R-squared) on the test set, providing a measure of model fit.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nreg.score(X_test, y_test)\n```\n\n----------------------------------------\n\nTITLE: Installing project dependencies using pip\nDESCRIPTION: Command to install all required dependencies from the requirements.txt file.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Uploading Vectors to MongoDB\nDESCRIPTION: Function to upload track vectors and associated metadata to MongoDB Atlas database\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef upload_vectors_to_mongodb(mongo_uri, table, db, collection): Upload track vectors and metadata to MongoDB Atlas\n```\n\n----------------------------------------\n\nTITLE: Analyzing Products per User Session by Brand in SQL\nDESCRIPTION: This SQL query calculates the average number of products per user session and total revenue by brand from ecommerce purchase events. It uses type casting to calculate the ratio of products to user sessions and rounds revenue figures for clarity.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT brand, COUNT(product_id)::FLOAT/count(distinct user_session) AS products_per_user_session, round(sum(price),2) AS revenue FROM ecommerce_clean WHERE event_type = 'purchase' GROUP BY 1 ORDER BY 3 DESC\n```\n\n----------------------------------------\n\nTITLE: Generating Track Vectors with Metadata\nDESCRIPTION: Bauplan model for training Skipgram embeddings, performing TSNE dimensionality reduction, and uploading results to MongoDB\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE') track_vectors_with_metadata(...): Train Skipgram, TSNE, upload to MongoDB\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Waiting Times by Zone in SQL\nDESCRIPTION: This SQL query calculates the average waiting time for taxis grouped by Borough and Zone. It aggregates data from the taxi_trip_waiting_times table and orders results by average waiting time in descending order.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Borough, Zone, AVG(waiting_time_minutes) AS avg_waiting_time\nFROM taxi_trip_waiting_times\nGROUP BY Borough, Zone\nORDER BY avg_waiting_time DESC;\n```\n\n----------------------------------------\n\nTITLE: Defining a Bauplan Model with Python Environment\nDESCRIPTION: This snippet demonstrates how to use the bauplan.model and bauplan.python decorators to define a data transformation model. It specifies the output columns, materialization strategy, Python version, and includes a data source definition.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(\n    columns=['*'],\n    materialization_strategy='NONE'\n)\n@bauplan.python('3.11')\ndef source_scan(\n    data=bauplan.Model(\n        'iot_kaggle',\n        columns=['*'],\n        filter=\"motion='false'\"\n    )\n):\n    # your code here\n    return data\n```\n\n----------------------------------------\n\nTITLE: Training Skipgram Model with Gensim\nDESCRIPTION: Implementation of Word2Vec skipgram model training using playlist data\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef skipgram_model(sequence_data): Gensim Word2Vec model training over playlists\n```\n\n----------------------------------------\n\nTITLE: Aggregating and Sorting Data with Pandas in Python\nDESCRIPTION: This snippet demonstrates how to perform grouping, aggregation, and sorting operations on a Pandas DataFrame. It's a common pattern for data analysis and summarization tasks.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntop_pickup_table = ( df.groupby([...]) .agg(...) .reset_index() .sort_values(...) )\n```\n\n----------------------------------------\n\nTITLE: Processing NY Taxi Data with Arrow and Pandas in Python\nDESCRIPTION: This code snippet outlines the two-step pipeline for processing NY taxi data. It uses Arrow for initial data processing and joining, then Pandas for aggregation and sorting. The pipeline optimizes S3 reads and separates Arrow and Pandas processing environments.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/02-data-visualization-app/README.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Step 1: trips_and_zones\n# Performs S3 scans, filter pushdown, column pruning, and join operation\n# Uses PyArrow for processing\n# Environment: Python 3.11 with Arrow native support\n\n# Step 2: top_pickup_locations\n# Converts Arrow table to Pandas DataFrame\n# Groups, aggregates, and sorts data\n# Materializes output into Bauplan catalog\n# Environment: Python 3.11 with Pandas 2.2.0\n```\n\n----------------------------------------\n\nTITLE: Processing OpenAI Predictions in Python\nDESCRIPTION: Function to make prediction requests to OpenAI API and parse boolean responses\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndef _request_prediction_from_open_ai(prompt, oai_client): call OpenAI and parse yes/no answer\n```\n\n----------------------------------------\n\nTITLE: Converting Arrow Table to Pandas DataFrame in Python\nDESCRIPTION: This snippet shows how to convert an Arrow table to a Pandas DataFrame. It's a simple one-line operation that can be useful when transitioning between Arrow and Pandas for different data processing tasks.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Python_snippets.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = data.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Cleaning Taxi Trip Data with Bauplan and Pandas in Python\nDESCRIPTION: This Bauplan model cleans taxi trip data by filtering out invalid entries. It uses both Arrow and Pandas for data processing, demonstrating the integration of these tools in a Bauplan workflow.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\n@bauplan.python('3.11', pip={'pandas': '2.2.0'})\ndef clean_taxi_trips(data=bauplan.Model('taxi_fhvhv', columns=[...], filter='...')):\n    df = data.to_pandas()\n    df = df[(df['trip_miles'] > 1.0) & (df['tips'] > 0.0) & (df['base_passenger_fare'] > 1.0)]\n    return df\n```\n\n----------------------------------------\n\nTITLE: Converting Playlists to Sequences using DuckDB\nDESCRIPTION: Bauplan model that uses DuckDB to aggregate track IDs into sequences per playlist\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model() playlists_to_sequences(...): DuckDB query to array-aggregate track_ids per playlist\n```\n\n----------------------------------------\n\nTITLE: Sampling Matching Products with Balanced Labels in SQL\nDESCRIPTION: This SQL query creates a balanced sample dataset of matching products by selecting equal numbers of positive (label=1) and negative (label=0) matches. It uses a parameterized limit {max_k} to control the sample size per class.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT ltable_id, rtable_id, label FROM matching_products WHERE label = 1 LIMIT {max_k} UNION ALL SELECT ltable_id, rtable_id, label FROM matching_products WHERE label = 0 LIMIT {max_k}\n```\n\n----------------------------------------\n\nTITLE: Normalizing Taxi Trip Data with Pandas in Python\nDESCRIPTION: This Bauplan model normalizes taxi trip data using Pandas. It filters data based on date and trip distance, and applies a logarithmic transformation to the trip miles. The model uses materialization strategy for efficient data processing.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE')\n@bauplan.python('3.11', pip={'pandas': '1.5.3', 'numpy': '1.23.2'})\ndef normalized_taxi_trips(\n    data=bauplan.Model('trips_and_zones')\n):\n    import pandas as pd\n    import numpy as np\n    import math\n\n    size_in_gb = round(data.nbytes / math.pow(1024, 3), 3)\n    print(f\"\\nThis table is {size_in_gb} GB and has {data.num_rows} rows\\n\")\n\n    df = data.to_pandas()\n    time_filter = pd.to_datetime('2022-01-01').tz_localize('UTC')\n    df = df[df['pickup_datetime'] >= time_filter]\n    df = df[df['trip_miles'] > 0.0]\n    df = df[df['trip_miles'] < 200.0]\n    df['log_trip_miles'] = np.log10(df['trip_miles'])\n    return df\n```\n\n----------------------------------------\n\nTITLE: Performing TSNE Analysis\nDESCRIPTION: Function to perform TSNE dimensionality reduction on embedding vectors using scikit-learn\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef tsne_analysis(embeddings): TSNE dimensionality reduction on embeddings\n```\n\n----------------------------------------\n\nTITLE: Top Pickup Locations Model Definition in Python\nDESCRIPTION: This snippet shows the definition of a Bauplan model for top pickup locations. It uses materialization strategy and specifies Python version and required pip packages.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE') @bauplan.python('3.11', pip={'pandas': '2.2.0'}) def top_pickup_locations(...):\n```\n\n----------------------------------------\n\nTITLE: Joining Trips and Zones Data with Chunk Combination in Python\nDESCRIPTION: This code snippet shows how to join trips and zones data using Bauplan, and then combine the resulting chunks. It's an example of a join operation with performance optimization.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npickup_location_table = (trips.join(zones, 'PULocationID', 'LocationID').combine_chunks())\n```\n\n----------------------------------------\n\nTITLE: Creating a Table in Bauplan (Python)\nDESCRIPTION: Demonstrates how to create a table from an S3 location using the Bauplan client. This method scans Parquet files to infer the schema and create the table.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\ntable = client.create_table(\n    table='my_table_name',\n    search_uri='s3://path/to/my/files/*.parquet',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Removing Outliers from DataFrame in Python\nDESCRIPTION: This snippet shows how to remove outliers from a Pandas DataFrame. It filters out rows where 'trip_miles' is less than or equal to 0.0 or greater than or equal to 200.0, effectively removing unrealistic or extreme values.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Python_snippets.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = df[df['trip_miles'] > 0.0]\ndf = df[df['trip_miles'] < 200.0]\n```\n\n----------------------------------------\n\nTITLE: Initializing and Querying with Bauplan Client\nDESCRIPTION: Demonstrates how to initialize the Bauplan client and execute a simple query to retrieve data from a table, returning an Arrow Table that can be converted to a pandas DataFrame.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\n# query the table and return result set as an arrow Table\nmy_table = client.query('SELECT sum(trips) trips FROM travel_table', branch_name='main')\n\n# efficiently cast the table to a pandas DataFrame\ndf = my_table.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Creating Namespaces in Bauplan\nDESCRIPTION: Demonstrates how to create a new namespace on a specific branch using the Bauplan client.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.create_namespace(\n    namespace='my_namespace_name',\n    branch='my_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Data and Metadata in Python using Bauplan Client\nDESCRIPTION: Illustrates how to retrieve table data and metadata using the Bauplan client. It prints field information and records for a specified table.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\ntable = client.get_table(\n    table='my_table_name',\n    ref='my_ref_or_branch_name',\n)\n\nfor c in table.fields:\n    print(c.name, c.required, c.type)\n\nprint(table.records)\n```\n\n----------------------------------------\n\nTITLE: Applying Log Transform to DataFrame Column in Python\nDESCRIPTION: This snippet demonstrates how to apply a logarithmic transformation to a column in a Pandas DataFrame. It creates a new column 'log_trip_miles' by applying a base-10 logarithm to the 'trip_miles' column, which can be useful for normalizing skewed data.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Python_snippets.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf['log_trip_miles'] = np.log10(df['trip_miles'])\n```\n\n----------------------------------------\n\nTITLE: Creating Branches in Bauplan\nDESCRIPTION: Shows how to create a new branch from an existing reference using the Bauplan client. Branch names should follow the convention of \"username.branch_name\".\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.create_branch(\n    branch='username.my_branch_name',\n    from_ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Finding Popular Tracks Using Unnest and Count in SQL\nDESCRIPTION: This SQL query identifies the most popular tracks by unnesting track_ids arrays from playlists and counting occurrences. It uses a parameterized limit {top_k} to return only the most frequently appearing tracks across playlists.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT unnest(track_ids) as track_id FROM playlists_to_sequences GROUP BY 1 ORDER BY COUNT(*) DESC LIMIT {top_k}\n```\n\n----------------------------------------\n\nTITLE: Scanning a Table with Filters in Bauplan with Python\nDESCRIPTION: Example showing how to use the scan method to query data from a table with column selection and filtering, returning the results as an Arrow Table.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nmy_table = client.scan(\n    table='my_table_name',\n    ref='my_ref_or_branch_name',\n    columns=['c1'],\n    filters='c2 > 10',\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Tag in Bauplan (Python)\nDESCRIPTION: Shows how to create a new tag at a given reference using the Bauplan client. This operation creates a named pointer to a specific state of the data.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.create_tag(\n    tag='my_tag',\n    from_ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting SQL Query Results to CSV with Bauplan in Python\nDESCRIPTION: Example showing how to use the query_to_csv_file method to execute a SQL query and save results to a CSV file. This method requires specifying a file path, SQL query, and optionally a reference branch.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nclient.query_to_csv_file(\n    path='./my.csv',\n    query='SELECT c1 FROM my_table',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Track IDs by Playlist in SQL\nDESCRIPTION: This SQL query aggregates track URIs into arrays ordered by position for each playlist. It uses the array_agg function to collect track URIs in position order, grouped by playlist_id for music playlist analysis.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT playlist_id, array_agg(track_uri ORDER BY pos ASC) as track_ids FROM tracks GROUP BY 1 ORDER BY 1 ASC\n```\n\n----------------------------------------\n\nTITLE: Deleting a Namespace in Bauplan (Python)\nDESCRIPTION: Demonstrates how to delete a namespace from a specific branch using the Bauplan client. This operation removes a logical grouping of tables from the data catalog.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.delete_namespace(\n    namespace='my_namespace_name',\n    branch='my_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Trips by Distance and Tips in SQL\nDESCRIPTION: This SQL query filters the trips_and_zones table to return only records where trip distance is greater than 1 mile and tips are greater than $0. It demonstrates basic WHERE clause filtering with multiple conditions.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM trips_and_zones\nWHERE trip_miles > 1.0 AND tips > 0.0;\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries with Bauplan Client in Python\nDESCRIPTION: Example demonstrating how to execute SQL queries and convert results to pandas DataFrame using the Bauplan client query method.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nmy_table = client.query(\n    query='SELECT c1 FROM my_table',\n    ref='my_ref_or_branch_name',\n)\n\ndf = my_table.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Branch in Bauplan (Python)\nDESCRIPTION: Demonstrates how to retrieve information about a specific branch using the Bauplan client. This operation fetches metadata about a named line of development in the data catalog.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nbranch = client.get_branch('my_branch_name')\nprint(branch.hash)\n```\n\n----------------------------------------\n\nTITLE: Counting Event Views by Hour and Brand in SQL\nDESCRIPTION: This SQL query counts the number of 'view' events by hour and brand from the ecommerce_clean table. It uses a CASE statement to count only view events while maintaining the hourly and brand groupings.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT event_hour, brand, SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) AS views FROM ecommerce_clean GROUP BY 1,2\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame by Timestamp in Python\nDESCRIPTION: This snippet demonstrates how to filter a Pandas DataFrame based on a timestamp. It creates a UTC-localized datetime object and uses it to filter rows in the DataFrame where 'pickup_datetime' is greater than or equal to the specified date.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Python_snippets.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntime_filter = pd.to_datetime('2022-01-01').tz_localize('UTC')\ndf = df[df['pickup_datetime'] >= time_filter]\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Namespaces in Python using Bauplan Client\nDESCRIPTION: Shows how to iterate through available namespaces using the Bauplan client. It requires specifying a reference (branch, tag, or ref) and prints each namespace name.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nfor namespace in client.get_namespaces('my_namespace_name'):\n    print(namespace.name)\n```\n\n----------------------------------------\n\nTITLE: Using a Connector in Bauplan Model Definition\nDESCRIPTION: This example shows how to use a connector (e.g., Dremio) when defining a Bauplan model. It demonstrates specifying the connector type and configuration key.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\ndef your_cool_model(\n    parent_0=bauplan.Model(\n        'some_parent_model',\n        columns=['bar'],\n        filter='bar > 1',\n        connector='dremio',\n        connector_config_key='bauplan',\n    )\n):\n    return pyarrow.Table.from_pandas(\n        pd.DataFrame({\n            'foo': parent_0['bar'] * 2,\n        })\n    )\n```\n\n----------------------------------------\n\nTITLE: Processing Taxi Trip Waiting Times with Python and Pandas\nDESCRIPTION: Calculates waiting times between request and on-scene timestamps for taxi trips using Bauplan model with Python 3.11 and Pandas 2.2.0\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE')\n@bauplan.python('3.11', pip={'pandas': '2.2.0'})\ndef taxi_trip_waiting_times(data=bauplan.Model('normalized_taxi_trips')):\n    waiting_time_min = pc.minutes_between(data['request_datetime'], data['on_scene_datetime'])\n    data = data.append_column('waiting_time_minutes', waiting_time_min)\n    return data\n```\n\n----------------------------------------\n\nTITLE: Creating Feature Correlation Heatmap\nDESCRIPTION: This snippet creates a heatmap to visualize the correlation between different features in the dataset. It uses seaborn and matplotlib for visualization.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/feature_exploration.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the correlation matrix\ncorr_matrix = df.corr()\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Heatmap of Feature Correlations')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Counting Purchase Events by Session and Hour in SQL\nDESCRIPTION: This SQL query counts purchase events from the ecommerce_clean table, grouped by user session and event hour. It specifically filters for events of type 'purchase' to analyze purchasing patterns across time periods for each user session.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT user_session, event_hour, count(*) FROM ecommerce_clean WHERE event_type = 'purchase' GROUP BY 1, 2\n```\n\n----------------------------------------\n\nTITLE: Authentication Methods in Bauplan Client\nDESCRIPTION: Shows various ways to authenticate with the Bauplan client, including using environment variables, configuration files, and direct parameter specification.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# by default, authenticate from BAUPLAN_API_KEY >> BAUPLAN_PROFILE >> ~/.bauplan/config.yml\nclient = bauplan.Client()\n# client used ~/.bauplan/config.yml profile 'default'\n\nos.environ['BAUPLAN_PROFILE'] = \"someprofile\"\nclient = bauplan.Client()\n# >> client now uses profile 'someprofile'\n\nos.environ['BAUPLAN_API_KEY'] = \"mykey\"\nclient = bauplan.Client()\n# >> client now authenticates with api_key value \"mykey\", because api key > profile\n\n# specify authentication directly - this supercedes BAUPLAN_API_KEY in the environment\nclient = bauplan.Client(api_key='MY_KEY')\n\n# specify a profile from ~/.bauplan/config.yml - this supercedes BAUPLAN_PROFILE in the environment\nclient = bauplan.Client(profile='default')\n```\n\n----------------------------------------\n\nTITLE: Visualizing Correlation with Target Column\nDESCRIPTION: This code creates a heatmap to specifically visualize the correlation of various features with the target column 'tips'. It sorts the correlations and uses seaborn for visualization.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/feature_exploration.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Extract correlations with the target column\ntarget_corr = corr_matrix[['tips']].drop('tips')\n# Sort the correlations\nsorted_target_corr = target_corr.sort_values(by='tips', ascending=False)\n# Plot the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(sorted_target_corr, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation with Target Column')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Bauplan SDK and Converting to Pandas DataFrame\nDESCRIPTION: This code instantiates a Bauplan client, defines an SQL query to fetch taxi data, executes the query, and converts the result to a Pandas DataFrame. It demonstrates data retrieval and transformation using Bauplan.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/feature_exploration.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# instantiate the sdk client \nclient = bauplan.Client()\n\n# define the target branch and the sql expression to pass to the query method\nactive_branch = \"main\"\nsql_query = \"\"\"\nSELECT\n    pickup_datetime,\n    dropoff_datetime,\n    PULocationID,\n    DOLocationID,\n    trip_miles,\n    trip_time,\n    base_passenger_fare,\n    tolls,\n    sales_tax,\n    tips\nFROM\n    taxi_fhvhv\nWHERE\n    pickup_datetime >= '2022-10-01T00:00:00-05:00' AND\n    pickup_datetime < '2022-11-01T00:00:00-05:00'\n\"\"\"\n\n# run a query and get in return an arrow table\ntable = client.query(\n    sql_query, \n    ref=active_branch\n)\n\n# convert the arrow table into a Pandas DataFrame\ndf = table.to_pandas()\n\n# display the Pandas DataFrame\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Data Quality Expectation Test for Null Values\nDESCRIPTION: Tests for null values in the on_scene_datetime column using Bauplan expectations framework\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.expectation()\n@bauplan.python('3.11')\ndef test_null_values_on_scene_datetime(data=bauplan.Model('normalized_taxi_trips')):\n    column_to_check = 'on_scene_datetime'\n    _is_expectation_correct = expect_column_no_nulls(data, column_to_check)\n    assert _is_expectation_correct, f\"expectation test failed: we expected {column_to_check} to have no null values\"\n    return _is_expectation_correct\n```\n\n----------------------------------------\n\nTITLE: Saving Trained Model with Bauplan Store in Python\nDESCRIPTION: This snippet shows how to save a trained model object using Bauplan's store functionality. It allows for persistence of trained models for later use in prediction or further analysis.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom bauplan.store import save_obj\nsave_obj(\"regression\", reg)\n```\n\n----------------------------------------\n\nTITLE: Debugging Data Size and Row Count in Python\nDESCRIPTION: This snippet shows how to calculate and print the size of an Arrow table in gigabytes and its number of rows. It's useful for debugging and understanding the scale of the data being processed in a pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Python_snippets.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsize_in_gb = round(data.nbytes / math.pow(1024, 3), 3)\nprint(f\"\\nThis table is {size_in_gb} GB and has {data.num_rows} rows\\n\")\n```\n\n----------------------------------------\n\nTITLE: Defining Materialization Strategy in Bauplan Models\nDESCRIPTION: This snippet demonstrates how to specify a materialization strategy for a Bauplan model using the 'REPLACE' option. This ensures the model will replace any existing data when run.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Best_Practices.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE')\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Model with Bauplan Store in Python\nDESCRIPTION: This code snippet demonstrates how to load a previously saved model object using Bauplan's store functionality. It's useful for retrieving trained models for making predictions or further analysis.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom bauplan.store import load_obj\nreg = load_obj(\"regression\")\n```\n\n----------------------------------------\n\nTITLE: Defining a Model in Bauplan Python\nDESCRIPTION: This snippet demonstrates how to define a model using the Bauplan Model class. It shows how to specify a parent model with column and filter constraints.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model()\ndef some_parent_model():\n    return pyarrow.Table.from_pydict({'bar': [1, 2, 3]})\n\n@bauplan.model()\ndef your_cool_model(\n    parent_0=bauplan.Model(\n        'some_parent_model',\n        columns=['bar'],\n        filter='bar > 1',\n    )\n):\n    return pyarrow.Table.from_pandas(\n        pd.DataFrame({\n            'foo': parent_0['bar'] * 2,\n        })\n    )\n```\n\n----------------------------------------\n\nTITLE: Exporting SQL Query Results to JSON with Bauplan in Python\nDESCRIPTION: Example showing how to use the query_to_json_file method to execute a SQL query and save results to a JSON file. This method supports both standard JSON and JSONL file formats through the file_format parameter.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nclient.query_to_json_file(\n    path='./my.json',\n    query='SELECT c1 FROM my_table',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Taxi FHVHV Model with Column Selection and Filtering in Python\nDESCRIPTION: This code snippet shows how to define a Bauplan model for taxi FHVHV data. It demonstrates column selection and filter pushdown techniques for optimized data retrieval.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrips=bauplan.Model(\n    'taxi_fhvhv',\n    columns=[\n        'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID',\n        'trip_miles', 'trip_time', 'base_passenger_fare', 'tolls', 'sales_tax', 'tips'\n    ],\n    filter=\"pickup_datetime >= '2022-12-15T00:00:00-05:00' AND pickup_datetime < '2023-01-01T00:00:00-05:00'\"\n)\n```\n\n----------------------------------------\n\nTITLE: Counting Trips by Pickup Location in SQL\nDESCRIPTION: This query counts taxi trips grouped by pickup location ID from the taxi_fhvhv table. It filters data for trips between December 2022 and January 2023, grouping by pickup location and ordering by trip count in descending order.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT PULocationID, COUNT(*) AS trip_count\nFROM taxi_fhvhv\nWHERE pickup_datetime BETWEEN '2022-12-01' AND '2023-01-01'\nGROUP BY PULocationID\nORDER BY trip_count DESC;\n```\n\n----------------------------------------\n\nTITLE: Creating Actual vs. Predicted Values Scatter Plot\nDESCRIPTION: This code creates a scatter plot comparing actual tip values against predicted tip values. It also includes a y=x line for reference. The plot helps visualize the accuracy of the predictions.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/prediction_visualization.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 6))\nplt.scatter(df['tips'], df['predictions'], alpha=0.6, c='blue', label='Predictions', s=100)\nplt.scatter(df['tips'], df['tips'], alpha=0.6, c='pink', label='Actual Values', s=100)\nplt.plot([min(df['tips']), max(df['tips'])], [min(df['tips']), max(df['tips'])], color='red', linewidth=1, label='y=x line')\nplt.xlabel('Actual Tips')\nplt.ylabel('Predicted Tips')\nplt.title('Actual vs Predicted Tips')\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Abbreviated Trips and Zones Model Definition in Python\nDESCRIPTION: This is a shortened version of the trips_and_zones model definition using Bauplan. It demonstrates the use of decorators for model and Python version specification.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model() @bauplan.python('3.11') def trips_and_zones(...):\n```\n\n----------------------------------------\n\nTITLE: Joining Track Data with Metadata in SQL\nDESCRIPTION: This SQL query joins a track table with track metadata to enrich track information with names and artist details. It demonstrates a basic JOIN operation to combine data from two related tables using track_id/track_uri as the join key.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/sql_templates.md#2025-04-21_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT t.track_id, track_metadata.track_name, track_metadata.artist_name FROM track_table t JOIN track_metadata ON t.track_id = track_metadata.track_uri\n```\n\n----------------------------------------\n\nTITLE: Generating Residual Plot\nDESCRIPTION: This snippet creates a residual plot, showing the differences between actual and predicted tip values against the predicted values. It helps identify any patterns in prediction errors.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/prediction_visualization.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresiduals = df['tips'] - df['predictions']\nplt.figure(figsize=(10, 6))\nplt.scatter(df['predictions'], residuals, alpha=0.6)\nplt.axhline(0, color='red', linestyle='--', linewidth=2)\nplt.xlabel('Predicted Tips')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Predicted Tips')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring environment variables for API access\nDESCRIPTION: Shell commands to set up necessary environment variables for Bauplan, AWS, and LLM API access.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\nexport LLM_API_KEY=<your-llm-api-key>\n```\n\n----------------------------------------\n\nTITLE: Exporting SQL Query Results to Parquet with Bauplan in Python\nDESCRIPTION: Example demonstrating how to use the query_to_parquet_file method to execute a SQL query and save results to a Parquet file. Parquet is an efficient columnar storage format that is useful for analytics workloads.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nclient.query_to_parquet_file(\n    path='./my.parquet',\n    query='SELECT c1 FROM my_table',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Plotting Distribution of Residuals\nDESCRIPTION: This code creates a histogram showing the distribution of residuals (differences between actual and predicted tip values). It uses Seaborn's histplot function with a KDE overlay.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/prediction_visualization.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 6))\nsns.histplot(residuals, kde=True)\nplt.xlabel('Residuals')\nplt.title('Distribution of Residuals')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables with Bash\nDESCRIPTION: Configuration of required environment variables for API keys and AWS credentials.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/10-pdf-analysis-with-openai/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\nexport OPENAI_API_KEY=<your-openai-api-key>\n```\n\n----------------------------------------\n\nTITLE: Setting Materialization Strategy in Bauplan Model\nDESCRIPTION: This snippet demonstrates how to set the materialization strategy for a Bauplan model. It uses the 'REPLACE' strategy, which is useful for optimizing data storage and retrieval.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model(materialization_strategy='REPLACE')\n```\n\n----------------------------------------\n\nTITLE: Importing Data with Bauplan Client in Python\nDESCRIPTION: Example showing how to import data into an existing table from S3 using the Bauplan client import_data method with error handling.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nplan_state = client.import_data(\n    table='my_table_name',\n    search_uri='s3://path/to/my/files/*.parquet',\n    branch='my_branch_name',\n)\nif plan_state.error:\n    plan_error_action(...)\nsuccess_action(plan_state.plan)\n```\n\n----------------------------------------\n\nTITLE: Handling Exceptions in Bauplan Operations\nDESCRIPTION: Demonstrates how to handle exceptions that may occur during Bauplan operations, including checking job status and using try/except blocks.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    state = client.run(...)\n    state = client.query(...)\n    state = client.scan(...)\n    state = client.plan_table_creation(...)\nexcept bauplan.exceptions.JobError as e:\n    ...\n\nstate = client.run(...)\nif state.job_status != \"success\":\n    ...\n```\n\n----------------------------------------\n\nTITLE: Reverting a Table to a Previous State in Bauplan with Python\nDESCRIPTION: Example demonstrating how to use the revert_table method to restore a table to a previous state from a specific reference into a target branch.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.revert_table(\n    table='my_table_name',\n    source_ref='my_ref_or_branch_name',\n    into_branch='main',\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Embedding Pipeline Environment\nDESCRIPTION: Configures a Python 3.11 environment with Gensim, Scikit-learn, DuckDB, and PyMongo for embedding pipeline\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'gensim': '4.3.3', 'scikit_learn': '1.5.2', 'duckdb': '1.0.0', 'pymongo': '4.10.1'})\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas with Scikit-Learn Environment\nDESCRIPTION: Establishes a Python 3.10 environment with Pandas 1.5.3 and Scikit-Learn 1.3.2\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.10', pip={'pandas': '1.5.3', 'scikit-learn': '1.3.2'})\n```\n\n----------------------------------------\n\nTITLE: Loading Prediction Data with Bauplan SDK\nDESCRIPTION: This code demonstrates how to use the Bauplan SDK to load prediction data from a specified branch. It queries a table named 'tip_predictions' and converts the result to a Pandas DataFrame.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/prediction_visualization.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\n\n# instantiate the sdk client\nclient = bauplan.Client()\n\n# define the target branch and the sql expression to pass to the query method.\n# Remember to pass the right branch.\nactive_branch = \"main\"  #  Put your active branch in here!! \nsql_query = \"SELECT * FROM tip_predictions\"\n\n# run the query, get in return an arrow table and convert it into a Pandas DataFrame\ntable = client.query(\n    sql_query, \n    ref=active_branch)\n\n# convert the arrow table into a Pandas DataFrame\ndf = table.to_pandas()\n\n# display the Pandas DataFrame\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Bauplan\nDESCRIPTION: This code snippet lists the required Python packages and their specific versions for the Bauplan project. It includes Streamlit for web app development, Pandas for data manipulation, Matplotlib and Seaborn for data visualization, and Plotly for interactive charts.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/02-data-visualization-app/app/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nbauplan\nstreamlit==1.28.1\npandas==2.2.0\nmatplotlib==3.8.1\nseaborn==0.13.0\nplotly==5.24.1\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas and NumPy Environment\nDESCRIPTION: Sets up a Python 3.11 environment with Pandas 1.5.3 and NumPy 1.23.2 dependencies\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'pandas': '1.5.3', 'numpy': '1.23.2'})\n```\n\n----------------------------------------\n\nTITLE: Merging Branches with Bauplan Client in Python\nDESCRIPTION: Example demonstrating how to merge one branch into another using the Bauplan client merge_branch method.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.merge_branch(\n    source_ref='my_ref_or_branch_name',\n    into_branch='main',\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pandas 2.2.0 for Data Processing\nDESCRIPTION: Establishes a Python 3.11 environment with Pandas 2.2.0 for data processing tasks\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'pandas': '2.2.0'})\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB 1.0.0 Environment\nDESCRIPTION: Sets up a Python 3.11 environment with DuckDB 1.0.0\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'duckdb': '1.0.0'})\n```\n\n----------------------------------------\n\nTITLE: Running Prefect Server and Deploying Pipeline (Bash)\nDESCRIPTION: Commands to start the Prefect server, deploy the pipeline, and execute it. These steps are necessary for running and monitoring the data product pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/13-data-products/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\nLANGUAGE: bash\nCODE:\n```\nprefect deploy -f dataproduct_pipeline.py\n```\n\nLANGUAGE: bash\nCODE:\n```\nprefect run -n dataproduct-pipeline\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet lists the required Python packages and their specific versions for the 'bauplan' project. It includes Streamlit for web apps, Pandas for data manipulation, Boto3 for AWS services, Matplotlib for visualization, OpenAI for AI capabilities, and Pinecone for vector database operations.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/11-RAG-service-support-agent/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nbauplan\nstreamlit==1.41.1\npandas==2.2.0\nboto3==1.35.77\nmatplotlib==3.8.1\nopenai==1.57.2\npinecone==5.4.2\npinecone-plugin-records==1.1.0\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB with OpenAI Environment\nDESCRIPTION: Creates a Python 3.11 environment with DuckDB 1.0.0 and OpenAI 1.57.2 integration\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'duckdb': '1.0.0', 'openai': '1.57.2'})\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables (Bash)\nDESCRIPTION: Sets up environment variables for Bauplan API key and AWS credentials, which are required for accessing S3 and Bauplan services.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/13-data-products/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies List\nDESCRIPTION: Lists required Python packages and their versions including Streamlit, Pandas, Boto3, Matplotlib and PyMongo. These dependencies are essential for running the bauplan project.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbauplan\nstreamlit==1.37.0\npandas==2.2.0\nboto3==1.35.77\nmatplotlib==3.8.1\npmongo==4.10.1\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Bauplan Project\nDESCRIPTION: This code snippet defines the exact versions of Python packages required for the Bauplan project. It includes Streamlit for web app creation, Prefect for workflow management, data visualization libraries like Matplotlib, Seaborn, and Plotly, and Boto3 for AWS SDK integration.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nstreamlit==1.40.1\nprefect==3.1.2\nmatplotlib==3.9.2\nseaborn==0.13.2\nplotly==5.24.1\nboto3==1.35.64\nbauplan\n```\n\n----------------------------------------\n\nTITLE: Deploying the LLM Tabular Pipeline\nDESCRIPTION: Command to deploy the LLM tabular data processing pipeline to Prefect.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nprefect deploy -f llm_tabular_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Running a Bauplan Pipeline\nDESCRIPTION: Commands to create, checkout and run a Bauplan data pipeline branch. The example shows how to initialize a branch with a username and branch name before executing the pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/01-quick-start/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ bauplan branch create <username>.<branch_name>\n$ bauplan checkout <username>.<branch_name>\n$ bauplan run\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: Defines the exact versions of Python packages required for the project to ensure consistency and compatibility. Includes core packages for web development (Streamlit), data processing (Pandas), AWS integration (Boto3), and data visualization (Matplotlib).\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbauplan\nstreamlit==1.41.1\npandas==2.2.0\nboto3==1.35.77\nmatplotlib==3.8.1\n```\n\n----------------------------------------\n\nTITLE: Deploying LLM pipeline to Prefect\nDESCRIPTION: Command to deploy the defined pipeline to the Prefect server for execution and scheduling.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nprefect deploy -f llm_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up Pandas 2.2.0 Environment\nDESCRIPTION: Configures a Python 3.11 environment with Pandas 2.2.0\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'pandas': '2.2.0'})\n```\n\n----------------------------------------\n\nTITLE: Describing Bauplan Pipeline for NYC Taxi Data Analysis\nDESCRIPTION: This markdown snippet outlines a Bauplan pipeline that processes NYC taxi data. It details steps for data normalization, waiting time calculation, aggregation, and includes a data quality check using an expectation test.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/04-data-quality-expectations/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Data Quality and Expectations\n\nIn this example, we illustrate how to use expectation tests. These tests are statistical and quality checks applied to Bauplan models to ensure that the structure and values of the data meet our expectations. Expectation tests help detect data quality issues early and can be incorporated seamlessly into various workflows.\n\n---\n1.  Description of the Pipeline\nThis Bauplan pipeline analyzes NYC taxi data to compute average taxi waiting times by pickup zone, while ensuring data quality through integrated expectations.\n\n Step 1: normalized_taxi_trips\nS3 scan over taxi_fhvhv and taxi_zones tables.\n\nFilter pushdown: selects December 2022 trips.\n\nArrow Join: joins trips with zones by PULocationID and LocationID.\n\nResult: An Arrow table mapping each taxi trip to its pickup location metadata.\n\n Step 2: taxi_trip_waiting_times\nCalculates waiting time (in minutes) between:\n\nrequest_datetime (ride requested)\n\non_scene_datetime (taxi arrives at pickup)\n\nAppends a new column waiting_time_minutes.\n\nResult: Extended Arrow table including waiting times.\n\n Step 3: zone_avg_waiting_times\nUses DuckDB to:\n\nCompute average waiting times for each Borough and Zone.\n\nOrder zones by descending waiting time.\n\nQueries Arrow table directly without conversion (DuckDB reads Arrow natively).\n\nMaterialization strategy: REPLACE to persist the final result.\n\n Step 4: test_null_values_on_scene_datetime (Expectation)\nBefore allowing computations to proceed, tests that:\n\nColumn on_scene_datetime has no null values.\n\nIf nulls exist:\n\nRaises an assertion error and stops the pipeline execution.\n\nQuality assurance step: prevents incorrect waiting time calculations.\n\n Architectural Highlights:\nMix of Arrow and DuckDB: optimized for large-scale tabular computations.\n\nFilter pushdown and join optimization at S3 scan level.\n\nIntegrated data expectations: built-in validation before heavy computation.\n\nClear, modular separation between models and expectations for maintainability.\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for RAG Pipeline\nDESCRIPTION: Shell commands to set up environment variables required for the RAG pipeline, including API keys and database URLs.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/11-RAG-service-support-agent/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\nexport OPENAI_API_KEY=<your-openai-api-key>\nexport VECTOR_DB_URL=<your-vector-db-url>  # Optional, depending on vector DB\n```\n\n----------------------------------------\n\nTITLE: Setting Up DuckDB 0.10.3 Environment\nDESCRIPTION: Creates a Python 3.11 environment with DuckDB 0.10.3\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Environment_Setups.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.python('3.11', pip={'duckdb': '0.10.3'})\n```\n\n----------------------------------------\n\nTITLE: Starting the Prefect Server\nDESCRIPTION: Command to start the Prefect server for workflow management and monitoring.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\n----------------------------------------\n\nTITLE: Checking Namespace Existence in Python using Bauplan Client\nDESCRIPTION: Illustrates how to check if a namespace exists using the Bauplan client. It requires specifying the namespace name and a reference (branch, tag, or ref).\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.has_namespace(\n    namespace='my_namespace_name',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Table Existence with Bauplan Client in Python\nDESCRIPTION: Example showing how to check if a table exists in a specific reference or branch using the Bauplan client has_table method.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.has_table(\n    table='my_table_name',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Prefect server for workflow orchestration\nDESCRIPTION: Command to start the Prefect server for monitoring and managing workflows.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\n----------------------------------------\n\nTITLE: Checking Branch Existence in Python using Bauplan Client\nDESCRIPTION: Demonstrates how to check if a branch exists using the Bauplan client. It uses an assertion to verify the branch's existence.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.has_branch('my_branch_name')\n```\n\n----------------------------------------\n\nTITLE: Listing Branches in Bauplan (Python)\nDESCRIPTION: Shows how to list available branches in the Bauplan catalog. This operation retrieves metadata about multiple lines of development in the data catalog, with optional filtering and pagination.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nfor branch in client.get_branches():\n    print(branch.name, branch.hash)\n```\n\n----------------------------------------\n\nTITLE: Running and Deploying RAG Pipeline with Prefect\nDESCRIPTION: Commands to start the Prefect server, deploy the RAG pipeline, and execute it using Prefect CLI.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/11-RAG-service-support-agent/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n\nprefect deploy -f rag_pipeline.py\n\nprefect run -n rag-pipeline\n```\n\n----------------------------------------\n\nTITLE: Checking Tag Existence with Bauplan Client in Python\nDESCRIPTION: Example demonstrating how to verify if a tag exists using the Bauplan client has_tag method.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.has_tag(\n    tag='my_tag_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Namespace in Python using Bauplan Client\nDESCRIPTION: Demonstrates how to retrieve a namespace using the Bauplan client. It requires specifying the namespace name and a reference (branch, tag, or ref).\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nnamespace = client.get_namespace(\n    namespace='my_namespace_name',\n    ref='my_ref_or_branch_name',\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying the Prefect Pipeline\nDESCRIPTION: Command to deploy the pipeline to the Prefect server.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nprefect deploy -f nrt_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Bauplan Project\nDESCRIPTION: This snippet defines the required Python packages and their versions for the 'bauplan' project. It includes Streamlit 1.41.1 for web app creation, Pandas 2.2.0 for data handling, Boto3 1.35.77 for AWS services integration, and Matplotlib 3.8.1 for plotting and visualization.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/10-pdf-analysis-with-openai/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nbauplan\nstreamlit==1.41.1\npandas==2.2.0\nboto3==1.35.77\nmatplotlib==3.8.1\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Bauplan and Prefect\nDESCRIPTION: Commands to set up necessary environment variables including API keys for Bauplan, AWS credentials, and LLM API key.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\nexport LLM_API_KEY=<your-llm-api-key>\n```\n\n----------------------------------------\n\nTITLE: Running the LLM pipeline with Prefect\nDESCRIPTION: Command to manually trigger the execution of the deployed LLM pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nprefect run -n llm-pipeline\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: Specifies exact versions of required Python packages. Includes Streamlit for web app creation, Pandas for data manipulation, Boto3 for AWS integration, and Matplotlib for data visualization.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbauplan\nstreamlit==1.28.1\npandas==2.2.0\nboto3==1.35.77\nmatplotlib==3.8.1\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Bauplan and Prefect\nDESCRIPTION: Command to install the required dependencies listed in the requirements.txt file for the LLM tabular data processing pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Prefect Pipeline Commands with Bash\nDESCRIPTION: Commands for starting the Prefect server, deploying, and running the pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/10-pdf-analysis-with-openai/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\nLANGUAGE: bash\nCODE:\n```\nprefect deploy -f pdf_analysis_pipeline.py\n```\n\nLANGUAGE: bash\nCODE:\n```\nprefect run -n pdf-analysis-pipeline\n```\n\n----------------------------------------\n\nTITLE: Accessing Bauplan logs for monitoring\nDESCRIPTION: Command to retrieve and view Bauplan logs for monitoring the data processing operations.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbauplan logs\n```\n\n----------------------------------------\n\nTITLE: Running the LLM Tabular Pipeline\nDESCRIPTION: Command to execute the deployed LLM tabular data processing pipeline through Prefect.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nprefect run -n llm-tabular-pipeline\n```\n\n----------------------------------------\n\nTITLE: Monitoring Pipeline Logs with Bash\nDESCRIPTION: Command to check Bauplan logs for pipeline monitoring.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/10-pdf-analysis-with-openai/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbauplan logs\n```\n\n----------------------------------------\n\nTITLE: Checking Bauplan Logs\nDESCRIPTION: Command to view Bauplan logs for data processing details.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbauplan logs\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables - Bash\nDESCRIPTION: Setting up environment variables for API keys and connection strings needed by the recommender system.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\nexport MONGODB_URI=<your-mongodb-atlas-uri>\n```\n\n----------------------------------------\n\nTITLE: Deleting a Branch in Bauplan (Python)\nDESCRIPTION: Illustrates how to delete a branch using the Bauplan client. This operation removes a named line of development from the data catalog.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.delete_branch('my_branch_name')\n```\n\n----------------------------------------\n\nTITLE: Starting the Prefect Server\nDESCRIPTION: Command to start the Prefect server for managing and monitoring workflow executions.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nprefect server start\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Library Dependencies\nDESCRIPTION: This snippet lists the required Python libraries and their versions for the 'bauplan' project. It includes matplotlib and seaborn for data visualization, pandas for data manipulation, jupyterlab for interactive development, and streamlit for creating web applications.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nbauplan\nmatplotlib==3.8.1\nseaborn==0.13.0\npandas==2.2.0\njupyterlab>=4\nstreamlit==1.28.1\n```\n\n----------------------------------------\n\nTITLE: Deleting a Tag in Bauplan (Python)\nDESCRIPTION: Illustrates how to delete a tag using the Bauplan client. This operation removes a named pointer to a specific state of the data.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\nclient = bauplan.Client()\n\nassert client.delete_tag('my_tag_name')\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Bauplan and AWS\nDESCRIPTION: Setting up required environment variables for Bauplan API key and AWS credentials.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport BAUPLAN_API_KEY=<your-api-key>\nexport AWS_ACCESS_KEY_ID=<your-access-key>\nexport AWS_SECRET_ACCESS_KEY=<your-secret-key>\n```\n\n----------------------------------------\n\nTITLE: Abbreviated Taxi FHVHV Model Definition in Python\nDESCRIPTION: This is a shortened version of the taxi FHVHV model definition using Bauplan. It demonstrates the use of column selection and filtering in a concise format.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrips=bauplan.Model('taxi_fhvhv', columns=[...], filter='...')\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies using Pip\nDESCRIPTION: Command to install required Python packages from the requirements file.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting up project environment with Git and pip\nDESCRIPTION: Commands for cloning the example repository and installing dependencies to get started with the LLM pipeline project.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/07-entity-matching-with-llm/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n```\n\n----------------------------------------\n\nTITLE: Monitoring Pipeline Execution - Bash\nDESCRIPTION: Command to check Bauplan logs for processing details when monitoring the recommender system pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbauplan logs\n```\n\n----------------------------------------\n\nTITLE: Setting Up the Environment - Bash\nDESCRIPTION: Commands to clone the example repository and install the necessary dependencies for the recommender system.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/08-playlist-recomendations-mongodb/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install bauplan pymongo sentence-transformers pandas\n```\n\n----------------------------------------\n\nTITLE: Cloning Example Repository and Installing Dependencies (Bash)\nDESCRIPTION: Commands to clone the example repository and install required Python packages for the data product pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/13-data-products/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install bauplan prefect pandas\n```\n\n----------------------------------------\n\nTITLE: Checking Bauplan Logs\nDESCRIPTION: Command to view Bauplan logs for detailed information about data processing operations.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbauplan logs\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies with Bash\nDESCRIPTION: Initial repository setup and dependency installation commands using git and pip.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/10-pdf-analysis-with-openai/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install bauplan prefect PyPDF2 pandas openai\n```\n\n----------------------------------------\n\nTITLE: Cloning the Example Repository for Bauplan and Prefect\nDESCRIPTION: Commands to clone the example repository for the LLM tabular data processing pipeline and navigate to the project directory.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/09-unstructured-to-structured-with-llm/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n```\n\n----------------------------------------\n\nTITLE: Monitoring Pipeline with Bauplan Logs (Bash)\nDESCRIPTION: Command to check Bauplan logs for detailed information about data processing in the pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/13-data-products/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbauplan logs\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Installing Dependencies for RAG Pipeline\nDESCRIPTION: Commands to clone the example repository and install the necessary Python packages for the RAG pipeline.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/11-RAG-service-support-agent/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n\npip install bauplan prefect sentence-transformers pandas faiss-cpu openai\n```\n\n----------------------------------------\n\nTITLE: Cloning the Example Repository using Bash\nDESCRIPTION: Command line instructions to clone the example repository and navigate to the project directory.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/BauplanLabs/wap_with_bauplan_and_prefect.git\ncd wap_with_bauplan_and_prefect\n```\n\n----------------------------------------\n\nTITLE: Running the Prefect Pipeline\nDESCRIPTION: Command to execute the deployed pipeline via the Prefect server.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/06-near-real-time/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nprefect run -n nrt-pipeline\n```\n\n----------------------------------------\n\nTITLE: Importing Bauplan SDK\nDESCRIPTION: This snippet imports the Bauplan SDK, which is used for data retrieval and manipulation in the subsequent code.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/feature_exploration.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport bauplan\n```\n\n----------------------------------------\n\nTITLE: Checking Python Interpreter Path\nDESCRIPTION: This snippet prints the path of the Python interpreter being used, which is useful for debugging and ensuring the correct environment is active.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/prediction_visualization.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nprint(sys.executable)\n```\n\n----------------------------------------\n\nTITLE: Checking Python Executable Path\nDESCRIPTION: This snippet prints the path of the Python executable being used. It's useful for verifying the Python environment.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/03-ml-regression-model/notebooks/feature_exploration.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nprint(sys.executable)\n```\n\n----------------------------------------\n\nTITLE: Defining an Expectation in Bauplan Python\nDESCRIPTION: This snippet illustrates how to define an expectation using the Bauplan expectation decorator. It includes specifying a Python environment and using a Model as input.\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Module_Documentation.md#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.expectation()\n@bauplan.python('3.10')\ndef test_joined_dataset(\n    data=bauplan.Model(\n        'join_dataset',\n        columns=['anomaly']\n    )\n):\n    # your data validation code here\n    return expect_column_no_nulls(data, 'anomaly')\n```\n\n----------------------------------------\n\nTITLE: Analyzing Popular Tracks with DuckDB\nDESCRIPTION: Bauplan model implementing track popularity analysis by unnesting track IDs and counting occurrences\nSOURCE: https://github.com/marcoeg/bauplan/blob/main/Documentation/Code_Examples.md#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n@bauplan.model() popular_tracks(...): DuckDB query unnesting track_ids and counting most common tracks\n```"
  }
]
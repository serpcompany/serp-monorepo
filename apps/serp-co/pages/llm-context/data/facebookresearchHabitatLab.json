[
  {
    "owner": "facebookresearch",
    "repo": "habitat-lab",
    "content": "TITLE: Defining a Simple Forward-Only Agent in Habitat (Python)\nDESCRIPTION: Defines a custom agent in Habitat that moves forward until it reaches the goal, then stops. The agent uses a distance threshold to determine when the goal is reached and interacts with the environment using the `act` function.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# An example agent which can be submitted to habitat-challenge.\n# To participate and for more details refer to:\n# - https://aihabitat.org/challenge/2020/\n# - https://github.com/facebookresearch/habitat-challenge\n\n\nclass ForwardOnlyAgent(habitat.Agent):\n    def __init__(self, success_distance, goal_sensor_uuid):\n        self.dist_threshold_to_stop = success_distance\n        self.goal_sensor_uuid = goal_sensor_uuid\n\n    def reset(self):\n        pass\n\n    def is_goal_reached(self, observations):\n        dist = observations[self.goal_sensor_uuid][0]\n        return dist <= self.dist_threshold_to_stop\n\n    def act(self, observations):\n        if self.is_goal_reached(observations):\n            action = HabitatSimActions.stop\n        else:\n            action = HabitatSimActions.move_forward\n        return {\"action\": action}\n```\n\n----------------------------------------\n\nTITLE: Environment Interaction with Habitat API\nDESCRIPTION: This code snippet demonstrates a minimal environment interaction loop using the Habitat API. It initializes a Habitat environment with a specified configuration file, then takes random actions within the environment, recording observations, rendering frames, and saving the simulation as a video. The `insert_render_options` function is used to configure the rendering.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith habitat.Env(\n    config=insert_render_options(\n        habitat.get_config(\n            os.path.join(\n                dir_path,\n                \"habitat-lab/habitat/config/benchmark/rearrange/skills/pick.yaml\",\n            ),\n        )\n    )\n) as env:\n    observations = env.reset()  # noqa: F841\n\n    print(\"Agent acting inside environment.\")\n    count_steps = 0\n    # To save the video\n    video_file_path = os.path.join(output_path, \"example_interact.mp4\")\n    video_writer = imageio.get_writer(video_file_path, fps=30)\n\n    while not env.episode_over:\n        observations = env.step(env.action_space.sample())  # noqa: F841\n        info = env.get_metrics()\n\n        render_obs = observations_to_image(observations, info)\n        render_obs = overlay_frame(render_obs, info)\n\n        video_writer.append_data(render_obs)\n\n        count_steps += 1\n    print(\"Episode finished after {} steps.\".format(count_steps))\n\n    video_writer.close()\n    if vut.is_notebook():\n        vut.display_video(video_file_path)\n```\n\n----------------------------------------\n\nTITLE: Interact with PointNav Env - Python\nDESCRIPTION: This snippet demonstrates how to interact with the PointNav environment. It resets the environment, displays the initial observation, and then enters a loop to execute actions. The loop continues until the 'stop' action is taken. The code checks if interactive control is enabled, and if so, prompts the user for an action. Otherwise it pops an action from a hardcoded list. It displays the agent's progress, current distance and angle to goal and then closes the environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naction = None\n    obs = env.reset()\n    valid_actions = [\"turn_left\", \"turn_right\", \"move_forward\", \"stop\"]\n    interactive_control = False  # @param {type:\"boolean\"}\n    while action != \"stop\":\n        display_sample(obs[\"rgb\"])\n        print(\n            \"distance to goal: {:.2f}\".format(\n                obs[\"pointgoal_with_gps_compass\"][0]\n            )\n        )\n        print(\n            \"angle to goal (radians): {:.2f}\".format(\n                obs[\"pointgoal_with_gps_compass\"][1]\n            )\n        )\n        if interactive_control:\n            action = input(\n                \"enter action out of {}:\\n\".format(\", \".join(valid_actions))\n            )\n            assert (\n                action in valid_actions\n            ), \"invalid action {} entered, choose one amongst \" + \",\".join(\n                valid_actions\n            )\n        else:\n            action = valid_actions.pop()\n        obs = env.step(\n            {\n                \"action\": action,\n            }\n        )\n\n    env.close()\n```\n\n----------------------------------------\n\nTITLE: Interact with Habitat Environment (Python)\nDESCRIPTION: This Python code snippet demonstrates how to interact with the Habitat environment. It initializes the environment using the configuration file, resets the environment, takes random actions, retrieves observations and metrics, and saves a video of the agent's interaction.  Requires `habitat`, `imageio`, and `visualizations` library.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nwith habitat.Env(\n    config=insert_render_options(habitat.get_config(nav_pick_cfg_path))\n) as env:\n    env.reset()\n\n    print(\"Agent acting inside environment.\")\n    count_steps = 0\n    # To save the video\n    video_file_path = os.path.join(output_path, \"example_interact.mp4\")\n    video_writer = imageio.get_writer(video_file_path, fps=30)\n\n    while not env.episode_over:\n        action = env.action_space.sample()\n        observations = env.step(action)  # noqa: F841\n        info = env.get_metrics()\n\n        render_obs = observations_to_image(observations, info)\n        render_obs = overlay_frame(render_obs, info)\n\n        video_writer.append_data(render_obs)\n\n        count_steps += 1\n    print(\"Episode finished after {} steps.\".format(count_steps))\n\n    video_writer.close()\n    if vut.is_notebook():\n        vut.display_video(video_file_path)\n```\n\n----------------------------------------\n\nTITLE: Defining Environment Configuration\nDESCRIPTION: This snippet defines functions to create the simulator and Habitat configurations. The `make_sim_cfg` function creates a simulator configuration, setting properties like HBAO, physics, scene, and object paths. The `make_hab_cfg` function creates the Habitat configuration, integrating simulator and task configurations. The `init_rearrange_env` function creates and returns a Habitat environment using the defined configurations.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom habitat.config.default_structured_configs import TaskConfig, EnvironmentConfig, DatasetConfig, HabitatConfig\nfrom habitat.config.default_structured_configs import ArmActionConfig, BaseVelocityActionConfig, OracleNavActionConfig, ActionConfig\nfrom habitat.core.env import Env\ndef make_sim_cfg(agent_dict):\n    # Start the scene config\n    sim_cfg = SimulatorConfig(type=\"RearrangeSim-v0\")\n    \n    # Enable Horizon Based Ambient Occlusion (HBAO) to approximate shadows.\n    sim_cfg.habitat_sim_v0.enable_hbao = True\n    \n    sim_cfg.habitat_sim_v0.enable_physics = True\n\n    \n    # Set up an example scene\n    sim_cfg.scene = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/scenes/103997919_171031233.scene_instance.json\")\n    sim_cfg.scene_dataset = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/hab3-hssd.scene_dataset_config.json\")\n    sim_cfg.additional_object_paths = [os.path.join(data_path, 'objects/ycb/configs/')]\n\n    \n    cfg = OmegaConf.create(sim_cfg)\n\n    # Set the scene agents\n    cfg.agents = agent_dict\n    cfg.agents_order = list(cfg.agents.keys())\n    return cfg\n\ndef make_hab_cfg(agent_dict, action_dict):\n    sim_cfg = make_sim_cfg(agent_dict)\n    task_cfg = TaskConfig(type=\"RearrangeEmptyTask-v0\")\n    task_cfg.actions = action_dict\n    env_cfg = EnvironmentConfig()\n    dataset_cfg = DatasetConfig(type=\"RearrangeDataset-v0\", data_path=\"data/hab3_bench_assets/episode_datasets/small_large.json.gz\")\n    \n    \n    hab_cfg = HabitatConfig()\nhab_cfg.environment = env_cfg\n    hab_cfg.task = task_cfg\n    hab_cfg.dataset = dataset_cfg\n    hab_cfg.simulator = sim_cfg\n    hab_cfg.simulator.seed = hab_cfg.seed\n\n    return hab_cfg\n\ndef init_rearrange_env(agent_dict, action_dict):\n    hab_cfg = make_hab_cfg(agent_dict, action_dict)\n    res_cfg = OmegaConf.create(hab_cfg)\n    return Env(res_cfg)\n```\n\n----------------------------------------\n\nTITLE: RL Training with DD-PPO (Shell)\nDESCRIPTION: This shell command starts training a pick skill policy using the DD-PPO implementation in Habitat Baselines. It uses the `rearrange/rl_skill.yaml` configuration file for training the Pick skill.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython -u -m habitat_baselines.run \\\n        --config-name=rearrange/rl_skill.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating Pose Sequence with Humanoid Controller in Python\nDESCRIPTION: This snippet demonstrates how to generate a sequence of poses using a humanoid controller. It iterates through the desired number of poses, calculating the next pose, retrieving it using `get_pose()`, and then applying it to the environment. The resulting observations are then used to create a video.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nobservations = []\nfor _ in range(humanoid_controller.humanoid_motion.num_poses):\n    # These computes the current pose and calculates the next pose\n    humanoid_controller.calculate_pose()\n    humanoid_controller.next_pose()\n    \n    # The get_pose function gives as a humanoid pose in the same format as HumanoidJointAction\n    new_pose = humanoid_controller.get_pose()\n    action_dict = {\n        \"action\": \"humanoid_joint_action\",\n        \"action_args\": {\"human_joints_trans\": new_pose}\n    }\n    observations.append(env.step(action_dict))\n    \nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading an Episode from a Dataset\nDESCRIPTION: This snippet loads an episode from a pre-generated dataset in JSON format, using gzip to decompress the file. It then initializes the RearrangeSim simulator and configures it with the loaded episode data, resetting the simulation and setting the agent's base to a fixed position. This allows the agent to be reconfigured with a pre-defined scene and object placement.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom habitat.datasets.rearrange.rearrange_dataset import RearrangeEpisode\nimport gzip\nimport json\n\n# Define the agent configuration\nepisode_file = os.path.join(data_path, \"hab3_bench_assets/episode_datasets/small_large.json.gz\")\nsim = init_rearrange_sim(agent_dict)\n# Load the dataset\nwith gzip.open(episode_file, \"rt\") as f: \n    episode_files = json.loads(f.read())\n\n# Get the first episode\nepisode = episode_files[\"episodes\"][0]\nrearrange_episode = RearrangeEpisode(**episode)\n\nart_agent = sim.articulated_agent\nart_agent._fixed_base = True\nsim.agents_mgr.on_new_scene()\n\n\nsim.reconfigure(sim.habitat_config, ep_info=rearrange_episode)\nsim.reset()\n\nart_agent.sim_obj.motion_type = MotionType.KINEMATIC\nsim.articulated_agent.base_pos =  init_pos \n_ = sim.step({})\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Pick Object Action\nDESCRIPTION: This code defines a custom action called `PickObjIdAction` for picking up objects by ID within the Habitat environment. It inherits from `ArticulatedAgentAction` and uses the Habitat registry to register the new action. The action space is defined as a discrete space of possible object IDs, and the `step` function implements the logic to snap the agent's grasp to the specified object. Dependencies: habitat.tasks.rearrange, habitat.core, gym.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom habitat.tasks.rearrange.actions.articulated_agent_action import ArticulatedAgentAction\nfrom habitat.core.registry import registry\nfrom gym import spaces\n\n\n@registry.register_task_action\nclass PickObjIdAction(ArticulatedAgentAction):\n    \n    @property\n    def action_space(self):\n        MAX_OBJ_ID = 1000\n        return spaces.Dict({\n            f\"{self._action_arg_prefix}pick_obj_id\": spaces.Discrete(MAX_OBJ_ID)\n        })\n\n    def step(self, *args, **kwargs):\n        obj_id = kwargs[f\"{self._action_arg_prefix}pick_obj_id\"]\n        print(self.cur_grasp_mgr, obj_id)\n        self.cur_grasp_mgr.snap_to_obj(obj_id)\n\naction_dict = {\n    \"pick_obj_id_action\": ActionConfig(type=\"PickObjIdAction\"),\n    \"base_velocity_action\": BaseVelocityActionConfig(),\n    \"oracle_coord_action\": OracleNavActionConfig(type=\"OracleNavCoordinateAction\", spawn_max_dist_to_obj=1.0)\n}\nenv = init_rearrange_env(agent_dict, action_dict)\n```\n\n----------------------------------------\n\nTITLE: Define New Structured Config (Python)\nDESCRIPTION: This snippet shows how to define a new custom Structured Config in Habitat using Python, dataclasses, and Hydra's ConfigStore. It defines a `CustomStructuredConfig` class and registers it with the ConfigStore under the group `habitat/custom_structured_config` and name `custom_structured_config`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom hydra.core.config_store import ConfigStore\n\n@dataclass\nclass CustomStructuredConfig:\n    custom_config_key: KeyType = DefaultValue\n\ncs = ConfigStore.instance()\ncs.store(\n    group=\"habitat/custom_structured_config\",  # config group\n    name=\"custom_structured_config\",           # config name\n    node=CustomStructuredConfig,\n    # Note, it is also possible to override the package (that's derived from the Config Group by default)\n    # package=\"habitat.new.custom_structured_config\",\n)\n```\n\n----------------------------------------\n\nTITLE: Teleporting the Agent Near an Object\nDESCRIPTION: This snippet resets the simulation, retrieves the first object's ID, and gets its translation. It then samples a navigable point near the object and teleports the agent to that location. This allows the agent to be placed in a position where it can interact with a specific object in the scene.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsim.reset()\nart_agent.sim_obj.motion_type = MotionType.KINEMATIC\nobj_id = sim.scene_obj_ids[0]\nfirst_object = rom.get_object_by_id(obj_id)\n\nobject_trans = first_object.translation\nprint(first_object.handle, \"is in\", object_trans)\n\nsample = sim.pathfinder.get_random_navigable_point_near(\n    circle_center=object_trans, radius=1.0, island_index=-1\n)\nvec_sample_obj = object_trans - sample\n\nangle_sample_obj = np.arctan2(-vec_sample_obj[2], vec_sample_obj[0])\n\nsim.articulated_agent.base_pos = sample\nsim.articulated_agent.base_rot = angle_sample_obj\nobs = sim.step({})\n\nplt.imshow(obs[\"head_rgb\"])\n```\n\n----------------------------------------\n\nTITLE: Gym Environment Creation from Config - Python\nDESCRIPTION: This Python code snippet shows how to create a Habitat environment using a configuration file and overrides. The `habitat.gym.make_gym_from_config` method allows for customizing environment parameters.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconfig = habitat.get_config(\n  \"benchmark/rearrange/skills/pick.yaml\",\n  overrides=[\"habitat.environment.max_episode_steps=20\"]\n)\nenv = habitat.gym.make_gym_from_config(config)\n```\n\n----------------------------------------\n\nTITLE: Defining Humanoid Actions in Habitat using Python\nDESCRIPTION: This snippet defines a set of actions for controlling a humanoid agent within the Habitat environment.  The actions include joint control, navigation, and object picking, each configured with specific settings. It then initializes the rearrange environment with these defined actions.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Define the actions\n\n\naction_dict = {\n    \"humanoid_joint_action\": HumanoidJointActionConfig(),\n    \"humanoid_navigate_action\": OracleNavActionConfig(type=\"OracleNavCoordinateAction\", \n                                                      motion_control=\"human_joints\",\n                                                      spawn_max_dist_to_obj=1.0),\n    \"humanoid_pick_obj_id_action\": HumanoidPickActionConfig(type=\"HumanoidPickObjIdAction\")\n    \n}\nenv = init_rearrange_env(agent_dict, action_dict)\n```\n\n----------------------------------------\n\nTITLE: Running PointNav Task in Habitat\nDESCRIPTION: This Python code sets up and runs a PointNav task in the Habitat environment. The agent navigates from a source location to a target location based on user input using keyboard controls. It relies on Habitat-Sim and Habitat Lab installations and scene data.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/quickstart.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport habitat\nfrom habitat.sims.habitat_simulator.actions import HabitatSimActions\nimport cv2\n\n\nFORWARD_KEY=\"w\"\nLEFT_KEY=\"a\"\nRIGHT_KEY=\"d\"\nFINISH=\"f\"\n\n\ndef transform_rgb_bgr(image):\n    return image[:, :, [2, 1, 0]]\n\n\ndef example():\n    env = habitat.Env(\n        config=habitat.get_config(\"benchmark/nav/pointnav/pointnav_habitat_test.yaml\")\n    )\n\n    print(\"Environment creation successful\")\n    observations = env.reset()\n    print(\"Destination, distance: {:3f}, theta(radians): {:.2f}\".format(\n        observations[\"pointgoal_with_gps_compass\"][0],\n        observations[\"pointgoal_with_gps_compass\"][1]))\n    cv2.imshow(\"RGB\", transform_rgb_bgr(observations[\"rgb\"]))\n\n    print(\"Agent stepping around inside environment.\")\n\n    count_steps = 0\n    while not env.episode_over:\n        keystroke = cv2.waitKey(0)\n\n        if keystroke == ord(FORWARD_KEY):\n            action = HabitatSimActions.move_forward\n            print(\"action: FORWARD\")\n        elif keystroke == ord(LEFT_KEY):\n            action = HabitatSimActions.turn_left\n            print(\"action: LEFT\")\n        elif keystroke == ord(RIGHT_KEY):\n            action = HabitatSimActions.turn_right\n            print(\"action: RIGHT\")\n        elif keystroke == ord(FINISH):\n            action = HabitatSimActions.stop\n            print(\"action: FINISH\")\n        else:\n            print(\"INVALID KEY\")\n            continue\n\n        observations = env.step(action)\n        count_steps += 1\n\n        print(\"Destination, distance: {:3f}, theta(radians): {:.2f}\".format(\n            observations[\"pointgoal_with_gps_compass\"][0],\n            observations[\"pointgoal_with_gps_compass\"][1]))\n        cv2.imshow(\"RGB\", transform_rgb_bgr(observations[\"rgb\"]))\n\n    print(\"Episode finished after {} steps.\".format(count_steps))\n\n    if (\n        action == HabitatSimActions.stop\n        and observations[\"pointgoal_with_gps_compass\"][0] < 0.2\n    ):\n        print(\"you successfully navigated to destination point\")\n    else:\n        print(\"your navigation was unsuccessful\")\n\n\nif __name__ == \"__main__\":\n    example()\n```\n\n----------------------------------------\n\nTITLE: Navigating to a Specific Object\nDESCRIPTION: This snippet demonstrates how to navigate the agent to a specific object using the `OracleNavCoordAction`. It resets the environment, retrieves an object's location, calculates the distance between the agent and object, and then uses the navigation action to move the agent towards the object.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nenv.reset()\nrom = env.sim.get_rigid_object_manager()\n# env.sim.articulated_agent.base_pos = init_pos\n# As before, we get a navigation point next to an object id\n\nobj_id = env.sim.scene_obj_ids[0]\nfirst_object = rom.get_object_by_id(obj_id)\n\nobject_trans = first_object.translation\nprint(first_object.handle, \"is in\", object_trans)\n\n# print(sample)\nobservations = []\ndelta = 2.0\n\nobject_agent_vec = env.sim.articulated_agent.base_pos - object_trans\nobject_agent_vec.y = 0\ndist_agent_object = object_agent_vec.length()\n```\n\n----------------------------------------\n\nTITLE: Importing necessary libraries for controller\nDESCRIPTION: This snippet imports required libraries for defining a custom robot controller.  It includes `typing` for type hints, `numpy` for numerical computations, `torch` for tensor operations, `torchcontrol` for control modules, and `polymetis` for the RobotInterface.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/polymetis_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nimport torchcontrol as toco\nfrom polymetis import RobotInterface\n```\n\n----------------------------------------\n\nTITLE: Adding Actions to a Habitat Task\nDESCRIPTION: This snippet shows how to add discrete navigation actions to a Habitat task configuration. By adding action names (e.g., `move_forward`, `turn_left`) to the `defaults` list under the `/habitat/task/actions` path, you can specify which actions are available for the agent to perform. This defines the agent's action space for interacting with the environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/CONFIG_KEYS.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\ndefaults:\n  - /habitat/task/actions:\n    - move_forward\n    - turn_left\n```\n\n----------------------------------------\n\nTITLE: Picking Object by ID and Making Video\nDESCRIPTION: This code segment demonstrates picking an object in the Habitat environment. It initializes the environment, navigates the agent close to the object, and then calls the defined \"pick_obj_id_action\". Finally, it calls `vut.make_video` to generate a video of the episode. Dependencies: habitat environment, visual_utils.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nenv.reset()\nrom = env.sim.get_rigid_object_manager()\n# env.sim.articulated_agent.base_pos = init_pos\n# As before, we get a navigation point next to an object id\n\nobj_id = env.sim.scene_obj_ids[0]\nfirst_object = rom.get_object_by_id(obj_id)\n\nobject_trans = first_object.translation\nprint(first_object.handle, \"is in\", object_trans)\n\nobservations = []\ndelta = 2.0\n\nobject_agent_vec = env.sim.articulated_agent.base_pos - object_trans\nobject_agent_vec.y = 0\ndist_agent_object = object_agent_vec.length()\n# Walk towards the object\n\nagent_displ = np.inf\nagent_rot = np.inf\nprev_rot = env.sim.articulated_agent.base_rot\nprev_pos = env.sim.articulated_agent.base_pos\nwhile agent_displ > 1e-9 or agent_rot > 1e-9:\n    prev_rot = env.sim.articulated_agent.base_rot\n    prev_pos = env.sim.articulated_agent.base_pos\n    action_dict = {\n        \"action\": (\"oracle_coord_action\"), \n        \"action_args\": {\n              \"oracle_nav_lookat_action\": object_trans,\n              \"mode\": 1\n          }\n    }\n    observations.append(env.step(action_dict))\n    \n    cur_rot = env.sim.articulated_agent.base_rot\n    cur_pos = env.sim.articulated_agent.base_pos\n    agent_displ = (cur_pos - prev_pos).length()\n    agent_rot = np.abs(cur_rot - prev_rot)\n    # print(agent_rot, agent_displ)\n\nfor _ in range(20):\n    action_dict = {\"action\": (), \"action_args\": {}}\n    observations.append(env.step(action_dict))    \n\naction_dict = {\"action\": (\"pick_obj_id_action\"), \"action_args\": {\"pick_obj_id\": obj_id}}\nobservations.append(env.step(action_dict))\nfor _ in range(100):\n    action_dict = {\"action\": (), \"action_args\": {}}\n    observations.append(env.step(action_dict))    \nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Habitat Simulation Environment\nDESCRIPTION: This code defines functions to configure the Habitat simulation environment. `make_sim_cfg` sets up the simulator configuration, including the scene and physics settings. `make_hab_cfg` configures the overall Habitat configuration, including the environment, task, dataset, and simulator settings. `init_rearrange_env` creates the Habitat environment using the configured settings.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom habitat.config.default_structured_configs import TaskConfig, EnvironmentConfig, DatasetConfig, HabitatConfig\nfrom habitat.config.default_structured_configs import ArmActionConfig, BaseVelocityActionConfig, OracleNavActionConfig\nfrom habitat.core.env import Env\ndef make_sim_cfg(agent_dict):\n    # Start the scene config\n    sim_cfg = SimulatorConfig(type=\"RearrangeSim-v0\")\n    \n    # This is for better graphics\n    sim_cfg.habitat_sim_v0.enable_hbao = True\n    sim_cfg.habitat_sim_v0.enable_physics = True\n\n    \n    # Set up an example scene\n    sim_cfg.scene = \"data/hab3_bench_assets/hab3-hssd/scenes/103997919_171031233.scene_instance.json\"\n    sim_cfg.scene_dataset = \"data/hab3_bench_assets/hab3-hssd/hab3-hssd.scene_dataset_config.json\"\n    sim_cfg.additional_object_paths = ['data/objects/ycb/configs/']\n\n    \n    cfg = OmegaConf.create(sim_cfg)\n\n    # Set the scene agents\n    cfg.agents = agent_dict\n    cfg.agents_order = list(cfg.agents.keys())\n    return cfg\n\ndef make_hab_cfg(agent_dict, action_dict):\n    sim_cfg = make_sim_cfg(agent_dict)\n    task_cfg = TaskConfig(type=\"RearrangeEmptyTask-v0\")\n    task_cfg.actions = action_dict\n    env_cfg = EnvironmentConfig()\n    dataset_cfg = DatasetConfig(type=\"RearrangeDataset-v0\", data_path=\"data/hab3_bench_assets/episode_datasets/small_large.json.gz\")\n    \n    \n    hab_cfg = HabitatConfig()\n    hab_cfg.environment = env_cfg\n    hab_cfg.task = task_cfg\n    hab_cfg.dataset = dataset_cfg\n    hab_cfg.simulator = sim_cfg\n    hab_cfg.simulator.seed = hab_cfg.seed\n\n    return hab_cfg\n\ndef init_rearrange_env(agent_dict, action_dict):\n    hab_cfg = make_hab_cfg(agent_dict, action_dict)\n    res_cfg = OmegaConf.create(hab_cfg)\n    return Env(res_cfg)\n```\n\n----------------------------------------\n\nTITLE: Overriding Environment Configuration (Python)\nDESCRIPTION: This snippet demonstrates how to override the default configuration of a Habitat environment using the `override_options` argument in `gym.make`. It modifies the `HabitatPick-v0` environment to use the `SuctionGraspAction` for the gripper. It then prints the action space of the modified environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/habitat2_gym_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenv = gym.make(\n    \"HabitatPick-v0\",\n    override_options=[\n        \"habitat.task.actions.arm_action.grip_controller=SuctionGraspAction\",\n    ],\n)\nprint(\"Action space with suction grip\", env.action_space)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Initializing the Habitat Environment\nDESCRIPTION: This snippet defines the humanoid_joint_action, initializes the Habitat environment with a humanoid agent and defined actions using the `init_rearrange_env` function, effectively loading the scene and preparing the simulation for interaction.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define the actions\n\naction_dict = {\n    \"humanoid_joint_action\": HumanoidJointActionConfig()\n}\nenv = init_rearrange_env(agent_dict, action_dict)\n```\n\n----------------------------------------\n\nTITLE: Installing Habitat Baselines\nDESCRIPTION: This command installs the habitat-lab package along with the habitat-baselines sub-package, including additional requirements specified in the requirements.txt files within the sub-module directories. It uses pip in editable mode (-e) to install directly from the source code.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e habitat-lab\npip install -e habitat-baselines\n```\n\n----------------------------------------\n\nTITLE: Walking to a Direction with Humanoid Controller in Python\nDESCRIPTION: This snippet demonstrates how to use the `HumanoidRearrangeController` to make the humanoid walk to a specified direction. It involves resetting the environment and the controller, calculating the desired pose using `calculate_walk_pose()`, and applying it to the environment using the humanoid joint action.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# We reset the controller\nenv.reset()\nhumanoid_controller.reset(env.sim.articulated_agent.base_transformation)\nobservations = []\nprint(env.sim.articulated_agent.base_pos)\nfor _ in range(100):\n    # This computes a pose that moves the agent to relative_position\n    relative_position = env.sim.articulated_agent.base_pos + mn.Vector3(0,0,1)\n    humanoid_controller.calculate_walk_pose(relative_position)\n    \n    # The get_pose function gives as a humanoid pose in the same format as HumanoidJointAction\n    new_pose = humanoid_controller.get_pose()\n    action_dict = {\n        \"action\": \"humanoid_joint_action\",\n        \"action_args\": {\"human_joints_trans\": new_pose}\n    }\n    observations.append(env.step(action_dict))\n    \nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Observation Display Utility - Python\nDESCRIPTION: This snippet defines a function `display_sample` that visualizes sensor observations (RGB, semantic, depth) using matplotlib. It takes numpy arrays representing the observations and displays them as images in a single figure. The function uses PIL to convert the arrays into images and matplotlib to display them. This is useful for debugging and understanding the agent's sensory input.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef display_sample(\n    rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])\n):  # noqa: B006\n    from habitat_sim.utils.common import d3_40_colors_rgb\n\n    rgb_img = Image.fromarray(rgb_obs, mode=\"RGB\")\n\n    arr = [rgb_img]\n    titles = [\"rgb\"]\n    if semantic_obs.size != 0:\n        semantic_img = Image.new(\n            \"P\", (semantic_obs.shape[1], semantic_obs.shape[0])\n        )\n        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n        semantic_img = semantic_img.convert(\"RGBA\")\n        arr.append(semantic_img)\n        titles.append(\"semantic\")\n\n    if depth_obs.size != 0:\n        depth_img = Image.fromarray(\n            (depth_obs / 10 * 255).astype(np.uint8), mode=\"L\"\n        )\n        arr.append(depth_img)\n        titles.append(\"depth\")\n\n    plt.figure(figsize=(12, 8))\n    for i, data in enumerate(arr):\n        ax = plt.subplot(1, 3, i + 1)\n        ax.axis(\"off\")\n        ax.set_title(titles[i])\n        plt.imshow(data)\n    plt.show(block=False)\n```\n\n----------------------------------------\n\nTITLE: Social Rearrangement Training (Habitat)\nDESCRIPTION: These python commands initiate training for the social rearrangement task in Habitat using the specified configuration files. The first command trains a single policy, the second trains with a population of 8 humanoid policies, and the third trains a planner-based policy. The number of pool agents or the planner index can be adjusted using command-line arguments.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython habitat_baselines/run.py --config-name=social_rearrange/pop_play.yaml\n```\n\nLANGUAGE: python\nCODE:\n```\npython habitat_baselines/run.py --config-name=social_rearrange/pop_play.yaml habitat_baselines.rl.agent.num_pool_agents_per_type=[1,8]\n```\n\nLANGUAGE: python\nCODE:\n```\npython habitat_baselines/run.py --config-name=social_rearrange/plan_pop.yaml habitat_baselines.rl.policy.agent_1.hierarchical_policy.high_level_policy.plan_idx=4\n```\n\n----------------------------------------\n\nTITLE: Dataset Processing Script Execution\nDESCRIPTION: This command executes the dataset processing script to transform Habitat's 3D models for use in Unity.  It simplifies the models to improve performance on VR devices.  It requires paths to the HSSD-HAB dataset and HSSD-models.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/pick_throw_vr/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ./scripts/unity_dataset_processing/unity_dataset_processing.py \\\n--hssd-hab-root-dir data/scene_datasets/hssd-hab \\\n--hssd-models-root-dir path_to/hssd-models/objects \\\n--scenes 105515448_173104512\n```\n\n----------------------------------------\n\nTITLE: Training PPO Agent with VER Trainer\nDESCRIPTION: This command trains a PPO agent using the VER trainer, specified via command line argument `habitat_baselines.trainer_name=ver`. It uses the ppo_pointnav_example.yaml configuration file. The `-u` flag ensures stdout is flushed after each write operation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=pointnav/ppo_pointnav_example.yaml \\\n  habitat_baselines.trainer_name=ver\n```\n\n----------------------------------------\n\nTITLE: Running the Minimal HITL Example in Habitat\nDESCRIPTION: This command executes the `minimal.py` script to launch the minimal HITL application within the Habitat-Lab environment.  It assumes the current working directory is the root `habitat-lab` directory.  The script loads and steps through the Habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/minimal/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/minimal/minimal.py'\n```\n\n----------------------------------------\n\nTITLE: Querying Action Space\nDESCRIPTION: This snippet iterates through the available actions in the environment's action space and prints the name and corresponding action space for each action. This allows the user to see which actions are available and how they can be parameterized.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# We can query the actions available, and their action space:\nfor action_name, action_space in env.action_space.items():\n    print(action_name, action_space)\n```\n\n----------------------------------------\n\nTITLE: Defining a NavPickTask\nDESCRIPTION: This code snippet defines a custom task, `NavPickTaskV1`, inheriting from `RearrangeTask`. It implements the `reset` function to randomly select a target object and place the robot at a random navigable point. This function is called at the beginning of each episode.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@registry.register_task(name=\"RearrangeDemoNavPickTask-v0\")\nclass NavPickTaskV1(RearrangeTask):\n    \"\"\"\n    Primarily this is used to implement the episode reset functionality.\n    Can also implement custom episode step functionality.\n    \"\"\"\n\n    def reset(self, episode):\n        self.target_object_index = np.random.randint(\n            0, self._sim.get_n_targets()\n        )\n        start_pos = self._sim.pathfinder.get_random_navigable_point()\n        self._sim.articulated_agent.base_pos = start_pos\n\n        # Put any reset logic here.\n        return super().reset(episode)\n```\n\n----------------------------------------\n\nTITLE: Including a Visual Agent in Habitat Configuration\nDESCRIPTION: This snippet demonstrates how to include a pre-defined visual agent in your Habitat configuration by adding it to the `defaults` list. The `habitat.simulator.agents` path specifies the location of the agent definitions, and `<the key of the agent>` should be replaced with the specific agent you want to use (e.g., `depth_head_agent`, `rgb_head_agent`). This allows you to easily configure the visual observation space for your agent.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/CONFIG_KEYS.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\ndefaults:\n  - /habitat/simulator/agents@habitat.simulator.agents.main_agent: <the key of the agent>\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Evaluation for Social Navigation (Habitat)\nDESCRIPTION: This bash command evaluates a trained Spot robot policy in the Habitat environment for social navigation, running 500 episodes. It sets up the environment parameters, including the configuration file, evaluation flag, number of environments, and checkpoint paths. It also configures specific action parameters for both agents and sensor parameters for the humanoid detector.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n    --config-name=social_nav/social_nav.yaml \\\n    benchmark/multi_agent=hssd_spot_human_social_nav \\\n    habitat_baselines.evaluate=True \\\n    habitat_baselines.num_checkpoints=5000 \\\n    habitat_baselines.total_num_steps=1.0e9 \\\n    habitat_baselines.num_environments=12 \\\n    habitat_baselines.video_dir=video_social_nav \\\n    habitat_baselines.checkpoint_folder=checkpoints_social_nav \\\n    habitat_baselines.eval_ckpt_path_dir=checkpoints_social_nav/social_nav_latest.pth \\\n    habitat.task.actions.agent_0_base_velocity.longitudinal_lin_speed=10.0 \\\n    habitat.task.actions.agent_0_base_velocity.ang_speed=10.0 \\\n    habitat.task.actions.agent_0_base_velocity.allow_dyn_slide=True \\\n    habitat.task.actions.agent_0_base_velocity.enable_rotation_check_for_dyn_slide=False \\\n    habitat.task.actions.agent_1_oracle_nav_randcoord_action.human_stop_and_walk_to_robot_distance_threshold=-1.0 \\\n    habitat.task.actions.agent_1_oracle_nav_randcoord_action.lin_speed=10.0 \\\n    habitat.task.actions.agent_1_oracle_nav_randcoord_action.ang_speed=10.0 \\\n    habitat.task.actions.agent_1_oracle_nav_action.lin_speed=10.0 \\\n    habitat.task.actions.agent_1_oracle_nav_action.ang_speed=10.0 \\\n    habitat.task.measurements.social_nav_reward.facing_human_reward=3.0 \\\n    habitat.task.measurements.social_nav_reward.count_coll_pen=0.01 \\\n    habitat.task.measurements.social_nav_reward.max_count_colls=-1 \\\n    habitat.task.measurements.social_nav_reward.count_coll_end_pen=5 \\\n    habitat.task.measurements.social_nav_reward.use_geo_distance=True \\\n    habitat.task.measurements.social_nav_reward.facing_human_dis=3.0 \\\n    habitat.task.measurements.social_nav_seek_success.following_step_succ_threshold=400 \\\n    habitat.task.measurements.social_nav_seek_success.need_to_face_human=True \\\n    habitat.task.measurements.social_nav_seek_success.use_geo_distance=True \\\n    habitat.task.measurements.social_nav_seek_success.facing_threshold=0.5 \\\n    habitat.task.lab_sensors.humanoid_detector_sensor.return_image=True \\\n    habitat.task.lab_sensors.humanoid_detector_sensor.is_return_image_bbox=True \\\n    habitat.task.success_reward=10.0 \\\n    habitat.task.end_on_success=False \\\n    habitat.task.slack_reward=-0.1 \\\n    habitat.environment.max_episode_steps=1500 \\\n    habitat.simulator.kinematic_mode=True \\\n    habitat.simulator.ac_freq_ratio=4 \\\n    habitat.simulator.ctrl_freq=120 \\\n    habitat.simulator.agents.agent_0.joint_start_noise=0.0 \\\n    habitat_baselines.load_resume_state_config=False \\\n    habitat_baselines.test_episode_count=500 \\\n    habitat_baselines.eval.extra_sim_sensors.third_rgb_sensor.height=1080 \\\n    habitat_baselines.eval.extra_sim_sensors.third_rgb_sensor.width=1920\n```\n\n----------------------------------------\n\nTITLE: Minimal HITL Application in Python\nDESCRIPTION: This Python script implements a minimal Human-in-the-Loop (HITL) application using the Habitat framework. It defines an `AppStateMinimal` class that inherits from `AppState` and overrides the `sim_update` method to step the Habitat environment and set a fixed overhead camera. It also defines a `main` function decorated with `@hydra.main` to initialize the application using a Hydra configuration file.  Dependencies include hydra, magnum, and modules from habitat_hitl.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-hitl/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# minimal.py\n\nimport hydra\nimport magnum\n\nfrom habitat_hitl.app_states.app_state_abc import AppState\nfrom habitat_hitl.core.gui_input import GuiInput\nfrom habitat_hitl.core.hitl_main import hitl_main\nfrom habitat_hitl.core.hydra_utils import register_hydra_plugins\n\n\nclass AppStateMinimal(AppState):\n    \"\"\"\n    A minimal HITL app that loads and steps a Habitat environment, with\n    a fixed overhead camera.\n    \"\"\"\n\n    def __init__(self, app_service):\n        self._app_service = app_service\n\n    def sim_update(self, dt, post_sim_update_dict):\n        \"\"\"\n        The HITL framework calls sim_update continuously (for each\n        \"frame\"), before rendering the app's GUI window.\n        \"\"\"\n        # run the episode until it ends\n        if not self._app_service.env.episode_over:\n            self._app_service.compute_action_and_step_env()\n\n        # set the camera for the main 3D viewport\n        post_sim_update_dict[\"cam_transform\"] = magnum.Matrix4.look_at(\n            eye=magnum.Vector3(-20, 20, -20),\n            target=magnum.Vector3(0, 0, 0),\n            up=magnum.Vector3(0, 1, 0),\n        )\n\n        # exit when the ESC key is pressed\n        if self._app_service.gui_input.get_key_down(KeyCode.ESC):\n            post_sim_update_dict[\"application_exit\"] = True\n\n\n@hydra.main(version_base=None, config_path=\"./\", config_name=\"minimal_cfg\")\ndef main(config):\n    hitl_main(config, lambda app_service: AppStateMinimal(app_service))\n\n\nif __name__ == \"__main__\":\n    register_hydra_plugins()\n    main()\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Composite Files - YAML - YAML\nDESCRIPTION: This snippet shows how to specify composite files for the batch renderer in a YAML configuration file. It defines a list of file paths for the `habitat.simulator.renderer.composite_files` parameter.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/core/batch_rendering/README.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nhabitat.simulator.renderer.composite_files:\n    - path/to/composite_1.gltf\n    - path/to/composite_2.gltf\n```\n\n----------------------------------------\n\nTITLE: Topdown Map Measurement and Agent Integration\nDESCRIPTION: This snippet demonstrates using the `TopDownMap` measurement with an agent that follows the shortest path. It defines a `ShortestPathFollowerAgent` class and the `example_top_down_map_measure` function. The function configures the environment to include the `TopDownMap` and `Collisions` measurements, creates an agent, and generates a video of the agent navigating through an episode. It depends on `numpy`, `habitat`, and `habitat_sim` libraries.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab_TopdownMap_Visualization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ShortestPathFollowerAgent(Agent):\n    r\"\"\"Implementation of the :ref:`habitat.core.agent.Agent` interface that\n    uses :ref`habitat.tasks.nav.shortest_path_follower.ShortestPathFollower` utility class\n    for extracting the action on the shortest path to the goal.\n    \"\"\"\n\n    def __init__(self, env: habitat.Env, goal_radius: float):\n        self.env = env\n        self.shortest_path_follower = ShortestPathFollower(\n            sim=cast(\"HabitatSim\", env.sim),\n            goal_radius=goal_radius,\n            return_one_hot=False,\n        )\n\n    def act(self, observations: \"Observations\") -> Union[int, np.ndarray]:\n        return self.shortest_path_follower.get_next_action(\n            cast(NavigationEpisode, self.env.current_episode).goals[0].position\n        )\n\n    def reset(self) -> None:\n        pass\n\n\ndef example_top_down_map_measure():\n    # Create habitat config\n    config = habitat.get_config(\n        config_path=os.path.join(\n            dir_path,\n            \"habitat-lab/habitat/config/benchmark/nav/pointnav/pointnav_habitat_test.yaml\",\n        )\n    )\n    # Add habitat.tasks.nav.nav.TopDownMap and habitat.tasks.nav.nav.Collisions measures\n    with habitat.config.read_write(config):\n        config.habitat.task.measurements.update(\n            {\n                \"top_down_map\": TopDownMapMeasurementConfig(\n                    map_padding=3,\n                    map_resolution=1024,\n                    draw_source=True,\n                    draw_border=True,\n                    draw_shortest_path=True,\n                    draw_view_points=True,\n                    draw_goal_positions=True,\n                    draw_goal_aabbs=True,\n                    fog_of_war=FogOfWarConfig(\n                        draw=True,\n                        visibility_dist=5.0,\n                        fov=90,\n                    ),\n                ),\n                \"collisions\": CollisionsMeasurementConfig(),\n            }\n        )\n    # Create dataset\n    dataset = habitat.make_dataset(\n        id_dataset=config.habitat.dataset.type, config=config.habitat.dataset\n    )\n    # Create simulation environment\n    with habitat.Env(config=config, dataset=dataset) as env:\n        # Create ShortestPathFollowerAgent agent\n        agent = ShortestPathFollowerAgent(\n            env=env,\n            goal_radius=config.habitat.task.measurements.success.success_distance,\n        )\n        # Create video of agent navigating in the first episode\n        num_episodes = 1\n        for _ in range(num_episodes):\n            # Load the first episode and reset agent\n            observations = env.reset()\n            agent.reset()\n\n            # Get metrics\n            info = env.get_metrics()\n            # Concatenate RGB-D observation and topdowm map into one image\n            frame = observations_to_image(observations, info)\n\n            # Remove top_down_map from metrics\n            info.pop(\"top_down_map\")\n            # Overlay numeric metrics onto frame\n            frame = overlay_frame(frame, info)\n            # Add fame to vis_frames\n            vis_frames = [frame]\n\n            # Repeat the steps above while agent doesn't reach the goal\n            while not env.episode_over:\n                # Get the next best action\n                action = agent.act(observations)\n                if action is None:\n                    break\n\n                # Step in the environment\n                observations = env.step(action)\n                info = env.get_metrics()\n                frame = observations_to_image(observations, info)\n\n                info.pop(\"top_down_map\")\n                frame = overlay_frame(frame, info)\n                vis_frames.append(frame)\n\n            current_episode = env.current_episode\n            video_name = f\"{os.path.basename(current_episode.scene_id)}_{current_episode.episode_id}\"\n            # Create video from images and save to disk\n            images_to_video(\n                vis_frames, output_path, video_name, fps=6, quality=9\n            )\n            vis_frames.clear()\n            # Display video\n            vut.display_video(f\"{output_path}/{video_name}.mp4\")\n```\n\n----------------------------------------\n\nTITLE: Running HRL Episode with Oracle Skills\nDESCRIPTION: This command runs a rearrangement episode using oracle low-level skills and a fixed task planner. It sets the `evaluate` flag to `True`, uses the `hl_fixed` policy, and specifies `oracle_skills` as the defined skills within the hierarchical policy.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=rearrange/rl_hierarchical.yaml \\\n  habitat_baselines.evaluate=True \\\n  habitat_baselines/rl/policy=hl_fixed \\\n  habitat_baselines/rl/policy/hierarchical_policy/defined_skills=oracle_skills\n```\n\n----------------------------------------\n\nTITLE: Moving and Dropping the Object\nDESCRIPTION: This snippet moves the agent forward while holding the object, then releases the object by calling `desnap` on the grasp manager. It simulates moving the agent with the attached object and then dropping the object at a later point. The observations are saved and finally compiled into a video.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nnum_iter = 100\nobservations = []\n\nsim.articulated_agent.base_pos = sample\nfor _ in range(num_iter):    \n    forward_vec = art_agent.base_transformation.transform_vector(mn.Vector3(1,0,0))\n    art_agent.base_pos = art_agent.base_pos + forward_vec * 0.02\n    observations.append(sim.step({}))\n    \n# Remove the object\ngrasp_manager.desnap()\nfor _ in range(20):\n    observations.append(sim.step({}))\nvut.make_video(\n    observations,\n    \"head_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Navigation and Video\nDESCRIPTION: This code showcases navigation of two agents towards a single object in a multi-agent setup, followed by the creation of a video. It initializes the environment, gets the target object's location, and then iteratively moves both agents towards that location until they are close enough. Agent actions are prefixed with the agent name (e.g., 'agent_0_oracle_coord_action'). Dependencies: habitat environment, visual_utils.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nenv.reset()\nrom = env.sim.get_rigid_object_manager()\n# env.sim.articulated_agent.base_pos = init_pos\n# As before, we get a navigation point next to an object id\n\nobj_id = env.sim.scene_obj_ids[0]\nfirst_object = rom.get_object_by_id(obj_id)\n\nobject_trans = first_object.translation\nobservations = []\n\n# Walk towards the object\n\nagent_displ = np.inf\nagent_rot = np.inf\nprev_rot = env.sim.agents_mgr[0].articulated_agent.base_rot\nprev_pos = env.sim.agents_mgr[0].articulated_agent.base_pos\nwhile agent_displ > 1e-9 or agent_rot > 1e-9:\n    prev_rot = env.sim.agents_mgr[0].articulated_agent.base_rot\n    prev_pos = env.sim.agents_mgr[0].articulated_agent.base_pos\n    action_dict = {\n        \"action\": (\"agent_0_oracle_coord_action\", \"agent_1_oracle_coord_action\"), \n        \"action_args\": {\n              \"agent_0_oracle_nav_lookat_action\": object_trans,\n              \"agent_0_mode\": 1,\n              \"agent_1_oracle_nav_lookat_action\": object_trans,\n              \"agent_1_mode\": 1\n          }\n    }\n    observations.append(env.step(action_dict))\n    \n    cur_rot = env.sim.agents_mgr[0].articulated_agent.base_rot\n    cur_pos = env.sim.agents_mgr[0].articulated_agent.base_pos\n    agent_displ = (cur_pos - prev_pos).length()\n    agent_rot = np.abs(cur_rot - prev_rot)\n    # print(agent_rot, agent_displ)\nvut.make_video(\n    observations,\n    \"agent_1_third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring Paths\nDESCRIPTION: This snippet imports necessary libraries such as `os`, `git`, `gym`, `imageio`, `numpy`, `hydra`, `habitat`, and `PIL` for Habitat 2.0. It then configures the data and output paths based on the project's root directory, while also setting environment variables to reduce the simulator logging output.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Imports\nimport os\n\nimport git\nimport gym\nimport imageio\nimport numpy as np\nfrom hydra.core.config_store import ConfigStore\n\nimport habitat\nimport habitat.gym\nfrom habitat.core.embodied_task import Measure\nfrom habitat.core.registry import registry\nfrom habitat.tasks.rearrange.rearrange_sensors import RearrangeReward\nfrom habitat.tasks.rearrange.rearrange_task import RearrangeTask\nfrom habitat.utils.visualizations.utils import (\n    observations_to_image,\n    overlay_frame,\n)\nfrom habitat_sim.utils import viz_utils as vut\n\n# Quiet the Habitat simulator logging\nos.environ[\"MAGNUM_LOG\"] = \"quiet\"\nos.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n\n\ndef insert_render_options(config):\n    # Added settings to make rendering higher resolution for better visualization\n    with habitat.config.read_write(config):\n        config.habitat.simulator.concur_render = False\n        agent_config = get_agent_config(sim_config=config.habitat.simulator)\n        agent_config.sim_sensors.update(\n            {\"third_rgb_sensor\": ThirdRGBSensorConfig(height=512, width=512)}\n        )\n    return config\n\n\nimport importlib\n\n# If the import block fails due to an error like \"'PIL.TiffTags' has no attribute\n# 'IFD'\", then restart the Colab runtime instance and rerun this cell and the previous cell.\nimport PIL\n\nimportlib.reload(\n    PIL.TiffTags  # type: ignore[attr-defined]\n)  # To potentially avoid PIL problem\n\nrepo = git.Repo(\".\", search_parent_directories=True)\ndir_path = repo.working_tree_dir\ndata_path = os.path.join(dir_path, \"data\")\noutput_path = os.path.join(\n    dir_path, \"examples/tutorials/habitat_lab_visualization/\"\n)\nos.makedirs(output_path, exist_ok=True)\nos.chdir(dir_path)\n```\n\n----------------------------------------\n\nTITLE: PointNav Benchmark Output Config - YAML\nDESCRIPTION: This extensive YAML configuration provides a complete example of a PointNav benchmark configuration. It includes settings for the environment, simulator, task, and dataset, showcasing the hierarchical structure and available options for configuring a Habitat simulation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nhabitat:\n  seed: 100\n  env_task: GymHabitatEnv\n  env_task_gym_dependencies: []\n  env_task_gym_id: ''\n  environment:\n    max_episode_steps: 500\n    max_episode_seconds: 10000000\n    iterator_options:\n      cycle: true\n      shuffle: true\n      group_by_scene: true\n      num_episode_sample: -1\n      max_scene_repeat_episodes: -1\n      max_scene_repeat_steps: 10000\n      step_repetition_range: 0.2\n  simulator:\n    type: Sim-v0\n    forward_step_size: 0.25\n    turn_angle: 10\n    create_renderer: false\n    requires_textures: true\n    lag_observations: 0\n    auto_sleep: false\n    step_physics: true\n    concur_render: false\n    needs_markers: true\n    update_articulated_agent: true\n    scene: data/scene_datasets/habitat-test-scenes/van-gogh-room.glb\n    scene_dataset: default\n    additional_object_paths: []\n    seed: ${habitat.seed}\n    default_agent_id: 0\n    debug_render: false\n    debug_render_robot: false\n    kinematic_mode: false\n    debug_render_goal: true\n    robot_joint_start_noise: 0.0\n    ctrl_freq: 120.0\n    ac_freq_ratio: 4\n    load_objs: true\n    hold_thresh: 0.09\n    grasp_impulse: 1000.0\n    agents:\n      rgbd_agent:\n        height: 1.5\n        radius: 0.1\n        sim_sensors:\n          rgb_sensor:\n            type: HabitatSimRGBSensor\n            height: 256\n            width: 256\n            position:\n            - 0.0\n            - 1.25\n            - 0.0\n            orientation:\n            - 0.0\n            - 0.0\n            - 0.0\n            hfov: 90\n            sensor_subtype: PINHOLE\n            noise_model: None\n            noise_model_kwargs: {}\n          depth_sensor:\n            type: HabitatSimDepthSensor\n            height: 256\n            width: 256\n            position:\n            - 0.0\n            - 1.25\n            - 0.0\n            orientation:\n            - 0.0\n            - 0.0\n            - 0.0\n            hfov: 90\n            sensor_subtype: PINHOLE\n            noise_model: None\n            noise_model_kwargs: {}\n            min_depth: 0.0\n            max_depth: 10.0\n            normalize_depth: true\n        is_set_start_state: false\n        start_position:\n        - 0.0\n        - 0.0\n        - 0.0\n        start_rotation:\n        - 0.0\n        - 0.0\n        - 0.0\n        - 1.0\n        joint_start_noise: 0.0\n        articulated_agent_urdf: data/robots/hab_fetch/robots/hab_fetch.urdf\n        articulated_agent_type: FetchRobot\n        ik_arm_urdf: data/robots/hab_fetch/robots/fetch_onlyarm.urdf\n    agents_order:\n    - rgbd_agent\n    habitat_sim_v0:\n      gpu_device_id: 0\n      gpu_gpu: false\n      allow_sliding: true\n      frustum_culling: true\n      enable_physics: false\n      physics_config_file: ./data/default.physics_config.json\n      leave_context_with_background_renderer: false\n      enable_gfx_replay_save: false\n    ep_info: null\n  task:\n    reward_measure: distance_to_goal_reward\n    success_measure: spl\n    success_reward: 2.5\n    slack_reward: -0.01\n    end_on_success: true\n    type: Nav-v0\n    lab_sensors:\n      pointgoal_with_gps_compass_sensor:\n        type: PointGoalWithGPSCompassSensor\n        goal_format: POLAR\n        dimensionality: 2\n    measurements:\n      distance_to_goal:\n        type: DistanceToGoal\n        distance_to: POINT\n      success:\n        type: Success\n        success_distance: 0.2\n      spl:\n        type: SPL\n      distance_to_goal_reward:\n        type: DistanceToGoalReward\n    goal_sensor_uuid: pointgoal_with_gps_compass\n    count_obj_collisions: true\n    settle_steps: 5\n    constraint_violation_ends_episode: true\n    constraint_violation_drops_object: false\n    force_regenerate: false\n    should_save_to_cache: true\n    object_in_hand_sample_prob: 0.167\n    render_target: true\n    ee_sample_factor: 0.2\n    ee_exclude_region: 0.0\n    base_angle_noise: 0.15\n    base_noise: 0.05\n    spawn_region_scale: 0.2\n    joint_max_impulse: -1.0\n    desired_resting_position:\n    - 0.5\n    - 0.0\n    - 1.0\n    use_marker_t: true\n    cache_robot_init: false\n    success_state: 0.0\n    easy_init: false\n    should_enforce_target_within_reach: false\n    task_spec_base_path: habitat/task/rearrange/pddl/\n    task_spec: ''\n    pddl_domain_def: replica_cad\n    obj_succ_thresh: 0.3\n    art_succ_thresh: 0.15\n    robot_at_thresh: 2.0\n    actions:\n      stop:\n        type: StopAction\n      move_forward:\n        type: MoveForwardAction\n      turn_left:\n        type: TurnLeftAction\n        turn_angle: 10\n      turn_right:\n        type: TurnRightAction\n        turn_angle: 10\n  dataset:\n    type: PointNav-v1\n    split: train\n    scenes_dir: data/scene_datasets\n    content_scenes:\n    - '*'\n    data_path: data/datasets/pointnav/gibson/v1/{split}/{split}.json.gz\n  gym:\n    obs_keys: null\n    action_keys: null\n    achieved_goal_keys: []\n    desired_goal_keys: []\n```\n\n----------------------------------------\n\nTITLE: Multi-Agent Training for Social Navigation (Habitat)\nDESCRIPTION: This bash command initiates multi-agent training for social navigation in the Habitat environment. It configures a Spot robot with a low-level navigation policy and a humanoid with a fixed policy. Key parameters include the number of environments, total steps, and directory paths for TensorBoard, videos, and checkpoints.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n    --config-name=social_nav/social_nav.yaml \\\n    benchmark/multi_agent=hssd_spot_human_social_nav \\\n    habitat_baselines.evaluate=False \\\n    habitat_baselines.num_checkpoints=5000 \\\n    habitat_baselines.total_num_steps=1.0e9 \\\n    habitat_baselines.num_environments=24 \\\n    habitat_baselines.tensorboard_dir=tb_social_nav \\\n    habitat_baselines.video_dir=video_social_nav \\\n    habitat_baselines.checkpoint_folder=checkpoints_social_nav \\\n    habitat_baselines.eval_ckpt_path_dir=checkpoints_social_nav \\\n    habitat.task.actions.agent_0_base_velocity.longitudinal_lin_speed=10.0 \\\n    habitat.task.actions.agent_0_base_velocity.ang_speed=10.0 \\\n    habitat.task.actions.agent_0_base_velocity.allow_dyn_slide=True \\\n    habitat.task.actions.agent_0_base_velocity.enable_rotation_check_for_dyn_slide=False \\\n    habitat.task.actions.agent_1_oracle_nav_randcoord_action.lin_speed=10.0 \\\n    habitat.task.actions.agent_1_oracle_nav_randcoord_action.ang_speed=10.0 \\\n    habitat.task.actions.agent_1_oracle_nav_action.lin_speed=10.0 \\\n    habitat.task.actions.agent_1_oracle_nav_action.ang_speed=10.0 \\\n    habitat.task.measurements.social_nav_reward.facing_human_reward=3.0 \\\n    habitat.task.measurements.social_nav_reward.count_coll_pen=0.01 \\\n    habitat.task.measurements.social_nav_reward.max_count_colls=-1 \\\n    habitat.task.measurements.social_nav_reward.count_coll_end_pen=5 \\\n    habitat.task.measurements.social_nav_reward.use_geo_distance=True \\\n    habitat.task.measurements.social_nav_reward.facing_human_dis=3.0 \\\n    habitat.task.measurements.social_nav_seek_success.following_step_succ_threshold=400 \\\n    habitat.task.measurements.social_nav_seek_success.need_to_face_human=True \\\n    habitat.task.measurements.social_nav_seek_success.use_geo_distance=True \\\n    habitat.task.measurements.social_nav_seek_success.facing_threshold=0.5 \\\n    habitat.task.lab_sensors.humanoid_detector_sensor.return_image=True \\\n    habitat.task.lab_sensors.humanoid_detector_sensor.is_return_image_bbox=True \\\n    habitat.task.success_reward=10.0 \\\n    habitat.task.end_on_success=True \\\n    habitat.task.slack_reward=-0.1 \\\n    habitat.environment.max_episode_steps=1500 \\\n    habitat.simulator.kinematic_mode=True \\\n    habitat.simulator.ac_freq_ratio=4 \\\n    habitat.simulator.ctrl_freq=120 \\\n    habitat.simulator.agents.agent_0.joint_start_noise=0.0\n```\n\n----------------------------------------\n\nTITLE: Evaluating EQA-CNN-Pretrain Model with Habitat Baselines\nDESCRIPTION: This code snippet demonstrates how to evaluate the EQA-CNN-Pretrain model using Habitat Baselines. It executes the `habitat_baselines.run` module with a specific configuration file (`eqa/il_eqa_cnn_pretrain.yaml`) and sets the `habitat_baselines.evaluate` flag to `True`.  This command uses the configurations defined in the yaml file to evaluate the model's performance. The results of the evaluation are stored in the `data/eqa/eqa_cnn_pretrain/results/val` directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=eqa/il_eqa_cnn_pretrain.yaml \\\n  habitat_baselines.evaluate=True\n```\n\n----------------------------------------\n\nTITLE: Displaying Sensor Data\nDESCRIPTION: Defines a function `display_sample` to display RGB, semantic, and depth observations using Matplotlib.  It converts the observations into PIL images for display. Configures the environment to include a semantic sensor and sets the turning angle for the agent. Simulates agent actions and visualizes the sensor outputs.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat-lab-demo.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\nfrom habitat_sim.utils.common import d3_40_colors_rgb\nfrom habitat.config.default import get_agent_config\nfrom habitat.config.default_structured_configs import HabitatSimSemanticSensorConfig\n\ndef display_sample(rgb_obs, semantic_obs, depth_obs):\n    rgb_img = Image.fromarray(rgb_obs, mode=\"RGB\")\n\n    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n    semantic_img.putpalette(d3_40_colors_rgb.flatten())\n    semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n    semantic_img = semantic_img.convert(\"RGBA\")\n\n    depth_img = Image.fromarray((depth_obs * 255).astype(np.uint8), mode=\"L\")\n\n    arr = [rgb_img, semantic_img, depth_img]\n\n    titles = ['rgb', 'semantic', 'depth']\n    plt.figure(figsize=(12 ,8))\n    for i, data in enumerate(arr):\n        ax = plt.subplot(1, 3, i+1)\n        ax.axis('off')\n        ax.set_title(titles[i])\n        plt.imshow(data)\n    plt.show()\n\nconfig = habitat.get_config(config_paths=\"benchmark/nav/pointnav/pointnav_mp3d.yaml\")\nwith read_write(config):\n    config.habitat.dataset.split = \"val\"\n    agent_config = get_agent_config(sim_config=config.habitat.simulator)\n    agent_config.sim_sensors.update(\n        {\"semantic_sensor\": HabitatSimSemanticSensorConfig(height=256, width=256)}\n    )\n    config.habitat.simulator.turn_angle = 30\n\nenv = habitat.Env(config=config)\nenv.episodes = random.sample(env.episodes, 2)\n\nmax_steps = 4\n\naction_mapping = {\n    0: 'stop',\n    1: 'move_forward',\n    2: 'turn left',\n    3: 'turn right'\n}\n\nfor i in range(len(env.episodes)):\n    observations = env.reset()\n\n    display_sample(observations['rgb'], observations['semantic'], np.squeeze(observations['depth']))\n\n    count_steps = 0\n    while count_steps < max_steps:\n        action = random.choice(list(action_mapping.keys()))\n        print(action_mapping[action])\n        observations = env.step(action)\n        display_sample(observations['rgb'], observations['semantic'], np.squeeze(observations['depth']))\n\n        count_steps += 1\n        if env.episode_over:\n            break\n\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Defining Humanoid Agent Configuration\nDESCRIPTION: This code snippet defines the configuration for a humanoid agent in Habitat. It sets the URDF path, articulated agent type, and motion data path for the humanoid. It also configures sensors (third_rgb and head_rgb) to be attached to the agent.  Finally, it creates a dictionary containing the agent configuration.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define the agent configuration\nmain_agent_config = AgentConfig()\nurdf_path = \"data/hab3_bench_assets/humanoids/female_0/female_0.urdf\"\nmain_agent_config.articulated_agent_urdf = urdf_path\nmain_agent_config.articulated_agent_type = \"KinematicHumanoid\"\nmain_agent_config.motion_data_path = \"data/hab3_bench_assets/humanoids/female_0/female_0_motion_data_smplx.pkl\"\n\n\n# Define sensors that will be attached to this agent, here a third_rgb sensor and a head_rgb.\n# We will later talk about why giving the sensors these names\nmain_agent_config.sim_sensors = {\n    \"third_rgb\": ThirdRGBSensorConfig(),\n    \"head_rgb\": HeadRGBSensorConfig(),\n}\n\n# We create a dictionary with names of agents and their corresponding agent configuration\nagent_dict = {\"main_agent\": main_agent_config}\n```\n\n----------------------------------------\n\nTITLE: Navigating and Picking Objects with Humanoid Actions in Python\nDESCRIPTION: This snippet demonstrates how to navigate a humanoid agent to an object and then pick it up using predefined actions. It involves resetting the environment, obtaining object information, using the 'humanoid_navigate_action' to move towards the object, and then executing the 'humanoid_pick_obj_id_action' to pick up the object.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nenv.reset()\nrom = env.sim.get_rigid_object_manager()\n# env.sim.articulated_agent.base_pos = init_pos\n# As before, we get a navigation point next to an object id\n\nobj_id = env.sim.scene_obj_ids[0]\nfirst_object = rom.get_object_by_id(obj_id)\n\nobject_trans = first_object.translation\nprint(first_object.handle, \"is in\", object_trans)\n# TODO: unoccluded object did not work\n# print(sample)\nobservations = []\ndelta = 2.0\n\nobject_agent_vec = env.sim.articulated_agent.base_pos - object_trans\nobject_agent_vec.y = 0\ndist_agent_object = object_agent_vec.length()\n# Walk towards the object\n\nagent_displ = np.inf\nagent_rot = np.inf\nprev_rot = env.sim.articulated_agent.base_rot\nprev_pos = env.sim.articulated_agent.base_pos\nwhile agent_displ > 1e-9 or agent_rot > 1e-9:\n    prev_rot = env.sim.articulated_agent.base_rot\n    prev_pos = env.sim.articulated_agent.base_pos\n    action_dict = {\n        \"action\": (\"humanoid_navigate_action\"), \n        \"action_args\": {\n              \"oracle_nav_lookat_action\": object_trans,\n              \"mode\": 1\n          }\n    }\n    observations.append(env.step(action_dict))\n    \n    cur_rot = env.sim.articulated_agent.base_rot\n    cur_pos = env.sim.articulated_agent.base_pos\n    agent_displ = (cur_pos - prev_pos).length()\n    agent_rot = np.abs(cur_rot - prev_rot)\n    \n# Wait\nfor _ in range(20):\n    action_dict = {\"action\": (), \"action_args\": {}}\n    observations.append(env.step(action_dict))\n\n# Pick object\nobservations.append(env.step(action_dict))\nfor _ in range(100):\n    \n    action_dict = {\"action\": (\"humanoid_pick_obj_id_action\"), \"action_args\": {\"humanoid_pick_obj_id\": obj_id}}\n    observations.append(env.step(action_dict)) \n    \nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Moving and Rotating the Humanoid Base\nDESCRIPTION: This code snippet demonstrates how to move and rotate the base of the articulated agent (humanoid) within the Habitat environment. It iteratively updates the base position and rotation of the agent and collects sensor observations to make a video.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsim = env.sim\nobservations = []\nnum_iter = 100\npos_delta = mn.Vector3(0.02,0,0)\nrot_delta = np.pi / (8 * num_iter)\nart_agent = sim.articulated_agent\nsim.reset()\n# set_fixed_camera(sim)\nfor _ in range(num_iter):\n    # TODO: this actually seems to give issues...\n    art_agent.base_pos = art_agent.base_pos + pos_delta\n    art_agent.base_rot = art_agent.base_rot + rot_delta\n    sim.step({})\n    observations.append(sim.get_sensor_observations())\n\n\nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Intrinsic Parameters (K)\nDESCRIPTION: This snippet calculates the intrinsic parameters matrix (K) based on the horizontal field of view (hfov). It then generates a grid of x and y coordinates, unprojects the depth map, and transforms the points from camera 0 to camera 1's coordinate frame.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nK = np.array([\n    [1 / np.tan(hfov / 2.), 0., 0., 0.],\n    [0., 1 / np.tan(hfov / 2.), 0., 0.],\n    [0., 0.,  1, 0],\n    [0., 0., 0, 1]])\n\n# Now get an approximation for the true world coordinates -- see if they make sense\n# [-1, 1] for x and [1, -1] for y as array indexing is y-down while world is y-up\nxs, ys = np.meshgrid(np.linspace(-1,1,W), np.linspace(1,-1,W))\ndepth = depths[0].reshape(1,W,W)\nxs = xs.reshape(1,W,W)\nys = ys.reshape(1,W,W)\n\n# Unproject\n# negate depth as the camera looks along -Z\nxys = np.vstack((xs * depth , ys * depth, -depth, np.ones(depth.shape)))\nxys = xys.reshape(4, -1)\nxy_c0 = np.matmul(np.linalg.inv(K), xys)\n\n# Now load in the cameras, are in the format camera --> world\n# Camera 1:\nquaternion_0 = cameras[0].sensor_states['depth'].rotation\ntranslation_0 = cameras[0].sensor_states['depth'].position\nrotation_0 = quaternion.as_rotation_matrix(quaternion_0)\nT_world_camera0 = np.eye(4)\nT_world_camera0[0:3,0:3] = rotation_0\nT_world_camera0[0:3,3] = translation_0\n\n# Camera 2:\ntranslation_1 = cameras[1].sensor_states['depth'].position\nquaternion_1 = cameras[1].sensor_states['depth'].rotation\nrotation_1 = quaternion.as_rotation_matrix(quaternion_1)\nT_world_camera1 = np.eye(4)\nT_world_camera1[0:3,0:3] =  rotation_1\nT_world_camera1[0:3,3] = translation_1\n\n# Invert to get world --> camera\nT_camera1_world = np.linalg.inv(T_world_camera1)\n\n# Transformation matrix between views\n# Aka the position of camera0 in camera1's coordinate frame\nT_camera1_camera0 = np.matmul(T_camera1_world, T_world_camera0)\n\n# Finally transform actual points\nxy_c1 = np.matmul(T_camera1_camera0, xy_c0)\nxy_newimg = np.matmul(K, xy_c1)\n\n# Normalize by negative depth\nxys_newimg = xy_newimg[0:2,:] / -xy_newimg[2:3,:]\n# Flip back to y-down to match array indexing\nxys_newimg[1] *= -1\n```\n\n----------------------------------------\n\nTITLE: Launching Training with Batch Renderer - Python\nDESCRIPTION: This snippet shows the command to launch training with the batch renderer. It overrides several configuration parameters, including enabling the batch renderer, enabling gfx replay save, and disabling the default renderer creation and concurrent rendering.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/core/batch_rendering/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nhabitat-baselines/habitat_baselines/run.py --config-name=rearrange/rl_skill.yaml habitat_baselines.trainer_name=\"ppo\" habitat.simulator.renderer.enable_batch_renderer=True habitat.simulator.habitat_sim_v0.enable_gfx_replay_save=True habitat.simulator.create_renderer=False habitat.simulator.concur_render=False habitat.simulator.renderer.composite_files=[path/to/composite_file.gltf]\n```\n\n----------------------------------------\n\nTITLE: Creating simulator configuration\nDESCRIPTION: This function creates a simulator configuration (`SimulatorConfig`) for the Habitat environment. It sets various parameters such as the scene path, scene dataset path, additional object paths, and agent configurations. The function uses the `OmegaConf` library for managing the configuration.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef make_sim_cfg(agent_dict):\n    # Start the scene config\n    sim_cfg = SimulatorConfig(type=\"RearrangeSim-v0\")\n    \n    # This is for better graphics\n    sim_cfg.habitat_sim_v0.enable_hbao = True\n    sim_cfg.habitat_sim_v0.enable_physics = True\n\n    \n    # Set up an example scene\n    sim_cfg.scene = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/scenes/103997919_171031233.scene_instance.json\")\n    sim_cfg.scene_dataset = os.path.join(data_path, \"hab3_bench_assets/hab3-hssd/hab3-hssd.scene_dataset_config.json\")\n    sim_cfg.additional_object_paths = [os.path.join(data_path, 'objects/ycb/configs/')]\n\n    \n    cfg = OmegaConf.create(sim_cfg)\n\n    # Set the scene agents\n    cfg.agents = agent_dict\n    cfg.agents_order = list(cfg.agents.keys())\n    return cfg\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-Agent Configuration\nDESCRIPTION: This code snippet demonstrates how to set up a multi-agent environment with two agents, one main agent, and one Spot robot. It duplicates the main agent's configuration, modifies the second agent's URDF and type, and then creates an agent dictionary. Action dictionaries are created for each agent and then combined into a single multi_agent_action_dict, and finally the environment is initialized. Dependencies: copy, os, habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# The main difference is in how we define the agent_dict.\n# Important: When using more than one agent, we should call them agent_{idx} with idx being between 0 and \n# the number of agents. This is required so that we can parse actions\nimport copy\nsecond_agent_config = copy.deepcopy(main_agent_config)\nsecond_agent_config.articulated_agent_urdf = os.path.join(data_path, \"robots/hab_spot_arm/urdf/hab_spot_arm.urdf\")\nsecond_agent_config.articulated_agent_type = \"SpotRobot\"\n\n\nagent_dict = {\"agent_0\": main_agent_config, \"agent_1\": second_agent_config}\naction_dict = {\n    \"oracle_magic_grasp_action\": ArmActionConfig(type=\"MagicGraspAction\"),\n    \"base_velocity_action\": BaseVelocityActionConfig(),\n    \"oracle_coord_action\": OracleNavActionConfig(type=\"OracleNavCoordinateAction\", spawn_max_dist_to_obj=1.0)\n}\n\nmulti_agent_action_dict = {}\nfor action_name, action_config in action_dict.items():\n    for agent_id in range(2):\n        multi_agent_action_dict[f\"agent_{agent_id}_{action_name}\"] = action_config \nenv = init_rearrange_env(agent_dict, multi_agent_action_dict)\n```\n\n----------------------------------------\n\nTITLE: Resetting the Environment and Visualizing Observations\nDESCRIPTION: This code resets the Habitat environment and retrieves observations from the sensors. It then visualizes the observations using matplotlib, displaying each sensor's output in a separate subplot. This allows for a quick visual inspection of the environment's initial state.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nobs = env.reset()\n_, ax = plt.subplots(1,len(obs.keys()))\n\nfor ind, name in enumerate(obs.keys()):\n    ax[ind].imshow(obs[name])\n    ax[ind].set_axis_off()\n    ax[ind].set_title(name)\n```\n\n----------------------------------------\n\nTITLE: Environment Interaction with Gym API\nDESCRIPTION: This snippet demonstrates interacting with a Habitat environment using the Gym API. It creates a Gym environment, takes random actions within the environment, renders the frames, and saves the simulation as a video. The environment is created using `gym.make(\"HabitatRenderPick-v0\")`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nenv = gym.make(\"HabitatRenderPick-v0\")\n\nvideo_file_path = os.path.join(output_path, \"example_interact.mp4\")\nvideo_writer = imageio.get_writer(video_file_path, fps=30)\n\ndone = False\nenv.reset()\nwhile not done:\n    obs, reward, done, info = env.step(env.action_space.sample())\n    video_writer.append_data(env.render(mode=\"rgb_array\"))\n\nvideo_writer.close()\nif vut.is_notebook():\n    vut.display_video(video_file_path)\n```\n\n----------------------------------------\n\nTITLE: Birdseye View with Agent on Border\nDESCRIPTION: This snippet generates birdseye views of the target, placing the agent on different borders of the environment. It iterates through different edge positions to create `NavigationEpisode` instances and then calls `maps.pointnav_draw_target_birdseye_view` for each episode. It depends on `numpy`, `matplotlib`, and `habitat` libraries.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab_TopdownMap_Visualization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef example_pointnav_draw_target_birdseye_view_agent_on_border():\n    # Define NavigationGoal\n    goal_radius = 0.5\n    goal = NavigationGoal(position=[0, 0.25, 0], radius=goal_radius)\n    # For defined goal create 4 NavigationEpisodes\n    # with agent being placed on different borders,\n    # draw birdseye view for each episode and save image to disk\n    ii = 0\n    for x_edge in [-1, 0, 1]:\n        for y_edge in [-1, 0, 1]:\n            if not np.bitwise_xor(x_edge == 0, y_edge == 0):\n                continue\n            ii += 1\n            agent_position = [7.8 * x_edge, 0.25, 7.8 * y_edge]\n            agent_rotation = np.pi / 2\n\n            dummy_episode = NavigationEpisode(\n                goals=[goal],\n                episode_id=\"dummy_id\",\n                scene_id=\"dummy_scene\",\n                start_position=agent_position,\n                start_rotation=agent_rotation,  # type: ignore[arg-type]\n            )\n\n            agent_position = np.array(agent_position)\n            target_image = maps.pointnav_draw_target_birdseye_view(\n                agent_position,\n                agent_rotation,\n                np.asarray(dummy_episode.goals[0].position),\n                goal_radius=dummy_episode.goals[0].radius,\n                agent_radius_px=25,\n            )\n            plt.imshow(target_image)\n            plt.title(\"pointnav_target_image_edge_%d.png\" % ii)\n            plt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Physics and Dropping an Agent\nDESCRIPTION: This snippet initializes the RearrangeSim simulator, disables the fixed base constraint of the articulated agent, and sets the agent's initial position above the floor to simulate a fall. It then steps through the simulation to observe the agent falling and saves observations to be later used for video creation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsim = init_rearrange_sim(agent_dict)\nart_agent = sim.articulated_agent\nart_agent._fixed_base = False\nsim.agents_mgr.on_new_scene()\n\n# The base is not fixed anymore\nart_agent.sim_obj.motion_type = MotionType.DYNAMIC\n\n\nart_agent.base_pos = init_pos + mn.Vector3(0,1.5,0)\n\n_ = sim.step({})\nobservations = []\nfps = 60 # Default value for make video\ndt = 1./fps\nfor _ in range(120):    \n    sim.step_physics(dt)\n    observations.append(sim.get_sensor_observations())\n    \n \n\nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating agent articulations and recording\nDESCRIPTION: This snippet updates the arm joints of the articulated agent, simulating movement of the arm. It iterates through a range of joint positions, sets the agent's arm joint positions, takes a simulation step, and records the sensor observations. The arm is moved from a minimum position to a maximum position. The arm joint position and the end effector translation are printed at the beginning and end of the movements. A video is created from `third_rgb`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsim.reset()\n\nobservations = []\n# We start by setting the arm to the minimum value\nlower_limit = art_agent.arm_joint_limits[0].copy()\nlower_limit[lower_limit == -np.inf] = 0\nupper_limit = art_agent.arm_joint_limits[1].copy()\nupper_limit[upper_limit == np.inf] = 0\nfor i in range(num_iter):\n    alpha = i/num_iter\n    current_joints = upper_limit * alpha + lower_limit * (1 - alpha)\n    art_agent.arm_joint_pos = current_joints\n    sim.step({})\n    observations.append(sim.get_sensor_observations())\n    if i in [0, num_iter-1]:\n        print(f\"Step {i}:\")\n        print(\"Arm joint positions:\", art_agent.arm_joint_pos)\n        print(\"Arm end effector translation:\", art_agent.ee_transform().translation)\n        print(art_agent.sim_obj.joint_positions)\n\nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Sine Policy\nDESCRIPTION: This code defines a custom policy `MySinePolicy` that executes a sine trajectory on joint 6. It inherits from `toco.PolicyModule` and uses a `JointSpacePD` feedback module. The policy computes desired joint positions based on a sine wave and applies PD control to generate joint torques.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/polymetis_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MySinePolicy(toco.PolicyModule):\n    \"\"\"\n    Custom policy that executes a sine trajectory on joint 6\n    (magnitude = 0.5 radian, frequency = 1 second)\n    \"\"\"\n\n    def __init__(self, time_horizon, hz, magnitude, period, kq, kqd, **kwargs):\n        \"\"\"\n        Args:\n            time_horizon (int):         Number of steps policy should execute\n            hz (double):                Frequency of controller\n            kq, kqd (torch.Tensor):     PD gains (1d array)\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.hz = hz\n        self.time_horizon = time_horizon\n        self.m = magnitude\n        self.T = period\n\n        # Initialize modules\n        self.feedback = toco.modules.JointSpacePD(kq, kqd)\n\n        # Initialize variables\n        self.steps = 0\n        self.q_initial = torch.zeros_like(kq)\n\n    def forward(self, state_dict: Dict[str, torch.Tensor]):\n        # Parse states\n        q_current = state_dict[\"joint_positions\"]\n        qd_current = state_dict[\"joint_velocities\"]\n\n        # Initialize\n        if self.steps == 0:\n            self.q_initial = q_current.clone()\n\n        # Compute reference position and velocity\n        q_desired = self.q_initial.clone()\n        q_desired[5] = self.q_initial[5] + self.m * torch.sin(\n            np.pi * self.steps / (self.hz * self.T)\n        )\n        qd_desired = torch.zeros_like(qd_current)\n\n        # Execute PD control\n        output = self.feedback(\n            q_current, qd_current, q_desired, torch.zeros_like(qd_current)\n        )\n\n        # Check termination\n        if self.steps > self.time_horizon:\n            self.set_terminated()\n        self.steps += 1\n\n        return {\"joint_torques\": output}\n```\n\n----------------------------------------\n\nTITLE: Downloading necessary data\nDESCRIPTION: This snippet downloads the necessary datasets for the tutorial, including assets for the Spot robot, benchmark assets, and YCB objects. It uses the `habitat_sim.utils.datasets_download` module to download the datasets. The `ln -s` command creates a symbolic link to the data directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Download necessary data. This step may take a while but will only be executed once.\n! ln -s ../../data .\n# We will download spot to show interaction between the spot robot and fetch\n! python -m habitat_sim.utils.datasets_download --no-replace --uids hab_spot_arm hab3_bench_assets ycb\n```\n\n----------------------------------------\n\nTITLE: Launching Pick_throw_vr in Headless Server Mode\nDESCRIPTION: This command launches the Pick_throw_vr application in headless server mode. It uses a config override to activate headless server functionality, which likely disables rendering on the server side.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/pick_throw_vr/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/pick_throw_vr/pick_throw_vr.py \\\n+experiment=headless_server\n```\n\n----------------------------------------\n\nTITLE: Generating Topdown Map from Simulation\nDESCRIPTION: This snippet demonstrates how to generate a topdown map from the Habitat simulator. It loads a configuration, creates a dataset and environment, and uses `maps.get_topdown_map_from_sim` to create the map. The snippet recolors the generated map for better visualization. It depends on `numpy`, `matplotlib`, and `habitat` libraries.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab_TopdownMap_Visualization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef example_get_topdown_map():\n    # Create habitat config\n    config = habitat.get_config(\n        config_path=os.path.join(\n            dir_path,\n            \"habitat-lab/habitat/config/benchmark/nav/pointnav/pointnav_habitat_test.yaml\",\n        )\n    )\n    # Create dataset\n    dataset = habitat.make_dataset(\n        id_dataset=config.habitat.dataset.type, config=config.habitat.dataset\n    )\n    # Create simulation environment\n    with habitat.Env(config=config, dataset=dataset) as env:\n        # Load the first episode\n        env.reset()\n        # Generate topdown map\n        top_down_map = maps.get_topdown_map_from_sim(\n            cast(\"HabitatSim\", env.sim), map_resolution=1024\n        )\n        recolor_map = np.array(\n            [[255, 255, 255], [128, 128, 128], [0, 0, 0]], dtype=np.uint8\n        )\n        # By default, `get_topdown_map_from_sim` returns image\n        # containing 0 if occupied, 1 if unoccupied, and 2 if border\n        # The line below recolors returned image so that\n        # occupied regions are colored in [255, 255, 255],\n        # unoccupied in [128, 128, 128] and border is [0, 0, 0]\n        top_down_map = recolor_map[top_down_map]\n        plt.imshow(top_down_map)\n        plt.title(\"top_down_map.png\")\n        plt.show()\n```\n\n----------------------------------------\n\nTITLE: Define Dataset Configuration (YAML)\nDESCRIPTION: This YAML configuration defines the dataset generation parameters, including scene paths, object paths, and receptacle definitions.  It specifies which scenes, objects and receptacles should be used to create episodes.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\ndataset_cfg_txt = \"\"\"\n---\ndataset_path: \"data/replica_cad/replicaCAD.scene_dataset_config.json\"\nadditional_object_paths:\n  - \"data/objects/ycb/configs/\"\nscene_sets:\n  -\n    name: \"v3_sc\"\n    included_substrings:\n      - \"v3_sc\"\n    excluded_substrings: []\n    comment: \"This set (v3_sc) selects all 105 ReplicaCAD variations with static furniture.\"\n\nobject_sets:\n  -\n    name: \"kitchen\"\n    included_substrings:\n      - \"002_master_chef_can\"\n      - \"003_cracker_box\"\n    excluded_substrings: []\n    comment: \"Leave included_substrings empty to select all objects.\"\n\nreceptacle_sets:\n  -\n    name: \"table\"\n    included_object_substrings:\n      - \"frl_apartment_table_01\"\n    excluded_object_substrings: []\n    included_receptacle_substrings:\n      - \"\"\n    excluded_receptacle_substrings: []\n    comment: \"The empty substrings act like wildcards, selecting all receptacles for all objects.\"\n\nscene_sampler:\n  type: \"subset\"\n  params:\n    scene_sets: [\"v3_sc\"]\n  comment: \"Samples from ReplicaCAD 105 variations with static furniture.\"\n\n\nobject_samplers:\n  -\n    name: \"kitchen_counter\"\n    type: \"uniform\"\n    params:\n      object_sets: [\"kitchen\"]\n      receptacle_sets: [\"table\"]\n      num_samples: [1, 1]\n      orientation_sampling: \"up\"\n\nobject_target_samplers:\n  -\n    name: \"kitchen_counter_targets\"\n    type: \"uniform\"\n    params:\n      object_samplers: [\"kitchen_counter\"]\n      receptacle_sets: [\"table\"]\n      num_samples: [1, 1]\n      orientation_sampling: \"up\"\n\"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Interact with New Task Env - Python\nDESCRIPTION: This snippet demonstrates how to interact with the newly defined custom task. It resets the environment, displays the initial observation, and then enters a loop to execute actions until the episode is over. The code checks if interactive control is enabled and if so, prompts the user for an action. Otherwise, it pops an action from a predefined list. It then steps the environment and prints whether the episode is over. Finally the env is closed.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\naction = None\n    env.reset()\n    valid_actions = [\"turn_left\", \"turn_right\", \"move_forward\", \"stop\"]\n    interactive_control = False  # @param {type:\"boolean\"}\n    while env.episode_over is not True:\n        display_sample(obs[\"rgb\"])\n        if interactive_control:\n            action = input(\n                \"enter action out of {}:\\n\".format(\", \".join(valid_actions))\n            )\n            assert (\n                action in valid_actions\n            ), \"invalid action {} entered, choose one amongst \" + \",\".join(\n                valid_actions\n            )\n        else:\n            action = valid_actions.pop()\n        obs = env.step(\n            {\n                \"action\": action,\n                \"action_args\": None,\n            }\n        )\n        print(\"Episode over:\", env.episode_over)\n\n    env.close()\n```\n\n----------------------------------------\n\nTITLE: Downloading Humanoid Avatar (Habitat)\nDESCRIPTION: Downloads the humanoid avatar data files required for the Habitat simulation environment. This command uses the `habitat_sim.utils.datasets_download` module to retrieve the necessary files and store them in the `data/` directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython -m habitat_sim.utils.datasets_download --uids habitat_humanoids  --data-path data/\n```\n\n----------------------------------------\n\nTITLE: Moving the agent base and recording\nDESCRIPTION: This code snippet moves the agent base in a series of steps, recording the sensor observations at each step, and creates a video from the observations. It sets the agent's position and rotation incrementally, simulating movement in the environment. Two videos are created, one for `scene_camera_rgb` and one for `third_rgb`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nobservations = []\nnum_iter = 100\npos_delta = mn.Vector3(0.02,0,0)\nrot_delta = np.pi / (8 * num_iter)\nart_agent.base_pos = init_pos\n\nsim.reset()\n# set_fixed_camera(sim)\nfor _ in range(num_iter):\n    # TODO: this actually seems to give issues...\n    art_agent.base_pos = art_agent.base_pos + pos_delta\n    art_agent.base_rot = art_agent.base_rot + rot_delta\n    sim.step({})\n    observations.append(sim.get_sensor_observations())\n\nvut.make_video(\n    observations,\n    \"scene_camera_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining NavPickReward Measurement\nDESCRIPTION: This code snippet defines a reward function, `NavPickReward`, for the NavPick task. It inherits from `RearrangeReward` and includes penalties for collisions. The reward is inversely proportional to the distance between the end-effector and the target object. It requires the `DistanceToTargetObject` measurement.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@registry.register_measure\nclass NavPickReward(RearrangeReward):\n    \"\"\"\n    For every new task, you NEED to implement a reward function.\n    `RearrangeReward` automatically includes penalties for collisions into the reward function.\n    \"\"\"\n\n    cls_uuid: str = \"navpick_reward\"\n\n    def __init__(self, sim, config, *args, **kwargs):\n        self._sim = sim\n        self._config = config\n        # You can get you custom gonfiguration fields defined in NavPickRewardMeasurementConfig\n        self._scaling_factor = config.scaling_factor\n        super().__init__(sim=sim, config=config, **kwargs)\n\n    @staticmethod\n    def _get_uuid(*args, **kwargs):\n        return NavPickReward.cls_uuid\n\n    def reset_metric(self, *args, task, episode, **kwargs):\n        # Measurements can be computed from other measurements.\n        task.measurements.check_measure_dependencies(\n            self.uuid,\n            [\n                DistanceToTargetObject.cls_uuid,\n            ],\n        )\n        self.update_metric(*args, task=task, episode=episode, **kwargs)\n\n    def update_metric(self, *args, task, episode, **kwargs):\n        ee_to_object_distance = task.measurements.measures[\n            DistanceToTargetObject.cls_uuid\n        ].get_metric()\n\n        self._metric = -ee_to_object_distance * self._scaling_factor\n```\n\n----------------------------------------\n\nTITLE: Training NAV (PACMAN) Model with Habitat Baselines\nDESCRIPTION: This code snippet shows how to train the NAV model, also known as PACMAN, using Habitat Baselines. It executes the `habitat_baselines.run` module with a specific configuration file (`eqa/il_pacman_nav.yaml`).  The training process is set up according to parameters specified in the configuration file.  Training checkpoints are stored by default in `data/eqa/nav/checkpoints`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=eqa/il_pacman_nav.yaml\n```\n\n----------------------------------------\n\nTITLE: Benchmark Assets Download (Shell)\nDESCRIPTION: This shell command downloads the benchmark assets required for running the Habitat 2.0 benchmark. It uses the `habitat_sim.utils.datasets_download` module.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython -m habitat_sim.utils.datasets_download --uids hab2_bench_assets\n```\n\n----------------------------------------\n\nTITLE: Reaching a Position with Humanoid Controller in Python\nDESCRIPTION: This snippet shows how to make the humanoid reach a specific position with its hand using the `HumanoidRearrangeController`. The code involves calculating the desired hand pose, using `calculate_reach_pose()`, and applying the resulting pose to the environment via humanoid joint actions. Random offsets are added to the hand pose in each step.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# We reset the controller\nenv.reset()\nhumanoid_controller.reset(env.sim.articulated_agent.base_transformation)\nobservations = []\nprint(env.sim.articulated_agent.base_pos)\n\n# Get the hand pose\noffset =  env.sim.articulated_agent.base_transformation.transform_vector(mn.Vector3(0, 0.3, 0))\nhand_pose = env.sim.articulated_agent.ee_transform(0).translation + offset\nfor _ in range(100):\n    # This computes a pose that moves the agent to relative_position\n    hand_pose = hand_pose + mn.Vector3((np.random.rand(3) - 0.5) * 0.1)\n    humanoid_controller.calculate_reach_pose(hand_pose, index_hand=0)\n    \n    # The get_pose function gives as a humanoid pose in the same format as HumanoidJointAction\n    new_pose = humanoid_controller.get_pose()\n    action_dict = {\n        \"action\": \"humanoid_joint_action\",\n        \"action_args\": {\"human_joints_trans\": new_pose}\n    }\n    observations.append(env.step(action_dict))\n    \nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting agent position\nDESCRIPTION: This snippet demonstrates how to set the position of the articulated agent in the simulation environment. It initializes an agent, sets its motion type to kinematic, changes its base position, and then performs a simulation step to update the agent's position. It prints the agent position before and after.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninit_pos = mn.Vector3(-5.5,0,-1.5)\nart_agent = sim.articulated_agent\n# We will see later about this\nart_agent.sim_obj.motion_type = MotionType.KINEMATIC\nprint(\"Current agent position:\", art_agent.base_pos)\nart_agent.base_pos = init_pos \nprint(\"New agent position:\", art_agent.base_pos)\n# We take a step to update agent position\n_ = sim.step({})\n```\n\n----------------------------------------\n\nTITLE: Defining DistanceToTargetObject Measurement\nDESCRIPTION: This code snippet defines a measurement, `DistanceToTargetObject`, that calculates the Euclidean distance between the robot's end-effector and the target object. It inherits from the `Measure` class and implements the `reset_metric` and `update_metric` functions to compute the distance. It relies on the simulator and robot state.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@registry.register_measure\nclass DistanceToTargetObject(Measure):\n    \"\"\"\n    Gets the Euclidean distance to the target object from the end-effector.\n    \"\"\"\n\n    cls_uuid: str = \"distance_to_object\"\n\n    def __init__(self, sim, config, *args, **kwargs):\n        self._sim = sim\n        self._config = config\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def _get_uuid(*args, **kwargs):\n        return DistanceToTargetObject.cls_uuid\n\n    def reset_metric(self, *args, episode, **kwargs):\n        self.update_metric(*args, episode=episode, **kwargs)\n\n    def update_metric(self, *args, task, episode, **kwargs):\n        ee_pos = self._sim.articulated_agent.ee_transform().translation\n\n        idxs, _ = self._sim.get_targets()\n        scene_pos = self._sim.get_scene_pos()[idxs[task.target_object_index]]\n\n        # Metric information is stored in the `self._metric` variable.\n        self._metric = np.linalg.norm(scene_pos - ee_pos, ord=2, axis=-1)\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Setting Up Paths (Python)\nDESCRIPTION: This snippet initializes the environment, sets up paths, and configures logging for the Habitat simulator. It imports necessary libraries such as `os`, `git`, `PIL`, `imageio`, and `habitat_sim`. It checks for a Colab environment and sets the base configuration path accordingly. It also creates an output path for visualizations.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/habitat2_gym_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport git\n\nif \"COLAB_GPU\" in os.environ:\n    print(\"Setting Habitat base path\")\n    %env HABLAB_BASE_CFG_PATH=/content/habitat-lab\n    import importlib\n\n    import PIL\n\n    importlib.reload(PIL.TiffTags)  # type: ignore[attr-defined]\n\nimport imageio\n\n# Video rendering utility.\nfrom habitat_sim.utils import viz_utils as vut\n\n# Quiet the Habitat simulator logging\nos.environ[\"MAGNUM_LOG\"] = \"quiet\"\nos.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n\nrepo = git.Repo(\".\", search_parent_directories=True)\ndir_path = repo.working_tree_dir\noutput_path = os.path.join(\n    dir_path, \"examples/tutorials/habitat_lab_visualization/\"\n)\nos.makedirs(output_path, exist_ok=True)\nos.chdir(dir_path)\n# If the import block below fails due to an error like \"'PIL.TiffTags' has no attribute\n# 'IFD'\", then restart the Colab runtime instance and rerun this cell and the previous cell.\n```\n\n----------------------------------------\n\nTITLE: Initializing Habitat Visualization Environment\nDESCRIPTION: This snippet sets up the environment for Habitat Lab visualization. It imports necessary libraries, configures logging, defines file paths, and changes the working directory. It depends on libraries like `os`, `typing`, `git`, `matplotlib`, `numpy`, and `habitat`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab_TopdownMap_Visualization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import TYPE_CHECKING, Union, cast\n\nimport git\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport habitat\nfrom habitat.config.default_structured_configs import (\n    CollisionsMeasurementConfig,\n    FogOfWarConfig,\n    TopDownMapMeasurementConfig,\n)\nfrom habitat.core.agent import Agent\nfrom habitat.tasks.nav.nav import NavigationEpisode, NavigationGoal\nfrom habitat.tasks.nav.shortest_path_follower import ShortestPathFollower\nfrom habitat.utils.visualizations import maps\nfrom habitat.utils.visualizations.utils import (\n    images_to_video,\n    observations_to_image,\n    overlay_frame,\n)\nfrom habitat_sim.utils import viz_utils as vut\n\n# Quiet the Habitat simulator logging\nos.environ[\"MAGNUM_LOG\"] = \"quiet\"\nos.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n\nif TYPE_CHECKING:\n    from habitat.core.simulator import Observations\n    from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim\n\nrepo = git.Repo(\".\", search_parent_directories=True)\ndir_path = repo.working_tree_dir\ndata_path = os.path.join(dir_path, \"data\")\noutput_path = os.path.join(\n    dir_path, \"examples/tutorials/habitat_lab_visualization/\"\n)\nos.makedirs(output_path, exist_ok=True)\nos.chdir(dir_path)\n```\n\n----------------------------------------\n\nTITLE: Converting SMPL-X Motion File\nDESCRIPTION: This code snippet shows how to convert a motion file from a format commonly used in SMPL-X (including AMASS or Motion Diffusion Models) into a `.pkl` file, which can be consumed by the HumanoidSeqPoseController. The convert_helper.convert_motion_file function is used to convert the input motion file to the desired format.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom habitat.utils.humanoid_utils import MotionConverterSMPLX\nPATH_TO_URDF = \"data/humanoids/humanoid_data/female_2/female_2.urdf\"\nPATH_TO_MOTION_NPZ = \"data/humanoids/humanoid_data/walk_motion/CMU_10_04_stageii.npz\"\nconvert_helper = MotionConverterSMPLX(urdf_path=PATH_TO_URDF)\nconvert_helper.convert_motion_file(\n    motion_path=PATH_TO_MOTION_NPZ,\n    output_path=PATH_TO_MOTION_NPZ.replace(\".npz\", \"\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Define New Navigation Task - Python\nDESCRIPTION: This snippet demonstrates how to create a new custom navigation task by subclassing the `NavigationTask` class and registering it with the Habitat registry. It overrides the `_check_episode_is_active` method to define custom episode termination criteria.  It also modifies the main config to use this newly registered task.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    config = habitat.get_config(\n        config_path=os.path.join(\n            dir_path,\n            \"habitat-lab/habitat/config/benchmark/nav/pointnav/pointnav_habitat_test.yaml\",\n        ),\n        overrides=[\n            \"habitat.environment.max_episode_steps=10\",\n            \"habitat.environment.iterator_options.shuffle=False\",\n        ],\n    )\n\n\n@registry.register_task(name=\"TestNav-v0\")\nclass NewNavigationTask(NavigationTask):\n    def __init__(self, config, sim, dataset):\n        logger.info(\"Creating a new type of task\")\n        super().__init__(config=config, sim=sim, dataset=dataset)\n\n    def _check_episode_is_active(self, *args, **kwargs):\n        logger.info(\n            \"Current agent position: {}\".format(self._sim.get_agent_state())\n        )\n        collision = self._sim.previous_step_collided\n        stop_called = not getattr(self, \"is_stop_called\", False)\n        return collision or stop_called\n\n\nif __name__ == \"__main__\":\n    with habitat.config.read_write(config):\n        config.habitat.task.type = \"TestNav-v0\"\n\n    try:\n        env.close()\n    except NameError:\n        pass\n    env = habitat.Env(config=config)\n```\n\n----------------------------------------\n\nTITLE: Run Episode Generator (Python)\nDESCRIPTION: This Python command line instruction executes the Habitat episode generator using the configuration file `nav_pick_dataset.yaml`. It generates 10 episodes and saves them to a file named `data/nav_pick.json.gz`. The generated dataset can then be used for training and evaluation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n!python -m habitat.datasets.rearrange.run_episode_generator --run --config {nav_pick_cfg_path} --num-episodes 10 --out data/nav_pick.json.gz\n```\n\n----------------------------------------\n\nTITLE: Receptacle Configuration in user_defined - Python\nDESCRIPTION: This snippet demonstrates how to define a receptacle using the 'user_defined' field in a configuration file. It includes parameters such as name, parent object handle, parent link, position, rotation, scale, up vector, and mesh filepath. This configuration is used to define the properties of a receptacle for object placement.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/metadata-taxonomy.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    \"user_defined\": {\n        \"receptacle_mesh_table0001_receptacle_mesh\": {\n            \"name\": \"table0001_receptacle_mesh\",\n            \"parent_object\": \"0a5df6da61cd2e78e972690b501452152309e56b\", #handle of the parent ManagedObject's template\n            \"parent_link\": \"table0001\", #if attached to an ArticulatedLink, this is the local index\n            \"position\": [0,0,0], # position of the receptacle in parent's local space\n            \"rotation\": [1,0,0,0],#orientation (quaternion) of the receptacle in parent's local space\n            \"scale\": [1,1,1], #scale of the receptacles in parent's local space\n            \"up\": [0,0,1], #up vector for the receptacle in parent's local space (for tilt culling and placement snapping)\n            \"mesh_filepath\": \"table0001_receptacle_mesh.glb\" #filepath for the receptacle's mesh asset (.glb with triangulated faces expected)\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Launching Habitat Robot with Custom Model\nDESCRIPTION: This command launches the Habitat robot client with Polymetis, specifying a custom robot model.  It uses `robot_client=habitat_sim` and `robot_model=ROBOT_MODEL`, sets the absolute path to the scene file, disables real-time execution, and enables the GUI. The `ROBOT_MODEL` variable determines the robot's configuration.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/polymetis_example.ipynb#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlaunch_robot.py robot_client=habitat_sim robot_model=ROBOT_MODEL habitat_scene_path=/PATH/TO/scene use_real_time=false gui=true\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Agent Position Sensor in Habitat (Python)\nDESCRIPTION: This snippet demonstrates how to create a custom sensor in Habitat that provides the agent's position as an observation.  It involves defining a class that inherits from `habitat.Sensor`, implementing methods for specifying the sensor's UUID, type, observation space, and the `get_observation` method to retrieve the agent's position from the simulator.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n@registry.register_sensor(name=\"agent_position_sensor\")\nclass AgentPositionSensor(habitat.Sensor):\n    def __init__(self, sim, config, **kwargs):\n        super().__init__(config=config)\n        self._sim = sim\n\n    # Defines the name of the sensor in the sensor suite dictionary\n    def _get_uuid(self, *args, **kwargs):\n        return \"agent_position\"\n\n    # Defines the type of the sensor\n    def _get_sensor_type(self, *args, **kwargs):\n        return habitat.SensorTypes.POSITION\n\n    # Defines the size and range of the observations of the sensor\n    def _get_observation_space(self, *args, **kwargs):\n        return spaces.Box(\n            low=np.finfo(np.float32).min,\n            high=np.finfo(np.float32).max,\n            shape=(3,),\n            dtype=np.float32,\n        )\n\n    # This is called whenever reset is called or an action is taken\n    def get_observation(self, observations, *args, episode, **kwargs):\n        return self._sim.get_agent_state().position\n```\n\n----------------------------------------\n\nTITLE: Download Habitat 3.0 Benchmark Assets Manually using git clone\nDESCRIPTION: This command downloads the Habitat 3.0 benchmark assets manually using git clone. It requires a Hugging Face username and password, and clones the 'ai-habitat/hab3_bench_assets' repository.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab3_bench/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://huggingface.co/datasets/ai-habitat/hab3_bench_assets --username <your HF username> --password <your HF password>\n```\n\n----------------------------------------\n\nTITLE: Defining NavPickSuccess Measurement\nDESCRIPTION: This code snippet defines a success condition, `NavPickSuccess`, for the NavPick task. It checks if the agent is holding the correct target object by comparing the grasped object's index with the target object's index. It inherits from the `Measure` class.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@registry.register_measure\nclass NavPickSuccess(Measure):\n    \"\"\"\n    For every new task, you NEED to implement a \"success\" condition.\n    \"\"\"\n\n    cls_uuid: str = \"navpick_success\"\n\n    def __init__(self, sim, config, *args, **kwargs):\n        self._sim = sim\n        self._config = config\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def _get_uuid(*args, **kwargs):\n        return NavPickSuccess.cls_uuid\n\n    def reset_metric(self, *args, episode, task, observations, **kwargs):\n        self.update_metric(\n            *args,\n            episode=episode,\n            task=task,\n            observations=observations,\n            **kwargs\n        )\n\n    def update_metric(self, *args, episode, task, observations, **kwargs):\n        # Check that the agent is holding the correct object.\n        abs_targ_obj_idx = self._sim.scene_obj_ids[task.target_object_index]\n        self._metric = abs_targ_obj_idx == self._sim.grasp_mgr.snap_idx\n```\n\n----------------------------------------\n\nTITLE: Generating Random Humanoid Joint Rotations\nDESCRIPTION: This code defines helper functions to generate random rotations for the humanoid joints. The `random_rotation` function creates a random quaternion rotation. The `custom_sample_humanoid` function then uses these rotations to generate a random configuration for the humanoid's joints and base transform, returning it as a dictionary suitable for use with a humanoid action.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# TODO: maybe we can make joint_action a subclass of dict, and have a custom function for it\nimport random\ndef random_rotation():\n    random_dir = mn.Vector3(np.random.rand(3)).normalized()\n    random_angle = random.random() * np.pi\n    random_rat = mn.Quaternion.rotation(mn.Rad(random_angle), random_dir)\n    return random_rat\ndef custom_sample_humanoid():\n    base_transform = mn.Matrix4() \n    random_rot = random_rotation()\n    offset_transform = mn.Matrix4.from_(random_rot.to_matrix(), mn.Vector3())\n    joints = []\n    num_joints = 54\n    for _ in range(num_joints):\n        Q = random_rotation()\n        joints = joints + list(Q.vector) + [float(Q.scalar)]\n    offset_trans = list(np.asarray(offset_transform.transposed()).flatten())\n    base_trans = list(np.asarray(base_transform.transposed()).flatten())\n    random_vec = joints + offset_trans + base_trans\n    return {\n        \"human_joints_trans\": random_vec\n    }\n```\n\n----------------------------------------\n\nTITLE: HabitatDatasetSource Configuration Example (Scene Dataset)\nDESCRIPTION: An example of configuring a HabitatDatasetSource to process a scene dataset with stages, objects, and articulated objects. It includes a whitelist to only process a specific scene and excludes orphan assets.  Objects are decimated, while stages and articulated objects are not.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nHabitatDatasetSource(\n   name=\"hssd-hab-articulated\",\n   dataset_config=\"hssd-hab/hssd-hab-articulated.scene_dataset_config.json\",\n   stages=ProcessingSettings(\n         operation=Operation.PROCESS,\n         decimate=False,\n         group=GroupType.GROUP_BY_SCENE,\n   ),\n   objects=ProcessingSettings(\n         operation=Operation.PROCESS,\n         decimate=True,\n         group=GroupType.GROUP_BY_SCENE,\n   ),\n   articulated_objects=ProcessingSettings(\n         operation=Operation.PROCESS,\n         decimate=False,\n         group=GroupType.GROUP_BY_SCENE,\n   ),\n   scene_whitelist=[\"102344250\"],   # Only process `102344250.scene_instance.json`\n   include_orphan_assets=False,     # Exclude objects that are not referenced by any scene.\n)\n```\n\n----------------------------------------\n\nTITLE: Navigation Towards Object\nDESCRIPTION: This code snippet implements a navigation loop that moves an agent towards a specified object using oracle navigation. It iteratively calculates the agent's displacement and rotation, then executes an 'oracle_coord_action' until the displacement and rotation fall below a threshold. Dependencies: numpy, habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nagent_displ = np.inf\nagent_rot = np.inf\nprev_rot = env.sim.articulated_agent.base_rot\nprev_pos = env.sim.articulated_agent.base_pos\nwhile agent_displ > 1e-9 or agent_rot > 1e-9:\n    prev_rot = env.sim.articulated_agent.base_rot\n    prev_pos = env.sim.articulated_agent.base_pos\n    action_dict = {\n        \"action\": (\"oracle_coord_action\"), \n        \"action_args\": {\n              \"oracle_nav_lookat_action\": object_trans,\n              \"mode\": 1\n          }\n    }\n    observations.append(env.step(action_dict))\n    \n    cur_rot = env.sim.articulated_agent.base_rot\n    cur_pos = env.sim.articulated_agent.base_pos\n    agent_displ = (cur_pos - prev_pos).length()\n    agent_rot = np.abs(cur_rot - prev_rot)\n```\n\n----------------------------------------\n\nTITLE: Marker Sets Configuration - Python\nDESCRIPTION: This snippet demonstrates how to define marker sets using the 'user_defined' field.  Marker sets are 3D point sets defined for various purposes, such as defining handles for opening ArticulatedObject links or specifying faucet points on sinks. These points are typically defined in object-local space.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/metadata-taxonomy.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    \"user_defined\": {\n        \"marker_sets\": {\n\n            \"handle_marker_sets\":{ #these are handles for opening an ArticulatedObject's links.\n                0: { # these marker sets are attached to link_id \"0\".\n                    \"handle_0\": { #this is a set of 3D points.\n                        0: [x,y,z] #we index because JSON needs a dict and Configuration cannot digest lists\n                        1: [x,y,z]\n                        2: [x,y,z]\n                    },\n                    ...\n                },\n                ...\n            },\n\n            \"faucet_marker_set\":{ #these are faucet points on sinks in object local space\n                0: { # these marker sets are attached to link_id \"0\". \"-1\" implies base link or rigid object.\n                    0: [x,y,z] #this is a faucet\n                    ...\n                },\n                ...\n            }\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Colab Setup and Imports - Python\nDESCRIPTION: This snippet handles the setup and imports required for running Habitat Lab in a Google Colab environment. It includes importing necessary libraries such as os, random, git, numpy, PIL, and various modules from the habitat and habitat_baselines packages. It also sets the working directory and data path, which are crucial for accessing the Habitat Lab data and configurations.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport random\n\nimport git\nimport numpy as np\nfrom gym import spaces\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nrepo = git.Repo(\".\", search_parent_directories=True)\ndir_path = repo.working_tree_dir\ndata_path = os.path.join(dir_path, \"data\")\nos.chdir(dir_path)\n\nfrom PIL import Image\n\nimport habitat\nfrom habitat.core.logging import logger\nfrom habitat.core.registry import registry\nfrom habitat.sims.habitat_simulator.actions import HabitatSimActions\nfrom habitat.tasks.nav.nav import NavigationTask\nfrom habitat_baselines.common.baseline_registry import baseline_registry\nfrom habitat_baselines.config.default import get_config as get_baselines_config\n```\n\n----------------------------------------\n\nTITLE: Applying Random Humanoid Joint Actions\nDESCRIPTION: This code snippet applies the `custom_sample_humanoid` function to generate random rotations, then construct an action dictionary to call humanoid_joint_action in a loop to observe the generated motion using habitat simulation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# We can now call the defined actions\nobservations = []\nnum_iter = 40\nenv.reset()\nfor _ in range(num_iter):\n    params = custom_sample_humanoid()\n    action_dict = {\n        \"action\": \"humanoid_joint_action\",\n        \"action_args\": params\n    }\n    observations.append(env.step(action_dict))\nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Social Rearrangement Zero-Shot Evaluation (Habitat)\nDESCRIPTION: This python command evaluates the trained policy for the social rearrangement task in Habitat in a zero-shot manner, against an unseen agent population. It uses planner-based collaborators and evaluates against different planner agents by changing `plan_idx`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython habitat_baselines/run.py --config-name=social_rearrange/pop_play.yaml habitat_baselines.evaluate=True habitat_baselines.eval_ckpt_path_dir=PATH_TO_CKPT.pth +habitat_baselines.rl.policy.agent_1.hierarchical_policy.high_level_policy.select_random_goal=False +habitat_baselines.rl.policy.agent_1.hierarchical_policy.high_level_policy.plan_idx=1\n```\n\n----------------------------------------\n\nTITLE: Print Environment Metrics - Python\nDESCRIPTION: This snippet retrieves and prints the environment's metrics. This allows for evaluation of the agent's performance during an episode. These metrics usually include measures such as success rate, SPL, and distance to goal. Printing the metrics provides insight into how well the agent is performing in the environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(env.get_metrics())\n```\n\n----------------------------------------\n\nTITLE: Defining Measurement Configurations\nDESCRIPTION: This code snippet defines dataclasses for measurement configurations (`DistanceToTargetObjectMeasurementConfig`, `NavPickRewardMeasurementConfig`, and `NavPickSuccessMeasurementConfig`). These configurations are used to define the parameters for each measurement and are typically used with Hydra for configuration management. Includes reward specific configurations.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass DistanceToTargetObjectMeasurementConfig(MeasurementConfig):\n    type: str = \"DistanceToTargetObject\"\n\n\n@dataclass\nclass NavPickRewardMeasurementConfig(MeasurementConfig):\n    type: str = \"NavPickReward\"\n    scaling_factor: float = 0.1\n    # General Rearrange Reward config\n    constraint_violate_pen: float = 10.0\n    force_pen: float = 0.001\n    max_force_pen: float = 1.0\n    force_end_pen: float = 10.0\n    count_coll_pen: float = -1.0\n    max_count_colls: int = -1\n    count_coll_end_pen: float = 1.0\n\n\n@dataclass\nclass NavPickSuccessMeasurementConfig(MeasurementConfig):\n    type: str = \"NavPickSuccess\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Stretch Asset\nDESCRIPTION: This code snippet demonstrates how to download the Stretch robot asset for use in Habitat simulations. It utilizes the `habitat_sim.utils.datasets_download` module to download the asset and specify the path where the data should be stored.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/robots/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m habitat_sim.utils.datasets_download --uids hab_stretch --data-path /path/to/data/\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using the Custom Agent Position Sensor (Python)\nDESCRIPTION: This snippet configures the Habitat environment to use the custom `AgentPositionSensor`. It loads a configuration file, overrides some settings, defines the sensor configuration using `LabSensorConfig`, and then adds the sensor to the task's lab_sensors. Finally, it initializes the Habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    config = habitat.get_config(\n        config_path=os.path.join(\n            dir_path,\n            \"habitat-lab/habitat/config/benchmark/nav/pointnav/pointnav_habitat_test.yaml\",\n        ),\n        overrides=[\n            \"habitat.environment.max_episode_steps=10\",\n            \"habitat.environment.iterator_options.shuffle=False\",\n        ],\n    )\n\n    from habitat.config.default_structured_configs import LabSensorConfig\n\n    # We use the base sensor config, but you could also define your own\n    # AgentPositionSensorConfig that inherits from LabSensorConfig\n\n    with habitat.config.read_write(config):\n        # Now define the config for the sensor\n        config.habitat.task.lab_sensors[\n            \"agent_position_sensor\"\n        ] = LabSensorConfig(type=\"agent_position_sensor\")\n\n    try:\n        env.close()\n    except NameError:\n        pass\n    env = habitat.Env(config=config)\n```\n\n----------------------------------------\n\nTITLE: Calling Defined Actions\nDESCRIPTION: This snippet demonstrates how to call predefined actions in the environment. It generates random parameters for the `base_velocity_action` from its action space and then steps the environment using a dictionary containing the action name and parameters.  The process is repeated multiple times to simulate agent movement and results in a generated video.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# We can now call the defined actions\nobservations = []\nnum_iter = 40\nfor _ in range(num_iter):\n    params = env.action_space[\"base_velocity_action\"].sample()\n    action_dict = {\n        \"action\": \"base_velocity_action\",\n        \"action_args\": params\n    }\n    observations.append(env.step(action_dict))\nvut.make_video(\n    observations,\n    \"third_rgb\",\n    \"color\",\n    \"robot_tutorial_video\",\n    open_vid=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Interacting with Habitat Environment (Python)\nDESCRIPTION: This snippet demonstrates how to create and interact with a Habitat environment using the Gym API. It creates an environment, resets it, steps through the simulation using random actions, renders the scene, and saves the video. It uses `imageio` for video writing and `habitat_sim.utils.viz_utils` for displaying the video if running in a notebook.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/habitat2_gym_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nenv = gym.make(\"HabitatRenderPick-v0\")\n\nvideo_file_path = os.path.join(output_path, \"example_interact.mp4\")\nvideo_writer = imageio.get_writer(video_file_path, fps=30)\n\ndone = False\nenv.reset()\nwhile not done:\n    obs, reward, done, info = env.step(env.action_space.sample())\n    video_writer.append_data(env.render(\"rgb_array\"))\n\nvideo_writer.close()\nif vut.is_notebook():\n    vut.display_video(video_file_path)\n\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Playing a Teaser Video with IPython\nDESCRIPTION: This snippet attempts to display an inline video using IPython's IFrame. It is wrapped in a try-except block to handle cases where IPython is not available. It is likely that this code snippet will not work, since the src value for the IFrame is not publicy available.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Play a teaser video\nfrom dataclasses import dataclass\n\nfrom habitat.config.default import get_agent_config\nfrom habitat.config.default_structured_configs import (\n    MeasurementConfig,\n    ThirdRGBSensorConfig,\n)\n\ntry:\n    from IPython.display import IFrame\n\n    # NOTE: this file is unreachable\n    IFrame(\n        src=\"https://drive.google.com/file/d/1ltrse38i8pnJPGAXlThylcdy8PMjUMKh/preview\",\n        width=640,\n        height=480,\n    )\n\nexcept Exception:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Evaluating VQA Model with Habitat Baselines\nDESCRIPTION: This code snippet shows how to evaluate the VQA model using Habitat Baselines. It runs the `habitat_baselines.run` module with a configuration file (`/eqa/il_vqa.yaml`) and sets `habitat_baselines.evaluate` to `True`. The evaluation results are stored in the `data/eqa/vqa/results/val` directory. The VQA model's performance is assessed based on the configurations in the yaml file.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=/eqa/il_vqa.yaml \\\n  habitat_baselines.evaluate=True\n```\n\n----------------------------------------\n\nTITLE: Launching Pick_throw_vr with Mouse/Keyboard\nDESCRIPTION: This command launches the Pick_throw_vr application with mouse and keyboard control. It directly executes the main script for the application.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/pick_throw_vr/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/pick_throw_vr/pick_throw_vr.py\n```\n\n----------------------------------------\n\nTITLE: Setup PointNav Task - Python\nDESCRIPTION: This snippet configures and initializes a PointNav task environment. It loads a configuration file, overrides specific parameters like maximum episode steps and shuffle option. It then creates a Habitat environment using the loaded configuration. The try-except block handles potential NameErrors when attempting to close a non-existent environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    config = habitat.get_config(\n        config_path=os.path.join(\n            dir_path,\n            \"habitat-lab/habitat/config/benchmark/nav/pointnav/pointnav_habitat_test.yaml\",\n        ),\n        overrides=[\n            \"habitat.environment.max_episode_steps=10\",\n            \"habitat.environment.iterator_options.shuffle=False\",\n        ],\n    )\n\n    try:\n        env.close()  # type: ignore[has-type]\n    except NameError:\n        pass\n    env = habitat.Env(config=config)\n```\n\n----------------------------------------\n\nTITLE: Printing Agent Position Observation (Python)\nDESCRIPTION: Prints the agent position obtained from the observation dictionary. Assumes the 'agent_position' key exists in the observations, indicating the custom sensor is working correctly.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n    print(obs[\"agent_position\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing Habitat Environment\nDESCRIPTION: This snippet initializes the Habitat environment for testing, setting up the configuration, agent configuration, and intrinsic parameters for the depth sensor. It also resets the environment and retrieves the initial agent state.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nimport quaternion\nimport matplotlib.pyplot as plt\n\nimport habitat\nfrom habitat.config import read_write\nfrom habitat.config.default import get_agent_config\n\nimport torch.nn.functional as F\nimport torch\nfrom torchvision.transforms import ToTensor\n\n# Set up the environment for testing\nconfig = habitat.get_config(config_paths=\"benchmark/nav/pointnav/pointnav_habitat_test.yaml\")\nwith read_write(config):\n    config.habitat.dataset.split = \"val\"\n    agent_config = get_agent_config(sim_config=config.habitat.simulator)\n    agent_config.sim_sensors.depth_sensor.normalize_depth = False\n\n# Intrinsic parameters, assuming width matches height. Requires a simple refactor otherwise\nW = agent_config.sim_sensors.depth_sensor.width\nH = agent_config.sim_sensors.depth_sensor.height\n\nassert(W == H)\nhfov = float(agent_config.sim_sensors.depth_sensor.hfov) * np.pi / 180.\n\n\nenv = habitat.Env(config=config)\n\n\nobs = env.reset()\ninitial_state = env._sim.get_agent_state(0)\ninit_translation = initial_state.position\ninit_rotation = initial_state.rotation\n```\n\n----------------------------------------\n\nTITLE: Generating a New Benchmark Episode\nDESCRIPTION: This command generates a new benchmark episode using the `run_episode_generator.py` script.  It requires a configuration file (`bench_config.yaml`) and specifies the number of episodes to generate (`--num-episodes 1`) and the output file (`--out data/ep_datasets/bench_scene.json.gz`).\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab2_bench/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython habitat/datasets/rearrange/run_episode_generator.py --run --config habitat/datasets/rearrange/configs/bench_config.yaml --num-episodes 1 --out data/ep_datasets/bench_scene.json.gz\n```\n\n----------------------------------------\n\nTITLE: Running Custom Sine Policy with RobotInterface\nDESCRIPTION: This snippet initializes the `RobotInterface`, resets the robot to its home position, creates an instance of the `MySinePolicy`, and runs the policy using `robot.send_torch_policy()`.  It retrieves robot metadata for setting up the controller and prints a message before execution.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/polymetis_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialize robot interface\nrobot = RobotInterface(\n    ip_address=\"localhost\",\n)\n\n\n# Reset\nrobot.go_home()\n\n# Create policy instance\nhz = robot.metadata.hz\ndefault_kq = torch.Tensor(robot.metadata.default_Kq)\ndefault_kqd = torch.Tensor(robot.metadata.default_Kqd)\npolicy = MySinePolicy(\n    time_horizon=5 * hz,\n    hz=hz,\n    magnitude=0.5,\n    period=2.0,\n    kq=default_kq,\n    kqd=default_kqd,\n)\n\n# Run policy\nprint(\"\\nRunning custom sine policy ...\\n\")\nstate_log = robot.send_torch_policy(policy)\n```\n\n----------------------------------------\n\nTITLE: Defining agent configurations\nDESCRIPTION: This snippet defines the configuration for an articulated agent (FetchRobot). It sets the URDF path, agent type, and specifies the sensors attached to the agent (third_rgb and head_rgb).  The configuration is stored in an `AgentConfig` object and added to a dictionary of agents.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Define the agent configuration\nmain_agent_config = AgentConfig()\nurdf_path = os.path.join(data_path, \"robots/hab_fetch/robots/hab_fetch.urdf\")\nmain_agent_config.articulated_agent_urdf = urdf_path\nmain_agent_config.articulated_agent_type = \"FetchRobot\"\n\n# Define sensors that will be attached to this agent, here a third_rgb sensor and a head_rgb.\n# We will later talk about why we are giving the sensors these names\nmain_agent_config.sim_sensors = {\n    \"third_rgb\": ThirdRGBSensorConfig(),\n    \"head_rgb\": HeadRGBSensorConfig(),\n}\n\n# We create a dictionary with names of agents and their corresponding agent configuration\nagent_dict = {\"main_agent\": main_agent_config}\n```\n\n----------------------------------------\n\nTITLE: Querying Multi-Agent Observations\nDESCRIPTION: This code segment shows how to query observations from a multi-agent environment. It resets the environment, retrieves observations for each agent (identified by keys in the observation dictionary), and then plots these observations using matplotlib. The loop iterates through the keys and uses them to access agent specific information. Dependencies: matplotlib, habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nobservations = env.reset()\n_, ax = plt.subplots(1,len(observations.keys()))\n\nfor ind, name in enumerate(observations.keys()):\n    ax[ind].imshow(observations[name])\n    ax[ind].set_axis_off()\n    ax[ind].set_title(name)\n```\n\n----------------------------------------\n\nTITLE: Training EQA-CNN-Pretrain Model with Habitat Baselines\nDESCRIPTION: This code snippet shows how to train the EQA-CNN-Pretrain model using Habitat Baselines. It executes the `habitat_baselines.run` module with a specific configuration file (`eqa/il_eqa_cnn_pretrain.yaml`). The training process uses the specified configuration to set up the model, environment, and training parameters. Training checkpoints are stored by default in the `data/eqa/eqa_cnn_pretrain/checkpoints` directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=eqa/il_eqa_cnn_pretrain.yaml\n```\n\n----------------------------------------\n\nTITLE: Define Habitat Configuration (YAML)\nDESCRIPTION: This YAML configuration defines the environment, task, simulator, and dataset settings for the Habitat rearrangement task. It specifies the task type, reward functions, action spaces, and simulator parameters, including agent properties, physics settings, and dataset paths. It also defines measurements such as distance to target object, articulated agent force, and success metrics.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\n# @package _global_\n\ndefaults:\n  - /habitat: habitat_config_base\n  - /habitat/simulator/agents@habitat.simulator.agents.main_agent: agent_base\n  - /habitat/simulator/sim_sensors@habitat.simulator.agents.main_agent.sim_sensors.head_rgb_sensor: head_rgb_sensor\n  - /habitat/task: task_config_base\n  - /habitat/task/actions:\n    - arm_action\n    - base_velocity\n  - /habitat/task/measurements:\n    - articulated_agent_force\n    - force_terminate\n    - distance_to_target_object\n    - nav_pick_reward\n    - nav_pick_success\n  - /habitat/task/lab_sensors:\n    - target_start_sensor\n    - joint_sensor\n  - /habitat/dataset/rearrangement: replica_cad\n\nhabitat:\n  environment:\n    # Number of steps within an episode.\n    max_episode_steps: 200\n  task:\n    type: RearrangeDemoNavPickTask-v0\n    # Measurements\n    measurements:\n      distance_to_target_object:\n        type: \"DistanceToTargetObject\"\n      articulated_agent_force:\n        type: \"RobotForce\"\n        min_force: 20.0\n      force_terminate:\n        type: \"ForceTerminate\"\n        # Maximum amount of allowed force in Newtons.\n        max_accum_force: 5000.0\n      nav_pick_reward:\n        type: \"NavPickReward\"\n        scaling_factor: 0.1\n        # General Rearrange Reward config\n        constraint_violate_pen: 10.0\n        force_pen: 0.001\n        max_force_pen: 1.0\n        force_end_pen: 10.0\n      nav_pick_success:\n        type: \"NavPickSuccess\"\n    actions:\n      # Define the action space.\n      arm_action:\n        type: \"ArmAction\"\n        arm_controller: \"ArmRelPosAction\"\n        grip_controller: \"MagicGraspAction\"\n        arm_joint_dimensionality: 7\n        grasp_thresh_dist: 0.15\n        disable_grip: False\n        delta_pos_limit: 0.0125\n        ee_ctrl_lim: 0.015\n      base_velocity:\n        type: \"BaseVelAction\"\n        lin_speed: 12.0\n        ang_speed: 12.0\n        allow_dyn_slide: True\n        allow_back: True\n  simulator:\n    type: RearrangeSim-v0\n    additional_object_paths:\n      - \"data/objects/ycb/configs/\"\n    debug_render: False\n    concur_render: False\n    auto_sleep: False\n    agents:\n      main_agent:\n        height: 1.5\n        is_set_start_state: False\n        radius: 0.1\n        sim_sensors:\n          head_rgb_sensor:\n            height: 128\n            width: 128\n        start_position: [0, 0, 0]\n        start_rotation: [0, 0, 0, 1]\n        articulated_agent_urdf: ./data/robots/hab_fetch/robots/hab_fetch.urdf\n        articulated_agent_type: \"FetchRobot\"\n\n    # Agent setup\n    # ARM_REST: [0.6, 0.0, 0.9]\n    ctrl_freq: 120.0\n    ac_freq_ratio: 4\n\n    # Grasping\n    hold_thresh: 0.09\n    grasp_impulse: 1000.0\n\n    habitat_sim_v0:\n      allow_sliding: True\n      enable_physics: True\n      gpu_device_id: 0\n      gpu_gpu: False\n      physics_config_file: ./data/default.physics_config.json\n  dataset:\n    type: RearrangeDataset-v0\n    split: train\n    # The dataset to use. Later we will generate our own dataset.\n    data_path: data/datasets/replica_cad/rearrange/v2/{split}/all_receptacles_10k_1k.json.gz\n    scenes_dir: \"data/replica_cad/\"\n\n```\n\n----------------------------------------\n\nTITLE: Set Random Seeds - Python\nDESCRIPTION: This snippet sets the random seeds for the random, numpy, and Habitat environments to ensure reproducibility of experiments. It retrieves the seed value and total number of training steps from input parameters and sets them in the configuration. Setting random seeds is essential for comparing results and debugging RL training runs.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    seed = \"42\"  # @param {type:\"string\"}\n    steps_in_thousands = \"10\"  # @param {type:\"string\"}\n\n    with habitat.config.read_write(config):\n        config.habitat.seed = int(seed)\n        config.habitat_baselines.total_num_steps = int(steps_in_thousands)\n        config.habitat_baselines.log_interval = 1\n\n    random.seed(config.habitat.seed)\n    np.random.seed(config.habitat.seed)\n```\n\n----------------------------------------\n\nTITLE: RL Training with Skill Specification (Shell)\nDESCRIPTION: This shell command trains different skills by specifying the `benchmark/rearrange` argument. The `skill_name` can be `close_cab`, `close_fridge`, `open_fridge`, `pick`, `place`, or `nav_to_obj`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbenchmark/rearrange=skill_name\n```\n\n----------------------------------------\n\nTITLE: Testing Humanoid Arm Movement (Habitat)\nDESCRIPTION: Runs an interactive demo in Habitat where the humanoid avatar's arms move randomly. It disables inverse kinematics and enables the humanoid control using the specified configuration file.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython examples/interactive_play.py --never-end --disable-inverse-kinematics --control-humanoid --cfg benchmark/rearrange/play_human.yaml\n```\n\n----------------------------------------\n\nTITLE: Evaluating NAV (PACMAN) Model with Habitat Baselines\nDESCRIPTION: This code snippet illustrates how to evaluate the NAV (PACMAN) model using Habitat Baselines.  It executes the `habitat_baselines.run` module using a specific configuration file (`eqa/il_pacman_nav.yaml`) and setting the `habitat_baselines.evaluate` flag to `True`.  Evaluation results are stored in the `data/eqa/nav/results/val` directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=eqa/il_pacman_nav.yaml \\\n  habitat_baselines.evaluate=True\n```\n\n----------------------------------------\n\nTITLE: Training VQA Model with Habitat Baselines\nDESCRIPTION: This code snippet demonstrates how to train the Visual Question Answering (VQA) model using Habitat Baselines. It executes the `habitat_baselines.run` module with a specific configuration file (`eqa/il_vqa.yaml`).  The training process is set up according to parameters specified in the configuration file. Training checkpoints are by default stored in `data/eqa/vqa/checkpoints`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/il/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=eqa/il_vqa.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing the scene\nDESCRIPTION: This snippet initializes the simulation environment by calling the `init_rearrange_sim` function, passing in the dictionary of agent configurations.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsim = init_rearrange_sim(agent_dict)\n```\n\n----------------------------------------\n\nTITLE: Initializing RearrangeSim\nDESCRIPTION: This function initializes the `RearrangeSim` simulator with the provided agent configurations. It creates the simulator, initializes the agents, and adds an extra camera sensor for third-person recording.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef init_rearrange_sim(agent_dict):\n    # Start the scene config\n    sim_cfg = make_sim_cfg(agent_dict)    \n    cfg = OmegaConf.create(sim_cfg)\n    \n    # Create the scene\n    sim = RearrangeSim(cfg)\n\n    # This is needed to initialize the agents\n    sim.agents_mgr.on_new_scene()\n\n    # For this tutorial, we will also add an extra camera that will be used for third person recording.\n    camera_sensor_spec = habitat_sim.CameraSensorSpec()\n    camera_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n    camera_sensor_spec.uuid = \"scene_camera_rgb\"\n\n    # TODO: this is a bit dirty but I think its nice as it shows how to modify a camera sensor...\n    sim.add_sensor(camera_sensor_spec, 0)\n\n    return sim\n```\n\n----------------------------------------\n\nTITLE: Inspecting Observation Space (Python)\nDESCRIPTION: This snippet demonstrates how to inspect the observation space of different Habitat environments. It creates two environments, `HabitatPick-v0` (dictionary observation space) and `HabitatReachState-v0` (array observation space), and prints their observation spaces. It closes each environment after inspection.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/habitat2_gym_tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Dictionary observation space\nenv = gym.make(\"HabitatPick-v0\")\nprint(\n    \"Pick observation space\",\n    {k: v.shape for k, v in env.observation_space.spaces.items()},\n)\nenv.close()\n\n# Array observation space\nenv = gym.make(\"HabitatReachState-v0\")\nprint(\"Reach observation space\", env.observation_space)\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Snapping Object to Robot Arm\nDESCRIPTION: This snippet demonstrates how to attach a specified object to the robot arm using the grasp manager. It retrieves the grasp manager for the first agent and then uses the `snap_to_obj` method to attach the object with the given ID to the arm. The simulation is then stepped forward and the resulting observation is displayed.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# We use a grasp manager to interact with the object:\nagent_id = 0\ngrasp_manager = sim.agents_mgr[agent_id].grasp_mgrs[0]\ngrasp_manager.snap_to_obj(obj_id)\nobs = sim.step({})\nplt.imshow(obs[\"head_rgb\"])\n```\n\n----------------------------------------\n\nTITLE: Querying Articulated and Rigid Objects\nDESCRIPTION: This snippet demonstrates how to access and query articulated and rigid objects within the simulation. It retrieves the ArticulatedObjectManager and RigidObjectManager from the simulator, and then iterates through the objects managed by each manager, printing their handles and IDs.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\naom = sim.get_articulated_object_manager()\nrom = sim.get_rigid_object_manager()\n\n# We can query the articulated and rigid objects\n\nprint(\"List of articulated objects:\")\nfor handle, ao in aom.get_objects_by_handle_substring().items():\n    print(handle, \"id\", aom.get_object_id_by_handle(handle))\n\nprint(\"\\nList of rigid objects:\")\nobj_ids = []\nfor handle, ro in rom.get_objects_by_handle_substring().items():\n    if ro.awake:\n        print(handle, \"id\", ro.object_id)\n        obj_ids.append(ro.object_id)\n```\n\n----------------------------------------\n\nTITLE: Converting SMPL-X Motion Data (Habitat)\nDESCRIPTION: Converts motion data from SMPL-X format (e.g., from AMASS or Motion Diffusion Models) to a format usable by the SequentialPoseController in Habitat. It uses the `MotionConverterSMPLX` helper class to perform the conversion, generating a `.pkl` file as output.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nPATH_TO_URDF = \"data/humanoids/humanoid_data/female_2/female_2.urdf\"\nPATH_TO_MOTION_NPZ = \"data/humanoids/humanoid_data/walk_motion/CMU_10_04_stageii.npz\"\nconvert_helper = MotionConverterSMPLX(urdf_path=PATH_TO_URDF)\nconvert_helper.convert_motion_file(\n    motion_path=PATH_TO_MOTION_NPZ,\n    output_path=PATH_TO_MOTION_NPZ.replace(\".npz\", \"\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment with Actions\nDESCRIPTION: This snippet initializes the Habitat environment with a set of pre-defined actions, which include a magic grasp action, a base velocity action, and an oracle navigation action.  The `ArmActionConfig`, `BaseVelocityActionConfig`, and `OracleNavActionConfig` are action specifications to configure the environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\naction_dict = {\n    \"oracle_magic_grasp_action\": ArmActionConfig(type=\"MagicGraspAction\"),\n    \"base_velocity_action\": BaseVelocityActionConfig(),\n    \"oracle_coord_action\": OracleNavActionConfig(type=\"OracleNavCoordinateAction\", spawn_max_dist_to_obj=1.0)\n}\nenv = init_rearrange_env(agent_dict, action_dict)\n```\n\n----------------------------------------\n\nTITLE: Running Basic Viewer HITL App with Episode Filter\nDESCRIPTION: This command launches the Basic Viewer HITL application, filtering episodes based on the provided criteria. The `habitat_hitl.episodes_filter` parameter specifies which episodes to include, using a syntax of individual indices, ranges, and stepped ranges. This example uses Python to execute the script.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/basic_viewer/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/basic_viewer/basic_viewer.py habitat_hitl.episodes_filter='0 2 4 10:15 1000:4000:500'\n```\n\n----------------------------------------\n\nTITLE: Initializing Humanoid Sequence Pose Controller\nDESCRIPTION: This code snippet initializes the HumanoidSeqPoseController with a motion file and sets the reference pose for the controller. The base transformation to make the humanoid motion relative to the current agent.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nenv.reset()\nmotion_path = \"data/humanoids/humanoid_data/walk_motion/CMU_10_04_stageii.pkl\" \n# We define here humanoid controller\nhumanoid_controller = HumanoidSeqPoseController(motion_path)\n\n\n# Because we want the humanoid controller to generate a motion relative to the current agent, we need to set\n# the reference pose\nhumanoid_controller.reset(env.sim.articulated_agent.base_transformation)\nhumanoid_controller.apply_base_transformation(env.sim.articulated_agent.base_transformation)\n```\n\n----------------------------------------\n\nTITLE: Testing Sequential Pose Controller (Habitat)\nDESCRIPTION: Executes a pytest test case to verify the functionality of the SequentialPoseController for humanoid animation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npython -m pytest test/test_humanoid.py:test_humanoid_seqpose_controller\n```\n\n----------------------------------------\n\nTITLE: Update Config Object via Code (Python)\nDESCRIPTION: This code snippet demonstrates how to update a Habitat config object via code using the `read_write` context manager. It modifies the `concur_render` setting and updates simulator sensors with a new `ThirdRGBSensorConfig`. It imports `read_write` from `habitat.config.read_write`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom habitat.config.read_write import read_write\n\nwith read_write(config):\n    config.habitat.simulator.concur_render = False\n    agent_config = get_agent_config(config.habitat.simulator)\n    agent_config.sim_sensors.update(\n        {\"third_rgb_sensor\": ThirdRGBSensorConfig(height=512, width=512)}\n    )\n```\n\n----------------------------------------\n\nTITLE: Registering Measurement Configurations with ConfigStore\nDESCRIPTION: This code snippet registers the measurement configurations with the ConfigStore, which is part of the Hydra configuration management system. This allows these configurations to be used and overridden in configuration files.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncs = ConfigStore.instance()\ncs.store(\n    package=\"habitat.task.measurements.distance_to_target_object\",\n    group=\"habitat/task/measurements\",\n    name=\"distance_to_target_object\",\n    node=DistanceToTargetObjectMeasurementConfig,\n)\ncs.store(\n    package=\"habitat.task.measurements.nav_pick_reward\",\n    group=\"habitat/task/measurements\",\n    name=\"nav_pick_reward\",\n    node=NavPickRewardMeasurementConfig,\n)\ncs.store(\n    package=\"habitat.task.measurements.nav_pick_success\",\n    group=\"habitat/task/measurements\",\n    name=\"nav_pick_success\",\n    node=NavPickSuccessMeasurementConfig,\n)\n```\n\n----------------------------------------\n\nTITLE: Download HITL assets using python\nDESCRIPTION: This script downloads required assets for the example HITL applications. It uses the `habitat_sim.utils.datasets_download` module to fetch datasets like `hab3-episodes`, `habitat_humanoids`, `hab_spot_arm`, `ycb`, and `hssd-hab` and stores them in the `data/` directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-hitl/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m habitat_sim.utils.datasets_download \\\n    --uids hab3-episodes habitat_humanoids hab_spot_arm ycb hssd-hab \\\n    --data-path data/\n```\n\n----------------------------------------\n\nTITLE: Querying Agent Positions\nDESCRIPTION: This snippet demonstrates how to query the position of an agent in a multi-agent Habitat environment. It uses the `agents_mgr` property of the simulation to access a specific agent by its index and then retrieves the agent's `articulated_agent` object to access its properties, such as position. The simulation stores agents in a structure where the agent index is used to obtain the agent specific object.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# To query the agent positions, we need to use `agents_mgr[agent_index]` articulated_agent\nenv.sim.agents_mgr[1].articulated_agent\n```\n\n----------------------------------------\n\nTITLE: Verifying Gradient Reduction with pytest in Python\nDESCRIPTION: This code snippet provides the command to run a pytest unit test to verify that gradient reduction is working correctly in the DD-PPO implementation. This is crucial because issues with gradient reduction can occur due to the non-standard interface between DD-PPO and PyTorch's DistributedDataParallel.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/rl/ddppo/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npytest test/test_ddppo_reduce.py\n```\n\n----------------------------------------\n\nTITLE: Initialize and Train Trainer - Python\nDESCRIPTION: This snippet initializes and trains the RL agent. It retrieves the trainer class based on the configuration, initializes the trainer with the configuration, and then calls the `train` method to start the training process. This is the core part of the RL training loop, where the agent learns to navigate the environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    trainer_init = baseline_registry.get_trainer(\n        config.habitat_baselines.trainer_name\n    )\n    trainer = trainer_init(config)\n    trainer.train()\n```\n\n----------------------------------------\n\nTITLE: Generating Humanoid with Custom Parameters (Habitat)\nDESCRIPTION: Executes a Blender script to generate a new humanoid body with user-specified parameters, like output directory or a JSON file containing beta parameters. It leverages the SMPL-X Blender add-on to create and export the humanoid model, allowing the customization of parameters.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\npath_to_blender -b -P 'scripts/export_smplx_bodies.py' -- --output-dir your_output_dir --body-file your_body_file\n```\n\n----------------------------------------\n\nTITLE: Batch Renderer Configuration - YAML\nDESCRIPTION: This snippet presents the required configuration fields for training with the batch renderer in YAML format. It sets the trainer name to 'ppo', enables the batch renderer, enables gfx replay save, and disables default renderer creation and concurrent rendering.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/core/batch_rendering/README.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nhabitat_baselines.trainer_name: \"ppo\"\nhabitat.simulator.renderer.enable_batch_renderer: True\nhabitat.simulator.habitat_sim_v0.enable_gfx_replay_save: True\nhabitat.simulator.create_renderer: False\nhabitat.simulator.concur_render: False\n```\n\n----------------------------------------\n\nTITLE: TP-SRL Evaluation (Shell)\nDESCRIPTION: This shell command evaluates the Task-Planning with Skills RL (TP-SRL) baseline on a specific HAB task (e.g., `tidy_house`). It uses the `rearrange/tp_srl.yaml` configuration and sets `habitat_baselines.evaluate=True` to run in evaluation mode.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -u -m habitat_baselines.run \\\n        --config-name=rearrange/tp_srl.yaml \\\n        habitat_baselines.evaluate=True \\\n        benchmark/rearrange=tidy_house\n```\n\n----------------------------------------\n\nTITLE: Launching Rearrange HITL application\nDESCRIPTION: This command launches the Rearrange HITL application using the provided python script. It assumes that the user is in the root directory of the habitat-lab repository or has appropriately configured their PYTHONPATH. No dependencies are explicitly listed, but it requires a functional Habitat-Lab environment to be installed and configured.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/rearrange/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/rearrange/rearrange.py\n```\n\n----------------------------------------\n\nTITLE: Write Habitat Config to YAML file (Python)\nDESCRIPTION: This Python code snippet writes the Habitat configuration defined in the preceding YAML to a file named `nav_pick_demo.yaml`. This configuration file is then used to instantiate and run the Habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nnav_pick_cfg_path = os.path.join(data_path, \"nav_pick_demo.yaml\")\nwith open(nav_pick_cfg_path, \"w\") as f:\n    f.write(cfg_txt)\n```\n\n----------------------------------------\n\nTITLE: Displaying sensor observations\nDESCRIPTION: This snippet displays the sensor observations as images using Matplotlib. It iterates through the observation dictionary and plots each sensor observation in a separate subplot. It also turns off the axis and sets the title of each subplot to the sensor name.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n_, ax = plt.subplots(1,len(observations.keys()))\n\nfor ind, name in enumerate(observations.keys()):\n    ax[ind].imshow(observations[name])\n    ax[ind].set_axis_off()\n    ax[ind].set_title(name)\n```\n\n----------------------------------------\n\nTITLE: Base PointNav Configuration (YAML)\nDESCRIPTION: Defines the base PointNav configuration, including environment settings, simulator agents, and sensor configurations.  It sets maximum episode steps and configures RGB and depth sensors.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n# @package _global_\n\ndefaults:\n  - /habitat: habitat_config_base\n  - /habitat/task: pointnav\n  - /habitat/simulator/agents:\n    - rgbd_agent\n  - _self_\n\nhabitat:\n  environment:\n    max_episode_steps: 500\n  simulator:\n    agents:\n      rgbd_agent:\n        sim_sensors:\n          rgb_sensor:\n            width: 256\n            height: 256\n          depth_sensor:\n            width: 256\n            height: 256\n```\n\n----------------------------------------\n\nTITLE: Download Example Scene - Python\nDESCRIPTION: This snippet uses the habitat_sim.utils.datasets_download module to download an example scene (mp3d_example_scene) for Habitat Lab. It specifies the data path where the scene should be downloaded and uses the --no-replace flag to prevent overwriting existing files. Downloading the dataset is a prerequisite for running experiments in the Habitat environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!python -m habitat_sim.utils.datasets_download --uids mp3d_example_scene --data-path {data_path} --no-replace\n```\n\n----------------------------------------\n\nTITLE: Habitat Configuration Values Example\nDESCRIPTION: This YAML snippet showcases the Habitat configuration structure, focusing on the simulator and agent settings. It demonstrates how to define sensor configurations, including RGB and depth sensors, with specific width and height values for an RGBD agent.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nhabitat:\n  environment:\n    max_episode_steps: 500\n  simulator:\n    agents:\n      rgbd_agent:\n        sim_sensors:\n          rgb_sensor:\n            width: 256\n            height: 256\n          depth_sensor:\n            width: 256\n            height: 256\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Interactive Testing\nDESCRIPTION: This code snippet demonstrates how to install the necessary dependencies for interactive testing of the Stretch robot, including Pygame for visualization and PyBullet for inverse kinematics. It uses pip to install specific versions of these packages to ensure compatibility.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/robots/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pygame==2.0.1 pybullet==3.0.4\n```\n\n----------------------------------------\n\nTITLE: Training PPO Agent\nDESCRIPTION: This command trains a PPO agent using the specified configuration file (ppo_pointnav_example.yaml). It executes the habitat_baselines.run module using python.  The `-u` flag forces the interpreter to flush the stdout stream after each write.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=pointnav/ppo_pointnav_example.yaml\n```\n\n----------------------------------------\n\nTITLE: Habitat Pick Task Example - Python\nDESCRIPTION: This Python code snippet demonstrates how to load a pre-specified RearrangePick task and robot, reset the environment, and step through it with random actions. It uses the Habitat Gym interface.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport gym\nimport habitat.gym\n\n# Load embodied AI task (RearrangePick) and a pre-specified virtual robot\nenv = gym.make(\"HabitatRenderPick-v0\")\nobservations = env.reset()\n\nterminal = False\n\n# Step through environment with random actions\nwhile not terminal:\n    observations, reward, terminal, info = env.step(env.action_space.sample())\n```\n\n----------------------------------------\n\nTITLE: Scene Receptacle Filter File Configuration - Python\nDESCRIPTION: This snippet shows how to specify a scene receptacle filter file using the 'user_defined' field. The 'scene_filter_file' key points to a file containing filter strings for receptacles in the scene. This allows for filtering receptacles based on certain criteria.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/metadata-taxonomy.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    \"user_defined\": {\n        \"scene_filter_file\": \"scene_filter_files/102344022.rec_filter.json\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Interactive Play Script Execution (Shell)\nDESCRIPTION: This shell command runs the interactive play script, allowing users to test Habitat environments with keyboard and mouse control. The `--never-end` flag keeps the environment running indefinitely. Requires PyGame to be installed.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython examples/interactive_play.py --never-end\n```\n\n----------------------------------------\n\nTITLE: Initializing HumanoidRearrangeController in Python\nDESCRIPTION: This snippet shows how to initialize a `HumanoidRearrangeController` with a specific motion file.  The motion file path is defined, and then the controller is instantiated using that path.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# As before, we first define the controller, here we use a special motion file we provide for each agent.\nmotion_path = \"data/hab3_bench_assets/humanoids/female_0/female_0_motion_data_smplx.pkl\" \n# We define here humanoid controller\nhumanoid_controller = HumanoidRearrangeController(motion_path)\n```\n\n----------------------------------------\n\nTITLE: Listing Spot URDF Files\nDESCRIPTION: This code snippet uses the `ls` command to list all the URDF (Universal Robot Description Format) files located in the specified directory for the Spot robot. It helps verify the presence of robot description files after downloading the assets.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nls data/robots/hab_spot_arm/urdf\n```\n\n----------------------------------------\n\nTITLE: Adding Navigation Measures to Habitat Configuration\nDESCRIPTION: This code snippet demonstrates how to add measures to a Habitat configuration file using the `defaults` list. Measures provide data about the environment and agent during each step of a task, such as step count and distance to the goal.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/CONFIG_KEYS.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ndefaults:\n  - /habitat/task/measurements:\n    - articulated_agent_force\n    - force_terminate\n```\n\n----------------------------------------\n\nTITLE: Habitat Citation (BibTeX)\nDESCRIPTION: This BibTeX entry provides the citation information for the Habitat platform, enabling users to properly acknowledge its use in their research.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/quickstart.rst#_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{habitat19arxiv,\n  title =   {Habitat: A Platform for Embodied AI Research},\n  author =  {Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh and Dhruv Batra},\n  journal = {arXiv preprint arXiv:1904.01201},\n  year =    {2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Validating Magnum Installation\nDESCRIPTION: Activates the Conda environment and then attempts to import modules from the Magnum library. If no error is raised, the Magnum installation is considered successful. Requires that the Magnum Python libraries are correctly installed and the Conda environment is activated.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda activate magnum\npython -c \"from magnum import math, meshtools, scenetools, trade\"\n```\n\n----------------------------------------\n\nTITLE: Launch Sim Viewer Application (habitat-lab)\nDESCRIPTION: This command launches the sim_viewer.py application, which provides a basic viewer for interacting with the habitat-sim Simulator. It should be run from the root habitat-lab directory to ensure proper module resolution and resource loading.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/sim_viewer/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/sim_viewer/sim_viewer.py\n```\n\n----------------------------------------\n\nTITLE: Slurm Batch Script Configuration (Habitat)\nDESCRIPTION: This bash script configures a Slurm job for running Habitat training. It allocates resources such as GPUs, CPUs, memory, and nodes. It is essential for distributed training in a cluster environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n#SBATCH --gres gpu:4\n#SBATCH --cpus-per-task 10\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 4\n#SBATCH --mem-per-cpu=6GB\n```\n\n----------------------------------------\n\nTITLE: Write Dataset Config to YAML file (Python)\nDESCRIPTION: This Python code snippet writes the dataset configuration defined in the preceding YAML to a file named `nav_pick_dataset.yaml`. This configuration file is then used by the episode generator.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat2_Quickstart.ipynb#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nnav_pick_cfg_path = os.path.join(data_path, \"nav_pick_dataset.yaml\")\nwith open(nav_pick_cfg_path, \"w\") as f:\n    f.write(dataset_cfg_txt)\n```\n\n----------------------------------------\n\nTITLE: Checking Python Typing with Mypy\nDESCRIPTION: This command uses mypy to perform static type checking on the Python code, ensuring API consistency and catching potential type-related errors before runtime. It helps maintain code quality and prevent unexpected behavior.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmypy . --ignore-missing-imports\n```\n\n----------------------------------------\n\nTITLE: Load Baselines Config - Python\nDESCRIPTION: This snippet loads a baseline configuration for RL training. It uses `get_baselines_config` to load a configuration file (ppo_pointnav_example.yaml) for the PointNav task using the PPO algorithm. This configuration contains hyperparameters and settings for training the agent.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    config = get_baselines_config(\"pointnav/ppo_pointnav_example.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Starting Habitat Docker Session\nDESCRIPTION: This command starts an interactive bash session inside the Habitat Docker container.  It uses the NVIDIA runtime to enable GPU support and mounts the necessary volumes for data access. The -it flags allow for interactive terminal access.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --runtime=nvidia -it fairembodied/habitat-challenge:testing_2022_habitat_base_docker\n```\n\n----------------------------------------\n\nTITLE: Launching Habitat Robot with Polymetis\nDESCRIPTION: This command launches the Habitat robot client with Polymetis.  It specifies the robot client as `habitat_sim`, sets the absolute path to the scene file, disables real-time execution, and enables the GUI. The `habitat_scene_path` needs to be an absolute path.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/polymetis_example.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlaunch_robot.py robot_client=habitat_sim habitat_scene_path=/PATH/TO/scene use_real_time=false gui=true\n```\n\n----------------------------------------\n\nTITLE: Testing PPO Agent\nDESCRIPTION: This command tests a PPO agent using the specified configuration file (ppo_pointnav_example.yaml). It sets the `evaluate` flag to `True` to activate the evaluation mode within the habitat_baselines.run module.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=pointnav/ppo_pointnav_example.yaml \\\n  habitat_baselines.evaluate=True\n```\n\n----------------------------------------\n\nTITLE: Configuring Composite Files - Command Line - Python\nDESCRIPTION: This snippet shows how to specify composite files for the batch renderer from the command line. It uses the `habitat-baselines/habitat_baselines/run.py` script and overrides the `habitat.simulator.renderer.composite_files` configuration.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/core/batch_rendering/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nhabitat-baselines/habitat_baselines/run.py habitat.simulator.renderer.composite_files=[path/to/composite_1.gltf, path/to/composite_2.gltf]\n```\n\n----------------------------------------\n\nTITLE: Taking sensor observations\nDESCRIPTION: This snippet demonstrates how to obtain sensor observations from the simulation environment using the `get_sensor_observations()` method. The observations are stored in a dictionary, where keys are the names of the sensors and values are the sensor data.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nobservations = sim.get_sensor_observations()\nprint(observations.keys())\n```\n\n----------------------------------------\n\nTITLE: Visualizing Transformed Image\nDESCRIPTION: This code snippet visualizes the transformation by warping the second image into the first image's view using the estimated transformation. It creates a sampler, warps the image using `F.grid_sample`, and then displays the original images, the warped image, and the difference between the warped and ground truth images using `matplotlib`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create sampler\nsampler = torch.Tensor(xys_newimg).view(2, W, W).permute(1,2,0).unsqueeze(0)\n\n# Create generated image\nimg1_tensor = ToTensor()(rgbs[0]).unsqueeze(0)\nimg2_tensor = ToTensor()(rgbs[1]).unsqueeze(0)\nimg2_warped = F.grid_sample(img2_tensor, sampler)\n\n# Visualise\nplt.figure(figsize=(10,10))\nax1 = plt.subplot(221)\nax1.imshow(img1_tensor.squeeze().permute(1,2,0))\nax1.set_title(\"View 1\", fontsize='large')\nax1.axis('off')\nax1 = plt.subplot(222)\nax1.imshow(img2_tensor.squeeze().permute(1,2,0))\nax1.set_title(\"View 2\", fontsize='large')\nax1.axis('off')\nax1 = plt.subplot(223)\nplt.imshow(img2_warped.squeeze().permute(1,2,0))\nax1.set_title(\"View 2 warped into View 1 \\n according to the estimated transformation\", fontsize='large')\nax1.axis('off')\nax1 = plt.subplot(224)\nax1.imshow(np.abs(img2_warped.squeeze().permute(1,2,0) - img1_tensor.squeeze().permute(1,2,0)))\nax1.set_title(\"Difference between warped \\n and ground truth images\", fontsize='large')\nax1.axis('off')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Training HRL Policy\nDESCRIPTION: This command trains a Hierarchical Reinforcement Learning (HRL) policy using the rearrange/rl_hierarchical.yaml configuration file.  The `-u` flag forces the interpreter to flush the stdout stream after each write.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=rearrange/rl_hierarchical.yaml\n```\n\n----------------------------------------\n\nTITLE: Adding Lab Sensors to Habitat Configuration\nDESCRIPTION: This snippet shows how to add lab sensors to a Habitat configuration file. Lab sensors provide non-rendered sensor observations, such as object goal information and compass readings, which can be used for Navigation tasks.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/CONFIG_KEYS.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ndefaults:\n  - /habitat/task/lab_sensors:\n    - objectgoal_sensor\n    - compass_sensor\n```\n\n----------------------------------------\n\nTITLE: Overriding Configuration via Command Line - Bash\nDESCRIPTION: This bash command demonstrates how to override specific configuration values directly from the command line when running a Habitat baseline.  It overrides `habitat.environment.max_episode_steps` to 250 and `habitat_baselines.total_num_steps` to 100.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run --config-name=pointnav/ddppo_pointnav.yaml \\\nhabitat.environment.max_episode_steps=250 \\\nhabitat_baselines.total_num_steps=100\n```\n\n----------------------------------------\n\nTITLE: Launching XR Reader Application in Python\nDESCRIPTION: This command launches the XR reader application using Python. It assumes the command is executed from the root `habitat-lab` directory. The script `xr_reader.py` is responsible for handling the XR input and displaying it.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/experimental/xr_reader/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/experimental/xr_reader/xr_reader.py\n```\n\n----------------------------------------\n\nTITLE: Object States Configuration - Python\nDESCRIPTION: This snippet illustrates the 'object_states' configuration within the 'user_defined' field. This sub-configuration holds fields related to ObjectStateMachine and ObjectStateSpec logic. The exact taxonomy is in flux and should be considered reserved.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/metadata-taxonomy.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    \"user_defined\": {\n        \"object_states\": {\n\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Downloading Habitat Datasets - Python\nDESCRIPTION: These commands use the `habitat_sim.utils.datasets_download` utility to download test 3D scenes and point-goal navigation episodes. These datasets are used for testing the functionality of Habitat-Sim and Habitat-Lab.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path data/\npython -m habitat_sim.utils.datasets_download --uids habitat_test_pointnav_dataset --data-path data/\n```\n\n----------------------------------------\n\nTITLE: Importing Habitat Gym Environment (Python)\nDESCRIPTION: This snippet imports the necessary libraries to use Habitat 2.0 environments as standard gym environments.  It imports `gym` and `habitat.gym` which allows to define and interact with habitat environments.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/habitat2_gym_tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The ONLY two lines you need to add to start importing Habitat 2.0 Gym environments.\nimport gym\n\n# flake8: noqa\nimport habitat.gym\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Creation and Activation - Bash\nDESCRIPTION: This code snippet creates a new conda environment named 'habitat' with Python 3.9 and CMake 3.14.0, then activates the environment. This isolates the project dependencies.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# We require python>=3.9 and cmake>=3.14\nconda create -n habitat python=3.9 cmake=3.14.0\nconda activate habitat\n```\n\n----------------------------------------\n\nTITLE: Copying Pre-generated Benchmark Episode\nDESCRIPTION: This command copies a pre-generated benchmark episode from the dataset to the episode datasets directory. This allows users to use a pre-configured scene for their benchmarking efforts, instead of generating one themselves.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab2_bench/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncp data/hab2_bench_assets/bench_scene.json.gz data/ep_datasets/\n```\n\n----------------------------------------\n\nTITLE: Copy Benchmark Episodes (Shell)\nDESCRIPTION: This shell command copies the benchmark episode data from the downloaded assets folder to the `data/ep_datasets/` folder. This step prepares the data for running the benchmark.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncp data/hab2_bench_assets/bench_scene.json.gz data/ep_datasets/\n```\n\n----------------------------------------\n\nTITLE: Reading Habitat Configuration - Python\nDESCRIPTION: This code snippet demonstrates how to read a Habitat configuration file using the `habitat.get_config` function. It imports the habitat package and then uses the specified function to load the configuration from the specified YAML file.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport habitat\n\nconfig = habitat.get_config(\"benchmark/nav/pointnav/pointnav_gibson.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Enabling overlapped rollouts and learning\nDESCRIPTION: This command enables overlapping experience collection and learning in VER. This can improve performance in CPU-bound environments but may harm sample efficiency and increase memory usage.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/rl/ver/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython .... \\\n  habitat_baselines.rl.ver.overlap_rollouts_and_learn=True\n```\n\n----------------------------------------\n\nTITLE: AssetSource Configuration Example\nDESCRIPTION: An example of configuring an AssetSource to process all '.glb' files within the 'humanoids' directory and its subdirectories. The assets are copied without decimation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nAssetSource(\n   name=\"humanoids\",\n   assets=[\"humanoids/humanoid_**/*.glb\"],\n   settings=ProcessingSettings(\n         operation=Operation.COPY,\n         decimate=False,\n   ),\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Assets with habitat_sim\nDESCRIPTION: This command uses the `habitat_sim.utils.datasets_download` module to download the benchmark assets required for Habitat 2.0 benchmarking. The `--uids hab2_bench_assets` argument specifies the unique identifier for the assets to be downloaded.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab2_bench/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython -m habitat_sim.utils.datasets_download --uids hab2_bench_assets\n```\n\n----------------------------------------\n\nTITLE: Habitat-Lab Installation - Bash\nDESCRIPTION: This snippet clones the Habitat-Lab repository, navigates into the directory, and installs it in editable mode using pip.  Editable mode allows changes to the source code to be immediately reflected without reinstallation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --branch stable https://github.com/facebookresearch/habitat-lab.git\ncd habitat-lab\npip install -e habitat-lab  # install habitat_lab\n```\n\n----------------------------------------\n\nTITLE: Stepping the Environment\nDESCRIPTION: This snippet resets the environment and then performs a step using an empty action dictionary. This retrieves an initial observation from the environment. The observation is then visualized using Matplotlib.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Let's get an observation as before:\nenv.reset()\nobs = env.step({\"action\": (), \"action_args\": {}})\nplt.imshow(obs[\"third_rgb\"])\n```\n\n----------------------------------------\n\nTITLE: Activating Habitat Conda Environment\nDESCRIPTION: These commands initialize conda, source the bashrc file, and activate the Habitat conda environment. This ensures that all necessary dependencies and packages are available for running Habitat.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nconda init; source ~/.bashrc; source activate habitat\n```\n\n----------------------------------------\n\nTITLE: Plotting Benchmark Results\nDESCRIPTION: This command executes the `plot_bench.py` script, which is used to generate plots from the benchmark results.  It helps visualize and analyze the performance data collected during the benchmark.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab2_bench/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npython scripts/hab2_bench/plot_bench.py\n```\n\n----------------------------------------\n\nTITLE: Download Habitat 3.0 Benchmark Assets using habitat-sim with conda\nDESCRIPTION: This command downloads the Habitat 3.0 benchmark assets using the habitat_sim.utils.datasets_download utility when Habitat-sim is installed with conda. It requires the 'hab3_bench_assets' UID and executes a python command.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab3_bench/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m habitat_sim.utils.datasets_download --uids hab3_bench_assets\n```\n\n----------------------------------------\n\nTITLE: Override Config Group Options in Defaults List (YAML)\nDESCRIPTION: This snippet shows how to override Config Group Options in the Defaults list using YAML.  It demonstrates overriding the 'dataset' configuration from 'any_base_config' to 'gibson'.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# @package _global_\n\ndefaults:\n  - any_base_config\n  - override /habitat/dataset/pointnav: gibson  # override the any_base_config dataset to gibson\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Magnum\nDESCRIPTION: This command creates a new conda environment named 'magnum' with Python 3.10. This isolates Magnum and its dependencies from other projects, preventing conflicts.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/pick_throw_vr/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name magnum python=3.10\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment for Dataset Processing\nDESCRIPTION: Creates a new Conda environment named 'datasets' with Python 3.10. This environment is intended for installing and running the dataset processing scripts.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n datasets python=3.10\nconda activate datasets\n```\n\n----------------------------------------\n\nTITLE: Launching Pick_throw_vr with Networking Enabled\nDESCRIPTION: This command launches the Pick_throw_vr application as a server, enabling remote client connections (e.g., from a VR headset). It uses a config override to activate networking functionality.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/hitl/pick_throw_vr/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython examples/hitl/pick_throw_vr/pick_throw_vr.py habitat_hitl.networking.enable=True\n```\n\n----------------------------------------\n\nTITLE: Running the Habitat 2.0 Benchmark\nDESCRIPTION: This command executes the `bench_runner.sh` script, which is responsible for running the Habitat 2.0 benchmark. It likely handles the execution of the simulation with specific configurations and metrics.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab2_bench/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nbash scripts/hab2_bench/bench_runner.sh\n```\n\n----------------------------------------\n\nTITLE: Uninstall and Install pyopenssl - Python\nDESCRIPTION: This snippet uninstalls and reinstalls the pyopenssl package. This is likely done to resolve potential compatibility issues or to ensure the correct version is installed for the Habitat Lab environment. This step is important for resolving SSL-related problems that might arise during environment setup or data downloads.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip uninstall --yes pyopenssl\n!pip install pyopenssl\n```\n\n----------------------------------------\n\nTITLE: Accessing agent camera parameters\nDESCRIPTION: This snippet demonstrates how to access the camera parameters of the articulated agent using the `art_agent.params.cameras.keys()` method. This provides information about the different camera configurations associated with the agent.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nart_agent.params.cameras.keys()\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: Imports necessary libraries for Habitat Lab, including habitat, matplotlib, and random. Matplotlib is used for plotting and visualization, random for selecting random actions, and habitat for the simulation environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat-lab-demo.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport matplotlib.pyplot as plt\n\nimport habitat\nfrom habitat.config import read_write\n```\n\n----------------------------------------\n\nTITLE: Pulling Habitat Docker Image\nDESCRIPTION: This command pulls the Habitat Docker image from the fairembodied repository. It retrieves the specified image version, which includes all the necessary dependencies for running Habitat.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull fairembodied/habitat-challenge:testing_2022_habitat_base_docker\n```\n\n----------------------------------------\n\nTITLE: Running VER with default settings\nDESCRIPTION: This command configures the training process to use VER with default settings. It sets the trainer_name to \"ver\" in the configuration.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/rl/ver/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run \\\n  --config-name=pointnav/ppo_pointnav_example.yaml \\\n  habitat_baselines.trainer_name=ver\n```\n\n----------------------------------------\n\nTITLE: Importing necessary modules\nDESCRIPTION: This snippet imports the necessary Python modules for interacting with the Habitat simulation environment, including habitat_sim, magnum, warnings, and other modules for scene configuration, visualization, and robot control. It also disables warnings.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport habitat_sim\nimport magnum as mn\nimport warnings\nfrom habitat.tasks.rearrange.rearrange_sim import RearrangeSim\nwarnings.filterwarnings('ignore')\nfrom habitat_sim.utils.settings import make_cfg\nfrom matplotlib import pyplot as plt\nfrom habitat_sim.utils import viz_utils as vut\nfrom omegaconf import DictConfig\nimport numpy as np\nfrom habitat.articulated_agents.robots import FetchRobot\nfrom habitat.config.default import get_agent_config\nfrom habitat.config.default_structured_configs import ThirdRGBSensorConfig, HeadRGBSensorConfig, HeadPanopticSensorConfig\nfrom habitat.config.default_structured_configs import SimulatorConfig, HabitatSimV0Config, AgentConfig\nfrom habitat.config.default import get_agent_config\nimport habitat\nfrom habitat_sim.physics import JointMotorSettings, MotionType\nfrom omegaconf import OmegaConf\n\nimport git, os\nrepo = git.Repo(\".\", search_parent_directories=True)\ndir_path = repo.working_tree_dir\ndata_path = os.path.join(dir_path, \"data\")\nos.chdir(dir_path)\n```\n\n----------------------------------------\n\nTITLE: Habitat-Sim Installation - Conda\nDESCRIPTION: This command installs the habitat-sim package with bullet physics from the conda-forge and aihabitat channels. It's a core dependency for Habitat-Lab.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install habitat-sim withbullet -c conda-forge -c aihabitat\n```\n\n----------------------------------------\n\nTITLE: Semantic Annotations Output\nDESCRIPTION: Demonstrates the standard output generated by the `print_scene_recur` function, illustrating the hierarchical structure of semantic scene annotations.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat-lab-demo.rst#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nLevel id:0, center:[11.0210495  3.996935   3.3452997], dims:[ 43.0625    8.19569 -30.1122 ]\nRegion id:0_0, category:rec/game, center:[16.61225    2.7802274 11.577564 ], dims:[10.364299   5.5838847 -4.14447  ]\nObject id:0_0_0, category:ceiling, center:[16.5905   4.54488 11.269  ], dims:[9.984315  4.0917997 2.1377602]\nObject id:0_0_1, category:wall, center:[16.5865     2.6818905 13.4147   ], dims:[9.69278   0.5280709 5.4398193]\nObject id:0_0_2, category:wall, center:[21.6013     1.7400599 11.3493   ], dims:[3.5423203  0.41668844 3.921341  ]\nObject id:0_0_3, category:door, center:[11.5374     1.2431393 10.386599 ], dims:[1.2573967  2.5311599  0.41445923]\nObject id:0_0_4, category:door, center:[20.6332     1.2136002 13.5958   ], dims:[0.15834427 2.4860601  1.1674671 ]\nObject id:0_0_5, category:wall, center:[16.5946    2.66614   9.331001], dims:[9.72554    0.23693037 5.3787804 ]\nObject id:0_0_6, category:window, center:[16.5822    2.852209 13.596898], dims:[1.5934639  0.16375065 1.2588081 ]\nObject id:0_0_7, category:beam, center:[16.6094    5.32839  11.348299], dims:[0.5116577  0.35226822 3.8936386 ]\nObject id:0_0_8, category:floor, center:[16.586       0.07907867 11.406     ], dims:[10.48608    4.3792195  0.2833004]\nObject id:0_0_9, category:lighting, center:[11.798      1.9214487 11.313999 ], dims:[0.25683594 0.5076561  0.15560722]\nObject id:0_0_10, category:wall, center:[11.57       1.7476702 11.3347   ], dims:[3.54352    0.41701245 3.9231815 ]\nObject id:0_0_11, category:misc, center:[16.5943   2.29591 11.4341 ], dims:[10.428299  4.48172   4.676901]\nObject id:0_0_12, category:door, center:[11.5234     1.2489185 12.228199 ], dims:[1.2521439  2.5423803  0.46386147]\nObject id:0_0_13, category:door, center:[16.5833     1.1790485 13.490699 ], dims:[5.45306   0.3474083 2.4161606]\nObject id:0_0_14, category:window, center:[21.6362     1.2518396 12.2613   ], dims:[1.1998444  2.5486398  0.37800598]\n```\n\n----------------------------------------\n\nTITLE: Running Example Task - Python\nDESCRIPTION: This command runs the `example.py` script to test the Pick task. It uses a pre-specified virtual robot and a configuration file to define the task and agent.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython examples/example.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Simulator\nDESCRIPTION: This snippet shows the output of initializing the simulator. It includes timestamped messages indicating the start of the simulation environment and task.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat-lab-demo.rst#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n2019-06-06 16:11:35,200 initializing sim Sim-v0\n2019-06-06 16:11:46,171 initializing task Nav-v0\n```\n\n----------------------------------------\n\nTITLE: HabitatDatasetSource Configuration Example (Object Dataset)\nDESCRIPTION: An example of configuring a HabitatDatasetSource to process an object dataset. The dataset config file is specified, and all objects are processed without decimation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nHabitatDatasetSource(\n   name=\"ai2thor_object_dataset\",\n   dataset_config=\"objects/ovmm_objects/train_val/ai2thorhab/ai2thor_object_dataset.scene_dataset_config.json\",\n   objects=ProcessingSettings(\n      operation=Operation.PROCESS,\n      decimate=False,\n   ),\n)\n```\n\n----------------------------------------\n\nTITLE: Running Habitat Example Script\nDESCRIPTION: These commands change the directory to habitat-lab and run the example.py script.  This script demonstrates basic Habitat functionality and prints output to the console.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncd habitat-lab; python examples/example.py\n```\n\n----------------------------------------\n\nTITLE: Habitat Environment Initialization Output\nDESCRIPTION: This shell session output shows the log messages during the initialization of the Habitat simulator and task. It includes loading the navmesh for the scene.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_1\n\nLANGUAGE: shell-session\nCODE:\n```\n2019-06-11 10:03:34,049 initializing sim Sim-v0\nI0611 10:03:34.056092 64715 simulator.py:78] Loaded navmesh ../data/scene_datasets/habitat-test-scenes/skokloster-castle.navmesh\n2019-06-11 10:03:35,053 initializing task Nav-v0\n```\n\n----------------------------------------\n\nTITLE: Habitat-Baselines Installation - Bash\nDESCRIPTION: Installs Habitat-Baselines in editable mode via pip. This includes baselines algorithms and all required additional dependencies for training agents.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -e habitat-baselines  # install habitat_baselines\n```\n\n----------------------------------------\n\nTITLE: PointNav Habitat Test Configuration (YAML)\nDESCRIPTION: Extends the base PointNav configuration to use the Habitat test dataset.  It includes 'pointnav_base' and overrides the dataset to 'habitat_test'.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# @package _global_\n\ndefaults:\n  - pointnav_base\n  - /habitat/dataset/pointnav: habitat_test\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration\nDESCRIPTION: Configures the Habitat environment using a YAML configuration file and overrides the dataset split to \"val\". It then instantiates the environment using the configured settings.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat-lab-demo.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconfig = habitat.get_config(config_paths=\"benchmark/nav/pointnav/pointnav_mp3d.yaml\")\nwith read_write(config):\n    config.habitat.dataset.split = \"val\"\n\nenv = habitat.Env(config=config)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Execution (Shell)\nDESCRIPTION: This shell command executes the benchmark runner script.  This script performs the actual benchmark evaluation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nbash scripts/hab2_bench/bench_runner.sh\n```\n\n----------------------------------------\n\nTITLE: Running Interactive Play Script - Bash\nDESCRIPTION: Executes the interactive_play.py script, enabling keyboard and mouse control of a Fetch robot in a ReplicaCAD environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Interactive play script\npython examples/interactive_play.py --never-end\n```\n\n----------------------------------------\n\nTITLE: Interactive Play Dependencies Installation - Bash\nDESCRIPTION: Installs pygame and pybullet libraries. Pygame is used for visualization, and pybullet is used for inverse kinematics during interactive play.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/README.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install pygame==2.0.1 pybullet==3.0.4\n```\n\n----------------------------------------\n\nTITLE: Download Habitat 3.0 Benchmark Assets using habitat-sim from source\nDESCRIPTION: This command downloads the Habitat 3.0 benchmark assets using the habitat_sim.utils.datasets_download utility when Habitat-sim is installed from source. It requires specifying the full path to the datasets_download.py script and the 'hab3_bench_assets' UID.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab3_bench/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython /path/to/habitat_sim/src_python/habitat_sim/utils/datasets_download.py --uids hab3_bench_assets\n```\n\n----------------------------------------\n\nTITLE: Testing Humanoid Controller (Habitat)\nDESCRIPTION: Executes a pytest test case to verify the functionality of the HumanoidRearrangeController. This test generates an animation of a humanoid walking on an empty plane.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython -m pytest test/test_humanoid.py:test_humanoid_controller\n```\n\n----------------------------------------\n\nTITLE: Default Link Configuration for ArticulatedObjects - Python\nDESCRIPTION: This snippet shows how to define the 'default_link' for an ArticulatedObject using the 'user_defined' field. The default link is an integer index representing the link that should be actuated if only one joint can be actuated. Cannot be the base link (-1).\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/metadata-taxonomy.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    \"user_defined\": {\n        \"default_link\": 5 #the link id which is \"default\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Resetting the Habitat Environment (Python)\nDESCRIPTION: Resets the Habitat environment to a new episode and returns the initial observations. Requires an initialized Habitat environment named `env`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n    obs = env.reset()\n```\n\n----------------------------------------\n\nTITLE: Accessing the Habitat Simulator\nDESCRIPTION: This snippet shows how to access the underlying Habitat simulator instance from within the environment.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(env._sim)\n```\n\n----------------------------------------\n\nTITLE: Accessing Observation Keys (Python)\nDESCRIPTION: Retrieves the keys of the observation dictionary returned by the Habitat environment. Useful for identifying available sensor data.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n    obs.keys()\n```\n\n----------------------------------------\n\nTITLE: Generating Humanoid with Blender (Habitat)\nDESCRIPTION: Executes a Blender script to generate a new humanoid body with random gender and body parameters. It leverages the SMPL-X Blender add-on to create and export the humanoid model, storing it under `data/humanoids/humanoid_data/avatar_0`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\n# For MacOS, the path to blender typically is /Applications/Blender.app/Contents/MacOS/Blender\npath_to_blender -b -P 'scripts/export_smplx_bodies.py'\n```\n\n----------------------------------------\n\nTITLE: Setting the number of inference workers\nDESCRIPTION: This command sets the number of inference workers in VER. The optimal number depends on the size of the policy model.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/rl/ver/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython .... \\\n  habitat_baselines.rl.ver.num_inference_workers=<number of inference workers>\n```\n\n----------------------------------------\n\nTITLE: Generating Random Overlapping Views\nDESCRIPTION: This code snippet generates two random, overlapping views by randomly permuting the rotation and translation of the agent within the Habitat environment. It stores the RGB and depth observations, as well as the camera states.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndepths = []\nrgbs = []\ncameras = []\n\n\nfor i in range(0, 2):\n    rotation = uniform_quat(init_rotation)\n    translation = init_translation + np.random.rand(3,) * 0.5 - 0.25\n\n    obs = env._sim.get_observations_at(position=translation, rotation=rotation, keep_agent_at_new_pose=True)\n    depths += [obs[\"depth\"][...,0]]\n    rgbs += [obs[\"rgb\"]]\n\n    cameras += [env._sim.get_agent_state()]\n\nenv.close()\n```\n\n----------------------------------------\n\nTITLE: Running Interactive Play Script\nDESCRIPTION: This code snippet shows how to run the interactive play script for controlling the Stretch robot. It uses the `examples/interactive_play.py` script with specific arguments to enable continuous interaction and specify the configuration file.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/robots/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython examples/interactive_play.py --never-end --cfg /path/to/data/play_stretch.yaml\n```\n\n----------------------------------------\n\nTITLE: TP-SRL Skill Training (Shell)\nDESCRIPTION: This shell command trains a specific skill policy (e.g., Place) using reinforcement learning, storing checkpoints in the specified folder. It utilizes the `rearrange/rl_skill.yaml` configuration and the `benchmark/rearrange` argument to specify the skill.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython -u -m habitat_baselines.run \\\n        --config-name=rearrange/rl_skill.yaml \\\n        checkpoint_folder=./place_checkpoints/ \\\n        benchmark/rearrange=place\n```\n\n----------------------------------------\n\nTITLE: Closing the Habitat Environment (Python)\nDESCRIPTION: Closes the Habitat environment, releasing resources.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n    env.close()\n```\n\n----------------------------------------\n\nTITLE: Create symbolic link for Habitat 3.0 Benchmark Assets\nDESCRIPTION: This command creates a symbolic link to the downloaded Habitat 3.0 benchmark assets. It uses the 'ls -s' command to link the dataset path to the 'data/hab3_bench_assets' directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/hab3_bench/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nls -s /path/to/hab_3_bench_assets/ data/hab3_bench_assets\n```\n\n----------------------------------------\n\nTITLE: Point Navigation Target Birdseye View\nDESCRIPTION: This snippet defines a function that draws a birdseye view of the target in a point navigation task. It creates a dummy `NavigationEpisode`, sets agent position and rotation, and uses `maps.pointnav_draw_target_birdseye_view` to generate the image.  It depends on `numpy`, `matplotlib`, and `habitat` libraries.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab_TopdownMap_Visualization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef example_pointnav_draw_target_birdseye_view():\n    # Define NavigationEpisode parameters\n    goal_radius = 0.5\n    goal = NavigationGoal(position=[10, 0.25, 10], radius=goal_radius)\n    agent_position = [0, 0.25, 0]\n    agent_rotation = -np.pi / 4\n\n    # Create dummy episode for birdseye view visualization\n    dummy_episode = NavigationEpisode(\n        goals=[goal],\n        episode_id=\"dummy_id\",\n        scene_id=\"dummy_scene\",\n        start_position=agent_position,\n        start_rotation=agent_rotation,  # type: ignore[arg-type]\n    )\n\n    agent_position = np.array(agent_position)\n    # Draw birdseye view\n    target_image = maps.pointnav_draw_target_birdseye_view(\n        agent_position,\n        agent_rotation,\n        np.asarray(dummy_episode.goals[0].position),\n        goal_radius=dummy_episode.goals[0].radius,\n        agent_radius_px=25,\n    )\n    plt.imshow(target_image)\n    plt.title(\"pointnav_target_image.png\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Minimal HITL Hydra Configuration YAML\nDESCRIPTION: This YAML file defines the Hydra configuration for a minimal HITL application. It specifies default configurations including the `pop_play` Habitat baseline for a Spot robot and humanoid in HSSD scenes, and default parameters for the HITL framework defined in `hitl_defaults.yaml`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-hitl/README.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# minimal_cfg.yaml\n\n# Read more about Hydra at habitat-lab/habitat/config/README.md.\n# @package _global_\n\ndefaults:\n  # We load the `pop_play` Habitat baseline featuring a Spot robot\n  # and a humanoid in HSSD scenes. See habitat-baselines/README.md.\n  - social_rearrange: pop_play\n  # Load default parameters for the HITL framework. See\n  # habitat-hitl/habitat_hitl/config/hitl_defaults.yaml.\n  - hitl_defaults\n  - _self_\n\n```\n\n----------------------------------------\n\nTITLE: Running Pytest for Habitat-Lab\nDESCRIPTION: This command executes the pytest testing framework to ensure the project's tests are passing. It's a crucial step before submitting a pull request to verify the code's functionality and prevent regressions.  This command needs testing data that can be downloaded before use.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython -m pytest\n```\n\n----------------------------------------\n\nTITLE: Adding Magnum Python Path on Linux\nDESCRIPTION: Adds the path to the Magnum Python libraries to the Python environment's `site-packages` by creating a `.pth` file. This allows Python to locate and import the Magnum libraries. Requires knowing the absolute path to the Magnum Python libraries.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n/home/USER/Documents/magnum-tools/linux-x64/python/\n```\n\n----------------------------------------\n\nTITLE: Overriding Config Group Option via Command Line - Bash\nDESCRIPTION: This bash command shows how to override the Config Group Option value from the command line. In this example, the benchmark config is overridden to be `pointnav_hm3d`.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -u -m habitat_baselines.run --config-name=pointnav/ddppo_pointnav.yaml \\\nbenchmark/nav/pointnav=pointnav_hm3d  # overriding benchmark config to be pointnav_hm3d\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Overrides via YAML\nDESCRIPTION: This YAML snippet demonstrates how to define configuration overrides within a YAML file, placing these definitions after the Defaults List. It includes examples of setting config groups.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# @package _global_\n\ndefaults:\n  - /habitat: habitat_config_base\n  - /habitat/task: pointnav\n  - /habitat/simulator/agents:\n    - rgbd_agent\n  - /habitat/dataset/pointnav: gibson\n  - _self_\n```\n\n----------------------------------------\n\nTITLE: Disabling variable experience rollouts\nDESCRIPTION: This command disables the variable experience rollouts feature in VER. It is recommended when there are extreme differences in simulation time between environments.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-baselines/habitat_baselines/rl/ver/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython .... \\\n  habitat_baselines.rl.ver.variable_experience=False\n```\n\n----------------------------------------\n\nTITLE: Visualization Output\nDESCRIPTION: This shell session output shows the Matplotlib visualization output.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_6\n\nLANGUAGE: shell-session\nCODE:\n```\n(-0.5, 255.5, 255.5, -0.5)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Dataset Processing Script\nDESCRIPTION: An example of how to invoke the dataset processing script from the command line. The `--input` parameter specifies the path to the 'data/' directory to be processed.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython processing_script.py --input path/to/data\n```\n\n----------------------------------------\n\nTITLE: Generating Uniform Quaternion Rotations\nDESCRIPTION: This function generates a quaternion representing a rotation by slightly perturbing a given original rotation. It takes an original angle as input and returns a new quaternion representing the slightly modified rotation.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/view-transform-warp.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef uniform_quat(original_angle):\n    original_euler = quaternion.as_euler_angles(original_angle)\n    euler_angles = np.array([(np.random.rand() - 0.5) * np.pi / 9. + original_euler[0],\n                            (np.random.rand() - 0.5) * np.pi / 9. + original_euler[1],\n                            (np.random.rand() - 0.5) * np.pi / 9. + original_euler[2]])\n    quaternions = quaternion.from_euler_angles(euler_angles)\n\n\n    return quaternions\n```\n\n----------------------------------------\n\nTITLE: Humanoid Avatar JSON Configuration (Habitat)\nDESCRIPTION: Example JSON configuration format for specifying parameters for the generated humanoids. The example includes properties for naming avatars, setting beta values, and specifying gender or texture.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/articulated_agents/humanoids/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n[\n    {\n        \"name\": \"The name of the avatar. If not provided, will be avatar_{ind}, with {ind} being the avatar index\",\n        \"betas\": \"A list of 10 beta parameters (see SMPL-X). If not provided, samples at random from a normal distribution\",\n        \"gender\": \"The gender of the avatar (neutral, female, male). If not provided, samples at random among the 3\",\n        \"texture\": \"The texture of the avatar. If not provided, samples among the ones we provide by default\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Downloading Spot Robot\nDESCRIPTION: This code snippet uses a python script to download the spot robot asset required for multi-agent interaction example. It specifically downloads the `hab_spot_arm` dataset. It uses the `habitat_sim.utils.datasets_download` module.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/articulated_agents_tutorial.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n! python -m habitat_sim.utils.datasets_download --uids hab_spot_arm --no-replace\n```\n\n----------------------------------------\n\nTITLE: Benchmark Results Plotting (Shell)\nDESCRIPTION: This shell command runs a Python script to generate the results table from the benchmark execution. It uses the `plot_bench.py` script located in the `scripts/hab2_bench/` directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat2.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython scripts/hab2_bench/plot_bench.py\n```\n\n----------------------------------------\n\nTITLE: Tensorboard Visualization Example - Python\nDESCRIPTION: This snippet demonstrates how to display a TensorBoard video demo image. It attempts to display a GIF image located at \"./res/img/tensorboard_video_demo.gif\" using IPython.display. This is typically used for visualizing training progress and agent behavior using TensorBoard. The try-except block handles potential ImportErrors if IPython is not available.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/notebooks/Habitat_Lab.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    from IPython import display\n\n    with open(\"./res/img/tensorboard_video_demo.gif\", \"rb\") as f:\n        display.display(display.Image(data=f.read(), format=\"png\"))\nexcept ImportError:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Modules for Habitat Simulation\nDESCRIPTION: This snippet imports necessary modules for Habitat simulation, including habitat_sim, magnum, and various utility functions for creating and managing simulations, configuring agents, and visualizing results. It also includes modules for articulated agent controllers and configurations for humanoid actions.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/examples/tutorials/humanoids_tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport habitat_sim\nimport magnum as mn\nimport warnings\nfrom habitat.tasks.rearrange.rearrange_sim import RearrangeSim\nwarnings.filterwarnings('ignore')\nfrom habitat_sim.utils.settings import make_cfg\nfrom matplotlib import pyplot as plt\nfrom habitat_sim.utils import viz_utils as vut\nfrom omegaconf import DictConfig\nimport numpy as np\nfrom habitat.articulated_agents.robots import FetchRobot\nfrom habitat.config.default import get_agent_config\nfrom habitat.config.default_structured_configs import ThirdRGBSensorConfig, HeadRGBSensorConfig, HeadPanopticSensorConfig\nfrom habitat.config.default_structured_configs import SimulatorConfig, HabitatSimV0Config, AgentConfig\nfrom habitat.config.default import get_agent_config\nimport habitat\nfrom habitat_sim.physics import JointMotorSettings, MotionType\nfrom omegaconf import OmegaConf\nfrom habitat.articulated_agent_controllers import (\n    HumanoidRearrangeController,\n    HumanoidSeqPoseController,\n)\nfrom habitat.config.default_structured_configs import HumanoidJointActionConfig, HumanoidPickActionConfig\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV\nDESCRIPTION: This command installs the OpenCV library, a dependency for the example code, used for image display.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/quickstart.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install opencv-python\n```\n\n----------------------------------------\n\nTITLE: Scene Semantic Annotations\nDESCRIPTION: Prints the semantic annotation information (id, category, bounding box details) for the current scene in a hierarchical fashion. It recursively iterates through levels, regions, and objects within the scene, printing their properties.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/docs/pages/habitat-lab-demo.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef print_scene_recur(scene, limit_output=10):\n    count = 0\n    for level in scene.levels:\n        print(\n            f\"Level id:{level.id}, center:{level.aabb.center},\"\n            f\" dims:{level.aabb.sizes}\"\n        )\n        for region in level.regions:\n            print(\n                f\"Region id:{region.id}, category:{region.category.name()},\"\n                f\" center:{region.aabb.center}, dims:{region.aabb.sizes}\"\n            )\n            for obj in region.objects:\n                print(\n                    f\"Object id:{obj.id}, category:{obj.category.name()},\"\n                    f\" center:{obj.aabb.center}, dims:{obj.aabb.sizes}\"\n                )\n                count += 1\n                if count >= limit_output:\n                    return None\n\n# Print semantic annotation information (id, category, bounding box details)\n# for the current scene in a hierarchical fashion\nscene = env.sim.semantic_annotations()\nprint_scene_recur(scene, limit_output=15)\n\nenv.close()\n# Note: Since only one OpenGL is allowed per process,\n# you have to close the current env before instantiating a new one.\n```\n\n----------------------------------------\n\nTITLE: Installing Habitat Dataset Processing Package\nDESCRIPTION: Installs the Habitat dataset processing package in editable mode using pip. This allows for modifications to the package code to be reflected immediately without re-installation. Requires being in the 'scripts/habitat_dataset_processing' directory.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/scripts/habitat_dataset_processing/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd scripts/habitat_dataset_processing\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: PointNav Gibson Configuration (YAML)\nDESCRIPTION: Extends the base PointNav configuration to use the Gibson dataset. It includes 'pointnav_base' and overrides the dataset to 'gibson'.\nSOURCE: https://github.com/facebookresearch/habitat-lab/blob/main/habitat-lab/habitat/config/README.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n# @package _global_\n\ndefaults:\n  - pointnav_base\n  - /habitat/dataset/pointnav: gibson\n```"
  }
]
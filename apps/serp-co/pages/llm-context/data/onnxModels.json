[
  {
    "owner": "onnx",
    "repo": "models",
    "content": "TITLE: Calculating IoU for Bounding Boxes (Python)\nDESCRIPTION: Calculates the Intersection Over Union (IoU) between two sets of bounding boxes. This function is a crucial component of Non-Maximum Suppression (NMS) for filtering redundant bounding box detections. The IoU value represents the overlap between predicted bounding boxes and is used to suppress duplicate detections of the same object.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef bboxes_iou(boxes1, boxes2):\n    '''calculate the Intersection Over Union value'''\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = np.maximum(right_down - left_up, 0.0)\n    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n    union_area    = boxes1_area + boxes2_area - inter_area\n    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n\n    return ious\n```\n\n----------------------------------------\n\nTITLE: Calculating Intersection Over Union (IOU) - Python\nDESCRIPTION: The `bboxes_iou` function calculates the Intersection Over Union (IOU) between two sets of bounding boxes. It computes the area of intersection and the area of union, then returns the IOU score, a measure of overlap between the boxes.  Handles edge cases to prevent division by zero.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef bboxes_iou(boxes1, boxes2):\n    '''calculate the Intersection Over Union value'''\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = np.maximum(right_down - left_up, 0.0)\n    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n    union_area    = boxes1_area + boxes2_area - inter_area\n    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n\n    return ious\n```\n\n----------------------------------------\n\nTITLE: Post-process FCN Output for Visualization in Python\nDESCRIPTION: This snippet defines functions to post-process the output of the FCN model and visualize the segmentation results. It includes functions to generate a color palette (`get_palette`), colorize the output labels (`colorize`), and visualize the output by blending the segmented image with the original image (`visualize_output`). It relies on libraries such as `matplotlib`, `cv2`, and `PIL`.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib.colors import hsv_to_rgb\nimport cv2\n\nclasses = [line.rstrip('\\n') for line in open('voc_classes.txt')]\nnum_classes = len(classes)\n\ndef get_palette():\n    # prepare and return palette\n    palette = [0] * num_classes * 3\n    \n    for hue in range(num_classes):\n        if hue == 0: # Background color\n            colors = (0, 0, 0)\n        else:\n            colors = hsv_to_rgb((hue / num_classes, 0.75, 0.75))\n            \n        for i in range(3):\n            palette[hue * 3 + i] = int(colors[i] * 255)\n            \n    return palette\n\ndef colorize(labels):\n    # generate colorized image from output labels and color palette\n    result_img = Image.fromarray(labels).convert('P', colors=num_classes)\n    result_img.putpalette(get_palette())\n    return np.array(result_img.convert('RGB'))\n\ndef visualize_output(image, output):\n    assert(image.shape[0] == output.shape[1] and \\\n           image.shape[1] == output.shape[2]) # Same height and width\n    assert(output.shape[0] == num_classes)\n    \n    # get classification labels\n    raw_labels = np.argmax(output, axis=0).astype(np.uint8)\n\n    # comput confidence score\n    confidence = float(np.max(output, axis=0).mean())\n\n    # generate segmented image\n    result_img = colorize(raw_labels)\n    \n    # generate blended image\n    blended_img = cv2.addWeighted(image[:, :, ::-1], 0.5, result_img, 0.5, 0)\n    \n    result_img = Image.fromarray(result_img)\n    blended_img = Image.fromarray(blended_img)\n\n    return confidence, result_img, blended_img, raw_labels\n\nconf, result_img, blended_img, _ = visualize_output(orig_tensor, output[0])\n```\n\n----------------------------------------\n\nTITLE: Non-Maximum Suppression (NMS) in Python\nDESCRIPTION: Applies Non-Maximum Suppression (NMS) to a set of bounding boxes. NMS filters out overlapping bounding boxes that likely represent the same object, keeping only the box with the highest confidence score. The function iterates through each class of objects, selects the bounding box with the highest confidence, and removes overlapping boxes based on an IoU threshold. It also supports soft-NMS.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef nms(bboxes, iou_threshold, sigma=0.3, method='nms'):\n    \"\"\"\n    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n\n    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n          https://github.com/bharatsingh430/soft-nms\n    \"\"\"\n    classes_in_img = list(set(bboxes[:, 5]))\n    best_bboxes = []\n\n    for cls in classes_in_img:\n        cls_mask = (bboxes[:, 5] == cls)\n        cls_bboxes = bboxes[cls_mask]\n\n        while len(cls_bboxes) > 0:\n            max_ind = np.argmax(cls_bboxes[:, 4])\n            best_bbox = cls_bboxes[max_ind]\n            best_bboxes.append(best_bbox)\n            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n            weight = np.ones((len(iou),), dtype=np.float32)\n\n            assert method in ['nms', 'soft-nms']\n\n            if method == 'nms':\n                iou_mask = iou > iou_threshold\n                weight[iou_mask] = 0.0\n\n            if method == 'soft-nms':\n                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n\n            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n            score_mask = cls_bboxes[:, 4] > 0.\n            cls_bboxes = cls_bboxes[score_mask]\n\n    return best_bboxes\n```\n\n----------------------------------------\n\nTITLE: Reading Class Names from File - Python\nDESCRIPTION: The `read_class_names` function loads class names from a specified file. It reads each line from the file, strips the newline character, and stores the class names in a dictionary with their corresponding IDs. The `class_file_name` parameter indicates the location of the class names file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef read_class_names(class_file_name):\n    '''loads class name from a file'''\n    names = {}\n    with open(class_file_name, 'r') as data:\n        for ID, name in enumerate(data):\n            names[ID] = name.strip('\\n')\n    return names\n```\n\n----------------------------------------\n\nTITLE: Non-Maximum Suppression (NMS) Implementation - Python\nDESCRIPTION: The `nms` function implements Non-Maximum Suppression (NMS) to filter overlapping bounding boxes. It iterates through each class, selects the box with the highest score, removes overlapping boxes based on the IOU threshold, and returns the best bounding boxes. It supports both standard NMS and soft-NMS methods. The `method` parameter dictates which NMS version is used.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef nms(bboxes, iou_threshold, sigma=0.3, method='nms'):\n    \"\"\"\n    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n\n    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n          https://github.com/bharatsingh430/soft-nms\n    \"\"\"\n    classes_in_img = list(set(bboxes[:, 5]))\n    best_bboxes = []\n\n    for cls in classes_in_img:\n        cls_mask = (bboxes[:, 5] == cls)\n        cls_bboxes = bboxes[cls_mask]\n\n        while len(cls_bboxes) > 0:\n            max_ind = np.argmax(cls_bboxes[:, 4])\n            best_bbox = cls_bboxes[max_ind]\n            best_bboxes.append(best_bbox)\n            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n            weight = np.ones((len(iou),), dtype=np.float32)\n\n            assert method in ['nms', 'soft-nms']\n\n            if method == 'nms':\n                iou_mask = iou > iou_threshold\n                weight[iou_mask] = 0.0\n\n            if method == 'soft-nms':\n                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n\n            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n            score_mask = cls_bboxes[:, 4] > 0.\n            cls_bboxes = cls_bboxes[score_mask]\n\n    return best_bboxes\n```\n\n----------------------------------------\n\nTITLE: Make Prediction in Python\nDESCRIPTION: Takes the path to an image, preprocesses it, runs it through the ONNX model, and prints the top prediction. The image is loaded using `get_image`, preprocessed with `preprocess`, fed to the ONNX Runtime session, and the predicted class label and probability are printed to the console. Input: `path` - The path to the image file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef predict(path):\n    img = get_image(path, show=True)\n    img = preprocess(img)\n    ort_inputs = {session.get_inputs()[0].name: img}\n    preds = session.run(None, ort_inputs)[0]\n    preds = np.squeeze(preds)\n    a = np.argsort(preds)[::-1]\n    print('class=%s ; probability=%f' %(labels[a[0]],preds[a[0]]))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for YOLOv4 Inference in Python\nDESCRIPTION: This Python code snippet demonstrates how to preprocess an image for YOLOv4 inference. It resizes and pads the image to the target size (416x416) and normalizes pixel values.  It uses the OpenCV (cv2) and NumPy libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2\n\n# this function is from tensorflow-yolov4-tflite/core/utils.py\ndef image_preprocess(image, target_size, gt_boxes=None):\n\n    ih, iw = target_size\n    h, w, _ = image.shape\n\n    scale = min(iw/w, ih/h)\n    nw, nh = int(scale * w), int(scale * h)\n    image_resized = cv2.resize(image, (nw, nh))\n\n    image_padded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n    image_padded[dh:nh+dh, dw:nw+dw, :] = image_resized\n    image_padded = image_padded / 255.\n\n    if gt_boxes is None:\n        return image_padded\n\n    else:\n        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n        return image_padded, gt_boxes\n\n# input\ninput_size = 416\n\noriginal_image = cv2.imread(\"input.jpg\")\noriginal_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\noriginal_image_size = original_image.shape[:2]\n\nimage_data = image_preprocess(np.copy(original_image), [input_size, input_size])\nimage_data = image_data[np.newaxis, ...].astype(np.float32)\n\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Inference Session for BERT\nDESCRIPTION: This code snippet sets up and runs an ONNX inference session using the `onnxruntime` library. It loads the `bert.onnx` model, feeds preprocessed input data (input IDs, input mask, segment IDs), and retrieves the model's output (unique IDs, start logits, and end logits). The code iterates through input examples and performs inference using the loaded ONNX model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/BERT-Squad.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# run inference\n\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# ort.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsession = ort.InferenceSession('bert.onnx')\n\nfor input_meta in session.get_inputs():\n    print(input_meta)\nn = len(input_ids)\nbs = batch_size\nall_results = []\nstart = timer()\nfor idx in range(0, n):\n    item = eval_examples[idx]\n    # this is using batch_size=1\n    # feed the input data as int64\n    data = {\"unique_ids_raw_output___9:0\": np.array([item.qas_id], dtype=np.int64),\n            \"input_ids:0\": input_ids[idx:idx+bs],\n            \"input_mask:0\": input_mask[idx:idx+bs],\n            \"segment_ids:0\": segment_ids[idx:idx+bs]}\n    result = session.run([\"unique_ids:0\",\"unstack:0\", \"unstack:1\"], data)\n    in_batch = result[1].shape[0]\n    start_logits = [float(x) for x in result[1][0].flat]\n    end_logits = [float(x) for x in result[2][0].flat]\n    for i in range(0, in_batch):\n        unique_id = len(all_results)\n        all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits))\n```\n\n----------------------------------------\n\nTITLE: Data Preparation for Sentiment Analysis (Python)\nDESCRIPTION: This function loads the movie sentiment dataset, preprocesses it into a pandas DataFrame with 'text' and 'labels' columns, and splits the data into training and testing sets based on the specified test size. It uses the `fetch_movie_sentiment` function from the `alibi` library to load the dataset and `train_test_split` from `sklearn` for splitting.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/dependencies/roberta-sequence-classification-validation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport math\nimport pandas as pd\nfrom simpletransformers.model import TransformerModel\nfrom sklearn.model_selection import train_test_split\n\nfrom alibi.datasets import fetch_movie_sentiment\n\ndef prepare_data(test_size):\n    # load data\n    X, y = fetch_movie_sentiment(return_X_y=True)\n\n    # prepare data\n    data = pd.DataFrame()\n    data['text'] = X\n    data['labels'] = y\n\n    if math.isclose(test_size, 0.0):\n        return data, None\n    else:\n        train, test = train_test_split(data, test_size=test_size)\n        return train, test\n```\n\n----------------------------------------\n\nTITLE: Getting Last Hidden States from GPT-2 Model in Python\nDESCRIPTION: This code snippet illustrates how to access the last hidden states from the output of the GPT-2 model. It assumes that the model has already been run with input_ids and the output is stored in the 'outputs' variable. It then extracts the first element of the 'outputs' tuple, which contains the last hidden states.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/gpt-2/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\noutputs = model(input_ids)\nlast_hidden_states = outputs[0]\n```\n\n----------------------------------------\n\nTITLE: Postprocessing YOLOv4 Bounding Boxes (Python)\nDESCRIPTION: Converts YOLOv4 model predictions into bounding boxes. It applies the inverse of the model's transformations to the raw output, including scaling based on the stride and anchor box dimensions.  The function iterates through each prediction layer, computes predicted xy and wh values, and concatenates them to form the final bounding box coordinates.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE=[1,1,1]):\n    '''define anchor boxes'''\n    for i, pred in enumerate(pred_bbox):\n        conv_shape = pred.shape\n        output_size = conv_shape[1]\n        conv_raw_dxdy = pred[:, :, :, :, 0:2]\n        conv_raw_dwdh = pred[:, :, :, :, 2:4]\n        xy_grid = np.meshgrid(np.arange(output_size), np.arange(output_size))\n        xy_grid = np.expand_dims(np.stack(xy_grid, axis=-1), axis=2)\n\n        xy_grid = np.tile(np.expand_dims(xy_grid, axis=0), [1, 1, 1, 3, 1])\n        xy_grid = xy_grid.astype(np.float)\n\n        pred_xy = ((special.expit(conv_raw_dxdy) * XYSCALE[i]) - 0.5 * (XYSCALE[i] - 1) + xy_grid) * STRIDES[i]\n        pred_wh = (np.exp(conv_raw_dwdh) * ANCHORS[i])\n        pred[:, :, :, :, 0:4] = np.concatenate([pred_xy, pred_wh], axis=-1)\n\n    pred_bbox = [np.reshape(x, (-1, np.shape(x)[-1])) for x in pred_bbox]\n    pred_bbox = np.concatenate(pred_bbox, axis=0)\n    return pred_bbox\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image Data in Python\nDESCRIPTION: This code snippet provides functions for preprocessing input images, including resizing with aspect ratio, center cropping, and normalization. It uses OpenCV (cv2) for image manipulation and NumPy for array operations. The input image is expected to be in BGR format and is converted to RGB before preprocessing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/efficientnet-lite4/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport onnxruntime as rt\nimport cv2\nimport json\n\n# load the labels text file\nlabels = json.load(open(\"labels_map.txt\", \"r\"))\n\n# set image file dimensions to 224x224 by resizing and cropping image from center\ndef pre_process_edgetpu(img, dims):\n    output_height, output_width, _ = dims\n    img = resize_with_aspectratio(img, output_height, output_width, inter_pol=cv2.INTER_LINEAR)\n    img = center_crop(img, output_height, output_width)\n    img = np.asarray(img, dtype='float32')\n    # converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\n    img -= [127.0, 127.0, 127.0]\n    img /= [128.0, 128.0, 128.0]\n    return img\n\n# resize the image with a proportional scale\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.INTER_LINEAR):\n    height, width, _ = img.shape\n    new_height = int(100. * out_height / scale)\n    new_width = int(100. * out_width / scale)\n    if height > width:\n        w = new_width\n        h = int(new_height * height / width)\n    else:\n        h = new_height\n        w = int(new_width * width / height)\n    img = cv2.resize(img, (w, h), interpolation=inter_pol)\n    return img\n\n# crop the image around the center based on given height and width\ndef center_crop(img, out_height, out_width):\n    height, width, _ = img.shape\n    left = int((width - out_width) / 2)\n    right = int((width + out_width) / 2)\n    top = int((height - out_height) / 2)\n    bottom = int((height + out_height) / 2)\n    img = img[top:bottom, left:right]\n    return img\n\n# read the image\nfname = \"image_file\"\nimg = cv2.imread(fname)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# pre-process the image like mobilenet and resize it to 224x224\nimg = pre_process_edgetpu(img, (224, 224, 3))\nplt.axis('off')\nplt.imshow(img)\nplt.show()\n\n# create a batch of 1 (that batch size is buned into the saved_model)\nimg_batch = np.expand_dims(img, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Running Inference with ONNX Runtime\nDESCRIPTION: This code performs inference using the ONNX Runtime (ORT) on a given ONNX model ('model.onnx'). It initializes an InferenceSession, retrieves the output names and input name, and then runs the inference with the preprocessed image data. Finally, it prints the shape of the output detections.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# rt.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsess = rt.InferenceSession(\"model.onnx\")\n\noutputs = sess.get_outputs()\noutput_names = list(map(lambda output: output.name, outputs))\ninput_name = sess.get_inputs()[0].name\n\ndetections = sess.run(output_names, {input_name: image_data})\nprint(\"Output shape:\", list(map(lambda detection: detection.shape, detections)))\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for YOLOv4 ONNX\nDESCRIPTION: This code snippet preprocesses an image for YOLOv4 inference. It resizes the image, pads it to the target size, and normalizes the pixel values. It takes an image and target size as input and returns the preprocessed image. If ground truth boxes are provided, they are also scaled and translated accordingly.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nfrom onnx import numpy_helper\nimport onnx\nimport os\nfrom PIL import Image\nfrom matplotlib.pyplot import imshow\nimport onnxruntime as rt\nfrom scipy import special\nimport colorsys\nimport random\n%matplotlib inline\n\ndef image_preprocess(image, target_size, gt_boxes=None):\n\n    ih, iw = target_size\n    h, w, _ = image.shape\n\n    scale = min(iw/w, ih/h)\n    nw, nh = int(scale * w), int(scale * h)\n    image_resized = cv2.resize(image, (nw, nh))\n\n    image_padded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n    image_padded[dh:nh+dh, dw:nw+dw, :] = image_resized\n    image_padded = image_padded / 255.\n\n    if gt_boxes is None:\n        return image_padded\n\n    else:\n        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n        return image_padded, gt_boxes\n```\n\n----------------------------------------\n\nTITLE: T5 Generative Model Example (Python)\nDESCRIPTION: This snippet provides a complete example of how to use the generative T5 model for translation. It includes initializing the encoder/decoder, and calling the model with a prompt.  Requires the `onnxt5` package.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/t5/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnxt5 import GenerativeT5\nfrom onnxt5.api import get_encoder_decoder_tokenizer\ndecoder_sess, encoder_sess, tokenizer = get_encoder_decoder_tokenizer()\ngenerative_t5 = GenerativeT5(encoder_sess, decoder_sess, tokenizer, onnx=True)\nprompt = 'translate English to French: I was a victim of a series of accidents.'\noutput_text, output_logits = generative_t5(prompt, max_length=100, temperature=0.)\n```\n\n----------------------------------------\n\nTITLE: RoBERTa-SequenceClassification Postprocessing\nDESCRIPTION: This code snippet demonstrates post-processing of the RoBERTa-SequenceClassification model output. It takes the output tensor from the ONNX Runtime inference, determines the predicted sentiment (positive or negative) by finding the index with the maximum value, and prints the corresponding sentiment label.  It assumes that `ort_out` contains the model's output.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npred = np.argmax(ort_out)\nif(pred == 0):\n    print(\"Prediction: negative\")\nelif(pred == 1):\n    print(\"Prediction: positive\")\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Libraries in Python\nDESCRIPTION: This snippet imports essential libraries for image processing, numerical computation, ONNX handling, and PyTorch model operations. It includes PIL for image handling, NumPy for numerical operations, onnx for ONNX model manipulation, onnxruntime for ONNX inference, PyTorch for model definition and inference, and Torchvision for pre-trained models and image transformations.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport numpy as np\nfrom onnx import numpy_helper\nimport os\nimport onnxruntime as rt\nimport torch\nfrom torchvision import transforms, models\nimport urllib\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with T5 and onnxt5 Utilities (Python)\nDESCRIPTION: This snippet demonstrates how to generate embeddings for a given text using the T5 model and the `onnxt5` library.  It initializes the encoder and decoder sessions using the provided utility function, then runs the embedding generation for a given prompt. Requires the `onnxt5` package to be installed (`pip install onnxt5`).\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/t5/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnxt5.api import get_encoder_decoder_tokenizer, run_embeddings_text\n\ndecoder_sess, encoder_sess, tokenizer = get_encoder_decoder_tokenizer()\nprompt = 'Listen, Billy Pilgrim has come unstuck in time.'\nencoder_embeddings, decoder_embeddings = run_embeddings_text(encoder_sess, decoder_sess, tokenizer, prompt)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing MNIST Image - Python\nDESCRIPTION: This Python snippet preprocesses an image for input into the MNIST model. It loads the image, converts it to grayscale, resizes it to 28x28, normalizes the pixel values to the range [0.0, 1.0], and reshapes it into the expected input tensor format (1x1x28x28). Dependencies: numpy and cv2 (opencv-python).\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mnist/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport cv2\n\nimage = cv2.imread('input.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ngray = cv2.resize(gray, (28,28)).astype(np.float32)/255\ninput = np.reshape(gray, (1,1,28,28)\n```\n\n----------------------------------------\n\nTITLE: Load and Prepare Image for ONNX Inference in Python\nDESCRIPTION: This code loads an image using PIL, converts it to a NumPy array, applies the preprocessing transformations defined earlier, and prepares it as a tensor suitable for input to the ONNX model. The input tensor is detached from the computation graph, moved to the CPU, and converted to a NumPy array.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninput_image = Image.open(\"000000025205.jpg\")\norig_tensor = np.asarray(input_image)\ninput_tensor = preprocess(input_image)\ninput_tensor = input_tensor.unsqueeze(0)\ninput_tensor = input_tensor.detach().cpu().numpy()\n```\n\n----------------------------------------\n\nTITLE: Filtering and Refining Bounding Boxes - Python\nDESCRIPTION: The `postprocess_boxes` function filters and refines the predicted bounding boxes based on the original image shape, input size, and score threshold. It converts bounding box coordinates, clips them to the image boundaries, discards invalid boxes, and filters boxes based on confidence scores, returning the refined coordinates, scores, and class indices.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef postprocess_boxes(pred_bbox, org_img_shape, input_size, score_threshold):\n    '''remove boundary boxs with a low detection probability'''\n    valid_scale=[0, np.inf]\n    pred_bbox = np.array(pred_bbox)\n\n    pred_xywh = pred_bbox[:, 0:4]\n    pred_conf = pred_bbox[:, 4]\n    pred_prob = pred_bbox[:, 5:]\n\n    # # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n    # # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n    org_h, org_w = org_img_shape\n    resize_ratio = min(input_size / org_w, input_size / org_h)\n\n    dw = (input_size - resize_ratio * org_w) / 2\n    dh = (input_size - resize_ratio * org_h) / 2\n\n    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n\n    # # (3) clip some boxes that are out of range\n    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n    pred_coor[invalid_mask] = 0\n\n    # # (4) discard some invalid boxes\n    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n\n    # # (5) discard some boxes with low scores\n    classes = np.argmax(pred_prob, axis=-1)\n    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n    score_mask = scores > score_threshold\n    mask = np.logical_and(scale_mask, score_mask)\n    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n\n    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies (Python)\nDESCRIPTION: This snippet imports necessary libraries for image processing, model definition, ONNX Runtime execution, and data manipulation. It includes modules for PyTorch, ONNX Runtime, NumPy, PIL (Pillow), and Matplotlib.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/dependencies/Run_Super_Resolution_Model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport io\nimport numpy as np\nimport onnxruntime\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx\nimport torch.nn as nn\nimport torch.nn.init as init\nimport matplotlib.pyplot as plt\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nfrom resizeimage import resizeimage\nimport numpy as np\nimport pdb\nimport onnx\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Encoding Text for GPT-2 with Tokenizer in Python\nDESCRIPTION: This snippet demonstrates how to encode input text using the GPT2Tokenizer from the transformers library. It initializes the tokenizer and then encodes the input text into a tensor of token IDs. This tensor can then be used as input to the GPT-2 model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/gpt-2/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntext = \"Here is some text to encode : Hello World\"\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokens_tensor = torch.tensor([torch.tensor(tokenizer.encode(text))])\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing using PyTorch Transforms\nDESCRIPTION: This snippet shows the preprocessing steps required for input images. It uses PyTorch's `transforms` module to resize the image, center crop it, convert it to a tensor, and normalize its pixel values. The normalization is done using specific mean and standard deviation values calculated from the ImageNet dataset.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/shufflenet/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninput_image = Image.open(filename)\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\n```\n\n----------------------------------------\n\nTITLE: Import ONNX Model in Python\nDESCRIPTION: Loads an ONNX model from the specified file path ('resnet50-v1-12.onnx') using the onnx.load() function. Then, it creates an ONNX Runtime InferenceSession to execute the model. The model's serialized string is passed to the InferenceSession constructor.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_path = 'resnet50-v1-12.onnx'\nmodel = onnx.load(model_path)\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider']).\nsession = ort.InferenceSession(model.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: RoBERTa Input Preprocessing with Tokenizer\nDESCRIPTION: This code snippet demonstrates how to preprocess input text for RoBERTa models using the RobertaTokenizer. It loads a pre-trained tokenizer, encodes the input text, and converts it into a tensor suitable for the model. It requires the torch, numpy, simpletransformers, and transformers libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom simpletransformers.model import TransformerModel\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\ntext = \"This film is so good\"\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ninput_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n```\n\n----------------------------------------\n\nTITLE: ONNX Model Preprocessing\nDESCRIPTION: This snippet defines an ONNX model for preprocessing images before feeding them into a ResNet model. It includes resizing, center cropping, normalization with mean and standard deviation, and transposing the image layout. The model uses custom operators within the 'local' domain for preprocessing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import parser\nfrom onnx import checker\n\nresnet_preproc = parser.parse_model('''\n<\n  ir_version: 8,\n  opset_import: [ \\\"\\\" : 18, \\\"local\\\" : 1 ],\n  metadata_props: [ \\\"preprocessing_fn\\\" : \\\"local.preprocess\\\"]\n>\nresnet_preproc_g (seq(uint8[?, ?, 3]) images) => (float[B, 3, 224, 224] preproc_data)\n{\n    preproc_data = local.preprocess(images)\n}\n\n<\n  opset_import: [ \\\"\\\" : 18 ],\n  domain: \\\"local\\\",\n  doc_string: \\\"Preprocessing function.\\\"\n>\npreprocess (input_batch) => (output_tensor) {\n    tmp_seq = SequenceMap <\n        body = sample_preprocessing(uint8[?, ?, 3] sample_in) => (float[3, 224, 224] sample_out) {\n            target_size = Constant <value = int64[2] {256, 256}> ()\n            image_resized = Resize <mode = \\\"linear\\\",\n                                    antialias = 1,\n                                    axes = [0, 1],\n                                    keep_aspect_ratio_policy = \\\"not_smaller\\\"> (sample_in, , , target_size)\n\n            target_crop = Constant <value = int64[2] {224, 224}> ()\n            image_sliced = CenterCropPad <axes = [0, 1]> (image_resized, target_crop)\n\n            kMean = Constant <value = float[3] {123.675, 116.28, 103.53}> ()\n            kStddev = Constant <value = float[3] {58.395, 57.12, 57.375}> ()\n            im_norm_tmp1 = Cast <to = 1> (image_sliced)\n            im_norm_tmp2 = Sub (im_norm_tmp1, kMean)\n            im_norm = Div (im_norm_tmp2, kStddev)\n\n            sample_out = Transpose <perm = [2, 0, 1]> (im_norm)\n        }\n    > (input_batch)\n    output_tensor = ConcatFromSequence < axis = 0, new_axis = 1 >(tmp_seq)\n}\n\n''')\nchecker.check_model(resnet_preproc)\n```\n\n----------------------------------------\n\nTITLE: ResNetV2 Class Definition in MXNet/Gluon\nDESCRIPTION: Defines the ResNetV2 class using MXNet's HybridBlock, based on the 'Identity Mappings in Deep Residual Networks' paper. It includes similar parameters to ResNetV1, but implements the ResNetV2 architecture, incorporating batch normalization and identity mappings for improved performance.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass ResNetV2(HybridBlock):\n    r\"\"\"ResNet V2 model from\n    `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    Parameters\n    ----------\n    block : HybridBlock\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    channels : list of int\n        Numbers of channels in each block. Length should be one larger than layers list.\n    classes : int, default 1000\n        Number of classification classes.\n    thumbnail : bool, default False\n        Enable thumbnail.\n    \"\"\"\n    def __init__(self, block, layers, channels, classes=1000, thumbnail=False, **kwargs):\n        super(ResNetV2, self).__init__(**kwargs)\n        assert len(layers) == len(channels) - 1\n        with self.name_scope():\n            self.features = nn.HybridSequential(prefix='')\n            self.features.add(nn.BatchNorm(scale=False, center=False))\n            if thumbnail:\n                self.features.add(_conv3x3(channels[0], 1, 0))\n            else:\n                self.features.add(nn.Conv2D(channels[0], 7, 2, 3, use_bias=False))\n                self.features.add(nn.BatchNorm())\n                self.features.add(nn.Activation('relu'))\n                self.features.add(nn.MaxPool2D(3, 2, 1))\n\n            in_channels = channels[0]\n            for i, num_layer in enumerate(layers):\n                stride = 1 if i == 0 else 2\n                self.features.add(self._make_layer(block, num_layer, channels[i+1],\n                                                   stride, i+1, in_channels=in_channels))\n                in_channels = channels[i+1]\n            self.features.add(nn.BatchNorm())\n            self.features.add(nn.Activation('relu'))\n            self.features.add(nn.GlobalAvgPool2D())\n            self.features.add(nn.Flatten())\n\n            self.output = nn.Dense(classes, in_units=in_channels)\n\n    def _make_layer(self, block, layers, channels, stride, stage_index, in_channels=0):\n        layer = nn.HybridSequential(prefix='stage%d_'%stage_index)\n        with layer.name_scope():\n            layer.add(block(channels, stride, channels != in_channels, in_channels=in_channels,\n                            prefix=''))\n            for _ in range(layers-1):\n                layer.add(block(channels, 1, False, in_channels=channels, prefix=''))\n        return layer\n\n    def hybrid_forward(self, F, x):\n        x = self.features(x)\n        x = self.output(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Inference Results in Python\nDESCRIPTION: This code snippet demonstrates how to process and print the output results from the EfficientNet-Lite4 model. It loads the model using ONNX Runtime, performs inference, and then prints the top 5 predicted labels along with their associated probabilities.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/efficientnet-lite4/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# load the model\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# rt.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsess = rt.InferenceSession(MODEL + \".onnx\")\n# run inference and print results\nresults = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\nresult = reversed(results[0].argsort()[-5:])\nfor r in result:\n    print(r, labels[str(r)], results[0][r])\n```\n\n----------------------------------------\n\nTITLE: ONNX Runtime Inference and Metrics\nDESCRIPTION: Sets up ONNX Runtime for inference and defines functions to predict pixelwise class labels, calculate Intersection over Union (IoU), and compute pixelwise accuracy. It includes the `compute_validation_accuracy` function, which iterates through image IDs, performs inference, and computes the mean IoU and global pixelwise accuracy. The `predict` method transforms the model output into a one-hot encoded tensor for each pixel.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/validation_accuracy.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import numpy_helper\nimport os\nimport onnxruntime as rt\n\n\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# rt.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsess = rt.InferenceSession(\"../model/fcn-resnet101-11.onnx\")\n\ndef predict(sess, input_tensor):\n    \"\"\"\n    Given an input tensor, create a (N, 21, height, width) one-hot\n    binary mask of pixelwise class predictions\n    \"\"\"\n    # The names for the FCN inputs/outputs are known, use them directly\n    model_tensor = sess.run([\"out\"], {\"input\": input_tensor})[0]\n    batch_size, nclasses, height, width = model_tensor.shape\n    \n    raw_labels = np.argmax(model_tensor, axis=1).astype(np.uint8)\n    # We need to convert from the argmax of each pixel into a one-hot binary tensor,\n    # which can be done with numpy's excellent boolean indexing\n    output_tensor = np.zeros((nclasses, batch_size, height, width), dtype=np.uint8)\n    \n    for c in range(nclasses):\n        output_tensor[c][raw_labels==c] = 1\n\n    output_tensor = np.transpose(output_tensor, [1, 0, 2, 3])\n    \n    return output_tensor\n\ndef iou(model_tensor, target_tensor):\n    # Don't include the background when summing\n    model_tensor = model_tensor[:, 1:, :, :]\n    target_tensor = target_tensor[:, 1:, :, :]\n    \n    intersection = np.sum(np.logical_and(model_tensor, target_tensor))\n    union = np.sum(np.logical_or(model_tensor, target_tensor))\n    \n    if union == 0:\n        # Can only happen if nothing was there and nothing was predicted,\n        # which is a perfect score\n        return 1\n    else:\n        return intersection / union\n\ndef pixelwise_accuracy(model_tensor, target_tensor):\n    batch_size, nclasses, height, width = model_tensor.shape\n    # Again, don't include the background in the accuracy\n    total_pixels = batch_size * (nclasses - 1) * height * width\n    model_tensor = model_tensor[:, 1:, :, :]\n    target_tensor = target_tensor[:, 1:, :, :]\n    \n    # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n    return np.sum(model_tensor == target_tensor) / total_pixels\n\ndef compute_validation_accuracy(gt, sess, imgIds, print_every=50):\n    \"\"\"\n    Given the ground truth annotations, compute the mean IoU and\n    global pixelwise accuracy for a given model instantiated in sess\n    on the image ids.\n    \n    This code is serial (non-batched) so it will be fairly slow.\n    \"\"\"\n    \n    totalIoU = 0\n    totalAcc = 0\n    totalImgs = len(imgIds)\n    \n    for i in range(totalImgs):\n        imgId = imgIds[i]\n        input_tensor, target_tensor = load_image_and_ann(gt, imgId)\n        model_tensor = predict(sess, input_tensor)\n\n        totalIoU += iou(model_tensor, target_tensor)\n        totalAcc += pixelwise_accuracy(model_tensor, target_tensor)\n        if print_every is not None and i % print_every == print_every - 1:\n            print(f\"Completed {i+1}/{totalImgs}.\\tmean IoU: {totalIoU / (i+1)}\\tGPA: {totalAcc / (i+1)}\")\n        \n    return totalIoU / totalImgs, totalAcc / totalImgs\n\nmeanIoU, gpa = compute_validation_accuracy(cocoGt, sess, imgIds)\n```\n\n----------------------------------------\n\nTITLE: Configure Face Detection Model\nDESCRIPTION: This code configures the MTCNN face detector by determining the context (CPU or GPU) and setting the detection threshold. It also specifies the path to the MTCNN model folder and initializes the MtcnnDetector object with the configured parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Determine and set context\nif len(mx.test_utils.list_gpus())==0:\n    ctx = mx.cpu()\nelse:\n    ctx = mx.gpu(0)\n# Configure face detector\ndet_threshold = [0.6,0.7,0.8]\nmtcnn_path = os.path.join(os.path.dirname('__file__'), 'mtcnn-model')\ndetector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=1, accurate_landmark = True, threshold=det_threshold)\n```\n\n----------------------------------------\n\nTITLE: Downloading VGG16 ONNX Model\nDESCRIPTION: Downloads the VGG16 model in ONNX format from the specified URL. This is the first step in preparing the model for quantization or inference. Requires wget.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/vgg/model/vgg16-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Prediction Function for GoogleNet in Python\nDESCRIPTION: This Python function predicts the class of an image using a pre-trained model. It takes the image path as input, preprocesses the image, performs a forward pass through the model, applies softmax to obtain probabilities, and returns the indices of the sorted probabilities in descending order, effectively providing the top predicted classes.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/googlenet/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef predict(path):\n    # based on : https://mxnet.apache.org/versions/1.0.0/tutorials/python/predict_image.html\n    img = get_image(path)\n    img = preprocess(img)\n    mod.forward(Batch([mx.nd.array(img)]))\n    # Take softmax to generate probabilities\n    prob = mod.get_outputs()[0].asnumpy()\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n    return a\n```\n\n----------------------------------------\n\nTITLE: Validation Error Calculation\nDESCRIPTION: The `test` function calculates the validation error on the provided `val_data` using a given context `ctx`. It iterates through the validation data, performs a forward pass, updates accuracy metrics (top1 and top5), and returns the top1 and top5 errors. It uses the `gluon` library for data loading and metric calculation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Test function\ndef test(ctx, val_data):\n    # Reset accuracy metrics\n    acc_top1.reset()\n    acc_top5.reset()\n    for i, batch in enumerate(val_data):\n        # Load validation batch\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n        # Perform forward pass\n        outputs = [net(X) for X in data]\n        # Update accuracy metrics\n        acc_top1.update(label, outputs)\n        acc_top5.update(label, outputs)\n    # Retrieve and return top1 and top5 errors\n    _, top1 = acc_top1.get()\n    _, top5 = acc_top5.get()\n    return (1-top1, 1-top5)\n```\n\n----------------------------------------\n\nTITLE: Model Training Loop with Gluon\nDESCRIPTION: This `train` function trains the model for a specified number of epochs using the given context `ctx`. It initializes the network, prepares training and validation data loaders, defines the trainer and loss function, and then iterates through the epochs and batches to perform forward and backward passes, update the model parameters, and compute the training and validation errors. It also includes learning rate decay, model snapshot saving, and training progress logging. It uses `gluon`, `autograd`, `mxnet`, and `time` libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Train function\ndef train(epochs, ctx):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    # Initialize network - Use method in MSRA paper <https://arxiv.org/abs/1502.01852>\n    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n    # Prepare train and validation batches\n    transform_train = preprocess_train_data(normalize, jitter_param, lighting_param)\n    transform_test = preprocess_test_data(normalize)\n    train_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=True).transform_first(transform_train),\n        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n    val_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    # Define trainer\n    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n    # Define loss\n    L = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    lr_decay_count = 0\n\n    best_val_score = 1\n    # Main training loop - loop over epochs\n    for epoch in range(epochs):\n        tic = time.time()\n        # Reset accuracy metrics\n        acc_top1.reset()\n        acc_top5.reset()\n        btic = time.time()\n        train_loss = 0\n        num_batch = len(train_data)\n        \n        # Check and perform learning rate decay\n        if lr_decay_period and epoch and epoch % lr_decay_period == 0:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n        elif lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n            lr_decay_count += 1\n        # Loop over batches in an epoch\n        for i, batch in enumerate(train_data):\n            # Load train batch\n            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n            label_smooth = label\n            # Perform forward pass\n            with ag.record():\n                outputs = [net(X) for X in data]\n                loss = [L(yhat, y) for yhat, y in zip(outputs, label_smooth)]\n            # Perform backward pass\n            ag.backward(loss)\n            # PErform updates\n            trainer.step(batch_size)\n            # Update accuracy metrics\n            acc_top1.update(label, outputs)\n            acc_top5.update(label, outputs)\n            # Update loss\n            train_loss += sum([l.sum().asscalar() for l in loss])\n            # Log training progress (after each `log_interval` batches)\n            if log_interval and not (i+1)%log_interval:\n                _, top1 = acc_top1.get()\n                _, top5 = acc_top5.get()\n                err_top1, err_top5 = (1-top1, 1-top5)\n                logging.info('Epoch[%d] Batch [%d]\tSpeed: %f samples/sec\ttop1-err=%f\ttop5-err=%f'%(epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n                btic = time.time()\n\n        # Retrieve training errors and loss\n        _, top1 = acc_top1.get()\n        _, top5 = acc_top5.get()\n        err_top1, err_top5 = (1-top1, 1-top5)\n        train_loss /= num_batch * batch_size\n\n        # Compute validation errors\n        err_top1_val, err_top5_val = test(ctx, val_data)\n        # Update training history\n        train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n        # Update plot\n        train_history.plot(['training-top1-err', 'validation-top1-err','training-top5-err', 'validation-top5-err'],\n                           save_path='%s/%s_top_error.png'%(save_plot_dir, model_name))\n\n        # Log training progress (after each epoch)\n        logging.info('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n        logging.info('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n\n        # Save a snapshot of the best model - use net.export to get MXNet symbols and params\n        if err_top1_val < best_val_score and epoch > 50:\n            best_val_score = err_top1_val\n            net.export('%s/%.4f-imagenet-%s-best'%(save_dir, best_val_score, model_name), epoch)\n        # Save a snapshot of the model after each 'save_frequency' epochs\n        if save_frequency and save_dir and (epoch + 1) % save_frequency == 0:\n            net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epoch)\n    # Save a snapshot of the model at the end of training\n    if save_frequency and save_dir:\n        net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epochs-1)\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing in Python\nDESCRIPTION: This code snippet shows how to preprocess an image for the SSD-MobilenetV1 model using PIL (Pillow) and NumPy. It opens an image file, converts it to a NumPy array, reshapes it to the expected input format (NHWC), and displays the image using matplotlib.pyplot. Dependencies include numpy, PIL, and matplotlib.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageColor\nimport math\nimport matplotlib.pyplot as plt\n\n# open and display image file\nimg = Image.open(\"image file\")\nplt.axis('off')\nplt.imshow(img)\nplt.show()\n\n# reshape the flat array returned by img.getdata() to HWC and than add an additial\ndimension to make NHWC, aka a batch of images with 1 image in it\nimg_data = np.array(img.getdata()).reshape(img.size[1], img.size[0], 3)\nimg_data = np.expand_dims(img_data.astype(np.uint8), axis=0)\n```\n\n----------------------------------------\n\nTITLE: Run FCN Inference with ONNX Runtime in Python\nDESCRIPTION: This code initializes an ONNX Runtime inference session with a pre-trained FCN model. It retrieves the output names and input name of the model, then runs the inference with the preprocessed input tensor. The output shape is printed for verification, and the output and auxiliary outputs are stored in `output` and `aux` respectively.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import numpy_helper\nimport os\nimport onnxruntime as rt\n\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsess = rt.InferenceSession(\"../model/fcn-resnet101-11.onnx\")\n\noutputs = sess.get_outputs()\noutput_names = list(map(lambda output: output.name, outputs))\ninput_name = sess.get_inputs()[0].name\n\ndetections = sess.run(output_names, {input_name: input_tensor})\nprint(\"Output shape:\", list(map(lambda detection: detection.shape, detections)))\noutput, aux = detections\n```\n\n----------------------------------------\n\nTITLE: Postprocessing RetinaNet Detections (PyTorch)\nDESCRIPTION: This code snippet shows how to postprocess the RetinaNet model's outputs. It includes generating anchor boxes, decoding and filtering box predictions, and applying non-maximum suppression (NMS) to obtain the final detections. It depends on `torch`, `retinanet.box.generate_anchors`, `retinanet.box.decode`, and `retinanet.box.nms` from NVIDIA's retinanet-examples repository.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/retinanet/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom retinanet.box import generate_anchors, decode, nms\n\ndef detection_postprocess(image, cls_heads, box_heads):\n\t# Inference post-processing\n\tanchors = {}\n\tdecoded = []\n\n\tfor cls_head, box_head in zip(cls_heads, box_heads):\n\t    # Generate level's anchors\n\t    stride = image.shape[-1] // cls_head.shape[-1]\n\t    if stride not in anchors:\n\t        anchors[stride] = generate_anchors(stride, ratio_vals=[1.0, 2.0, 0.5],\n\t                                           scales_vals=[4 * 2 ** (i / 3) for i in range(3)])\n\t    # Decode and filter boxes\n\t    decoded.append(decode(cls_head, box_head, stride,\n\t                          threshold=0.05, top_n=1000, anchors=anchors[stride]))\n\n\t# Perform non-maximum suppression\n\tdecoded = [torch.cat(tensors, 1) for tensors in zip(*decoded)]\n\t# NMS threshold = 0.5\n\tscores, boxes, labels = nms(*decoded, nms=0.5, ndetections=100)\n\treturn scores, boxes, labels\n\n\nscores, boxes, labels = detection_postprocess(input_image, cls_heads, box_heads)\n```\n\n----------------------------------------\n\nTITLE: Preprocess Image for Face Recognition\nDESCRIPTION: The `preprocess` function aligns a face image based on either landmark points or a bounding box. If landmark points are provided, an affine transformation is applied. If only a bounding box is available, the image is cropped and resized. The `get_input` function uses a face detector to find faces in the input image and then preprocesses them using the `preprocess` function, returning the aligned face image.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(img, bbox=None, landmark=None, **kwargs):\n    M = None\n    image_size = []\n    str_image_size = kwargs.get('image_size', '')\n    # Assert input shape\n    if len(str_image_size)>0:\n        image_size = [int(x) for x in str_image_size.split(',')]\n        if len(image_size)==1:\n            image_size = [image_size[0], image_size[0]]\n        assert len(image_size)==2\n        assert image_size[0]==112\n        assert image_size[0]==112 or image_size[1]==96\n    \n    # Do alignment using landmark points\n    if landmark is not None:\n        assert len(image_size)==2\n        src = np.array([\n          [30.2946, 51.6963],\n          [65.5318, 51.5014],\n          [48.0252, 71.7366],\n          [33.5493, 92.3655],\n          [62.7299, 92.2041] ], dtype=np.float32 )\n        if image_size[1]==112:\n            src[:,0] += 8.0\n        dst = landmark.astype(np.float32)\n        tform = trans.SimilarityTransform()\n        tform.estimate(dst, src)\n        M = tform.params[0:2,:]\n        assert len(image_size)==2\n        warped = cv2.warpAffine(img,M,(image_size[1],image_size[0]), borderValue = 0.0)\n        return warped\n    \n    # If no landmark points available, do alignment using bounding box. If no bounding box available use center crop\n    if M is None:\n        if bbox is None:\n            det = np.zeros(4, dtype=np.int32)\n            det[0] = int(img.shape[1]*0.0625)\n            det[1] = int(img.shape[0]*0.0625)\n            det[2] = img.shape[1] - det[0]\n            det[3] = img.shape[0] - det[1]\n        else:\n            det = bbox\n        margin = kwargs.get('margin', 44)\n        bb = np.zeros(4, dtype=np.int32)\n        bb[0] = np.maximum(det[0]-margin/2, 0)\n        bb[1] = np.maximum(det[1]-margin/2, 0)\n        bb[2] = np.minimum(det[2]+margin/2, img.shape[1])\n        bb[3] = np.minimum(det[3]+margin/2, img.shape[0])\n        ret = img[bb[1]:bb[3],bb[0]:bb[2],:]\n        if len(image_size)>0:\n            ret = cv2.resize(ret, (image_size[1], image_size[0]))\n        return ret\n    \ndef get_input(detector,face_img):\n    # Pass input images through face detector\n    ret = detector.detect_face(face_img, det_type = 0)\n    if ret is None:\n        return None\n    bbox, points = ret\n    if bbox.shape[0]==0:\n        return None\n    bbox = bbox[0,0:4]\n    points = points[0,:].reshape((2,5)).T\n    # Call preprocess() to generate aligned images\n    nimg = preprocess(face_img, bbox, points, image_size='112,112')\n    nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)\n    aligned = np.transpose(nimg, (2,0,1))\n    return aligned\n```\n\n----------------------------------------\n\nTITLE: Define Training Loop in Python\nDESCRIPTION: This function implements the main training loop for the model. It iterates through epochs and batches, performs forward and backward passes, updates model parameters using the trainer, and calculates training and validation errors. It also handles learning rate decay, model saving, and logging of training progress.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef train(epochs, ctx):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    # Initialize network\n    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n    # Prepare train and validation batches\n    transform_train = preprocess_train_data(normalize, jitter_param, lighting_param)\n    transform_test = preprocess_test_data(normalize)\n    train_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=True).transform_first(transform_train),\n        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n    val_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    # Define trainer\n    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n    # Define loss\n    L = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    lr_decay_count = 0\n\n    best_val_score = 1\n    # Main training loop - loop over epochs\n    for epoch in range(epochs):\n        tic = time.time()\n        # Reset accuracy metrics\n        acc_top1.reset()\n        acc_top5.reset()\n        btic = time.time()\n        train_loss = 0\n        num_batch = len(train_data)\n        \n        # Check and perform learning rate decay\n        if lr_decay_period and epoch and epoch % lr_decay_period == 0:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n        elif lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n            lr_decay_count += 1\n        # Loop over batches in an epoch\n        for i, batch in enumerate(train_data):\n            # Load train batch\n            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n            label_smooth = label\n            # Perform forward pass\n            with ag.record():\n                outputs = [net(X) for X in data]\n                loss = [L(yhat, y) for yhat, y in zip(outputs, label_smooth)]\n            # Perform backward pass\n            for l in loss:\n                l.backward()\n            # Perform updates\n            trainer.step(batch_size)\n            # Update accuracy metrics\n            acc_top1.update(label, outputs)\n            acc_top5.update(label, outputs)\n            # Update loss\n            train_loss += sum([l.sum().asscalar() for l in loss])\n            # Log training progress (after each `log_interval` batches)\n            if log_interval and not (i+1)%log_interval:\n                _, top1 = acc_top1.get()\n                _, top5 = acc_top5.get()\n                err_top1, err_top5 = (1-top1, 1-top5)\n                logging.info('Epoch[%d] Batch [%d]\\tSpeed: %f samples/sec\\ttop1-err=%f\\ttop5-err=%f'%(epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n                btic = time.time()\n                \n        # Retrieve training errors and loss\n        _, top1 = acc_top1.get()\n        _, top5 = acc_top5.get()\n        err_top1, err_top5 = (1-top1, 1-top5)\n        train_loss /= num_batch * batch_size\n\n        # Compute validation errors\n        err_top1_val, err_top5_val = test(ctx, val_data)\n        # Update training history\n        train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n        # Update plot\n        train_history.plot(['training-top1-err', 'validation-top1-err','training-top5-err', 'validation-top5-err'],\n                           save_path='%s/%s_top_error.png'%(save_plot_dir, model_name))\n        logging.info('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n        logging.info('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n    \n        # Save a snapshot of the best model - use net.export to get MXNet symbols and params\n        if err_top1_val < best_val_score and epoch > 50:\n            best_val_score = err_top1_val\n            net.export('%s/%.4f-imagenet-%s-best'%(save_dir, best_val_score, model_name), epoch)\n        # Save a snapshot of the model after each 'save_frequency' epochs\n        if save_frequency and save_dir and (epoch + 1) % save_frequency == 0:\n            net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epoch)\n    # Save a snapshot of the model at the end of training\n    if save_frequency and save_dir:\n        net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epochs-1)\n```\n\n----------------------------------------\n\nTITLE: Merging Preprocessing and Network Models in ONNX\nDESCRIPTION: This code snippet demonstrates how to merge a preprocessing ONNX model with a network ONNX model using `onnx.compose.merge_models`. This allows you to prepend preprocessing steps directly to the model, creating a single ONNX graph that handles both preprocessing and inference.  It also includes version conversion and model checking.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import version_converter\nfrom onnx import checker\n\nnetwork_model = onnx.version_converter.convert_version(network_model, 18)\nnetwork_model.ir_version = 8\nchecker.check_model(network_model)\n\nmodel_w_preproc = onnx.compose.merge_models(\n    preprocessing_model, network_model,\n    io_map=[('preproc_data', 'data')]\n)\nchecker.check_model(model_w_preproc)\n\n```\n\n----------------------------------------\n\nTITLE: Drawing Bounding Boxes on Images - Python\nDESCRIPTION: The `draw_bbox` function draws bounding boxes on an image. It takes the image, bounding boxes, class names, and a flag to show labels as input. It assigns colors to each class, draws rectangles around the bounding boxes, and optionally displays class labels and scores. It requires the `cv2` library for drawing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef draw_bbox(image, bboxes, classes=read_class_names(\"coco.names\"), show_label=True):\n    \"\"\"\n    bboxes: [x_min, y_min, x_max, y_max, probability, cls_id] format coordinates.\n    \"\"\"\n\n    num_classes = len(classes)\n    image_h, image_w, _ = image.shape\n    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n\n    random.seed(0)\n    random.shuffle(colors)\n    random.seed(None)\n\n    for i, bbox in enumerate(bboxes):\n        coor = np.array(bbox[:4], dtype=np.int32)\n        fontScale = 0.5\n        score = bbox[4]\n        class_ind = int(bbox[5])\n        bbox_color = colors[class_ind]\n        bbox_thick = int(0.6 * (image_h + image_w) / 600)\n        c1, c2 = (coor[0], coor[1]), (coor[2], coor[3])\n        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)\n\n        if show_label:\n            bbox_mess = '%s: %.2f' % (classes[class_ind], score)\n            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, thickness=bbox_thick//2)[0]\n            cv2.rectangle(image, c1, (c1[0] + t_size[0], c1[1] - t_size[1] - 3), bbox_color, -1)\n            cv2.putText(image, bbox_mess, (c1[0], c1[1]-2), cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale, (0, 0, 0), bbox_thick//2, lineType=cv2.LINE_AA)\n\n    return image\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX Opset Version (Python)\nDESCRIPTION: This Python snippet converts the ONNX model's opset version to 12.  This is done to improve quantization compatibility. The snippet loads the model, converts its version, and saves the converted model. Requires the onnx package and onnx version converter.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/ultraface/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import version_converter\nmodel = onnx.load('version-RFB-320.onnx')\nmodel = version_converter.convert_version(model, 12)\nonnx.save_model(model, 'version-RFB-320-12.onnx')\n```\n\n----------------------------------------\n\nTITLE: Preprocess Image in Python\nDESCRIPTION: Preprocesses an image for inference by scaling pixel values to the range [0, 1], resizing the image to 256x256, performing a center crop of 224x224, normalizing the image using ImageNet mean and standard deviation, transposing the image to NCHW format, casting the data type to float32, and adding a batch dimension. Input: `img` - Input image as a NumPy array.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(img):\n    img = img / 255.\n    img = cv2.resize(img, (256, 256))\n    h, w = img.shape[0], img.shape[1]\n    y0 = (h - 224) // 2\n    x0 = (w - 224) // 2\n    img = img[y0 : y0+224, x0 : x0+224, :]\n    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n    img = np.transpose(img, axes=[2, 0, 1])\n    img = img.astype(np.float32)\n    img = np.expand_dims(img, axis=0)\n    return img\n```\n\n----------------------------------------\n\nTITLE: Training Function - MXNet/Gluon\nDESCRIPTION: Defines a `train` function to train the model for a specified number of epochs using the given context. It initializes the network, prepares training and validation data loaders, defines the trainer and loss function, and then enters the main training loop. The function saves the model periodically and at the end of training, and generates plots of the training progress. Requires number of epochs `epochs` and context `ctx`.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Train function\ndef train(epochs, ctx):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    # Initialize network - Use method in MSRA paper <https://arxiv.org/abs/1502.01852>\n    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n    # Prepare train and validation batches\n    transform_train = preprocess_train_data(normalize, jitter_param, lighting_param)\n    transform_test = preprocess_test_data(normalize)\n    train_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=True).transform_first(transform_train),\n        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n    val_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    # Define trainer\n    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n    # Define loss\n    L = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    lr_decay_count = 0\n\n    best_val_score = 1\n    # Main training loop - loop over epochs\n    for epoch in range(epochs):\n        tic = time.time()\n        # Reset accuracy metrics\n        acc_top1.reset()\n        acc_top5.reset()\n        btic = time.time()\n        train_loss = 0\n        num_batch = len(train_data)\n        \n        # Check and perform learning rate decay\n        if lr_decay_period and epoch and epoch % lr_decay_period == 0:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n        elif lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n            lr_decay_count += 1\n        # Loop over batches in an epoch\n        for i, batch in enumerate(train_data):\n            # Load train batch\n            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n            label_smooth = label\n            # Perform forward pass\n            with ag.record():\n                outputs = [net(X) for X in data]\n                loss = [L(yhat, y) for yhat, y in zip(outputs, label_smooth)]\n            # Perform backward pass\n            ag.backward(loss)\n            # PErform updates\n            trainer.step(batch_size)\n            # Update accuracy metrics\n            acc_top1.update(label, outputs)\n            acc_top5.update(label, outputs)\n            # Update loss\n            train_loss += sum([l.sum().asscalar() for l in loss])\n            # Log training progress (after each `log_interval` batches)\n            if log_interval and not (i+1)%log_interval:\n                _, top1 = acc_top1.get()\n                _, top5 = acc_top5.get()\n                err_top1, err_top5 = (1-top1, 1-top5)\n                logging.info('Epoch[%d] Batch [%d]\\tSpeed: %f samples/sec\\ttop1-err=%f\\ttop5-err=%f'%(epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n                btic = time.time()\n\n        # Retrieve training errors and loss\n        _, top1 = acc_top1.get()\n        _, top5 = acc_top5.get()\n        err_top1, err_top5 = (1-top1, 1-top5)\n        train_loss /= num_batch * batch_size\n\n        # Compute validation errors\n        err_top1_val, err_top5_val = test(ctx, val_data)\n        # Update training history\n        train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n        # Update plot\n        train_history.plot(['training-top1-err', 'validation-top1-err','training-top5-err', 'validation-top5-err'],\n                           save_path='%s/%s_top_error.png'%(save_plot_dir, model_name))\n\n        # Log training progress (after each epoch)\n        logging.info('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n        logging.info('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n\n        # Save a snapshot of the best model - use net.export to get MXNet symbols and params\n        if err_top1_val < best_val_score and epoch > 50:\n            best_val_score = err_top1_val\n            net.export('%s/%.4f-imagenet-%s-best'%(save_dir, best_val_score, model_name), epoch)\n        # Save a snapshot of the model after each 'save_frequency' epochs\n        if save_frequency and save_dir and (epoch + 1) % save_frequency == 0:\n            net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epoch)\n    # Save a snapshot of the model at the end of training\n    if save_frequency and save_dir:\n        net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epochs-1)\n```\n\n----------------------------------------\n\nTITLE: Define Training Data Preprocessing in Python\nDESCRIPTION: This function defines the preprocessing steps applied to the training data. It includes resizing, random cropping, horizontal flipping, color jittering, and lighting augmentation using the `transforms` module. The data is then converted to a tensor and normalized using the pre-defined `normalize` transform.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train_data(normalize, jitter_param, lighting_param):\n    transform_train = transforms.Compose([\n        transforms.Resize(480),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomFlipLeftRight(),\n        transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                     saturation=jitter_param),\n        transforms.RandomLighting(lighting_param),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_train\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Results for BERT-Squad\nDESCRIPTION: This code snippet postprocesses the output of the BERT-Squad model to write predictions (answers to the questions) in a JSON file. It creates an output directory if it doesn't exist and calls the write_predictions function to generate the output files containing predicted answers and n-best predictions.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# postprocess results\noutput_dir = 'predictions'\nos.makedirs(output_dir, exist_ok=True)\noutput_prediction_file = os.path.join(output_dir, \"predictions.json\")\noutput_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\nwrite_predictions(eval_examples, extra_data, all_results,\n                  n_best_size, max_answer_length,\n                  True, output_prediction_file, output_nbest_file)\n```\n\n----------------------------------------\n\nTITLE: YOLOv4 Postprocessing Pipeline - Python\nDESCRIPTION: This code snippet demonstrates the typical postprocessing pipeline for YOLOv4 detections. It initializes anchor, stride, and scale parameters, then calls the `postprocess_bbbox`, `postprocess_boxes`, `nms`, and `draw_bbox` functions to transform the raw detections into a visual output image. It assumes the existence of a `detections` array, `original_image_size`, and `input_size` variables.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nANCHORS = \"./yolov4_anchors.txt\"\nSTRIDES = [8, 16, 32]\nXYSCALE = [1.2, 1.1, 1.05]\n\nANCHORS = get_anchors(ANCHORS)\nSTRIDES = np.array(STRIDES)\n\npred_bbox = postprocess_bbbox(detections, ANCHORS, STRIDES, XYSCALE)\nbboxes = postprocess_boxes(pred_bbox, original_image_size, input_size, 0.25)\nbboxes = nms(bboxes, 0.213, method='nms')\nimage = draw_bbox(original_image, bboxes)\n```\n\n----------------------------------------\n\nTITLE: YOLOv3 Output Postprocessing\nDESCRIPTION: This Python code snippet shows how to post-process the output of the YOLOv3 model to extract bounding boxes, scores, and classes. It iterates through the indices of detected objects and retrieves the corresponding bounding box coordinates, scores, and class labels from the model's output tensors.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov3/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nout_boxes, out_scores, out_classes = [], [], []\nfor idx_ in indices:\n    out_classes.append(idx_[1])\n    out_scores.append(scores[tuple(idx_)])\n    idx_1 = (idx_[0], idx_[2])\n    out_boxes.append(boxes[idx_1])\n```\n\n----------------------------------------\n\nTITLE: Postprocessing: Draw Detections in Python\nDESCRIPTION: This code snippet implements post-processing for the SSD-MobilenetV1 model. It defines a function `draw_detection` to draw bounding boxes and labels on an image based on the model's output. It then iterates through the detected objects, calls `draw_detection` for each, and displays the resulting image with bounding boxes using matplotlib.pyplot. It requires PIL (Pillow), NumPy, and matplotlib.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# draw boundary boxes and label for each detection\ndef draw_detection(draw, d, c):\n    width, height = draw.im.size\n    # the box is relative to the image size so we multiply with height and width to get pixels\n    top = d[0] * height\n    left = d[1] * width\n    bottom = d[2] * height\n    right = d[3] * width\n    top = max(0, np.floor(top + 0.5).astype('int32'))\n    left = max(0, np.floor(left + 0.5).astype('int32'))\n    bottom = min(height, np.floor(bottom + 0.5).astype('int32'))\n    right = min(width, np.floor(right + 0.5).astype('int32'))\n    label = coco_classes[c]\n    label_size = draw.textsize(label)\n    if top - label_size[1] >= 0:\n        text_origin = tuple(np.array([left, top - label_size[1]]))\n    else:\n        text_origin = tuple(np.array([left, top + 1]))\n    color = ImageColor.getrgb(\"red\")\n    thickness = 0\n    draw.rectangle([left + thickness, top + thickness, right - thickness, bottom - thickness],\n    outline=color)\n    draw.text(text_origin, label, fill=color), font=font\n\n# loop over the results - each returned tensor is a batch\nbatch_size = num_detections.shape[0]\ndraw = ImageDraw.Draw(img)\nfor batch in range(0, batch_size):\n    for detection in range(0, int(num_detections[batch])):\n        c = detection_classes[batch][detection]\n        d = detection_boxes[batch][detection]\n        draw_detection(draw, d, c)\n\n# show image file with object detection boundary boxes and labels\nplt.figure(figsize=(80, 40))\nplt.axis('off')\nplt.imshow(img)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Postprocessing ONNX Model Output in Python\nDESCRIPTION: This snippet demonstrates how to post-process the output from the ONNX model. It finds the index of the maximum value in the output array using `np.argmax`. Based on this index, it prints a sentiment prediction: \"negative\" if the index is 0, and \"positive\" if the index is 1.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/dependencies/roberta-sequence-classification-inference.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npred = np.argmax(ort_out)\nif(pred == 0):\n    print(\"Prediction: negative\")\nelif(pred == 1):\n    print(\"Prediction: positive\")\n```\n\n----------------------------------------\n\nTITLE: Drawing Bounding Boxes on Image (Python)\nDESCRIPTION: Draws bounding boxes, class labels, and confidence scores on an image. The function takes an image, a list of bounding boxes, a dictionary of class names, and an optional flag to show labels. It generates a set of unique colors for each class and draws rectangles around the detected objects along with their class labels and scores.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef draw_bbox(image, bboxes, classes=read_class_names(\"coco.names\"), show_label=True):\n    \"\"\"\n    bboxes: [x_min, y_min, x_max, y_max, probability, cls_id] format coordinates.\n    \"\"\"\n\n    num_classes = len(classes)\n    image_h, image_w, _ = image.shape\n    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n\n    random.seed(0)\n    random.shuffle(colors)\n    random.seed(None)\n\n    for i, bbox in enumerate(bboxes):\n        coor = np.array(bbox[:4], dtype=np.int32)\n        fontScale = 0.5\n        score = bbox[4]\n        class_ind = int(bbox[5])\n        bbox_color = colors[class_ind]\n        bbox_thick = int(0.6 * (image_h + image_w) / 600)\n        c1, c2 = (coor[0], coor[1]), (coor[2], coor[3])\n        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)\n\n        if show_label:\n            bbox_mess = '%s: %.2f' % (classes[class_ind], score)\n            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, thickness=bbox_thick//2)[0]\n            cv2.rectangle(image, c1, (c1[0] + t_size[0], c1[1] - t_size[1] - 3), bbox_color, -1)\n\n            cv2.putText(image, bbox_mess, (c1[0], c1[1]-2), cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale, (0, 0, 0), bbox_thick//2, lineType=cv2.LINE_AA)\n\n    return image\n```\n\n----------------------------------------\n\nTITLE: Predict Image Class\nDESCRIPTION: This function takes the path to an image, preprocesses it, performs inference using a pre-loaded MXNet module (`mod`), and prints the top 5 predicted classes along with their probabilities. It relies on the `get_image` and `preprocess` functions, as well as the `labels` list.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef predict(path):\n    img = get_image(path, show=True)\n    img = preprocess(img)\n    mod.forward(Batch([img]))\n    # Take softmax to generate probabilities\n    scores = mx.ndarray.softmax(mod.get_outputs()[0]).asnumpy()\n    # print the top-5 inferences class\n    scores = np.squeeze(scores)\n    a = np.argsort(scores)[::-1]\n    for i in a[0:5]:\n        print('class=%s ; probability=%f' %(labels[i],scores[i]))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for Faster R-CNN (Python)\nDESCRIPTION: This Python code snippet demonstrates how to preprocess an image for inference with the Faster R-CNN model. It resizes the image, converts it to BGR format, transposes the dimensions, normalizes the color channels using a specified mean, and pads the image to ensure its dimensions are divisible by 32. The function requires the PIL library for image handling and NumPy for array manipulation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/faster-rcnn/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\n\ndef preprocess(image):\n    # Resize\n    ratio = 800.0 / min(image.size[0], image.size[1])\n    image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])), Image.BILINEAR)\n\n    # Convert to BGR\n    image = np.array(image)[:, :, [2, 1, 0]].astype('float32')\n\n    # HWC -> CHW\n    image = np.transpose(image, [2, 0, 1])\n\n    # Normalize\n    mean_vec = np.array([102.9801, 115.9465, 122.7717])\n    for i in range(image.shape[0]):\n        image[i, :, :] = image[i, :, :] - mean_vec[i]\n\n    # Pad to be divisible of 32\n    import math\n    padded_h = int(math.ceil(image.shape[1] / 32) * 32)\n    padded_w = int(math.ceil(image.shape[2] / 32) * 32)\n\n    padded_image = np.zeros((3, padded_h, padded_w), dtype=np.float32)\n    padded_image[:, :image.shape[1], :image.shape[2]] = image\n    image = padded_image\n\n    return image\n\nimg = Image.open('dependencies/demo.jpg')\nimg_data = preprocess(img)\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies for ONNX Model Validation - Python\nDESCRIPTION: This code snippet imports the necessary libraries for ONNX model validation using MXNet, including matplotlib for plotting, mxnet for deep learning framework, numpy for numerical operations, gluon for high-level API, gluoncv for computer vision tasks, collections for data structures, multiprocessing for parallel processing, and import_model from mxnet.contrib.onnx.onnx2mx for importing ONNX models into MXNet.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, nd\nfrom mxnet.gluon.data.vision import transforms\nfrom gluoncv.data import imagenet\nfrom collections import namedtuple\nimport multiprocessing\nfrom mxnet.contrib.onnx.onnx2mx.import_model import import_model\n```\n\n----------------------------------------\n\nTITLE: Quantizing the ONNX Model with Intel Neural Compressor\nDESCRIPTION: This snippet shows how to use the Intel Neural Compressor to quantize the ONNX model. It involves running a bash script (`run_tuning.sh`) with several parameters to specify the input model, configuration file, data paths, and output model path. This quantization process optimizes the model for efficient inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model  \\# model path as *.onnx\n                   --config=DUC.yaml \\\n                   --data_path=/path/to/leftImg8bit/val \\\n                   --label_path=/path/to/gtFine/val \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for Style Transfer - Python\nDESCRIPTION: This code snippet demonstrates how to preprocess an image for use with a neural style transfer ONNX model. It loads an image using PIL, resizes it, converts it to a NumPy array, transposes the dimensions, and expands the dimensions to match the expected input shape of the model. The image is loaded in the range of [0, 255].\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/style_transfer/fast_neural_style/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom PIL import Image\nimport numpy as np\n\n# loading input and resize if needed\nimage = Image.open(\"PATH TO IMAGE\")\nsize_reduction_factor = 1\nimage = image.resize((int(image.size[0] / size_reduction_factor), int(image.size[1] / size_reduction_factor)), Image.ANTIALIAS)\n\n# Preprocess image\nx = np.array(image).astype('float32')\nx = np.transpose(x, [2, 0, 1])\nx = np.expand_dims(x, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing BERT Input with ONNX Runtime\nDESCRIPTION: This snippet preprocesses the input data for the BERT model using the `tokenization` module and the `run_onnx_squad` script.  It reads the input JSON file, tokenizes the text, and converts it into numerical features suitable for the BERT model. It relies on external modules like `numpy`, `onnxruntime`, `tokenization` and a custom `run_onnx_squad` script.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/BERT-Squad.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport onnxruntime as ort\nimport tokenization\nimport os\nfrom run_onnx_squad import *\nimport json\n\ninput_file = 'inputs.json'\nwith open(input_file) as json_file:  \n    test_data = json.load(json_file)\n    print(json.dumps(test_data, indent=2))\n  \n# preprocess input\npredict_file = 'inputs.json'\n\n# Use read_squad_examples method from run_onnx_squad to read the input file\neval_examples = read_squad_examples(input_file=predict_file)\n\nmax_seq_length = 256\ndoc_stride = 128\nmax_query_length = 64\nbatch_size = 1\nn_best_size = 20\nmax_answer_length = 30\n\n\nvocab_file = os.path.join('uncased_L-12_H-768_A-12', 'vocab.txt')\ntokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n\nmy_list = []\n\n\n# Use convert_examples_to_features method from run_onnx_squad to get parameters from the input \ninput_ids, input_mask, segment_ids, extra_data = convert_examples_to_features(eval_examples, tokenizer, \n                                                                              max_seq_length, doc_stride, max_query_length)\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for FCN Inference in Python\nDESCRIPTION: This snippet preprocesses an input image using the `torchvision` library. It defines a transformation pipeline to convert the image to a tensor, normalize its pixel values using pre-defined mean and standard deviation, and prepares it for input to the FCN model. The `transforms.Compose` function chains together the `transforms.ToTensor` and `transforms.Normalize` operations.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms, models\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Text for Roberta Model in Python\nDESCRIPTION: This snippet preprocesses the input text using RobertaTokenizer from the `transformers` library and converts it into input IDs suitable for the Roberta model. It involves tokenizing the text and creating a tensor from the encoded input, then reshaping it to match the expected input format of the model. The output is a batched tensor ready for the model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/dependencies/roberta-sequence-classification-inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nfrom simpletransformers.model import TransformerModel\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\ntext = \"This film is so good\"\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\ninput_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n```\n\n----------------------------------------\n\nTITLE: Quantizing VGG16 Model\nDESCRIPTION: This script quantizes the VGG16 model using Intel Neural Compressor. It takes the input model path, configuration file, and output model path as arguments. Requires a configuration file (vgg16.yaml) and the Intel Neural Compressor tool.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=vgg16.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Preprocess Image\nDESCRIPTION: This function preprocesses an input image. It resizes the image to 256x256, takes a center crop of 224x224, converts it to a tensor, and normalizes it using ImageNet normalization parameters. Returns the preprocessed image tensor.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(img):   \n    transform_fn = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    img = transform_fn(img)\n    img = img.expand_dims(axis=0)\n    return img\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for Model Inference in Python\nDESCRIPTION: This code provides utility functions for running inference using PyTorch and ONNX Runtime (ORT). The `torch_inference` function executes the PyTorch model and returns the output. The `ort_inference` function loads an ONNX model, runs inference, and compares the results with the PyTorch output if available.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef torch_inference(model, input):\n    print(\"====== Torch Inference ======\")\n    output=model(input)\n    return output\n\n\ndef ort_inference(file, inputs, outputs=None):\n    print(\"====== ORT Inference ======\")\n    inputs_flatten = full_flatten(inputs)\n    outputs_flatten = full_flatten(outputs)\n\n    # Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n    # other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n    # based on the build flags) when instantiating InferenceSession.\n    # For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n    # onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\n    sess = rt.InferenceSession(file)\n    ort_inputs = dict((sess.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(inputs_flatten))\n    res = sess.run(None, ort_inputs)\n\n    if outputs is not None:\n        print(\"== Checking model output ==\")\n        [np.testing.assert_allclose(to_numpy(output), res[i], rtol=1e-03, atol=2e-04) for i, output in enumerate(outputs_flatten)]\n    \n    print(\"== Done ==\")\n    return res\n```\n\n----------------------------------------\n\nTITLE: Define Evaluation Metrics - Python\nDESCRIPTION: This code snippet defines the evaluation metrics for assessing the performance of the ONNX model. It initializes an `Accuracy` metric for top-1 accuracy and a `TopKAccuracy` metric for top-5 accuracy using `mx.metric` module. These metrics will be used to evaluate the model's predictions against the ground truth labels.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Define evaluation metrics\nacc_top1 = mx.metric.Accuracy()\nacc_top5 = mx.metric.TopKAccuracy(5)\n```\n\n----------------------------------------\n\nTITLE: Define ArcFace Model Python\nDESCRIPTION: This code defines the `get_symbol` function, which loads the ResNet100 model definition from `fresnet.py` and defines the ArcFace loss function. It takes various parameters, including model hyperparameters, training settings, and paths to dataset and pretrained models. It then configures the ArcFace loss by normalizing the embedding and weights, calculating the cosine and sine of the angle between the embedding and the weight vectors, and applying the margin penalty.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_symbol(arg_params, aux_params, image_channel, image_h, image_w, num_layers, num_classes, data_dir,prefix,pretrained,ckpt,verbose,max_steps,end_epoch,lr,lr_steps,wd,fc7_wd_mult,\n              mom,emb_size,per_batch_size,margin_m,margin_s,target,beta,beta_min,beta_freeze,gamma,power,scale):\n    data_shape = (image_channel,image_h,image_w)\n    image_shape = \",\".join([str(x) for x in data_shape])\n    margin_symbols = []\n    print('init resnet', num_layers)\n    \n    # Load Resnet100 model - model definition is present in fresnet.py\n    embedding = fresnet.get_symbol(emb_size, num_layers, \n        version_se=0, version_input=1, \n        version_output='E', version_unit=3,\n        version_act='prelu')\n    all_label = mx.symbol.Variable('softmax_label')\n    gt_label = all_label\n    extra_loss = None\n    _weight = mx.symbol.Variable(\"fc7_weight\", shape=(num_classes, emb_size), lr_mult=1.0, wd_mult=fc7_wd_mult)\n    \n    # Define ArcFace loss\n    s = margin_s\n    m = margin_m\n    assert s>0.0\n    assert m>=0.0\n    assert m<(math.pi/2)\n    _weight = mx.symbol.L2Normalization(_weight, mode='instance')\n    nembedding = mx.symbol.L2Normalization(embedding, mode='instance', name='fc1n')*s\n    fc7 = mx.sym.FullyConnected(data=nembedding, weight = _weight, no_bias = True, num_hidden=num_classes, name='fc7')\n    zy = mx.sym.pick(fc7, gt_label, axis=1)\n    cos_t = zy/s\n    cos_m = math.cos(m)\n    sin_m = math.sin(m)\n    mm = math.sin(math.pi-m)*m\n    threshold = math.cos(math.pi-m)\n    cond_v = cos_t - threshold\n    cond = mx.symbol.Activation(data=cond_v, act_type='relu')\n    body = cos_t*cos_t\n    body = 1.0-body\n    sin_t = mx.sym.sqrt(body)\n    new_zy = cos_t*cos_m\n    b = sin_t*sin_m\n    new_zy = new_zy - b\n    new_zy = new_zy*s\n    zy_keep = zy - s*mm\n    new_zy = mx.sym.where(cond, new_zy, zy_keep)\n    diff = new_zy - zy\n    diff = mx.sym.expand_dims(diff, 1)\n    gt_one_hot = mx.sym.one_hot(gt_label, depth = num_classes, on_value = 1.0, off_value = 0.0)\n    body = mx.sym.broadcast_mul(gt_one_hot, diff)\n    fc7 = fc7+body\n    out_list = [mx.symbol.BlockGrad(embedding)]\n    softmax = mx.symbol.SoftmaxOutput(data=fc7, label = gt_label, name='softmax', normalization='valid')\n    out_list.append(softmax)\n    out = mx.symbol.Group(out_list)\n    return (out, arg_params, aux_params)\n```\n\n----------------------------------------\n\nTITLE: Inference with ONNX Runtime in Python\nDESCRIPTION: This code snippet demonstrates how to load an SSD-MobilenetV1 ONNX model and perform inference using the ONNX Runtime (ort). It assumes that the onnxruntime package is installed and that the model path and image data are available. The code initializes an InferenceSession, runs the model with the input image data, and then prints the number of detections and classes.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport onnxruntime as rt\n\n# Load model and run inference\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# rt.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsess = rt.InferenceSession(os.path.join(WORK, MODEL + \".onnx\"))\nresult = sess.run(outputs, {\"image_tensor:0\": img_data})\nnum_detections, detection_boxes, detection_scores, detection_classes = result\n\n# print number of detections\nprint(num_detections)\nprint(detection_classes)\n\n# produce outputs in this order\noutputs = [\"num_detections:0\", \"detection_boxes:0\", \"detection_scores:0\", \"detection_classes:0\"]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for Emotion FERPlus Model (Python)\nDESCRIPTION: This Python function preprocesses an image for use with the Emotion FERPlus model. It opens the image, resizes it to 64x64 pixels, converts it to a NumPy array, and reshapes it to the expected input shape (1x1x64x64). It requires the NumPy and PIL (Pillow) libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/emotion_ferplus/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\n\ndef preprocess(image_path):\n  input_shape = (1, 1, 64, 64)\n  img = Image.open(image_path)\n  img = img.resize((64, 64), Image.ANTIALIAS)\n  img_data = np.array(img)\n  img_data = np.resize(img_data, input_shape)\n  return img_data\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for Tiny YOLOv3 with Python\nDESCRIPTION: This code snippet demonstrates how to preprocess an image for the Tiny YOLOv3 model using Python, PIL, and NumPy. It includes resizing the image with padding to maintain aspect ratio (letterboxing), converting it to a NumPy array, normalizing pixel values, transposing dimensions, and expanding the dimensions for batch processing. It depends on the PIL (Pillow) and NumPy libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/tiny-yolov3/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\n\n# this function is from yolo3.utils.letterbox_image\ndef letterbox_image(image, size):\n    '''resize image with unchanged aspect ratio using padding'''\n    iw, ih = image.size\n    w, h = size\n    scale = min(w/iw, h/ih)\n    nw = int(iw*scale)\n    nh = int(ih*scale)\n\n    image = image.resize((nw,nh), Image.BICUBIC)\n    new_image = Image.new('RGB', size, (128,128,128))\n    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n    return new_image\n\ndef preprocess(img):\n    model_image_size = (416, 416)\n    boxed_image = letterbox_image(img, tuple(reversed(model_image_size)))\n    image_data = np.array(boxed_image, dtype='float32')\n    image_data /= 255.\n    image_data = np.transpose(image_data, [2, 0, 1])\n    image_data = np.expand_dims(image_data, 0)\n    return image_data\n\nimage = Image.open(img_path)\n# input\nimage_data = preprocess(image)\nimage_size = np.array([image.size[1], image.size[0]], dtype=np.float32).reshape(1, 2)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for Super Resolution (Python)\nDESCRIPTION: This code snippet demonstrates how to preprocess an image for the Super Resolution model. It involves resizing the image to 224x224, converting it to YCbCr format, extracting the Y channel, and converting it to a NumPy array. Finally, the array is reshaped and normalized to be used as input to the model. It requires the PIL, resizeimage and NumPy libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom PIL import Image\nfrom resizeimage import resizeimage\nimport numpy as np\norig_img = Image.open('IMAGE_FILE_PATH')\nimg = resizeimage.resize_cover(orig_img, [224,224], validate=False)\nimg_ycbcr = img.convert('YCbCr')\nimg_y_0, img_cb, img_cr = img_ycbcr.split()\nimg_ndarray = np.asarray(img_y_0)\nimg_4 = np.expand_dims(np.expand_dims(img_ndarray, axis=0), axis=0)\nimg_5 = img_4.astype(np.float32) / 255.0\nimg_5\n```\n\n----------------------------------------\n\nTITLE: Generate and Print Prediction in Python\nDESCRIPTION: Sets the path to the input image ('kitten.jpg') and calls the `predict` function to generate and print the top prediction for that image. This uses the previously defined functions for image loading, preprocessing, and inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Enter path to the inference image below\nimg_path = 'kitten.jpg'\npredict(img_path)\n```\n\n----------------------------------------\n\nTITLE: Training Setup with MXNet Gluon\nDESCRIPTION: This snippet configures the training process for an MXNet Gluon model. It sets up logging, defines the number of classes, extrapolates batch sizes across available GPUs, defines the context for computation (CPU or GPU), sets the learning rate decay schedule, and defines the optimizer and its parameters. Crucially, it retrieves the Gluon model using the previously defined `models` dictionary.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Specify logging function\nlogging.basicConfig(level=logging.INFO)\n\n# Specify classes (1000 for ImageNet)\nclasses = 1000\n# Extrapolate batches to all devices\nbatch_size *= max(1, num_gpus)\n# Define context\ncontext = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n\nlr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')] + [np.inf]\n\nkwargs = { 'classes': classes}\n\n# Define optimizer (nag = Nestrov Accelerated Gradient)\noptimizer = 'nag'\noptimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum}\nkwargs['thumbnail'] = False\n\n# Retrieve gluon model\nnet = models[model_name](**kwargs)\n\n# Define accuracy measures - top1 error and top5 error\nacc_top1 = mx.metric.Accuracy()\nacc_top5 = mx.metric.TopKAccuracy(5)\ntrain_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n                                 'validation-top1-err', 'validation-top5-err'])\nmakedirs(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for FCN Model with Torchvision (Python)\nDESCRIPTION: This code snippet demonstrates how to preprocess an image for the FCN model using the torchvision library in Python. It involves loading the image, converting it to a tensor, and normalizing it using pre-defined mean and standard deviation values. The preprocessed image data is then detached and converted to a NumPy array.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom torchvision import transforms\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nimg = Image.open('dependencies/000000017968.jpg')\nimg_data = preprocess(img).detach().cpu().numpy()\n```\n\n----------------------------------------\n\nTITLE: Checking and Printing ONNX Model Graph in Python\nDESCRIPTION: This snippet checks the loaded ONNX model for structural correctness and prints its graph in a human-readable format. It uses `onnx.checker.check_model` to validate the model and `onnx.helper.printable_graph` to generate a readable representation of the model's graph.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/onnx-model-validation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nonnx.checker.check_model(model)  # Check that the IR is well formed\nprint(onnx.helper.printable_graph(model.graph))  # Print a human readable representation of the graph\n```\n\n----------------------------------------\n\nTITLE: Run Model on ONNX Runtime (Python)\nDESCRIPTION: This snippet loads an ONNX model using ONNX Runtime and runs inference on the preprocessed image. It creates an ONNX Runtime InferenceSession, prepares the input data, and executes the model. Replace \"FILE_PATH_TO_ONNX_FILE\" with the actual path to the ONNX file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/dependencies/Run_Super_Resolution_Model.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nort_session = onnxruntime.InferenceSession(\"FILE_PATH_TO_ONNX_FILE\")\nort_inputs = {ort_session.get_inputs()[0].name: img_5} \nort_outs = ort_session.run(None, ort_inputs)\nimg_out_y = ort_outs[0]\n```\n\n----------------------------------------\n\nTITLE: MobileNetV2 Definition - Python\nDESCRIPTION: This code defines the MobileNetV2 model architecture using MXNet Gluon. It includes helper classes and functions like RELU6, _add_conv, _add_conv_dw, and LinearBottleneck, which are used to build the inverted residual blocks. The MobileNetV2 class defines the overall model structure, including the feature extraction layers and the output layer. The `get_mobilenet_v2`, `mobilenet_v2_1_0`, and `mobilenet_v2_0_5` functions are constructors for different MobileNetV2 variants.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n##This block contains definition for Mobilenet v2\n\n# Helpers\nclass RELU6(nn.HybridBlock):\n    \"\"\"Relu6 used in MobileNetV2.\"\"\"\n\n    def __init__(self, **kwargs):\n        super(RELU6, self).__init__(**kwargs)\n\n    def hybrid_forward(self, F, x):\n        return F.clip(x, 0, 6, name=\"relu6\")\n\n\ndef _add_conv(out, channels=1, kernel=1, stride=1, pad=0,\n              num_group=1, active=True, relu6=False):\n    out.add(nn.Conv2D(channels, kernel, stride, pad, groups=num_group, use_bias=False))\n    out.add(nn.BatchNorm(scale=True))\n    if active:\n        out.add(RELU6() if relu6 else nn.Activation('relu'))\n\n\ndef _add_conv_dw(out, dw_channels, channels, stride, relu6=False):\n    _add_conv(out, channels=dw_channels, kernel=3, stride=stride,\n              pad=1, num_group=dw_channels, relu6=relu6)\n    _add_conv(out, channels=channels, relu6=relu6)\n\n\nclass LinearBottleneck(nn.HybridBlock):\n    r\"\"\"LinearBottleneck used in MobileNetV2 model from the\n    `\"Inverted Residuals and Linear Bottlenecks:\n      Mobile Networks for Classification, Detection and Segmentation\"\n    <https://arxiv.org/abs/1801.04381>`_ paper.\n    Parameters\n    ----------\n    in_channels : int\n        Number of input channels.\n    channels : int\n        Number of output channels.\n    t : int\n        Layer expansion ratio.\n    stride : int\n        stride\n    \"\"\"\n\n    def __init__(self, in_channels, channels, t, stride, **kwargs):\n        super(LinearBottleneck, self).__init__(**kwargs)\n        self.use_shortcut = stride == 1 and in_channels == channels\n        with self.name_scope():\n            self.out = nn.HybridSequential()\n\n            _add_conv(self.out, in_channels * t, relu6=True)\n            _add_conv(self.out, in_channels * t, kernel=3, stride=stride,\n                      pad=1, num_group=in_channels * t, relu6=True)\n            _add_conv(self.out, channels, active=False, relu6=True)\n\n    def hybrid_forward(self, F, x):\n        out = self.out(x)\n        if self.use_shortcut:\n            out = F.elemwise_add(out, x)\n        return out\n\n\n# Net\nclass MobileNetV2(nn.HybridBlock):\n    r\"\"\"MobileNetV2 model from the\n    `\"Inverted Residuals and Linear Bottlenecks:\n      Mobile Networks for Classification, Detection and Segmentation\"\n    <https://arxiv.org/abs/1801.04381>`_ paper.\n    Parameters\n    ----------\n    multiplier : float, default 1.0\n        The width multiplier for controling the model size. The actual number of channels\n        is equal to the original channel size multiplied by this multiplier.\n    classes : int, default 1000\n        Number of classes for the output layer.\n    \"\"\"\n\n    def __init__(self, multiplier=1.0, classes=1000, **kwargs):\n        super(MobileNetV2, self).__init__(**kwargs)\n        with self.name_scope():\n            self.features = nn.HybridSequential(prefix='features_')\n            with self.features.name_scope():\n                _add_conv(self.features, int(32 * multiplier), kernel=3,\n                          stride=2, pad=1, relu6=True)\n\n                in_channels_group = [int(x * multiplier) for x in [32] + [16] + [24] * 2\n                                     + [32] * 3 + [64] * 4 + [96] * 3 + [160] * 3]\n                channels_group = [int(x * multiplier) for x in [16] + [24] * 2 + [32] * 3\n                                  + [64] * 4 + [96] * 3 + [160] * 3 + [320]]\n                ts = [1] + [6] * 16\n                strides = [1, 2] * 2 + [1, 1, 2] + [1] * 6 + [2] + [1] * 3\n\n                for in_c, c, t, s in zip(in_channels_group, channels_group, ts, strides):\n                    self.features.add(LinearBottleneck(in_channels=in_c, channels=c,\n                                                       t=t, stride=s))\n\n                last_channels = int(1280 * multiplier) if multiplier > 1.0 else 1280\n                _add_conv(self.features, last_channels, relu6=True)\n\n                self.features.add(nn.GlobalAvgPool2D())\n\n            self.output = nn.HybridSequential(prefix='output_')\n            with self.output.name_scope():\n                self.output.add(\n                    nn.Conv2D(classes, 1, use_bias=False, prefix='pred_'),\n                    nn.Flatten()\n                )\n\n    def hybrid_forward(self, F, x):\n        x = self.features(x)\n        x = self.output(x)\n        return x\n\n\n# Constructor\ndef get_mobilenet_v2(multiplier, **kwargs):\n    r\"\"\"MobileNetV2 model from the\n    `\"Inverted Residuals and Linear Bottlenecks:\n      Mobile Networks for Classification, Detection and Segmentation\"\n    <https://arxiv.org/abs/1801.04381>`_ paper.\n    Parameters\n    ----------\n    multiplier : float\n        The width multiplier for controling the model size. Only multipliers that are no\n        less than 0.25 are supported. The actual number of channels is equal to the original\n        channel size multiplied by this multiplier.\n    \"\"\"\n    net = MobileNetV2(multiplier, **kwargs)\n    return net\n\ndef mobilenet_v2_1_0(**kwargs):\n    r\"\"\"MobileNetV2 model from the\n    `\"Inverted Residuals and Linear Bottlenecks:\n      Mobile Networks for Classification, Detection and Segmentation\"\n    <https://arxiv.org/abs/1801.04381>`_ paper.\n    \"\"\"\n    return get_mobilenet_v2(1.0, **kwargs)\n\ndef mobilenet_v2_0_5(**kwargs):\n    r\"\"\"MobileNetV2 model from the\n    `\"Inverted Residuals and Linear Bottlenecks:\n      Mobile Networks for Classification, Detection and Segmentation\"\n    <https://arxiv.org/abs/1801.04381>`_ paper.\n    \"\"\"\n    return get_mobilenet_v2(0.5, **kwargs)\nmodels = {  \n            'mobilenetv2_1.0': mobilenet_v2_1_0,\n            'mobilenetv2_0.5': mobilenet_v2_0_5\n         }\n```\n\n----------------------------------------\n\nTITLE: Compute Evaluations - Python\nDESCRIPTION: This snippet performs a forward pass over each batch of the validation data using the loaded ONNX model, updates the evaluation metric, and prints the progress. It iterates through the validation dataset using the configured data loader, feeds the data to the model, retrieves the outputs, and updates the IoU metric to track the model's performance.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# reset data loader\nval_dataloader.reset()\n# reset evaluation metric\nmetric.reset()\n# loop over batches\nfor nbatch, eval_batch in enumerate(val_dataloader):\n    # perform forward pass\n    mod.forward(eval_batch, is_train=False)\n    # get outputs\n    outputs=mod.get_outputs()\n    # update evaluation metric\n    metric.update(eval_batch.label,outputs)\n    # print progress\n    if nbatch%10==0:\n        print('{} / {} batches done'.format(nbatch,int(3500/batch_size)))\n```\n\n----------------------------------------\n\nTITLE: Define Hyperparameters - Python\nDESCRIPTION: This code snippet defines the hyperparameters for training the MobileNetV2 model, including the model name, data directory, batch size, number of GPUs, number of workers, number of epochs, learning rate, momentum, weight decay, learning rate decay, and save directories. These parameters control the training process and can be tuned for optimal performance.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# specify model - choose from (mobilenetv2_1.0, mobilenetv2_0.5)\nmodel_name = 'mobilenetv2_1.0' \n\n# path to training and validation images to use\ndata_dir = '/home/ubuntu/imagenet/img_dataset'\n\n# training batch size per device (CPU/GPU)\nbatch_size = 40\n\n# number of GPUs to use (automatically detect the number of GPUs)\nnum_gpus = len(mx.test_utils.list_gpus())\n\n# number of pre-processing workers (automatically detect the number of workers)\nnum_workers = multiprocessing.cpu_count()\n\n# number of training epochs \n#used as 480 for all of the models , used 1 over here to show demo for 1 epoch\nnum_epochs = 1\n\n# learning rate\nlr = 0.045\n\n# momentum value for optimizer\nmomentum = 0.9\n\n# weight decay rate\nwd = 0.00004\n\n# decay rate of learning rate\nlr_decay = 0.98\n\n# interval for periodic learning rate decays\nlr_decay_period = 1\n\n# epoches at which learning rate decays\nlr_decay_epoch = '30,60,90'\n\n# mode in which to train the model. options are symbolic, imperative, hybrid\nmode = 'hybrid'\n\n# Number of batches to wait before logging\nlog_interval = 50\n\n# frequency of model saving\nsave_frequency = 10\n\n# directory of saved models\nsave_dir = 'params'\n\n#directory of training logs\nlogging_dir = 'logs'\n\n# the path to save the history plot\nsave_plot_dir = '.'\n```\n\n----------------------------------------\n\nTITLE: Writing Input JSON File for BERT-Squad\nDESCRIPTION: This code snippet demonstrates how to create an inputs.json file containing context paragraphs and questions for the BERT-Squad model. The file is formatted as a JSON object with a specific structure that includes version, data, paragraphs (with context and questions), and title.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%writefile inputs.json\n{\n  \"version\": \"1.4\",\n  \"data\": [\n    {\n      \"paragraphs\": [\n        {\n          \"context\": \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\",\n          \"qas\": [\n            {\n              \"question\": \"where is the businesses choosing to go?\",\n              \"id\": \"1\"\n            },\n            {\n              \"question\": \"how may votes did the ballot measure need?\",\n              \"id\": \"2\"\n            },\n            {\n              \"question\": \"By what year many Silicon Valley businesses were choosing the Moscone Center?\",\n              \"id\": \"3\"\n            }\n          ]\n        }\n      ],\n      \"title\": \"Conference Center\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Define Image Normalization Transforms in Python\nDESCRIPTION: This code snippet defines the normalization parameters used for image preprocessing. It specifies the mean and standard deviation values for the RGB channels, which are later used to normalize the image data during training and validation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nnormalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\njitter_param = 0.4\nlighting_param = 0.1\n```\n\n----------------------------------------\n\nTITLE: Loading YOLOv4 Anchors from File (Python)\nDESCRIPTION: Loads anchor box configurations from a specified file. The function reads the anchor values as comma-separated floats, converts them to a NumPy array, and reshapes the array into a 3x3x2 structure representing the anchor boxes for each scale.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy import special\nimport colorsys\nimport random\nimport numpy as np\nimport cv2\n\n\ndef get_anchors(anchors_path, tiny=False):\n    '''loads the anchors from a file'''\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = np.array(anchors.split(','), dtype=np.float32)\n    return anchors.reshape(3, 3, 2)\n```\n\n----------------------------------------\n\nTITLE: YOLOv3 Image Preprocessing\nDESCRIPTION: This Python code snippet demonstrates how to preprocess an image for use with the YOLOv3 model. It includes resizing the image with padding to maintain aspect ratio, converting it to a numpy array, normalizing pixel values to the range [0, 1], transposing the dimensions, and expanding the dimensions to create a batch.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov3/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\n\n# this function is from yolo3.utils.letterbox_image\ndef letterbox_image(image, size):\n    '''resize image with unchanged aspect ratio using padding'''\n    iw, ih = image.size\n    w, h = size\n    scale = min(w/iw, h/ih)\n    nw = int(iw*scale)\n    nh = int(ih*scale)\n\n    image = image.resize((nw,nh), Image.BICUBIC)\n    new_image = Image.new('RGB', size, (128,128,128))\n    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n    return new_image\n\ndef preprocess(img):\n    model_image_size = (416, 416)\n    boxed_image = letterbox_image(img, tuple(reversed(model_image_size)))\n    image_data = np.array(boxed_image, dtype='float32')\n    image_data /= 255.\n    image_data = np.transpose(image_data, [2, 0, 1])\n    image_data = np.expand_dims(image_data, 0)\n    return image_data\n\nimage = Image.open(img_path)\n# input\nimage_data = preprocess(image)\nimage_size = np.array([image.size[1], image.size[0]], dtype=np.int32).reshape(1, 2)\n```\n\n----------------------------------------\n\nTITLE: Define Train Function - MXNet\nDESCRIPTION: This code defines the main training loop for the model. It initializes the network, prepares the training and validation data loaders, defines the optimizer and loss function, and iterates through epochs and batches. It also includes learning rate decay, periodic model saving, and logging of training progress and validation errors.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Train function\ndef train(epochs, ctx):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    # Initialize network - Use method in MSRA paper <https://arxiv.org/abs/1502.01852>\n    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n    # Prepare train and validation batches\n    transform_train = preprocess_train_data(normalize, jitter_param, lighting_param)\n    transform_test = preprocess_test_data(normalize)\n    train_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=True).transform_first(transform_train),\n        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n    val_data = gluon.data.DataLoader(\n        imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    # Define trainer\n    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n    # Define loss\n    L = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    lr_decay_count = 0\n\n    best_val_score = 1\n    # Main training loop - loop over epochs\n    for epoch in range(epochs):\n        tic = time.time()\n        # Reset accuracy metrics\n        acc_top1.reset()\n        acc_top5.reset()\n        btic = time.time()\n        train_loss = 0\n        num_batch = len(train_data)\n        \n        # Check and perform learning rate decay\n        if lr_decay_period and epoch and epoch % lr_decay_period == 0:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n        elif lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n            lr_decay_count += 1\n        # Loop over batches in an epoch\n        for i, batch in enumerate(train_data):\n            # Load train batch\n            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n            label_smooth = label\n            # Perform forward pass\n            with ag.record():\n                outputs = [net(X) for X in data]\n                loss = [L(yhat, y) for yhat, y in zip(outputs, label_smooth)]\n            # Perform backward pass\n            ag.backward(loss)\n            # Perform updates\n            trainer.step(batch_size)\n            # Initialize last conv layer weights with normal distribution after first forward pass\n            if i==0 and epoch==0:\n                new_classifier_w = mx.nd.random_normal(shape=(1000, 512, 1, 1), scale=0.01)\n                final_conv_layer_params = net.output[0].params\n                final_conv_layer_params.get('weight').set_data(new_classifier_w)\n            # Update accuracy metrics\n            acc_top1.update(label, outputs)\n            acc_top5.update(label, outputs)\n            # Update loss\n            train_loss += sum([l.sum().asscalar() for l in loss])\n            # Log training progress (after each `log_interval` batches)\n            if log_interval and not (i+1)%log_interval:\n                _, top1 = acc_top1.get()\n                _, top5 = acc_top5.get()\n                err_top1, err_top5 = (1-top1, 1-top5)\n                logging.info('Epoch[%d] Batch [%d]\tSpeed: %f samples/sec\ttop1-err=%f\ttop5-err=%f'%(\n                             epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n                btic = time.time()\n        # Retrieve training errors and loss\n        _, top1 = acc_top1.get()\n        _, top5 = acc_top5.get()\n        err_top1, err_top5 = (1-top1, 1-top5)\n        train_loss /= num_batch * batch_size\n        \n        # Compute validation errors\n        err_top1_val, err_top5_val = test(ctx, val_data)\n        # Update training history\n        train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n        # Update plot\n        train_history.plot(['training-top1-err', 'validation-top1-err','training-top5-err', 'validation-top5-err'],\n                           save_path='%s/%s_top_error.png'%(save_plot_dir, model_name))\n        \n        # Log training progress (after each epoch)\n        logging.info('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n        logging.info('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n        \n        # Save a snapshot of the best model - use net.export to get MXNet symbols and params\n        if err_top1_val < best_val_score and epoch > 50:\n            best_val_score = err_top1_val\n            net.export('%s/%.4f-imagenet-%s-best'%(save_dir, best_val_score, model_name), epoch)\n        # Save a snapshot of the model after each 'save_frequency' epochs\n        if save_frequency and save_dir and (epoch + 1) % save_frequency == 0:\n            net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epoch)\n    # Save a snapshot of the model at the end of training\n    if save_frequency and save_dir:\n        net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epochs-1)\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Output for Tiny YOLOv3 with Python\nDESCRIPTION: This code snippet demonstrates post-processing steps for the output of the Tiny YOLOv3 model using Python. It extracts bounding boxes, scores, and classes from the model's output indices, scores, and boxes. It iterates through the selected indices, retrieves corresponding scores and bounding box coordinates, and appends them to respective lists.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/tiny-yolov3/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nout_boxes, out_scores, out_classes = [], [], []\nfor idx_ in indices[0]:\n    out_classes.append(idx_[1])\n    out_scores.append(scores[tuple(idx_)])\n    idx_1 = (idx_[0], idx_[2])\n    out_boxes.append(boxes[idx_1])\n```\n\n----------------------------------------\n\nTITLE: Evaluate Model Performance in Python\nDESCRIPTION: This section includes functions for evaluating the performance of the ArcFace model using K-fold cross-validation.  `LFold` handles data splitting, `calculate_roc()` computes the ROC curve and accuracy, `calculate_accuracy()` computes the accuracy, and `evaluate()` orchestrates the evaluation process. These functions compute accuracy by comparing the distance between embedding vectors of image pairs against a threshold.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_validation.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass LFold:\n    def __init__(self, n_splits = 2, shuffle = False):\n        self.n_splits = n_splits\n        if self.n_splits>1:\n            self.k_fold = KFold(n_splits = n_splits, shuffle = shuffle)\n\n    def split(self, indices):\n        if self.n_splits>1:\n            return self.k_fold.split(indices)\n        else:\n            return [(indices, indices)]\n\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = LFold(n_splits=nrof_folds, shuffle=False)\n    \n    tprs = np.zeros((nrof_folds,nrof_thresholds))\n    fprs = np.zeros((nrof_folds,nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    indices = np.arange(nrof_pairs)\n    \n    diff = np.subtract(embeddings1, embeddings2)\n    dist = np.sum(np.square(diff),1)\n    \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):        \n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n          \n    tpr = np.mean(tprs,0)\n    fpr = np.mean(fprs,0)\n    return tpr, fpr, accuracy\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n  \n    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n    acc = float(tp+tn)/dist.size\n    return tpr, fpr, acc\n\ndef evaluate(embeddings, actual_issame, nrof_folds=10):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2,\n        np.asarray(actual_issame), nrof_folds=nrof_folds)\n    return accuracy\n```\n\n----------------------------------------\n\nTITLE: Specifying Model and Training Hyperparameters (MXNet)\nDESCRIPTION: This code snippet defines key hyperparameters for training ResNet models, including model selection, data directory, batch size, number of GPUs, number of workers, number of epochs, learning rate, momentum, weight decay, learning rate decay, learning rate decay period, learning rate decay epoch, training mode, logging interval, save frequency, and directory paths for saving models, logs, and plots.  These parameters control the training process and influence the performance of the resulting model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# specify model - choose from (resnet18_v1,resnet18_v2,resnet34_v1,resnet34_v2,resnet50_v1,resnet50_v2,\n#resnet101_v1,resnet101_v2,resnet152_v1,resnet152_v2)\nmodel_name = 'resnet18_v1' \n\n# path to training and validation images to use\ndata_dir = '/home/ubuntu/imagenet/img_dataset'\n\n# training batch size per device (CPU/GPU)\n# Used batch size = 64 for resnet18_v1,resnet18_v2,resnet34_v1,resnet34_v2,resnet50_v1,resnet50_v2,resnet101_v1,\n#resnet101_v2\n#Used batch size=32 for resnet152_v1,resnet152_v2\nbatch_size = 64\n\n# number of GPUs to use (automatically detect the number of GPUs)\nnum_gpus = len(mx.test_utils.list_gpus())\n\n# number of pre-processing workers (automatically detect the number of workers)\nnum_workers = multiprocessing.cpu_count()\n\n# number of training epochs \n#used as 150 for all of the models , used 1 over here to show demo for 1 epoch\nnum_epochs = 1\n\n# learning rate\nlr = 0.01\n\n# momentum value for optimizer\nmomentum = 0.9\n\n# weight decay rate\nwd = 0.0002\n\n# decay rate of learning rate\nlr_decay = 0.1\n\n# interval for periodic learning rate decays\nlr_decay_period = 0\n\n# epoches at which learning rate decays\nlr_decay_epoch = '30,60,90'\n\n# mode in which to train the model. options are symbolic, imperative, hybrid\nmode = 'hybrid'\n\n# Number of batches to wait before logging\nlog_interval = 50\n\n# frequency of model saving\nsave_frequency = 10\n\n# directory of saved models\nsave_dir = 'params'\n\n#directory of training logs\nlogging_dir = 'logs'\n\n# the path to save the history plot\nsave_plot_dir = '.'\n\n```\n\n----------------------------------------\n\nTITLE: Define Validation Data Preprocessing in Python\nDESCRIPTION: This function defines the preprocessing steps for the validation data. It includes resizing the images, applying a center crop, converting the data to a tensor, and normalizing the tensor using the `normalize` transform. This ensures consistent input for the validation process.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_test_data(normalize):\n    transform_test = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_test\n```\n\n----------------------------------------\n\nTITLE: Generate Feature Embedding with MXNet\nDESCRIPTION: This function `get_feature` performs a forward pass on the aligned face image using the loaded MXNet model to generate the feature embedding. It takes the MXNet model and the aligned image as input, expands the dimensions of the aligned image, creates a data batch, performs the forward pass, and normalizes the resulting embedding vector.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_feature(model,aligned):\n    input_blob = np.expand_dims(aligned, axis=0)\n    data = mx.nd.array(input_blob)\n    db = mx.io.DataBatch(data=(data,))\n    model.forward(db, is_train=False)\n    embedding = model.get_outputs()[0].asnumpy()\n    embedding = sklearn.preprocessing.normalize(embedding).flatten()\n    return embedding\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Training Data with Transforms\nDESCRIPTION: This function preprocesses training data by applying a series of transformations using `torchvision.transforms`. It includes random resizing, cropping, horizontal flipping, color jittering, lighting adjustments, and conversion to a tensor, followed by normalization. The function takes normalization, jitter, and lighting parameters as inputs.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train_data(normalize, jitter_param, lighting_param):\n    transform_train = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomFlipLeftRight(),\n        transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                     saturation=jitter_param),\n        transforms.RandomLighting(lighting_param),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_train\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for Model Saving in Python\nDESCRIPTION: This code defines several utility functions to flatten nested lists, convert PyTorch tensors to NumPy arrays, save tensors as protobuf files, and save the PyTorch model in ONNX format along with test data. The `save_model` function handles ONNX export with dynamic axes and saving input/output tensors for testing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef flatten(inputs):\n    return [[flatten(i) for i in inputs] if isinstance(inputs, (list, tuple)) else inputs]\n\n\ndef update_flatten_list(inputs, res_list):\n    for i in inputs:\n        res_list.append(i) if not isinstance(i, (list, tuple)) else update_flatten_list(i, res_list)\n    return res_list\n\ndef full_flatten(inputs):\n    inputs_flatten = flatten(inputs)\n    return update_flatten_list(inputs_flatten, [])\n\n\ndef to_numpy(x):\n    if type(x) is not np.ndarray:\n        x = x.detach().cpu().numpy() if x.requires_grad else x.cpu().numpy()\n    return x\n\n\ndef save_tensor_proto(file_path, name, data):\n    tp = numpy_helper.from_array(data)\n    tp.name = name\n\n    with open(file_path, 'wb') as f:\n        f.write(tp.SerializeToString())\n\n\ndef save_data(test_data_dir, prefix, names, data_list):\n    if isinstance(data_list, torch.autograd.Variable) or isinstance(data_list, torch.Tensor):\n        data_list = [data_list]\n    for i, d in enumerate(data_list):\n        d = d.data.cpu().numpy()\n        save_tensor_proto(os.path.join(test_data_dir, '{0}_{1}.pb'.format(prefix, i)), names[i], d)\n\n\ndef save_model(name, model, data_dir, inputs, outputs, input_names=None, output_names=None, **kwargs):\n    if hasattr(model, 'train'):\n        model.train(False)\n    output_dir = './'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    output_dir = os.path.join(output_dir, 'test_' + name)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    inputs_flatten = full_flatten(inputs)\n    outputs_flatten = full_flatten(outputs)\n    if input_names is None:\n        input_names = []\n        for i, _ in enumerate(inputs_flatten):\n            input_names.append('input' + str(i+1))\n    else:\n        np.testing.assert_equal(len(input_names), len(inputs_flatten),\n                                \"Number of input names provided is not equal to the number of inputs.\")\n\n    if output_names is None:\n        output_names = []\n        for i, _ in enumerate(outputs_flatten):\n            output_names.append('output' + str(i+1))\n    else:\n        np.testing.assert_equal(len(output_names), len(outputs_flatten),\n                                \"Number of output names provided is not equal to the number of output.\")\n\n    model_path = os.path.join(output_dir, 'model.onnx')\n    torch.onnx.export(model, inputs, model_path, verbose=True, input_names=input_names,\n                      output_names=output_names, example_outputs=outputs, **kwargs)\n\n    test_data_dir = os.path.join(output_dir, data_dir)\n    if not os.path.exists(test_data_dir):\n        os.makedirs(test_data_dir)\n\n    save_data(test_data_dir, \"input\", input_names, inputs_flatten)\n    save_data(test_data_dir, \"output\", output_names, outputs_flatten)\n\n    return model_path, test_data_dir\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Image from Style Transfer - Python\nDESCRIPTION: This code snippet demonstrates how to postprocess the output of a neural style transfer ONNX model to obtain a viewable image. It clips the result to the range [0, 255], transposes the dimensions to (height, width, channels), converts the data type to uint8, and creates a PIL image from the resulting NumPy array.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/style_transfer/fast_neural_style/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nresult = np.clip(result, 0, 255)\nresult = result.transpose(1,2,0).astype(\"uint8\")\nimg = Image.fromarray(result)\n```\n\n----------------------------------------\n\nTITLE: Saving Image Data as TensorProto\nDESCRIPTION: This snippet saves the preprocessed image data in TensorProto format, which is often used for testing ONNX models. It creates a directory if it doesn't exist, converts the NumPy array (image_data) to a TensorProto object, and then serializes and saves the TensorProto to a file named 'input1.pb' within the specified directory.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_dir = \"./test_data_set\"\ntest_data_dir = os.path.join(data_dir)\nif not os.path.exists(test_data_dir):\n    os.makedirs(test_data_dir)\n        \n# Convert the NumPy array to a TensorProto\ntensor = numpy_helper.from_array(image_data)\n# print('TensorProto:\\n{}'.format(tensor))\n\n# Save the TensorProto\nwith open(os.path.join(test_data_dir, \"input1.pb\"), 'wb') as f:\n    f.write(tensor.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Define Validation/Test Function in Python\nDESCRIPTION: This function evaluates the model's performance on the validation dataset. It iterates through the validation data, performs a forward pass using the provided context (`ctx`), and updates the top-1 and top-5 accuracy metrics. The function returns the top-1 and top-5 error rates.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef test(ctx, val_data):\n    # Reset accuracy metrics\n    acc_top1.reset()\n    acc_top5.reset()\n    for i, batch in enumerate(val_data):\n        # Load validation batch\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n        # Perform forward pass\n        outputs = [net(X) for X in data]\n        # Update accuracy metrics\n        acc_top1.update(label, outputs)\n        acc_top5.update(label, outputs)\n    # Retrieve and return top1 and top5 errors\n    _, top1 = acc_top1.get()\n    _, top5 = acc_top5.get()\n    return (1-top1, 1-top5)\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Emotion FERPlus Model Output (Python)\nDESCRIPTION: This Python function postprocesses the output scores from the Emotion FERPlus model. It applies a softmax function to the scores and returns the class IDs in decreasing order of probability, effectively ranking the predicted emotions.  It requires the NumPy library and a softmax function to be defined.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/emotion_ferplus/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef softmax(scores):\n  # your softmax function\n\ndef postprocess(scores):\n  '''\n  This function takes the scores generated by the network and returns the class IDs in decreasing\n  order of probability.\n  '''\n  prob = softmax(scores)\n  prob = np.squeeze(prob)\n  classes = np.argsort(prob)[::-1]\n  return classes\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing and Loading\nDESCRIPTION: Defines functions for image preprocessing and loading. It utilizes torchvision transforms to convert images to tensors and normalize them. It also includes a function to load images from the specified directory based on the COCO annotations. `get_image_filename` retrieves the file path and `load_image` applies preprocessing to the image.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/validation_accuracy.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import transforms\nfrom PIL import Image\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef get_image_filename(gt, imgId):\n    return str(dataDir/dataType/gt.imgs[imgId]['file_name'])\n\ndef load_image(gt, imgId):\n    input_image = Image.open(get_image_filename(gt, imgId)).convert('RGB')\n    input_tensor = preprocess(input_image)\n    input_tensor = input_tensor.unsqueeze(0)\n    input_tensor = input_tensor.detach().cpu().numpy()\n    \n    return input_tensor\n```\n\n----------------------------------------\n\nTITLE: Load ONNX Model into MXNet\nDESCRIPTION: This function loads an ONNX model into MXNet by importing the model, defining the model using the symbol file, and binding the parameters to the model. It requires the MXNet context and the path to the ONNX model file as input. The function returns the loaded MXNet model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_model(ctx, model):\n    image_size = (112,112)\n    # Import ONNX model\n    sym, arg_params, aux_params = import_model(model)\n    # Define and binds parameters to the network\n    model = mx.mod.Module(symbol=sym, context=ctx, label_names = None)\n    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])\n    model.set_params(arg_params, aux_params)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Running Inference with ONNX Runtime in Python\nDESCRIPTION: This code snippet demonstrates how to load the EfficientNet-Lite4 ONNX model and run inference using the ONNX Runtime. It explicitly sets the execution providers and retrieves the results.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/efficientnet-lite4/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport onnxruntime as rt\n\n# load model\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# rt.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nsess = rt.InferenceSession(MODEL + \".onnx\")\n# run inference\nresults = sess.run([\"Softmax:0\"], {\"images:0\": img_batch})[0]\n```\n\n----------------------------------------\n\nTITLE: Load Dataset Properties Python\nDESCRIPTION: This code defines a helper function, `load_property`, that loads the number of classes (`num_classes`) and image size (`image_size`) from a 'property' file located in the dataset directory. This information is essential for initializing the model architecture and data iterator. The function parses the 'property' file, extracts the relevant values, and returns them as an `EasyDict` object.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Helper function for loading num_classes and input image sizes\ndef load_property(data_dir):\n    prop = edict()\n    for line in open(os.path.join(data_dir, 'property')):\n        vec = line.strip().split(',')\n        assert len(vec)==3\n        prop.num_classes = int(vec[0])\n        prop.image_size = [int(vec[1]), int(vec[2])]\n    return prop\n```\n\n----------------------------------------\n\nTITLE: GPT-2 Tokenizer Initialization in Python\nDESCRIPTION: This code initializes a GPT-2 tokenizer from the Hugging Face transformers library. It sets the padding side to 'left' and configures the pad token to be the same as the end-of-sequence (eos) token. This is necessary for correctly formatting input sequences for the GPT-2 model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/gpt2-bs/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\n```\n\n----------------------------------------\n\nTITLE: Load Network for Validation - Python\nDESCRIPTION: This code snippet loads the MXNet module, binds the input data shape, and sets the parameters of the network. The `mx.mod.Module` is used to define the network, and `mod.bind` specifies the input data shape. `mod.set_params` loads the parameters from the imported ONNX model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load module\nmod = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\nmod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))], \n         label_shapes=mod._label_shapes)\nmod.set_params(arg_params, aux_params, allow_missing=True)\n```\n\n----------------------------------------\n\nTITLE: T5 Generative Inference with onnxt5 Utilities (Python)\nDESCRIPTION: This snippet demonstrates how to perform generative inference with the T5 model using the onnxt5 utilities. It initializes the encoder and decoder sessions, loads the tokenizer, and generates text from a given prompt.  Requires the `onnxt5` package to be installed (`pip install onnxt5`).\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/t5/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnxt5 import GenerativeT5\nfrom onnxt5.api import get_encoder_decoder_tokenizer\ndecoder_sess, encoder_sess, tokenizer = get_encoder_decoder_tokenizer()\ngenerative_t5 = GenerativeT5(encoder_sess, decoder_sess, tokenizer, onnx=True)\nprompt = 'translate English to French: I was a victim of a series of accidents.'\noutput_text, output_logits = generative_t5(prompt, max_length=100, temperature=0.)\n# output_text: \"J'ai été victime d'une série d'accidents.\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Model Hyperparameters - Python\nDESCRIPTION: This code snippet defines the hyperparameters for training the SqueezeNet model. It includes parameters such as model name, data directory, batch size, number of GPUs, number of workers, learning rate, momentum, weight decay, learning rate decay, and save directories. These parameters control the training process and can be tuned to optimize performance.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# specify model - squeezenet1.0 or squeezenet1.1\nmodel_name = 'squeezenet1.0'\n\n# training and validation pictures to use\ndata_dir = '/home/ubuntu/imagenet/img_dataset'\n\n# training batch size per device (CPU/GPU)\nbatch_size = 128\n\n# number of GPUs to use (automatically detect the number of GPUs)\nnum_gpus = len(mx.test_utils.list_gpus())\n\n# number of pre-processing workers (automatically detect the number of workers)\nnum_workers = multiprocessing.cpu_count()\n\n# number of training epochs\nnum_epochs = 100\n\n# learning rate\nlr = 0.01\n\n# momentum value for optimizer\nmomentum = 0.9\n\n# weight decay rate\nwd = 0.0002\n\n# decay rate of learning rate\nlr_decay = 0.1\n\n# interval for periodic learning rate decays\nlr_decay_period = 0\n\n# epoches at which learning rate decays\nlr_decay_epoch = '60,90'\n\n# mode in which to train the model. options are symbolic, imperative, hybrid\nmode = 'hybrid'\n\n# Number of batches to wait before logging\nlog_interval = 100\n\n# frequency of model saving\nsave_frequency = 10\n\n# directory of saved models\nsave_dir = 'params'\n\n#directory of training logs\nlogging_dir = 'logs'\n\n# the path to save the history plot\nsave_plot_dir = '.'\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Input Image for RetinaNet (PyTorch)\nDESCRIPTION: This code snippet demonstrates how to preprocess an input image tensor for the RetinaNet model using PyTorch's `transforms` module. It normalizes the image using pre-defined mean and standard deviation values before converting it to a mini-batch. The input image should be loaded in a range of [0, 1].\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/retinanet/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import transforms\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\n# Create a mini-batch as expected by the model.\ninput_batch = input_tensor.unsqueeze(0)\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Image for Mask R-CNN with Python\nDESCRIPTION: This code snippet demonstrates how to postprocess the output of the Mask R-CNN model to display detected objects with bounding boxes, class annotations, and segmentation masks on the original image. It uses matplotlib, pycocotools, and cv2. It filters detections based on a score threshold.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/mask-rcnn/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport pycocotools.mask as mask_util\nimport cv2\n\nclasses = [line.rstrip('\\n') for line in open('coco_classes.txt')]\n\ndef display_objdetect_image(image, boxes, labels, scores, masks, score_threshold=0.7):\n    # Resize boxes\n    ratio = 800.0 / min(image.size[0], image.size[1])\n    boxes /= ratio\n\n    _, ax = plt.subplots(1, figsize=(12,9))\n\n    image = np.array(image)\n\n    for mask, box, label, score in zip(masks, boxes, labels, scores):\n        # Showing boxes with score > 0.7\n        if score <= score_threshold:\n            continue\n\n        # Finding contour based on mask\n        mask = mask[0, :, :, None]\n        int_box = [int(i) for i in box]\n        mask = cv2.resize(mask, (int_box[2]-int_box[0]+1, int_box[3]-int_box[1]+1))\n        mask = mask > 0.5\n        im_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n        x_0 = max(int_box[0], 0)\n        x_1 = min(int_box[2] + 1, image.shape[1])\n        y_0 = max(int_box[1], 0)\n        y_1 = min(int_box[3] + 1, image.shape[0])\n        mask_y_0 = max(y_0 - box[1], 0)\n        mask_y_1 = mask_y_0 + y_1 - y_0\n        mask_x_0 = max(x_0 - box[0], 0)\n        mask_x_1 = mask_x_0 + x_1 - x_0\n        im_mask[y_0:y_1, x_0:x_1] = mask[\n            mask_y_0 : mask_y_1, mask_x_0 : mask_x_1\n        ]\n        im_mask = im_mask[:, :, None]\n\n        # OpenCV version 4.x\n        contours, hierarchy = cv2.findContours(\n            im_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        image = cv2.drawContours(image, contours, -1, 25, 3)\n\n        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='b', facecolor='none')\n        ax.annotate(classes[label] + ':' + str(np.round(score, 2)), (box[0], box[1]), color='w', fontsize=12)\n        ax.add_patch(rect)\n\n    ax.imshow(image)\n    plt.show()\n\ndisplay_objdetect_image(img, boxes, labels, scores, masks)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image (Python)\nDESCRIPTION: This snippet preprocesses an image by opening it using PIL, resizing it to 224x224, converting it to the YCbCr color space, extracting the Y channel, converting it to a NumPy array, and normalizing it to the range [0, 1]. The file path to the image needs to be replaced in the Image.open() call.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/dependencies/Run_Super_Resolution_Model.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\norig_img = Image.open(\"FILE_PATH_TO_IMAGE\")\nimg = resizeimage.resize_cover(orig_img, [224,224], validate=False)\nimg_ycbcr = img.convert('YCbCr')\nimg_y_0, img_cb, img_cr = img_ycbcr.split()\nimg_ndarray = np.asarray(img_y_0)\n\nimg_4 = np.expand_dims(np.expand_dims(img_ndarray, axis=0), axis=0)\nimg_5 = img_4.astype(np.float32) / 255.0\nimg_5\n```\n\n----------------------------------------\n\nTITLE: Define Main Training Execution in Python\nDESCRIPTION: This code defines the main function that initializes the neural network, starts the training process by calling the `train` function with the specified number of epochs and context, and finally exports the trained model using `net.export`. It executes the training process when the script is run directly.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    net.hybridize()\n    train(num_epochs, context)\n    #net.export(model_name)\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Super Resolution Output (Python)\nDESCRIPTION: This code snippet shows how to post-process the output of the Super Resolution model to generate the final image. It merges the upscaled Y channel with the resized Cb and Cr channels, converts the image back to RGB format, and displays the final image using Matplotlib. It assumes `img_out_y`, `img_cb`, and `img_cr` are already defined. Requires the PIL library.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\nplt.imshow(final_img)\n```\n\n----------------------------------------\n\nTITLE: Read Image in Python\nDESCRIPTION: Reads an image from the given file path, converts it to RGB format, and returns it as a NumPy array. Optionally, it displays the image using matplotlib if the 'show' parameter is set to True. Parameters: `path` - The path to the image file. `show` - A boolean indicating whether to display the image.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_image(path, show=False):\n    with Image.open(path) as img:\n        img = np.array(img.convert('RGB'))\n    if show:\n        plt.imshow(img)\n        plt.axis('off')\n    return img\n```\n\n----------------------------------------\n\nTITLE: VGG Model Definition in Gluon (Python)\nDESCRIPTION: This code defines the VGG model architecture using Gluon.  It includes the VGG class, which creates the feature extraction layers and the final output layer. It also defines helper functions to construct specific VGG variants like vgg16, vgg19, vgg16_bn, and vgg19_bn.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"VGG, implemented in Gluon.\"\"\"\n__all__ = ['VGG', 'vgg16', 'vgg19', 'vgg16_bn', 'vgg19_bn', 'get_vgg']\n\n\nclass VGG(HybridBlock):\n    r\"\"VGG model from the \\\"Very Deep Convolutional Networks for Large-Scale Image Recognition\\\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n    Parameters\n    ----------\n    layers : list of int\n        Numbers of layers in each feature block.\n    filters : list of int\n        Numbers of filters in each feature block. List length should match the layers.\n    classes : int, default 1000\n        Number of classification classes.\n    batch_norm : bool, default False\n        Use batch normalization.\n    \"\"\"\n    def __init__(self, layers, filters, classes=1000, batch_norm=False, **kwargs):\n        super(VGG, self).__init__(**kwargs)\n        assert len(layers) == len(filters)\n        with self.name_scope():\n            self.features = self._make_features(layers, filters, batch_norm)\n            self.features.add(nn.Dense(4096, activation='relu',\n                                       weight_initializer='normal',\n                                       bias_initializer='zeros'))\n            self.features.add(nn.Dropout(rate=0.5))\n            self.features.add(nn.Dense(4096, activation='relu',\n                                       weight_initializer='normal',\n                                       bias_initializer='zeros'))\n            self.features.add(nn.Dropout(rate=0.5))\n            self.output = nn.Dense(classes,\n                                   weight_initializer='normal',\n                                   bias_initializer='zeros')\n\n    def _make_features(self, layers, filters, batch_norm):\n        featurizer = nn.HybridSequential(prefix='')\n        for i, num in enumerate(layers):\n            for _ in range(num):\n                featurizer.add(nn.Conv2D(filters[i], kernel_size=3, padding=1,\n                                         weight_initializer=Xavier(rnd_type='gaussian',\n                                                                   factor_type='out',\n                                                                   magnitude=2),\n                                         bias_initializer='zeros'))\n                if batch_norm:\n                    featurizer.add(nn.BatchNorm())\n                featurizer.add(nn.Activation('relu'))\n            featurizer.add(nn.MaxPool2D(strides=2))\n        return featurizer\n\n    def hybrid_forward(self, F, x):\n        x = self.features(x)\n        x = self.output(x)\n        return x\n\n\n# Specification\nvgg_spec = {16: ([2, 2, 3, 3, 3], [64, 128, 256, 512, 512]),\n            19: ([2, 2, 4, 4, 4], [64, 128, 256, 512, 512])}\n\n\n# Constructors\ndef get_vgg(num_layers, root=os.path.join('~', '.mxnet', 'models'), **kwargs):\n    r\"\"VGG model from the \\\"Very Deep Convolutional Networks for Large-Scale Image Recognition\\\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n    Parameters\n    ----------\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 16, 19.\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    layers, filters = vgg_spec[num_layers]\n    net = VGG(layers, filters, **kwargs)\n    return net\n\ndef vgg16(**kwargs):\n    r\"\"VGG-16 model from the \\\"Very Deep Convolutional Networks for Large-Scale Image Recognition\\\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n    Parameters\n    ----------\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    return get_vgg(16, **kwargs)\n\ndef vgg19(**kwargs):\n    r\"\"VGG-19 model from the \\\"Very Deep Convolutional Networks for Large-Scale Image Recognition\\\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n    Parameters\n    ----------\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    return get_vgg(19, **kwargs)\n\ndef vgg16_bn(**kwargs):\n    r\"\"VGG-16 model with batch normalization from the\n    \\\"Very Deep Convolutional Networks for Large-Scale Image Recognition\\\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n    Parameters\n    ----------\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    kwargs['batch_norm'] = True\n    return get_vgg(16, **kwargs)\n\ndef vgg19_bn(**kwargs):\n    r\"\"VGG-19 model with batch normalization from the\n    \\\"Very Deep Convolutional Networks for Large-Scale Image Recognition\\\"\n    <https://arxiv.org/abs/1409.1556>`_ paper.\n    Parameters\n    ----------\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    kwargs['batch_norm'] = True\n    return get_vgg(19, **kwargs)\n\nmodels = {    'vgg16': vgg16,\n              'vgg19': vgg19,\n              'vgg16_bn': vgg16_bn,\n              'vgg19_bn': vgg19_bn\n         }\n\n```\n\n----------------------------------------\n\nTITLE: Super Resolution Model Definition (PyTorch)\nDESCRIPTION: This defines a SuperResolutionNet class in PyTorch, which is a convolutional neural network designed to upscale images.  It consists of convolutional layers, ReLU activation functions, and a PixelShuffle layer for increasing the image resolution. It initializes the weights of the convolutional layers using orthogonal initialization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/dependencies/Run_Super_Resolution_Model.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SuperResolutionNet(nn.Module):\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=inplace)\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv4.weight)\n\n# Create the super-resolution model by using the above model definition.\ntorch_model = SuperResolutionNet(upscale_factor=3)\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model with ONNX Runtime in Python\nDESCRIPTION: This snippet demonstrates how to load and run an ONNX model using ONNX Runtime. It creates an inference session with the specified ONNX model file. It defines a helper function to convert PyTorch tensors to NumPy arrays. Then, it prepares the input for the ONNX model and runs the inference session, retrieving the output from the model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/dependencies/roberta-sequence-classification-inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport onnxruntime\n\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\nort_session = onnxruntime.InferenceSession(\"roberta-sequence-classification-9.onnx\")\n\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_ids)}\nort_out = ort_session.run(None, ort_inputs)\n```\n\n----------------------------------------\n\nTITLE: Defining ResNet Models Dictionary\nDESCRIPTION: Defines a dictionary, `models`, that maps model names (e.g., 'resnet18_v1') to their corresponding functions (e.g., `resnet18_v1`). This dictionary serves as a central registry for available ResNet models, allowing easy access and instantiation based on the model name.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodels = {    'resnet18_v1': resnet18_v1,\n              'resnet34_v1': resnet34_v1,\n              'resnet50_v1': resnet50_v1,\n              'resnet101_v1': resnet101_v1,\n              'resnet152_v1': resnet152_v1,\n              'resnet18_v2': resnet18_v2,\n              'resnet34_v2': resnet34_v2,\n              'resnet50_v2': resnet50_v2,\n              'resnet101_v2': resnet101_v2,\n              'resnet152_v2': resnet152_v2\n         }\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Model Output\nDESCRIPTION: This code snippet illustrates how to post-process the output of the BiDAF model to extract the answer from the context. It assumes the start and end positions of the answer are provided as numpy arrays and retrieves the corresponding words from the context based on these positions.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bidirectional_attention_flow/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# assuming answer contains the np arrays for start_pos/end_pos\nstart = np.asscalar(answer[0])\nend = np.asscalar(answer[1])\nprint([w.encode() for w in cw[start:end+1].reshape(-1)])\n```\n\n----------------------------------------\n\nTITLE: T5 Decoder Model Postprocessing (Python)\nDESCRIPTION: This snippet demonstrates post-processing steps for the T5-decoder-with-lm-head model. It shows how to generate the encoder's last hidden state and then use that to generate the full model's embeddings, using the output indexes to get correct data.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/t5/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# To generate the encoder's last hidden state\nencoder_output = encoder_sess.run(None, {\"input_ids\": input_ids})[0]\n# To generate the full model's embeddings\ndecoder_output = decoder_sess.run(None, {\n                                        \"input_ids\": input_ids,\n                                        \"encoder_hidden_states\": encoder_output\n    })[0]\n```\n\n----------------------------------------\n\nTITLE: Saving Output Data as TensorProto\nDESCRIPTION: This snippet saves the output from the ONNX model in TensorProto format. It creates a directory if it doesn't exist, converts the NumPy array to a TensorProto object, and then serializes and saves the TensorProto to a file named 'output1.pb' within the specified directory.  This is useful for debugging and testing the ONNX model's output.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndata_dir = \"./test_data_set\"\ntest_data_dir = os.path.join(data_dir)\nif not os.path.exists(test_data_dir):\n    os.makedirs(test_data_dir)\n        \n# Convert the NumPy array to a TensorProto\ntensor = numpy_helper.from_array(image_data)\n# print('TensorProto:\\n{}'.format(tensor))\n\n# Save the TensorProto\nwith open(os.path.join(test_data_dir, \"output1.pb\"), 'wb') as f:\n    f.write(tensor.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Load Pretrained Model Weights (PyTorch)\nDESCRIPTION: This snippet loads pretrained weights for the super-resolution model from a URL. It initializes the model with the loaded weights and sets it to inference mode (`torch_model.eval()`). The `map_location` argument handles loading the model on CPU if a GPU is not available. A dummy input tensor is created for testing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/dependencies/Run_Super_Resolution_Model.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n\n# set the model to inference mode\ntorch_model.eval()\n\n\nprint(torch.__version__)\n\nx = torch.randn(1, 1, 224, 224, requires_grad=True)\ntorch_model.eval()\n```\n\n----------------------------------------\n\nTITLE: ResNet Specification Definition (resnet_spec)\nDESCRIPTION: Defines a dictionary, `resnet_spec`, that specifies the architecture details for different ResNet layer configurations (18, 34, 50, 101, and 152). Each entry maps the number of layers to a tuple containing the block type ('basic_block' or 'bottle_neck'), the number of layers in each block, and the number of channels in each block.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresnet_spec = {18: ('basic_block', [2, 2, 2, 2], [64, 64, 128, 256, 512]),\n               34: ('basic_block', [3, 4, 6, 3], [64, 64, 128, 256, 512]),\n               50: ('bottle_neck', [3, 4, 6, 3], [64, 256, 512, 1024, 2048]),\n               101: ('bottle_neck', [3, 4, 23, 3], [64, 256, 512, 1024, 2048]),\n               152: ('bottle_neck', [3, 8, 36, 3], [64, 256, 512, 1024, 2048])}\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Bounding Boxes in YOLOv4 - Python\nDESCRIPTION: The `postprocess_bbbox` function refines the predicted bounding boxes from the YOLOv4 model. It iterates through the prediction layers, adjusts the bounding box coordinates based on the grid cell offset, anchors, and strides. It concatenates the processed coordinates and returns a reshaped array of bounding box predictions.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE=[1,1,1]):\n    '''define anchor boxes'''\n    for i, pred in enumerate(pred_bbox):\n        conv_shape = pred.shape\n        output_size = conv_shape[1]\n        conv_raw_dxdy = pred[:, :, :, :, 0:2]\n        conv_raw_dwdh = pred[:, :, :, :, 2:4]\n        xy_grid = np.meshgrid(np.arange(output_size), np.arange(output_size))\n        xy_grid = np.expand_dims(np.stack(xy_grid, axis=-1), axis=2)\n\n        xy_grid = np.tile(np.expand_dims(xy_grid, axis=0), [1, 1, 1, 3, 1])\n        xy_grid = xy_grid.astype(np.float)\n\n        pred_xy = ((special.expit(conv_raw_dxdy) * XYSCALE[i]) - 0.5 * (XYSCALE[i] - 1) + xy_grid) * STRIDES[i]\n        pred_wh = (np.exp(conv_raw_dwdh) * ANCHORS[i])\n        pred[:, :, :, :, 0:4] = np.concatenate([pred_xy, pred_wh], axis=-1)\n\n    pred_bbox = [np.reshape(x, (-1, np.shape(x)[-1])) for x in pred_bbox]\n    pred_bbox = np.concatenate(pred_bbox, axis=0)\n    return pred_bbox\n```\n\n----------------------------------------\n\nTITLE: Visualize Auxiliary Output from FCN in Python\nDESCRIPTION: This code visualizes the auxiliary output from the FCN model.  It uses the `visualize_output` function to process the auxiliary output (`aux[0]`) and then stores the confidence score, segmented image, and blended image. This is similar to visualizing the primary output.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\naux_conf, aux_result, aux_blended, _ = visualize_output(orig_tensor, aux[0])\n```\n\n----------------------------------------\n\nTITLE: FCN Output Visualization and Overlay (Python)\nDESCRIPTION: This Python code snippet demonstrates how to post-process the output of an FCN model, overlaying the segmentation results onto the original image.  It includes functions for colorizing the output labels based on a predefined palette, and blending the colorized segmentation with the original image for visualization.  It requires `PIL`, `matplotlib`, `numpy`, and `cv2` libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom matplotlib.colors import hsv_to_rgb\nimport numpy as np\nimport cv2\n\n\nclasses = [line.rstrip('\\n') for line in open('voc_classes.txt')]\nnum_classes = len(classes)\n\ndef get_palette():\n    # prepare and return palette\n    palette = [0] * num_classes * 3\n\n    for hue in range(num_classes):\n        if hue == 0: # Background color\n            colors = (0, 0, 0)\n        else:\n            colors = hsv_to_rgb((hue / num_classes, 0.75, 0.75))\n\n        for i in range(3):\n            palette[hue * 3 + i] = int(colors[i] * 255)\n\n    return palette\n\ndef colorize(labels):\n    # generate colorized image from output labels and color palette\n    result_img = Image.fromarray(labels).convert('P', colors=num_classes)\n    result_img.putpalette(get_palette())\n    return np.array(result_img.convert('RGB'))\n\ndef visualize_output(image, output):\n    assert(image.shape[0] == output.shape[1] and \\\n           image.shape[1] == output.shape[2]) # Same height and width\n    assert(output.shape[0] == num_classes)\n\n    # get classification labels\n    raw_labels = np.argmax(output, axis=0).astype(np.uint8)\n\n    # comput confidence score\n    confidence = float(np.max(output, axis=0).mean())\n\n    # generate segmented image\n    result_img = colorize(raw_labels)\n\n    # generate blended image\n    blended_img = cv2.addWeighted(image[:, :, ::-1], 0.5, result_img, 0.5, 0)\n\n    result_img = Image.fromarray(result_img)\n    blended_img = Image.fromarray(blended_img)\n\n    return confidence, result_img, blended_img, raw_labels\n\nconf, result_img, blended_img, raw_labels = visualize_output(orig_tensor, one_output)\n```\n\n----------------------------------------\n\nTITLE: Category ID Mapping and Data Loading\nDESCRIPTION: Includes a dictionary for mapping COCO category IDs to Pascal VOC IDs and a function `load_image_and_ann` to load images and corresponding annotations. It also defines a function `getImgIdsUnion` to retrieve image IDs that contain at least one of the specified categories. The `COCO_TO_VOC` maps the COCO category to the Pascal VOC category IDs. The `load_image_and_ann` loads the images, generate one-hot encoded tensors from annotations, and includes a background class for the pixels not labeled.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/validation_accuracy.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Dictionary to do map between different category ids\n# key = COCO id, value = Pascal VOC id\nCOCO_TO_VOC = {\n    1: 15,  # person\n    2: 2,   # bicycle\n    3: 7,   # car\n    4: 14,  # motorbike\n    5: 1,   # airplane\n    6: 6,   # bus\n    7: 19,  # train\n    9: 4,   # boat\n    16: 3,  # bird\n    17: 8,  # cat\n    18: 12, # dog\n    19: 13, # horse\n    20: 17, # sheep\n    21: 10, # cow\n    44: 5,  # bottle\n    62: 9,  # chair\n    63: 18, # couch/sofa\n    64: 16, # potted plant\n    67: 11, # dining table\n    72: 20, # tv\n}\n\nVOC_CAT_IDS = list(COCO_TO_VOC.keys())\n\ndef load_image_and_ann(gt, imgId):\n    input_tensor = load_image(gt, imgId)\n    \n    _, _, height, width = input_tensor.shape\n    output_tensor = np.zeros((21, height, width), dtype=np.uint8)\n    \n    annIds = gt.getAnnIds(imgId, VOC_CAT_IDS)\n    for ann in gt.loadAnns(annIds):\n        mask = gt.annToMask(ann)\n        output_tensor[COCO_TO_VOC[ann['category_id']]] |= mask\n        \n    # Set everything not labeled to be background\n    output_tensor[0] = 1 - np.max(output_tensor, axis=0)\n    \n    # Add extra dimension to comply with batch format\n    output_tensor = output_tensor[np.newaxis, ...]\n    \n    return input_tensor, output_tensor\n\ndef getImgIdsUnion(gt, catIds):\n    \"\"\"\n    Returns all the images that have *any* of the categories in `catIds`,\n    unlike the built-in `gt.getImgIds` which returns all the images containing\n    *all* of the categories in `catIds`.\n    \"\"\"\n    imgIds = set()\n    \n    for catId in catIds:\n        imgIds |= set(gt.catToImgs[catId])\n        \n    return list(imgIds)\n\nimgIds = getImgIdsUnion(cocoGt, VOC_CAT_IDS)\n```\n\n----------------------------------------\n\nTITLE: ResNetV1 Class Definition in MXNet/Gluon\nDESCRIPTION: Defines the ResNetV1 class using MXNet's HybridBlock. It takes parameters such as the block type (BasicBlockV1, BottleneckV1), number of layers, number of channels, number of classes, and a thumbnail flag. It constructs the ResNetV1 architecture with convolutional layers, batch normalization, activation functions, and pooling layers.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass ResNetV1(HybridBlock):\n    r\"\"\"ResNet V1 model from\n    `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    Parameters\n    ----------\n    block : HybridBlock\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    channels : list of int\n        Numbers of channels in each block. Length should be one larger than layers list.\n    classes : int, default 1000\n        Number of classification classes.\n    thumbnail : bool, default False\n        Enable thumbnail.\n    \"\"\"\n    def __init__(self, block, layers, channels, classes=1000, thumbnail=False, **kwargs):\n        super(ResNetV1, self).__init__(**kwargs)\n        assert len(layers) == len(channels) - 1\n        with self.name_scope():\n            self.features = nn.HybridSequential(prefix='')\n            if thumbnail:\n                self.features.add(_conv3x3(channels[0], 1, 0))\n            else:\n                self.features.add(nn.Conv2D(channels[0], 7, 2, 3, use_bias=False))\n                self.features.add(nn.BatchNorm())\n                self.features.add(nn.Activation('relu'))\n                self.features.add(nn.MaxPool2D(3, 2, 1))\n\n            for i, num_layer in enumerate(layers):\n                stride = 1 if i == 0 else 2\n                self.features.add(self._make_layer(block, num_layer, channels[i+1],\n                                                   stride, i+1, in_channels=channels[i]))\n            self.features.add(nn.GlobalAvgPool2D())\n\n            self.output = nn.Dense(classes, in_units=channels[-1])\n\n    def _make_layer(self, block, layers, channels, stride, stage_index, in_channels=0):\n        layer = nn.HybridSequential(prefix='stage%d_'%stage_index)\n        with layer.name_scope():\n            layer.add(block(channels, stride, channels != in_channels, in_channels=in_channels,\n                            prefix=''))\n            for _ in range(layers-1):\n                layer.add(block(channels, 1, False, in_channels=channels, prefix=''))\n        return layer\n\n    def hybrid_forward(self, F, x):\n        x = self.features(x)\n        x = self.output(x)\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Printing BERT Prediction Results from JSON\nDESCRIPTION: This code snippet reads the generated `predictions.json` file and prints its contents to the console in a formatted JSON structure, using the `json` module. It allows reviewing the predicted answers generated by the BERT-Squad model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/BERT-Squad.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# print results\nimport json\nwith open(output_prediction_file) as json_file:  \n    test_data = json.load(json_file)\n    print(json.dumps(test_data, indent=2))\n```\n\n----------------------------------------\n\nTITLE: Compute Similarity Metrics\nDESCRIPTION: This code calculates the squared distance and cosine similarity between the feature embeddings of the two input images. The squared distance is computed as the sum of squared differences between the embeddings, and the cosine similarity is computed as the dot product of the embeddings.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Compute squared distance between embeddings\ndist = np.sum(np.square(out1-out2))\n# Compute cosine similarity between embedddings\nsim = np.dot(out1, out2.T)\n```\n\n----------------------------------------\n\nTITLE: Quantizing AlexNet Model with Intel Neural Compressor\nDESCRIPTION: Quantizes the AlexNet model using the Intel Neural Compressor. This script utilizes a configuration file (alexnet.yaml) to specify quantization parameters and requires paths to the input model, data, and labels. It saves the quantized model to a specified output path.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/alexnet/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=alexnet.yaml \\\n                   --data_path=/path/to/imagenet \\\n                   --label_path=/path/to/imagenet/label \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Display Original Image in Python\nDESCRIPTION: This snippet uses `matplotlib` to display the original image. It uses the `imshow` function to render the NumPy array representing the image. `%matplotlib inline` ensures that the plot is displayed within the notebook.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib.pyplot import imshow\n%matplotlib inline\n\nimshow(orig_tensor)\n```\n\n----------------------------------------\n\nTITLE: ResNet Constructor (get_resnet)\nDESCRIPTION: Defines a function, `get_resnet`, that constructs a ResNet model based on the specified version and number of layers. It retrieves the appropriate ResNet class and block class from the pre-defined lists and dictionaries based on the version and number of layers, and then instantiates the model with the correct parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_resnet(version, num_layers, **kwargs):\n    r\"\"\"ResNet V1 model from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    ResNet V2 model from `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    Parameters\n    ----------\n    version : int\n        Version of ResNet. Options are 1, 2.\n    num_layers : int\n        Numbers of layers. Options are 18, 34, 50, 101, 152.\n    \"\"\"\n    assert num_layers in resnet_spec, \\\n        \"Invalid number of layers: %d. Options are %s\"%(\\\n            num_layers, str(resnet_spec.keys()))\n    block_type, layers, channels = resnet_spec[num_layers]\n    assert version >= 1 and version <= 2, \\\n        \"Invalid resnet version: %d. Options are 1 and 2.\"%version\n    resnet_class = resnet_net_versions[version-1]\n    block_class = resnet_block_versions[version-1][block_type]\n    net = resnet_class(block_class, layers, channels, **kwargs)\n    return net\n```\n\n----------------------------------------\n\nTITLE: Defining ImageNet Class Labels\nDESCRIPTION: This snippet defines a dictionary where keys are integer IDs and values are the corresponding ImageNet class names. It allows for easy lookup of class names given a predicted ID from an image classification model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/efficientnet-lite4/dependencies/labels_map.txt#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"0\": \"tench, Tinca tinca\", \"1\": \"goldfish, Carassius auratus\", \"2\": \"great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\", \"3\": \"tiger shark, Galeocerdo cuvieri\", \"4\": \"hammerhead, hammerhead shark\", \"5\": \"electric ray, crampfish, numbfish, torpedo\", \"6\": \"stingray\", \"7\": \"cock\", \"8\": \"hen\", \"9\": \"ostrich, Struthio camelus\", \"10\": \"brambling, Fringilla montifringilla\", \"11\": \"goldfinch, Carduelis carduelis\", \"12\": \"house finch, linnet, Carpodacus mexicanus\", \"13\": \"junco, snowbird\", \"14\": \"indigo bunting, indigo finch, indigo bird, Passerina cyanea\", \"15\": \"robin, American robin, Turdus migratorius\", \"16\": \"bulbul\", \"17\": \"jay\", \"18\": \"magpie\", \"19\": \"chickadee\", \"20\": \"water ouzel, dipper\", \"21\": \"kite\", \"22\": \"bald eagle, American eagle, Haliaeetus leucocephalus\", \"23\": \"vulture\", \"24\": \"great grey owl, great gray owl, Strix nebulosa\", \"25\": \"European fire salamander, Salamandra salamandra\", \"26\": \"common newt, Triturus vulgaris\", \"27\": \"eft\", \"28\": \"spotted salamander, Ambystoma maculatum\", \"29\": \"axolotl, mud puppy, Ambystoma mexicanum\", \"30\": \"bullfrog, Rana catesbeiana\", \"31\": \"tree frog, tree-frog\", \"32\": \"tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\", \"33\": \"loggerhead, loggerhead turtle, Caretta caretta\", \"34\": \"leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\", \"35\": \"mud turtle\", \"36\": \"terrapin\", \"37\": \"box turtle, box tortoise\", \"38\": \"banded gecko\", \"39\": \"common iguana, iguana, Iguana iguana\", \"40\": \"American chameleon, anole, Anolis carolinensis\", \"41\": \"whiptail, whiptail lizard\", \"42\": \"agama\", \"43\": \"frilled lizard, Chlamydosaurus kingi\", \"44\": \"alligator lizard\", \"45\": \"Gila monster, Heloderma suspectum\", \"46\": \"green lizard, Lacerta viridis\", \"47\": \"African chameleon, Chamaeleo chamaeleon\", \"48\": \"Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\", \"49\": \"African crocodile, Nile crocodile, Crocodylus niloticus\", \"50\": \"American alligator, Alligator mississipiensis\", \"51\": \"triceratops\", \"52\": \"thunder snake, worm snake, Carphophis amoenus\", \"53\": \"ringneck snake, ring-necked snake, ring snake\", \"54\": \"hognose snake, puff adder, sand viper\", \"55\": \"green snake, grass snake\", \"56\": \"king snake, kingsnake\", \"57\": \"garter snake, grass snake\", \"58\": \"water snake\", \"59\": \"vine snake\", \"60\": \"night snake, Hypsiglena torquata\", \"61\": \"boa constrictor, Constrictor constrictor\", \"62\": \"rock python, rock snake, Python sebae\", \"63\": \"Indian cobra, Naja naja\", \"64\": \"green mamba\", \"65\": \"sea snake\", \"66\": \"horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\", \"67\": \"diamondback, diamondback rattlesnake, Crotalus adamanteus\", \"68\": \"sidewinder, horned rattlesnake, Crotalus cerastes\", \"69\": \"trilobite\", \"70\": \"harvestman, daddy longlegs, Phalangium opilio\", \"71\": \"scorpion\", \"72\": \"black and gold garden spider, Argiope aurantia\", \"73\": \"barn spider, Araneus cavaticus\", \"74\": \"garden spider, Aranea diademata\", \"75\": \"black widow, Latrodectus mactans\", \"76\": \"tarantula\", \"77\": \"wolf spider, hunting spider\", \"78\": \"tick\", \"79\": \"centipede\", \"80\": \"black grouse\", \"81\": \"ptarmigan\", \"82\": \"ruffed grouse, partridge, Bonasa umbellus\", \"83\": \"prairie chicken, prairie grouse, prairie fowl\", \"84\": \"peacock\", \"85\": \"quail\", \"86\": \"partridge\", \"87\": \"African grey, African gray, Psittacus erithacus\", \"88\": \"macaw\", \"89\": \"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\", \"90\": \"lorikeet\", \"91\": \"coucal\", \"92\": \"bee eater\", \"93\": \"hornbill\", \"94\": \"hummingbird\", \"95\": \"jacamar\", \"96\": \"toucan\", \"97\": \"drake\", \"98\": \"red-breasted merganser, Mergus serrator\", \"99\": \"goose\", \"100\": \"black swan, Cygnus atratus\", \"101\": \"tusker\", \"102\": \"echidna, spiny anteater, anteater\", \"103\": \"platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\", \"104\": \"wallaby, brush kangaroo\", \"105\": \"koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\", \"106\": \"wombat\", \"107\": \"jellyfish\", \"108\": \"sea anemone, anemone\", \"109\": \"brain coral\", \"110\": \"flatworm, platyhelminth\", \"111\": \"nematode, nematode worm, roundworm\", \"112\": \"conch\", \"113\": \"snail\", \"114\": \"slug\", \"115\": \"sea slug, nudibranch\", \"116\": \"chiton, coat-of-mail shell, sea cradle, polyplacophore\", \"117\": \"chambered nautilus, pearly nautilus, nautilus\", \"118\": \"Dungeness crab, Cancer magister\", \"119\": \"rock crab, Cancer irroratus\", \"120\": \"fiddler crab\", \"121\": \"king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\", \"122\": \"American lobster, Northern lobster, Maine lobster, Homarus americanus\", \"123\": \"spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\", \"124\": \"crayfish, crawfish, crawdad, crawdaddy\", \"125\": \"hermit crab\", \"126\": \"isopod\", \"127\": \"white stork, Ciconia ciconia\", \"128\": \"black stork, Ciconia nigra\", \"129\": \"spoonbill\", \"130\": \"flamingo\", \"131\": \"little blue heron, Egretta caerulea\", \"132\": \"American egret, great white heron, Egretta albus\", \"133\": \"bittern\", \"134\": \"crane\", \"135\": \"limpkin, Aramus pictus\", \"136\": \"European gallinule, Porphyrio porphyrio\", \"137\": \"American coot, marsh hen, mud hen, water hen, Fulica americana\", \"138\": \"bustard\", \"139\": \"ruddy turnstone, Arenaria interpres\", \"140\": \"red-backed sandpiper, dunlin, Erolia alpina\", \"141\": \"redshank, Tringa totanus\", \"142\": \"dowitcher\", \"143\": \"oystercatcher, oyster catcher\", \"144\": \"pelican\", \"145\": \"king penguin, Aptenodytes patagonica\", \"146\": \"albatross, mollymawk\", \"147\": \"grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\", \"148\": \"killer whale, killer, orca, grampus, sea wolf, Orcinus orca\", \"149\": \"dugong, Dugong dugon\", \"150\": \"sea lion\", \"151\": \"Chihuahua\", \"152\": \"Japanese spaniel\", \"153\": \"Maltese dog, Maltese terrier, Maltese\", \"154\": \"Pekinese, Pekingese, Peke\", \"155\": \"Shih-Tzu\", \"156\": \"Blenheim spaniel\", \"157\": \"papillon\", \"158\": \"toy terrier\", \"159\": \"Rhodesian ridgeback\", \"160\": \"Afghan hound, Afghan\", \"161\": \"basset, basset hound\", \"162\": \"beagle\", \"163\": \"bloodhound, sleuthhound\", \"164\": \"bluetick\", \"165\": \"black-and-tan coonhound\", \"166\": \"Walker hound, Walker foxhound\", \"167\": \"English foxhound\", \"168\": \"redbone\", \"169\": \"borzoi, Russian wolfhound\", \"170\": \"Irish wolfhound\", \"171\": \"Italian greyhound\", \"172\": \"whippet\", \"173\": \"Ibizan hound, Ibizan Podenco\", \"174\": \"Norwegian elkhound, elkhound\", \"175\": \"otterhound, otter hound\", \"176\": \"Saluki, gazelle hound\", \"177\": \"Scottish deerhound, deerhound\", \"178\": \"Weimaraner\", \"179\": \"Staffordshire bullterrier, Staffordshire bull terrier\", \"180\": \"American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\", \"181\": \"Bedlington terrier\", \"182\": \"Border terrier\", \"183\": \"Kerry blue terrier\", \"184\": \"Irish terrier\", \"185\": \"Norfolk terrier\", \"186\": \"Norwich terrier\", \"187\": \"Yorkshire terrier\", \"188\": \"wire-haired fox terrier\", \"189\": \"Lakeland terrier\", \"190\": \"Sealyham terrier, Sealyham\", \"191\": \"Airedale, Airedale terrier\", \"192\": \"cairn, cairn terrier\", \"193\": \"Australian terrier\", \"194\": \"Dandie Dinmont, Dandie Dinmont terrier\", \"195\": \"Boston bull, Boston terrier\", \"196\": \"miniature schnauzer\", \"197\": \"giant schnauzer\", \"198\": \"standard schnauzer\", \"199\": \"Scotch terrier, Scottish terrier, Scottie\", \"200\": \"Tibetan terrier, chrysanthemum dog\", \"201\": \"silky terrier, Sydney silky\", \"202\": \"soft-coated wheaten terrier\", \"203\": \"West Highland white terrier\", \"204\": \"Lhasa, Lhasa apso\", \"205\": \"flat-coated retriever\", \"206\": \"curly-coated retriever\", \"207\": \"golden retriever\", \"208\": \"Labrador retriever\", \"209\": \"Chesapeake Bay retriever\", \"210\": \"German short-haired pointer\", \"211\": \"vizsla, Hungarian pointer\", \"212\": \"English setter\", \"213\": \"Irish setter, red setter\", \"214\": \"Gordon setter\", \"215\": \"Brittany spaniel\", \"216\": \"clumber, clumber spaniel\", \"217\": \"English springer, English springer spaniel\", \"218\": \"Welsh springer spaniel\", \"219\": \"cocker spaniel, English cocker spaniel, cocker\", \"220\": \"Sussex spaniel\", \"221\": \"Irish water spaniel\", \"222\": \"kuvasz\", \"223\": \"schipperke\", \"224\": \"groenendael\", \"225\": \"malinois\", \"226\": \"briard\", \"227\": \"kelpie\", \"228\": \"komondor\", \"229\": \"Old English sheepdog, bobtail\", \"230\": \"Shetland sheepdog, Shetland sheep dog, Shetland\", \"231\": \"collie\", \"232\": \"Border collie\", \"233\": \"Bouvier des Flandres, Bouviers des Flandres\", \"234\": \"Rottweiler\", \"235\": \"German shepherd, German shepherd dog, German police dog, alsatian\", \"236\": \"Doberman, Doberman pinscher\", \"237\": \"miniature pinscher\", \"238\": \"Greater Swiss Mountain dog\", \"239\": \"Bernese mountain dog\", \"240\": \"Appenzeller\", \"241\": \"EntleBucher\", \"242\": \"boxer\", \"243\": \"bull mastiff\", \"244\": \"Tibetan mastiff\", \"245\": \"French bulldog\", \"246\": \"Great Dane\", \"247\": \"Saint Bernard, St Bernard\", \"248\": \"Eskimo dog, husky\", \"249\": \"malamute, malemute, Alaskan malamute\", \"250\": \"Siberian husky\", \"251\": \"dalmatian, coach dog, carriage dog\", \"252\": \"affenpinscher, monkey pinscher, monkey dog\", \"253\": \"basenji\", \"254\": \"pug, pug-dog\", \"255\": \"Leonberg\", \"256\": \"Newfoundland, Newfoundland dog\", \"257\": \"Great Pyrenees\", \"258\": \"Samoyed, Samoyede\", \"259\": \"Pomeranian\", \"260\": \"chow, chow chow\", \"261\": \"keeshond\", \"262\": \"Brabancon griffon\", \"263\": \"Pembroke, Pembroke Welsh corgi\", \"264\": \"Cardigan, Cardigan Welsh corgi\", \"265\": \"toy poodle\", \"266\": \"miniature poodle\", \"267\": \"standard poodle\", \"268\": \"Mexican hairless\", \"269\": \"timber wolf, grey wolf, gray wolf, Canis lupus\", \"270\": \"white wolf, Arctic wolf, Canis lupus tundrarum\", \"271\": \"red wolf, maned wolf, Canis rufus, Canis niger\", \"272\": \"coyote, prairie wolf, brush wolf, Canis latrans\", \"273\": \"dingo, warrigal, warragal, Canis dingo\", \"274\": \"dhole, Cuon alpinus\", \"275\": \"African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\", \"276\": \"hyena, hyaena\", \"277\": \"red fox, Vulpes vulpes\", \"278\": \"kit fox, Vulpes macrotis\", \"279\": \"Arctic fox, white fox, Alopex lagopus\", \"280\": \"grey fox, gray fox, Uroc\n```\n\n----------------------------------------\n\nTITLE: Compute Evaluations - Python\nDESCRIPTION: This code snippet computes the evaluations for the ONNX model by performing a forward pass over each batch of the validation data. The code iterates through the validation data loader, performs a forward pass using the MXNet module, and updates the accuracy metrics (top-1 and top-5). The results are printed periodically to track progress.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Compute evaluations\nBatch = namedtuple('Batch', ['data'])\nacc_top1.reset()\nacc_top5.reset()\nnum_batches = int(50000/batch_size)\nprint('[0 / %d] batches done'%(num_batches))\n# Loop over batches\nfor i, batch in enumerate(val_data):\n    # Load batch\n    data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n    label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n    # Perform forward pass\n    mod.forward(Batch([data[0]]))\n    outputs=mod.get_outputs()\n    # Update accuracy metrics\n    acc_top1.update(label, outputs)\n    acc_top5.update(label, outputs)\n    if (i+1)%50==0:\n        print('[%d / %d] batches done'%(i+1,num_batches))\n```\n\n----------------------------------------\n\nTITLE: Define Evaluation Metric (IoU) - Python\nDESCRIPTION: This code defines a custom evaluation metric, IoUMetric, based on the mean Intersection Over Union (mIOU). It includes a helper function, check_label_shapes, to ensure the shape consistency between predicted and ground truth labels. The IoUMetric class calculates the intersection and union of predicted and ground truth labels to measure segmentation accuracy, accounting for an ignore label.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef check_label_shapes(labels, preds, shape=0):\n    if shape == 0:\n        label_shape, pred_shape = len(labels), len(preds)\n    else:\n        label_shape, pred_shape = labels.shape, preds.shape\n\n    if label_shape != pred_shape:\n        raise ValueError(\"Shape of labels {} does not match shape of \"\n                         \"predictions {}\".format(label_shape, pred_shape))\n\nclass IoUMetric(mx.metric.EvalMetric):\n    def __init__(self, ignore_label, label_num, name='IoU'):\n        self._ignore_label = ignore_label\n        self._label_num = label_num\n        super(IoUMetric, self).__init__(name=name)\n\n    def reset(self):\n        self._tp = [0.0] * self._label_num\n        self._denom = [0.0] * self._label_num\n\n    def update(self, labels, preds):\n        check_label_shapes(labels, preds)\n        for i in range(len(labels)):\n            pred_label = mx.ndarray.argmax_channel(preds[i]).asnumpy().astype('int32')\n            label = labels[i].asnumpy().astype('int32')\n\n            check_label_shapes(label, pred_label)\n\n            iou = 0\n            eps = 1e-6\n            for j in range(self._label_num):\n                pred_cur = (pred_label.flat == j)\n                gt_cur = (label.flat == j)\n                tp = np.logical_and(pred_cur, gt_cur).sum()\n                denom = np.logical_or(pred_cur, gt_cur).sum() - np.logical_and(pred_cur, label.flat == self._ignore_label).sum()\n                assert tp <= denom\n                self._tp[j] += tp\n                self._denom[j] += denom\n                iou += self._tp[j] / (self._denom[j] + eps)\n            iou /= self._label_num\n            self.sum_metric = iou\n            self.num_inst = 1\n\n            \n# Create evaluation metric\nmet = IoUMetric(ignore_label=255, label_num=19, name=\"IoU\")\nmetric = mx.metric.create(met)\n```\n\n----------------------------------------\n\nTITLE: Define Data Preprocessing Functions - PyTorch\nDESCRIPTION: This code defines preprocessing functions for training and validation data using PyTorch's `transforms`. It includes normalization, resizing, cropping, and data augmentation techniques like random flips and color jittering.  The functions `preprocess_train_data` and `preprocess_test_data` are defined for respective data sets.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnormalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\njitter_param = 0.4\nlighting_param = 0.1\n\n# Input pre-processing for train data\ndef preprocess_train_data(normalize, jitter_param, lighting_param):\n    transform_train = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomFlipLeftRight(),\n        transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                     saturation=jitter_param),\n        transforms.RandomLighting(lighting_param),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_train\n\n# Input pre-processing for validation data\ndef preprocess_test_data(normalize):\n    transform_test = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_test\n```\n\n----------------------------------------\n\nTITLE: Generating Text with GPT-2-LM-HEAD in Python\nDESCRIPTION: This snippet generates text using the GPT-2-LM-HEAD model. It initializes the model, tokenizer, and input context. It then iterates to predict the next word, appending the predicted word to the output sequence. The generated text is then decoded using the tokenizer and printed. Requires the `transformers` library and PyTorch.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/gpt-2/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import GPT2Tokenizer\n\nbatch_size = 1\nlength = 10\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\ntext = \"Here is some text to encode : Hello World!\"\ntokens = np.array(tokenizer.encode(text))\ncontext = torch.tensor(tokens, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\nprev = context\noutput = context\n\nfor i in range(length):\n    outputs = model(prev)\n    logits = outputs[0]\n    logits = logits[:, -1, :]\n    log_probs = F.softmax(logits, dim=-1)\n    _, prev = torch.topk(log_probs, k=1, dim=-1)\n    output = torch.cat((output, prev), dim=1)\n\noutput = output[:, len(tokens):].tolist()\ngenerated = 0\nfor i in range(batch_size):\n    generated += 1\n    text = tokenizer.decode(output[i])\n    print(text)\n```\n\n----------------------------------------\n\nTITLE: Testing Function - MXNet/Gluon\nDESCRIPTION: Defines a `test` function to compute validation errors on the given validation data using the specified context. It iterates through the validation data, performs a forward pass through the network, and updates the accuracy metrics. The function requires a trained model `net`, context `ctx` and validation data `val_data` as input.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Test function\ndef test(ctx, val_data):\n    # Reset accuracy metrics\n    acc_top1.reset()\n    acc_top5.reset()\n    for i, batch in enumerate(val_data):\n        # Load validation batch\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n        # Perform forward pass\n        outputs = [net(X) for X in data]\n        # Update accuracy metrics\n        acc_top1.update(label, outputs)\n        acc_top5.update(label, outputs)\n    # Retrieve and return top1 and top5 errors\n    _, top1 = acc_top1.get()\n    _, top5 = acc_top5.get()\n    return (1-top1, 1-top5)\n```\n\n----------------------------------------\n\nTITLE: ResNet Model Definition Functions (resnet*_v*)\nDESCRIPTION: Defines a set of functions (e.g., `resnet18_v1`, `resnet34_v2`) that create specific ResNet models by calling the `get_resnet` function with the appropriate version and number of layers. These functions provide a convenient way to instantiate specific ResNet architectures.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef resnet18_v1(**kwargs):\n    r\"\"\"ResNet-18 V1 model from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    \"\"\"\n    return get_resnet(1, 18, **kwargs)\n\ndef resnet34_v1(**kwargs):\n    r\"\"\"ResNet-34 V1 model from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    \"\"\"\n    return get_resnet(1, 34, **kwargs)\n\ndef resnet50_v1(**kwargs):\n    r\"\"\"ResNet-50 V1 model from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    \"\"\"\n    return get_resnet(1, 50, **kwargs)\n\ndef resnet101_v1(**kwargs):\n    r\"\"\"ResNet-101 V1 model from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    \"\"\"\n    return get_resnet(1, 101, **kwargs)\n\ndef resnet152_v1(**kwargs):\n    r\"\"\"ResNet-152 V1 model from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    \"\"\"\n    return get_resnet(1, 152, **kwargs)\n\ndef resnet18_v2(**kwargs):\n    r\"\"\"ResNet-18 V2 model from `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    \"\"\"\n    return get_resnet(2, 18, **kwargs)\n\ndef resnet34_v2(**kwargs):\n    r\"\"\"ResNet-34 V2 model from `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    \"\"\"\n    return get_resnet(2, 34, **kwargs)\n\ndef resnet50_v2(**kwargs):\n    r\"\"\"ResNet-50 V2 model from `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    \"\"\"\n    return get_resnet(2, 50, **kwargs)\n\ndef resnet101_v2(**kwargs):\n    r\"\"\"ResNet-101 V2 model from `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    \"\"\"\n    return get_resnet(2, 101, **kwargs)\n\ndef resnet152_v2(**kwargs):\n    r\"\"\"ResNet-152 V2 model from `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    \"\"\"\n    return get_resnet(2, 152, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Saving Tensor Data as Protobuf\nDESCRIPTION: The `SaveData` function saves tensor data to protobuf files for ONNX testing. It converts PyTorch tensors to NumPy arrays and then uses `SaveTensorProto` to serialize and save them.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/style_transfer/fast_neural_style/dependencies/conversion.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef SaveData(test_data_dir, prefix, data_list):\n    if isinstance(data_list, torch.autograd.Variable) or isinstance(data_list, torch.Tensor):\n        data_list = [data_list]\n    for i, d in enumerate(data_list):\n        d = d.data.cpu().numpy()\n        SaveTensorProto(os.path.join(test_data_dir, '{0}_{1}.pb'.format(prefix, i)), prefix + str(i+1), d)\n```\n\n----------------------------------------\n\nTITLE: Quantizing Mask R-CNN Model using Bash\nDESCRIPTION: This bash script uses Intel® Neural Compressor to quantize the Mask R-CNN model.  It requires specifying the input model path, the configuration file, the path to the COCO2017 dataset, and the output model path. The parameters are passed to the run_tuning.sh script.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/mask-rcnn/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --config=mask_rcnn.yaml \\\n                   --data_path=path/to/COCO2017 \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Defining 3x3 Convolutional Layer (MXNet)\nDESCRIPTION: This code defines a helper function `_conv3x3` which returns a 3x3 convolutional layer using `nn.Conv2D` from MXNet's Gluon API. It initializes the layer with specified channels, stride, input channels, no bias and padding set to 1. The function creates a convolutional layer which is then used inside ResNet blocks.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _conv3x3(channels, stride, in_channels):\n    return nn.Conv2D(channels, kernel_size=3, strides=stride, padding=1,\n                     use_bias=False, in_channels=in_channels)\n```\n\n----------------------------------------\n\nTITLE: Define Accuracy Metrics - MXNet\nDESCRIPTION: This code initializes MXNet accuracy metrics for top-1 and top-5 error calculation and sets up a training history logger.  It imports the mxnet library and initializes the metrics.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nacc_top1 = mx.metric.Accuracy()\nacc_top5 = mx.metric.TopKAccuracy(5)\ntrain_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n                                 'validation-top1-err', 'validation-top5-err'])\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for SSD Model in Python\nDESCRIPTION: This Python code snippet demonstrates how to preprocess an image for use with the SSD object detection model. It includes resizing the image to (1200, 1200) using bilinear interpolation, transposing the dimensions, normalizing the pixel values using a mean and standard deviation, and converting the image to a NumPy array. This code requires the `numpy` and `PIL` (Pillow) libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\n\ndef preprocess(img_path):\n    input_shape = (1, 3, 1200, 1200)\n    img = Image.open(img_path)\n    img = img.resize((1200, 1200), Image.BILINEAR)\n    img_data = np.array(img)\n    img_data = np.transpose(img_data, [2, 0, 1])\n    img_data = np.expand_dims(img_data, 0)\n    mean_vec = np.array([0.485, 0.456, 0.406])\n    stddev_vec = np.array([0.229, 0.224, 0.225])\n    norm_img_data = np.zeros(img_data.shape).astype('float32')\n    for i in range(img_data.shape[1]):\n        norm_img_data[:,i,:,:] = (img_data[:,i,:,:]/255 - mean_vec[i]) / stddev_vec[i]\n    return norm_img_data\n```\n\n----------------------------------------\n\nTITLE: Quantize Model (Bash)\nDESCRIPTION: This bash command initiates the quantization process for the Emotion FERPlus model using the Intel® Neural Compressor. It calls the 'run_tuning.sh' script with specified input model path, dataset location, and output model path. Make sure to change the paths to the actual locations. This command should be executed from the specified directory.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/emotion_ferplus/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd neural-compressor/examples/onnxrt/body_analysis/onnx_model_zoo/emotion_ferplus/quantization/ptq_static\nbash run_tuning.sh --input_model=path/to/model  \\ # model path as *.onnx\n                   --dataset_location=/path/to/data \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Validation Data with Transforms\nDESCRIPTION: This function preprocesses validation data. It resizes the image, performs center cropping, converts it to a tensor, and normalizes it. It depends on `torchvision.transforms` and takes a normalization transform as input.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_test_data(normalize):\n    transform_test = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_test\n```\n\n----------------------------------------\n\nTITLE: Download and load label file\nDESCRIPTION: This snippet downloads the synset.txt file containing class labels for ImageNet from a specified URL. It then opens the file and reads the labels into a list.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/synset.txt')\nwith open('synset.txt', 'r') as f:\n    labels = [l.rstrip() for l in f]\n```\n\n----------------------------------------\n\nTITLE: Setup Training Environment - Python\nDESCRIPTION: This code snippet sets up the training environment by specifying the logging function, number of classes, batch size, context (CPU or GPU), learning rate decay epochs, optimizer, and retrieving the gluon model. It defines the necessary components for configuring the training loop.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Specify logging fucntion\nlogging.basicConfig(level=logging.INFO)\n\n# Specify classes (1000 for ImageNet)\nclasses = 1000\n# Extrapolate batches to all devices\nbatch_size *= max(1, num_gpus)\n# Define context\ncontext = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n\nlr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')] + [np.inf]\n\nkwargs = {'classes': classes}\n\n# Define optimizer (nag = Nestrov Accelerated Gradient)\noptimizer = 'nag'\noptimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum}\n\n# Retireve gluon model\nnet = models[model_name](**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Download Pre-trained Face Detection Models\nDESCRIPTION: This code snippet downloads the pre-trained face detection models (MTCNN) from a specified URL. It downloads the params, symbol, caffemodel, and prototxt files for each of the four detectors. The downloaded files are stored in the 'mtcnn-model' directory.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(4):\n    mx.test_utils.download(dirname='mtcnn-model', url='https://s3.amazonaws.com/onnx-model-zoo/arcface/mtcnn-model/det{}-0001.params'.format(i+1))\n    mx.test_utils.download(dirname='mtcnn-model', url='https://s3.amazonaws.com/onnx-model-zoo/arcface/mtcnn-model/det{}-symbol.json'.format(i+1))\n    mx.test_utils.download(dirname='mtcnn-model', url='https://s3.amazonaws.com/onnx-model-zoo/arcface/mtcnn-model/det{}.caffemodel'.format(i+1))\n    mx.test_utils.download(dirname='mtcnn-model', url='https://s3.amazonaws.com/onnx-model-zoo/arcface/mtcnn-model/det{}.prototxt'.format(i+1))\n```\n\n----------------------------------------\n\nTITLE: GPT-2 Inference with onnxruntime-extensions in Python\nDESCRIPTION: This code snippet demonstrates how to perform inference with the GPT-2 model using onnxruntime-extensions. It loads the ONNX model, tokenizes the input text, and generates text using the model's built-in beam search algorithm. The tokenizer needs to be configured with padding enabled and a pad token set.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/gpt2-bs/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnxruntime_extensions import PyOrtFunction\n\ngpt2_all = PyOrtFunction.from_model('model/gpt2-lm-head-bs-12.onnx')\nencdict = tokenizer('What is the best story', padding=True, return_tensors='np')\n\noutputs = gpt2_all(encdict['input_ids'], encdict['attention_mask'].astype('float32'), 30)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n----------------------------------------\n\nTITLE: YOLOv3 Model Quantization (Bash)\nDESCRIPTION: This bash script executes the `run_tuning.sh` script with specified parameters to quantize a YOLOv3 model using Intel® Neural Compressor. It requires specifying the input model path, configuration file, data path, and output model path.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov3/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --config=yolov3.yaml \\\n                   --data_path=path/to/COCO2017 \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image Data - PyTorch Transforms\nDESCRIPTION: Defines preprocessing functions for training and validation images using PyTorch's `transforms`.  `preprocess_train_data` applies random resizing, cropping, flipping, color jittering, and lighting augmentation.  `preprocess_test_data` resizes, center crops, and normalizes images.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnormalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\njitter_param = 0.0\nlighting_param = 0.0\n\n# Input pre-processing for train data\ndef preprocess_train_data(normalize, jitter_param, lighting_param):\n    transform_train = transforms.Compose([\n        transforms.Resize(480),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomFlipLeftRight(),\n        transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                     saturation=jitter_param),\n        transforms.RandomLighting(lighting_param),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_train\n\n# Input pre-processing for validation data\ndef preprocess_test_data(normalize):\n    transform_test = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize\n    ])\n    return transform_test\n```\n\n----------------------------------------\n\nTITLE: Configure Data Loader - Python\nDESCRIPTION: This snippet configures the CityLoader class (inherited from mx.io.DataIter) to load and preprocess the validation data, setting parameters such as data and label paths, RGB mean values, batch size, and data/label shapes. This defines how the data is loaded and prepared for input to the DUC model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nloader = CityLoader\nval_args = {\n    'data_path'             : data_dir,\n    'label_path'            : label_dir,\n    'rgb_mean'              : (122.675, 116.669, 104.008),\n    'batch_size'            : batch_size,\n    'scale_factors'         : [1],\n    'data_name'             : 'data',\n    'label_name'            : 'seg_loss_label',\n    'data_shape'            : [tuple(list([batch_size, 3, 800, 800]))],\n    'label_shape'           : [tuple([batch_size, (160000)])],\n    'use_random_crop'       : False,\n    'use_mirror'            : False,\n    'ds_rate'               : 8,\n    'convert_label'         : True,\n    'multi_thread'          : False,\n    'cell_width'            : 2,\n    'random_bound'          : [120,120],\n}\nval_dataloader = loader('val.lst', val_args)\n```\n\n----------------------------------------\n\nTITLE: BottleneckV1 Definition in Gluon\nDESCRIPTION: Defines the BottleneckV1, used in ResNetV1 for 50, 101, and 152 layer architectures. It consists of a 1x1 convolution, a 3x3 convolution, and another 1x1 convolution, with batch normalization and ReLU activation after each convolutional layer. An optional downsampling layer is used when stride > 1.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BottleneckV1(HybridBlock):\n    r\"\"\"Bottleneck V1 from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    This is used for ResNet V1 for 50, 101, 152 layers.\n    Parameters\n    ----------\n    channels : int\n        Number of output channels.\n    stride : int\n        Stride size.\n    downsample : bool, default False\n        Whether to downsample the input.\n    in_channels : int, default 0\n        Number of input channels. Default is 0, to infer from the graph.\n    \"\"\"\n    def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs):\n        super(BottleneckV1, self).__init__(**kwargs)\n        self.body = nn.HybridSequential(prefix='')\n        self.body.add(nn.Conv2D(channels//4, kernel_size=1, strides=stride))\n        self.body.add(nn.BatchNorm())\n        self.body.add(nn.Activation('relu'))\n        self.body.add(_conv3x3(channels//4, 1, channels//4))\n        self.body.add(nn.BatchNorm())\n        self.body.add(nn.Activation('relu'))\n        self.body.add(nn.Conv2D(channels, kernel_size=1, strides=1))\n        self.body.add(nn.BatchNorm())\n        if downsample:\n            self.downsample = nn.HybridSequential(prefix='')\n            self.downsample.add(nn.Conv2D(channels, kernel_size=1, strides=stride,\n                                          use_bias=False, in_channels=in_channels))\n            self.downsample.add(nn.BatchNorm())\n        else:\n            self.downsample = None\n\n    def hybrid_forward(self, F, x):\n        residual = x\n\n        x = self.body(x)\n\n        if self.downsample:\n            residual = self.downsample(residual)\n\n        x = F.Activation(x + residual, act_type='relu')\n        return x\n```\n\n----------------------------------------\n\nTITLE: Define Input Shape\nDESCRIPTION: Defines the input tensor shape for the Inception v2 model. The input tensor is a float array with dimensions [1, 3, 224, 224], representing a single image with 3 color channels and a size of 224x224 pixels.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/inception_v2/README.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ndata_0: float[1, 3, 224, 224]\n```\n\n----------------------------------------\n\nTITLE: Image Pre-processing Function for GoogleNet in Python\nDESCRIPTION: This Python function preprocesses images for inference with GoogleNet. It resizes the image to 224x224, converts it to a NumPy array, subtracts mean values for each color channel (B, G, R), rearranges the color channels to BGR order, and expands the dimensions to create a batch of size 1.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/googlenet/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_image(path):\n'''\n    Using path to image, return the RGB load image\n'''\n    img = imageio.imread(path, pilmode='RGB')\n    return img\n\n# Pre-processing function for ImageNet models using numpy\ndef preprocess(img):\n    '''\n    Preprocessing required on the images for inference with mxnet gluon\n    The function takes loaded image and returns processed tensor\n    '''\n    img = np.array(Image.fromarray(img).resize((224, 224))).astype(np.float32)\n    img[:, :, 0] -= 123.68\n    img[:, :, 1] -= 116.779\n    img[:, :, 2] -= 103.939\n    img[:,:,[0,1,2]] = img[:,:,[2,1,0]]\n    img = img.transpose((2, 0, 1))\n    img = np.expand_dims(img, axis=0)\n\n    return img\n```\n\n----------------------------------------\n\nTITLE: BottleneckV2 Definition in Gluon\nDESCRIPTION: Defines the BottleneckV2, used in ResNetV2 for 50, 101, and 152 layer architectures. It uses batch normalization and ReLU before each convolutional layer.  An optional downsampling layer is used when stride > 1.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BottleneckV2(HybridBlock):\n    r\"\"\"Bottleneck V2 from\n    `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    This is used for ResNet V2 for 50, 101, 152 layers.\n    Parameters\n    ----------\n    channels : int\n        Number of output channels.\n    stride : int\n        Stride size.\n    downsample : bool, default False\n        Whether to downsample the input.\n    in_channels : int, default 0\n        Number of input channels. Default is 0, to infer from the graph.\n    \"\"\"\n    def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs):\n        super(BottleneckV2, self).__init__(**kwargs)\n        self.bn1 = nn.BatchNorm()\n        self.conv1 = nn.Conv2D(channels//4, kernel_size=1, strides=1, use_bias=False)\n        self.bn2 = nn.BatchNorm()\n        self.conv2 = _conv3x3(channels//4, stride, channels//4)\n        self.bn3 = nn.BatchNorm()\n        self.conv3 = nn.Conv2D(channels, kernel_size=1, strides=1, use_bias=False)\n        if downsample:\n            self.downsample = nn.Conv2D(channels, 1, stride, use_bias=False,\n                                        in_channels=in_channels)\n        else:\n            self.downsample = None\n\n    def hybrid_forward(self, F, x):\n        residual = x\n        x = self.bn1(x)\n        x = F.Activation(x, act_type='relu')\n        if self.downsample:\n            residual = self.downsample(x)\n        x = self.conv1(x)\n\n        x = self.bn2(x)\n        x = F.Activation(x, act_type='relu')\n        x = self.conv2(x)\n\n        x = self.bn3(x)\n        x = F.Activation(x, act_type='relu')\n        x = self.conv3(x)\n\n        return x + residual\n```\n\n----------------------------------------\n\nTITLE: Define Test Function - MXNet\nDESCRIPTION: This code defines a test function to evaluate the model's performance on the validation dataset. It resets the accuracy metrics, loads validation data in batches, performs a forward pass, updates the metrics, and returns the top-1 and top-5 errors.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Test function\ndef test(ctx, val_data):\n    # Reset accuracy metrics\n    acc_top1.reset()\n    acc_top5.reset()\n    for i, batch in enumerate(val_data):\n        # Load validation batch\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n        # Perform forward pass\n        outputs = [net(X) for X in data]\n        # Update accuracy metrics\n        acc_top1.update(label, outputs)\n        acc_top5.update(label, outputs)\n    # Retrieve and return top1 and top5 errors\n    _, top1 = acc_top1.get()\n    _, top5 = acc_top5.get()\n    return (1-top1, 1-top5)\n```\n\n----------------------------------------\n\nTITLE: Run Model Training in MXNet\nDESCRIPTION: The `main` function calls the `train_net` function with the specified training parameters. This snippet demonstrates how to initiate the training process by providing the necessary configuration to the `train_net` function.  The `if __name__ == '__main__'` block ensures that the training process starts only when the script is executed directly.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    train_net(data_dir,prefix,pretrained,ckpt,verbose,max_steps,end_epoch,lr,lr_steps,wd,fc7_wd_mult,\n              mom,emb_size,per_batch_size,margin_m,margin_s,target,beta,beta_min,beta_freeze,gamma,power,scale)\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Recursive List/Tuple Processing\nDESCRIPTION: These functions, `f` and `g`, are used to recursively process nested lists and tuples. `f` applies a function to each element, while `g` flattens the nested structure into a single list.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/style_transfer/fast_neural_style/dependencies/conversion.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import numpy_helper\n\ndef f(t):\n    return [f(i) for i in t] if isinstance(t, (list, tuple)) else t\n\ndef g(t, res):\n    for i in t:\n        res.append(i) if not isinstance(i, (list, tuple)) else g(i, res)\n    return res\n```\n\n----------------------------------------\n\nTITLE: Filtering and Scaling YOLOv4 Boxes (Python)\nDESCRIPTION: Filters bounding boxes based on score thresholds and scales their coordinates to match the original image size.  The function converts the bounding box format from (x, y, w, h) to (xmin, ymin, xmax, ymax), clips boxes that are out of range, and discards boxes with low detection probabilities.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef postprocess_boxes(pred_bbox, org_img_shape, input_size, score_threshold):\n    '''remove boundary boxs with a low detection probability'''\n    valid_scale=[0, np.inf]\n    pred_bbox = np.array(pred_bbox)\n\n    pred_xywh = pred_bbox[:, 0:4]\n    pred_conf = pred_bbox[:, 4]\n    pred_prob = pred_bbox[:, 5:]\n\n    # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n    # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n    org_h, org_w = org_img_shape\n    resize_ratio = min(input_size / org_w, input_size / org_h)\n\n    dw = (input_size - resize_ratio * org_w) / 2\n    dh = (input_size - resize_ratio * org_h) / 2\n\n    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n\n    # (3) clip some boxes that are out of range\n    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n    pred_coor[invalid_mask] = 0\n\n    # (4) discard some invalid boxes\n    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n\n    # (5) discard some boxes with low scores\n    classes = np.argmax(pred_prob, axis=-1)\n    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n    score_mask = scores > score_threshold\n    mask = np.logical_and(scale_mask, score_mask)\n    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n\n    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n```\n\n----------------------------------------\n\nTITLE: Prepare Dataset List - Python\nDESCRIPTION: This snippet prepares a validation images list (val.lst) containing image and label paths along with cropping metrics, creating this file for use with the data loader. It scans the specified data and label directories, pairs images with their corresponding labels, and constructs a list of image-label pairs with associated cropping information.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nindex = 0\nval_lst = []\n# images\nall_images = glob.glob(os.path.join(data_dir, '*/*.png'))\nall_images.sort()\nfor p in all_images:\n    l = p.replace(data_dir, label_dir).replace('leftImg8bit', 'gtFine_labelIds')\n    if os.path.isfile(l):\n        index += 1\n        for i in range(1, 8):\n            val_lst.append([str(index), p, l, \"512\", str(256 * i)])\n\nval_out = open('val.lst', \"w\")\nfor line in val_lst:\n    print('\\t'.join(line),file=val_out)\n```\n\n----------------------------------------\n\nTITLE: Define Data Preprocessing (Python)\nDESCRIPTION: This snippet configures the preprocessing transformations for both training and validation datasets. It normalizes images and defines jitter and lighting parameters for data augmentation during training.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnormalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\njitter_param = 0.4\nlighting_param = 0.1\n\n```\n\n----------------------------------------\n\nTITLE: Downloading MobileNet ONNX Model (Shell)\nDESCRIPTION: This shell command downloads a pre-trained MobileNet v2 ONNX model from the ONNX model zoo. It uses `wget` to retrieve the model file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/mobilenet/model/mobilenetv2-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Manual T5 Generative Model Creation and Inference (Python)\nDESCRIPTION: This snippet shows how to manually create a generative T5 model using ONNX Runtime and the `transformers` library.  It initializes the tokenizer and ONNX Runtime inference sessions for the encoder and decoder. It then uses a custom `GenerativeT5` class to perform inference. Requires `onnxruntime` and `transformers` packages.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/t5/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnxruntime import InferenceSession\nfrom transformers import T5Tokenizer\nfrom .dependencies.models import GenerativeT5\n\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\n# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n# based on the build flags) when instantiating InferenceSession.\n# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n# InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\ndecoder_sess = InferenceSession(str(path_t5_decoder))\nencoder_sess = InferenceSession(str(path_t5_encoder))\ngenerative_t5 = GenerativeT5(encoder_sess, decoder_sess, tokenizer, onnx=True)\ngenerative_t5('translate English to French: I was a victim of a series of accidents.', 21, temperature=0.)[0]\n```\n\n----------------------------------------\n\nTITLE: Define Training Function with MXNet\nDESCRIPTION: Defines the `train_net` function for training a neural network using MXNet. It sets up the context (CPU or GPU), data iterators, optimizer, and evaluation metrics. The function trains the model, logs the training progress, saves periodic checkpoints, and computes and displays validation accuracies periodically.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef train_net(data_dir,prefix,pretrained,ckpt,verbose,max_steps,end_epoch,lr,lr_steps,wd,fc7_wd_mult,\n              mom,emb_size,per_batch_size,margin_m,margin_s,target,beta,beta_min,beta_freeze,gamma,power,scale):\n    # define context\n    ctx = []\n    num_gpus = max(mx.test_utils.list_gpus()) + 1\n    if num_gpus>0:\n        for i in range(num_gpus):\n            ctx.append(mx.gpu(i))\n    if len(ctx)==0:\n        ctx = [mx.cpu()]\n        print('use cpu')\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    prefix_dir = os.path.dirname(prefix)\n    if not os.path.exists(prefix_dir):\n        os.makedirs(prefix_dir)\n    ctx_num = len(ctx)\n    num_layers = 100\n    print('num_layers',num_layers)\n    batch_size = per_batch_size*ctx_num\n    rescale_threshold = 0\n    image_channel = 3\n\n    os.environ['BETA'] = str(beta)\n    data_dir_list = data_dir.split(',')\n    assert len(data_dir_list)==1\n    data_dir = data_dir_list[0]\n    path_imgrec = None\n    path_imglist = None\n    prop = load_property(data_dir)\n    num_classes = prop.num_classes\n    image_size = prop.image_size\n    image_h = image_size[0]\n    image_w = image_size[1]\n    print('image_size', image_size)\n    assert(num_classes>0)\n    print('num_classes', num_classes)\n    path_imgrec = os.path.join(data_dir, \"train.rec\")\n\n    data_shape = (image_channel,image_size[0],image_size[1])\n    mean = None\n\n    begin_epoch = 0\n    base_lr = lr\n    base_wd = wd\n    base_mom = mom\n    if len(pretrained)==0:\n        arg_params = None\n        aux_params = None\n        sym, arg_params, aux_params = get_symbol(arg_params, aux_params, image_channel, image_h, image_w, \n                                                 num_layers, num_classes, data_dir,prefix,pretrained,ckpt,\n                                                 verbose,max_steps,end_epoch,lr,lr_steps,wd,fc7_wd_mult,\n                                                 mom,emb_size,per_batch_size,margin_m,margin_s,target,beta,\n                                                 beta_min,beta_freeze,gamma,power,scale)\n    else:\n        vec = pretrained.split(',')\n        print('loading', vec)\n        _, arg_params, aux_params = mx.model.load_checkpoint(vec[0], int(vec[1]))\n        sym, arg_params, aux_params = get_symbol(arg_params, aux_params)\n\n\n    model = mx.mod.Module(\n        context       = ctx,\n        symbol        = sym,\n    )\n    val_dataiter = None\n\n    train_dataiter = FaceImageIter(\n        batch_size           = batch_size,\n        data_shape           = data_shape,\n        path_imgrec          = path_imgrec,\n        shuffle              = True,\n        rand_mirror          = 1,\n        mean                 = mean,\n        cutoff               = 0,\n    )\n\n    _metric = AccMetric()\n    eval_metrics = [mx.metric.create(_metric)]\n\n    initializer = mx.init.Xavier(rnd_type='gaussian', factor_type=\"out\", magnitude=2) #resnet style\n    _rescale = 1.0/ctx_num\n    opt = optimizer.SGD(learning_rate=base_lr, momentum=base_mom, wd=base_wd, rescale_grad=_rescale)\n    som = 20\n    _cb = mx.callback.Speedometer(batch_size, som)\n\n    ver_list = []\n    ver_name_list = []\n    for name in target.split(','):\n        path = os.path.join(data_dir,name+\".bin\")\n        if os.path.exists(path):\n            data_set = verification.load_bin(path, image_size)\n            ver_list.append(data_set)\n            ver_name_list.append(name)\n            print('ver', name)\n\n\n\n    def ver_test(nbatch):\n        results = []\n        for i in xrange(len(ver_list)):\n            acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, batch_size, 10, None, None)\n            print('[%s][%d]XNorm: %f' % (ver_name_list[i], nbatch, xnorm))\n            print('[%s][%d]Accuracy-Flip: %1.5f+-%1.5f' % (ver_name_list[i], nbatch, acc2, std2))\n            results.append(acc2)\n        return results\n\n\n\n    highest_acc = [0.0, 0.0]  #lfw and target\n    global_step = [0]\n    save_step = [0]\n    \n    p = 512.0/batch_size\n    for l in xrange(len(lr_steps)):\n        lr_steps[l] = int(lr_steps[l]*p)\n    print('lr_steps', lr_steps)\n    def _batch_callback(param):\n        global_step[0]+=1\n        mbatch = global_step[0]\n        for _lr in lr_steps:\n            if mbatch==beta_freeze+_lr:\n                opt.lr *= 0.1\n                print('lr change to', opt.lr)\n                break\n\n        _cb(param)\n        if mbatch%1000==0:\n            print('lr-batch-epoch:',opt.lr,param.nbatch,param.epoch)\n\n        if mbatch>=0 and mbatch%verbose==0:\n            acc_list = ver_test(mbatch)\n            save_step[0]+=1\n            msave = save_step[0]\n            do_save = False\n            if len(acc_list)>0:\n                lfw_score = acc_list[0]\n                if lfw_score>highest_acc[0]:\n                    highest_acc[0] = lfw_score\n                    if lfw_score>=0.998:\n                        do_save = True\n                if acc_list[-1]>=highest_acc[-1]:\n                    highest_acc[-1] = acc_list[-1]\n                    if lfw_score>=0.99:\n                        do_save = True\n            if ckpt==0:\n                do_save = False\n            elif ckpt>1:\n                do_save = True\n            if do_save:\n                print('saving', msave)\n                arg, aux = model.get_params()\n                mx.model.save_checkpoint(prefix, msave, model.symbol, arg, aux)\n            print('[%d]Accuracy-Highest: %1.5f'%(mbatch, highest_acc[-1]))\n        if mbatch<=beta_freeze:\n            _beta = beta\n        else:\n            move = max(0, mbatch-beta_freeze)\n            _beta = max(beta_min, beta*math.pow(1+gamma*move, -1.0*power))\n        os.environ['BETA'] = str(_beta)\n        if max_steps>0 and mbatch>max_steps:\n            sys.exit(0)\n\n    epoch_cb = None\n\n    model.fit(train_dataiter,\n        begin_epoch        = begin_epoch,\n        num_epoch          = end_epoch,\n        eval_data          = val_dataiter,\n        eval_metric        = eval_metrics,\n        kvstore            = 'device',\n        optimizer          = opt,\n        initializer        = initializer,\n        arg_params         = arg_params,\n        aux_params         = aux_params,\n        allow_missing      = True,\n        batch_end_callback = _batch_callback,\n        epoch_end_callback = epoch_cb )\n\n```\n\n----------------------------------------\n\nTITLE: Define Accuracy Metrics - MXNet/Gluon\nDESCRIPTION: Initializes top-1 and top-5 accuracy metrics using MXNet's `mx.metric.Accuracy` and `mx.metric.TopKAccuracy` classes. Also sets up a `TrainingHistory` object to log training and validation errors.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nacc_top1 = mx.metric.Accuracy()\nacc_top5 = mx.metric.TopKAccuracy(5)\ntrain_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n                                 'validation-top1-err', 'validation-top5-err'])\nmakedirs(save_dir)\n```\n\n----------------------------------------\n\nTITLE: Set Context, Paths, and Parameters - Python\nDESCRIPTION: This code snippet sets the context (CPU or GPU) for MXNet operations, defines the path to the ImageNet dataset, sets the batch size for processing images, specifies the number of workers for preprocessing, and defines the path to the ONNX model file.  The batch size is set to 128, but can be adjusted based on available resources.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Determine and set context\nif len(mx.test_utils.list_gpus())==0:\n    ctx = [mx.cpu()]\nelse:\n    ctx = [mx.gpu(0)]\n\n# path to imagenet dataset folder\ndata_dir = '/home/ubuntu/imagenet/img_dataset/'\n\n# batch size (set to 1 for cpu)\nbatch_size = 128\n\n# number of preprocessing workers\nnum_workers = multiprocessing.cpu_count()\n\n# path to ONNX model file\nmodel_path = 'squeezenet1.1.onnx'\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Image for YOLOv4\nDESCRIPTION: This snippet loads an image, preprocesses it using the `image_preprocess` function, and prints the shape of the preprocessed image. It reads an image file ('kite.jpg'), converts its color space, and then preprocesses the image to the required input size (416x416) for the YOLOv4 model. The resulting image data is reshaped to add a batch dimension and converted to a float32 numpy array.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninput_size = 416\n\noriginal_image = cv2.imread(\"kite.jpg\")\noriginal_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\noriginal_image_size = original_image.shape[:2]\n\nimage_data = image_preprocess(np.copy(original_image), [input_size, input_size])\nimage_data = image_data[np.newaxis, ...].astype(np.float32)\n\nprint(\"Preprocessed image shape:\",image_data.shape) # shape of the preprocessed input\n```\n\n----------------------------------------\n\nTITLE: Download ONNX Model (Shell)\nDESCRIPTION: This shell command downloads the Emotion FERPlus model in ONNX format from the ONNX Model Zoo using wget.  It retrieves the model file 'emotion-ferplus-8.onnx'.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/emotion_ferplus/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx\n```\n\n----------------------------------------\n\nTITLE: Generate Predictions\nDESCRIPTION: This snippet sets the image path and calls the `predict` function to generate and display the top 5 predictions for the given image.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Enter path to the inference image below\nimg_path = 'kitten.jpg'\npredict(img_path)\n```\n\n----------------------------------------\n\nTITLE: BasicBlockV1 Definition in Gluon\nDESCRIPTION: Defines the BasicBlockV1, used in ResNetV1 for 18 and 34 layer architectures. It consists of two 3x3 convolutional layers with batch normalization and ReLU activation. An optional downsampling layer is used for stride > 1.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BasicBlockV1(HybridBlock):\n    r\"\"\"BasicBlock V1 from `\"Deep Residual Learning for Image Recognition\"\n    <http://arxiv.org/abs/1512.03385>`_ paper.\n    This is used for ResNet V1 for 18, 34 layers.\n    Parameters\n    ----------\n    channels : int\n        Number of output channels.\n    stride : int\n        Stride size.\n    downsample : bool, default False\n        Whether to downsample the input.\n    in_channels : int, default 0\n        Number of input channels. Default is 0, to infer from the graph.\n    \"\"\"\n    def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs):\n        super(BasicBlockV1, self).__init__(**kwargs)\n        self.body = nn.HybridSequential(prefix='')\n        self.body.add(_conv3x3(channels, stride, in_channels))\n        self.body.add(nn.BatchNorm())\n        self.body.add(nn.Activation('relu'))\n        self.body.add(_conv3x3(channels, 1, channels))\n        self.body.add(nn.BatchNorm())\n        if downsample:\n            self.downsample = nn.HybridSequential(prefix='')\n            self.downsample.add(nn.Conv2D(channels, kernel_size=1, strides=stride,\n                                          use_bias=False, in_channels=in_channels))\n            self.downsample.add(nn.BatchNorm())\n        else:\n            self.downsample = None\n\n    def hybrid_forward(self, F, x):\n        residual = x\n\n        x = self.body(x)\n\n        if self.downsample:\n            residual = self.downsample(residual)\n\n        x = F.Activation(residual+x, act_type='relu')\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Download test image\nDESCRIPTION: This snippet downloads a test image (kitten.jpg) from a specified URL using mxnet's test utilities. This image will be used for inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmx.test_utils.download('https://s3.amazonaws.com/model-server/inputs/kitten.jpg')\n```\n\n----------------------------------------\n\nTITLE: Serializing NumPy Array to TensorProto\nDESCRIPTION: The `SaveTensorProto` function serializes a NumPy array to a TensorProto and saves it to a file. It uses the `onnx.numpy_helper` to create the TensorProto object.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/style_transfer/fast_neural_style/dependencies/conversion.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef SaveTensorProto(file_path, name, data):\n    tp = numpy_helper.from_array(data)\n    tp.name = name\n\n    with open(file_path, 'wb') as f:\n        f.write(tp.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for ResNet Training\nDESCRIPTION: This code snippet imports the necessary libraries for training ResNet models, including matplotlib, argparse, time, logging, mxnet, numpy, gluon, and gluoncv. It also imports modules for data transformation, image datasets, utility functions, and multiprocessing. The code is essential for setting up the environment and loading the required functions for training.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport argparse, time, logging\n\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, nd\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.data import imagenet\nfrom gluoncv.utils import makedirs, TrainingHistory\n\nimport os\nfrom mxnet.context import cpu\nfrom mxnet.gluon.block import HybridBlock\nfrom mxnet.gluon.contrib.nn import HybridConcurrent\nimport multiprocessing\n```\n\n----------------------------------------\n\nTITLE: Compute validation accuracies using ONNX and MXNet\nDESCRIPTION: This code snippet loads an ONNX model, sets the execution context (CPU or GPU), binds parameters to the model, and computes validation accuracies for specified verification targets. It iterates through the targets, loads data, computes the accuracy using the 'test' function, and prints the results including XNorm, accuracy, and standard deviation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_validation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == '__main__':\n    \n    # Load image size\n    prop = load_property(data_dir)\n    image_size = prop.image_size\n    print('image_size', image_size)\n    \n    # Determine and set context\n    if len(mx.test_utils.list_gpus())==0:\n        ctx = mx.cpu()\n        batch_size=1\n    else:\n        ctx = mx.gpu(0)\n        \n    time0 = datetime.datetime.now()\n    \n    # Import ONNX model\n    sym, arg_params, aux_params = import_model(model)\n    all_layers = sym.get_internals()\n    sym = all_layers['fc1_output']\n    # Define model\n    model = mx.mod.Module(symbol=sym, context=ctx, label_names = None)\n    # Bind parameters to the model\n    model.bind(data_shapes=[('data', (batch_size, 3, image_size[0], image_size[1]))])\n    model.set_params(arg_params, aux_params)\n    time_now = datetime.datetime.now()\n    diff = time_now - time0\n    print('model loading time', diff.total_seconds())\n\n    ver_list = []\n    ver_name_list = []\n    \n    # Iterate over verification targets\n    for name in target.split(','):\n        path = os.path.join(data_dir,name+\".bin\")\n        # Load data\n        if os.path.exists(path):\n            print('loading.. ', name)\n            data_set = load_bin(path, image_size)\n            ver_list.append(data_set)\n            ver_name_list.append(name)\n    \n    # Iterate over verification targets\n    for i in range(len(ver_list)):\n        # Compute and print validation accuracies\n        acc2, std2, xnorm, embeddings_list = test(ver_list[i], model, batch_size, nfolds)\n        print('[%s]XNorm: %f' % (ver_name_list[i], xnorm))\n        print('[%s]Accuracy-Flip: %1.5f+-%1.5f' % (ver_name_list[i], acc2, std2))\n```\n\n----------------------------------------\n\nTITLE: Display Auxiliary Segmentation Result in Python\nDESCRIPTION: This snippet displays the segmentation result from the auxiliary output using `matplotlib`. It prints the confidence score (`aux_conf`) and displays the segmented image (`aux_result`) as a NumPy array. The `%matplotlib inline` is assumed to be already set, as it's not repeated here.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(aux_conf)\nimshow(np.asarray(aux_result))\n```\n\n----------------------------------------\n\nTITLE: Slim Model for Inference in MXNet\nDESCRIPTION: This snippet slims a trained MXNet model by removing the last fully connected layer, which is typically not needed for inference. It loads the model, identifies the layers to remove (specifically those starting with 'fc7'), deletes the corresponding parameters, and saves the slimmed model. This reduces the model size and can improve inference speed.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Choose model to slim (give path to syms and params)\nprefix = '/home/ubuntu/resnet100'\nepoch = 1\n\n# Load model\nsym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n# Populate list containing nodes to be removed\nall_layers = sym.get_internals()\nsym = all_layers['fc1_output']\ndellist = []\nfor k,v in arg_params.iteritems():\n    if k.startswith('fc7'):\n        dellist.append(k)\n        \n# Remove nodes\nfor d in dellist:\n    del arg_params[d]\n\n# Save slimed model\nmx.model.save_checkpoint(prefix, 0, sym, arg_params, aux_params)\n```\n\n----------------------------------------\n\nTITLE: Specify Model Parameters Python\nDESCRIPTION: This code snippet defines the hyperparameters and paths required for training the ArcFace model.  It specifies the dataset path (`data_dir`), model saving path (`prefix`), pretrained model path (`pretrained`), checkpoint saving option (`ckpt`), and other training parameters such as learning rate (`lr`), weight decay (`wd`), momentum (`mom`), embedding size (`emb_size`), batch size (`per_batch_size`), and margin parameters (`margin_m`, `margin_s`).  These parameters are crucial for controlling the training process and achieving optimal performance.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Path to dataset\ndata_dir = '/home/ubuntu/faces_ms1m_112x112'\n# Path to directory where models will be saved\nprefix = '/home/ubuntu/resnet100'\n# Load pretrained model\npretrained = ''\n# Checkpoint saving option. 0: discard saving. 1: save when necessary. 2: always save\nckpt = 1\n# do verification testing and model saving every verbose batches\nverbose = 2000\n# max training batches\nmax_steps = 0\n# number of training epochs\nend_epoch = 30\n# initial learning rate\nlr = 0.1\n# learning rate decay iterations\nlr_steps = [100000, 140000, 160000]\n# weight decay\nwd = 0.0005\n# weight decay multiplier for fc7\nfc7_wd_mult = 1.0\n# momentum\nmom = 0.9\n# embedding length\nemb_size = 512\n# batch size in each context\nper_batch_size = 64\n# margin for loss\nmargin_m = 0.5\n# scale for feature\nmargin_s = 64.0\n# verification targets\ntarget = 'lfw,cfp_fp,agedb_30'\nbeta = 1000.0\nbeta_min = 5.0\nbeta_freeze = 0\ngamma = 0.12\npower = 1.0\nscale = 0.9993\n```\n\n----------------------------------------\n\nTITLE: Downloading Mask R-CNN Model using Shell\nDESCRIPTION: This shell command downloads the Mask R-CNN model file (MaskRCNN-12.onnx) from the onnx/models repository on GitHub using wget. This is a prerequisite for quantizing the model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/mask-rcnn/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Display Auxiliary Blended Image in Python\nDESCRIPTION: This snippet displays the blended image generated from the auxiliary output of the FCN model.  It uses `imshow` from matplotlib to show the `aux_blended` image, which combines the original input image with the colorized segmentation map derived from the auxiliary output.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimshow(np.asarray(aux_blended))\n```\n\n----------------------------------------\n\nTITLE: Quantize ONNX Model (Bash)\nDESCRIPTION: This bash script executes the model quantization using Intel Neural Compressor.  It navigates to the ptq_static directory within the neural-compressor examples and executes the run_tuning.sh script, specifying the input model path, dataset location, and output model path. Requires Intel Neural Compressor setup.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/ultraface/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd neural-compressor/examples/onnxrt/body_analysis/onnx_model_zoo/ultraface/quantization/ptq_static\nbash run_tuning.sh --input_model=path/to/model  \\ # model path as *.onnx\n                   --dataset_location=/path/to/data \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Downloading PyTorch Segmentation Model in Python\nDESCRIPTION: This snippet downloads either the FCN ResNet101 or FCN ResNet50 model from Torchvision's model zoo based on the value of the `DO_101` boolean variable. The downloaded model is then set to evaluation mode (`model.eval()`) and a custom `exporting` attribute is set to `True`.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDO_101 = True\n\nif DO_101:\n    model = models.segmentation.fcn_resnet101(pretrained=True)\nelse:\n    model = models.segmentation.fcn_resnet50(pretrained=True)\n\nmodel.eval()\nmodel.exporting = True\n```\n\n----------------------------------------\n\nTITLE: Quantize BiDAF Model (Dynamic)\nDESCRIPTION: This bash command executes a script (run_tuning.sh) to dynamically quantize the BiDAF model using Intel Neural Compressor. The script takes the input model path, dataset location, and output model path as arguments. The script is assumed to handle the quantization process.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bidirectional_attention_flow/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\# model path as *.onnx\n                   --dataset_location=path/to/squad/dev-v1.1.json\n                   --output_model=path/to/model_tune\n```\n\n----------------------------------------\n\nTITLE: Preprocess Images for Validation - Python\nDESCRIPTION: This code snippet defines image transformations and loads the ImageNet validation dataset. It normalizes the images, resizes them to 256x256, takes a center crop of 224x224, and converts them to tensors. The `gluon.data.DataLoader` loads the data in batches for efficient processing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define image transforms\nnormalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\ntransform_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Load and process input\nval_data = gluon.data.DataLoader(\n    imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image for Mask R-CNN with Python\nDESCRIPTION: This code snippet demonstrates how to preprocess an image for use with the Mask R-CNN model. It includes resizing, converting to BGR, transposing the dimensions, normalizing the color channels, and padding the image to be divisible by 32.  It requires the numpy and PIL libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/mask-rcnn/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nfrom PIL import Image\n\ndef preprocess(image):\n    # Resize\n    ratio = 800.0 / min(image.size[0], image.size[1])\n    image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])), Image.BILINEAR)\n\n    # Convert to BGR\n    image = np.array(image)[:, :, [2, 1, 0]].astype('float32')\n\n    # HWC -> CHW\n    image = np.transpose(image, [2, 0, 1])\n\n    # Normalize\n    mean_vec = np.array([102.9801, 115.9465, 122.7717])\n    for i in range(image.shape[0]):\n        image[i, :, :] = image[i, :, :] - mean_vec[i]\n\n    # Pad to be divisible of 32\n    import math\n    padded_h = int(math.ceil(image.shape[1] / 32) * 32)\n    padded_w = int(math.ceil(image.shape[2] / 32) * 32)\n\n    padded_image = np.zeros((3, padded_h, padded_w), dtype=np.float32)\n    padded_image[:, :image.shape[1], :image.shape[2]] = image\n    image = padded_image\n\n    return image\n\nimg = Image.open('dependencies/demo.jpg')\nimg_data = preprocess(img)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Input for BERT-Squad\nDESCRIPTION: This code snippet preprocesses input data for the BERT-Squad model. It reads input examples from a JSON file, tokenizes the input text, and converts the tokens into numerical features suitable for the model. Key parameters such as max_seq_length, doc_stride, and max_query_length are defined.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# preprocess input\npredict_file = 'inputs.json'\n\n# Use read_squad_examples method from run_onnx_squad to read the input file\neval_examples = read_squad_examples(input_file=predict_file)\n\nmax_seq_length = 256\ndoc_stride = 128\nmax_query_length = 64\nbatch_size = 1\nn_best_size = 20\nmax_answer_length = 30\n\nvocab_file = os.path.join('uncased_L-12_H-768_A-12', 'vocab.txt')\ntokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n\n# Use convert_examples_to_features method from run_onnx_squad to get parameters from the input\ninput_ids, input_mask, segment_ids, extra_data = convert_examples_to_features(eval_examples, tokenizer,\n                                                                              max_seq_length, doc_stride, max_query_length)\n```\n\n----------------------------------------\n\nTITLE: Import ONNX model\nDESCRIPTION: This snippet imports an ONNX model into MXNet.  It uses the `import_model` function from `mxnet.contrib.onnx.onnx2mx` to convert the ONNX model to MXNet symbols and parameters. Requires the path to the ONNX model file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Enter path to the ONNX model file\nmodel_path= 'squeezenet1.1.onnx'\nsym, arg_params, aux_params = import_model(model_path)\n```\n\n----------------------------------------\n\nTITLE: Preprocess Second Image and Get Embedding\nDESCRIPTION: This code preprocesses the second image using the `get_input` function with the MTCNN detector, then displays the preprocessed image, and finally computes the feature embedding using the `get_feature` function with the loaded MXNet model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Preprocess second image\npre2 = get_input(detector,img2)\n# Display preprocessed image\nplt.imshow(np.transpose(pre2,(1,2,0)))\nplt.show()\n# Get embedding of second image\nout2 = get_feature(model,pre2)\n```\n\n----------------------------------------\n\nTITLE: Quantizing MobileNet with Intel Neural Compressor (Bash)\nDESCRIPTION: This bash script quantizes a MobileNet model using Intel Neural Compressor. It requires specifying the input model path, a configuration file (mobilenetv2.yaml), and the output model path.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=mobilenetv2.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Printing Distance and Similarity in Python\nDESCRIPTION: This snippet prints the calculated distance and similarity values. It assumes that the variables 'dist' and 'sim' are already defined and contain the numerical results of a computation, likely from an ONNX model's prediction.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nprint('Distance = %f' %(dist))\nprint('Similarity = %f' %(sim))\n```\n\n----------------------------------------\n\nTITLE: Define Training Helpers (Python)\nDESCRIPTION: This section initializes logging, extrapolates the batch size based on the number of GPUs, defines the execution context (CPU or GPU), sets the learning rate decay schedule, and initializes the chosen VGG model from the models dictionary. It also sets up accuracy metrics and training history.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Specify logging function\nlogging.basicConfig(level=logging.INFO)\n\n# Specify classes (1000 for ImageNet)\nclasses = 1000\n# Extrapolate batches to all devices\nbatch_size *= max(1, num_gpus)\n# Define context\ncontext = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n\nlr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')] + [np.inf]\n\nkwargs = {'classes': classes, 'batch_norm': False}\n\n# Define optimizer (nag = Nestrov Accelerated Gradient)\noptimizer = 'nag'\noptimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum}\n\n# Retrieve gluon model\nnet = models[model_name](**kwargs)\n\n# Define accuracy measures - top1 error and top5 error\nacc_top1 = mx.metric.Accuracy()\nacc_top5 = mx.metric.TopKAccuracy(5)\ntrain_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n                                 'validation-top1-err', 'validation-top5-err'])\n\nmakedirs(save_dir)\n\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Image (Python)\nDESCRIPTION: This snippet postprocesses the output image from ONNX Runtime. It converts the output Y channel back to an image, merges it with the Cb and Cr channels (resized to match the output image size), and converts the final image to RGB format for display using Matplotlib.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/super_resolution/sub_pixel_cnn_2016/dependencies/Run_Super_Resolution_Model.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimg_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n# get the output image follow post-processing step from PyTorch implementation\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\nplt.imshow(final_img)\n```\n\n----------------------------------------\n\nTITLE: Download FCN ResNet50 Model (Shell)\nDESCRIPTION: This shell command downloads a pre-trained FCN ResNet50 model from the ONNX model zoo. The model is used for image segmentation tasks.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/fcn/model/fcn-resnet50-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies Python\nDESCRIPTION: This code snippet imports the necessary Python libraries and modules for training the ArcFace model. It includes modules for numerical computation (numpy), image processing (OpenCV, scikit-image), data manipulation (EasyDict), deep learning framework (MXNet), model definition (fresnet), and evaluation (verification). The `image_iter` module is used for efficient data loading and augmentation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport math\nimport numpy as np\nimport random\nimport logging\nimport pickle\nimport numpy as np\nfrom image_iter import FaceImageIter\nfrom image_iter import FaceImageIterList\nimport mxnet as mx\nfrom mxnet import ndarray as nd\nimport mxnet.optimizer as optimizer\nimport fresnet\nimport verification\nimport sklearn\nfrom easydict import EasyDict as edict\nimport multiprocessing\n```\n\n----------------------------------------\n\nTITLE: Helper Code for SqueezeNet Training - Python\nDESCRIPTION: This code snippet defines helper functions and configurations for training the SqueezeNet model. It specifies logging, sets the number of classes, extrapolates batch sizes, defines the execution context (CPU or GPU), and sets up the optimizer. It retrieves the Gluon model based on the specified `model_name` and prepares it for training.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Specify logging fucntion\nlogging.basicConfig(level=logging.INFO)\n\n# Specify classes (1000 for ImageNet)\nclasses = 1000\n# Extrapolate batches to all devices\nbatch_size *= max(1, num_gpus)\n# Define context\ncontext = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n\nlr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')] + [np.inf]\n\nkwargs = {'classes': classes}\n\n# Define optimizer (nag = Nestrov Accelerated Gradient)\noptimizer = 'nag'\noptimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum}\n\n# Retrieve gluon model\nif model_name == 'squeezenet1.0':\n    net = squeezenet1_0(**kwargs)\nelse:\n    net = squeezenet1_1(**kwargs)\n```\n\n----------------------------------------\n\nTITLE: Test Model Accuracy in Python\nDESCRIPTION: The `test()` function takes a validation set (`data_set`) and an MXNet model (`mx_model`) as input, and uses `evaluate()` to compute accuracies using n-folds cross-validation.  It performs inference on the data, normalizes the embeddings, and returns the accuracy, standard deviation, and other metrics.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_validation.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef test(data_set, mx_model, batch_size, nfolds=10, data_extra = None, label_shape = None):\n    print('testing verification..')\n    data_list = data_set[0]\n    issame_list = data_set[1]\n    model = mx_model\n    embeddings_list = []\n    if data_extra is not None:\n        _data_extra = nd.array(data_extra)\n    time_consumed = 0.0\n    if label_shape is None:\n        _label = nd.ones( (batch_size,) )\n    else:\n        _label = nd.ones( label_shape )\n    for i in range( len(data_list) ):\n        data = data_list[i]\n        embeddings = None\n        ba = 0\n        while ba<data.shape[0]:\n            bb = min(ba+batch_size, data.shape[0])\n            count = bb-ba\n            _data = nd.slice_axis(data, axis=0, begin=bb-batch_size, end=bb)\n            time0 = datetime.datetime.now()\n            if data_extra is None:\n                db = mx.io.DataBatch(data=(_data,), label=(_label,))\n            else:\n                db = mx.io.DataBatch(data=(_data,_data_extra), label=(_label,))\n            model.forward(db, is_train=False)\n            net_out = model.get_outputs()\n            _embeddings = net_out[0].asnumpy()\n            time_now = datetime.datetime.now()\n            diff = time_now - time0\n            time_consumed+=diff.total_seconds()\n            if embeddings is None:\n                embeddings = np.zeros( (data.shape[0], _embeddings.shape[1]) )\n            embeddings[ba:bb,:] = _embeddings[(batch_size-count):,:]\n            ba = bb\n        embeddings_list.append(embeddings)\n\n    _xnorm = 0.0\n    _xnorm_cnt = 0\n    for embed in embeddings_list:\n        for i in range(embed.shape[0]):\n            _em = embed[i]\n            _norm=np.linalg.norm(_em)\n            _xnorm+=_norm\n            _xnorm_cnt+=1\n    _xnorm /= _xnorm_cnt\n\n    embeddings = embeddings_list[0].copy()\n    embeddings = sklearn.preprocessing.normalize(embeddings)\n    embeddings = embeddings_list[0] + embeddings_list[1]\n    embeddings = sklearn.preprocessing.normalize(embeddings)\n    print(embeddings.shape)\n    print('infer time', time_consumed)\n    accuracy = evaluate(embeddings, issame_list, nrof_folds=nfolds)\n    acc2, std2 = np.mean(accuracy), np.std(accuracy)\n    return acc2, std2, _xnorm, embeddings_list\n```\n\n----------------------------------------\n\nTITLE: Download MNIST Model - Shell\nDESCRIPTION: This shell command downloads the MNIST-12 ONNX model from the ONNX model repository. It uses wget to retrieve the model file from the specified URL. Prerequisite: wget needs to be installed.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mnist/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies - Python\nDESCRIPTION: This code snippet imports necessary libraries for training the MobileNetV2 model, including mxnet, numpy, gluoncv, and matplotlib. It also imports modules for argument parsing, time tracking, logging, data transformation, and multiprocessing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport argparse, time, logging\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, nd\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.data import imagenet\nfrom gluoncv.utils import makedirs, TrainingHistory\n\nimport os\nfrom mxnet.context import cpu\nfrom mxnet.gluon.block import HybridBlock\nfrom mxnet.gluon.contrib.nn import HybridConcurrent\nimport multiprocessing\n```\n\n----------------------------------------\n\nTITLE: Writing Input JSON File for BERT-Squad\nDESCRIPTION: This snippet uses the `%%writefile` magic command to create an `inputs.json` file. The file contains a JSON structure representing a question answering dataset with context and question-answer pairs for the BERT-Squad model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/BERT-Squad.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%writefile inputs.json\n{\n  \"version\": \"1.4\",\n  \"data\": [\n    {\n      \"paragraphs\": [\n        {\n          \"context\": \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\",\n          \"qas\": [\n            {\n              \"question\": \"where is the businesses choosing to go?\",\n              \"id\": \"1\"\n            },\n            {\n              \"question\": \"how may votes did the ballot measure need?\",\n              \"id\": \"2\"\n            },\n            {\n              \"question\": \"By what year many Silicon Valley businesses were choosing the Moscone Center?\",\n              \"id\": \"3\"\n            }\n          ]\n        }\n      ],\n      \"title\": \"Conference Center\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Load and Display Second Image\nDESCRIPTION: This code loads the second image ('player2.jpg') using OpenCV and displays it using Matplotlib after converting its color space from BGR to RGB.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Load second image\nimg2 = cv2.imread('player2.jpg')\n# Display second image\nplt.imshow(cv2.cvtColor(img2,cv2.COLOR_BGR2RGB))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Download ONNX Model using wget\nDESCRIPTION: This shell command downloads the ShuffleNet-v2-12.onnx model from the ONNX model zoo using wget.  This is the first step in preparing the model for quantization or inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/shufflenet/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/shufflenet/model/shufflenet-v2-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Display Blended Image in Python\nDESCRIPTION: This snippet displays the blended image, which is the original image overlaid with the segmentation map. It uses `imshow` from `matplotlib` to render the blended image (blended_img), providing a visual representation of the segmentation result in the context of the original scene.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimshow(np.asarray(blended_img))\n```\n\n----------------------------------------\n\nTITLE: Quantize Inception v1 Model (Bash)\nDESCRIPTION: Quantizes the Inception v1 model using a bash script and the Intel Neural Compressor.  It requires specifying the model path, configuration file, data path, label path, and output model path as parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/inception_v1/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=inception_v1.yaml \\\n                   --data_path=/path/to/imagenet \\\n                   --label_path=/path/to/imagenet/label \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: BasicBlockV2 Definition in Gluon\nDESCRIPTION: Defines the BasicBlockV2, used in ResNetV2 for 18 and 34 layer architectures. It uses batch normalization and ReLU before the convolutional layers.  An optional downsampling layer is used when stride > 1.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass BasicBlockV2(HybridBlock):\n    r\"\"\"BasicBlock V2 from\n    `\"Identity Mappings in Deep Residual Networks\"\n    <https://arxiv.org/abs/1603.05027>`_ paper.\n    This is used for ResNet V2 for 18, 34 layers.\n    Parameters\n    ----------\n    channels : int\n        Number of output channels.\n    stride : int\n        Stride size.\n    downsample : bool, default False\n        Whether to downsample the input.\n    in_channels : int, default 0\n        Number of input channels. Default is 0, to infer from the graph.\n    \"\"\"\n    def __init__(self, channels, stride, downsample=False, in_channels=0, **kwargs):\n        super(BasicBlockV2, self).__init__(**kwargs)\n        self.bn1 = nn.BatchNorm()\n        self.conv1 = _conv3x3(channels, stride, in_channels)\n        self.bn2 = nn.BatchNorm()\n        self.conv2 = _conv3x3(channels, 1, channels)\n        if downsample:\n            self.downsample = nn.Conv2D(channels, 1, stride, use_bias=False,\n                                        in_channels=in_channels)\n        else:\n            self.downsample = None\n\n    def hybrid_forward(self, F, x):\n        residual = x\n        x = self.bn1(x)\n        x = F.Activation(x, act_type='relu')\n        if self.downsample:\n            residual = self.downsample(x)\n        x = self.conv1(x)\n\n        x = self.bn2(x)\n        x = F.Activation(x, act_type='relu')\n        x = self.conv2(x)\n\n        return x + residual\n```\n\n----------------------------------------\n\nTITLE: Main Function - Model Execution\nDESCRIPTION: Defines a `main` function to hybridize the network and start the training process by calling the `train` function with the specified number of epochs and context. The `if __name__ == '__main__':` block ensures that the `main` function is executed when the script is run.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mobilenet/train_mobilenet.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    net.hybridize()\n    train(num_epochs, context)\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for SqueezeNet Training - Python\nDESCRIPTION: This snippet imports necessary libraries for training a SqueezeNet model using MXNet. It includes modules for data handling, model definition, optimization, and visualization. Ensure MXNet, GluonCV, NumPy, and Matplotlib are installed before running this code.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport time, logging\n\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, nd\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.data import imagenet\nfrom gluoncv.utils import makedirs, TrainingHistory\n\nimport os\nfrom mxnet.context import cpu\nfrom mxnet.gluon.block import HybridBlock\nfrom mxnet.gluon.contrib.nn import HybridConcurrent\nimport multiprocessing\n```\n\n----------------------------------------\n\nTITLE: Downloading the ONNX Model using wget\nDESCRIPTION: This snippet uses the wget command to download a pre-trained ONNX model for the DUC (Dense Upsampling Convolution) architecture.  The model is downloaded from the ONNX model zoo on GitHub. This step is a prerequisite for further tasks such as quantization or inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/duc/model/ResNet101-DUC-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX Opset Version\nDESCRIPTION: This Python code converts the opset version of the downloaded BiDAF model to version 11. This conversion is necessary to enable more quantization capabilities within ONNX.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bidirectional_attention_flow/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import version_converter\n\nmodel = onnx.load('bidaf-9.onnx')\nmodel = version_converter.convert_version(model, 11)\nonnx.save_model(model, 'bidaf-11.onnx')\n```\n\n----------------------------------------\n\nTITLE: Print Results - Python\nDESCRIPTION: This snippet prints the mean Intersection Over Union (mIOU) calculated during the evaluation process. The metric provides a quantitative measure of the model's segmentation accuracy on the Cityscapes validation dataset.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"mean Intersection Over Union (mIOU): {}\".format(metric.get()[1]))\n```\n\n----------------------------------------\n\nTITLE: Download Input Images and ONNX Model\nDESCRIPTION: This code downloads the input images (player1.jpg and player2.jpg) and the ONNX model (resnet100.onnx) from the specified URLs using the mx.test_utils.download function. It also defines the path to the ONNX model file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Download first image\nmx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/arcface/player1.jpg')\n# Download second image\nmx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/arcface/player2.jpg')\n# Download onnx model\nmx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/arcface/resnet100.onnx')\n# Path to ONNX model\nmodel_name = '/home/ubuntu/resnet100.onnx'\n```\n\n----------------------------------------\n\nTITLE: Read Image\nDESCRIPTION: This function reads an image from a given path using `mx.image.imread`. It optionally displays the image using matplotlib. Returns the image object.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nBatch = namedtuple('Batch', ['data'])\ndef get_image(path, show=False):\n    img = mx.image.imread(path)\n    if img is None:\n        return None\n    if show:\n        plt.imshow(img.asnumpy())\n        plt.axis('off')\n    return img\n```\n\n----------------------------------------\n\nTITLE: Print Results of Validation - Python\nDESCRIPTION: This code snippet prints the top-1 and top-5 accuracy of the model on the validation dataset. It retrieves the accuracy values from the `acc_top1` and `acc_top5` metrics and prints them to the console using string formatting.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Print results\n_, top1 = acc_top1.get()\n_, top5 = acc_top5.get()\nprint(\"Top-1 accuracy: {}, Top-5 accuracy: {}\".format(top1, top5))\n```\n\n----------------------------------------\n\nTITLE: Quantize SSD Model with Intel Neural Compressor in Shell\nDESCRIPTION: This bash script quantizes the SSD model using the Intel Neural Compressor. It requires specifying the input model path, configuration file, and output model path as arguments. The configuration file (`ssd.yaml`) should contain the appropriate dataset path for quantization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=ssd.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Initialization\nDESCRIPTION: Initializes the COCO dataset by specifying the data directory and annotation file. It loads the COCO ground truth annotations using the pycocotools COCO class.  The `dataDir` variable should be set to the correct path for the COCO dataset to be loaded.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/validation_accuracy.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprefix = 'instances'\ndataType = 'val2017'\n# Replace the following line with the actual location of your COCO dataset\ndataDir = Path('D:\\\\Documents\\\\AI\\\\COCO\\\\raw-data')\n\nannFile = dataDir/'annotations'/f'{prefix}_{dataType}.json'\ncocoGt = COCO(str(annFile)) # gt == \"ground truth\"\n```\n\n----------------------------------------\n\nTITLE: Postprocessing BERT Predictions to JSON Files\nDESCRIPTION: This snippet performs post-processing of the BERT model's output, generating prediction and n-best prediction JSON files. It utilizes the `write_predictions` function (assumed to be defined in `run_onnx_squad`) and creates a `predictions` directory if it doesn't exist.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/BERT-Squad.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# postprocessing\noutput_dir = 'predictions'\nos.makedirs(output_dir, exist_ok=True)\noutput_prediction_file = os.path.join(output_dir, \"predictions.json\")\noutput_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\nwrite_predictions(eval_examples, extra_data, all_results,\n                  n_best_size, max_answer_length,\n                  True, output_prediction_file, output_nbest_file)\n```\n\n----------------------------------------\n\nTITLE: Define Loss Value Metric Class Python\nDESCRIPTION: This code defines a custom loss value metric class, `LossValueMetric`, inherited from `mx.metric.EvalMetric`. It tracks the loss value during training. The `update` method extracts the loss value from the predicted outputs and updates the overall loss. This helps monitor the training progress and identify potential issues such as vanishing or exploding gradients.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Helper class for loss metrics\nclass LossValueMetric(mx.metric.EvalMetric):\n    def __init__(self):\n        self.axis = 1\n        super(LossValueMetric, self).__init__(\n            'lossvalue', axis=self.axis,\n            output_names=None, label_names=None)\n        self.losses = []\n\n    def update(self, labels, preds):\n        loss = preds[-1].asnumpy()[0]\n        self.sum_metric += loss\n        self.num_inst += 1.0\n        gt_label = preds[-2].asnumpy()\n```\n\n----------------------------------------\n\nTITLE: Downloading DenseNet-121 ONNX Model\nDESCRIPTION: This shell command downloads the DenseNet-121 ONNX model from the ONNX model zoo.  The command uses wget to retrieve the specified ONNX file from the given URL. This model is needed as an input for further steps like quantization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/densenet-121/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/densenet-121/model/densenet-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Download ONNX Model Shell Command\nDESCRIPTION: Downloads the ArcFace ResNet100-8 ONNX model from the ONNX Model Zoo using wget. This model is then used for further processing, such as conversion to a different opset version for quantization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/body_analysis/arcface/model/arcfaceresnet100-8.onnx\n```\n\n----------------------------------------\n\nTITLE: Defining CaffeNet Output\nDESCRIPTION: This defines the output tensor for the CaffeNet model. The output is a 2D tensor with dimensions [1, 1000]. It represents the probability distribution over 1000 classes (e.g., ImageNet classes). The data type is float.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/caffenet/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nprob_1: float[1, 1000]\n```\n\n----------------------------------------\n\nTITLE: Saving TensorFlow model for ONNX conversion\nDESCRIPTION: This snippet saves the TensorFlow model using the provided weights file. The script `save_model.py` is executed to generate the TensorFlow model in the specified output directory. Key parameters include `--weights`, `--output`, `--input_size`, and `--model`.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/Conversion.ipynb#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n!python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4.tf --input_size 416 --model yolov4\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: Imports necessary libraries for data manipulation, visualization, and COCO dataset handling. Includes matplotlib for plotting, pycocotools for COCO dataset interactions, pathlib for file path manipulation, and numpy for numerical operations.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/validation_accuracy.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pycocotools.coco import COCO\nfrom pycocotools.mask import iou, encode\nfrom pathlib import Path\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Preprocess First Image and Get Embedding\nDESCRIPTION: This code preprocesses the first image using the `get_input` function with the MTCNN detector, then displays the preprocessed image, and finally computes the feature embedding using the `get_feature` function with the loaded MXNet model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Preprocess first image\npre1 = get_input(detector,img1)\n# Display preprocessed image\nplt.imshow(np.transpose(pre1,(1,2,0)))\nplt.show()\n# Get embedding of first image\nout1 = get_feature(model,pre1)\n```\n\n----------------------------------------\n\nTITLE: Input Data Shape Definition\nDESCRIPTION: Defines the expected input shape for the ShuffleNet model.  It specifies a float tensor with a batch size of 1, 3 color channels, and a height and width of 224 pixels. This is the expected input format for the ONNX model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/shufflenet/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndata_0: float[1, 3, 224, 224]\n```\n\n----------------------------------------\n\nTITLE: Create Mini-Batch for Model Input\nDESCRIPTION: This code creates a mini-batch from the preprocessed input tensor by adding a dimension to represent the batch size.  This is necessary because the model expects input in batch format even when processing a single image.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/shufflenet/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninput_batch = input_tensor.unsqueeze(0)\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies for DUC Validation - Python\nDESCRIPTION: This snippet imports necessary libraries for DUC model validation including mxnet for neural networks, numpy for numerical operations, glob for file pattern matching, os for file system operations, mxnet.contrib.onnx for ONNX model import, and cityscapes_loader for loading and processing Cityscapes data.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom __future__ import print_function\nimport mxnet as mx\nimport numpy as np\nimport glob\nimport os\nfrom mxnet.contrib.onnx import import_model\nfrom cityscapes_loader import CityLoader\n```\n\n----------------------------------------\n\nTITLE: Reading Class Names from File (Python)\nDESCRIPTION: Reads class names from a text file and stores them in a dictionary.  The file is expected to contain one class name per line. The function assigns an ID to each class name based on its line number in the file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef read_class_names(class_file_name):\n    '''loads class name from a file'''\n    names = {}\n    with open(class_file_name, 'r') as data:\n        for ID, name in enumerate(data):\n            names[ID] = name.strip('\\n')\n    return names\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX Opset Version Python\nDESCRIPTION: This Python script converts the ONNX model's opset version to 11. This is required for utilizing the full quantization capabilities. It loads the model, converts its version, and saves the updated model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import version_converter\nmodel = onnx.load('arcfaceresnet100-8.onnx')\nmodel = version_converter.convert_version(model, 11)\nonnx.save_model(model, 'arcfaceresnet100-11.onnx')\n```\n\n----------------------------------------\n\nTITLE: Defining CaffeNet Input\nDESCRIPTION: This defines the input tensor for the CaffeNet model. The input is a 4D tensor with dimensions [1, 3, 224, 224]. It represents a single image with 3 color channels (RGB) and a size of 224x224 pixels. The data type is float.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/caffenet/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ndata_0: float[1, 3, 224, 224]\n```\n\n----------------------------------------\n\nTITLE: Convert ONNX Opset Version (Python)\nDESCRIPTION: This Python code snippet converts the opset version of an ONNX model to version 12. It loads the model, performs the conversion, and saves the updated model.  It requires the 'onnx' library.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/emotion_ferplus/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import version_converter\nmodel = onnx.load('emotion-ferplus-8.onnx')\nmodel = version_converter.convert_version(model, 12)\nonnx.save_model(model, 'emotion-ferplus-12.onnx')\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow model to ONNX\nDESCRIPTION: This snippet installs the necessary libraries and converts the TensorFlow model to ONNX format using tf2onnx. It installs `onnxruntime` and `tensorflow-onnx` via pip, and then uses the `tf2onnx.convert` module to perform the conversion. The converted model is saved as `model.onnx`.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/Conversion.ipynb#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# installations\n!pip install onnxruntime\n!pip install git+https://github.com/onnx/tensorflow-onnx\n    \n# Conversion\npython -m tf2onnx.convert --saved-model ./checkpoints/yolov4.tf --output model.onnx --opset 11 --verbose \n```\n\n----------------------------------------\n\nTITLE: Quantizing ZFNet-512 with Intel Neural Compressor (Bash)\nDESCRIPTION: This bash script utilizes Intel Neural Compressor to quantize the ZFNet-512 model.  It requires specifying the input model path, configuration file, dataset path, label path, and output model path as parameters. Ensure the correct paths are set for successful quantization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/zfnet-512/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=zfnet512.yaml \\\n                   --data_path=/path/to/imagenet \\\n                   --label_path=/path/to/imagenet/label \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: T5 Encoder Model Postprocessing (Python)\nDESCRIPTION: This snippet demonstrates post-processing steps for the T5-encoder model.  It shows how to retrieve the last hidden states from the model output using the model's output index.  Requires that the input IDs have already been generated.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/t5/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlast_hidden_states = model(input_ids)[0]\n```\n\n----------------------------------------\n\nTITLE: Loading Anchors from File in YOLOv4 - Python\nDESCRIPTION: This function, `get_anchors`, loads anchor boxes from a specified file. It reads the anchor values, splits them into a NumPy array, and reshapes them into the expected (3, 3, 2) format for YOLOv4. The `anchors_path` parameter specifies the path to the anchor file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_anchors(anchors_path, tiny=False):\n    '''loads the anchors from a file'''\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = np.array(anchors.split(','), dtype=np.float32)\n    return anchors.reshape(3, 3, 2)\n```\n\n----------------------------------------\n\nTITLE: Extract ImageNet Data using Python\nDESCRIPTION: This script extracts the ImageNet dataset from the downloaded archive files. It requires the 'extract_imagenet.py' script and 'imagenet_val_maps.pklz' file to be in the same directory. The script takes two arguments: '--download-dir' specifying the location of the downloaded archives, and '--target-dir' specifying the directory to extract the images to.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_prep.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython extract_imagenet.py --download-dir *path to download folder* --target-dir *path to extract folder*\n```\n\n----------------------------------------\n\nTITLE: Saving the ONNX Model with Dynamic Axes in Python\nDESCRIPTION: This snippet saves the PyTorch model in ONNX format using the `save_model` utility function. It specifies dynamic axes for the input and output tensors, allowing the model to handle arbitrary batch sizes, heights, and widths. The `opset_version` is set to 11.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif DO_101:\n    model_name = 'fcn_resnet101'\nelse:\n    model_name = 'fcn_resnet50'\n\nmodel_path, data_dir = save_model(\n    model_name, model.cpu(),\n    data_dir,\n    input_tensor, [output_tensor, aux_tensor],\n    input_names=['input'], output_names=['out', 'aux'],\n    dynamic_axes={\n        'input': {0: 'batch', 2: 'height', 3: 'width'},\n        'out': {0: 'batch', 2: 'height', 3: 'width'},\n        'aux': {0: 'batch', 2: 'height', 3: 'width'},\n    },\n    opset_version=11\n)\n```\n\n----------------------------------------\n\nTITLE: SqueezeNet Model Definition in Gluon - Python\nDESCRIPTION: This code defines the SqueezeNet model architecture using the Gluon API in MXNet. It includes helper functions for creating fire modules (`_make_fire` and `_make_fire_conv`) and the main `SqueezeNet` class, which defines the network structure based on the specified version (1.0 or 1.1). The `get_squeezenet` function creates an instance of the SqueezeNet model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"SqueezeNet, implemented in Gluon.\"\"\"\n__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n\n# Helpers\ndef _make_fire(squeeze_channels, expand1x1_channels, expand3x3_channels):\n    out = nn.HybridSequential(prefix='')\n    out.add(_make_fire_conv(squeeze_channels, 1))\n\n    paths = HybridConcurrent(axis=1, prefix='')\n    paths.add(_make_fire_conv(expand1x1_channels, 1))\n    paths.add(_make_fire_conv(expand3x3_channels, 3, 1))\n    out.add(paths)\n\n    return out\n\ndef _make_fire_conv(channels, kernel_size, padding=0):\n    out = nn.HybridSequential(prefix='')\n    out.add(nn.Conv2D(channels, kernel_size, padding=padding))\n    out.add(nn.Activation('relu'))\n    return out\n\n# Net\nclass SqueezeNet(HybridBlock):\n    r\"\"\"SqueezeNet model from the \\\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n    and <0.5MB model size\\\" <https://arxiv.org/abs/1602.07360>`_ paper.\n    SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n    Parameters\n    ----------\n    version : str\n        Version of squeezenet. Options are '1.0', '1.1'.\n    classes : int, default 1000\n        Number of classification classes.\n    \"\"\"\n    def __init__(self, version, classes=1000, **kwargs):\n        super(SqueezeNet, self).__init__(**kwargs)\n        assert version in ['1.0', '1.1'], (\"Unsupported SqueezeNet version {version}:\"\n                                           \"1.0 or 1.1 expected\".format(version=version))\n        with self.name_scope():\n            self.features = nn.HybridSequential(prefix='')\n            if version == '1.0':\n                self.features.add(nn.Conv2D(96, kernel_size=7, strides=2))\n                self.features.add(nn.Activation('relu'))\n                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n                self.features.add(_make_fire(16, 64, 64))\n                self.features.add(_make_fire(16, 64, 64))\n                self.features.add(_make_fire(32, 128, 128))\n                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n                self.features.add(_make_fire(32, 128, 128))\n                self.features.add(_make_fire(48, 192, 192))\n                self.features.add(_make_fire(48, 192, 192))\n                self.features.add(_make_fire(64, 256, 256))\n                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n                self.features.add(_make_fire(64, 256, 256))\n            else:\n                self.features.add(nn.Conv2D(64, kernel_size=3, strides=2))\n                self.features.add(nn.Activation('relu'))\n                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n                self.features.add(_make_fire(16, 64, 64))\n                self.features.add(_make_fire(16, 64, 64))\n                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n                self.features.add(_make_fire(32, 128, 128))\n                self.features.add(_make_fire(32, 128, 128))\n                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n                self.features.add(_make_fire(48, 192, 192))\n                self.features.add(_make_fire(48, 192, 192))\n                self.features.add(_make_fire(64, 256, 256))\n                self.features.add(_make_fire(64, 256, 256))\n            self.features.add(nn.Dropout(0.5))\n\n            self.output = nn.HybridSequential(prefix='')\n            self.output.add(nn.Conv2D(classes, kernel_size=1))\n            self.output.add(nn.Activation('relu'))\n            self.output.add(nn.AvgPool2D(13))\n            self.output.add(nn.Flatten())\n\n    def hybrid_forward(self, F, x):\n        x = self.features(x)\n        x = self.output(x)\n        return x\n\n# Constructor\ndef get_squeezenet(version, root=os.path.join('~', '.mxnet', 'models'), **kwargs):\n    r\"\"\"SqueezeNet model from the \\\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n    and <0.5MB model size\\\" <https://arxiv.org/abs/1602.07360>`_ paper.\n    SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n    Parameters\n    ----------\n    version : str\n        Version of squeezenet. Options are '1.0', '1.1'.\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    net = SqueezeNet(version, **kwargs)\n    return net\n\ndef squeezenet1_0(**kwargs):\n    r\"\"\"SqueezeNet 1.0 model from the \\\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n    and <0.5MB model size\\\" <https://arxiv.org/abs/1602.07360>`_ paper.\n    Parameters\n    ----------\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    return get_squeezenet('1.0', **kwargs)\n\ndef squeezenet1_1(**kwargs):\n    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n    Parameters\n    ----------\n    root : str, default '~/.mxnet/models'\n        Location for keeping the model parameters.\n    \"\"\"\n    return get_squeezenet('1.1', **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Quantizing DenseNet-121 ONNX Model (bash)\nDESCRIPTION: This bash script quantizes the DenseNet-121 ONNX model using the Intel® Neural Compressor. The script takes the input model path, a configuration file (densenet.yaml), and an output model path as arguments. It leverages the run_tuning.sh script, presumably provided by Intel Neural Compressor, to perform the quantization process.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/densenet-121/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=densenet.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Define Output Shape\nDESCRIPTION: Defines the output tensor shape for the Inception v2 model. The output tensor is a float array with dimensions [1, 1000], representing a single classification result with probabilities for 1000 different classes.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/inception_v2/README.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nprob_1: float[1, 1000]\n```\n\n----------------------------------------\n\nTITLE: Run Model Training - MXNet\nDESCRIPTION: This code defines a main function to start the model training process. It hybridizes the network (if applicable) and calls the train function with the specified number of epochs and context.  The context specifies the device where training takes place.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/train_squeezenet.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    net.hybridize()\n    train(num_epochs, context)\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Install Git LFS\nDESCRIPTION: This command installs Git LFS (Large File Storage), which is necessary to properly download large ONNX model files from the repository. It ensures that the Git client can handle large files by using pointers in the repository and storing the actual file content elsewhere.\nSOURCE: https://github.com/onnx/models/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Quantizing the Faster R-CNN Model (Bash)\nDESCRIPTION: This bash script outlines the process for quantizing a Faster R-CNN model using specified configurations and data paths. It assumes the existence of a `run_tuning.sh` script and configuration file (`faster_rcnn.yaml`) for the quantization process.  The script takes input model, config file, data path, and output model paths as arguments.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/faster-rcnn/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --config=faster_rcnn.yaml \\\n                   --data_path=path/to/COCO2017 \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: ONNX Model Validation - TensorProto Input\nDESCRIPTION: This code snippet demonstrates how to load an ONNX model and validate it using test data stored as serialized protobuf TensorProto files (.pb). It loads input and output tensors from the specified directory, runs the model using an ONNX backend, and compares the results with the reference outputs using numpy's assert_almost_equal function.\nSOURCE: https://github.com/onnx/models/blob/main/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport onnx\nimport os\nimport glob\nimport onnx_backend as backend\n\nfrom onnx import numpy_helper\n\nmodel = onnx.load('model.onnx')\ntest_data_dir = 'test_data_set_0'\n\n# Load inputs\ninputs = []\ninputs_num = len(glob.glob(os.path.join(test_data_dir, 'input_*.pb')))\nfor i in range(inputs_num):\n    input_file = os.path.join(test_data_dir, 'input_{}.pb'.format(i))\n    tensor = onnx.TensorProto()\n    with open(input_file, 'rb') as f:\n        tensor.ParseFromString(f.read())\n    inputs.append(numpy_helper.to_array(tensor))\n\n# Load reference outputs\nref_outputs = []\nref_outputs_num = len(glob.glob(os.path.join(test_data_dir, 'output_*.pb')))\nfor i in range(ref_outputs_num):\n    output_file = os.path.join(test_data_dir, 'output_{}.pb'.format(i))\n    tensor = onnx.TensorProto()\n    with open(output_file, 'rb') as f:\n        tensor.ParseFromString(f.read())\n    ref_outputs.append(numpy_helper.to_array(tensor))\n\n# Run the model on the backend\noutputs = list(backend.run_model(model, inputs))\n\n# Compare the results with reference outputs.\nfor ref_o, o in zip(ref_outputs, outputs):\n    np.testing.assert_almost_equal(ref_o, o)\n```\n\n----------------------------------------\n\nTITLE: Load Data for Validation in Python\nDESCRIPTION: These functions load the validation datasets and information. `load_bin()` loads the image data and `issame_list` from a pickle file. `load_property()` loads metadata like image size and number of classes from a property file in the dataset directory.  The data is processed into MXNet NDArrays.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_validation.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef load_bin(path, image_size):\n    try:\n        # python 3\n        bins, issame_list = pickle.load(open(path, 'rb'), encoding='bytes')\n    except:\n        # python 2\n        bins, issame_list = pickle.load(open(path, 'rb'))\n    data_list = []\n    for flip in [0,1]:\n        data = nd.empty((len(issame_list)*2, 3, image_size[0], image_size[1]))\n        data_list.append(data)\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]\n        img = mx.image.imdecode(_bin)\n        img = nd.transpose(img, axes=(2, 0, 1))\n        for flip in [0,1]:\n            if flip==1:\n                img = mx.ndarray.flip(data=img, axis=2)\n            data_list[flip][i][:] = img\n        if i%1000==0:\n            print('loading bin', i)\n    print(data_list[0].shape)\n    return (data_list, issame_list)\n\ndef load_property(data_dir):\n    prop = edict()\n    for line in open(os.path.join(data_dir, 'property')):\n        vec = line.strip().split(',')\n        assert len(vec)==3\n        prop.num_classes = int(vec[0])\n        prop.image_size = [int(vec[1]), int(vec[2])]\n    return prop\n```\n\n----------------------------------------\n\nTITLE: Specify Model and Hyperparameters (Python)\nDESCRIPTION: This code block defines various hyperparameters and settings for training the VGG model. Parameters include model name, data directory, batch size, number of GPUs, number of workers, learning rate, momentum, weight decay, and save locations.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# specify model - choose from vgg16, vgg16_bn, vgg19, vgg19_bn (_bn versions are with batch normalization)\nmodel_name = 'vgg16'\n\n# training and validation pictures to use\ndata_dir = '/home/ubuntu/imagenet/img_dataset'\n\n# training batch size per device (CPU/GPU)\nbatch_size = 32\n\n# number of GPUs to use (automatically detect the number of GPUs)\nnum_gpus = len(mx.test_utils.list_gpus())\n\n# number of pre-processing workers (automatically detect the number of workers)\nnum_workers = multiprocessing.cpu_count()\n\n# number of training epochs\nnum_epochs = 100\n\n# learning rate\nlr = 0.01\n\n# momentum value for optimizer\nmomentum = 0.9\n\n# weight decay rate\nwd = 0.0001\n\n# decay rate of learning rate\nlr_decay = 0.1\n\n# interval for periodic learning rate decays\nlr_decay_period = 0\n\n# epoches at which learning rate decays\nlr_decay_epoch = '50,80'\n\n# mode in which to train the model. options are symbolic, imperative, hybrid\nmode = 'hybrid'\n\n# Number of batches to wait before logging\nlog_interval = 1000\n\n# frequency of model saving\nsave_frequency = 10\n\n# directory of saved models\nsave_dir = 'params'\n\n#directory of training logs\nlogging_dir = 'logs'\n\n# the path to save the history plot\nsave_plot_dir = '.'\n\n```\n\n----------------------------------------\n\nTITLE: Quantizing BERT-Squad Model\nDESCRIPTION: This bash script command quantizes the BERT-Squad model using the Intel® Neural Compressor. It takes input model path, output model path, dataset location, and configuration file as arguments for the quantization process.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=/path/to/model \\# model path as *.onnx\n                   --output_model=/path/to/model_tune \\\n                   --dataset_location=/path/to/SQuAD/dataset \\\n                   --config=bert.yaml\n```\n\n----------------------------------------\n\nTITLE: Download ONNX Model (Shell)\nDESCRIPTION: This shell command downloads the specified ONNX model file from the ONNX Model Zoo using wget.  The command retrieves the version-RFB-320.onnx file, which is a face detection model. Requires wget to be installed.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/ultraface/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/body_analysis/ultraface/models/version-RFB-320.onnx\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning and Evaluating RoBERTa Model (Python)\nDESCRIPTION: This snippet initializes a RoBERTa-based transformer model using the Simple Transformers library, fine-tunes it on the training data obtained from the `prepare_data` function, and evaluates its performance on the test data. It disables fp16 for training and uses the `train_model` and `eval_model` methods of the `TransformerModel` class.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/roberta/dependencies/roberta-sequence-classification-validation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrain, test = prepare_data(0.20)\nmodel = TransformerModel('roberta', 'roberta-base', args=({'fp16': False}))\nmodel.train_model(train)\n\n\nresult, model_outputs, wrong_predictions = model.eval_model(test)\n```\n\n----------------------------------------\n\nTITLE: Downloading AlexNet ONNX Model\nDESCRIPTION: Downloads the AlexNet ONNX model from the GitHub repository. This script uses wget to retrieve the model file, which is required for further processing like quantization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/alexnet/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/alexnet/model/bvlcalexnet-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Download BiDAF Model Script\nDESCRIPTION: This shell command downloads the BiDAF model in ONNX format from the ONNX Model Zoo.  This is a prerequisite step for quantizing the model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bidirectional_attention_flow/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx\n```\n\n----------------------------------------\n\nTITLE: Set Paths and Parameters - Python\nDESCRIPTION: This snippet configures the execution context (CPU or GPU), specifies paths to the validation data and labels, and sets the batch size for processing the Cityscapes dataset. The context determines whether the computations will be performed on the CPU or GPU.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Determine and set context\nif len(mx.test_utils.list_gpus())==0:\n    ctx = mx.cpu()\nelse:\n    ctx = mx.gpu(0)\n\n# Path to validation data\ndata_dir = '/home/ubuntu/TuSimple-DUC/dataset/leftImg8bit/val'\n# Path to validation labels\nlabel_dir = '/home/ubuntu/TuSimple-DUC/dataset/gtFine/val'\n# Set batch size\nbatch_size = 16\n```\n\n----------------------------------------\n\nTITLE: ONNX Model Validation - NumPy Input\nDESCRIPTION: This code snippet demonstrates how to load an ONNX model and validate it using test data stored as serialized NumPy archives (.npz). It loads input and output tensors from the .npz file, runs the model using an ONNX backend, and compares the results with the reference outputs using numpy's assert_almost_equal function.\nSOURCE: https://github.com/onnx/models/blob/main/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport onnx\nimport onnx_backend as backend\n\n# Load the model and sample inputs and outputs\nmodel = onnx.load(model_pb_path)\nsample = np.load(npz_path, encoding='bytes')\ninputs = list(sample['inputs'])\noutputs = list(sample['outputs'])\n\n# Run the model with an onnx backend and verify the results\nnp.testing.assert_almost_equal(outputs, backend.run_model(model, inputs))\n```\n\n----------------------------------------\n\nTITLE: Image Preprocessing for Face Detection (Python)\nDESCRIPTION: This Python snippet demonstrates how to preprocess an image for the ultra-lightweight face detection model. It converts the image from BGR to RGB, resizes it, subtracts the mean, divides by the scale factor, transposes the dimensions, and expands the dimensions to match the model's expected input format. Requires cv2 and numpy.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/ultraface/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimage = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\nimage = cv2.resize(image, (320, 240))\nimage_mean = np.array([127, 127, 127])\nimage = (image - image_mean) / 128\nimage = np.transpose(image, [2, 0, 1])\nimage = np.expand_dims(image, axis=0)\nimage = image.astype(np.float32)\n```\n\n----------------------------------------\n\nTITLE: Convert PyTorch Model to ONNX\nDESCRIPTION: This snippet converts a PyTorch model to ONNX format using `torch.onnx.export`. It loads a pre-trained model, preprocesses the state dictionary, and exports the model to an ONNX file. Input and output data are also saved for testing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/style_transfer/fast_neural_style/dependencies/conversion.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport re\nimport os\nimport onnxruntime as rt\nfrom transformer_net import TransformerNet\n\ninput = torch.randn(1, 3, 224, 224)\nwith torch.no_grad():\n    model = TransformerNet()\n    model_dict = torch.load(\"PATH TO PYTORCH MODEL\")\n    for k in list(model_dict.keys()):\n        if re.search(r'in\\d+\\.running_(mean|var)$', k):\n            del model_dict[k]\n    model.load_state_dict(model_dict)\n    output = model(input)\n    \ninput_names = ['input1']\noutput_names = ['output1']\ndir = \"PATH TO CONVERTED ONNX MODEL\"\nif not os.path.exists(dir):\n    os.makedirs(dir)\ndata_dir = os.path.join(dir, \"data_set\")\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\nif isinstance(model, torch.jit.ScriptModule):\n    torch.onnx._export(model, tuple((input,)), os.path.join(dir, 'model.onnx'), verbose=True, input_names=input_names, output_names=output_names, example_outputs=(output,))\nelse:\n    torch.onnx.export(model, tuple((input,)), os.path.join(dir, 'model.onnx'), verbose=True, input_names=input_names, output_names=output_names)\n\ninput = f(input)\ninput = g(input, [])\noutput = f(output)\noutput = g(output, [])\n        \nSaveData(data_dir, 'input', input)\nSaveData(data_dir, 'output', output)\n```\n\n----------------------------------------\n\nTITLE: Download ONNX Model - Python\nDESCRIPTION: This snippet downloads the ResNet101_DUC_HDC.onnx model from a specified URL using `mx.test_utils.download` and defines the local path where the model is stored. The downloaded model will be used for inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmx.test_utils.download('https://s3.amazonaws.com/onnx-model-zoo/duc/ResNet101_DUC_HDC.onnx')\n# Path to ONNX model\nmodel_path = 'ResNet101_DUC_HDC.onnx'\n```\n\n----------------------------------------\n\nTITLE: Download SSD-MobilenetV1 Model using Shell\nDESCRIPTION: This shell script downloads the SSD-MobilenetV1-12 ONNX model from the ONNX model zoo. It uses the `wget` command to retrieve the model file from the specified URL and save it to the current directory.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_12.onnx\n```\n\n----------------------------------------\n\nTITLE: Quantize GoogleNet Model using a bash script\nDESCRIPTION: This bash script executes the `run_tuning.sh` script for quantizing a GoogleNet model using Intel® Neural Compressor. The script requires specifying the input model path, configuration file, paths to the ImageNet dataset and labels, and the output model path. This will quantize the model and save the quantized version to the specified output path.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/googlenet/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --config=googlenet.yaml \\\n                   --data_path=/path/to/imagenet \\\n                   --label_path=/path/to/imagenet/label \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Load ONNX Model into MXNet - Python\nDESCRIPTION: This snippet imports the ONNX model into MXNet by creating MXNet symbols and parameters from the ONNX model file, then defines a network module and binds parameters to the network. This prepares the ONNX model for inference using MXNet.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/duc/dependencies/duc-validation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# import ONNX model into MXNet symbols and params\nsym,arg,aux = import_model(model_path)\n# define network module\nmod = mx.mod.Module(symbol=sym, data_names=['data'], context=ctx, label_names=None)\n# bind parameters to the network\nmod.bind(for_training=False, data_shapes=[('data', (batch_size, 3, 800, 800))], label_shapes=mod._label_shapes)\nmod.set_params(arg_params=arg, aux_params=aux,allow_missing=True, allow_extra=True)\n```\n\n----------------------------------------\n\nTITLE: Download Specific ONNX Model with Git LFS\nDESCRIPTION: This command downloads a specific ONNX model file using Git LFS. The `--include` option specifies the path to the desired model file, while `--exclude=\"\"` ensures that no files are excluded. This command requires Git LFS to be installed.\nSOURCE: https://github.com/onnx/models/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs pull --include=\"[path to model].onnx\" --exclude=\"\"\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies in Python\nDESCRIPTION: Imports necessary libraries for ONNX model loading, numerical operations, ONNX Runtime inference, image processing, and visualization. This includes onnx, numpy, onnxruntime, PIL (Image), cv2, and matplotlib.pyplot. It verifies that all dependencies are installed.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport numpy as np\nimport onnxruntime as ort\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Downloading the ONNX Model using Shell\nDESCRIPTION: This shell command downloads the EfficientNet-Lite4 ONNX model from the ONNX model zoo using `wget`. It retrieves the `efficientnet-lite4-11.onnx` file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/efficientnet-lite4/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/efficientnet-lite4/model/efficientnet-lite4-11.onnx\n```\n\n----------------------------------------\n\nTITLE: R-CNN ILSVRC13 Output Definition\nDESCRIPTION: Defines the output tensor shape and data type for the R-CNN ILSVRC13 model. The output is a 2D float tensor representing the classification scores with dimensions [1, 200], where 1 is the batch size, and 200 is the number of classes (ILSVRC2013 dataset).\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/rcnn_ilsvrc13/README.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nfc-rcnn_1: float[1, 200]\n```\n\n----------------------------------------\n\nTITLE: Defining ResNet Network and Block Versions\nDESCRIPTION: Defines lists `resnet_net_versions` and `resnet_block_versions` which store the ResNet V1/V2 classes and the corresponding basic/bottleneck blocks for each version. This allows to easily switch between ResNet versions and their respective building blocks.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/train_resnet.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresnet_net_versions = [ResNetV1, ResNetV2]\nresnet_block_versions = [{'basic_block': BasicBlockV1, 'bottle_neck': BottleneckV1},\n                         {'basic_block': BasicBlockV2, 'bottle_neck': BottleneckV2}]\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Input Text with NLTK\nDESCRIPTION: This Python code demonstrates how to preprocess input text (context and query) for the BiDAF model. It uses the NLTK library for tokenization and converts the tokens into numpy arrays suitable for model input. The function preprocess tokenizes the input text, converts words to lowercase, splits words into characters, and pads character sequences to a length of 16.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bidirectional_attention_flow/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport string\nfrom nltk import word_tokenize\n\ndef preprocess(text):\n    tokens = word_tokenize(text)\n    # split into lower-case word tokens, in numpy array with shape of (seq, 1)\n    words = np.asarray([w.lower() for w in tokens]).reshape(-1, 1)\n    # split words into chars, in numpy array with shape of (seq, 1, 1, 16)\n    chars = [[c for c in t][:16] for t in tokens]\n    chars = [cs+['']*(16-len(cs)) for cs in chars]\n    chars = np.asarray(chars).reshape(-1, 1, 1, 16)\n    return words, chars\n\n# input\ncontext = 'A quick brown fox jumps over the lazy dog.'\nquery = 'What color is the fox?'\ncw, cc = preprocess(context)\nqw, qc = preprocess(query)\n```\n\n----------------------------------------\n\nTITLE: YOLOv3 Model Download (Shell)\nDESCRIPTION: This shell command downloads the YOLOv3-12 model from the ONNX model repository. The downloaded model can then be used for inference or quantization.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov3/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/yolov3/model/yolov3-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Displaying Image Output - Python\nDESCRIPTION: This snippet displays the processed image using `PIL` and `matplotlib`. It converts the image to a `PIL.Image` object and then displays it using the `.show()` method and `matplotlib`'s `imshow`. `%matplotlib inline` is assumed to be used in a Jupyter Notebook environment.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimage = Image.fromarray(image)\nimage.show()\n\n%matplotlib inline\nimshow(np.asarray(image))\n```\n\n----------------------------------------\n\nTITLE: Specify Paths and Parameters in Python\nDESCRIPTION: This code snippet sets the paths to the dataset and model files, as well as parameters such as the verification targets, batch size, and number of folds for cross-validation. These parameters are used in the model validation process.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_validation.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Path to dataset\ndata_dir = '/home/ubuntu/faces_ms1m_112x112/'\n# Path to model file\nmodel = '/home/ubuntu/resnet100.onnx'\n# Verification targets\ntarget = 'lfw,cfp_ff,cfp_fp,agedb_30'\n# Batch size\nbatch_size = 64\n# Number of folds for cross validation\nnfolds = 10\n```\n\n----------------------------------------\n\nTITLE: Testing ONNX Model Inference in Python\nDESCRIPTION: This code runs inference using the saved ONNX model with ONNX Runtime, comparing the output against the output from the original PyTorch model. It utilizes the `ort_inference` function to load the ONNX model, perform inference, and check the accuracy of the results.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nort_inference(model_path, input_tensor.detach().cpu().numpy(), [output_tensor, aux_tensor])\n```\n\n----------------------------------------\n\nTITLE: Display Segmentation Result in Python\nDESCRIPTION: This snippet displays the segmentation result using `matplotlib`. It prints the confidence score and uses `imshow` to render the segmented image (result_img) obtained from the post-processing step. `%matplotlib inline` ensures the plot is displayed within the notebook.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/inference.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nprint(conf)\nimshow(np.asarray(result_img))\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies for ArcFace Inference\nDESCRIPTION: This code snippet imports the necessary Python libraries for performing ArcFace inference, including cv2, numpy, mxnet, and other utility libraries for image processing, face detection, and ONNX model import.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport sys\nimport numpy as np\nimport mxnet as mx\nimport os\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport random\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom time import sleep\nfrom easydict import EasyDict as edict\nfrom mtcnn_detector import MtcnnDetector\nfrom skimage import transform as trans\nimport matplotlib.pyplot as plt\nfrom mxnet.contrib.onnx.onnx2mx.import_model import import_model\n```\n\n----------------------------------------\n\nTITLE: Download ResNet ONNX Model\nDESCRIPTION: Downloads the ResNet50-v1-12.onnx model from the specified URL. This is a prerequisite for quantization and inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v1-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Quantizing the Model using Shell\nDESCRIPTION: This shell script executes the `run_tuning.sh` script to quantize the EfficientNet-Lite4 model using the Intel Neural Compressor. It specifies the input model path, configuration file, and output model path as parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/efficientnet-lite4/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --config=efficientnet.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Quantizing SqueezeNet Model with Intel Neural Compressor\nDESCRIPTION: This bash script quantizes a SqueezeNet model using Intel Neural Compressor with ONNX Runtime backend. It requires specifying the input model path, configuration file, dataset path, label path, and output model path. The dataset path should point to the location of the ImageNet dataset.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=squeezenet.yaml \\\n                   --data_path=/path/to/imagenet \\\n                   --label_path=/path/to/imagenet/label \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Downloading ZFNet-512 ONNX Model (Shell)\nDESCRIPTION: Downloads the ZFNet-512 ONNX model using wget. This is a necessary step to prepare the model for quantization or inference. The downloaded model will be named zfnet512-12.onnx.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/zfnet-512/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/zfnet-512/model/zfnet512-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Model in Python\nDESCRIPTION: This snippet loads an ONNX model from a specified file path. It imports the `onnx` and `os` modules. The `onnx.load` function is used to load the ONNX model from the file 'model.onnx' located in the current directory.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/onnx-model-validation.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport os\n\n# Load the ONNX model\nmodel = onnx.load(os.path.join('model.onnx'))\n```\n\n----------------------------------------\n\nTITLE: Quantize Model with Intel Neural Compressor\nDESCRIPTION: This shell script quantizes the SSD-MobilenetV1 model using Intel Neural Compressor.  It assumes that `run_tuning.sh` and `ssd_mobilenet_v1.yaml` are present in the current directory.  The script requires the user to specify the input model path, the config file (ssd_mobilenet_v1.yaml), and the output model path.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model  \\\n                   --config=ssd_mobilenet_v1.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Downloading CaffeNet Model\nDESCRIPTION: This shell script command downloads the CaffeNet-12.onnx model from the onnx/models repository on GitHub. This is a prerequisite step for quantizing or using the model.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/caffenet/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/caffenet/model/caffenet-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Output Data Shape Definition\nDESCRIPTION: Defines the output shape for the ShuffleNet model. The output is a float tensor of shape (1, 1000), representing confidence scores for 1000 ImageNet classes.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/shufflenet/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nsoftmax_1: float[1, 1000]\n```\n\n----------------------------------------\n\nTITLE: Load ONNX Model\nDESCRIPTION: This snippet loads the ONNX model using the `get_model` function defined earlier, providing the MXNet context and the path to the ONNX model file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Load ONNX model\nmodel = get_model(ctx , model_name)\n```\n\n----------------------------------------\n\nTITLE: Quantize ResNet Model using Intel Neural Compressor\nDESCRIPTION: Quantizes a ResNet model using Intel Neural Compressor.  It requires specifying the input model path, a configuration file (resnet50_v1_5.yaml), and the desired output model path. The dataset path needs to be appropriately set in the configuration file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/resnet/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=resnet50_v1_5.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Quantize FCN ResNet50 Model with Intel Neural Compressor (Bash)\nDESCRIPTION: This bash script demonstrates how to quantize an FCN ResNet50 model using the Intel Neural Compressor. The script takes the input model path, configuration file, data path, label path, and output model path as arguments. Make sure to update the paths according to your environment and configurations.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model  \\# model path as *.onnx\n                   --config=fcn_rn50.yaml \\\n                   --data_path=path/to/coco/val2017 \\\n                   --label_path=path/to/coco/annotations/instances_val2017.json \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Image Pre-processing Imports for GoogleNet in Python\nDESCRIPTION: Imports the necessary libraries for image processing in Python. `imageio` is used to read images, and `PIL` (Pillow) is used for image manipulation, specifically for resizing.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/googlenet/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport imageio\nfrom PIL import Image\n```\n\n----------------------------------------\n\nTITLE: Validating the ONNX model\nDESCRIPTION: This snippet validates the converted ONNX model using the `onnx` library. It loads the ONNX model, checks its integrity using `onnx.checker.check_model`, and prints a human-readable representation of the model graph. Dependencies include the `onnx` and `os` libraries.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/Conversion.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport os\n\n# Load the ONNX model\nmodel = onnx.load(os.path.join('model.onnx'))\nonnx.checker.check_model(model)  # Check that the IR is well formed\nprint(onnx.helper.printable_graph(model.graph))  # Print a human readable representation of the graph\n```\n\n----------------------------------------\n\nTITLE: Download SSD Model using wget in Shell\nDESCRIPTION: This shell command downloads the SSD-12 ONNX model using `wget`. This is a prerequisite step for model quantization and inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/ssd/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/ssd/model/ssd-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Print Validation Metrics\nDESCRIPTION: Prints the calculated mean Intersection over Union (IoU) and global pixelwise accuracy (GPA) after validation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/validation_accuracy.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"{meanIoU=} {gpa=}\")\n```\n\n----------------------------------------\n\nTITLE: Downloading SqueezeNet Model\nDESCRIPTION: This shell command downloads a pre-trained SqueezeNet 1.0 model in ONNX format from the specified URL using wget. The model is intended for use with ONNX Runtime for image classification tasks.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/squeezenet/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/squeezenet/model/squeezenet1.0-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Download Inception v1 ONNX Model (Shell)\nDESCRIPTION: Downloads the Inception v1 ONNX model from the specified URL using wget. This is a prerequisite step for quantization or inference.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/inception_v1/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Downloading BERT-Squad Model\nDESCRIPTION: This shell command downloads the BERT-Squad model (opset version 12) from the specified URL. The wget command is used to retrieve the ONNX model file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/text/machine_comprehension/bert-squad/model/bertsquad-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Cloning TensorFlow YOLOv4 repository\nDESCRIPTION: This snippet clones the TensorFlow YOLOv4 repository from GitHub.  It uses git to clone the repository and then changes the current directory to the cloned repository.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/Conversion.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!git clone https://github.com/hunglc007/tensorflow-yolov4-tflite\n!cd tensorflow-yolov4-tflite\n```\n\n----------------------------------------\n\nTITLE: Downloading the Faster R-CNN Model (Shell)\nDESCRIPTION: This shell command downloads the Faster R-CNN model file (FasterRCNN-12.onnx) from the specified GitHub repository.  It uses the `wget` utility to retrieve the file.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/faster-rcnn/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Quantize MNIST Model - Bash\nDESCRIPTION: This bash script quantizes the MNIST model using Intel Neural Compressor. It uses the `run_tuning.sh` script and requires specifying the input model path, configuration file (mnist.yaml), and output model path. Prerequisites: Intel Neural Compressor and its dependencies need to be installed and configured.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/mnist/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --config=mnist.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Downloading BERT Model Files\nDESCRIPTION: This code snippet downloads the uncased BERT model files from a Google Cloud Storage URL and extracts them using `wget` and `unzip` commands. It requires `wget` and `unzip` utilities to be available in the environment.\nSOURCE: https://github.com/onnx/models/blob/main/validated/text/machine_comprehension/bert-squad/BERT-Squad.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip\n```\n\n----------------------------------------\n\nTITLE: Quantizing CaffeNet Model with Intel Neural Compressor\nDESCRIPTION: This bash script demonstrates how to quantize the CaffeNet model using Intel® Neural Compressor.  It requires specifying the model path, configuration file, data path, label path, and output model path. The configuration file (caffenet.yaml) should contain the quantization parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/caffenet/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=caffenet.yaml \\\n                   --data_path=/path/to/imagenet \\\n                   --label_path=/path/to/imagenet/label \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Load network for inference\nDESCRIPTION: This snippet loads a pre-trained MXNet model for inference. It determines the execution context (CPU or GPU), creates an `mx.mod.Module` instance, binds the input data shape, and sets the model parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Determine and set context\nif len(mx.test_utils.list_gpus())==0:\n    ctx = mx.cpu()\nelse:\n    ctx = mx.gpu(0)\n# Load module\nmod = mx.mod.Module(symbol=sym, context=ctx, label_names=None)\nmod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))], \n         label_shapes=mod._label_shapes)\nmod.set_params(arg_params, aux_params, allow_missing=True, allow_extra=True)\n```\n\n----------------------------------------\n\nTITLE: Model Quantization using Neural Compressor\nDESCRIPTION: This bash script demonstrates how to use Intel Neural Compressor to quantize the ShuffleNetv2 model. It takes the path to the input model, a configuration file (shufflenetv2.yaml), and the desired output path as parameters.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/shufflenet/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash run_tuning.sh --input_model=path/to/model \\\n                   --config=shufflenetv2.yaml \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Read Labels from File in Python\nDESCRIPTION: Reads labels from the 'synset.txt' file, stripping trailing whitespace from each line and storing them in a list called 'labels'. This list is used for mapping the model's output indices to human-readable labels.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/onnxrt_inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith open('synset.txt', 'r') as f:\n    labels = [l.rstrip() for l in f]\n```\n\n----------------------------------------\n\nTITLE: Input Shape Specification for ZFNet-512\nDESCRIPTION: Describes the input tensor shape expected by the ZFNet-512 ONNX model. The model expects a 4D tensor with dimensions: batch size (1), channels (3), height (224), and width (224).\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/zfnet-512/README.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ngpu_0/data_0: float[1, 3, 224, 224]\n```\n\n----------------------------------------\n\nTITLE: Define Accuracy Metric Class Python\nDESCRIPTION: This code defines a custom accuracy metric class, `AccMetric`, inherited from `mx.metric.EvalMetric`. It calculates the accuracy by comparing the predicted labels with the ground truth labels. The `update` method computes the accuracy for each batch and updates the overall accuracy. The accuracy is calculated by comparing the predicted labels (obtained from the softmax output) with the ground truth labels.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/train_arcface.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Helper class for accuracy metrics\nclass AccMetric(mx.metric.EvalMetric):\n    def __init__(self):\n        self.axis = 1\n        super(AccMetric, self).__init__(\n            'acc', axis=self.axis,\n            output_names=None, label_names=None)\n        self.losses = []\n        self.count = 0\n\n    def update(self, labels, preds):\n        self.count+=1\n        preds = [preds[1]] #use softmax output\n        for label, pred_label in zip(labels, preds):\n            if pred_label.shape != label.shape:\n                pred_label = mx.ndarray.argmax(pred_label, axis=self.axis)\n            pred_label = pred_label.asnumpy().astype('int32').flatten()\n            label = label.asnumpy()\n            if label.ndim==2:\n                label = label[:,0]\n            label = label.astype('int32').flatten()\n            assert label.shape==pred_label.shape\n            self.sum_metric += (pred_label.flat == label.flat).sum()\n            self.num_inst += len(pred_label.flat)\n```\n\n----------------------------------------\n\nTITLE: Main Function: Model Initialization and Training\nDESCRIPTION: The `main` function initializes the neural network (`net`) with specified weights, sets the network to hybrid mode for potentially faster execution, and calls the `train` function to begin the training process. It leverages the `net` object and the `train` function defined in the preceding snippets. It is designed to be the entry point of the training script.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    net.hybridize()\n    train(num_epochs, context)\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Download GoogleNet ONNX Model using wget\nDESCRIPTION: This shell command uses `wget` to download the GoogleNet ONNX model file (`googlenet-12.onnx`) from the specified URL on the ONNX models GitHub repository. It downloads directly to the current directory.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/inception_and_googlenet/googlenet/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget https://github.com/onnx/models/raw/main/vision/classification/inception_and_googlenet/googlenet/model/googlenet-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies with MXNet and GluonCV (Python)\nDESCRIPTION: This code snippet imports necessary libraries for training, including MXNet, NumPy, GluonCV, and Matplotlib. It verifies that all dependencies are installed correctly.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/vgg/train_vgg.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport time, logging\n\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, nd\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.data import imagenet\nfrom gluoncv.utils import makedirs, TrainingHistory\n\nimport os\nfrom mxnet.context import cpu\nfrom mxnet.gluon.block import HybridBlock\nfrom mxnet.initializer import Xavier\nfrom mxnet.gluon.contrib.nn import HybridConcurrent\nfrom __future__ import division\nimport multiprocessing\n```\n\n----------------------------------------\n\nTITLE: Model Quantization Bash Script\nDESCRIPTION: This bash script performs model quantization using Intel Neural Compressor. It navigates to the quantization directory, then executes `run_tuning.sh` to quantize the model. The script requires specifying the input model path, dataset location, and output model path.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd neural-compressor/examples/onnxrt/body_analysis/onnx_model_zoo/arcface/quantization/ptq_static\nbash run_tuning.sh --input_model=path/to/model \\  # model path as *.onnx\n                   --dataset_location=/path/to/faces_ms1m_112x112/task.bin \\\n                   --output_model=path/to/save\n```\n\n----------------------------------------\n\nTITLE: Load and Display First Image\nDESCRIPTION: This snippet loads the first image ('player1.jpg') using OpenCV and displays it using Matplotlib after converting its color space from BGR to RGB.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_inference.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Load first image\nimg1 = cv2.imread('player1.jpg')\n# Display first image\nplt.imshow(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Import dependencies\nDESCRIPTION: This snippet imports the necessary libraries for performing inference, including mxnet, matplotlib, numpy, and onnx-mxnet utilities.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mxnet as mx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import namedtuple\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.onnx.onnx2mx.import_model import import_model\nimport os\n```\n\n----------------------------------------\n\nTITLE: Displaying the Original Image\nDESCRIPTION: This code snippet displays the original image using matplotlib's `imshow` function.  It takes the numpy array representing the original image and shows it in the output.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/yolov4/dependencies/inference.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimshow(np.asarray(original_image))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Input Image in Python\nDESCRIPTION: This code downloads a test image, preprocesses it using Torchvision's `transforms`, and performs inference using the downloaded PyTorch model.  It applies normalization and converts the image to a tensor, then unsqueezes the tensor to add a batch dimension.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/fcn/dependencies/conversion.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata_dir = 'test_data_set_0'\nurl, filename = (\"https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/fcn/dependencies/000000017968.jpg\", \"000000017968.jpg\")\n#url, filename = (\"https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/fcn/dependencies/000000025205.jpg\", \"000000025205.jpg\")\n#urllib.request.urlretrieve(url, filename)\n\ninput_image = Image.open(filename)\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_tensor = input_tensor.unsqueeze(0)\noutput = torch_inference(model, input_tensor)\noutput_tensor, aux_tensor = output['out'], output['aux']\n```\n\n----------------------------------------\n\nTITLE: Postprocessing Detections (Python)\nDESCRIPTION: This Python code snippet demonstrates postprocessing steps for object detection results from a Faster R-CNN model. It displays the detected objects on the original image with bounding boxes and class annotations. It uses matplotlib to draw the image and the detections. The function filters detections based on a score threshold, and requires a 'coco_classes.txt' file listing class names.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/object_detection_segmentation/faster-rcnn/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nfrom PIL import Image\n\nclasses = [line.rstrip('\\n') for line in open('coco_classes.txt')]\n\ndef display_objdetect_image(image, boxes, labels, scores, score_threshold=0.7):\n    # Resize boxes\n    ratio = 800.0 / min(image.size[0], image.size[1])\n    boxes /= ratio\n\n    _, ax = plt.subplots(1, figsize=(12,9))\n    image = np.array(image)\n    ax.imshow(image)\n\n    # Showing boxes with score > 0.7\n    for box, label, score in zip(boxes, labels, scores):\n        if score > score_threshold:\n            rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='b', facecolor='none')\n            ax.annotate(classes[label] + ':' + str(np.round(score, 2)), (box[0], box[1]), color='w', fontsize=12)\n            ax.add_patch(rect)\n    plt.show()\n\nimg = Image.open('dependencies/demo.jpg')\ndisplay_objdetect_image(img, [0,0,0,0], [0], [0])\n```\n\n----------------------------------------\n\nTITLE: R-CNN ILSVRC13 Input Definition\nDESCRIPTION: Defines the input tensor shape and data type for the R-CNN ILSVRC13 model. The input is a 4D float tensor representing an image with dimensions [1, 3, 224, 224], where 1 is the batch size, 3 is the number of color channels (RGB), and 224x224 is the image resolution.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/rcnn_ilsvrc13/README.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ndata_0: float[1, 3, 224, 224]\n```\n\n----------------------------------------\n\nTITLE: Import ONNX Model - Python\nDESCRIPTION: This code snippet imports an ONNX model into MXNet using the `import_model` function from `mxnet.contrib.onnx.onnx2mx.import_model`.  It takes the `model_path` as input and returns the MXNet symbol (sym), argument parameters (arg_params), and auxiliary parameters (aux_params).\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/imagenet_validation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsym, arg_params, aux_params = import_model(model_path)\n```\n\n----------------------------------------\n\nTITLE: Import Dependencies in Python\nDESCRIPTION: This code snippet imports the necessary Python libraries for the ArcFace model validation, including numpy, scipy, sklearn, cv2, math, datetime, pickle, mxnet, and easydict.  It includes imports for ONNX model import and MXNet array manipulation.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/body_analysis/arcface/dependencies/arcface_validation.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nfrom scipy import misc\nfrom sklearn.model_selection import KFold\nfrom scipy import interpolate\nimport sklearn\nimport cv2\nimport math\nimport datetime\nimport pickle\nimport mxnet as mx\nfrom mxnet import ndarray as nd\nfrom easydict import EasyDict as edict\nfrom mxnet.contrib.onnx.onnx2mx.import_model import import_model\n```\n\n----------------------------------------\n\nTITLE: Output Shape Specification for ZFNet-512\nDESCRIPTION: Describes the output tensor shape produced by the ZFNet-512 ONNX model.  The model outputs a 2D tensor with dimensions: batch size (1) and number of classes (1000), representing the probabilities for each class.\nSOURCE: https://github.com/onnx/models/blob/main/validated/vision/classification/zfnet-512/README.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\ngpu_0/softmax_1: float[1, 1000]\n```\n\n----------------------------------------\n\nTITLE: Download All ONNX Models with Git LFS\nDESCRIPTION: This command downloads all ONNX model files in the repository using Git LFS. The `--include=\"*\"` option specifies that all files should be included, while `--exclude=\"\"` ensures that no files are excluded. Git LFS must be installed prior to running this command to correctly manage large files.\nSOURCE: https://github.com/onnx/models/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs pull --include=\"*\" --exclude=\"\"\n```"
  }
]
[
  {
    "owner": "apify",
    "repo": "apify-docs",
    "content": "TITLE: Passing Cookies to New Context in Puppeteer\nDESCRIPTION: This code snippet shows how to create a new incognito browser context in Puppeteer, set previously retrieved cookies, and navigate to a page while maintaining logged-in status.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n// Create a fresh non-persistent browser context\nconst sendEmailContext = await browser.createIncognitoBrowserContext();\n// Create a new page on the new browser context and set its cookies\n// to be the same ones from the page we used to log into the website.\nconst page2 = await sendEmailContext.newPage();\nawait page2.setCookie(...cookies);\n\n// Notice that we are logged in, even though we didn't\n// go through the logging in process again!\nawait page2.goto('https://mail.yahoo.com/');\nawait page2.waitForTimeout(10000);\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation of TypeScript Scraper\nDESCRIPTION: This snippet shows the final code for the TypeScript scraper, including all necessary imports, type definitions, and function implementations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// index.ts\nimport axios from 'axios';\nimport { SortOrder } from './types';\n\nimport type { ResponseData, Product, UserInput, ModifiedProduct } from './types';\n\nconst fetchData = async () => {\n    const { data } = await axios('https://dummyjson.com/products?limit=100');\n\n    return data as ResponseData;\n};\n\nconst sortData = (products: Product[], order: SortOrder) => {\n    switch (order) {\n        case SortOrder.ASC:\n            return [...products].sort((a, b) => a.price - b.price);\n        case SortOrder.DESC:\n            return [...products].sort((a, b) => b.price - a.price);\n        default:\n            return products;\n    }\n};\n\nasync function scrape(input: UserInput<true>): Promise<ModifiedProduct[]>;\nasync function scrape(input: UserInput<false>): Promise<Product[]>;\nasync function scrape(input: UserInput) {\n    const data = await fetchData();\n\n    const sorted = sortData(data.products, input.sort as SortOrder);\n\n    if (input.removeImages) {\n        return sorted.map((item) => {\n            const { images, ...rest } = item;\n\n            return rest;\n        });\n    }\n\n    return sorted;\n}\n\nconst main = async () => {\n    const INPUT: UserInput<false> = { sort: 'ascending', removeImages: false };\n\n    const result = await scrape(INPUT);\n\n    console.log(result[0].images);\n};\n\nmain();\n```\n\n----------------------------------------\n\nTITLE: Scraping GitHub Repositories with Pagination using Puppeteer\nDESCRIPTION: This code snippet demonstrates how to scrape GitHub repositories from multiple pages using Puppeteer. It includes functions for scraping individual pages, handling pagination, and concurrent scraping of multiple pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\nimport * as cheerio from 'cheerio';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\n// Scrapes all repositories from a single page\nconst scrapeRepos = async (page) => {\n    const $ = cheerio.load(await page.content());\n\n    return [...$('.list-view-item')].map((item) => {\n        const repoElement = $(item);\n        return {\n            title: repoElement.find('h4').text().trim(),\n            description: repoElement.find('.repos-list-description').text().trim(),\n            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,\n        };\n    });\n};\n\nconst browser = await puppeteer.launch({ headless: false });\nconst firstPage = await browser.newPage();\n\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageLabel = await firstPage.$eval(\n    'a[aria-label*=\"Page \"]:nth-last-child(2)',\n    (element) => element.getAttribute('aria-label'),\n);\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\n\n// Push all results from the first page to the repositories array\nrepositories.push(...(await scrapeRepos(page)));\n\nawait firstPage.close();\n\nconst pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);\nconst promises = pageNumbers.map((pageNumber) => (async () => {\n    const paginatedPage = await browser.newPage();\n\n    // Construct the URL by setting the ?page=... parameter to value of pageNumber\n    const url = new URL(REPOSITORIES_URL);\n    url.searchParams.set('page', pageNumber);\n\n    // Scrape the page\n    await paginatedPage.goto(url.href);\n    const results = await scrapeRepos(paginatedPage);\n\n    // Push results to the repositories array\n    repositories.push(...results);\n\n    await paginatedPage.close();\n})(),\n);\nawait Promise.all(promises);\n\n// For brievity logging just the count of repositories scraped\nconsole.log(repositories.length);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Complete Google Search Automation with Playwright\nDESCRIPTION: A full example of automating a Google search, including accepting cookies, typing a query, and clicking the first result using Playwright.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\n// Click the \"Accept all\" button\nawait page.click('button:has-text(\"Accept all\")');\n\n// Type the query into the search box\nawait page.type('textarea[title]', 'hello world');\n\n// Press enter\nawait page.keyboard.press('Enter');\n\n// Click the first result\nawait page.click('.g a');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Creating Residential Proxy Configuration in Apify\nDESCRIPTION: Creates a proxy configuration using Apify's residential proxy group. This allows the crawler to rotate through different residential IP addresses to avoid being blocked.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy-Loading Pagination with Puppeteer\nDESCRIPTION: This code snippet demonstrates how to implement lazy-loading pagination for web scraping using Puppeteer. It includes logic for auto-scrolling and handling the end of the page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\n// Create an array where all scraped products will\n// be pushed to\nconst products = [];\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nconst totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel({ deltaY: itemHeight * 3 });\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    // Data extraction login will go here\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Enhanced Playwright Scraper with Dynamic Content Handling\nDESCRIPTION: Complete implementation of a Playwright scraper that handles both static and dynamically loaded content, including recommended products section.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/headless_browser.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            return;\n        }\n\n        const $ = await parseWithCheerio();\n\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n        const recommendedProducts = $('.product-recommendations a.product-item__title')\n            .map((i, el) => $(el).text().trim())\n            .toArray();\n\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n            recommendedProducts,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Implementing Lazy-Loading Pagination with Playwright\nDESCRIPTION: This code snippet demonstrates how to implement lazy-loading pagination for web scraping using Playwright. It includes logic for auto-scrolling and handling the end of the page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\n// Create an array where all scraped products will\n// be pushed to\nconst products = [];\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nconst totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel(0, itemHeight * 3);\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    // Data extraction login will go here\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Complete Shopify Product Scraper Implementation in JavaScript\nDESCRIPTION: This is the full implementation of the Shopify product scraper, combining all previous snippets. It fetches the sales page, extracts product URLs, and then scrapes detailed information for each product, handling errors and logging progress.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconsole.log('Fetching products on sale.');\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n\nconsole.log(`Found ${productUrls.length} products.`);\n\nconst results = [];\nconst errors = [];\n\nfor (const url of productUrls) {\n    try {\n        console.log(`Fetching URL: ${url}`);\n        const productResponse = await gotScraping(url);\n        const $productPage = cheerio.load(productResponse.body);\n\n        const title = $productPage('h1').text().trim();\n        const vendor = $productPage('a.product-meta__vendor').text().trim();\n        const price = $productPage('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($productPage('span.rating__caption').text(), 10);\n        const description = $productPage('div[class*=\"description\"] div.rte').text().trim();\n\n        results.push({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    } catch (error) {\n        errors.push({ url, msg: error.message });\n    }\n}\n\nconsole.log('RESULTS:', results);\nconsole.log('ERRORS:', errors);\n```\n\n----------------------------------------\n\nTITLE: Launching Browser with Playwright/Puppeteer - Basic Example\nDESCRIPTION: Basic example of launching a browser instance using both Playwright and Puppeteer. By default, the browser launches in headless mode (no UI).\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nawait chromium.launch();\n\nconsole.log('launched!');\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nawait puppeteer.launch();\n\nconsole.log('launched!');\n```\n\n----------------------------------------\n\nTITLE: Defining Router and Handlers for Amazon Scraping in JavaScript\nDESCRIPTION: This code snippet defines a Cheerio router with handlers for the initial search page and individual product pages. It demonstrates how to select products, extract data, and add new requests to the crawler.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/modularity.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// routes.js\nimport { createCheerioRouter } from 'crawlee';\nimport { BASE_URL } from './constants.js';\n\nexport const router = createCheerioRouter();\n\nrouter.addDefaultHandler(({ log }) => {\n    log.info('Route reached.');\n});\n\n// Add a handler to our router to handle requests with the 'START' label\nrouter.addHandler('START', async ({ $, crawler, request }) => {\n    const { keyword } = request.userData;\n\n    const products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\n    // loop through the resulting products\n    for (const product of products) {\n        const element = $(product);\n        const titleElement = $(element.find('.a-text-normal[href]'));\n\n        const url = `${BASE_URL}${titleElement.attr('href')}`;\n\n        // scrape some data from each and to a request\n        // to the crawler for its page\n        await crawler.addRequests([{\n            url,\n            label: 'PRODUCT',\n            userData: {\n                // Pass the scraped data about the product to the next\n                // request so that it can be used there\n                data: {\n                    title: titleElement.first().text().trim(),\n                    asin: element.attr('data-asin'),\n                    itemUrl: url,\n                    keyword,\n                },\n            },\n        }]);\n    }\n});\n\nrouter.addHandler('PRODUCT', ({ log }) => log.info('on a product page!'));\n```\n\n----------------------------------------\n\nTITLE: Sending Multiple Emails Concurrently with Playwright\nDESCRIPTION: This code snippet demonstrates how to send multiple emails concurrently using Playwright. It creates an array of promises, each handling the email sending process in a separate browser context with stored cookies.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n// Grab the cookies from the default browser context,\n// which was used to log in\nconst cookies = await browser.contexts()[0].cookies();\n\nawait page.close();\n\n// Create an array of promises, running the cookie passing\n// and email sending logic each time\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    // Create a fresh non-persistent browser context\n    const sendEmailContext = await browser.newContext();\n    // Add the cookies from the previous one to this one so that\n    // we'll be logged into Yahoo without having to re-do the\n    // logging in automation\n    await sendEmailContext.addCookies(cookies);\n    const page2 = await sendEmailContext.newPage();\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    // Compose an email\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    // Populate the fields with the details from the object\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    // Send the email\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\n// Wait for all emails to be sent\nawait Promise.all(promises);\n```\n\n----------------------------------------\n\nTITLE: Web Scraping Shopify Products with Python\nDESCRIPTION: A comprehensive Python script that scrapes product information from a Shopify store, including prices, variants, and other details. It uses httpx for HTTP requests, BeautifulSoup for HTML parsing, and includes functions for data extraction and export to CSV and JSON formats.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n\ndef parse_product(product, base_url):\n    title_element = product.select_one(\".product-item__title\")\n    title = title_element.text.strip()\n    url = urljoin(base_url, title_element[\"href\"])\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price, \"url\": url}\n\ndef parse_variant(variant):\n    text = variant.text.strip()\n    name, price_text = text.split(\" - \")\n    price = Decimal(\n        price_text\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    return {\"variant_name\": name, \"price\": price}\n\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)\n\nlisting_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product, listing_url)\n    product_soup = download(item[\"url\"])\n    vendor = product_soup.select_one(\".product-meta__vendor\").text.strip()\n\n    if variants := product_soup.select(\".product-form__option.no-js option\"):\n        for variant in variants:\n            data.append(item | parse_variant(variant))\n    else:\n        item[\"variant_name\"] = None\n        data.append(item)\n\nwith open(\"products.csv\", \"w\") as file:\n    export_csv(file, data)\n\nwith open(\"products.json\", \"w\") as file:\n    export_json(file, data)\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to JSON with Crawlee\nDESCRIPTION: Example showing how to export scraped data to a JSON file using Dataset.exportToJSON() after the crawler completes its run.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// ...\nawait crawler.run();\n// Add this line to export to JSON.\nawait Dataset.exportToJSON('results');\n```\n\n----------------------------------------\n\nTITLE: Complete Lazy Loading Implementation with Playwright\nDESCRIPTION: Complete code for scraping a lazy-loaded e-commerce website using Playwright. It launches a browser, navigates to a product listing page, scrolls down to trigger lazy loading, and extracts product information until reaching a target number or the end of the page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\nimport * as cheerio from 'cheerio';\n\nconst products = [];\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nlet totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel(0, itemHeight * 3);\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    const $ = cheerio.load(await page.content());\n\n    // Grab the newly loaded items\n    const items = [...$('a[data-testid*=\"productTile\"]')].slice(products.length);\n\n    const newItems = items.map((item) => {\n        const elem = $(item);\n\n        return {\n            brand: elem.find('p[data-testid=\"brandName\"]').text().trim(),\n            price: elem.find('span[data-testid=\"finalPrice\"]').text().trim(),\n        };\n    });\n\n    products.push(...newItems);\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nconsole.log(products.slice(0, 75));\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Complete Scraper Actor Implementation\nDESCRIPTION: Full TypeScript implementation of the Scraper Actor that uses Cheerio to scrape an e-commerce website, utilizing the shared request queue and dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// ScraperActorMainTs reference\n```\n\n----------------------------------------\n\nTITLE: Saving HTML to File using Python\nDESCRIPTION: Python script demonstrating how to save downloaded HTML content to a file using pathlib\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom pathlib import Path\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\nPath(\"products.html\").write_text(response.text)\n```\n\n----------------------------------------\n\nTITLE: Parsing Cookies from API Response with set-cookie-parser\nDESCRIPTION: Demonstrates how to extract and format cookies from an API response using axios and set-cookie-parser. The code makes a request to a target site, parses the returned cookies, and formats them into a usable string for subsequent requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport axios from 'axios';\n\n// import the set-cookie-parser module\nimport setCookieParser from 'set-cookie-parser';\n\nconst getCookie = async () => {\n    // make a request to the target site\n    const response = await axios.get('https://www.example.com/');\n\n    // parse the cookies from the response\n    const cookies = setCookieParser.parse(response);\n\n    // format the parsed data into a usable string\n    const cookieString = cookies.map(({ name, value }) => `${name}=${value};`).join(' ');\n\n    // log the final cookie string to be used in a 'cookie' header\n    console.log(cookieString);\n};\n\ngetCookie();\n```\n\n----------------------------------------\n\nTITLE: Node.js Context Parsing with Cheerio - Puppeteer Version\nDESCRIPTION: Complete example showing how to use Cheerio to parse page content in the Node.js context with Puppeteer. Extracts product information using Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\nimport { load } from 'cheerio';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\nconst $ = load(await page.content());\n\nconst productCards = Array.from($('a[class*=\"ProductCard_root\"]'));\n\nconst products = productCards.map((element) => {\n    const card = $(element);\n\n    const name = card.find('h3[class*=\"ProductCard_name\"]').text();\n    const price = card.find('div[class*=\"ProductCard_price\"]').text();\n\n    return {\n        name,\n        price,\n    };\n});\n\nconsole.log(products);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Navigating to Website with Playwright\nDESCRIPTION: Demonstrates how to open a new page, navigate to Google.com, and implement a wait timeout using Playwright. Includes browser cleanup.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\n// Visit Google\nawait page.goto('https://google.com');\n\n// wait for 10 seconds before shutting down\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Extraction with Crawlee\nDESCRIPTION: A complete example showing how to use CheerioCrawler to scrape product data from a Shopify store and save it to disk using Dataset.pushData(). The code demonstrates URL crawling, data extraction, and storage functionality.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n// To save data to disk, we need to import Dataset.\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            // When on the start URL, we don't want to\n            // extract any data after we extract the links.\n            return;\n        }\n\n        // We copied and pasted the extraction code\n        // from the previous lesson with small\n        // refactoring: e.g. `$productPage` to `$`.\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n\n        // Instead of printing the results to\n        // console, we save everything to a file.\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Google Search Crawler with Proxy Rotation\nDESCRIPTION: Complete example of a Google search crawler implementing proxy rotation and handling common blocking scenarios like CAPTCHAs and failed page loads.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/handle_blocked_requests_puppeteer.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    requestList: someInitializedRequestList,\n    launchPuppeteerOptions: {\n        useApifyProxy: true,\n    },\n    gotoFunction: async ({ request, page, puppeteerPool }) => {\n        const response = page.goto(request.url).catch(() => null);\n        if (!response) {\n            await puppeteerPool.retire(page.browser());\n            throw new Error(`Page didn't load for ${request.url}`);\n        }\n        return response;\n    },\n    handlePageFunction: async ({ request, page, puppeteerPool }) => {\n        if (page.url().includes('sorry')) {\n            await puppeteerPool.retire(page.browser());\n            throw new Error(`We got captcha for ${request.url}`);\n        }\n    },\n    retireInstanceAfterRequestCount: 50,\n});\n\nApify.main(async () => {\n    await crawler.run();\n});\n```\n\n----------------------------------------\n\nTITLE: Complete LangGraph-Apify Integration Example\nDESCRIPTION: A complete Python script that demonstrates how to create a ReAct agent using LangGraph with Apify Actors tools to search for and analyze TikTok profiles. Includes all necessary imports, environment setup, and execution code.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain_apify import ApifyActorsTool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser = ApifyActorsTool(\"apify/rag-web-browser\")\ntiktok = ApifyActorsTool(\"clockworks/free-tiktok-scraper\")\n\ntools = [browser, tiktok]\nagent_executor = create_react_agent(llm, tools)\n\nfor state in agent_executor.stream(\n    stream_mode=\"values\",\n    input={\n        \"messages\": [\n            HumanMessage(content=\"Search the web for OpenAI TikTok profile and analyze their profile.\")\n        ]\n    }):\n    state[\"messages\"][-1].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Web Scraper with Crawlee in JavaScript\nDESCRIPTION: This code snippet demonstrates a basic web scraper using Crawlee to crawl a demo webstore, enqueue product links, and extract product data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// crawlee.js\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        if (request.label === 'START') {\n            await enqueueLinks({\n                selector: 'a[href*=\"/product/\"]',\n            });\n\n            // When on the START page, we don't want to\n            // extract any data after we extract the links.\n            return;\n        }\n\n        // We copied and pasted the extraction code\n        // from the previous lesson\n        const title = $('h3').text().trim();\n        const price = $('h3 + div').text().trim();\n        const description = $('div[class*=\"Text_body\"]').text().trim();\n\n        // Instead of saving the data to a variable,\n        // we immediately save everything to a file.\n        await Dataset.pushData({\n            title,\n            description,\n            price,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://demo-webstore.apify.org/search/on-sale',\n    // By labeling the Request, we can identify it\n    // later in the requestHandler.\n    label: 'START',\n}]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Input Validation in Orchestrator Actor\nDESCRIPTION: TypeScript code for initializing the Actor and validating its input parameters, ensuring the required targetActorId is present and setting default values for other parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor, log } from 'apify';\n\ninterface Input {\n    parallelRunsCount: number;\n    targetActorId: string;\n    targetActorInput: Record<string, any>;\n    targetActorRunOptions: Record<string, any>;\n}\n\nawait Actor.init();\n\nconst {\n    parallelRunsCount = 1,\n    targetActorId,\n    targetActorInput = {},\n    targetActorRunOptions = {},\n} = await Actor.getInput<Input>() ?? {} as Input;\nconst { apifyClient } = Actor;\n\nif (!targetActorId) throw new Error('Missing the \"targetActorId\" input!');\n```\n\n----------------------------------------\n\nTITLE: Navigating to Website with Puppeteer\nDESCRIPTION: Shows how to open a new page, navigate to Google.com, and implement a wait timeout using Puppeteer. Includes browser cleanup.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\n// Visit Google\nawait page.goto('https://google.com');\n\n// wait for 10 seconds before shutting down\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Complete Email Sending Automation with Puppeteer\nDESCRIPTION: This code snippet shows the complete implementation of email sending automation using Puppeteer. It includes logging into Yahoo, storing cookies, and sending multiple emails concurrently.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst emailsToSend = [\n    {\n        to: 'alice@example.com',\n        subject: 'Hello',\n        body: 'This is a message.',\n    },\n    {\n        to: 'bob@example.com',\n        subject: 'Testing',\n        body: 'I love the academy!',\n    },\n    {\n        to: 'carol@example.com',\n        subject: 'Apify is awesome!',\n        body: 'Some content.',\n    },\n];\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Login logic\nawait page.goto('https://www.yahoo.com/');\n\nawait Promise.all([page.waitForSelector('a[data-ylk*=\"sign-in\"]'), page.click('button[name=\"agree\"]')]);\nawait Promise.all([page.waitForNavigation(), page.click('a[data-ylk*=\"sign-in\"]')]);\n\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('input[name=\"signin\"]')]);\n\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('button[name=\"verifyPassword\"]')]);\n\nconst cookies = await page.cookies();\nawait page.close();\n\n// Email sending logic\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    const sendEmailContext = await browser.createIncognitoBrowserContext();\n    const page2 = await sendEmailContext.newPage();\n    await page2.setCookie(...cookies);\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\nawait Promise.all(promises);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Complete LangChain Integration Example\nDESCRIPTION: Full implementation combining all components for crawling web content, creating a vector index, and performing queries using LangChain and Apify.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_apify import ApifyWrapper\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n\napify = ApifyWrapper()\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nprint(\"Call website content crawler ...\")\nloader = apify.call_actor(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\"startUrls\": [{\"url\": \"https://python.langchain.com/docs/get_started/introduction\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"},\n    dataset_mapping_function=lambda item: Document(page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}),\n)\nprint(\"Compute embeddings...\")\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=InMemoryVectorStore,\n    embedding=OpenAIEmbeddings()\n).from_loaders([loader])\nquery = \"What is LangChain?\"\nresult = index.query_with_sources(query, llm=llm)\n\nprint(\"answer:\", result[\"answer\"])\nprint(\"source:\", result[\"sources\"])\n```\n\n----------------------------------------\n\nTITLE: Complete Web Scraper with Data Export\nDESCRIPTION: Full implementation of a web scraper using PlaywrightCrawler that crawls product data and exports it to CSV. Includes product detail extraction and handling of recommended products.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // We removed the headless: false option to hide the browser windows.\n    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            return;\n        }\n\n        // Fourth, parse the browser's page with Cheerio.\n        const $ = await parseWithCheerio();\n\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n        const recommendedProducts = $('.product-recommendations a.product-item__title')\n            .map((i, el) => $(el).text().trim())\n            .toArray();\n\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n            recommendedProducts,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();\nawait Dataset.exportToCSV('results');\n```\n\n----------------------------------------\n\nTITLE: Implementing Payment Verification with Presence Check in JavaScript\nDESCRIPTION: Demonstrates proper payment verification by checking for the presence of a success indicator element. Uses page.waitForSelector to ensure robust verification.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nasync function isPaymentSuccessful() {\n    try {\n        await page.waitForSelector('#PaymentAccepted');\n    } catch (error) {\n        return OUTPUT.paymentFailure;\n    }\n\n    return OUTPUT.paymentSuccess;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Rotation with PuppeteerCrawler in JavaScript\nDESCRIPTION: Example showing how to configure PuppeteerCrawler with proxy rotation and session management to avoid rate limiting. Uses ProxyConfiguration and SessionPool with maxUsageCount setting to rotate proxies after 15 requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/rate_limiting.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\nimport { Actor } from 'apify';\n\nconst myCrawler = new PuppeteerCrawler({\n    proxyConfiguration: await Actor.createProxyConfiguration({\n        groups: ['RESIDENTIAL'],\n    }),\n    sessionPoolOptions: {\n        // Note that a proxy is tied to a session\n        sessionOptions: {\n            // Let's say the website starts blocking requests after\n            // 20 requests have been sent in the span of 1 minute from\n            // a single user.\n            // We can stay on the safe side and retire the browser\n            // and rotate proxies after 15 pages (requests) have been opened.\n            maxUsageCount: 15,\n        },\n    },\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee via NPM\nDESCRIPTION: Command to install Crawlee package from npm registry into your project\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install crawlee\n```\n\n----------------------------------------\n\nTITLE: Complete Page Function Implementation\nDESCRIPTION: Full implementation of a page function that handles pagination and data scraping, including waiting for dynamic content and processing item details.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { request,\n        log,\n        skipLinks,\n        jQuery: $,\n        waitFor,\n    } = context;\n\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        let timeoutMillis; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await waitFor(buttonSelector, { timeoutMillis });\n                // 2 sec timeout after the first.\n                timeoutMillis = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            $(buttonSelector).click();\n        }\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Page Function for Apify Scraper in JavaScript\nDESCRIPTION: This code snippet shows a complete implementation of a pageFunction for an Apify scraper. It handles both START and DETAIL page types, extracts URL and unique identifier, and returns the scraped data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { request, log, skipLinks } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing Product Variant Prices with Python\nDESCRIPTION: This function parses the variant name and price from a given variant text. It splits the text into name and price, removes currency symbols and commas, and converts the price to a Decimal object.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef parse_variant(variant):\n    text = variant.text.strip()\n    name, price_text = text.split(\" - \")\n    price = Decimal(\n        price_text\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    return {\"variant_name\": name, \"price\": price}\n```\n\n----------------------------------------\n\nTITLE: Complete Page Function with jQuery Scraping\nDESCRIPTION: Full implementation of a page function that handles multiple page types, injects jQuery, and scrapes data using jQuery selectors within page.evaluate().\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error(`Unknown label: ${context.request.userData.label}`);\n    }\n\n    async function handleStart({ log, page }) {\n        log.info('Store opened!');\n        let timeout; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                await page.waitFor(buttonSelector, { timeout });\n                timeout = 2000;\n            } catch (err) {\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            await page.click(buttonSelector);\n        }\n    }\n\n    async function handleDetail(contextInner) {\n        const {\n            request,\n            log,\n            skipLinks,\n            page,\n            Apify,\n        } = contextInner;\n\n        // Inject jQuery\n        await Apify.utils.puppeteer.injectJQuery(page);\n\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Use jQuery only inside page.evaluate (inside browser)\n        const results = await page.evaluate(() => {\n            return {\n                title: $('header h1').text(),\n                description: $('header span.actor-description').text(),\n                modifiedDate: new Date(\n                    Number(\n                        $('ul.ActorHeader-stats time').attr('datetime'),\n                    ),\n                ).toISOString(),\n                runCount: Number(\n                    $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                        .text()\n                        .match(/[\\d,]+/)[0]\n                        .replace(/,/g, ''),\n                ),\n            };\n        });\n\n        return {\n            url,\n            uniqueIdentifier,\n            // Add results from browser to output\n            ...results,\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cookies with Puppeteer\nDESCRIPTION: This code snippet shows how to retrieve cookies from the current page in Puppeteer after logging in.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// Grab the cookies from the page used to log in\nconst cookies = await page.cookies();\n```\n\n----------------------------------------\n\nTITLE: Complete Example: Integrating Apify with OpenAI Vector Store and Assistant\nDESCRIPTION: A complete end-to-end example combining all previous steps into a single script. This demonstrates the entire workflow from creating an assistant to loading data and querying it.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom apify_client import ApifyClient\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR-OPENAI-API-KEY\")\napify_client = ApifyClient(\"YOUR-APIFY-API-TOKEN\")\n\nmy_assistant = client.beta.assistants.create(\n    instructions=\"As a customer support agent at Apify, your role is to assist customers\",\n    name=\"Support assistant\",\n    tools=[{\"type\": \"file_search\"}],\n    model=\"gpt-4o-mini\",\n)\n\n# Create a vector store\nvector_store = client.beta.vector_stores.create(name=\"Support assistant vector store\")\n\n# Update the assistant to use the new Vector Store\nassistant = client.beta.assistants.update(\n    assistant_id=my_assistant.id,\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n\nrun_input = {\"startUrls\": [{\"url\": \"https://docs.apify.com/platform\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"}\nactor_call_website_crawler = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n\ndataset_id = actor_call_website_crawler[\"defaultDatasetId\"]\n\nrun_input_vs = {\n    \"datasetId\": dataset_id,\n    \"assistantId\": my_assistant.id,\n    \"datasetFields\": [\"text\", \"url\"],\n    \"openaiApiKey\": \"YOUR-OPENAI-API-KEY\",\n    \"vectorStoreId\": vector_store.id,\n}\n\napify_client.actor(\"jiri.spilka/openai-vector-store-integration\").call(run_input=run_input_vs)\n\n# Create a thread and a message\nthread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"How can I scrape a website using Apify?\"\n)\n\n# Run with assistant and poll for the results\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    tool_choice={\"type\": \"file_search\"}\n)\n\nprint(\"Assistant response:\")\nfor m in client.beta.threads.messages.list(thread_id=run.thread_id):\n    print(m.content[0].text.value)\n```\n\n----------------------------------------\n\nTITLE: Downloading a Text File using request-promise in JavaScript\nDESCRIPTION: This code demonstrates how to download a text file using the request-promise module in JavaScript. It sends a GET request to a specified URL and stores the response in the fileData variable.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fileData = await request('https://some-site.com/file.txt');\n```\n\n----------------------------------------\n\nTITLE: Running Google Search Scraper with Node.js\nDESCRIPTION: Example of calling Apify task synchronously via API to scrape Google search results using got HTTP client. The code demonstrates how to authenticate, make the API request, and process the response data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// Use your favorite HTTP client\nimport got from 'got';\n\n// Specify your API token\n// (find it at https://console.apify.com/account#/integrations)\nconst myToken = '<YOUR_APIFY_TOKEN>';\n\n// Start apify/google-search-scraper Actor\n// and pass some queries into the JSON body\nconst response = await got({\n    url: `https://api.apify.com/v2/acts/apify~google-search-scraper/run-sync-get-dataset-items?token=${myToken}`,\n    method: 'POST',\n    json: {\n        queries: 'web scraping\\nweb crawling',\n    },\n    responseType: 'json',\n});\n\nconst items = response.body;\n\n// Log each non-promoted search result for both queries\nitems.forEach((item) => {\n    const { nonPromotedSearchResults } = item;\n    nonPromotedSearchResults.forEach((result) => {\n        const { title, url, description } = result;\n        console.log(`${title}: ${url} --- ${description}`);\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Scrape Function in TypeScript\nDESCRIPTION: This snippet defines the main scrape function that fetches data, sorts it, and optionally removes images based on user input. It uses function overloads to handle different return types.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// index.ts\n// ...\nimport { SortOrder } from './types';\n\nimport type { ResponseData, Product, UserInput, ModifiedProduct } from './types';\n// ...\n\n// Return a promise of either a \"Product\" array, or a \"ModifiedProduct\" array\nasync function scrape(input: UserInput): Promise<Product[] | ModifiedProduct[]> {\n    // Fetch the data\n    const data = await fetchData();\n\n    // Sort the products based on the input's \"sort\" property. We have\n    // to cast it to \"SortOrder\" because despite being equal, technically\n    // the string \"ascending\" isn't the same type as SortOrder.ASC\n    const sorted = sortData(data.products, input.sort as SortOrder);\n\n    // If the user wants to remove images, map through each product removing\n    // the images and return the result\n    if (input.removeImages) {\n        return sorted.map((item) => {\n            const { images, ...rest } = item;\n\n            return rest;\n        });\n    }\n\n    // Otherwise, just return the sorted products\n    return sorted;\n}\n```\n\n----------------------------------------\n\nTITLE: Passing Data Between Requests in Apify\nDESCRIPTION: Shows how to pass data from one request to another by using the userData object. This example adds a new request for a seller detail page while including the item data from the current page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nawait requestQueue.addRequest({\n    url: sellerDetailUrl,\n    userData: {\n        label: 'SELLERDETAIL',\n        data: itemObject,\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Recursively Replacing Nested Shadow DOMs with HTML\nDESCRIPTION: This snippet handles websites with nested shadow DOMs by recursively replacing them with their HTML content. It includes helper functions to extract HTML from shadow roots and process elements deeply.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_shadow_doms.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Returns HTML of given shadow DOM.\nconst getShadowDomHtml = (shadowRoot) => {\n    let shadowHTML = '';\n    for (const el of shadowRoot.childNodes) {\n        shadowHTML += el.nodeValue || el.outerHTML;\n    }\n    return shadowHTML;\n};\n\n// Recursively replaces shadow DOMs with their HTML.\nconst replaceShadowDomsWithHtml = (rootElement) => {\n    for (const el of rootElement.querySelectorAll('*')) {\n        if (el.shadowRoot) {\n            replaceShadowDomsWithHtml(shadowRoot);\n            el.innerHTML += getShadowDomHtml(el.shadowRoot);\n        }\n    }\n};\n\nreplaceShadowDomsWithHtml(document.body);\n```\n\n----------------------------------------\n\nTITLE: Main Execution Script for Amazon Scraper\nDESCRIPTION: This script sets up and runs the Amazon scraper. It initializes the CheerioCrawler, adds the initial request based on the input keyword, and starts the crawling process.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';\nimport { router } from './routes.js';\n\n// Grab our keyword from the input\nconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\n// Add our initial requests\nawait crawler.addRequests([\n    {\n        // Turn the inputted keyword into a link we can make a request with\n        url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\n        label: 'START',\n        userData: {\n            keyword,\n        },\n    },\n]);\n\nlog.info('Starting the crawl.');\nawait crawler.run();\nlog.info('Crawl finished.');\n```\n\n----------------------------------------\n\nTITLE: Complete Orchestrator Actor Implementation\nDESCRIPTION: Full TypeScript implementation of the Orchestrator Actor, including input validation, state management, parallel run orchestration, and waiting for all runs to complete.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// OrchestratorActorMainTs reference\n```\n\n----------------------------------------\n\nTITLE: Implementing Incremental Crawling with Apify SDK and Crawlee\nDESCRIPTION: Demonstrates an example of incremental crawling using Apify SDK and Crawlee. This script crawls Apify Docs, saving new page titles to a dataset and utilizing a persistent request queue for incremental scraping across multiple runs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// Basic example of incremental crawling with Crawlee.\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\ninterface Input {\n    startUrls: string[];\n    persistRquestQueueName: string;\n}\n\nawait Actor.init();\n\n// Structure of input is defined in input_schema.json\nconst {\n    startUrls = ['https://docs.apify.com/'],\n    persistRequestQueueName = 'persist-request-queue',\n} = (await Actor.getInput<Input>()) ?? ({} as Input);\n\n// Open or create request queue for incremental scrape.\n// By opening same request queue, the crawler will continue where it left off and skips already visited URLs.\nconst requestQueue = await Actor.openRequestQueue(persistRequestQueueName);\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    requestQueue, // Pass incremental request queue to the crawler.\n    requestHandler: async ({ enqueueLinks, request, $, log }) => {\n        log.info('enqueueing new URLs');\n        await enqueueLinks();\n\n        // Extract title from the page.\n        const title = $('title').text();\n        log.info(`New page with ${title}`, { url: request.loadedUrl });\n\n        // Save the URL and title of the loaded page to the output dataset.\n        await Dataset.pushData({ url: request.loadedUrl, title });\n    },\n});\n\nawait crawler.run(startUrls);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Complete Web Scraper Implementation with Node.js and Cheerio\nDESCRIPTION: This is the full implementation of a web scraper using Node.js, Got Scraping for HTTP requests, and Cheerio for HTML parsing. It extracts product titles and prices from a Shopify store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_continued.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\n\n// Find all products on the page\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n\n    const priceElement = $(product).find('span.price');\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Extracting Actor Details from Apify Store Page in JavaScript\nDESCRIPTION: This function scrapes various details about an actor from its page on the Apify Store, including title, description, modified date, and run count. It uses Puppeteer's page.$eval method for extracting data from specific elements.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { page, request } = context;\n    const { url } = request;\n\n    // ...\n\n    const uniqueIdentifier = url\n        .split('/')\n        .slice(-2)\n        .join('/');\n\n    const title = await page.$eval(\n        'header h1',\n        ((el) => el.textContent),\n    );\n    const description = await page.$eval(\n        'header span.actor-description',\n        ((el) => el.textContent),\n    );\n\n    const modifiedTimestamp = await page.$eval(\n        'ul.ActorHeader-stats time',\n        (el) => el.getAttribute('datetime'),\n    );\n    const modifiedDate = new Date(Number(modifiedTimestamp));\n\n    const runCountText = await page.$eval(\n        'ul.ActorHeader-stats > li:nth-of-type(3)',\n        ((el) => el.textContent),\n    );\n    const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n    return {\n        url,\n        uniqueIdentifier,\n        title,\n        description,\n        modifiedDate,\n        runCount,\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Multiple Emails Concurrently with Puppeteer\nDESCRIPTION: This code snippet shows how to send multiple emails concurrently using Puppeteer. It creates an array of promises, each handling the email sending process in a separate incognito browser context with stored cookies.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\n// Create an array of promises, running the cookie passing\n// and email sending logic each time\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    // Create a fresh non-persistent browser context\n    const sendEmailContext = await browser.createIncognitoBrowserContext();\n    // Create a new page on the new browser context and set its cookies\n    // to be the same ones from the page we used to log into the website.\n    const page2 = await sendEmailContext.newPage();\n    await page2.setCookie(...cookies);\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    // Compose an email\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    // Populate the fields with the details from the object\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    // Send the email\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\n// Wait for all emails to be sent\nawait Promise.all(promises);\n```\n\n----------------------------------------\n\nTITLE: Product Scraping with Python BeautifulSoup\nDESCRIPTION: Main script that downloads HTML content from a webpage, parses product information, and exports data to CSV and JSON files. Uses httpx for HTTP requests and BeautifulSoup for HTML parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\ndata = []\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    data.append({\"title\": title, \"min_price\": min_price, \"price\": price})\n\nwith open(\"products.csv\", \"w\") as file:\n    writer = csv.DictWriter(file, fieldnames=[\"title\", \"min_price\", \"price\"])\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef serialize(obj):\n    if isinstance(obj, Decimal):\n        return str(obj)\n    raise TypeError(\"Object not JSON serializable\")\n\nwith open(\"products.json\", \"w\") as file:\n    json.dump(data, file, default=serialize)\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in JavaScript\nDESCRIPTION: Shows how to push data to an Apify dataset in JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Append result object to the default dataset associated with the run\nawait Actor.pushData({ someResult: 123 });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Passing Cookies to New Context in Playwright\nDESCRIPTION: This code snippet demonstrates how to create a new browser context in Playwright, add previously retrieved cookies, and navigate to a page while maintaining logged-in status.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Create a fresh non-persistent browser context\nconst sendEmailContext = await browser.newContext();\n// Add the cookies from the previous one to this one so that\n// we'll be logged into Yahoo without having to re-do the\n// logging in automation\nawait sendEmailContext.addCookies(cookies);\nconst page2 = await sendEmailContext.newPage();\n\n// Notice that we are logged in, even though we didn't\n// go through the logging in process again!\nawait page2.goto('https://mail.yahoo.com/');\nawait page2.waitForTimeout(10000);\n```\n\n----------------------------------------\n\nTITLE: Extracting Google Search Results in Web Scraper\nDESCRIPTION: This code extracts search results from a Google search results page. It parses the DOM to retrieve the name, link, and text for each search result and returns them as an array of objects.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/processing_multiple_pages_web_scraper.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const $ = context.jQuery;\n\n    if (context.request.userData.label === 'enqueue') {\n        // copy from the previous part\n    } else if (context.request.userData.label === 'result') {\n        // create result array\n        const result = [];\n\n        // process all the results\n        $('.rc').each((index, elem) => {\n\n            // wrap element in jQuery\n            const gResult = $(elem);\n\n            // lookup link and text\n            const link = gResult.find('.r a');\n            const text = gResult.find('.s .st');\n\n            // extract data and add it to result array\n            result.push({\n                name: link.text(),\n                link: link.attr('href'),\n                text: text.text(),\n            });\n        });\n        // Now we finally return\n\n        return result;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Request Queue with Labeled Request in Apify\nDESCRIPTION: Shows how to create a request queue and add a request with a 'START' label in the userData attribute. This allows for identification and specific handling of different request types later in the crawler.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_0\n\nLANGUAGE: js\nCODE:\n```\n// Create a request list.\nconst requestQueue = await Apify.openRequestQueue();\n// Add the request to the queue\nawait requestQueue.addRequest({\n    url: 'https://www.example.com/',\n    userData: {\n        label: 'START',\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Refactored Scraping Script\nDESCRIPTION: Full refactored version of the scraping script with modular functions for downloading, parsing, and exporting data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n\ndef parse_product(product):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price}\n\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)\n\nlisting_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product)\n    data.append(item)\n\nwith open(\"products.csv\", \"w\") as file:\n    export_csv(file, data)\n\nwith open(\"products.json\", \"w\") as file:\n    export_json(file, data)\n```\n\n----------------------------------------\n\nTITLE: Starting a Simple Express.js Web Server in an Apify Actor\nDESCRIPTION: This code snippet demonstrates how to create a basic web server using Express.js within an Apify Actor. It sets up a single route that responds with 'Hello world' and runs the server on the port specified by the ACTOR_WEB_SERVER_PORT environment variable.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/container_web_server.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// npm install express\nimport { Actor } from 'apify';\nimport express from 'express';\n\nawait Actor.init();\n\nconst app = express();\nconst port = process.env.ACTOR_WEB_SERVER_PORT;\n\napp.get('/', (req, res) => {\n    res.send('Hello world from Express app!');\n});\n\napp.listen(port, () => console.log(`Web server is listening\n    and can be accessed at\n    ${process.env.ACTOR_WEB_SERVER_URL}!`));\n\n// Let the Actor run for an hour\nawait new Promise((r) => setTimeout(r, 60 * 60 * 1000));\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Headers for API Requests\nDESCRIPTION: Shows how to define custom headers for API requests, including common headers like User-Agent and Referer that are typically required for authentication.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst HEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko)'\n        + 'Chrome/96.0.4664.110 YaBrowser/22.1.0.2500 Yowser/2.5 Safari/537.36',\n    Referer: 'https://soundcloud.com',\n    // ...\n};\n```\n\n----------------------------------------\n\nTITLE: Parsing HTML with Cheerio in Node.js\nDESCRIPTION: This snippet shows how to use Got-scraping to download HTML and then parse it with Cheerio. It demonstrates extracting the text content of the h1 element from the parsed HTML.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_js_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\nconst headingElement = $('h1');\nconst headingText = headingElement.text();\n\n// Print page title to terminal\nconsole.log(headingText);\n```\n\n----------------------------------------\n\nTITLE: Optimized Scraping Function for Apify Store Actor Details in JavaScript\nDESCRIPTION: This function demonstrates an optimized approach to scraping actor details from the Apify Store. It uses parallel promises to fetch multiple pieces of information simultaneously, improving performance. It also includes logic for handling different page types.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    // page is Puppeteer's page\n    const { request, log, skipLinks, page } = context;\n\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Get attributes in parallel to speed up the process.\n        const titleP = page.$eval(\n            'header h1',\n            (el) => el.textContent,\n        );\n        const descriptionP = page.$eval(\n            'header span.actor-description',\n            (el) => el.textContent,\n        );\n        const modifiedTimestampP = page.$eval(\n            'ul.ActorHeader-stats time',\n            (el) => el.getAttribute('datetime'),\n        );\n        const runCountTextP = page.$eval(\n            'ul.ActorHeader-stats > li:nth-of-type(3)',\n            (el) => el.textContent,\n        );\n\n        const [\n            title,\n            description,\n            modifiedTimestamp,\n            runCountText,\n        ] = await Promise.all([\n            titleP,\n            descriptionP,\n            modifiedTimestampP,\n            runCountTextP,\n        ]);\n\n        const modifiedDate = new Date(Number(modifiedTimestamp));\n        const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n        return {\n            url,\n            uniqueIdentifier,\n            title,\n            description,\n            modifiedDate,\n            runCount,\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Actor Integration Example\nDESCRIPTION: Complete example showing how to run an Actor and retrieve results using the JavaScript Apify client library.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { ApifyClient } from 'apify-client';\n\nconst client = new ApifyClient({ token: 'YOUR_API_TOKEN' });\n\nconst input = { queries: 'Food in NYC' };\n\n// Run the Actor and wait for it to finish\n// .call method waits infinitely long using smart polling\n// Get back the run API object\nconst run = await client.actor('apify/google-search-scraper').call(input);\n\n// Fetch and print Actor results from the run's dataset (if any)\nconst { items } = await client.dataset(run.defaultDatasetId).listItems();\nitems.forEach((item) => {\n    console.dir(item);\n});\n```\n\n----------------------------------------\n\nTITLE: Executing Browser-Side Code with Playwright\nDESCRIPTION: This snippet shows how to correctly execute browser-side code using page.evaluate() in Playwright, changing the background color of a web page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nawait page.evaluate(() => {\n    document.body.style.background = 'green';\n});\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Complete Actor Data Extraction with URL and Identifier\nDESCRIPTION: A complete implementation that extracts all required data points from an Actor page, including URL, unique identifier, title, description, modified date, and run count, using jQuery and JavaScript string manipulation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { request, jQuery: $ } = context;\n    const { url } = request;\n\n    // ... rest of the code\n\n    const uniqueIdentifier = url.split('/').slice(-2).join('/');\n\n    return {\n        url,\n        uniqueIdentifier,\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Lazy Loading Implementation with Puppeteer\nDESCRIPTION: Complete code for scraping a lazy-loaded e-commerce website using Puppeteer. Similar to the Playwright version, it scrolls through the page to load products and extracts their details, but uses Puppeteer-specific API methods for browser automation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\nimport * as cheerio from 'cheerio';\n\nconst products = [];\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nlet totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel({ deltaY: itemHeight * 3 });\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    const $ = cheerio.load(await page.content());\n\n    // Grab the newly loaded items\n    const items = [...$('a[data-testid*=\"productTile\"]')].slice(products.length);\n\n    const newItems = items.map((item) => {\n        const elem = $(item);\n\n        return {\n            brand: elem.find('p[data-testid=\"brandName\"]').text().trim(),\n            price: elem.find('span[data-testid=\"finalPrice\"]').text().trim(),\n        };\n    });\n\n    products.push(...newItems);\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nconsole.log(products.slice(0, 75));\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Implementing Router Handlers for Amazon Scraper\nDESCRIPTION: This comprehensive snippet shows the implementation of all router handlers for the Amazon scraper. It includes handlers for the start page, product pages, and offer pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createCheerioRouter, Dataset } from 'crawlee';\nimport { BASE_URL, labels } from './constants';\n\nexport const router = createCheerioRouter();\n\nrouter.addHandler(labels.START, async ({ $, crawler, request }) => {\n    const { keyword } = request.userData;\n\n    const products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\n    for (const product of products) {\n        const element = $(product);\n        const titleElement = $(element.find('.a-text-normal[href]'));\n\n        const url = `${BASE_URL}${titleElement.attr('href')}`;\n\n        await crawler.addRequests([\n            {\n                url,\n                label: labels.PRODUCT,\n                userData: {\n                    data: {\n                        title: titleElement.first().text().trim(),\n                        asin: element.attr('data-asin'),\n                        itemUrl: url,\n                        keyword,\n                    },\n                },\n            },\n        ]);\n    }\n});\n\nrouter.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\n    const { data } = request.userData;\n\n    const element = $('div#productDescription');\n\n    await crawler.addRequests([\n        {\n            url: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,\n            label: labels.OFFERS,\n            userData: {\n                data: {\n                    ...data,\n                    description: element.text().trim(),\n                },\n            },\n        },\n    ]);\n});\n\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    for (const offer of $('#aod-offer')) {\n        const element = $(offer);\n\n        await Dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n        });\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Using CheerioCrawler with Proxy Configuration and Session Management\nDESCRIPTION: Shows how to configure CheerioCrawler with proxy configuration and session pooling. The crawler makes HTTP requests to API endpoints and processes JSON responses while maintaining the same session across requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    sessionPoolOptions: { maxPoolSize: 1 },\n    async requestHandler({ json }) {\n        // ...\n        console.log(json);\n    },\n});\n\nawait crawler.run([\n    'https://api.apify.com/v2/browser-info',\n    'https://proxy.apify.com/?format=json',\n]);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Logging into Yahoo with Playwright\nDESCRIPTION: This code snippet demonstrates how to log into Yahoo using Playwright. It navigates through the login process, including accepting cookies, entering credentials, and verifying successful login.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\n// Launch a browser and open a page\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.yahoo.com/');\n\n// Agree to the cookies terms, then click on the \"Sign in\" button\nawait page.click('button[name=\"agree\"]');\nawait page.waitForSelector('a:has-text(\"Sign in\")');\n\nawait page.click('a:has-text(\"Sign in\")');\nawait page.waitForLoadState('load');\n\n// Type in the username and continue forward\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait page.click('input[name=\"signin\"]');\n\n// Type in the password and continue forward\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait page.click('button[name=\"verifyPassword\"]');\nawait page.waitForLoadState('load');\n\n// Wait for 10 seconds so we can see that we have in fact\n// successfully logged in\nawait page.waitForTimeout(10000);\n```\n\n----------------------------------------\n\nTITLE: Scraping Detailed Product Information from Shopify Store in JavaScript\nDESCRIPTION: This code iterates through product URLs, fetches each product page, and extracts detailed information such as title, vendor, price, review count, and description using Cheerio.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst results = [];\nconst errors = [];\n\nfor (const url of productUrls) {\n    try {\n        const productResponse = await gotScraping(url);\n        const $productPage = cheerio.load(productResponse.body);\n\n        const title = $productPage('h1').text().trim();\n        const vendor = $productPage('a.product-meta__vendor').text().trim();\n        const price = $productPage('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($productPage('span.rating__caption').text(), 10);\n        const description = $productPage('div[class*=\"description\"] div.rte').text().trim();\n\n        results.push({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    } catch (error) {\n        errors.push({ url, msg: error.message });\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting a Form with Puppeteer in JavaScript\nDESCRIPTION: This code demonstrates how to submit a form using Puppeteer. It simulates a click on the submit button to send the form data, including the uploaded file.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.click('input[type=submit]');\n```\n\n----------------------------------------\n\nTITLE: Scraping Dynamic Tokens with Puppeteer\nDESCRIPTION: Shows how to use Puppeteer to dynamically scrape authentication tokens by intercepting and analyzing page responses. This example specifically targets the client_id parameter from network requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// import the puppeteer module\nimport puppeteer from 'puppeteer';\n\nconst scrapeClientId = async () => {\n    const browser = await puppeteer.launch({ headless: false });\n    const page = await browser.newPage();\n\n    // initialize a variable that will eventually hold the client_id\n    let clientId = null;\n\n    // handle each response\n    page.on('response', async (res) => {\n        // try to grab the 'client_id' parameter from each URL\n        const id = new URL(res.url()).searchParams.get('client_id') ?? null;\n\n        // if the parameter exists, set our clientId variable to the newly parsed value\n        if (id) clientId = id;\n    });\n\n    // visit the page\n    await page.goto('https://soundcloud.com/tiesto/tracks');\n\n    // wait for a selector that ensures the page has time to load and make requests to its API\n    await page.waitForSelector('.profileHeader__link');\n\n    await browser.close();\n    console.log(clientId); // log the retrieved client_id\n};\n\nscrapeClientId();\n```\n\n----------------------------------------\n\nTITLE: URL Parameter Extraction from Requests\nDESCRIPTION: Complete implementation showing how to extract and parse URL parameters from network requests. Includes browser setup and request handling.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Listen for all requests\npage.on('request', (req) => {\n    // If the URL doesn't include our keyword, ignore it\n    if (!req.url().includes('followings')) return;\n\n    // Convert the request URL into a URL object\n    const url = new URL(req.url());\n\n    // Print the search parameters in object form\n    console.log(Object.fromEntries(url.searchParams));\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Checking Node.js and npm versions in Shell\nDESCRIPTION: These commands are used to verify the installation of Node.js and npm by printing their versions. The lesson requires Node.js version 16 or higher.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/computer_preparation.md#2025-04-18_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnode -v\nnpm -v\n```\n\n----------------------------------------\n\nTITLE: Node.js Context Parsing with Cheerio - Playwright Version\nDESCRIPTION: Complete example showing how to use Cheerio to parse page content in the Node.js context with Playwright. Extracts product information using Cheerio's jQuery-like syntax.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\nimport { load } from 'cheerio';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\nconst $ = load(await page.content());\n\nconst productCards = Array.from($('a[class*=\"ProductCard_root\"]'));\n\nconst products = productCards.map((element) => {\n    const card = $(element);\n\n    const name = card.find('h3[class*=\"ProductCard_name\"]').text();\n    const price = card.find('div[class*=\"ProductCard_price\"]').text();\n\n    return {\n        name,\n        price,\n    };\n});\n\nconsole.log(products);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Initializing Web Scraping Environment in JavaScript\nDESCRIPTION: Sets up the basic environment for web scraping using got-scraping for HTTP requests and cheerio for HTML parsing. It fetches the HTML content of a product detail page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';\nconst response = await gotScraping(productUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\n// Attribute extraction code will go here.\n```\n\n----------------------------------------\n\nTITLE: Configuring Fingerprint Generation in Crawlee (JavaScript)\nDESCRIPTION: This snippet demonstrates how to configure fingerprint generation options in Crawlee using the PlaywrightCrawler. It specifies browser, device, and operating system parameters for fingerprint generation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [{ name: 'firefox', minVersion: 80 }],\n                devices: ['desktop'],\n                operatingSystems: ['windows'],\n            },\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Identifying and Retrying Failed Requests with Apify\nDESCRIPTION: This TypeScript snippet demonstrates how to identify failed requests from a finished scraper run by examining error messages and retry counts. It lists all requests from a queue, filters those that failed, resets their status, and prepares them for reprocessing without having to rerun the entire scraper.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/retry_failed_requests.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// The code is similar for both Crawlee-only but uses a different API\nimport { Actor } from 'apify';\n\nconst REQUEST_QUEUE_ID = 'pFCvCasdvsyvyZdfD'; // Replace with your valid request queue ID\nconst allRequests = [];\nlet exclusiveStartId = null;\n// List all requests from the queue, we have to do it in a loop because the request queue list is paginated\nfor (; ;) {\n    const { items: requests } = await Actor.apifyClient\n        .requestQueue(REQUEST_QUEUE_ID)\n        .listRequests({ exclusiveStartId, limit: 1000 });\n    allRequests.push(...requests);\n    // If we didn't get the full 1,000 requests, we have all and can finish the loop\n    if (requests.length < 1000) {\n        break;\n    }\n\n    // Otherwise, we need to set the exclusiveStartId to the last request id to get the next batch\n    exclusiveStartId = requests[requests.length - 1].id;\n}\n\nconsole.log(`Loaded ${allRequests.length} requests from the queue`);\n\n// Now we filter the failed requests\nconst failedRequests = allRequests.filter((request) => (request.errorMessages?.length || 0) > (request.retryCount || 0));\n\n// We need to update them 1 by 1 to the pristine state\nfor (const request of failedRequests) {\n    request.retryCount = 0;\n    request.errorMessages = [];\n    // This tells the request queue to handle it again\n    request.handledAt = null;\n    await Actor.apifyClient.requestQueue(REQUEST_QUEUE_ID).updateRequest(request);\n}\n\n// And now we can resurrect our scraper again; it will only process the failed requests.\n```\n\n----------------------------------------\n\nTITLE: Basic HTML Text Extraction with BeautifulSoup\nDESCRIPTION: Demonstrates how to fetch a webpage and extract text content from product items using BeautifulSoup's select method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/06_locating_elements.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    print(product.text)\n```\n\n----------------------------------------\n\nTITLE: Extracting Actor Details with Cheerio in JavaScript\nDESCRIPTION: This function uses Cheerio to extract various details about an actor from its detail page, including the title, description, modified date, and run count.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { $ } = context;\n    const { url } = request;\n    // ... rest of your code can come here\n\n    const uniqueIdentifier = url\n        .split('/')\n        .slice(-2)\n        .join('/');\n\n    return {\n        url,\n        uniqueIdentifier,\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Asynchronous Operations in Crawlee's PlaywrightCrawler\nDESCRIPTION: This code snippet demonstrates how to properly handle asynchronous operations in Crawlee's PlaywrightCrawler to prevent 'Target closed' errors. It shows how to use preNavigationHooks and requestHandler to ensure all promises are awaited before the page is closed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/how_to_fix_target_closed.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    // ...other options\n    preNavigationHooks: [\n        async ({ page, context }) => {\n            // Some action that takes time, we don't await here\n            // Try/catch all non awaited code because it can cause unhandled rejection which crashes the whole process\n            const responsePromise = page.waitForResponse('https://example.com/resource').catch((e) => e);\n            // Attach the promise to the context which is accessible to requestHandler\n            context.responsePromise = responsePromise;\n        },\n    ],\n    requestHandler: async ({ request, page, context }) => {\n        // We first wait for the response before doing anything else\n        const response = await context.responsePromise;\n        // Check if it errored out, otherwise proceed with parsing it\n        if (typeof response === 'string' || response instanceof Error) {\n            throw new Error(`Failed to load resource from response`, { cause: response });\n        }\n        // Now process the response and continue with the code synchronously\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Web Scraping Script with Puppeteer\nDESCRIPTION: This is a full script using Puppeteer to navigate to Google, perform a search, click on a result, extract the page title, and take a screenshot. It showcases the implementation of various Page methods in a practical web scraping scenario.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button + button');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Wait for the first result to appear on the page,\n// then click on it\nawait page.waitForSelector('.g a');\nawait Promise.all([page.waitForNavigation(), page.click('.g a')]);\n\n// Grab the page's title and log it to the console\nconst title = await page.title();\nconsole.log(title);\n\n// Take a screenshot and write it to the filesystem\nawait page.screenshot({ path: 'screenshot.png' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Using CheerioCrawler with Apify Proxy in JavaScript\nDESCRIPTION: This example shows how to use CheerioCrawler with Apify Proxy. It sets up the Actor, creates a proxy configuration, and initializes a crawler to fetch and log the body of a specified URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ body }) {\n        // ...\n        console.log(body);\n    },\n});\n\nawait crawler.run(['https://proxy.apify.com']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Organizing Web Scraping Logic with Function Decomposition in JavaScript\nDESCRIPTION: Demonstrates how to structure a pageFunction by breaking it into separate handler functions for different page types. Includes implementation of pagination handling and detail page scraping with proper error handling and data extraction.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error('Unknown request label.');\n    }\n\n    async function handleStart({ log, waitFor }) {\n        log.info('Store opened!');\n        let timeoutMillis; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await waitFor(buttonSelector, { timeoutMillis });\n                // 2 sec timeout after the first.\n                timeoutMillis = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            $(buttonSelector).click();\n        }\n    }\n\n    async function handleDetail({\n        request,\n        log,\n        skipLinks,\n        jQuery: $,\n    }) {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Variants with Python and Beautiful Soup\nDESCRIPTION: This code snippet demonstrates how to extract product variant information from an e-commerce website using Python and Beautiful Soup. It iterates through products, downloads detail pages, and parses variant data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nlisting_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product, listing_url)\n    product_soup = download(item[\"url\"])\n    vendor = product_soup.select_one(\".product-meta__vendor\").text.strip()\n\n    if variants := product_soup.select(\".product-form__option.no-js option\"):\n        for variant in variants:\n            data.append(item | {\"variant_name\": variant.text.strip()})\n    else:\n        item[\"variant_name\"] = None\n        data.append(item)\n```\n\n----------------------------------------\n\nTITLE: Configuring JavaScript Code Input for Apify Actor\nDESCRIPTION: This snippet demonstrates how to set up a string input field for JavaScript code in an Apify Actor input schema. It includes a title, description, and a prefilled JavaScript function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Page function\",\n    \"type\": \"string\",\n    \"description\": \"Function executed for each request\",\n    \"editor\": \"javascript\",\n    \"prefill\": \"async () => { return $('title').text(); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Identifying and Accessing iFrames with Puppeteer\nDESCRIPTION: This snippet demonstrates how to launch a Puppeteer browser, navigate to IMDB, and identify the Twitter widget iFrame. It uses a loop to iterate through child frames and find the one with a URL containing 'twitter'.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/scraping_iframes.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch();\n\nconst page = await browser.newPage();\n\nawait page.goto('https://www.imdb.com');\nawait page.waitForTimeout(5000); // we need to wait for Twitter widget to load\n\nlet twitterFrame; // this will be populated later by our identified frame\n\nfor (const frame of page.mainFrame().childFrames()) {\n    // Here you can use few identifying methods like url(),name(),title()\n    if (frame.url().includes('twitter')) {\n        console.log('we found the Twitter iframe');\n        twitterFrame = frame;\n        // we assign this frame to myFrame to use it later\n    }\n}\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Merging Data from Different Requests in Apify\nDESCRIPTION: Demonstrates how to access and merge data passed from a previous request with newly scraped data. This technique enables combining information from multiple pages into a single result object.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nconst result = { ...request.userData.data, ...sellerDetail };\n```\n\n----------------------------------------\n\nTITLE: Puppeteer Implementation with Navigation Handling\nDESCRIPTION: Complete example of browser automation using Puppeteer, showcasing proper navigation waiting and element interaction patterns.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/waiting.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as fs from 'fs/promises';\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button + button');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Wait for the first result to appear on the page,\n// then click on it\nawait page.waitForSelector('.g a');\nawait Promise.all([page.waitForNavigation(), page.click('.g a')]);\n\n// Our title extraction and screenshotting logic\n// will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in JavaScript\nDESCRIPTION: Demonstrates various key-value store operations including saving objects, binary files, and accessing different stores.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Save object to store (stringified to JSON)\nawait Actor.setValue('my_state', { something: 123 });\n\n// Save binary file to store with content type\nawait Actor.setValue('screenshot.png', buffer, { contentType: 'image/png' });\n\n// Get a record from the store (automatically parsed from JSON)\nconst value = await Actor.getValue('my_state');\n\n// Access another key-value store by its name\nconst store = await Actor.openKeyValueStore('screenshots-store');\nawait store.setValue('screenshot.png', buffer, { contentType: 'image/png' });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Implementing Metamorph in JavaScript\nDESCRIPTION: Example of implementing a hotel review scraper using metamorph to transform into web-scraper Actor. Shows initialization, input handling, and metamorph operation with new input configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/metamorph.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Get input of your Actor.\nconst { hotelUrl } = await Actor.getInput();\n\n// Create input for apify/web-scraper\nconst newInput = {\n    startUrls: [{ url: hotelUrl }],\n    pageFunction: () => {\n        // Here you pass the page function that\n        // scrapes all the reviews ...\n    },\n    // ... and here would be all the additional\n    // input parameters.\n};\n\n// Transform the Actor run to apify/web-scraper\n// with the new input.\nawait Actor.metamorph('apify/web-scraper', newInput);\n\n// The line here will never be reached, because the\n// Actor run will be interrupted.\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Reading Decrypted Secret Input in Actor (JavaScript)\nDESCRIPTION: This code demonstrates how to read the Actor input using Actor.getInput(). The secret fields are automatically decrypted when accessed this way, starting from apify package version 3.1.0.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/secret_input.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n> await Actor.getInput();\n{\n    username: 'username',\n    password: 'password'\n}\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of Browser Context in Node.js\nDESCRIPTION: This snippet demonstrates a common mistake where browser-specific code is incorrectly run in the Node.js context, resulting in an error.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// This code is incorrect!\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// visit google\nawait page.goto('https://www.google.com/');\n\n// change background to green\ndocument.body.style.background = 'green';\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Blocking Resources with CDP Session in Puppeteer\nDESCRIPTION: Shows how to use a Chrome DevTools Protocol (CDP) session in Puppeteer to block resources while maintaining browser cache functionality.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Define our blocked extensions\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Use CDP session to block resources\nawait page.client().send('Network.setBlockedURLs', { urls: blockedExtensions });\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Scraping GitHub Repositories with Pagination using Playwright\nDESCRIPTION: This code snippet demonstrates how to scrape GitHub repositories from multiple pages using Playwright. It includes functions for scraping individual pages, handling pagination, and concurrent scraping of multiple pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\nimport * as cheerio from 'cheerio';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\n// Scrapes all repositories from a single page\nconst scrapeRepos = async (page) => {\n    const $ = cheerio.load(await page.content());\n\n    return [...$('.list-view-item')].map((item) => {\n        const repoElement = $(item);\n        return {\n            title: repoElement.find('h4').text().trim(),\n            description: repoElement.find('.repos-list-description').text().trim(),\n            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,\n        };\n    });\n};\n\nconst browser = await chromium.launch({ headless: false });\nconst firstPage = await browser.newPage();\n\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageElement = firstPage.locator('a[aria-label*=\"Page \"]:nth-last-child(2)');\nconst lastPageLabel = await lastPageElement.getAttribute('aria-label');\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\n\n// Push all results from the first page to the repositories array\nrepositories.push(...(await scrapeRepos(firstPage)));\n\nawait firstPage.close();\n\nconst pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);\nconst promises = pageNumbers.map((pageNumber) => (async () => {\n    const paginatedPage = await browser.newPage();\n\n    // Construct the URL by setting the ?page=... parameter to value of pageNumber\n    const url = new URL(REPOSITORIES_URL);\n    url.searchParams.set('page', pageNumber);\n\n    // Scrape the page\n    await paginatedPage.goto(url.href);\n    const results = await scrapeRepos(paginatedPage);\n\n    // Push results to the repositories array\n    repositories.push(...results);\n\n    await paginatedPage.close();\n})(),\n);\nawait Promise.all(promises);\n\n// For brievity logging just the count of repositories scraped\nconsole.log(repositories.length);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Processing URLs and Crawling Product Pages with Node.js and Cheerio\nDESCRIPTION: This code snippet demonstrates how to extract product URLs from a store page, then crawl each product page to extract its title. It uses got-scraping for HTTP requests and Cheerio for HTML parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/first_crawl.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\n// Prepare an empty array for our product URLs.\nconst productUrls = [];\n\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n\n    // Collect absolute product URLs.\n    productUrls.push(absoluteUrl);\n}\n\n// Loop over the stored URLs to process\n// each product page individually.\nfor (const url of productUrls) {\n    // Download HTML.\n    const productResponse = await gotScraping(url);\n    const productHtml = productResponse.body;\n\n    // Load into Cheerio to parse the HTML.\n    const $productPage = cheerio.load(productHtml);\n\n    // Extract the product's title from the <h1> tag.\n    const productPageTitle = $productPage('h1').text().trim();\n\n    // Print the title to the terminal to see\n    // confirm we downloaded the correct pages.\n    console.log(productPageTitle);\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Error Reporting with Apify SDK in JavaScript\nDESCRIPTION: Illustrates a comprehensive error reporting system using Apify SDK, including saving snapshots, creating error reports, and storing them in a named dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/analyzing_pages_and_fixing_errors.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { puppeteerUtils } from 'crawlee';\n\nawait Actor.init();\n// ...\n// Let's create reporting dataset\n// If you already have one, this will continue adding to it\nconst reportingDataset = await Actor.openDataset('REPORTING');\n\ntry {\n    // Sensitive code block\n    // ...\n} catch (error) {\n    // Change the way you save it depending on what tool you use\n    const randomNumber = Math.random();\n    const key = `ERROR-LOGIN-${randomNumber}`;\n    // The store gets removed with the run after data retention period so the links will stop working eventually\n    // You can store the snapshots infinitely in a named KV store by adding `keyValueStoreName` option\n    await puppeteerUtils.saveSnapshot(page, { key });\n\n    // To create the reporting URLs, we need to know the Key-Value store and run IDs\n    const { actorRunId, defaultKeyValueStoreId } = Actor.getEnv();\n\n    // We create a report object\n    const report = {\n        errorType: 'login',\n        errorMessage: error.toString(),\n        // .html and .jpg file extensions are added automatically by the saveSnapshot function\n        htmlSnapshotUrl: `https://api.apify.com/v2/key-value-stores/${defaultKeyValueStoreId}/records/${key}.html`,\n        screenshotUrl: `https://api.apify.com/v2/key-value-stores/${defaultKeyValueStoreId}/records/${key}.jpg`,\n        runUrl: `https://console.apify.com/actors/runs/${actorRunId}`,\n    };\n\n    // And we push the report to our reporting dataset\n    await reportingDataset.pushData(report);\n\n    // You know where the code crashed so you can explain here\n    throw new Error('Request failed during login with an error', { cause: error });\n}\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Scraping F1 Teams Count with Python and Beautiful Soup\nDESCRIPTION: This exercise solution demonstrates how to scrape and count the number of F1 teams listed on the Formula 1 website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://www.formula1.com/en/teams\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nprint(len(soup.select(\".outline\")))\n```\n\n----------------------------------------\n\nTITLE: Crawling Product Detail Pages in JavaScript\nDESCRIPTION: Implements a web crawler that visits a category page, extracts product URLs, and then visits each product page to extract the title. It demonstrates error handling and URL parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n\nfor (const url of productUrls) {\n    try {\n        const productResponse = await gotScraping(url);\n        const productHtml = productResponse.body;\n        const $productPage = cheerio.load(productHtml);\n        const productPageTitle = $productPage('h1').text().trim();\n        console.log(productPageTitle);\n    } catch (error) {\n        console.error(error.message, url);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining and Enqueuing Pivot Ranges in JavaScript\nDESCRIPTION: This snippet demonstrates how to set up initial pivot price ranges, create filter URLs, and enqueue requests for a web scraping project using Apify and Crawlee.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-with-search.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { CheerioCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst MAX_PRODUCTS_PAGINATION = 1000;\n\n// Just an example, choose what makes sense for your site\nconst PIVOT_PRICE_RANGES = [\n    { min: 0, max: 9.99 },\n    { min: 10, max: 99.99 },\n    { min: 100, max: 999.99 },\n    { min: 1000, max: 9999.99 },\n    { min: 10000, max: null }, // open-ended\n];\n\n// Let's create a helper function for creating the filter URLs, you can move those to a utils.js file\nconst createFilterUrl = ({ min, max }) => {\n    const minString = `min_price=${min}`;\n    // We don't want to pass the parameter at all if it is null (open-ended)\n    const maxString = max ? `&max_price=${max}` : '';\n    return `https://www.mysite.com/products?${minString}${maxString}`;\n};\n\n// And another helper for getting filters back from the URL, we could also pass them in userData\nconst getFiltersFromUrl = (url) => {\n    const min = Number(url.match(/min_price=([0-9.]+)/)[1]);\n    // Max price might be empty\n    const maxMatch = url.match(/max_price=([0-9.]+)/);\n    const max = maxMatch ? Number(maxMatch[1]) : null;\n    return { min, max };\n};\n\n// Actor setup things here\nconst crawler = new CheerioCrawler({\n    async requestHandler(context) {\n        // ...\n    },\n});\n\n// Let's create the pivot requests\nconst initialRequests = [];\nfor (const { min, max } of PIVOT_PRICE_RANGES) {\n    initialRequests.push({\n        url: createFilterUrl({ min, max }),\n        label: 'FILTER',\n    });\n}\n// Let's start the crawl\nawait crawler.run(initialRequests);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Parsing Product Inventory with RegEx and BeautifulSoup\nDESCRIPTION: Script that scrapes product information from a Shopify store using BeautifulSoup and regular expressions to extract product titles and inventory numbers. Uses httpx for HTTP requests and re for regular expression parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport re\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    units_text = product.select_one(\".product-item__inventory\").text\n    if re_match := re.search(r\"\\d+\", units_text):\n        units = int(re_match.group())\n    else:\n        units = 0\n\n    print(title, units)\n```\n\n----------------------------------------\n\nTITLE: Updating Request Handler to Track Successful Requests\nDESCRIPTION: Modifies the request handler for offers to increment the total saved count in the Stats utility class for each offer processed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    const { asin } = data;\n\n    for (const offer of $('#aod-offer')) {\n        tracker.incrementASIN(asin);\n        // Add 1 to totalSaved for every offer\n        Stats.success();\n\n        const element = $(offer);\n\n        await dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n        });\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic Actor Structure\nDESCRIPTION: Basic structure for an Apify Actor with initialization and exit calls.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// ...\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Uploading a File with Puppeteer in JavaScript\nDESCRIPTION: This snippet shows how to upload a file using Puppeteer. It first selects the file input element, then uses the uploadFile() method to attach the previously downloaded file.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst fileInput = await page.$('input[type=file]');\nawait fileInput.uploadFile('./file.pdf');\n```\n\n----------------------------------------\n\nTITLE: Running Website Content Crawler with Apify Python SDK\nDESCRIPTION: Code that calls the Website Content Crawler Actor to crawl the Qdrant documentation website and extract text content using the Apify Python SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/qdrant.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nactor_call = client.actor(\"apify/website-content-crawler\").call(\n    run_input={\"startUrls\": [{\"url\": \"https://qdrant.tech/documentation/\"}]}\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler to Scrape Beer Data\nDESCRIPTION: This code uses PuppeteerCrawler to process each URL from the RequestList, extract beer data using Puppeteer, and save it to a dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page }) {\n        const beerPage = await page.evaluate(() => {\n            return document.getElementsByClassName('productreviews').length;\n        });\n        if (!beerPage) return;\n\n        const data = await page.evaluate(() => {\n            const title = document.getElementsByTagName('h1')[0].innerText;\n            const [brewery, beer] = title.split(':');\n            const description = document.getElementsByClassName('productreviews')[0].innerText;\n\n            return { brewery, beer, description };\n        });\n\n        await Dataset.pushData(data);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Browser Context Data Extraction with jQuery\nDESCRIPTION: Demonstrates how to inject jQuery into the page and use it within page.evaluate() to extract product data. Uses jQuery selectors to find and extract product names and prices.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.addScriptTag({ url: 'https://code.jquery.com/jquery-3.6.0.min.js' });\n\nconst products = await page.evaluate(() => {\n    const productCards = Array.from($('a[class*=\"ProductCard_root\"]'));\n\n    return productCards.map((element) => {\n        const card = $(element);\n\n        const name = card.find('h3[class*=\"ProductCard_name\"]').text();\n        const price = card.find('div[class*=\"ProductCard_price\"]').text();\n\n        return {\n            name,\n            price,\n        };\n    });\n});\n\nconsole.log(products);\n```\n\n----------------------------------------\n\nTITLE: Saving Error Snapshots with Puppeteer in JavaScript\nDESCRIPTION: Demonstrates how to save screenshots and HTML snapshots when an error occurs during web scraping using Puppeteer and the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/analyzing_pages_and_fixing_errors.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { puppeteerUtils } from 'crawlee';\n\n// ...\n// storeId is ID of current key value store, where we save snapshots\nconst storeId = Actor.getEnv().defaultKeyValueStoreId;\ntry {\n    // Sensitive code block\n    // ...\n} catch (error) {\n    // Change the way you save it depending on what tool you use\n    const randomNumber = Math.random();\n    const key = `ERROR-LOGIN-${randomNumber}`;\n    await puppeteerUtils.saveSnapshot(page, { key });\n    const screenshotLink = `https://api.apify.com/v2/key-value-stores/${storeId}/records/${key}.jpg`;\n\n    // You know where the code crashed so you can explain here\n    throw new Error('Request failed during login with an error', { cause: error });\n}\n// ...\n```\n\n----------------------------------------\n\nTITLE: Creating and Attaching Vector Store to the OpenAI Assistant\nDESCRIPTION: Creates a new OpenAI Vector Store and updates the assistant to use this vector store for the file_search tool, enabling it to access custom data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nvector_store = client.beta.vector_stores.create(name=\"Support assistant vector store\")\n\nassistant = client.beta.assistants.update(\n    assistant_id=my_assistant.id,\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Link URLs in Node.js using Cheerio\nDESCRIPTION: Node.js script that uses Cheerio to extract all link URLs from a specific webpage. It demonstrates downloading HTML content, parsing it with Cheerio, and extracting href attributes from all anchor tags.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/finding_links.md#2025-04-18_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport cheerio from 'cheerio';\n\nconst url = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconsole.log('Downloading HTML...');\nconst response = await gotScraping(url);\nconst html = response.body;\n\nconsole.log('Processing data...');\nconst $ = cheerio.load(html);\n\n// Here's the magic!\nconst links = $('a');\n\n// Print all URLs to the console\nfor (const link of links) {\n    const url = $(link).attr('href');\n    console.log(url);\n}\n\nconsole.log('Done.');\n```\n\n----------------------------------------\n\nTITLE: Generating Dynamic Headers with got-scraping\nDESCRIPTION: Demonstrates how to use got-scraping to automatically generate request-specific headers with custom options for browsers, devices, locales, and operating systems.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await gotScraping({\n    url: 'https://example.com',\n    headerGeneratorOptions: {\n        browsers: [\n            {\n                name: 'chrome',\n                minVersion: 87,\n                maxVersion: 89,\n            },\n        ],\n        devices: ['desktop'],\n        locales: ['de-DE', 'en-US'],\n        operatingSystems: ['windows', 'linux'],\n    },\n    headers: {\n        'some-header': 'Hello, Academy!',\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using gotScraping with Proxy Configuration and Session Management\nDESCRIPTION: Shows how to use the got-scraping library with Apify's proxy configuration. The example creates a proxy URL with a named session and uses it to make multiple HTTP requests with the same IP address, then compares the client IPs to verify they match.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { gotScraping } from 'got-scraping';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl('my_session');\n\nconst response1 = await gotScraping({\n    url: 'https://api.apify.com/v2/browser-info',\n    proxyUrl,\n    responseType: 'json',\n});\n\nconst response2 = await gotScraping({\n    url: 'https://api.apify.com/v2/browser-info',\n    proxyUrl,\n    responseType: 'json',\n});\n\nconsole.log(response1.body.clientIp);\nconsole.log('Should be the same as');\nconsole.log(response2.body.clientIp);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Pre-injecting Scripts with Playwright\nDESCRIPTION: Demonstrates how to inject custom code before page load using Playwright's page.addInitScript(). The example shows overriding the native addEventListener function to prevent event listeners from being added.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.addInitScript(() => {\n    // Override the prototype\n    Node.prototype.addEventListener = () => { /* do nothing */ };\n});\n\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Picking Session for Proxy Rotation in Node.js\nDESCRIPTION: Implements an algorithm to select a session from existing working sessions or create a new one with a random user agent. This function helps in managing and rotating proxy sessions efficiently.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst pickSession = (sessions, maxSessions = 100) => {\n\n    // sessions is our sessions object, at the beginning instantiated as {}\n    // maxSessions is a constant which should be the number of working proxies we aspire to have.\n    // The lower the number, the faster you will use the working proxies\n    // but the faster the new one will not be picked\n    // 100 is reasonable default\n    // Since sessions is an object, we prepare an array of the session names\n    const sessionsKeys = Object.keys(sessions);\n\n    console.log(`Currently we have ${sessionsKeys.length} working sessions`);\n\n    // We define a random floating number from 0 to 1 that will serve\n    // both as a chance to pick the session and its possible name\n    const randomNumber = Math.random();\n\n    // The chance to pick a session will be higher when we have more working sessions\n    const chanceToPickSession = sessionsKeys.length / maxSessions;\n\n    console.log(`Chance to pick a working session is ${Math.round(chanceToPickSession * 100)}%`);\n\n    // If the chance is higher than the random number, we pick one from the working sessions\n    const willPickSession = chanceToPickSession > randomNumber;\n\n    if (willPickSession) {\n        // We randomly pick one of the working sessions and return it\n        const indexToPick = Math.floor(sessionsKeys.length * Math.random());\n\n        const nameToPick = sessionsKeys[indexToPick];\n\n        console.log(`We picked a working session: ${nameToPick} on index ${indexToPick}`);\n\n        return sessions[nameToPick];\n    }\n    // We create a new session object, assign a random userAgent to it and return it\n\n    console.log(`Creating new session: ${randomNumber}`);\n\n    return {\n        name: randomNumber.toString(),\n        userAgent: Apify.utils.getRandomUserAgent(),\n    };\n\n};\n```\n\n----------------------------------------\n\nTITLE: Pushing Data to Dataset in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use Actor.pushData() to store data into a dataset using the Apify SDK in JavaScript. It includes various data types such as numeric, text, boolean, date, array, and object fields.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/index.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n// Initialize the JavaScript SDK\nawait Actor.init();\n\n/**\n * Actor code\n */\nawait Actor.pushData({\n    numericField: 10,\n    pictureUrl: 'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_92x30dp.png',\n    linkUrl: 'https://google.com',\n    textField: 'Google',\n    booleanField: true,\n    dateField: new Date(),\n    arrayField: ['#hello', '#world'],\n    objectField: {},\n});\n\n\n// Exit successfully\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Type Definitions for TypeScript Scraper\nDESCRIPTION: This snippet contains the type definitions used in the TypeScript scraper, including interfaces for Product, ResponseData, and UserInput, as well as an enum for SortOrder.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// types.ts\nexport interface Product {\n    id: number;\n    title: string;\n    description: string;\n    price: number;\n    discountPercentage: number;\n    rating: number;\n    stock: number;\n    brand: string;\n    category: string;\n    thumbnail: string;\n    images: string[];\n}\n\nexport interface ResponseData {\n    products: Product[];\n}\n\nexport type ModifiedProduct = Omit<Product, 'images'>;\n\nexport enum SortOrder {\n    ASC = 'ascending',\n    DESC = 'descending',\n}\n\nexport interface UserInput<RemoveImages extends boolean = boolean> {\n    sort: 'ascending' | 'descending';\n    removeImages: RemoveImages;\n}\n```\n\n----------------------------------------\n\nTITLE: Full Web Scraper Page Function with Conditional Logic\nDESCRIPTION: The complete page function for Web Scraper that handles both the initial page and detail pages. Implements conditional logic based on request labels, with specific data extraction for Actor detail pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    // use jQuery as $\n    const { request, log, skipLinks, jQuery: $ } = context;\n\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Locating Child Elements with BeautifulSoup\nDESCRIPTION: Shows how to locate and extract specific child elements (title and price) from product cards using multiple selectors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/06_locating_elements.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    titles = product.select(\".product-item__title\")\n    first_title = titles[0].text\n\n    prices = product.select(\".price\")\n    first_price = prices[0].text\n\n    print(first_title, first_price)\n```\n\n----------------------------------------\n\nTITLE: Complete CrewAI Integration Script\nDESCRIPTION: Full implementation of the CrewAI integration with Apify for TikTok profile analysis\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom crewai import Agent, Task, Crew\nfrom crewai_tools import ApifyActorsTool\nfrom langchain_openai import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser_tool = ApifyActorsTool(actor_name=\"apify/rag-web-browser\")\ntiktok_tool = ApifyActorsTool(actor_name=\"clockworks/free-tiktok-scraper\")\n\nsearch_agent = Agent(\n    role=\"Web Search Specialist\",\n    goal=\"Find the TikTok profile URL on the web\",\n    backstory=\"Expert in web searching and data retrieval\",\n    tools=[browser_tool],\n    llm=llm,\n    verbose=True\n)\n\nanalysis_agent = Agent(\n    role=\"TikTok Profile Analyst\",\n    goal=\"Extract and analyze data from the TikTok profile\",\n    backstory=\"Skilled in social media data extraction and analysis\",\n    tools=[tiktok_tool],\n    llm=llm,\n    verbose=True\n)\n\nsearch_task = Task(\n    description=\"Search the web for the OpenAI TikTok profile URL.\",\n    agent=search_agent,\n    expected_output=\"A URL linking to the OpenAI TikTok profile.\"\n)\nanalysis_task = Task(\n    description=\"Extract data from the OpenAI TikTok profile URL and provide a profile summary and details about the latest post.\",\n    agent=analysis_agent,\n    context=[search_task],\n    expected_output=\"A summary of the OpenAI TikTok profile including followers and likes, plus details about their most recent post.\"\n)\n\ncrew = Crew(\n    agents=[search_agent, analysis_agent],\n    tasks=[search_task, analysis_task],\n    process=\"sequential\"\n)\n\nresult = crew.kickoff()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Configuring Main Crawler Script\nDESCRIPTION: Main crawler configuration script that sets up CheerioCrawler with input handling and request routing for Amazon product searches.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/initializing_and_setting_up.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';\nimport { router } from './routes.js';\n\n// Grab our keyword from the input\nconst { keyword } = await KeyValueStore.getInput();\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n\n    // If you have access to Apify Proxy, you can use residential proxies and\n    // high retry count which helps with blocking\n    // If you don't, your local IP address will likely be fine for a few requests if you scrape slowly.\n    // proxyConfiguration: await Actor.createProxyConfiguration({ groups: ['RESIDENTIAL'] }),\n    // maxRequestRetries: 10,\n});\n\nlog.info('Starting the crawl.');\nawait crawler.run([{\n    // Turn the keyword into a link we can make a request with\n    url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\n    label: 'START',\n    userData: {\n        keyword,\n    },\n}]);\nlog.info('Crawl finished.');\n```\n\n----------------------------------------\n\nTITLE: Extracting Page Title with Puppeteer/Playwright\nDESCRIPTION: This snippet demonstrates how to grab the title of a web page using the page.title() method in both Puppeteer and Playwright. It retrieves the title and logs it to the console.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Grab the title and set it to a variable\nconst title = await page.title();\n\n// Log the title to the console\nconsole.log(title);\n```\n\n----------------------------------------\n\nTITLE: Configuring PlaywrightCrawler for Cloudflare Bypass\nDESCRIPTION: Code snippet showing how to configure a PlaywrightCrawler instance to handle Cloudflare challenges by removing default blocked status code handling. This allows the crawler to continue operation even when encountering 403 status codes during challenge evaluation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/cloudflare_challenge.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new PlaywrightCrawler({\n    ...otherOptions,\n    sessionPoolOptions: {\n        blockedStatusCodes: [],\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Using the ASPX Form Submission Function in Web Scraper\nDESCRIPTION: Example of how to use the enqueueAspxForm utility function in a Web Scraper Page function. This demonstrates submitting a form on an architect finder website with proper parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_forms_on_aspx_pages.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait enqueueAspxForm({\n    url: 'http://architectfinder.aia.org/frmSearch.aspx',\n    userData: { label: 'SEARCH-RESULT' },\n}, 'form[name=\"aspnetForm\"]', '#ctl00_ContentPlaceHolder1_btnSearch', false);\n```\n\n----------------------------------------\n\nTITLE: GO Actor Dockerfile Example\nDESCRIPTION: An example Dockerfile for a Go Actor. It uses the official Go Alpine image, copies the source code, downloads dependencies, and builds the executable. The CMD instruction specifies how to run the compiled executable.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_3\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM golang:1.17.1-alpine\n\nWORKDIR /app\nCOPY . .\n\nRUN go mod download\n\nRUN go build -o /example-actor\nCMD [\"/example-actor\"]\n```\n\n----------------------------------------\n\nTITLE: Retrieving JavaScript Objects with Puppeteer\nDESCRIPTION: This code snippet shows how to use Puppeteer to execute JavaScript in the browser context and retrieve a window object. It accesses the '__sc_hydration' object directly from the window.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/js_in_html.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst data = await page.evaluate(() => window.__sc_hydration);\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: Complete Page Function for Scraping Actor Details\nDESCRIPTION: This function handles both the initial page and detail pages. It extracts actor details on detail pages and logs information on the initial page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    // $ is Cheerio\n    const { request, log, skipLinks, $ } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Page Function for Apify Store Scraping in JavaScript\nDESCRIPTION: This code snippet shows the implementation of a page function for scraping Apify Store actors. It handles both the start page and detail pages, extracting relevant information and enqueueing new requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { request, log, skipLinks, $ } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n\n        const dataJson = $('#__NEXT_DATA__').html();\n        // We requested HTML, but the data are actually JSON.\n        const data = JSON.parse(dataJson);\n\n        for (const item of data.props.pageProps.items) {\n            const { name, username } = item;\n            const actorDetailUrl = `https://apify.com/${username}/${name}`;\n            await context.enqueueRequest({\n                url: actorDetailUrl,\n                userData: {\n                    label: 'DETAIL',\n                },\n            });\n        }\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Cheerio Crawler for Amazon Scraping in JavaScript\nDESCRIPTION: This code snippet shows the main file that initializes the Cheerio crawler, configures it with the router, and starts the crawling process for Amazon product search.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/modularity.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { CheerioCrawler, log, KeyValueStore } from 'crawlee';\nimport { router } from './routes.js';\nimport { BASE_URL } from './constants.js';\n\nconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\nawait crawler.addRequests([\n    {\n        // Use BASE_URL here instead\n        url: `${BASE_URL}/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\n        label: 'START',\n        userData: {\n            keyword,\n        },\n    },\n]);\n\nlog.info('Starting the crawl.');\nawait crawler.run();\nlog.info('Crawl finished.');\n```\n\n----------------------------------------\n\nTITLE: Switching to PuppeteerCrawler for Dynamic Content\nDESCRIPTION: Modifies the crawler to use PuppeteerCrawler instead of CheerioCrawler to handle dynamic content. This version still doesn't handle lazy-loaded images.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler } from 'crawlee';\n\nconst BASE_URL = 'https://demo-webstore.apify.org';\n\n// Switch CheerioCrawler to PuppeteerCrawler\nconst crawler = new PuppeteerCrawler({\n    // Replace \"$\" with \"page\"\n    requestHandler: async ({ parseWithCheerio, request }) => {\n        // Create the $ Cheerio object based on the page's content\n        const $ = await parseWithCheerio();\n\n        const products = $('a[href*=\"/product/\"]');\n\n        const results = [...products].map((product) => {\n            const elem = $(product);\n\n            const title = elem.find('h3').text();\n            const price = elem.find('div[class*=\"price\"]').text();\n            const image = elem.find('img[src]').attr('src');\n\n            return {\n                title,\n                price,\n                image: new URL(image, BASE_URL).href,\n            };\n        });\n\n        console.log(results);\n    },\n});\n\nawait crawler.run([{ url: 'https://demo-webstore.apify.org/search/new-arrivals' }]);\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in JavaScript\nDESCRIPTION: Demonstrates how to use the Apify JavaScript SDK to add single and multiple items to the default dataset in an Actor. It emphasizes the importance of using 'await' with pushData() to ensure data storage completion.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\n// Import the JavaScript SDK into your project\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\n// Add one item to the default dataset\nawait Actor.pushData({ foo: 'bar' });\n\n// Add multiple items to the default dataset\nawait Actor.pushData([{ foo: 'hotel' }, { foo: 'cafe' }]);\n\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Complete Web Scraping Script with Playwright\nDESCRIPTION: This is a full script using Playwright to navigate to Google, perform a search, click on a result, extract the page title, and take a screenshot. It demonstrates the use of various Page methods in a real-world scenario.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button:has-text(\"Accept all\")');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Click on the first result\nawait page.click('.g a');\nawait page.waitForLoadState('load');\n\n// Grab the page's title and log it to the console\nconst title = await page.title();\nconsole.log(title);\n\n// Take a screenshot and write it to the filesystem\nawait page.screenshot({ path: 'screenshot.png' });\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Playwright Implementation with Load State Handling\nDESCRIPTION: Complete example of browser automation using Playwright, including handling of cookies, search interactions, and proper page load state management.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/waiting.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as fs from 'fs/promises';\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button:has-text(\"Accept all\")');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Click on the first result\nawait page.click('.g a');\nawait page.waitForLoadState('load');\n\n// Our title extraction and screenshotting logic\n// will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Querying Elements with Cheerio in Node.js\nDESCRIPTION: This snippet demonstrates how to use Cheerio to query and select elements from parsed HTML, similar to using document.querySelectorAll() in the browser.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_continued.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// In Node.js with Cheerio\nconst products = $('.product-item');\n```\n\n----------------------------------------\n\nTITLE: Complete Actor Implementation\nDESCRIPTION: Final implementation combining all components including initialization, both client and API implementations, and execution logic.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/using_api_and_client.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport axios from 'axios';\n\nawait Actor.init();\n\nconst { useClient, memory, fields, maxItems } = await Actor.getInput();\n\nconst TASK = 'YOUR_USERNAME~demo-actor-task';\n\nconst withClient = async () => {\n    const client = Actor.newClient();\n    const task = client.task(TASK);\n\n    const { id } = await task.call({ memory });\n\n    const dataset = client.run(id).dataset();\n\n    const items = await dataset.downloadItems('csv', {\n        limit: maxItems,\n        fields,\n    });\n\n    return Actor.setValue('OUTPUT', items, { contentType: 'text/csv' });\n};\n\nconst withAPI = async () => {\n    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;\n    const url = new URL(uri);\n\n    url.search = new URLSearchParams({\n        memory,\n        format: 'csv',\n        limit: maxItems,\n        fields: fields.join(','),\n        token: process.env.APIFY_TOKEN,\n    });\n\n    const { data } = await axios.post(url.toString());\n\n    return Actor.setValue('OUTPUT', data, { contentType: 'text/csv' });\n};\n\nif (useClient) {\n    await withClient();\n} else {\n    await withAPI();\n}\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Web Scraping with JavaScript using Axios and Cheerio\nDESCRIPTION: This code snippet demonstrates a basic web scraper built with the Apify SDK. It fetches HTML content from a specified URL using Axios, parses it with Cheerio, extracts all headings (H1-H6) with their text content, and stores the data in an Apify Dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/creating_actors.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Axios - Promise based HTTP client for the browser and node.js (Read more at https://axios-http.com/docs/intro).\nimport { Actor } from 'apify';\nimport axios from 'axios';\n// Cheerio - The fast, flexible & elegant library for parsing and manipulating HTML and XML (Read more at https://cheerio.js.org/).\nimport * as cheerio from 'cheerio';\n// Apify SDK - toolkit for building Apify Actors (Read more at https://docs.apify.com/sdk/js/).\n\n// The init() call configures the Actor for its environment. It's recommended to start every Actor with an init().\nawait Actor.init();\n\n// Structure of input is defined in input_schema.json\nconst input = await Actor.getInput();\nconst { url } = input;\n\n// Fetch the HTML content of the page.\nconst response = await axios.get(url);\n\n// Parse the downloaded HTML with Cheerio to enable data extraction.\nconst $ = cheerio.load(response.data);\n\n// Extract all headings from the page (tag name and text).\nconst headings = [];\n$('h1, h2, h3, h4, h5, h6').each((i, element) => {\n    const headingObject = {\n        level: $(element).prop('tagName').toLowerCase(),\n        text: $(element).text(),\n    };\n    console.log('Extracted heading', headingObject);\n    headings.push(headingObject);\n});\n\n// Save headings to Dataset - a table-like storage.\nawait Actor.pushData(headings);\n\n// Gracefully exit the Actor process. It's recommended to quit all Actors with an exit().\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Apify Actor into LlamaIndex Documents\nDESCRIPTION: Python code demonstrating how to use the ApifyActor class to run the Website Content Crawler actor and convert its output into LlamaIndex Document objects. The code requires an Apify API token and specifies a target URL to crawl.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/llama.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import Document\nfrom llama_index.readers.apify import ApifyActor\n\nreader = ApifyActor(\"<My Apify API token>\")\n\ndocuments = reader.load_data(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\n        \"startUrls\": [{\"url\": \"https://docs.llamaindex.ai/en/latest/\"}]\n    },\n    dataset_mapping_function=lambda item: Document(\n        text=item.get(\"text\"),\n        metadata={\n            \"url\": item.get(\"url\"),\n        },\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Full Repository Scraping - Playwright Implementation\nDESCRIPTION: Complete implementation using Playwright to scrape repository data including titles, descriptions, and links from a single page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\nimport * as cheerio from 'cheerio';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\nconst scrapeRepos = async (page) => {\n    const $ = cheerio.load(await page.content());\n\n    return [...$('.list-view-item')].map((item) => {\n        const repoElement = $(item);\n        return {\n            title: repoElement.find('h4').text().trim(),\n            description: repoElement.find('.repos-list-description').text().trim(),\n            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,\n        };\n    });\n};\n\nconst browser = await chromium.launch({ headless: false });\nconst firstPage = await browser.newPage();\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageElement = firstPage.locator('a[aria-label*=\"Page \"]:nth-last-child(2)');\nconst lastPageLabel = await lastPageElement.getAttribute('aria-label');\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\n\nrepositories.push(...(await scrapeRepos(firstPage)));\n\nconsole.log(repositories);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Defining Actor Input Schema in JSON\nDESCRIPTION: This code snippet demonstrates how to define an input schema for a simple web crawler Actor. It specifies start URLs and a page function as required inputs, with prefill values and custom editors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_0\n\nLANGUAGE: json5\nCODE:\n```\n{\n    \"title\": \"Cheerio Crawler input\",\n    \"description\": \"To update crawler to another site, you need to change startUrls and pageFunction options!\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"startUrls\": {\n            \"title\": \"Start URLs\",\n            \"type\": \"array\",\n            \"description\": \"URLs to start with\",\n            \"prefill\": [\n                { \"url\": \"http://example.com\" },\n                { \"url\": \"http://example.com/some-path\" }\n            ],\n            \"editor\": \"requestListSources\"\n        },\n        \"pageFunction\": {\n            \"title\": \"Page function\",\n            \"type\": \"string\",\n            \"description\": \"Function executed for each request\",\n            \"prefill\": \"async () => { return $('title').text(); }\",\n            \"editor\": \"javascript\"\n        }\n    },\n    \"required\": [\"startUrls\", \"pageFunction\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Datasets Resource Input for Apify Actor in JSON\nDESCRIPTION: This snippet shows how to configure a multiple datasets resource input for an Apify Actor. It uses the 'array' type and 'resourceType' property to specify multiple dataset inputs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Datasets\",\n    \"type\": \"array\",\n    \"description\": \"Select multiple datasets\",\n    \"resourceType\": \"dataset\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Proxy Configuration in Crawlee\nDESCRIPTION: This snippet shows how to set up a ProxyConfiguration with custom proxy URLs for use in a Crawlee scraper.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: ['http://45.42.177.37:3128', 'http://43.128.166.24:59394', 'http://51.79.49.178:3128'],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Function Overloads for Scrape Function in TypeScript\nDESCRIPTION: This snippet demonstrates the use of function overloads to specify different return types based on the input parameter. It ensures type safety when accessing properties of the returned data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// index.ts\n\n// ...\n// If \"removeImages\" is true, a ModifiedProduct array will be returned\nasync function scrape(input: UserInput<true>): Promise<ModifiedProduct[]>;\n// If false, a normal product array is returned\nasync function scrape(input: UserInput<false>): Promise<Product[]>;\n// The main function declaration, which accepts all types in the declarations above.\n// Notice that it has no explicit return type, since they are defined in the\n// overloads above.\nasync function scrape(input: UserInput) {\n    const data = await fetchData();\n\n    const sorted = sortData(data.products, input.sort as SortOrder);\n\n    if (input.removeImages) {\n        return sorted.map((item) => {\n            const { images, ...rest } = item;\n\n            return rest;\n        });\n    }\n\n    return sorted;\n}\n```\n\n----------------------------------------\n\nTITLE: Example jQuery Scraping Logic for Browser Console Testing\nDESCRIPTION: A practical example of scraped data collection using jQuery selectors to extract information from a list of items. This can be tested directly in the browser console before adding to a Web Scraper configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nresults = [];\n$('.my-list-item').each((i, el) => {\n    results.push({\n        title: $(el).find('.title').text().trim(),\n        // other fields\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Data with Cheerio in JavaScript\nDESCRIPTION: Code snippet that demonstrates how to extract product data from a webpage using Cheerio. It loads the page content, selects product elements using CSS selectors, and maps them to structured objects with brand and price information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as cheerio from 'cheerio';\n\nconst $ = cheerio.load(await page.content());\n\n// Grab the newly loaded items\nconst items = [...$('a[data-testid*=\"productTile\"]')].slice(products.length);\n\nconst newItems = items.map((item) => {\n    const elem = $(item);\n\n    return {\n        brand: elem.find('p[data-testid=\"brandName\"]').text().trim(),\n        price: elem.find('span[data-testid=\"finalPrice\"]').text().trim(),\n    };\n});\n\nproducts.push(...newItems);\n```\n\n----------------------------------------\n\nTITLE: Using Named Datasets in Python\nDESCRIPTION: Demonstrates how to open and use a named dataset in Python, which can be shared between Actors or Actor runs. It shows opening a dataset and pushing data to it.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Save a named dataset to a variable\n        dataset = await Actor.open_dataset(name='some-name')\n\n        # Add data to the named dataset\n        await dataset.push_data({'foo': 'bar'})\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Objects from HTML in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract a JSON object from an HTML string by splitting the content and parsing it. It targets a specific window object named '__sc_hydration'.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/js_in_html.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst html = $.html();\n\nconst string = html.split('window.__sc_hydration = ')[1].split(';</script>')[0];\n\nconst data = JSON.parse(string);\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: Generating Browser Headers with browser-headers-generator (JavaScript)\nDESCRIPTION: This code snippet shows how to use the browser-headers-generator package to create randomized browser headers. It initializes the generator with specific operating system and browser options, then generates a set of random headers.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport BrowserHeadersGenerator from 'browser-headers-generator';\n\nconst browserHeadersGenerator = new BrowserHeadersGenerator({\n    operatingSystems: ['windows'],\n    browsers: ['chrome'],\n});\n\nawait browserHeadersGenerator.initialize();\n\nconst randomBrowserHeaders = await browserHeadersGenerator.getRandomizedHeaders();\n```\n\n----------------------------------------\n\nTITLE: Configuring Dockerfile for Apify Actor Node.js Environment\nDESCRIPTION: This Dockerfile sets up a Node.js environment for an Apify Actor, installing dependencies, copying source code, and defining the start command.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/source_code.md#2025-04-18_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version \\\n    && rm -r ~/.npm\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Error Handling in Web Crawling with Node.js and Cheerio\nDESCRIPTION: This code snippet extends the previous example by adding error handling. It uses try-catch blocks to prevent the crawler from crashing when encountering errors, allowing it to continue processing other URLs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/first_crawl.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n\nfor (const url of productUrls) {\n    // Everything else is exactly the same.\n    // We only wrapped the code in try/catch blocks.\n    // The try block passes all errors into the catch block.\n    // So, instead of crashing the crawler, they can be handled.\n    try {\n        // The try block attempts to execute our code\n        const productResponse = await gotScraping(url);\n        const productHtml = productResponse.body;\n        const $productPage = cheerio.load(productHtml);\n        const productPageTitle = $productPage('h1').text().trim();\n        console.log(productPageTitle);\n    } catch (error) {\n        // In the catch block, we handle errors.\n        // This time, we will print\n        // the error message and the url.\n        console.error(error.message, url);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Timezone and Date Information from BBC Weather Page\nDESCRIPTION: This code snippet extracts the timezone information and determines the first displayed date for the weather forecast from the BBC Weather page using BeautifulSoup.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n        # When parsing the first day, find out what day it represents,\n        # to know when do the results start\n        if day_offset == 0:\n            # Get the timezone offset written in the page footer and parse it\n            tz_description = soup.find_all(class_='wr-c-footer-timezone__item')[1].text\n            tz_offset_match = re.search(r'([+-]\\d\\d)(\\d\\d)', tz_description)\n            tz_offset_hours = int(tz_offset_match.group(1))\n            tz_offset_minutes = int(tz_offset_match.group(2))\n\n            # Get the current date and time at the scraped location\n            timezone_offset = timedelta(hours=tz_offset_hours, minutes=tz_offset_minutes)\n            location_timezone = timezone(timezone_offset)\n\n            location_current_datetime = datetime.now(tz=location_timezone)\n\n            # The times displayed for each day are from 6:00 AM that day to 5:00 AM the next day,\n            # so \"today\" on BBC Weather might actually mean \"yesterday\" in actual datetime.\n            # We have to parse the accessibility label containing the actual date on the header for the first day\n            # and compare it with the current date at the location, then adjust the date accordingly\n            day_carousel_item = soup.find(class_='wr-day--active')\n            day_carousel_title = day_carousel_item.find(class_='wr-day__title')['aria-label']\n            website_first_displayed_item_day = int(re.search(r'\\d{1,2}', day_carousel_title).group(0))\n\n            if location_current_datetime.day == website_first_displayed_item_day:\n                first_displayed_date = location_current_datetime.date()\n            else:\n                first_displayed_date = location_current_datetime.date() - timedelta(days=1)\n```\n\n----------------------------------------\n\nTITLE: Advanced Crawler with Link Following\nDESCRIPTION: Complete crawler implementation with link following capabilities using enqueueLinks\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        console.log('URL:', request.url);\n        console.log('Title:', $('h1').text().trim());\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n        }\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Logging into Yahoo with Puppeteer\nDESCRIPTION: This code snippet shows how to log into Yahoo using Puppeteer. It follows the same login process as the Playwright example, with slight differences in syntax and navigation handling.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\n// Launch a browser and open a page\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.yahoo.com/');\n\n// Agree to the cookies terms, then click on the \"Sign in\" button\nawait Promise.all([page.waitForSelector('a[data-ylk*=\"sign-in\"]'), page.click('button[name=\"agree\"]')]);\nawait Promise.all([page.waitForNavigation(), page.click('a[data-ylk*=\"sign-in\"]')]);\n\n// Type in the username and continue forward\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('input[name=\"signin\"]')]);\n\n// Type in the password and continue forward\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('button[name=\"verifyPassword\"]')]);\n\n// Wait for 10 seconds so we can see that we have in fact\n// successfully logged in\nawait page.waitForTimeout(10000);\n```\n\n----------------------------------------\n\nTITLE: Defining Custom GraphQL Query for Cheddar News API\nDESCRIPTION: Creates a custom GraphQL query to fetch media data from Cheddar's API, including title, publish date, and video URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_2\n\nLANGUAGE: graphql\nCODE:\n```\nquery SearchQuery($query: String!, $max_age: Int!) {\n  organization {\n    media(query: $query, max_age: $max_age , first: 1000) {\n      edges {\n        node {\n          title\n          public_at\n          hero_video {\n            video_urls {\n              url\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Details from E-commerce Page in JavaScript\nDESCRIPTION: Demonstrates how to extract various product details such as title, vendor, price, review count, and description from an e-commerce product page using cheerio selectors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';\nconst response = await gotScraping(productUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n\nconst product = {\n    title,\n    vendor,\n    price,\n    reviewCount,\n    description,\n};\n\nconsole.log(product);\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for Weather Data Processing\nDESCRIPTION: Imports necessary Python packages including io, os, ApifyClient, ActorJobStatus, and pandas for data processing and API interaction.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nimport os\n\nfrom apify_client import ApifyClient\nfrom apify_client.consts import ActorJobStatus\nimport pandas\n```\n\n----------------------------------------\n\nTITLE: Identifying Nested iFrames with Puppeteer\nDESCRIPTION: This code snippet shows how to identify nested iFrames by searching for specific elements within child frames. It demonstrates a more advanced technique for locating the desired iFrame containing a tweet list.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/scraping_iframes.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nlet twitterFrame;\n\nfor (const frame of page.mainFrame().childFrames()) {\n    if (frame.url().includes('twitter')) {\n        for (const nestedFrame of frame.childFrames()) {\n            const tweetList = await nestedFrame.$('.timeline-TweetList');\n            if (tweetList) {\n                console.log('We found the frame with tweet list');\n                twitterFrame = nestedFrame;\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Optimizing Dockerfile for Apify Actor with Node.js\nDESCRIPTION: This Dockerfile demonstrates best practices for building an Apify Actor image. It leverages layer caching by first copying and installing dependencies, then copying the rest of the source code. This approach speeds up builds when only the source code changes.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/performance.md#2025-04-18_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version \\\n    && rm -r ~/.npm\n\nCOPY . ./\n\nCMD npm start --silent\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Configuration\nDESCRIPTION: Extracts required environment variables for container port, URL, and key-value store identification.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst {\n    APIFY_CONTAINER_PORT,\n    APIFY_CONTAINER_URL,\n    APIFY_DEFAULT_KEY_VALUE_STORE_ID,\n} = process.env;\n```\n\n----------------------------------------\n\nTITLE: Extracting URLs from Sitemaps using Crawlee in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use the Crawlee library to find a website's robots.txt file, parse it for sitemaps, and extract all URLs from those sitemaps. It simplifies the process of crawling sitemaps by handling nested sitemaps, compressed files, and URL extraction.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-sitemaps.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RobotsFile } from 'crawlee';\n\nconst robots = await RobotsFile.find('https://www.mysite.com');\n\nconst allWebsiteUrls = await robots.parseUrlsFromSitemaps();\n```\n\n----------------------------------------\n\nTITLE: Managing Page Handling and Error Recovery in PuppeteerCrawler\nDESCRIPTION: Implements the page handling logic with session management, including error handling and browser instance retirement on failures. Successfully processed sessions are stored while failed ones are removed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst handlePageFunction = async ({ request, page, puppeteerPool }) => {\n    const { session } = request.userData;\n    console.log(`URL: ${request.url}, session: ${session.name}, userAgent: ${session.userAgent}`);\n\n    try {\n        // your main logic that is executed on each page\n        sessions[session.name] = session;\n    } catch (e) {\n        delete sessions[session.name];\n        await puppeteerPool.retire(page.browser());\n        throw e;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Emulating Geolocation with Playwright\nDESCRIPTION: Using Playwright's browserContext.setGeolocation() method to emulate geolocation settings. Should be combined with appropriate proxy settings matching the emulated location.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/geolocation.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nbrowserContext.setGeolocation()\n```\n\n----------------------------------------\n\nTITLE: Persisting State in ASINTracker with Apify SDK in JavaScript\nDESCRIPTION: Updates the ASINTracker class to persist state using the Apify SDK. It listens for the 'persistState' event and stores the state in the key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// asinTracker.js\nimport { Actor } from 'apify';\n// We've updated our constants.js file to include the name\n// of this new key in the key-value store\nconst { ASIN_TRACKER } = require('./constants');\n\nclass ASINTracker {\n    constructor() {\n        this.state = {};\n\n        Actor.on('persistState', async () => {\n            await Actor.setValue(ASIN_TRACKER, this.state);\n        });\n\n        setInterval(() => console.log(this.state), 10000);\n    }\n\n    incrementASIN(asin) {\n        if (this.state[asin] === undefined) {\n            this.state[asin] = 0;\n            return;\n        }\n\n        this.state[asin] += 1;\n    }\n}\n\nmodule.exports = new ASINTracker();\n```\n\n----------------------------------------\n\nTITLE: Scraping F1 News Dates from The Guardian\nDESCRIPTION: Script that scrapes Formula 1 news articles from The Guardian website using BeautifulSoup. Extracts article titles and publication dates from HTML time tags, parsing ISO 8601 datetime strings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nhttps://www.theguardian.com/sport/formulaone\n```\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n\nurl = \"https://www.theguardian.com/sport/formulaone\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor article in soup.select(\"#maincontent ul li\"):\n    title = article.select_one(\"h3\").text.strip()\n\n    time_iso = article.select_one(\"time\")[\"datetime\"].strip()\n    published_at = datetime.fromisoformat(time_iso)\n    published_on = published_at.date()\n\n    print(title, published_on)\n```\n\n----------------------------------------\n\nTITLE: Opening Datasets from Other Runs with Apify SDK in Python\nDESCRIPTION: This code shows how to open a dataset from another run using the Apify SDK in Python. The Actor.open_dataset() method allows accessing a dataset by name within an async context.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        other_dataset = await Actor.open_dataset(name='old-dataset')\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Frontend Pagination Implementation for Web and Puppeteer Scrapers\nDESCRIPTION: Code examples showing how to handle frontend pagination in both Web Scraper and Puppeteer Scraper. Demonstrates clicking the next pagination button to load more content dynamically.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n// Web Scraper\\\n$('li a span.pagination-next').click();\n\n// Puppeteer Scraper\\\nawait page.click('li a span.pagination-next');\n```\n\n----------------------------------------\n\nTITLE: Implementing Wait Functions in JavaScript Scraper\nDESCRIPTION: Examples of using the waitFor() function to handle different waiting scenarios in web scraping, including time-based waits, selector-based waits, and condition-based waits.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n// Waits for 2 seconds.\nawait waitFor(2000);\n// Waits until an element with id \"my-id\" appears\n// in the page.\nawait waitFor('#my-id');\n// Waits until a \"myObject\" variable appears\n// on the window object.\nawait waitFor(() => !!window.myObject);\n```\n\n----------------------------------------\n\nTITLE: Emulating Devices with Multiple Browser Contexts in Playwright\nDESCRIPTION: Demonstrates how to create multiple browser contexts in Playwright, each emulating a different device (iPhone and Android) using playwright.devices.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { chromium, devices } from 'playwright';\n\n// Launch the browser\nconst browser = await chromium.launch({ headless: false });\n\nconst iPhone = devices['iPhone 11 Pro'];\n// Create a new context for our iPhone emulation\nconst iPhoneContext = await browser.newContext({ ...iPhone });\n// Open a page on the newly created iPhone context\nconst iPhonePage = await iPhoneContext.newPage();\n\nconst android = devices['Galaxy Note 3'];\n// Create a new context for our Android emulation\nconst androidContext = await browser.newContext({ ...android });\n// Open a page on the newly created Android context\nconst androidPage = await androidContext.newPage();\n\n// The code in the next step will go here\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Python Web Scraper for F1 Articles\nDESCRIPTION: Python script that scrapes F1 news articles from The Guardian website, extracting author names and titles using httpx for HTTP requests and BeautifulSoup for HTML parsing. The script handles both individual authors and news agency contributors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n    return BeautifulSoup(response.text, \"html.parser\")\n\ndef parse_author(article_soup):\n    link = article_soup.select_one('aside a[rel=\"author\"]')\n    if link:\n        return link.text.strip()\n    address = article_soup.select_one('aside address')\n    if address:\n        return address.text.strip()\n    return None\n\nlisting_url = \"https://www.theguardian.com/sport/formulaone\"\nlisting_soup = download(listing_url)\nfor item in listing_soup.select(\"#maincontent ul li\"):\n    link = item.select_one(\"a\")\n    article_url = urljoin(listing_url, link[\"href\"])\n    article_soup = download(article_url)\n    title = article_soup.select_one(\"h1\").text.strip()\n    author = parse_author(article_soup)\n    print(f\"{author}: {title}\")\n```\n\n----------------------------------------\n\nTITLE: Complete Email Sending Automation with Playwright\nDESCRIPTION: This code snippet presents the complete implementation of email sending automation using Playwright. It includes logging into Yahoo, storing cookies, and sending multiple emails concurrently.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst emailsToSend = [\n    {\n        to: 'alice@example.com',\n        subject: 'Hello',\n        body: 'This is a message.',\n    },\n    {\n        to: 'bob@example.com',\n        subject: 'Testing',\n        body: 'I love the academy!',\n    },\n    {\n        to: 'carol@example.com',\n        subject: 'Apify is awesome!',\n        body: 'Some content.',\n    },\n];\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Login logic\nawait page.goto('https://www.yahoo.com/');\n\nawait page.click('button[name=\"agree\"]');\nawait page.waitForSelector('a:has-text(\"Sign in\")');\n\nawait page.click('a:has-text(\"Sign in\")');\nawait page.waitForLoadState('load');\n\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait page.click('input[name=\"signin\"]');\n\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait page.click('button[name=\"verifyPassword\"]');\nawait page.waitForLoadState('load');\n\nconst cookies = await browser.contexts()[0].cookies();\n\nawait page.close();\n\n// Email sending logic\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    const sendEmailContext = await browser.newContext();\n    await sendEmailContext.addCookies(cookies);\n    const page2 = await sendEmailContext.newPage();\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\nawait Promise.all(promises);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Implementing Metamorph in Python\nDESCRIPTION: Python implementation of a hotel review scraper using metamorph to transform into web-scraper Actor. Demonstrates async context management, input handling, and metamorph operation configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/metamorph.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Get input of your Actor\n        actor_input = await Actor.get_input() or {}\n\n        # Create input for apify/web-scraper\n        new_input = {\n            'startUrls': [{'url': actor_input['url']}],\n            'pageFunction': \"\"\"\n                # Here you pass the page function that\n                # scrapes all the reviews ...\n            \"\"\",\n            # ... and here would be all the additional input parameters\n        }\n\n        # Transform the Actor run to apify/web-scraper with the new input\n        await Actor.metamorph('apify/web-scraper', new_input)\n\n        # The line here will never be reached, because the Actor run will be interrupted\n        Actor.log.info('This should not be printed')\n```\n\n----------------------------------------\n\nTITLE: Creating Result Objects for Zappos Scraper in JavaScript\nDESCRIPTION: JavaScript code showing how to construct the result objects that match the dataset schema. This example demonstrates scraping product data from Zappos and structuring it to work with the defined schema.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/output_schema.md#2025-04-18_snippet_2\n\nLANGUAGE: js\nCODE:\n```\nconst results = {\n    url: request.loadedUrl,\n    imgUrl: $('#stage button[data-media=\"image\"] img[itemprop=\"image\"]').attr('src'),\n    brand: $('span[itemprop=\"brand\"]').text().trim(),\n    name: $('meta[itemprop=\"name\"]').attr('content'),\n    SKU: $('*[itemprop~=\"sku\"]').text().trim(),\n    inStock: !request.url.includes('oosRedirected=true'),\n    onSale: !$('div[itemprop=\"offers\"]').text().includes('OFF'),\n    price: $('span[itemprop=\"price\"]').text(),\n};\n```\n\n----------------------------------------\n\nTITLE: Blocking Resources with Puppeteer\nDESCRIPTION: Shows how to use Puppeteer to launch a browser, create a new page, enable request interception, and block requests for specific file types.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Enable request interception (skipping this step will result in an error)\nawait page.setRequestInterception(true);\n\n// Listen for all requests\npage.on('request', async (req) => {\n    // If the request ends in a blocked extension, abort the request\n    if (blockedExtensions.some((str) => req.url().endsWith(str))) return req.abort();\n    // Otherwise, continue\n    await req.continue();\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Defining Input Schema in JSON5 for Website Content Crawler Actor\nDESCRIPTION: This code snippet shows how to define an input schema for the Website Content Crawler Actor using JSON5. It includes various field types and configurations that the Apify platform supports for generating the input UI.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/index.md#2025-04-18_snippet_0\n\nLANGUAGE: json5\nCODE:\n```\n{\n    \"title\": \"Input schema for Website Content Crawler\",\n    \"description\": \"Enter the start URL(s) of the website(s) to crawl, configure other optional settings, and run the Actor to crawl the pages and extract their text content.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"startUrls\": {\n            \"title\": \"Start URLs\",\n            \"type\": \"array\",\n            \"description\": \"One or more URLs of the pages where the crawler will start. Note that the Actor will additionally only crawl sub-pages of these URLs. For example, for the start URL `https://www.example.com/blog`, it will crawl pages like `https://example.com/blog/article-1`, but will skip `https://example.com/docs/something-else`.\",\n            \"editor\": \"requestListSources\",\n            \"prefill\": [{ \"url\": \"https://docs.apify.com/\" }]\n        },\n        \"crawlerType\": {\n            \"sectionCaption\": \"Crawler settings\",\n            \"title\": \"Crawler type\",\n            \"type\": \"string\",\n            \"enum\": [\"playwright:chrome\", \"cheerio\", \"jsdom\"],\n            \"enumTitles\": [\"Headless web browser (Chrome+Playwright)\", \"Raw HTTP client (Cheerio)\", \"Raw HTTP client with JS execution (JSDOM) (experimental!)\"],\n            \"description\": \"Select the crawling engine:\\n- **Headless web browser** (default) - Useful for modern websites with anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it requires more computing resources and is slower. It is recommended to use at least 8 GB of RAM.\\n- **Raw HTTP client** - High-performance crawling mode that uses raw HTTP requests to fetch the pages. It is faster and cheaper, but it might not work on all websites.\",\n            \"default\": \"playwright:chrome\"\n        },\n        \"maxCrawlDepth\": {\n            \"title\": \"Max crawling depth\",\n            \"type\": \"integer\",\n            \"description\": \"The maximum number of links starting from the start URL that the crawler will recursively descend. The start URLs have a depth of 0, the pages linked directly from the start URLs have a depth of 1, and so on.\\n\\nThis setting is useful to prevent accidental crawler runaway. By setting it to 0, the Actor will only crawl start URLs.\",\n            \"minimum\": 0,\n            \"default\": 20\n        },\n        \"maxCrawlPages\": {\n            \"title\": \"Max pages\",\n            \"type\": \"integer\",\n            \"description\": \"The maximum number pages to crawl. It includes the start URLs, pagination pages, pages with no content, etc. The crawler will automatically finish after reaching this number. This setting is useful to prevent accidental crawler runaway.\",\n            \"minimum\": 0,\n            \"default\": 9999999\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: CSV Export Function\nDESCRIPTION: Function that exports data to a CSV file, automatically determining field names from the data structure.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n```\n\n----------------------------------------\n\nTITLE: Complete Filter Actor Implementation\nDESCRIPTION: Complete implementation of the filter Actor including initialization, data processing, and result storage.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst { datasetId } = await Actor.getInput();\nconst dataset = await Actor.openDataset(datasetId);\n\nconst { items } = await dataset.getData();\n\nconst filtered = items.reduce((acc, curr) => {\n    const prevPrice = acc?.[curr.asin] ? +acc[curr.asin].offer.slice(1) : null;\n    const price = +curr.offer.slice(1);\n\n    if (!acc[curr.asin] || prevPrice > price) acc[curr.asin] = curr;\n\n    return acc;\n}, {});\n\nawait Actor.pushData(Object.values(filtered));\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Configuring MITM Proxy to Block or Allow Requests in JavaScript\nDESCRIPTION: This code demonstrates how to configure the proxy to handle intercepted requests. It sets up an onRequest handler that can block requests based on a condition variable, logging blocked requests and returning custom content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/using_proxy_to_intercept_requests_puppeteer.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Setup blocking of requests in proxy\nconst proxyPort = 8000;\nconst proxy = setupProxy(proxyPort);\nproxy.onRequest((context, callback) => {\n    if (blockRequests) {\n        const request = context.clientToProxyRequest;\n        // Log out blocked requests\n        console.log('Blocked request:', request.headers.host, request.url);\n\n        // Close the connection with custom content\n        context.proxyToClientResponse.end('Blocked');\n        return;\n    }\n    return callback();\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Markdown Linter for Exercise Section\nDESCRIPTION: This snippet disables a specific markdown linting rule for the next line, likely to allow a non-standard markdown structure for the exercise section.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/_exercises.mdx#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<!-- markdownlint-disable-next-line MD041 -->\n```\n\n----------------------------------------\n\nTITLE: Constructing a US-only Proxy URL\nDESCRIPTION: This snippet demonstrates how to construct a proxy URL that selects proxies only from the United States. It's used to restrict the geographical location of the proxy servers used in web scraping tasks.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/bypassing_anti_scraping.md#2025-04-18_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nhttp://proxy.apify.com:8000\n```\n\n----------------------------------------\n\nTITLE: Basic Playwright Setup for Web Scraping\nDESCRIPTION: Initial setup code for launching Playwright browser and navigating to target page. Creates a new browser instance and page object, then navigates to a demo webstore.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\n// code will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Opening Request Queues in JavaScript SDK\nDESCRIPTION: Demonstrates how to open default and named request queues using the Apify JavaScript SDK. It shows the usage of Actor.openRequestQueue() method for managing request queues in a JavaScript Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// Import the JavaScript SDK into your project\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\n// Open the default request queue associated with\n// the Actor run\nconst queue = await Actor.openRequestQueue();\n\n// Open the 'my-queue' request queue\nconst queueWithName = await Actor.openRequestQueue('my-queue');\n\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Setting Session Rotation Parameters in Crawlee\nDESCRIPTION: Configures a SessionPool to discard sessions after 5 uses and to trash sessions that receive errors. This helps maintain clean sessions for scraping.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    requestList,\n    requestQueue,\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        sessionOptions: {\n            maxUsageCount: 5,\n            maxErrorScore: 1,\n        },\n    },\n    maxConcurrency: 50,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Clicking an Element with Playwright\nDESCRIPTION: Demonstrates how to click the 'Accept all' button on Google's cookie policy using Playwright's text-based selector.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Click the \"Accept all\" button\nawait page.click('button:has-text(\"Accept all\")');\n```\n\n----------------------------------------\n\nTITLE: Data Sorting Implementation\nDESCRIPTION: Implementation of product data sorting function with TypeScript type safety and handling of sort orders.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst sortData = (products: Product[], order: SortOrder) => {\n    switch (order) {\n        case SortOrder.ASC:\n            return [...products].sort((a, b) => a.price - b.price);\n        case SortOrder.DESC:\n            return [...products].sort((a, b) => b.price - a.price);\n        default:\n            return products;\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Proxy Configuration Structure in JSON5\nDESCRIPTION: This snippet illustrates the structure of the proxy configuration object. It includes properties for useApifyProxy, apifyProxyGroups, and proxyUrls.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_9\n\nLANGUAGE: json5\nCODE:\n```\n{\n    // Indicates whether Apify Proxy was selected.\n    \"useApifyProxy\": Boolean,\n\n    // Array of Apify Proxy groups. Is missing or null if\n    // Apify Proxy's automatic mode was selected\n    // or if proxies are not used.\n    \"apifyProxyGroups\": String[],\n\n    // Array of custom proxy URLs.\n    // Is missing or null if custom proxies were not used.\n    \"proxyUrls\": String[],\n}\n```\n\n----------------------------------------\n\nTITLE: Using PuppeteerCrawler with Apify Proxy in JavaScript\nDESCRIPTION: This snippet demonstrates how to use PuppeteerCrawler with Apify Proxy configuration. It initializes the Actor, creates a proxy configuration, and sets up a crawler to visit a URL and log the page content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ page }) {\n        console.log(await page.content());\n    },\n});\n\nawait crawler.run(['https://proxy.apify.com/?format=json']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Extracting URLs from Sitemaps with Crawlee's RobotsFile\nDESCRIPTION: This snippet demonstrates how to automatically extract all URLs from a website's sitemaps using Crawlee's RobotsFile class.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { RobotsFile } from 'crawlee';\n\nconst robots = await RobotsFile.find('https://www.mysite.com');\n\nconst allWebsiteUrls = await robots.parseUrlsFromSitemaps();\n```\n\n----------------------------------------\n\nTITLE: Running the LangGraph Agent with Apify Integration\nDESCRIPTION: Executes the agent with a query to search for an OpenAI TikTok profile and analyze it, streaming the results as they become available. The agent will use the configured tools to perform web searches and extract TikTok data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor state in agent_executor.stream(\n    stream_mode=\"values\",\n    input={\n        \"messages\": [\n            HumanMessage(content=\"Search the web for OpenAI TikTok profile and analyze their profile.\")\n        ]\n    }):\n    state[\"messages\"][-1].pretty_print()\n```\n\n----------------------------------------\n\nTITLE: Parsing Product Data with URL Handling in Python\nDESCRIPTION: A function that extracts product information from HTML elements, including properly joining relative URLs with a base URL. It returns a dictionary with product title, price information, and full URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef parse_product(product, base_url):\n    title_element = product.select_one(\".product-item__title\")\n    title = title_element.text.strip()\n    url = urljoin(base_url, title_element[\"href\"])\n\n    ...\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price, \"url\": url}\n```\n\n----------------------------------------\n\nTITLE: Initializing ApifyClient and Running Weather Scraper Actor\nDESCRIPTION: Sets up the ApifyClient, runs the weather scraper Actor, and checks for successful completion before accessing the dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the main ApifyClient instance\nclient = ApifyClient(os.environ['APIFY_TOKEN'], api_url=os.environ['APIFY_API_BASE_URL'])\n\n# Run the weather scraper and wait for it to finish\nprint('Downloading the weather data...')\nscraper_run = client.actor('~bbc-weather-scraper').call()\n\n# Check if the scraper finished successfully, otherwise raise an error\nif scraper_run['status'] != ActorJobStatus.SUCCEEDED:\n    raise RuntimeError('The weather scraper run has failed')\n\n# Get the resource sub-client for working with the dataset with the source data\ndataset_client = client.dataset(scraper_run['defaultDatasetId'])\n```\n\n----------------------------------------\n\nTITLE: Handling Validation Errors in JavaScript\nDESCRIPTION: Shows how to catch and handle dataset validation errors in JavaScript using try-catch blocks with the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ntry {\n    const response = await Actor.pushData(items);\n} catch (error) {\n    if (!error.data?.invalidItems) throw error;\n    error.data.invalidItems.forEach((item) => {\n        const { itemPosition, validationErrors } = item;\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Dockerfile for Python Apify Actor\nDESCRIPTION: A complete Dockerfile for a Python Actor. It uses the Apify Python base image, installs dependencies from requirements.txt, and copies the source code. It includes a CMD instruction to specify how to run the Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_2\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# First, specify the base Docker image.\n# You can also use any other image from Docker Hub.\nFROM apify/actor-python:3.9\n\n# Second, copy just requirements.txt into the Actor image,\n# since it should be the only file that affects \"pip install\" in the next step,\n# in order to speed up the build\nCOPY requirements.txt ./\n\n# Install the packages specified in requirements.txt,\n# Print the installed Python version, pip version\n# and all installed packages with their versions for debugging\nRUN echo \"Python version:\" \\\n && python --version \\\n && echo \"Pip version:\" \\\n && pip --version \\\n && echo \"Installing dependencies from requirements.txt:\" \\\n && pip install -r requirements.txt \\\n && echo \"All installed Python packages:\" \\\n && pip freeze\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after installing the dependencies, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n# Specify how to launch the source code of your Actor.\n# By default, the main.py file is run\nCMD python3 main.py\n```\n\n----------------------------------------\n\nTITLE: Handling Dynamic Content and Pagination in Puppeteer for Apify Store Scraping\nDESCRIPTION: This code snippet demonstrates how to handle dynamic content and implement pagination when scraping the Apify Store. It uses Puppeteer's waitFor and click methods to interact with the 'Show more' button for loading additional actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Waits for 2 seconds.\nawait page.waitFor(2000);\n// Waits until an element with id \"my-id\" appears in the page.\nawait page.waitFor('#my-id');\n// Waits until a \"myObject\" variable appears\n// on the window object.\nawait page.waitFor(() => !!window.myObject);\n\n// Wait for the 'Show more' button\nawait page.waitFor('div.show-more > button');\n\n// Click the 'Show more' button\nawait page.click('div.show-more > button');\n```\n\n----------------------------------------\n\nTITLE: Basic jQuery Injection in Page Function\nDESCRIPTION: Simple example of injecting jQuery into a page using Apify's utility function within a page function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { Apify, page } = context;\n    await Apify.utils.puppeteer.injectJQuery(page);\n\n    // your code ...\n}\n```\n\n----------------------------------------\n\nTITLE: Refactoring Page Function for Better Organization in JavaScript\nDESCRIPTION: This code snippet demonstrates how to refactor the page function for better organization and maintainability. It separates the logic for handling different page types into separate functions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error('Unknown request label.');\n    }\n\n    async function handleStart({ log, waitFor, $ }) {\n        log.info('Store opened!');\n\n        const dataJson = $('#__NEXT_DATA__').html();\n        // We requested HTML, but the data are actually JSON.\n        const data = JSON.parse(dataJson);\n\n        for (const item of data.props.pageProps.items) {\n            const { name, username } = item;\n            const actorDetailUrl = `https://apify.com/${username}/${name}`;\n            await context.enqueueRequest({\n                url: actorDetailUrl,\n                userData: {\n                    label: 'DETAIL',\n                },\n            });\n        }\n    }\n\n    async function handleDetail({ request, log, skipLinks, $ }) {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Event-Driven Flow Examples in JavaScript\nDESCRIPTION: Demonstrates proper event-driven programming patterns using Puppeteer/Playwright, showing both good and bad practices for handling asynchronous operations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// Avoid:\nawait page.waitForTimeout(timeout);\n\n// Good:\nawait page.waitForFunction(myFunction, options, args);\n\n// Good:\nawait page.waitForFunction(() => {\n    return window.location.href.includes('path');\n});\n\n// Good:\nawait page.waitForFunction(\n    (selector) => document.querySelector(selector).innerText,\n    { polling: 'mutation' },\n    '[data-qa=\"btnAppleSignUp\"]',\n);\n```\n\n----------------------------------------\n\nTITLE: Emulating Devices with Multiple Browser Contexts in Puppeteer\nDESCRIPTION: Shows how to create multiple browser contexts in Puppeteer, each emulating a different device (iPhone and Android) using puppeteer.devices.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\n// Launch the browser\nconst browser = await puppeteer.launch({ headless: false });\n\nconst iPhone = puppeteer.devices['iPhone 11 Pro'];\n// Create a new context for our iPhone emulation\nconst iPhoneContext = await browser.createIncognitoBrowserContext();\n// Open a page on the newly created iPhone context\nconst iPhonePage = await iPhoneContext.newPage();\n// Emulate the device\nawait iPhonePage.emulate(iPhone);\n\nconst android = puppeteer.devices['Galaxy Note 3'];\n// Create a new context for our Android emulation\nconst androidContext = await browser.createIncognitoBrowserContext();\n// Open a page on the newly created Android context\nconst androidPage = await androidContext.newPage();\n// Emulate the device\nawait androidPage.emulate(android);\n\n// The code in the next step will go here\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Waiting for Download Completion in Puppeteer\nDESCRIPTION: This snippet demonstrates a simple wait operation to allow time for the file download to complete. In real scenarios, you would check the file system for the download status.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.waitFor(60000);\n```\n\n----------------------------------------\n\nTITLE: Injecting Fingerprints into Playwright Browser (JavaScript)\nDESCRIPTION: This snippet demonstrates how to generate a fingerprint, create a Playwright browser context with the generated fingerprint, and inject the fingerprint into the context. It uses both the fingerprint-generator and fingerprint-injector packages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport FingerprintGenerator from 'fingerprint-generator';\nimport { FingerprintInjector } from 'fingerprint-injector';\nimport { chromium } from 'playwright';\n\n// Instantiate a fingerprint injector\nconst fingerprintInjector = new FingerprintInjector();\n\n// Launch a browser in Playwright\nconst browser = await chromium.launch();\n\n// Instantiate the fingerprint generator with\n// configuration options\nconst fingerprintGenerator = new FingerprintGenerator({\n    browsers: [\n        { name: 'firefox', minVersion: 80 },\n    ],\n    devices: [\n        'desktop',\n    ],\n    operatingSystems: [\n        'windows',\n    ],\n});\n\n// Grab a fingerprint\nconst generated = fingerprintGenerator.getFingerprint({\n    locales: ['en-US', 'en'],\n});\n\n// Create a new browser context, plugging in\n// some values from the fingerprint\nconst context = await browser.newContext({\n    userAgent: generated.fingerprint.userAgent,\n    locale: generated.fingerprint.navigator.language,\n});\n\n// Attach the fingerprint to the newly created\n// browser context\nawait fingerprintInjector.attachFingerprintToPlaywright(context, generated);\n\n// Create a new page and go to Google\nconst page = await context.newPage();\nawait page.goto('https://google.com');\n```\n\n----------------------------------------\n\nTITLE: Navigating to a Website with Multiple Browser Contexts\nDESCRIPTION: Demonstrates how to navigate to a website (deviceinfo.me) simultaneously using multiple browser contexts and wait for a specified timeout.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Go to deviceinfo.me on both at the same time\nawait Promise.all([iPhonePage.goto('https://www.deviceinfo.me/'), androidPage.goto('https://www.deviceinfo.me/')]);\n\n// Wait for 10 seconds on both before shutting down\nawait Promise.all([iPhonePage.waitForTimeout(10000), androidPage.waitForTimeout(10000)]);\n```\n\n----------------------------------------\n\nTITLE: Exporting Dataset to CSV with Crawlee\nDESCRIPTION: Example showing how to export scraped data to a CSV file using Dataset.exportToCSV() after the crawler completes its run. The CSV file will be saved in the key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// ...\nawait crawler.run();\n// Add this line to export to CSV.\nawait Dataset.exportToCSV('results');\n```\n\n----------------------------------------\n\nTITLE: Modifying GraphQL Query Variables\nDESCRIPTION: This snippet demonstrates how to modify the variables in a GraphQL query without changing the query structure. It shows how to change the search term from 'test' to 'cats'.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/modifying_variables.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"...\": \"...\",\n    \"variables\": { \"query\": \"cats\",\"count\": 10,\"cursor\": null }\n}\n```\n\n----------------------------------------\n\nTITLE: Emulating Geolocation with Puppeteer\nDESCRIPTION: Using Puppeteer's page.setGeolocation() function to override browser geolocation settings. This should be used with a matching proxy location to avoid detection.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/geolocation.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\npage.setGeolocation()\n```\n\n----------------------------------------\n\nTITLE: Creating a Weather Prediction Plot with Matplotlib\nDESCRIPTION: This snippet uses Matplotlib to create a plot of the processed weather data, setting titles, labels, and formatting options.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint('Plotting the data...')\naxes = mean_daily_temperatures.plot(figsize=(10, 5))\naxes.set_title('Weather prediction for holiday destinations')\naxes.set_xlabel(None)\naxes.yaxis.set_major_formatter(lambda val, _: f'{int(val)} °C')\naxes.grid(which='both', linestyle='dotted')\naxes.legend(loc='best')\naxes.figure.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Creating a Complex Function Type in TypeScript\nDESCRIPTION: This example shows how to create a complex function type alias for a function with multiple optional parameters and union return types. It demonstrates how to simplify verbose function declarations using type aliases.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/type_aliases.md#2025-04-18_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ntype AddFunction = (numbers: number[], toString?: boolean, printResult?: boolean, printWithMessage?: string) => number | string | void;\n\nconst addAll: AddFunction = (nums, toString, printResult, printWithMessage) => {\n    const result = nums.reduce((prev, curr) => prev + curr, 0);\n\n    if (!printResult) return toString ? result.toString() : result;\n\n    console.log(printWithMessage || 'Result:', toString ? result.toString : result);\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing a Number Addition Actor in JavaScript\nDESCRIPTION: This code snippet demonstrates how to create an Actor that takes two numbers as input, adds them together, and stores the result in the default dataset. It uses the Apify SDK to handle input retrieval and data storage.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/inputs_outputs.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Grab our numbers which were inputted\nconst { num1, num2 } = await Actor.getInput();\n\n// Calculate the solution\nconst solution = num1 + num2;\n\n// Push the solution to the dataset\nawait Actor.pushData({ solution });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Adding Items to a Dataset via API\nDESCRIPTION: API endpoint for adding new data items to an existing dataset. This requires a POST request with a JSON payload containing the data to be added.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/datasets/{DATASET_ID}/items\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running Qdrant Integration with Apify\nDESCRIPTION: Python code that configures and executes the Qdrant integration Actor to store crawled data in the Qdrant vector database with embeddings generation and chunking settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/qdrant.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nqdrant_integration_inputs = {\n    \"qdrantUrl\": QDRANT_URL,\n    \"qdrantApiKey\": QDRANT_API_KEY,\n    \"qdrantCollectionName\": QDRANT_COLLECTION_NAME,\n    \"qdrantAutoCreateCollection\": True,\n    \"datasetId\": actor_call[\"defaultDatasetId\"],\n    \"datasetFields\": [\"text\"],\n    \"enableDeltaUpdates\": True,\n    \"deltaUpdatesPrimaryDatasetFields\": [\"url\"],\n    \"deleteExpiredObjects\": True,\n    \"expiredObjectDeletionPeriodDays\": 30,\n    \"embeddingsProvider\": \"OpenAI\",\n    \"embeddingsApiKey\": OPENAI_API_KEY,\n    \"performChunking\": True,\n    \"chunkSize\": 1000,\n    \"chunkOverlap\": 0,\n}\nactor_call = client.actor(\"apify/qdrant-integration\").call(run_input=qdrant_integration_inputs)\n\n```\n\n----------------------------------------\n\nTITLE: Complete Page Function Implementation with Detail Scraping\nDESCRIPTION: Full implementation of the page function showing both pagination and detail page scraping logic. Includes parallel data extraction and type conversion for different fields.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { request, log, skipLinks, page } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        let timeout; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await page.waitFor(buttonSelector, { timeout });\n                // 2 sec timeout after the first.\n                timeout = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            await page.click(buttonSelector);\n        }\n    }\n\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Get attributes in parallel to speed up the process.\n        const titleP = page.$eval(\n            'header h1',\n            (el) => el.textContent,\n        );\n        const descriptionP = page.$eval(\n            'header span.actor-description',\n            (el) => el.textContent,\n        );\n        const modifiedTimestampP = page.$eval(\n            'ul.ActorHeader-stats time',\n            (el) => el.getAttribute('datetime'),\n        );\n        const runCountTextP = page.$eval(\n            'ul.ActorHeader-stats > li:nth-of-type(3)',\n            (el) => el.textContent,\n        );\n\n        const [\n            title,\n            description,\n            modifiedTimestamp,\n            runCountText,\n        ] = await Promise.all([\n            titleP,\n            descriptionP,\n            modifiedTimestampP,\n            runCountTextP,\n        ]);\n\n        const modifiedDate = new Date(Number(modifiedTimestamp));\n        const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n        return {\n            url,\n            uniqueIdentifier,\n            title,\n            description,\n            modifiedDate,\n            runCount,\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Blocking Resources with CDP Session in Playwright\nDESCRIPTION: Demonstrates how to use a Chrome DevTools Protocol (CDP) session in Playwright to block resources while maintaining browser cache functionality.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\n// Note, you can't use CDP session in other browsers!\n// Only in Chromium.\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Define our blocked extensions\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Use CDP session to block resources\nconst client = await page.context().newCDPSession(page);\n\nawait client.send('Network.setBlockedURLs', { urls: blockedExtensions });\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Saving Scraped Data to Apify Dataset\nDESCRIPTION: Shows how to save the final merged result object to the Apify dataset using the pushData method. This is typically the final step in processing scraped data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_4\n\nLANGUAGE: js\nCODE:\n```\nawait Apify.pushData(result);\n```\n\n----------------------------------------\n\nTITLE: Extracting Multiple Data Points from Product Elements with JavaScript\nDESCRIPTION: This snippet demonstrates how to extract both title and price from each product element, create an object with this data, and push it into a results array.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = [];\n\nfor (const product of products) {\n    const titleElement = product.querySelector('a.product-item__title');\n    const title = titleElement.textContent.trim();\n\n    const priceElement = product.querySelector('span.price');\n    const price = priceElement.childNodes[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Batch Operations with Request Queues in Python\nDESCRIPTION: This code demonstrates how to perform batch operations on request queues using the Python Apify client. It shows how to add multiple requests to a queue and how to delete multiple requests from a queue in a single API call.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfrom apify_client import ApifyClient\n\napify_client = ApifyClient('MY-APIFY-TOKEN')\n\nrequest_queue_client = apify_client.request_queue('my-queue-id')\n\n# Add multiple requests to the queue\nrequest_queue_client.batch_add_requests([\n    {'url': 'http://example.com/foo', 'uniqueKey': 'http://example.com/foo', 'method': 'GET'},\n    {'url': 'http://example.com/bar', 'uniqueKey': 'http://example.com/bar', 'method': 'GET'},\n])\n\n# Remove multiple requests from the queue\nrequest_queue_client.batch_delete_requests([\n    {'uniqueKey': 'http://example.com/foo'},\n    {'uniqueKey': 'http://example.com/bar'},\n])\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object for XML Conversion Example\nDESCRIPTION: Demonstrates a JavaScript object structure that will be converted to XML format, showcasing how object properties are transformed into XML tags and their values into children of these tags.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    name: 'Rashida Jones',\n    address: [\n        {\n            type: 'home',\n            street: '21st',\n            city: 'Chicago',\n        },\n        {\n            type: 'office',\n            street: null,\n            city: null,\n        },\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Using Node.js axios with Apify Proxy Authentication\nDESCRIPTION: Demonstrates how to configure axios in Node.js to use Apify's proxy with authentication. It specifies the proxy host, port, and authentication details, then makes a request to retrieve proxy information in JSON format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport axios from 'axios';\n\nconst proxy = {\n    protocol: 'http',\n    host: 'proxy.apify.com',\n    port: 8000,\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    auth: { username: 'auto', password: '<YOUR_PROXY_PASSWORD>' },\n};\n\nconst url = 'http://proxy.apify.com/?format=json';\n\nconst { data } = await axios.get(url, { proxy });\n\nconsole.log(data);\n```\n\n----------------------------------------\n\nTITLE: Validating Runtime Statistics in JavaScript\nDESCRIPTION: This snippet checks the Actor's runtime statistics, including the number of request retries and overall runtime.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/automated_tests.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait expectAsync(runResult).withStatistics((stats) => {\n    // In most cases, you want it to be as close to zero as possible\n    expect(stats.requestsRetries)\n        .withContext(runResult.format('Request retries'))\n        .toBeLessThan(3);\n\n    // What is the expected run time for the number of items?\n    expect(stats.crawlerRuntimeMillis)\n        .withContext(runResult.format('Run time'))\n        .toBeWithinRange(1 * 60000, 10 * 60000);\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing and Modifying All Browser Contexts in Playwright\nDESCRIPTION: Shows how to loop through all browser contexts in Playwright and add an event listener to log when a specific site is visited.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nfor (const context of browser.contexts()) {\n    // In Playwright, lots of events are supported in the \"on\" function of\n    // a BrowserContext instance\n    context.on('request', (req) => req.url() === 'https://www.deviceinfo.me/' && console.log('Site visited'));\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Clickable Elements Selector for JavaScript Links in Puppeteer Scraper\nDESCRIPTION: Shows how to set a selector for clickable elements that cause navigation, allowing Puppeteer Scraper to automatically click and enqueue these elements. This example is for a Turkish Remax website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n'a[onclick ^= getPage]';\n```\n\n----------------------------------------\n\nTITLE: Extracting Actor Data with Complex Selectors and Regex\nDESCRIPTION: Extracts title, description, modified date, and run count from an Actor page. Uses complex CSS selectors and regular expressions to parse formatted numbers, removing commas and converting to numeric values.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { jQuery: $ } = context;\n\n    // ... rest of the code\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Apify-Haystack Integration Example\nDESCRIPTION: A complete Python script that combines all the previous steps into a single file for crawling a website, processing its content, and retrieving documents using both BM25 and vector search.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.preprocessors import DocumentSplitter\nfrom haystack.components.retrievers import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\n\nfrom apify_haystack import ApifyDatasetFromActorCall\n\nos.environ[\"APIFY_API_TOKEN\"] = \"YOUR-APIFY-API-TOKEN\"\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR-OPENAI-API-KEY\"\n\ndocument_loader = ApifyDatasetFromActorCall(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\n        \"maxCrawlPages\": 3,  # limit the number of pages to crawl\n        \"startUrls\": [{\"url\": \"https://haystack.deepset.ai/\"}],\n    },\n    dataset_mapping_function=lambda item: Document(content=item[\"text\"] or \"\", meta={\"url\": item[\"url\"]}),\n)\n\ndocument_store = InMemoryDocumentStore()\nprint(f\"Initialized InMemoryDocumentStore with {document_store.count_documents()} documents\")\n\ndocument_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\ndocument_embedder = OpenAIDocumentEmbedder()\ndocument_writer = DocumentWriter(document_store)\n\npipe = Pipeline()\npipe.add_component(\"document_loader\", document_loader)\npipe.add_component(\"document_splitter\", document_splitter)\npipe.add_component(\"document_embedder\", document_embedder)\npipe.add_component(\"document_writer\", document_writer)\n\npipe.connect(\"document_loader\", \"document_splitter\")\npipe.connect(\"document_splitter\", \"document_embedder\")\npipe.connect(\"document_embedder\", \"document_writer\")\n\nprint(\"\\nCrawling will take some time ...\")\nprint(\"You can visit https://console.apify.com/actors/runs to monitor the progress\\n\")\n\npipe.run({})\nprint(f\"Added {document_store.count_documents()} to vector from Website Content Crawler\")\n\nprint(\"\\n ### Retrieving documents from the document store using BM25 ###\\n\")\nprint(\"query='Haystack'\\n\")\n\nbm25_retriever = InMemoryBM25Retriever(document_store)\n\nfor doc in bm25_retriever.run(\"Haystack\", top_k=1)[\"documents\"]:\n    print(doc.content)\n\nprint(\"\\n ### Retrieving documents from the document store using vector similarity ###\\n\")\nretrieval_pipe = Pipeline()\nretrieval_pipe.add_component(\"embedder\", OpenAITextEmbedder())\nretrieval_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=1))\n\nretrieval_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n\nresults = retrieval_pipe.run({\"embedder\": {\"text\": \"What is Haystack?\"}})\n\nfor doc in results[\"retriever\"][\"documents\"]:\n    print(doc.content)\n```\n\n----------------------------------------\n\nTITLE: Running Actors with JavaScript Client\nDESCRIPTION: Example of running the Google Maps Scraper Actor using the JavaScript API client. Shows how to initialize the client, start an actor run, and fetch results from the dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/index.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ApifyClient } from 'apify-client';\n\nconst client = new ApifyClient({\n    token: 'MY-API-TOKEN',\n});\n\n// Start the Google Maps Scraper Actor and wait for it to finish.\nconst actorRun = await client.actor('compass/crawler-google-places').call({\n    queries: 'apify',\n});\n// Fetch scraped results from the Actor's dataset.\nconst { items } = await client.dataset(actorRun.defaultDatasetId).listItems();\nconsole.dir(items);\n```\n\n----------------------------------------\n\nTITLE: Using Apify Proxy with Python SDK and Requests\nDESCRIPTION: This Python script demonstrates how to use Apify Proxy with the requests library. It creates a proxy configuration, generates a new proxy URL, and makes multiple requests to a specified endpoint using the proxy.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\nimport requests, asyncio\n\nasync def main():\n    async with Actor:\n        proxy_configuration = await Actor.create_proxy_configuration()\n        proxy_url = await proxy_configuration.new_url()\n        proxies = {\n            'http': proxy_url,\n            'https': proxy_url,\n        }\n\n        for _ in range(10):\n            response = requests.get('https://api.apify.com/v2/browser-info', proxies=proxies)\n            print(response.text)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Authentication Request with Bearer Token\nDESCRIPTION: Example of how to authenticate a request to an Actor in Standby mode using a bearer token in the Authorization header.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/actor_standby.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -H \"Authorization: Bearer my_apify_token\" \\\n  https://rag-web-browser.apify.actor/search?query=apify\n```\n\n----------------------------------------\n\nTITLE: Implementing State Persistence with Actor.on() - JavaScript\nDESCRIPTION: Example showing how to persist state during migration events using the Apify SDK in JavaScript. Uses Actor.on() to listen for migration events and setValue() to save state.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nActor.on('migrating', () => {\n    Actor.setValue('my-crawling-state', {\n        foo: 'bar',\n    });\n});\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Downloading HTML with Got-scraping in Node.js\nDESCRIPTION: This snippet demonstrates how to use the Got-scraping library to download HTML from a specific URL. It uses async/await syntax to handle the asynchronous request.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_js_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { gotScraping } from 'got-scraping';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\nconsole.log(html);\n```\n\n----------------------------------------\n\nTITLE: Getting Actor Input in JavaScript\nDESCRIPTION: Demonstrates how to access the Actor's input object stored in the default key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst input = await Actor.getInput();\nconsole.log(input);\n// prints: {'option1': 'aaa', 'option2': 456}\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Building a Haystack Pipeline for Document Processing\nDESCRIPTION: Creates a Haystack pipeline that connects the document loader, splitter, embedder, and writer components to process and store the web content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocument_store = InMemoryDocumentStore()\n\ndocument_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\ndocument_embedder = OpenAIDocumentEmbedder()\ndocument_writer = DocumentWriter(document_store)\n\npipe = Pipeline()\npipe.add_component(\"document_loader\", document_loader)\npipe.add_component(\"document_splitter\", document_splitter)\npipe.add_component(\"document_embedder\", document_embedder)\npipe.add_component(\"document_writer\", document_writer)\n\npipe.connect(\"document_loader\", \"document_splitter\")\npipe.connect(\"document_splitter\", \"document_embedder\")\npipe.connect(\"document_embedder\", \"document_writer\")\n```\n\n----------------------------------------\n\nTITLE: Error Handling Examples in JavaScript\nDESCRIPTION: These snippets demonstrate poor and good error logging practices. They highlight the importance of providing context and useful information in error messages for better debugging and user experience.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/best_practices.md#2025-04-18_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nCannot read property \"0\" from undefined\n```\n\nLANGUAGE: text\nCODE:\n```\nCould not parse an address, skipping the page. Url: https://www.example-website.com/people/1234\n```\n\nLANGUAGE: text\nCODE:\n```\nWe could not parse the price of product: Men's Trainers Orange, pushing anyways.\n```\n\n----------------------------------------\n\nTITLE: Making API Request to Apify's Google SERP API\nDESCRIPTION: This snippet shows the API endpoint format for running a Google SERP task synchronously. The request requires your API token and should include the necessary input parameters as a JSON object in the request body.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/apify_free_google_serp_api.md#2025-04-18_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttps://api.apify.com/v2/acts/TASK_NAME_OR_ID/runs?token=YOUR_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Client and Running Weather Scraper\nDESCRIPTION: This snippet initializes the ApifyClient, runs the weather scraper Actor, and checks if it completed successfully before accessing its dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = ApifyClient(os.environ['APIFY_TOKEN'], api_url=os.environ['APIFY_API_BASE_URL'])\n\nprint('Downloading the weather data...')\nscraper_run = client.actor('~bbc-weather-scraper').call()\n\nif scraper_run['status'] != ActorJobStatus.SUCCEEDED:\n    raise RuntimeError('The weather scraper run has failed')\n\ndataset_client = client.dataset(scraper_run['defaultDatasetId'])\n```\n\n----------------------------------------\n\nTITLE: Initializing Results Storage\nDESCRIPTION: Creating an empty list to store the scraped weather data results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# List with scraped results\nweather_data = []\n```\n\n----------------------------------------\n\nTITLE: Running an Apify Actor via cURL with JSON Input\nDESCRIPTION: cURL command to run an Actor synchronously through the Apify API. The command sends a POST request with JSON input and retrieves dataset results in CSV format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_api.md#2025-04-18_snippet_2\n\nLANGUAGE: curl\nCODE:\n```\ncurl -d '{\"num1\":1, \"num2\":8}' -H \"Content-Type: application/json\" -X POST \"https://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync-get-dataset-items?token=YOUR_TOKEN_HERE&format=csv\"\n```\n\n----------------------------------------\n\nTITLE: URL Processing and Screenshot Capture\nDESCRIPTION: Implements the POST endpoint for processing new URLs, capturing screenshots using Puppeteer, and storing them in the key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { launchPuppeteer } from 'crawlee';\n\napp.post('/add-url', async (req, res) => {\n    const { url } = req.body;\n    console.log(`Got new URL: ${url}`);\n\n    // Start chrome browser and open new page ...\n    const browser = await launchPuppeteer();\n    const page = await browser.newPage();\n\n    // ... go to our URL and grab a screenshot ...\n    await page.goto(url);\n    const screenshot = await page.screenshot({ type: 'jpeg' });\n\n    // ... close browser ...\n    await page.close();\n    await browser.close();\n\n    // ... save screenshot to key-value store and add URL to processedUrls.\n    await Actor.setValue(`${processedUrls.length}.jpg`, screenshot, { contentType: 'image/jpeg' });\n    processedUrls.push(url);\n\n    res.redirect('/');\n});\n```\n\n----------------------------------------\n\nTITLE: Response Reading Implementation\nDESCRIPTION: Code demonstrating how to read and parse JSON responses from network requests, including error handling for non-JSON responses.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Notice that the callback function is now async\npage.on('response', async (res) => {\n    if (!res.request().url().includes('followings')) return;\n\n    // Grab the response body in JSON format\n    try {\n        const json = await res.json();\n        console.log(json);\n    } catch (err) {\n        console.error('Response wasn\\'t JSON or failed to parse response.');\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Wait Timeout in JavaScript\nDESCRIPTION: Example of setting a custom timeout for the waitFor() function using options parameter.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nawait waitFor('.bad-class', { timeoutMillis: 5000 });\n```\n\n----------------------------------------\n\nTITLE: Intercepting Network Requests with Puppeteer Scraper in JavaScript\nDESCRIPTION: Demonstrates how to listen to all network requests being dispatched from the browser using Puppeteer Scraper. This example logs the URL of each request.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\ncontext.page.on('request', (req) => console.log(req.url()));\n```\n\n----------------------------------------\n\nTITLE: Initializing Express.js Server with Apify Actor\nDESCRIPTION: Sets up the basic Express.js server configuration and initializes the Apify Actor with necessary middleware for handling form submissions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport express from 'express';\n\nawait Actor.init();\n\nconst app = express();\n\napp.use(express.json());\napp.use(express.urlencoded({ extended: true }));\n```\n\n----------------------------------------\n\nTITLE: Running the Haystack Pipeline to Process Web Data\nDESCRIPTION: Executes the pipeline to crawl the website, process the content, and store it in the document store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipe.run({})\n```\n\n----------------------------------------\n\nTITLE: Using gotScraping with Apify Proxy in JavaScript\nDESCRIPTION: This snippet shows how to use the gotScraping library with Apify Proxy. It creates a proxy configuration, generates a new proxy URL, and makes two requests to demonstrate IP rotation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { gotScraping } from 'got-scraping';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n\nconst url = 'https://api.apify.com/v2/browser-info';\n\nconst response1 = await gotScraping({\n    url,\n    proxyUrl,\n    responseType: 'json',\n});\n\nconst response2 = await gotScraping({\n    url,\n    proxyUrl,\n    responseType: 'json',\n});\n\nconsole.log(response1.body.clientIp);\nconsole.log('Should be different than');\nconsole.log(response2.body.clientIp);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for Apify-Haystack Integration\nDESCRIPTION: Imports all necessary Python packages including Haystack components and the Apify integration module.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\nfrom haystack.components.retrievers import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.utils.auth import Secret\n\nfrom apify_haystack import ApifyDatasetFromActorCall\n```\n\n----------------------------------------\n\nTITLE: Creating New Browser Context in Playwright and Puppeteer\nDESCRIPTION: Shows how to create a new browser context using Playwright's browser.newContext() and Puppeteer's browser.createIncognitoBrowserContext() methods.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst myNewContext = await browser.newContext();\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst myNewContext = await browser.createIncognitoBrowserContext();\n```\n\n----------------------------------------\n\nTITLE: Using PHP cURL with Apify Proxy Authentication\nDESCRIPTION: Shows how to use PHP's cURL extension to make HTTP requests through Apify's proxy. It sets up a curl handler with proxy settings and authentication credentials, then executes the request and outputs the response.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_11\n\nLANGUAGE: php\nCODE:\n```\n<?php\n$curl = curl_init(\"http://proxy.apify.com/?format=json\");\ncurl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\ncurl_setopt($curl, CURLOPT_PROXY, \"http://proxy.apify.com:8000\");\n// Replace <YOUR_PROXY_PASSWORD> below with your password\n// found at https://console.apify.com/proxy\ncurl_setopt($curl, CURLOPT_PROXYUSERPWD, \"auto:<YOUR_PROXY_PASSWORD>\");\n$response = curl_exec($curl);\ncurl_close($curl);\nif ($response) echo $response;\n?>\n```\n\n----------------------------------------\n\nTITLE: Initializing Dataset Client in JavaScript\nDESCRIPTION: Example showing how to initialize and save a reference to a specific dataset using the JavaScript API client. This allows easy access to dataset operations in Node.js applications.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst myDatasetClient = apifyClient.dataset('jane-doe/my-dataset');\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Web Browser Function for OpenAI Assistant\nDESCRIPTION: This function implements the actual call to the RAG Web Browser Actor on Apify, fetching search results based on the given query and maximum number of results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef call_rag_web_browser(query: str, max_results: int) -> list[dict]:\n    \"\"\"\n    Query Google search, scrape the top N pages from the results, and returns their cleaned content as markdown.\n    First start the Actor and wait for it to finish. Then fetch results from the Actor run's default dataset.\n    \"\"\"\n    actor_call = apify_client.actor(\"apify/rag-web-browser\").call(run_input={\"query\": query, \"maxResults\": max_results})\n    return apify_client.dataset(actor_call[\"defaultDatasetId\"]).list_items().items\n```\n\n----------------------------------------\n\nTITLE: Complete Scraping Function with Cheerio\nDESCRIPTION: Final version that extracts title, description, modified date, and run count using complex selectors and data transformations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { $ } = context;\n    // ... rest of your code can come here\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Router Configuration\nDESCRIPTION: Router configuration script that creates a Cheerio router and adds a default request handler.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/initializing_and_setting_up.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// routes.js\nimport { createCheerioRouter } from 'crawlee';\n\nexport const router = createCheerioRouter();\n\nrouter.addDefaultHandler(({ log }) => {\n    log.info('Route reached.');\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and Apify Clients\nDESCRIPTION: This snippet initializes the OpenAI and Apify clients with their respective API keys. Users need to replace placeholder values with their actual API keys.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = OpenAI(api_key=\"YOUR OPENAI API KEY\")\napify_client = ApifyClient(\"YOUR APIFY API TOKEN\")\n```\n\n----------------------------------------\n\nTITLE: Opening a Web Page with Puppeteer in JavaScript\nDESCRIPTION: This snippet shows how to launch a Puppeteer browser instance, create a new page, and navigate to a specific URL. It's the initial setup for interacting with a web form.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://some-site.com/file-upload.php');\n```\n\n----------------------------------------\n\nTITLE: HTML Download Function with BeautifulSoup\nDESCRIPTION: Function that downloads HTML content from a given URL and returns a BeautifulSoup object for parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Ad-hoc Webhook in JavaScript SDK\nDESCRIPTION: Code snippet showing how to dynamically create a webhook from within an Actor's JavaScript code using the Apify SDK. This registers a webhook that triggers when the Actor run fails.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nawait Actor.addWebhook({\n    eventTypes: ['ACTOR.RUN.FAILED'],\n    requestUrl: 'https://example.com/run-failed',\n});\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Retrieving Documents Using BM25 Search\nDESCRIPTION: Demonstrates how to retrieve documents from the document store using BM25 text retrieval, which is based on keyword matching.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Added {document_store.count_documents()} to vector from Website Content Crawler\")\n\nprint(\"Retrieving documents from the document store using BM25\")\nprint(\"query='Haystack'\")\nbm25_retriever = InMemoryBM25Retriever(document_store)\nfor doc in bm25_retriever.run(\"Haystack\", top_k=1)[\"documents\"]:\n  print(doc.content)\n```\n\n----------------------------------------\n\nTITLE: Configuring Actor Output UI in JSON\nDESCRIPTION: This JSON configuration defines the structure and presentation of the Actor's output tab UI. It specifies the dataset schema, including field transformations and display properties for various data types such as images, links, text, boolean, arrays, objects, dates, and numbers.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/index.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"Actor Name\",\n    \"title\": \"Actor Title\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"views\": {\n                \"overview\": {\n                    \"title\": \"Overview\",\n                    \"transformation\": {\n                        \"fields\": [\n                            \"pictureUrl\",\n                            \"linkUrl\",\n                            \"textField\",\n                            \"booleanField\",\n                            \"arrayField\",\n                            \"objectField\",\n                            \"dateField\",\n                            \"numericField\"\n                        ]\n                    },\n                    \"display\": {\n                        \"component\": \"table\",\n                        \"properties\": {\n                            \"pictureUrl\": {\n                                \"label\": \"Image\",\n                                \"format\": \"image\"\n                            },\n                            \"linkUrl\": {\n                                \"label\": \"Link\",\n                                \"format\": \"link\"\n                            },\n                            \"textField\": {\n                                \"label\": \"Text\",\n                                \"format\": \"text\"\n                            },\n                            \"booleanField\": {\n                                \"label\": \"Boolean\",\n                                \"format\": \"boolean\"\n                            },\n                            \"arrayField\": {\n                                \"label\": \"Array\",\n                                \"format\": \"array\"\n                            },\n                            \"objectField\": {\n                                \"label\": \"Object\",\n                                \"format\": \"object\"\n                            },\n                            \"dateField\": {\n                                \"label\": \"Date\",\n                                \"format\": \"date\"\n                            },\n                            \"numericField\": {\n                                \"label\": \"Number\",\n                                \"format\": \"number\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Rotation in BasicCrawler\nDESCRIPTION: Example of setting up proxy rotation for BasicCrawler using request-promise package. Each request uses a fresh proxy from Apify's proxy pool.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/handle_blocked_requests_puppeteer.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst Apify = require('apify');\nconst requestPromise = require('request-promise');\n\nconst PROXY_PASSWORD = process.env.APIFY_PROXY_PASSWORD;\nconst proxyUrl = `http://auto:${PROXY_PASSWORD}@proxy.apify.com`;\n\nconst crawler = new Apify.BasicCrawler({\n    requestList: someInitializedRequestList,\n    handleRequestFunction: async ({ request }) => {\n        const response = await requestPromise({\n            url: request.url,\n            proxy: proxyUrl,\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing and Modifying All Browser Contexts in Puppeteer\nDESCRIPTION: Demonstrates how to loop through all browser contexts in Puppeteer and add an event listener to log when any target URL is changed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\nfor (const context of browser.browserContexts()) {\n    // In Puppeteer, only three events are supported in the \"on\" function\n    // of a BrowserContext instance\n    context.on('targetchanged', () => console.log('Site visited'));\n}\n```\n\n----------------------------------------\n\nTITLE: Launching Persistent Browser Context in Playwright\nDESCRIPTION: Demonstrates how to launch a persistent browser context in Playwright using chromium.launchPersistentContext(). This context stores cache, cookies, and storage on disk.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { chromium } from 'playwright';\n\n// Here, we launch a persistent browser context. The first\n// argument is the location to store the data.\nconst browser = await chromium.launchPersistentContext('./persistent-context', { headless: false });\n\nconst page = await browser.newPage();\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Storing Global Run Statistics in JSON\nDESCRIPTION: This snippet demonstrates the structure for storing global run statistics in the Key-Value store. It includes an errors object with error messages for each request path and the total number of saved items.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/saving_useful_stats.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"errors\": { // all of the errors for every request path\n        \"some-site.com/products/123\": [\n            \"error1\",\n            \"error2\"\n        ]\n    },\n    \"totalSaved\": 43 // total number of saved items throughout the entire run\n}\n```\n\n----------------------------------------\n\nTITLE: Counting Product Items on a Web Page with Python and Beautiful Soup\nDESCRIPTION: This code downloads a product listing page, parses the HTML, and counts the number of product items using a CSS selector.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nproducts = soup.select(\".product-item\")\nprint(len(products))\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Object Types in TypeScript\nDESCRIPTION: Demonstrates how to create a custom object type with string properties for a course object.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types_continued.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst course: {\n    name: string;\n    currentLesson: string;\n} = {\n    name: 'Switching to TypeScript',\n    currentLesson: 'Using types - II',\n};\n```\n\n----------------------------------------\n\nTITLE: TypeScript Type Definitions\nDESCRIPTION: Complete type definitions including Product interface, ResponseData interface, ModifiedProduct type, SortOrder enum, and UserInput interface with generics.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nexport interface Product {\n    id: number;\n    title: string;\n    description: string;\n    price: number;\n    discountPercentage: number;\n    rating: number;\n    stock: number;\n    brand: string;\n    category: string;\n    thumbnail: string;\n    images: string[];\n}\n\nexport interface ResponseData {\n    products: Product[];\n}\n\nexport type ModifiedProduct = Omit<Product, 'images'>;\n\nexport enum SortOrder {\n    ASC = 'ascending',\n    DESC = 'descending',\n}\n\nexport interface UserInput<RemoveImages extends boolean = boolean> {\n    sort: 'ascending' | 'descending';\n    removeImages: RemoveImages;\n}\n```\n\n----------------------------------------\n\nTITLE: Opening New Page with Playwright\nDESCRIPTION: Demonstrates how to launch a Chrome browser instance and create a new page using Playwright. The browser is launched in non-headless mode for visibility.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Compiling TypeScript to JavaScript\nDESCRIPTION: Use the TypeScript compiler (tsc) to compile a .ts file into JavaScript. This command generates a .js file that can be run with Node.js.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ntsc first-lines.ts\n```\n\n----------------------------------------\n\nTITLE: Managing Request Queues in Python SDK\nDESCRIPTION: Demonstrates various operations on request queues using the Apify Python SDK. It includes adding requests, fetching requests, handling specific requests, and dropping a queue.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\nfrom apify.storages import RequestQueue\n\nasync def main():\n    async with Actor:\n        queue: RequestQueue = await Actor.open_request_queue()\n\n        # Enqueue requests\n        await queue.add_request(request={'url': 'http:#example.com/aaa'})\n        await queue.add_request(request={'url': 'http:#example.com/foo'})\n        await queue.add_request(request={'url': 'http:#example.com/bar'}, forefront=True)\n\n        # Get the next requests from queue\n        request1 = await queue.fetch_next_request()\n        request2 = await queue.fetch_next_request()\n\n        # Get a specific request\n        specific_request = await queue.get_request('shi6Nh3bfs3')\n\n        # Reclaim a failed request back to the queue and process it again\n        await queue.reclaim_request(request2)\n\n        # Remove a queue\n        await queue.drop()\n```\n\n----------------------------------------\n\nTITLE: Complete Python Actor with Input/Output Handling\nDESCRIPTION: Full implementation of a Python Actor with custom input and output handling functions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# index.py\nfrom apify_client import ApifyClient\nfrom os import environ\nimport json\n\nclient = ApifyClient(token='YOUR_TOKEN')\n\ndef is_on_apify ():\n    return 'APIFY_IS_AT_HOME' in environ\n\ndef get_input ():\n    if not is_on_apify():\n        with open('./apify_storage/key_value_stores/default/INPUT.json') as actor_input:\n            return json.load(actor_input)\n\n    kv_store = client.key_value_store(environ.get('APIFY_DEFAULT_KEY_VALUE_STORE_ID'))\n    return kv_store.get_record('INPUT')['value']\n\n# Push the solution to the dataset\ndef set_output (data):\n    if not is_on_apify():\n        with open('./apify_storage/datasets/default/solution.json', 'w') as output:\n            return output.write(json.dumps(data, indent=2))\n\n    dataset = client.dataset(environ.get('APIFY_DEFAULT_DATASET_ID'))\n    dataset.push_items('OUTPUT', value=[json.dumps(data, indent=4)])\n\ndef add_all_numbers (nums):\n    total = 0\n\n    for num in nums:\n        total += num\n\n    return total\n\nactor_input = get_input()['numbers']\n\nsolution = add_all_numbers(actor_input)\n\nset_output({ 'solution': solution })\n```\n\n----------------------------------------\n\nTITLE: Blocking Resources with Playwright\nDESCRIPTION: Demonstrates how to use Playwright to launch a browser, create a new page, and block requests for specific file types.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Only listen for requests with one of our blocked extensions\n// Abort all matching requests\npage.route(`**/*{${blockedExtensions.join(',')}}`, async (route) => route.abort());\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Configuring SessionPool with Crawlee\nDESCRIPTION: Sets up a SessionPool in a Crawlee crawler configuration. This demonstrates the basic structure of where session pool configuration options should be placed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    requestList,\n    requestQueue,\n    proxyConfiguration,\n    useSessionPool: true,\n    // This is where our session pool\n    // configuration lives\n    sessionPoolOptions: {\n        // We can add options for each\n        // session created by the session\n        // pool here\n        sessionOptions: {\n\n        },\n    },\n    maxConcurrency: 50,\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and Apify Clients with API Keys\nDESCRIPTION: Sets up the OpenAI and Apify client instances using API keys for authentication. These clients are required for all subsequent API operations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom apify_client import ApifyClient\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR OPENAI API KEY\")\napify_client = ApifyClient(\"YOUR APIFY API TOKEN\")\n```\n\n----------------------------------------\n\nTITLE: Exporting Product Data to CSV in Python\nDESCRIPTION: Uses the csv module from Python's standard library to write the collected product data to a CSV file.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport csv\n\nwith open(\"products.csv\", \"w\") as file:\n    writer = csv.DictWriter(file, fieldnames=[\"title\", \"min_price\", \"price\"])\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n```\n\n----------------------------------------\n\nTITLE: Configuring Actor Input via JSON in Apify API\nDESCRIPTION: This JSON object demonstrates how to configure input parameters for an Actor when running it through the Apify API. It includes settings for maximum requests per crawl, proxy configuration, and the starting URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/input_and_output.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"maxRequestsPerCrawl\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"startUrl\": \"https://apify.com\"\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Fields from Dataset in JavaScript\nDESCRIPTION: Illustrates how to use the 'fields' option in the getData() method to retrieve specific data fields from a dataset in JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\nconst dataset = await Actor.openDataset();\n\n// Only get the 'hotel' and 'cafe' fields\nconst hotelAndCafeData = await dataset.getData({\n    fields: ['hotel', 'cafe'],\n});\n\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Implementing Country Selection Dropdown in Apify Actor Input\nDESCRIPTION: This example shows how to create a select input for country selection in an Apify Actor. It defines enum values and titles for USA, Germany, and France, with a default selection of 'us'.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Country\",\n    \"type\": \"string\",\n    \"description\": \"Select your country\",\n    \"editor\": \"select\",\n    \"default\": \"us\",\n    \"enum\": [\"us\", \"de\", \"fr\"],\n    \"enumTitles\": [\"USA\", \"Germany\", \"France\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Weather Data with Matplotlib\nDESCRIPTION: Creates a plot of the processed weather data using matplotlib, setting appropriate titles, labels, and formatting.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Create a plot of the data\nprint('Plotting the data...')\naxes = mean_daily_temperatures.plot(figsize=(10, 5))\naxes.set_title('Weather prediction for holiday destinations')\naxes.set_xlabel(None)\naxes.yaxis.set_major_formatter(lambda val, _: f'{int(val)} °C')\naxes.grid(which='both', linestyle='dotted')\naxes.legend(loc='best')\naxes.figure.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Exposing Functions with Puppeteer\nDESCRIPTION: Shows how to expose a function to the page context using Puppeteer's page.exposeFunction(). The example demonstrates exposing and executing a simple message function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nconst returnMessage = () => 'Apify academy!';\n\nawait page.exposeFunction(returnMessage.name, returnMessage);\n\nconst msg = await page.evaluate(() => returnMessage());\n\nconsole.log(msg);\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Using PHP Guzzle with Apify Proxy Authentication\nDESCRIPTION: Demonstrates how to configure the Guzzle HTTP client in PHP to use Apify's proxy. It creates a new Guzzle client with a proxy URL that includes authentication credentials, then makes an HTTP request and outputs the response.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_12\n\nLANGUAGE: php\nCODE:\n```\n<?php\nrequire 'vendor/autoload.php';\n\n\n$client = new \\GuzzleHttp\\Client([\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    'proxy' => 'http://auto:<YOUR_PROXY_PASSWORD>@proxy.apify.com:8000'\n]);\n\n$response = $client->get(\"http://proxy.apify.com/?format=json\");\necho $response->getBody();\n```\n\n----------------------------------------\n\nTITLE: Setting up Basic Actor.json Configuration with Dataset Schema\nDESCRIPTION: Template code for creating an actor.json file that defines the dataset schema with example fields. This configuration enables the Overview table interface for displaying Actor results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/output_schema.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"___ENTER_ACTOR_NAME____\",\n    \"title\": \"___ENTER_ACTOR_TITLE____\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"views\": {\n                \"overview\": {\n                    \"title\": \"Overview\",\n                    \"transformation\": {\n                        \"fields\": [\n                            \"___EXAMPLE_NUMERIC_FIELD___\",\n                            \"___EXAMPLE_PICTURE_URL_FIELD___\",\n                            \"___EXAMPLE_LINK_URL_FIELD___\",\n                            \"___EXAMPLE_TEXT_FIELD___\",\n                            \"___EXAMPLE_BOOLEAN_FIELD___\"\n                        ]\n                    },\n                    \"display\": {\n                        \"component\": \"table\",\n                        \"properties\": {\n                            \"___EXAMPLE_NUMERIC_FIELD___\": {\n                                \"label\": \"ID\",\n                                \"format\": \"number\"\n                            },\n                            \"___EXAMPLE_PICTURE_URL_FIELD___\": {\n                                \"format\": \"image\"\n                            },\n                            \"___EXAMPLE_LINK_URL_FIELD___\": {\n                                \"label\": \"Clickable link\",\n                                \"format\": \"link\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing HTML and Extracting Heading with Python and Beautiful Soup\nDESCRIPTION: This code demonstrates how to download HTML, parse it with Beautiful Soup, and extract the main heading (h1 tag) from the page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nprint(soup.select(\"h1\"))\n```\n\n----------------------------------------\n\nTITLE: Common Validation Types Examples\nDESCRIPTION: Collection of JSON Schema examples showing common validation patterns like optional fields, multiple types, nullable fields, and array validation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\"\n        },\n        \"price\": {\n            \"type\": \"number\"\n        }\n    },\n    \"required\": [\"name\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Named Datasets in JavaScript\nDESCRIPTION: Shows how to open and use a named dataset in JavaScript, which can be shared between Actors or Actor runs. It demonstrates opening a dataset and pushing data to it.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\n// Save a named dataset to a variable\nconst dataset = await Actor.openDataset('some-name');\n\n// Add data to the named dataset\nawait dataset.pushData({ foo: 'bar' });\n\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Implementing State Persistence with Actor.on() - Python\nDESCRIPTION: Example showing how to persist state during migration events using the Apify SDK in Python. Uses Actor.on() to listen for migration events and set_value() to save state.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor, Event\n\nasync def actor_migrate(_event_data):\n    await Actor.set_value('my-crawling-state', {'foo': 'bar'})\n\nasync def main():\n    async with Actor:\n        # ...\n        Actor.on(Event.MIGRATING, actor_migrate)\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Implementing GraphQL Query in JavaScript\nDESCRIPTION: Imports the GraphQL query, sets up variables, and makes an API request using got-scraping library in JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gql } from 'graphql-tag';\nimport scrapeAppToken from './scrapeAppToken.mjs';\n\nconst token = await scrapeAppToken();\n\nconst GET_LATEST = gql`\n    query SearchQuery($query: String!, $max_age: Int!) {\n        organization {\n            media(query: $query, max_age: $max_age, first: 1000) {\n                edges {\n                    node {\n                        title\n                        public_at\n                        hero_video {\n                            video_urls {\n                                url\n                            }\n                        }\n                        thumbnail_url\n                    }\n                }\n            }\n        }\n    }\n`;\n\nconst testInput = { hours: 48, query: 'stocks' };\n\nconst variables = { query: testInput.query, max_age: Math.round(testInput.hours) * 60 ** 2 };\n\nconst data = await gotScraping('https://api.cheddar.com/graphql', {\n    responseType: 'json',\n    method: 'POST',\n    headers: { 'X-App-Token': token, 'Content-Type': 'application/json' },\n    body: JSON.stringify({ query: GET_LATEST.loc.source.body, variables }),\n});\n```\n\n----------------------------------------\n\nTITLE: Filtering Products from JSON File in Python\nDESCRIPTION: This Python script reads a JSON file containing product data, filters products with a minimum price greater than $500, and prints the results using the pretty print function. It uses the json module for parsing, decimal for precise comparisons, and pprint for formatted output.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom pprint import pp\nfrom decimal import Decimal\n\nwith open(\"products.json\", \"r\") as file:\n    products = json.load(file)\n\nfor product in products:\n    if Decimal(product[\"min_price\"]) > 500:\n        pp(product)\n```\n\n----------------------------------------\n\nTITLE: Creating an Apify Actor Integration Webhook\nDESCRIPTION: This JSON configuration demonstrates how to create a webhook for Actor integration in Apify. It specifies the requestUrl pointing to the integration Actor, event type to trigger on, condition for filtering events, and payload template for data passing. The isApifyIntegration flag ensures proper display in Apify Console.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integrating_actors_via_api.md#2025-04-18_snippet_0\n\nLANGUAGE: json5\nCODE:\n```\n{\n    \"requestUrl\": \"https://api.apify.com/v2/acts/<integration-actor-id>/runs\",\n    \"eventTypes\": [\"ACTOR.RUN.SUCCEEDED\"],\n    \"condition\": {\n        \"actorId\": \"<actor-id>\",\n    },\n    \"shouldInterpolateStrings\": true,\n    \"isApifyIntegration\": true,\n    \"payloadTemplate\": \"{\\\"field\\\":\\\"value\\\",\\\"payload\\\":{\\\"resource\\\":\\\"{{resource}}\\\"}}\",\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping Title with Cheerio in JavaScript\nDESCRIPTION: Uses Cheerio to extract the title from a webpage by selecting the h1 element within a header tag.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { $ } = context;\n    // ... rest of your code can come here\n    return {\n        title: $('header h1').text(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Finding All Product Elements with JavaScript in DevTools\nDESCRIPTION: This snippet demonstrates how to use querySelectorAll() to find all product elements on a webpage and count them using the length property.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst products = document.querySelectorAll('.product-item');\nproducts.length;\n```\n\n----------------------------------------\n\nTITLE: Implementing Show More Button Pagination in Puppeteer\nDESCRIPTION: Demonstrates how to handle pagination by clicking a Show More button repeatedly until it's no longer available. Includes timeout handling with different initial and subsequent wait times.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    // ...\n    let timeout; // undefined\n    const buttonSelector = 'div.show-more > button';\n    for (;;) {\n        log.info('Waiting for the \"Show more\" button.');\n        try {\n        // Default timeout first time.\n            await page.waitFor(buttonSelector, { timeout });\n            // 2 sec timeout after the first.\n            timeout = 2000;\n        } catch (err) {\n        // Ignore the timeout error.\n            log.info('Could not find the \"Show more button\", '\n            + 'we\\'ve reached the end.');\n            break;\n        }\n        log.info('Clicking the \"Show more\" button.');\n        await page.click(buttonSelector);\n    }\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Proxy URLs in Python SDK\nDESCRIPTION: Utilize the proxy_configuration.new_url(session_id) method to incorporate custom proxy URLs into the proxy configuration when using the Python SDK. This enables the use of personal proxies in Apify projects.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/your_own_proxies.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nproxy_configuration.new_url(session_id)\n```\n\n----------------------------------------\n\nTITLE: Extracting Product URLs from Shopify Sales Page using Cheerio in JavaScript\nDESCRIPTION: This snippet uses Cheerio to parse the HTML and extract product URLs from the sales page. It converts relative URLs to absolute URLs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n```\n\n----------------------------------------\n\nTITLE: Using ChargingManager in Python for Pay-per-event (PPE) Model\nDESCRIPTION: This code demonstrates how to use the ChargingManager from the Apify Python SDK to handle charging for events in the Pay-per-event (PPE) pricing model.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/monetize.mdx#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        charging_manager = Actor.charging_manager()\n\n        charge_result = await charging_manager.charge(\n            'ACTOR_START',\n            amount=1,  # charge $1 for starting the Actor\n            idempotency_key='unique-key-for-this-run'\n        )\n\n        if charge_result.error:\n            print(f'Charging failed: {charge_result.error}')\n            return\n\n        if charge_result.max_reached:\n            print('Maximum cost per run reached. Stopping the Actor.')\n            return\n\n        # Continue with Actor logic\n\nif __name__ == '__main__':\n    Actor.main(main)\n```\n\n----------------------------------------\n\nTITLE: Dataset Operations in Python\nDESCRIPTION: Demonstrates pushing data to an Apify dataset in Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Append result object to the default dataset associated with the run\n        await Actor.push_data({'some_result': 123})\n```\n\n----------------------------------------\n\nTITLE: Exposing Functions with Playwright\nDESCRIPTION: Demonstrates how to expose a function to the page context using Playwright's page.exposeFunction(). The example shows exposing and executing a simple message function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nconst returnMessage = () => 'Apify academy!';\n\nawait page.exposeFunction(returnMessage.name, returnMessage);\n\nconst msg = await page.evaluate(() => returnMessage());\n\nconsole.log(msg);\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Implementing a Type Guard in TypeScript\nDESCRIPTION: This snippet shows how to use a type guard to safely assign an 'unknown' type to a specific type after runtime type checking.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nlet userInput: unknown;\nlet savedInput: string;\n\nuserInput = 5;\n\nif (typeof userInput === 'string') {\n    savedInput = userInput;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Weather Data with Pandas\nDESCRIPTION: This code loads the scraped weather data into a Pandas DataFrame, pivots it, and calculates rolling averages for temperature trends.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint('Parsing weather data...')\ndataset_items_stream = dataset_client.stream_items(item_format='csv')\nweather_data = pandas.read_csv(dataset_items_stream, parse_dates=['datetime'], date_parser=lambda val: pandas.to_datetime(val, utc=True))\n\npivot = weather_data.pivot(index='datetime', columns='location', values='temperature')\nmean_daily_temperatures = pivot.rolling(window='24h', min_periods=24, center=True).mean()\n```\n\n----------------------------------------\n\nTITLE: Handling Optional Dataset ID in JavaScript\nDESCRIPTION: This code snippet demonstrates a flexible approach to handle the dataset ID, allowing it to be provided either explicitly in the input or retrieved from the payload.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst { payload, datasetId } = await Actor.getInput();\nconst datasetIdToProcess = datasetId || payload?.resource?.defaultDatasetId;\n```\n\n----------------------------------------\n\nTITLE: Managing Request Queues in JavaScript SDK\nDESCRIPTION: Illustrates various operations on request queues using the Apify JavaScript SDK. It includes adding requests, fetching requests, handling specific requests, and dropping a queue.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// Import the JavaScript SDK into your project\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\nconst queue = await Actor.openRequestQueue();\n\n// Enqueue requests\nawait queue.addRequests([{ url: 'http://example.com/aaa' }]);\nawait queue.addRequests(['http://example.com/foo', 'http://example.com/bar'], {\n    forefront: true,\n});\n\n// Get the next request from queue\nconst request1 = await queue.fetchNextRequest();\nconst request2 = await queue.fetchNextRequest();\n\n// Get a specific request\nconst specificRequest = await queue.getRequest('shi6Nh3bfs3');\n\n// Reclaim a failed request back to the queue\n// and process it again\nawait queue.reclaimRequest(request2);\n\n// Remove a queue\nawait queue.drop();\n\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Running Website Content Crawler to Extract Data from URLs\nDESCRIPTION: Uses Apify's Website Content Crawler to scrape content from specified URLs with a page limit. The scraped data is stored in an Apify dataset for later use.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrun_input = {\"startUrls\": [{\"url\": \"https://docs.apify.com/platform\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"}\nactor_call_website_crawler = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n\ndataset_id = actor_call_website_crawler[\"defaultDatasetId\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Country-Specific Residential Proxy in Apify SDK (Python)\nDESCRIPTION: Shows how to set up a residential proxy with a specific country (France) in the Apify SDK using Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # ...\n        proxy_configuration = await Actor.create_proxy_configuration(\n            groups=['RESIDENTIAL'],\n            country_code='FR',\n        )\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Final Implementation with File Writing\nDESCRIPTION: Complete implementation including file system operations to save the CSV data to a file using Node.js fs module.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/save_to_csv.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { writeFileSync } from 'fs'; // <---- added a new import\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n\n    const priceElement = $(product).find('span.price');\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n\nconst csv = parse(results);\nwriteFileSync('products.csv', csv); // <---- added writing of CSV to file\n```\n\n----------------------------------------\n\nTITLE: Exporting Product Data to JSON in Python\nDESCRIPTION: Uses the json module from Python's standard library to write the collected product data to a JSON file, handling Decimal serialization.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom decimal import Decimal\n\ndef serialize(obj):\n    if isinstance(obj, Decimal):\n        return str(obj)\n    raise TypeError(\"Object not JSON serializable\")\n\nwith open(\"products.json\", \"w\") as file:\n    json.dump(data, file, default=serialize)\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Apify Dataset into LlamaIndex Documents\nDESCRIPTION: Python code showing how to use the ApifyDataset class to load data from an existing Apify dataset and convert it into LlamaIndex Document objects. The code requires an Apify API token and a dataset ID.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/llama.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.core import Document\nfrom llama_index.readers.apify import ApifyDataset\n\nreader = ApifyDataset(\"<My Apify API token>\")\ndocuments = reader.load_data(\n    dataset_id=\"my_dataset_id\",\n    dataset_mapping_function=lambda item: Document(\n        text=item.get(\"text\"),\n        metadata={\n            \"url\": item.get(\"url\"),\n        },\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Example Zappos Scraper Actor.json Implementation\nDESCRIPTION: A complete example of an actor.json file from the Zappos Scraper that demonstrates how to properly configure fields for display in the Overview table. It includes various field formats like image, text, boolean, and link.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/output_schema.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"zappos-scraper\",\n    \"title\": \"Zappos Scraper\",\n    \"description\": \"\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"title\": \"Zappos.com Dataset\",\n            \"description\": \"\",\n            \"views\": {\n                \"products\": {\n                    \"title\": \"Overview\",\n                    \"description\": \"It can take about one minute until the first results are available.\",\n                    \"transformation\": {\n                        \"fields\": [\n                            \"imgUrl\",\n                            \"brand\",\n                            \"name\",\n                            \"SKU\",\n                            \"inStock\",\n                            \"onSale\",\n                            \"price\",\n                            \"url\"\n                        ]\n                    },\n                    \"display\": {\n                        \"component\": \"table\",\n                        \"properties\": {\n                            \"imgUrl\": {\n                                \"label\": \"Product image\",\n                                \"format\": \"image\"\n                            },\n                            \"url\": {\n                                \"label\": \"Link\",\n                                \"format\": \"link\"\n                            },\n                            \"brand\": {\n                                \"format\": \"text\"\n                            },\n                            \"name\": {\n                                \"format\": \"text\"\n                            },\n                            \"SKU\": {\n                                \"format\": \"text\"\n                            },\n                            \"inStock\": {\n                                \"format\": \"boolean\"\n                            },\n                            \"onSale\": {\n                                \"format\": \"boolean\"\n                            },\n                            \"price\": {\n                                \"format\": \"text\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Residential Proxy Configuration in Apify SDK (JavaScript)\nDESCRIPTION: Demonstrates how to configure the Apify SDK in JavaScript to use residential proxies by setting the 'RESIDENTIAL' group in the proxy configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n});\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Passing Variables to Browser Context in Puppeteer\nDESCRIPTION: This snippet demonstrates how to pass variables from the Node.js context to the browser context in Puppeteer, changing the page title to a random string.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nconst params = { randomString: Math.random().toString(36).slice(2) };\n\nawait page.evaluate(({ randomString }) => {\n    document.querySelector('title').textContent = randomString;\n}, params);\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Queue Locking in Actor 1\nDESCRIPTION: This code demonstrates how to implement request queue locking in Actor 1 using the JavaScript Apify client. It creates a request queue, adds multiple requests, locks the first two requests, checks the lock expiration time, and shows how to prolong or delete a lock.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Actor, ApifyClient } from 'apify';\n\nawait Actor.init();\n\nconst client = new ApifyClient({\n    token: 'MY-APIFY-TOKEN',\n});\n\n// Creates a new request queue.\nconst requestQueue = await client.requestQueues().getOrCreate('example-queue');\n\n// Creates two clients with different keys for the same request queue.\nconst requestQueueClient = client.requestQueue(requestQueue.id, {\n    clientKey: 'requestqueueone',\n});\n\n// Adds multiple requests to the queue.\nawait requestQueueClient.batchAddRequests([\n    {\n        url: 'http://example.com/foo',\n        uniqueKey: 'http://example.com/foo',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/bar',\n        uniqueKey: 'http://example.com/bar',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/baz',\n        uniqueKey: 'http://example.com/baz',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/qux',\n        uniqueKey: 'http://example.com/qux',\n        method: 'GET',\n    },\n]);\n\n// Locks the first two requests at the head of the queue.\nconst processingRequestsClientOne = await requestQueueClient.listAndLockHead(\n    {\n        limit: 2,\n        lockSecs: 120,\n    },\n);\n\n// Checks when the lock will expire. The locked request will have a lockExpiresAt attribute.\nconst lockedRequest = processingRequestsClientOne.items[0];\nconst lockedRequestDetail = await requestQueueClient.getRequest(\n    lockedRequest.id,\n);\nconsole.log(`Request locked until ${lockedRequestDetail?.lockExpiresAt}`);\n\n// Prolongs the lock of the first request or unlocks it.\nawait requestQueueClient.prolongRequestLock(\n    lockedRequest.id,\n    { lockSecs: 120 },\n);\nawait requestQueueClient.deleteRequestLock(\n    lockedRequest.id,\n);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Creating a ReAct Agent with Apify Tools\nDESCRIPTION: Configures a ReAct agent with the LLM and Apify Actors tools, preparing it to process requests that require web browsing and TikTok data extraction.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntools = [browser, tiktok]\nagent_executor = create_react_agent(llm, tools)\n```\n\n----------------------------------------\n\nTITLE: Configuring Country-Specific Residential Proxy in Apify SDK (JavaScript)\nDESCRIPTION: Demonstrates how to set up a residential proxy with a specific country (France) in the Apify SDK using JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'FR',\n});\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Complete Actor Workflow Implementation\nDESCRIPTION: Full implementation showing how to run an actor and process its dataset items using the Apify client.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n// client.js\nimport { ApifyClient } from 'apify-client';\n\nconst client = new ApifyClient({\n    token: 'YOUR_TOKEN',\n});\n\nconst run = await client.actor('YOUR_USERNAME/adding-actor').call({\n    num1: 4,\n    num2: 2,\n});\n\nconst dataset = client.dataset(run.defaultDatasetId);\n\nconst { items } = await dataset.listItems();\n\nconsole.log(items);\n```\n\nLANGUAGE: python\nCODE:\n```\n# client.py\nfrom apify_client import ApifyClient\n\nclient = ApifyClient(token='YOUR_TOKEN')\n\nactor = client.actor('YOUR_USERNAME/adding-actor').call(run_input={\n    'num1': 4,\n    'num2': 2\n})\n\ndataset = client.dataset(run['defaultDatasetId'])\n\nitems = dataset.list_items().items\n\nprint(items)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from iFrame Elements with Puppeteer\nDESCRIPTION: This snippet demonstrates how to extract text content from elements within an identified iFrame. It uses the $$eval method to select multiple elements and map their text content to an array.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/scraping_iframes.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst textFeed = await twitterFrame.$$eval('.timeline-Tweet-text', (pElements) => pElements.map((elem) => elem.textContent));\n\nfor (const text of textFeed) {\n    console.log(text);\n    console.log('**********');\n}\n```\n\n----------------------------------------\n\nTITLE: Triggering File Download by Clicking Button in Puppeteer\nDESCRIPTION: This code snippet shows how to trigger a file download by clicking a button on the page using Puppeteer.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.click('.export-button');\n```\n\n----------------------------------------\n\nTITLE: Blocking Tracking Scripts in Puppeteer\nDESCRIPTION: Implementation example showing how to block specific tracking and analytics scripts. The code filters requests by URL patterns and aborts those containing specific tracking-related keywords.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/block_requests_puppeteer.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.setRequestInterception(true);\npage.on('request', (request) => {\n    const url = request.url();\n    const filters = [\n        'livefyre',\n        'moatad',\n        'analytics',\n        'controltag',\n        'chartbeat',\n    ];\n    const shouldAbort = filters.some((urlPart) => url.includes(urlPart));\n    if (shouldAbort) request.abort();\n    else request.continue();\n});\n```\n\n----------------------------------------\n\nTITLE: Using Python 3 urllib with Apify Proxy Authentication\nDESCRIPTION: Shows how to use Python 3's urllib module with Apify's proxy authentication. It creates a ProxyHandler with the proxy URL containing authentication credentials and disables SSL certificate verification for HTTPS requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.request as request\nimport ssl\n\n# Replace <YOUR_PROXY_PASSWORD> below with your password\n# found at https://console.apify.com/proxy\npassword = \"<YOUR_PROXY_PASSWORD>\"\nproxy_url = f\"http://auto:{password}@proxy.apify.com:8000\"\nproxy_handler = request.ProxyHandler({\n    \"http\": proxy_url,\n    \"https\": proxy_url,\n})\n\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\nhttpHandler = request.HTTPSHandler(context=ctx)\n\nopener = request.build_opener(httpHandler,proxy_handler)\nprint(opener.open(\"http://proxy.apify.com/?format=json\").read())\n```\n\n----------------------------------------\n\nTITLE: Reading Downloaded File from File System in Node.js\nDESCRIPTION: This code shows how to read a downloaded file from the file system using Node.js fs module. It lists files in the download directory and reads the first one.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport fs from 'fs';\n\nconst fileNames = fs.readdirSync('./my-downloads');\n\n// Let's pick the first one\nconst fileData = fs.readFileSync(`./my-downloads/${fileNames[0]}`);\n\n// ...Now we can do whatever we want with the data\n```\n\n----------------------------------------\n\nTITLE: Enabling Strict Type Checking in TypeScript\nDESCRIPTION: This snippet shows how to enable strict type checking in TypeScript by setting the 'strict' option to true in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"lib\": [\"ES2015\", \"ES2016\", \"ES2018\", \"ES2019.Object\", \"ES2018.AsyncIterable\", \"ES2020.String\", \"ES2019.Array\"],\n        \"outDir\": \"dist/\",\n        \"removeComments\": true,\n        \"noEmitOnError\": true,\n        \"strict\": true\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Proxy URLs in JavaScript SDK\nDESCRIPTION: Use the proxyConfiguration.newUrl(sessionId) method to add custom proxy URLs to the proxy configuration in the JavaScript SDK. This allows for the integration of personal proxies into Apify workflows.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/your_own_proxies.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nproxyConfiguration.newUrl(sessionId)\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Function for Running Scraper in TypeScript\nDESCRIPTION: This snippet defines the main function that initializes user input, calls the scrape function, and logs the result.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// index.ts\n\n// ...\nconst main = async () => {\n    const INPUT: UserInput<true> = { sort: 'ascending', removeImages: true };\n\n    const result = await scrape(INPUT);\n\n    console.log(result);\n};\n```\n\n----------------------------------------\n\nTITLE: Adding Properties to Input Schema in JSON\nDESCRIPTION: Extends the basic schema by adding a 'properties' object that defines the input fields the Actor expects. Each property includes a title and description to help users understand what input is required.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/input_schema.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Adding Actor input\",\n    \"description\": \"Add all values in list of numbers with an arbitrary length.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"numbers\": {\n            \"title\": \"Number list\",\n            \"description\": \"The list of numbers to add up.\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Requests for Amazon Product Offers\nDESCRIPTION: This code snippet shows how to create requests for Amazon product offer pages. It extracts the product description and adds a new request to the crawler's queue for offer data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nrouter.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\n    const { data } = request.userData;\n\n    const element = $('div#productDescription');\n\n    // Add to the request queue\n    await crawler.addRequests([{\n        url: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,\n        label: labels.OFFERS,\n        userData: {\n            data: {\n                ...data,\n                description: element.text().trim(),\n            },\n        },\n    }]);\n});\n```\n\n----------------------------------------\n\nTITLE: Product Data Parser Function\nDESCRIPTION: Function that parses product information from a BeautifulSoup element, extracting title and price data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef parse_product(product):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price}\n```\n\n----------------------------------------\n\nTITLE: Implementing ACTOR_MAX_PAID_DATASET_ITEMS Check in JavaScript\nDESCRIPTION: This code snippet demonstrates how to implement a check for the maximum number of paid dataset items in an Actor to prevent excess result generation in Pay-per-result (PPR) pricing model.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/monetize.mdx#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Example code for implementing ACTOR_MAX_PAID_DATASET_ITEMS check\n// This is a simplified version, actual implementation may vary\nconst Apify = require('apify');\n\nApify.main(async () => {\n    const maxItems = process.env.ACTOR_MAX_PAID_DATASET_ITEMS;\n    const dataset = await Apify.openDataset();\n    const itemCount = await dataset.getInfo().then(info => info.itemCount);\n\n    if (maxItems && itemCount >= maxItems) {\n        console.log(`Reached maximum of ${maxItems} items. Stopping the Actor.`);\n        return;\n    }\n\n    // Your Actor logic here\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Client\nDESCRIPTION: Create a new instance of ApifyClient with an API token for authentication.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = new ApifyClient({\n    token: 'YOUR_TOKEN',\n});\n```\n\nLANGUAGE: python\nCODE:\n```\nclient = ApifyClient(token='YOUR_TOKEN')\n```\n\n----------------------------------------\n\nTITLE: jQuery Injection in Pre-goto Function\nDESCRIPTION: Alternative approach to inject jQuery using the pre-goto function hook before page navigation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nasync function preGotoFunction({ page, Apify }) {\n    await Apify.utils.puppeteer.injectJQuery(page);\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Full Actor Configuration in JSON\nDESCRIPTION: A comprehensive example of an `actor.json` file, showcasing all available configuration options for an Apify Actor including name, version, memory requirements, environment variables, and file paths.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/actor_json.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"name-of-my-scraper\",\n    \"version\": \"0.0\",\n    \"buildTag\": \"latest\",\n    \"minMemoryMbytes\": 256,\n    \"maxMemoryMbytes\": 4096,\n    \"environmentVariables\": {\n        \"MYSQL_USER\": \"my_username\",\n        \"MYSQL_PASSWORD\": \"@mySecretPassword\"\n    },\n    \"usesStandbyMode\": false,\n    \"dockerfile\": \"./Dockerfile\",\n    \"readme\": \"./ACTOR.md\",\n    \"input\": \"./input_schema.json\",\n    \"storages\": {\n        \"dataset\": \"./dataset_schema.json\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM and Apify Actors Tools\nDESCRIPTION: Creates instances of the OpenAI chat model and Apify Actors tools for web browsing and TikTok data extraction, which will be used by the ReAct agent.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser = ApifyActorsTool(\"apify/rag-web-browser\")\ntiktok = ApifyActorsTool(\"clockworks/free-tiktok-scraper\")\n```\n\n----------------------------------------\n\nTITLE: Passing Variables to Browser Context in Playwright\nDESCRIPTION: This snippet shows how to pass variables from the Node.js context to the browser context in Playwright, changing the page title to a random string.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nconst params = { randomString: Math.random().toString(36).slice(2) };\n\nawait page.evaluate(({ randomString }) => {\n    document.querySelector('title').textContent = randomString;\n}, params);\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Implementing Request Queue Locking in Actor 2\nDESCRIPTION: This code demonstrates how Actor 2 interacts with the same request queue that Actor 1 is using. It shows how the locking mechanism prevents concurrent processing of the same request by multiple Actor runs, and how trying to modify a lock owned by a different client throws an error.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Actor, ApifyClient } from 'apify';\n\nawait Actor.init();\n\nconst client = new ApifyClient({\n    token: 'MY-APIFY-TOKEN',\n});\n\n// Waits for the first Actor to lock the requests.\nawait new Promise((resolve) => setTimeout(resolve, 5000));\n\n// Get the same request queue in different Actor run and with a different client key.\nconst requestQueue = await client.requestQueues().getOrCreate('example-queue');\n\nconst requestQueueClient = client.requestQueue(requestQueue.id, {\n    clientKey: 'requestqueuetwo',\n});\n\n// Get all requests from the queue and check one locked by the first Actor.\nconst requests = await requestQueueClient.listRequests();\nconst requestsLockedByAnotherRun = requests.items.filter((request) => request.lockByClient === 'requestqueueone');\nconst requestLockedByAnotherRunDetail = await requestQueueClient.getRequest(\n    requestsLockedByAnotherRun[0].id,\n);\n\n// Other clients cannot list and lock these requests; the listAndLockHead call returns other requests from the queue.\nconst processingRequestsClientTwo = await requestQueueClient.listAndLockHead(\n    {\n        limit: 10,\n        lockSecs: 60,\n    },\n);\nconst wasBothRunsLockedSameRequest = !!processingRequestsClientTwo.items.find(\n    (request) => request.id === requestLockedByAnotherRunDetail.id,\n);\n\nconsole.log(`Was the request locked by the first run locked by the second run? ${wasBothRunsLockedSameRequest}`);\nconsole.log(`Request locked until ${requestLockedByAnotherRunDetail?.lockExpiresAt}`);\n\n// Other clients cannot modify the lock; attempting to do so will throw an error.\ntry {\n    await requestQueueClient.prolongRequestLock(\n        requestLockedByAnotherRunDetail.id,\n        { lockSecs: 60 },\n    );\n} catch (err) {\n    // This will throw an error.\n}\n\n// Cleans up the queue.\nawait requestQueueClient.delete();\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Displaying Cookie Information Table in Markdown\nDESCRIPTION: This snippet presents a markdown table listing various cookies used on the Apify docs website. It includes cookie names, descriptions, types (strictly necessary or performance), and expiration periods in days.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/latest/policies/cookie-policy.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Cookie name                              | Cookie description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Type               | Expiration (in days) |\n|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|----------------------|\n| AWSALB                                   | AWS ELB application load balancer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Strictly necessary | 6                    |\n| OptanonConsent                           | This cookie is set by the cookie compliance solution from OneTrust. It stores information about the categories of cookies the site uses and whether visitors have given or withdrawn consent for the use of each category. This enables site owners to prevent cookies in each category from being set in the user's browser, when consent is not given. The cookie has a normal lifespan of one year, so that returning visitors to the site will have their preferences remembered. It contains no information that can identify the site visitor. | Strictly necessary | 364                  |\n| AWSALBCORS                               | This cookie is managed by AWS and is used for load balancing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Strictly necessary | 6                    |\n| ApifyProdUserId                          | This cookie is created by Apify after a user signs into their account and is used across Apify domains to identify if the user is signed in.                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 0                    |\n| ApifyProdUser                            | This cookie is created by Apify after a user signs into their account and is used across Apify domains to identify if the user is signed in.                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 0                    |\n| intercom-id-kod1r788                     | This cookie is used by Intercom service to identify user sessions for customer support chat.                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 270                  |\n| intercom-session-kod1r788                | This cookie is used by Intercom service to identify user sessions for customer support chat.                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 6                    |\n| \\_gaexp\\_rc                              | \\_ga                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Performance        | 0                    |\n| \\_hjTLDTest                              | When the Hotjar script executes we try to determine the most generic cookie path we should use, instead of the page hostname. This is done so that cookies can be shared across subdomains (where applicable). To determine this, we try to store the \\_hjTLDTest cookie for different URL substring alternatives until it fails. After this check, the cookie is removed.                                                                                                                                                                           | Performance        | 0                    |\n| \\_hjSessionUser\\_1441872                 | Hotjar cookie that is set when a user first lands on a page with the Hotjar script. It is used to persist the Hotjar User ID, unique to that site on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID.                                                                                                                                                                                                                                                                           | Performance        | 364                  |\n| \\_hjIncludedInPageviewSample             | This cookie is set to let Hotjar know whether that visitor is included in the data sampling defined by your site's pageview limit.                                                                                                                                                                                                                                                                                                                                                                                                                   | Performance        | 0                    |\n| \\_ga                                     | This cookie name is associated with Google Universal Analytics - which is a significant update to Google's more commonly used analytics service. This cookie is used to distinguish unique users by assigning a randomly generated number as a client identifier. It is included in each page request in a site and used to calculate visitor, session and campaign data for the sites analytics reports. By default it is set to expire after 2 years, although this is customisable by website owners. \\_ga                                        | Performance        | 729                  |\n```\n\n----------------------------------------\n\nTITLE: Core Product Scraping Functions\nDESCRIPTION: Main Python script containing functions for downloading pages, parsing products, and exporting data to CSV and JSON formats. Includes functionality for downloading product listings and extracting basic product information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n\ndef parse_product(product, base_url):\n    title_element = product.select_one(\".product-item__title\")\n    title = title_element.text.strip()\n    url = urljoin(base_url, title_element[\"href\"])\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price, \"url\": url}\n\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)\n```\n\n----------------------------------------\n\nTITLE: Taking Screenshot with Puppeteer/Playwright\nDESCRIPTION: This code snippet shows how to take a screenshot of a web page using the page.screenshot() method in Puppeteer and Playwright. It saves the screenshot as a PNG file in the project folder.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Take the screenshot and write it to the filesystem\nawait page.screenshot({ path: 'screenshot.png' });\n```\n\n----------------------------------------\n\nTITLE: Replacing Shadow DOM Content with HTML\nDESCRIPTION: This snippet iterates through all elements in the main DOM and replaces any shadow DOM content with its HTML. This allows access to the content using standard DOM manipulation methods.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_shadow_doms.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Iterate over all elements in the main DOM.\nfor (const el of document.getElementsByTagName('*')) {\n    // If element contains shadow root then replace its\n    // content with the HTML of shadow DOM.\n    if (el.shadowRoot) el.innerHTML = el.shadowRoot.innerHTML;\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Dataset Record with Hidden Fields\nDESCRIPTION: Illustrates a dataset record with hidden fields (prefixed with '#') including HTTP response and error details. These fields can be excluded when downloading data using query parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"url\": \"https://example.com\",\n    \"title\": \"Example page\",\n    \"data\": {\n        \"foo\": \"bar\"\n    },\n    \"#error\": null,\n    \"#response\": {\n        \"statusCode\": 201\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Specific Product Link Selection\nDESCRIPTION: Selecting product links using specific CSS class selectors to target product titles.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/filtering_links.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('a.product-item__title');\n```\n\nLANGUAGE: javascript\nCODE:\n```\n$('a.product-item__title');\n```\n\n----------------------------------------\n\nTITLE: Setting Status Message in JavaScript with Apify SDK\nDESCRIPTION: Demonstrates how to update an Actor's status message using the JavaScript SDK. The code initializes an Actor, sets a custom status message, and then exits. The SDK optimizes API calls by only invoking them when the status actually changes.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/status_messages.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// ...\nawait Actor.setStatusMessage('Crawled 45 of 100 pages');\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Web Browser Function for OpenAI Assistant\nDESCRIPTION: This snippet defines the function description for the RAG Web Browser, which will be used by the OpenAI Assistant to retrieve search results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrag_web_browser_function = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"call_rag_web_browser\",\n        \"description\": \"Query Google search, scrape the top N pages from the results, and returns their cleaned content as markdown\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": { \"type\": \"string\", \"description\": \"Use regular search words or enter Google Search URLs. \"},\n                \"maxResults\": {\"type\": \"integer\", \"description\": \"The number of top organic search results to return and scrape text from\"}\n            },\n            \"required\": [\"query\"]\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using RAG Web Browser Actor\nDESCRIPTION: Example of using Apify's RAG Web Browser Actor to crawl and scrape content from Google Search results for use with LangChain.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nloader = apify.call_actor(\n    actor_id=\"apify/rag-web-browser\",\n    run_input={\"query\": \"apify langchain web browser\", \"maxResults\": 3},\n    dataset_mapping_function=lambda item: Document(page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"metadata\"][\"url\"]}),\n)\nprint(\"Documents:\", loader.load())\n```\n\n----------------------------------------\n\nTITLE: Initiating Conversation with OpenAI Assistant\nDESCRIPTION: This snippet demonstrates how to start a conversation with the OpenAI Assistant by creating a thread, adding a message, and running the assistant.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"What are the latest LLM news?\"\n)\n\nrun = client.beta.threads.runs.create_and_poll(thread_id=thread.id, assistant_id=my_assistant.id)\n```\n\n----------------------------------------\n\nTITLE: Scraper Actor Input Schema\nDESCRIPTION: JSON schema defining the input structure for the Scraper Actor, which includes the IDs for the shared request queue and dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n// ScraperActorInputSchemaJson reference\n```\n\n----------------------------------------\n\nTITLE: Complete Link Scraping with URL Resolution\nDESCRIPTION: Complete script showing how to scrape product links and properly resolve relative URLs to absolute URLs using Node.js and Cheerio.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/relative_urls.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    console.log(absoluteUrl.href);\n}\n```\n\n----------------------------------------\n\nTITLE: Running an Actor\nDESCRIPTION: Example of running an Apify actor with input parameters using the client.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst run = await client.actor('YOUR_USERNAME/adding-actor').call({\n    num1: 4,\n    num2: 2,\n});\n```\n\nLANGUAGE: python\nCODE:\n```\nrun = client.actor('YOUR_USERNAME/adding-actor').call(run_input={\n    'num1': 4,\n    'num2': 2\n})\n```\n\n----------------------------------------\n\nTITLE: Complete Crawler with URL Queue\nDESCRIPTION: Extended crawler implementation that includes URL queue management and execution\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request }) => {\n        console.log('URL:', request.url);\n        console.log('Title:', $('h1').text().trim());\n    },\n});\n\nawait crawler.addRequests([\n    'https://warehouse-theme-metal.myshopify.com/collections/sales',\n]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Using Configuration Class in JavaScript\nDESCRIPTION: Illustrates how to use the Configuration class in JavaScript for more convenient access to Actor configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// get current token\nconst token = Actor.config.get('token');\n// use different token\nActor.config.set('token', 's0m3n3wt0k3n');\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Scraper Actor Input Handling\nDESCRIPTION: Code for the Scraper Actor to retrieve and use the request queue and dataset IDs provided by the Orchestrator Actor, establishing the connection between the two Actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\ninterface Input {\n    requestQueueId: string;\n    datasetId: string;\n}\n\nconst {\n    requestQueueId,\n    datasetId,\n} = await Actor.getInput<Input>() ?? {} as Input;\n\nconst requestQueue = await Actor.openRequestQueue(requestQueueId);\nconst dataset = await Actor.openDataset(datasetId);\n```\n\n----------------------------------------\n\nTITLE: Adding Request-Specific Statistics to Dataset Items\nDESCRIPTION: Enhances the request handler to include additional statistics in each dataset item, such as the date handled, number of retries, and current pending requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    const { asin } = data;\n\n    for (const offer of $('#aod-offer')) {\n        tracker.incrementASIN(asin);\n        // Add 1 to totalSaved for every offer\n        Stats.success();\n\n        const element = $(offer);\n\n        await dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n            // Store the handledAt date or current date if that is undefined\n            dateHandled: request.handledAt || new Date().toISOString(),\n            // Access the number of retries on the request object\n            numberOfRetries: request.retryCount,\n            // Grab the number of pending requests from the requestQueue\n            currentPendingRequests: (await requestQueue.getInfo()).pendingRequestCount,\n        });\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM and Apify Tools\nDESCRIPTION: Creating instances of the LLM and Apify Actor tools for web browsing and TikTok scraping\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser_tool = ApifyActorsTool(actor_name=\"apify/rag-web-browser\")\ntiktok_tool = ApifyActorsTool(actor_name=\"clockworks/free-tiktok-scraper\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Element Wait in Puppeteer\nDESCRIPTION: Example showing how to wait for elements to be present on a Google search page before interaction using Puppeteer's waitForSelector method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/waiting.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\nconst page = await browser.newPage();\nawait page.goto('https://www.google.com/');\n\nawait page.click('button + button');\n\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Wait for the element to be present on the page prior to clicking it\nawait page.waitForSelector('.g a');\nawait page.click('.g a');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Intercepting File Download Request in Puppeteer\nDESCRIPTION: This code demonstrates how to intercept a file download request in Puppeteer. It waits for a request to be sent and then aborts it, capturing the request details.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst xRequest = await new Promise((resolve) => {\n    page.on('request', (interceptedRequest) => {\n        interceptedRequest.abort(); // stop intercepting requests\n        resolve(interceptedRequest);\n    });\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Base Variables for GitHub Repository Scraping\nDESCRIPTION: Sets up initial variables for storing scraped data and defining base URLs for GitHub repository scraping.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n```\n\n----------------------------------------\n\nTITLE: Setting Up Request Interception in Puppeteer\nDESCRIPTION: Basic code to set up request interception in Puppeteer. This snippet shows how to enable request interception and define conditional logic to either abort or continue requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/block_requests_puppeteer.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.setRequestInterception(true);\npage.on('request', (request) => {\n    if (someCondition) request.abort();\n    else request.continue();\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing E-commerce Product Scraper in Node.js\nDESCRIPTION: A complete Node.js script that scrapes product information from a Shopify store. It uses got-scraping for HTTP requests, Cheerio for HTML parsing, and json2csv for data conversion. The script extracts product titles and prices from the sales collection page and saves them to a CSV file.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/recap_extraction_basics.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// First, we imported all the libraries we needed to\n// download, extract, and convert the data we wanted\nimport { writeFileSync } from 'fs';\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv';\n\n// Here, we fetched the website's HTML and saved it to a new variable.\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// We used Cheerio, a popular library, to parse (process)\n// the downloaded HTML so that we could manipulate it.\nconst $ = cheerio.load(html);\n\n// Using the .product-item CSS selector, we collected all the HTML\n// elements which contained data about individual products.\nconst products = $('.product-item');\n\n// Then, we prepared a new array to store the results.\nconst results = [];\n\n// And looped over all the elements to extract\n// information about the individual products.\nfor (const product of products) {\n    // The product's title was in an <a> element\n    // with the CSS class: product-item__title\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n    // The product's price was in a <span> element\n    // with the CSS class: price\n    const priceElement = $(product).find('span.price');\n    // Because the <span> also included some useless data,\n    // we had to extract the price from a specific HTML node.\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    // We added the data to the results array\n    // in the form of an object with keys and values.\n    results.push({ title, price });\n}\n\n// Finally, we formatted the results\n// as a CSV file instead of a JS object\nconst csv = parse(results);\n\n// Then, we saved the CSV to the disk\nwriteFileSync('products.csv', csv);\n```\n\n----------------------------------------\n\nTITLE: Pre-injecting Scripts with Puppeteer\nDESCRIPTION: Shows how to inject custom code before page load using Puppeteer's page.evaluateOnNewDocument(). The example demonstrates overriding the native addEventListener function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.evaluateOnNewDocument(() => {\n    // Override the prototype\n    Node.prototype.addEventListener = null;\n});\n\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Setting Status Message in Python with Apify SDK\nDESCRIPTION: Shows how to update an Actor's status message using the Python SDK. The code uses an async context manager to handle Actor initialization and sets a custom status message within the Actor's execution context.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/status_messages.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        await Actor.set_status_message('Crawled 45 of 100 pages')\n        # INFO  [Status message]: Crawled 45 of 100 pages\n```\n\n----------------------------------------\n\nTITLE: Using Non-null Assertion in TypeScript\nDESCRIPTION: This snippet shows how to use the non-null assertion operator to remove null and undefined from a value's type, asserting its presence.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nlet job: undefined | string;\n\nconst chars = job!.split('');\n```\n\n----------------------------------------\n\nTITLE: Opening New Page with Puppeteer\nDESCRIPTION: Shows how to launch a browser instance and create a new page using Puppeteer. The browser is launched in non-headless mode for visibility.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Initializing ASINTracker in main.js with Apify SDK in JavaScript\nDESCRIPTION: Updates the main.js file to initialize the ASINTracker at the beginning of the Actor's run. It ensures that the Actor.init() function is executed before the tracker's initialization.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\n// main.js\n\n// ...\nimport tracker from './asinTracker';\n\n// The Actor.init() function should be executed before\n// the tracker's initialization\nawait Actor.init();\n\nawait tracker.initialize();\n// ...\n```\n\n----------------------------------------\n\nTITLE: Querying Single Element with JavaScript\nDESCRIPTION: Demonstrates how to use document.querySelector() to find the first element matching a CSS selector on a webpage.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/using_devtools.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelector('.product-item');\n```\n\n----------------------------------------\n\nTITLE: Running Compiled JavaScript with Node.js\nDESCRIPTION: Execute the compiled JavaScript file using Node.js to see the output of the TypeScript code.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nnode first-lines.js\n```\n\n----------------------------------------\n\nTITLE: Interacting with OpenAI Assistant to Query Stored Data\nDESCRIPTION: Creates a thread, sends a user message, runs the assistant with vector search, and displays the assistant's response. This demonstrates how the assistant uses the vector store data to answer questions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nthread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"How can I scrape a website using Apify?\"\n)\n\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    tool_choice={\"type\": \"file_search\"}\n)\n\nprint(\"Assistant response:\")\nfor m in client.beta.threads.messages.list(thread_id=run.thread_id):\n    print(m.content[0].text.value)\n```\n\n----------------------------------------\n\nTITLE: Actor Exit Operations in JavaScript\nDESCRIPTION: Shows various ways to terminate an Actor in JavaScript, including successful exits, immediate exits, and failure scenarios.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Actor will finish with 'SUCCEEDED' status\nawait Actor.exit('Succeeded, crawled 50 pages');\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Exit right away without calling `exit` handlers at all\nawait Actor.exit('Done right now', { timeoutSecs: 0 });\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Actor will finish with 'FAILED' status\nawait Actor.exit('Could not finish the crawl, try increasing memory', { exitCode: 1 });\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Or nicer way using this syntactic sugar:\nawait Actor.fail('Could not finish the crawl, try increasing memory');\n```\n\n----------------------------------------\n\nTITLE: Defining Prefilled Input for Integration in JSON\nDESCRIPTION: This JSON snippet shows how to define a prefilled input for an Actor when it's being used as an integration, automatically using the default dataset ID of the triggering run.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"datasetId\": \"{{resource.defaultDatasetId}}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Scraped Data into OpenAI Vector Store using Integration Actor\nDESCRIPTION: Processes the scraped website data from the Apify dataset and loads it into the OpenAI Vector Store using the Vector Store Integration actor, making it accessible to the assistant.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrun_input_vs = {\n    \"datasetId\": dataset_id,\n    \"assistantId\": my_assistant.id,\n    \"datasetFields\": [\"text\", \"url\"],\n    \"openaiApiKey\": \"YOUR-OPENAI-API-KEY\",\n    \"vectorStoreId\": vector_store.id,\n}\n\napify_client.actor(\"jiri.spilka/openai-vector-store-integration\").call(run_input=run_input_vs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Error Handling in Cheerio Crawler\nDESCRIPTION: Configures the Cheerio Crawler to use an error handler that tracks errors using the Stats utility class. This handler is called for every failed request.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        persistStateKey: 'AMAZON-SESSIONS',\n        sessionOptions: {\n            maxUsageCount: 5,\n            maxErrorScore: 1,\n        },\n    },\n    maxConcurrency: 50,\n    requestHandler: router,\n    // Handle all failed requests\n    errorHandler: async ({ error, request }) => {\n        // Add an error for this url to our error tracker\n        Stats.addError(request.url, error?.message);\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Inspecting Scraped Results in Browser Console\nDESCRIPTION: Shows how to inspect the collected results in browser console by simply entering the variable name, which displays a formatted representation of the scraped data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nresults;\n// Will log a nicely formatted [{ title: 'my-article-1'}, { title: 'my-article-2'}] etc.\n```\n\n----------------------------------------\n\nTITLE: Applying Type Assertion in TypeScript\nDESCRIPTION: This example demonstrates how to use type assertion to tell TypeScript to treat a value as a specific type, bypassing compiler checks.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nlet userInput: unknown;\nlet savedInput: string;\n\nuserInput = 'hello world!';\n\nsavedInput = userInput as string;\n```\n\n----------------------------------------\n\nTITLE: Logging Extracted Product Data to Console with JavaScript\nDESCRIPTION: This simple command logs the entire results array, containing extracted product data, to the console for review.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Structuring Output JSON for Amazon Product Offers\nDESCRIPTION: Illustrates the expected output format for scraped Amazon product offers, including product details and offer-specific information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/index.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",\n        \"asin\": \"B07P6Y7954\",\n        \"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",\n        \"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",\n        \"keyword\": \"iphone\",\n        \"seller name\": \"Blutek Intl\",\n        \"offer\": \"$162.97\"\n    },\n    {\n        \"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",\n        \"asin\": \"B07P6Y7954\",\n        \"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",\n        \"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",\n        \"keyword\": \"iphone\",\n        \"sellerName\": \"PLATINUM DEALS\",\n        \"offer\": \"$169.98\"\n    },\n    {\n        \"...\": \"...\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Weather Data with Pandas\nDESCRIPTION: Loads the scraped weather data into a pandas DataFrame, transforms it into a pivot table, and calculates rolling averages for temperature trends.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Load the dataset items into a pandas dataframe\nprint('Parsing weather data...')\ndataset_items_stream = dataset_client.stream_items(item_format='csv')\nweather_data = pandas.read_csv(dataset_items_stream, parse_dates=['datetime'], date_parser=lambda val: pandas.to_datetime(val, utc=True))\n\n# Transform data to a pivot table for easier plotting\npivot = weather_data.pivot(index='datetime', columns='location', values='temperature')\nmean_daily_temperatures = pivot.rolling(window='24h', min_periods=24, center=True).mean()\n```\n\n----------------------------------------\n\nTITLE: Sharing Request Queues Between Runs in Python\nDESCRIPTION: This code shows how to access a request queue from another Actor or task run in Python. You can open a request queue using its name or ID and then use it like any other request queue.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        other_queue = await Actor.open_request_queue(name='old-queue')\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Starting a Simple Flask Web Server in an Apify Actor\nDESCRIPTION: This code snippet shows how to create a basic web server using Flask within an Apify Actor. It sets up a single route that responds with 'Hello world' and runs the server on the port specified by the ACTOR_WEB_SERVER_PORT environment variable.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/container_web_server.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# pip install flask\nimport asyncio\nimport os\nfrom apify import Actor\nfrom apify_shared.consts import ActorEnvVars\nfrom flask import Flask\n\nasync def main():\n    async with Actor:\n        # Create a Flask app\n        app = Flask(__name__)\n\n        # Define a route\n        @app.route('/')\n        def hello_world():\n            return 'Hello world from Flask app!'\n\n        # Log the public URL\n        url = os.environ.get(ActorEnvVars.WEB_SERVER_URL)\n        Actor.log.info(f'Web server is listening and can be accessed at {url}')\n\n        # Start the web server\n        port = os.environ.get(ActorEnvVars.WEB_SERVER_PORT)\n        app.run(host='0.0.0.0', port=port)\n```\n\n----------------------------------------\n\nTITLE: GraphQL Query Example\nDESCRIPTION: Example of a GraphQL query structure with variables for fetching repository data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/index.md#2025-04-18_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\nquery($number_of_repos: Int!) {\n  viewer {\n    name\n     repositories(last: $number_of_repos) {\n       nodes {\n         name\n       }\n     }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Output Submission for OpenAI Assistant\nDESCRIPTION: This function handles the submission of tool outputs to the OpenAI Assistant, specifically for the RAG Web Browser function calls.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef submit_tool_outputs(run_):\n    \"\"\" Submit tool outputs to continue the run \"\"\"\n    tool_output = []\n    for tool in run_.required_action.submit_tool_outputs.tool_calls:\n        if tool.function.name == \"call_rag_web_browser\":\n            d = json.loads(tool.function.arguments)\n            output = call_rag_web_browser(query=d[\"query\"], max_results=d[\"maxResults\"])\n            tool_output.append(ToolOutput(tool_call_id=tool.id, output=json.dumps(output)))\n            print(\"RAG Web Browser added as a tool output.\")\n\n    return client.beta.threads.runs.submit_tool_outputs_and_poll(thread_id=run_.thread_id, run_id=run_.id, tool_outputs=tool_output)\n```\n\n----------------------------------------\n\nTITLE: JSON Export Function with Decimal Handling\nDESCRIPTION: Function that exports data to a JSON file with proper handling of Decimal types and formatted indentation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)\n```\n\n----------------------------------------\n\nTITLE: Scraping App Token using Puppeteer in JavaScript\nDESCRIPTION: Function to scrape the 'X-App-Token' from Cheddar's website using Puppeteer for authentication in API requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst scrapeAppToken = async () => {\n    const browser = await puppeteer.launch();\n    const page = await browser.newPage();\n\n    let appToken = null;\n\n    page.on('response', async (res) => {\n        // grab the token from the request headers\n        const token = res.request().headers()?.['x-app-token'];\n\n        // if there is a token, grab it and close the browser\n        if (token) {\n            appToken = token;\n            await browser.close();\n        }\n    });\n\n    await page.goto('https://www.cheddar.com/');\n\n    await page.waitForNetworkIdle();\n\n    // otherwise, close the browser after networkidle\n    // has been fired\n    await browser.close();\n\n    // return the apptoken (or null)\n    return appToken;\n};\n\nexport default scrapeAppToken;\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables in JavaScript\nDESCRIPTION: Demonstrates how to access environment variables in Node.js using the process.env object.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(process.env.APIFY_USER_ID);\n```\n\n----------------------------------------\n\nTITLE: Defining CrewAI Agents\nDESCRIPTION: Creating agent definitions with specific roles, goals, and tools for web search and TikTok analysis\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsearch_agent = Agent(\n    role=\"Web Search Specialist\",\n    goal=\"Find the TikTok profile URL on the web\",\n    backstory=\"Expert in web searching and data retrieval\",\n    tools=[browser_tool],\n    llm=llm,\n    verbose=True\n)\n\nanalysis_agent = Agent(\n    role=\"TikTok Profile Analyst\",\n    goal=\"Extract and analyze data from the TikTok profile\",\n    backstory=\"Skilled in social media data extraction and analysis\",\n    tools=[tiktok_tool],\n    llm=llm,\n    verbose=True\n)\n```\n\n----------------------------------------\n\nTITLE: Running TypeScript Compiler in Watch Mode\nDESCRIPTION: This shell command demonstrates how to run the TypeScript compiler in watch mode, which automatically recompiles files when changes are detected.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ntsc -w\n```\n\n----------------------------------------\n\nTITLE: Basic Puppeteer Setup for Web Scraping\nDESCRIPTION: Initial setup code for launching Puppeteer browser and navigating to target page. Creates a new browser instance and page object, then navigates to a demo webstore.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\n// code will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Operations in Python\nDESCRIPTION: Shows basic key-value store operations in Python including saving and retrieving objects.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Save object to store (stringified to JSON)\n        await Actor.set_value('my_state', {'something': 123})\n\n        # Get a record from the store (automatically parsed from JSON)\n        value = await Actor.get_value('my_state')\n\n        # Log the obtained value\n        Actor.log.info(f'value = {value}')\n        # prints: value = {'something': 123}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for OpenAI Assistant and Apify Integration\nDESCRIPTION: This code imports necessary Python modules for working with OpenAI's API, Apify's client, and handling JSON data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport time\n\nfrom apify_client import ApifyClient\nfrom openai import OpenAI, Stream\nfrom openai.types.beta.threads.run_submit_tool_outputs_params import ToolOutput\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for LangGraph-Apify Integration\nDESCRIPTION: Imports necessary Python packages for creating a LangGraph agent with Apify Actors tools, including modules for API tools, messages, LLM access, and ReAct agent creation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain_apify import ApifyActorsTool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n```\n\n----------------------------------------\n\nTITLE: Paginated Request Processing\nDESCRIPTION: Implementation of promise-based pagination handling for scraping multiple pages of results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);\nconst promises = pageNumbers.map((pageNumber) => (async () => {\n    const paginatedPage = await browser.newPage();\n\n    const url = new URL(REPOSITORIES_URL);\n    url.searchParams.set('page', pageNumber);\n\n    await paginatedPage.goto(url.href);\n    const results = await scrapeRepos(paginatedPage);\n\n    repositories.push(...results);\n\n    await paginatedPage.close();\n})(),\n);\nawait Promise.all(promises);\n\nconsole.log(repositories.length);\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example for Scraped Product Data\nDESCRIPTION: An example of the JSON output structure for the scraped product data, showing how variant information, prices, and other details are formatted.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"variant_name\": \"Red\",\n    \"title\": \"Sony XB-950B1 Extra Bass Wireless Headphones with App Control\",\n    \"min_price\": \"128.00\",\n    \"price\": \"178.00\",\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/sony-xb950-extra-bass-wireless-headphones-with-app-control\",\n    \"vendor\": \"Sony\"\n  },\n  {\n    \"variant_name\": \"Black\",\n    \"title\": \"Sony XB-950B1 Extra Bass Wireless Headphones with App Control\",\n    \"min_price\": \"128.00\",\n    \"price\": \"178.00\",\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/sony-xb950-extra-bass-wireless-headphones-with-app-control\",\n    \"vendor\": \"Sony\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Type Safety Issues in JavaScript\nDESCRIPTION: This example shows common JavaScript type-related problems where undefined values and incorrect type conversions can lead to runtime errors that TypeScript would catch during development.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/index.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst john = {\n    name: 'john',\n    job: 'web developer',\n};\n\nconst bob = {\n    name: 'bob',\n    job: 'data analyst',\n    age: '27',\n};\n\nconst addAges = (num1, num2) => num1 + num2;\n\nconsole.log(addAges(bob.age, john.age));\n```\n\n----------------------------------------\n\nTITLE: Installing Crawlee Dependencies\nDESCRIPTION: Shell commands to initialize a new project and install the necessary Crawlee dependencies.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# this command will initialize your project\n# and install the \"crawlee\" and \"cheerio\" packages\nnpm init -y && npm i crawlee\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Authentication\nDESCRIPTION: Sets environment variables for the OpenAI API key and Apify API token, which are required for authentication with these services.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries for Data Processing\nDESCRIPTION: This code imports the required Python libraries for working with Apify, processing data with Pandas, and handling I/O operations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\nimport os\n\nfrom apify_client import ApifyClient\nfrom apify_client.consts import ActorJobStatus\nimport pandas\n```\n\n----------------------------------------\n\nTITLE: Simple Number Addition in JavaScript\nDESCRIPTION: A JavaScript function that takes multiple numbers as arguments and returns their sum using the reduce method. The function demonstrates basic arithmetic operations that will be converted into an Apify Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/index.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// index.js\nconst addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);\n\nconsole.log(addAllNumbers(1, 2, 3, 4)); // -> 10\n```\n\n----------------------------------------\n\nTITLE: Performing Batch Operations with Request Queues in JavaScript\nDESCRIPTION: This code demonstrates how to perform batch operations on request queues using the JavaScript Apify client. It shows how to add multiple requests to a queue and how to delete multiple requests from a queue in a single API call.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst { ApifyClient } = require('apify-client');\n\nconst client = new ApifyClient({\n    token: 'MY-APIFY-TOKEN',\n});\n\nconst requestQueueClient = client.requestQueue('my-queue-id');\n\n// Add multiple requests to the queue\nawait requestQueueClient.batchAddRequests([\n    {\n        url: 'http://example.com/foo',\n        uniqueKey: 'http://example.com/foo',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/bar',\n        uniqueKey: 'http://example.com/bar',\n        method: 'GET',\n    },\n]);\n\n// Remove multiple requests from the queue\nawait requestQueueClient.batchDeleteRequests([\n    { uniqueKey: 'http://example.com/foo' },\n    { uniqueKey: 'http://example.com/bar' },\n]);\n```\n\n----------------------------------------\n\nTITLE: Importing Apify Client\nDESCRIPTION: Import statements for the Apify client in both Node.js and Python environments.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// client.js\nimport { ApifyClient } from 'apify-client';\n```\n\nLANGUAGE: python\nCODE:\n```\n# client.py\nfrom apify_client import ApifyClient\n```\n\n----------------------------------------\n\nTITLE: Creating an OpenAI Assistant with File Search Tool\nDESCRIPTION: Creates an OpenAI Assistant configured as a customer support agent with the file_search tool enabled. The assistant uses the gpt-4o-mini model.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmy_assistant = client.beta.assistants.create(\n    instructions=\"As a customer support agent at Apify, your role is to assist customers\",\n    name=\"Support assistant\",\n    tools=[{\"type\": \"file_search\"}],\n    model=\"gpt-4o-mini\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Start URLs Array Input for Apify Actor in JSON\nDESCRIPTION: This snippet shows how to configure a start URLs array input for an Apify Actor. It uses the 'requestListSources' editor and includes a prefill URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Start URLs\",\n    \"type\": \"array\",\n    \"description\": \"URLs to start with\",\n    \"prefill\": [{ \"url\": \"https://apify.com\" }],\n    \"editor\": \"requestListSources\"\n}\n```\n\n----------------------------------------\n\nTITLE: Debugging Proxy Usage in Crawlee\nDESCRIPTION: This snippet shows how to access and log proxy information for debugging purposes in a Crawlee scraper.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    // Destructure \"proxyInfo\" from the \"context\" object\n    handlePageFunction: async ({ $, request, proxyInfo }) => {\n        // Log its value\n        console.log(proxyInfo);\n        // ...\n        // ...\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Rust Actor Dockerfile Example\nDESCRIPTION: An example Dockerfile for a Rust Actor. It uses a multi-stage build approach to cache dependencies, copies only what's needed for dependency resolution first, then builds the actual application. The CMD instruction specifies how to run the compiled executable.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_4\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Image with prebuilt Rust. We use the newest 1.* version\n# https://hub.docker.com/_/rust\nFROM rust:1\n\n# We copy only package setup so we cache building all dependencies\nCOPY Cargo* ./\n\n# We need to have dummy main.rs file to be able to build\nRUN mkdir src && echo \"fn main() {}\" > src/main.rs\n\n# Build dependencies only\n# Since we do this before copying  the rest of the files,\n# the dependencies will be cached by Docker, allowing fast\n# build times for new code changes\nRUN cargo build --release\n\n# Delete dummy main.rs\nRUN rm -rf src\n\n# Copy rest of the files\nCOPY . ./\n\n# Build the source files\nRUN cargo build --release\n\nCMD [\"./target/release/actor-example\"]\n```\n\n----------------------------------------\n\nTITLE: Using PuppeteerCrawler with Proxy Configuration and Session Management\nDESCRIPTION: Demonstrates how to use PuppeteerCrawler with proxy configuration and session pooling. The crawler makes requests to proxy.apify.com endpoints while using a single session to maintain the same IP address across requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    sessionPoolOptions: { maxPoolSize: 1 },\n    async requestHandler({ page }) {\n        console.log(await page.content());\n    },\n});\n\nawait crawler.run([\n    'https://proxy.apify.com/?format=json',\n    'https://proxy.apify.com',\n]);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Array Type Definition in TypeScript\nDESCRIPTION: Demonstrates how to define array types in TypeScript objects using the string[] syntax.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types_continued.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst course: {\n    name: string;\n    currentLesson: string;\n    typesLearned: string[];\n    learningBasicTypes?: boolean;\n} = {\n    name: 'Switching to TypeScript',\n    currentLesson: 'Using types - II',\n    typesLearned: ['number', 'boolean', 'string', 'object'],\n};\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from HTML Heading with Python and Beautiful Soup\nDESCRIPTION: This snippet shows how to extract and print only the text content of the first h1 tag found on the page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nheadings = soup.select(\"h1\")\nfirst_heading = headings[0]\nprint(first_heading.text)\n```\n\n----------------------------------------\n\nTITLE: Using Configuration Class in Python\nDESCRIPTION: Demonstrates how to use the Configuration class in Python for more convenient access to Actor configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        old_token = Actor.config.token\n        Actor.log.info(f'old_token = {old_token}')\n\n        # use different token\n        Actor.config.token = 's0m3n3wt0k3n'\n\n        new_token = Actor.config.token\n        Actor.log.info(f'new_token = {new_token}')\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Union Type Alias in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a type alias for a union type and use it in function parameters and variable declarations. It improves code readability by avoiding repetition of complex type annotations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/type_aliases.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ntype MyUnionType = string | number | boolean;\n\nconst returnValueAsString = (value: MyUnionType) => {\n    return `${value}`;\n};\n\nlet myValue: MyUnionType;\n\nmyValue = 55;\n\nconsole.log(returnValueAsString(myValue));\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Default Dataset in Python\nDESCRIPTION: Shows how to use the Apify Python SDK to add single and multiple items to the default dataset in an Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Add one item to the default dataset\n        await Actor.push_data({'foo': 'bar'})\n\n        # Add multiple items to the default dataset\n        await Actor.push_data([{'foo': 'hotel'}, {'foo': 'cafe'}])\n```\n\n----------------------------------------\n\nTITLE: Filling Form Inputs with Puppeteer in JavaScript\nDESCRIPTION: This code demonstrates how to fill in form inputs using Puppeteer's page.type() method. It shows how to enter text into multiple input fields identified by their name attributes.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.type('input[name=firstName]', 'John');\nawait page.type('input[name=surname]', 'Doe');\nawait page.type('input[name=email]', 'john.doe@example.com');\n```\n\n----------------------------------------\n\nTITLE: Extending Interfaces in TypeScript\nDESCRIPTION: This example illustrates how to use the 'extends' keyword to inherit properties from one interface to another. It creates an 'Employee' interface that extends the 'Person' interface, adding an 'occupation' property.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/interfaces.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Person {\n    name: string;\n    age: number;\n    hobbies: string[];\n}\n\ninterface Employee extends Person {\n    occupation: string;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Minimal Actor Configuration in JSON\nDESCRIPTION: A minimal example of an `actor.json` file, showing only the required fields for configuring an Apify Actor: the specification version, name, and version number.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/actor_json.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"name-of-my-scraper\",\n    \"version\": \"0.0\"\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Title with jQuery in Web Scraper\nDESCRIPTION: Uses jQuery to select and extract the title text from a webpage header. This function captures the title from an h1 element inside a header element.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Using jQuery.\nasync function pageFunction(context) {\n    const { jQuery: $ } = context;\n\n    // ... rest of the code\n    return {\n        title: $('header h1').text(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ASINTracker Utility Class in JavaScript\nDESCRIPTION: Defines an ASINTracker class to store, modify, and log tracked ASIN data. It includes methods for incrementing ASIN counts and exports an instance of the class.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// asinTracker.js\nclass ASINTracker {\n    constructor() {\n        this.state = {};\n\n        // Log the state to the console every ten\n        // seconds\n        setInterval(() => console.log(this.state), 10000);\n    }\n\n    // Add an offer to the ASIN's offer count\n    // If ASIN doesn't exist yet, set it to 0\n    incrementASIN(asin) {\n        if (this.state[asin] === undefined) {\n            this.state[asin] = 0;\n            return;\n        }\n\n        this.state[asin] += 1;\n    }\n}\n\n// It is only a utility class, so we will immediately\n// create an instance of it and export that. We only\n// need one instance for our use case.\nmodule.exports = new ASINTracker();\n```\n\n----------------------------------------\n\nTITLE: Injecting Moment.js Library Using Plain JavaScript in Web Scraper\nDESCRIPTION: This code snippet demonstrates how to inject the Moment.js library into a Web Scraper page function using plain JavaScript. It creates a script element, sets its source to the Moment.js CDN, and appends it to the document body. After loading, it uses Moment.js to format the current date and time.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/add_external_libraries_web_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const libraryUrl = 'https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js';\n\n    // Inject Moment.js\\\n    await new Promise((resolve) => {\n        const script = document.createElement('script');\n        script.src = libraryUrl;\n        script.addEventListener('load', resolve);\n        document.body.append(script);\n    });\n\n    // Confirm that it works.\\\n    const now = moment().format('ddd, hA');\n    context.log.info(`NOW: ${now}`);\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Element Content with JavaScript in Chrome DevTools Console\nDESCRIPTION: This snippet demonstrates how to interact with a webpage element using JavaScript in the Chrome DevTools Console. It shows how to access the text content and HTML of an element, as well as how to modify the element's text.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/browser_devtools.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ntemp1.textContent;\n```\n\nLANGUAGE: javascript\nCODE:\n```\ntemp1.outerHTML;\n```\n\nLANGUAGE: javascript\nCODE:\n```\ntemp1.textContent = 'Hello World!';\n```\n\n----------------------------------------\n\nTITLE: Handling Validation Errors in Python\nDESCRIPTION: Shows how to catch and handle dataset validation errors in Python using try-except blocks with the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    await Actor.push_data(items)\nexcept ApifyApiError as error:\n    if \"invalidItems\" in error.data:\n        validation_errors = e.data[\"invalidItems\"]\n```\n\n----------------------------------------\n\nTITLE: Saving Downloaded File to Disk using fs/promises in JavaScript\nDESCRIPTION: This code demonstrates how to save a downloaded file to disk using the fs/promises module in JavaScript. It writes the fileData to a specified file path.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait fs.writeFile('./file.pdf', fileData);\n```\n\n----------------------------------------\n\nTITLE: Opening Request Queue and Dataset in Orchestrator\nDESCRIPTION: Code to open a request queue and dataset in the Orchestrator Actor, which will be shared with the scraper Actor runs for storing results and managing requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst requestQueue = await Actor.openRequestQueue();\nconst dataset = await Actor.openDataset();\n```\n\n----------------------------------------\n\nTITLE: Basic Crawler Implementation\nDESCRIPTION: Implementation of a basic crawler that visits URLs and extracts page titles using CheerioCrawler\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request }) => {\n        console.log('URL:', request.url);\n        console.log('Title:', $('h1').text().trim());\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Modifying Request URL with Puppeteer\nDESCRIPTION: Shows how to intercept and modify a request URL in Puppeteer, changing the destination from one SoundCloud profile to another.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.setRequestInterception(true);\n\n// Listen for all requests\npage.on('request', async (req) => {\n    // If it doesn't match, continue the route normally\n    if (!/soundcloud.com\\/tiesto/.test(req.url())) return req.continue();\n    // Otherwise, continue  the route, but replace \"tiesto\"\n    // in the URL with \"mestomusic\"\n    await req.continue({ url: req.url().replace('tiesto', 'mestomusic') });\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Accessing Request Queues via API Client in JavaScript\nDESCRIPTION: This code shows how to access a request queue using the JavaScript API client. You can access a request queue by its name or ID, including request queues from other users.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_15\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst otherQueueClient = apifyClient.requestQueue('jane-doe/old-queue');\n```\n\n----------------------------------------\n\nTITLE: Initializing Dataset Client in Python\nDESCRIPTION: Example showing how to initialize and save a reference to a specific dataset using the Python API client. This allows easy access to dataset operations in Python applications.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmy_dataset_client = apify_client.dataset('jane-doe/my-dataset')\n```\n\n----------------------------------------\n\nTITLE: Launching Puppeteer with Local MITM Proxy in JavaScript\nDESCRIPTION: This snippet shows how to launch Puppeteer with the configured local proxy. It uses command-line arguments to disable sandbox mode for compatibility with Docker containers and specifies the proxy server address.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/using_proxy_to_intercept_requests_puppeteer.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Launch puppeteer with local proxy\nconst browser = await puppeteer.launch({\n    args: ['--no-sandbox', `--proxy-server=localhost:${proxyPort}`],\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Limits in actor.json\nDESCRIPTION: Reference to setting memory limits in the actor.json configuration file to control platform usage costs for pay-per-result Actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/monetizing_your_actor.md#2025-04-18_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n`actor.json`\n```\n\n----------------------------------------\n\nTITLE: Using Apify Proxy with Session Mode in PHP\nDESCRIPTION: This snippet illustrates how to use Apify's proxy service with session mode in PHP. It configures a Guzzle HTTP client to maintain the same IP address between requests using a session identifier.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_13\n\nLANGUAGE: php\nCODE:\n```\n$client = new \\GuzzleHttp\\Client([\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    'proxy' => 'http://session-my_session:<YOUR_PROXY_PASSWORD>@proxy.apify.com:8000'\n]);\n\n// Both responses should contain the same clientIp\n$response = $client->get(\"https://api.apify.com/v2/browser-info\");\necho $response->getBody();\n\n$response = $client->get(\"https://api.apify.com/v2/browser-info\");\necho $response->getBody();\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables in Python\nDESCRIPTION: Shows how to access environment variables in Python using the os.environ dictionary.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nprint(os.environ['APIFY_USER_ID'])\n```\n\n----------------------------------------\n\nTITLE: Fingerprinting Function Calls in JavaScript\nDESCRIPTION: Example of JavaScript function calls used for browser fingerprinting, including WebGL parameters, codec support checking, and permission queries.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/fingerprinting.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Get the WebGL vendor information\nWebGLRenderingContext.getParameter(37445);\n\n// Get the WebGL renderer information\nWebGLRenderingContext.getParameter(37446);\n\n// Pass any codec into this function (ex. \"audio/aac\"). It will return\n// either \"maybe,\" \"probably,\" or \"\" indicating whether\n// or not the browser can play that codec. An empty\n// string means that  it can't be played.\nHTMLMediaElement.canPlayType('some/codec');\n\n// can ask for a permission if it is not already enabled.\n// allows you to know which permissions the user has\n// enabled, and which are disabled\nnavigator.permissions.query('some_permission');\n```\n\n----------------------------------------\n\nTITLE: Accessing Datasets with Python API Client\nDESCRIPTION: This snippet shows how to access a dataset using the Python API client. It creates a dataset client instance that can be used to interact with a dataset belonging to another user.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nother_dataset_client = apify_client.dataset('jane-doe/old-dataset')\n```\n\n----------------------------------------\n\nTITLE: Injecting Moment.js Library Using jQuery in Web Scraper\nDESCRIPTION: This code snippet shows how to inject the Moment.js library into a Web Scraper page function using jQuery. It uses the $.getScript() method to load the library from the CDN. After loading, it demonstrates the use of Moment.js by formatting the current date and time.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/add_external_libraries_web_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const libraryUrl = 'https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js';\n\n    const $ = context.jQuery;\n\n    // Inject Moment.js\\\n    await $.getScript(libraryUrl);\n\n    // Confirm that it works.\\\n    const now = moment().format('ddd, hA');\n    context.log.info(`NOW: ${now}`);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Stats in Main JavaScript File\nDESCRIPTION: Demonstrates how to import and initialize the Stats utility class in the main JavaScript file of an Apify Actor, alongside other initializations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// ...\nimport Stats from './Stats.js';\n\nawait Actor.init();\nawait asinTracker.initialize();\nawait Stats.initialize();\n// ...\n```\n\n----------------------------------------\n\nTITLE: SoundCloud API Response Example\nDESCRIPTION: Sample JSON response showing next_href pagination implementation in SoundCloud's API\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/handling_pagination.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"next_href\": \"https://api-v2.soundcloud.com/users/141707/tracks?offset=2020-03-13T00%3A00%3A00.000Z%2Ctracks%2C00774168919&limit=20&representation=https%3A%2F%2Fapi-v2.soundcloud.com%2Fusers%2F141707%2Ftracks%3Flimit%3D20\",\n    \"query_urn\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Integer Configuration with Memory Example\nDESCRIPTION: Example configuration for an integer input field representing memory allocation with maximum limit and unit specification.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Memory\",\n    \"type\": \"integer\",\n    \"description\": \"Select memory in megabytes\",\n    \"default\": 64,\n    \"maximum\": 1024,\n    \"unit\": \"MB\"\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Actor with Basic Setup\nDESCRIPTION: Basic Actor setup including imports, initialization, input extraction and task constant definition.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/using_api_and_client.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport axios from 'axios';\n\nawait Actor.init();\n\nconst { useClient, memory, fields, maxItems } = await Actor.getInput();\n\nconst TASK = 'YOUR_USERNAME~demo-actor-task';\n\n// our future code will go here\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Dependencies for Apify and OpenAI Integration\nDESCRIPTION: Installs the necessary Python packages (apify-client and openai) to work with both Apify and OpenAI APIs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install apify-client openai\n```\n\n----------------------------------------\n\nTITLE: Verifying Key-Value Store Content in JavaScript\nDESCRIPTION: This snippet checks the content type of a specific key in the Actor's key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/automated_tests.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait expectAsync(runResult).withKeyValueStore(({ contentType }) => {\n    // Check for the proper content type of the saved key-value item\n    expect(contentType)\n        .withContext(runResult.format('KVS contentType'))\n        .toBe('image/gif');\n},\n\n// This also checks for existence of the key-value key\n{ keyName: 'apify.com-scroll_losless-comp' },\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Input for Apify Actor in JSON\nDESCRIPTION: This snippet shows how to configure a proxy input for an Apify Actor. It uses the 'proxy' editor and includes a prefill option for useApifyProxy.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Proxy configuration\",\n    \"type\": \"object\",\n    \"description\": \"Select proxies to be used by your crawler.\",\n    \"prefill\": { \"useApifyProxy\": true },\n    \"editor\": \"proxy\"\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Web Server Implementation\nDESCRIPTION: The complete implementation of the web server Actor that handles URL submissions and screenshot generation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport express from 'express';\n\nawait Actor.init();\n\nconst app = express();\n\napp.use(express.json());\napp.use(express.urlencoded({ extended: true }));\n\nconst {\n    APIFY_CONTAINER_PORT,\n    APIFY_CONTAINER_URL,\n    APIFY_DEFAULT_KEY_VALUE_STORE_ID,\n} = process.env;\n\nconst processedUrls = [];\n\napp.get('/', (req, res) => {\n    let listItems = '';\n\n    // For each of the processed\n    processedUrls.forEach((url, index) => {\n        const imageUrl = `https://api.apify.com/v2/key-value-stores/${APIFY_DEFAULT_KEY_VALUE_STORE_ID}/records/${index}.jpg`;\n\n        // Display the screenshots below the form\n        listItems += `<li>\n    <a href=\"${imageUrl}\" target=\"_blank\">\n        <img src=\"${imageUrl}\" width=\"300px\" />\n        <br />\n        ${url}\n    </a>\n</li>`;\n    });\n\n    const pageHtml = `<html>\n    <head><title>Example</title></head>\n    <body>\n        <form method=\"POST\" action=\"${APIFY_CONTAINER_URL}/add-url\">\n            URL: <input type=\"text\" name=\"url\" placeholder=\"http://example.com\" />\n            <input type=\"submit\" value=\"Add\" />\n            <hr />\n            <ul>${listItems}</ul>\n        </form>\n    </body>\n</html>`;\n\n    res.send(pageHtml);\n});\n\napp.post('/add-url', async (req, res) => {\n    const { url } = req.body;\n    console.log(`Got new URL: ${url}`);\n\n    // Start chrome browser and open new page ...\n    const browser = await Actor.launchPuppeteer();\n    const page = await browser.newPage();\n\n    // ... go to our URL and grab a screenshot ...\n    await page.goto(url);\n    const screenshot = await page.screenshot({ type: 'jpeg' });\n\n    // ... close browser ...\n    await page.close();\n    await browser.close();\n\n    // ... save screenshot to key-value store and add URL to processedUrls.\n    await Actor.setValue(`${processedUrls.length}.jpg`, screenshot, { contentType: 'image/jpeg' });\n    processedUrls.push(url);\n\n    res.redirect('/');\n});\n\napp.listen(APIFY_CONTAINER_PORT, () => {\n    console.log(`Application is listening at URL ${APIFY_CONTAINER_URL}.`);\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Instructions in AWS Bedrock\nDESCRIPTION: Basic instructions for configuring an AI agent in AWS Bedrock to handle queries and use RAG Web Browser for search results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/aws_bedrock.md#2025-04-18_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nYou are a smart and helpful assistant.\nAnswer question based on the search results.\nUse an expert, friendly, and informative tone\nAlways use RAG Web Browser if you need to retrieve the\nlatest search results and answer questions.\n```\n\n----------------------------------------\n\nTITLE: Basic Link Scraping with Node.js and Cheerio\nDESCRIPTION: Basic script demonstrating how to scrape product links from a webpage using Node.js and Cheerio, showing the issue with relative URLs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/relative_urls.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\nfor (const link of productLinks) {\n    const url = $(link).attr('href');\n    console.log(url);\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript API Client Usage\nDESCRIPTION: Example showing how to initialize and use the JavaScript API client to access request queues.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst myQueueClient = apifyClient.requestQueue('jane-doe/my-request-queue');\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies for Python Data Processing\nDESCRIPTION: This snippet shows the contents of the requirements.txt file, specifying the necessary Python packages (matplotlib and pandas) for data processing and visualization.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmatplotlib\npandas\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiselect Field for Apify Actor in JSON\nDESCRIPTION: This snippet shows how to configure a multiselect field for an Apify Actor. It uses the 'select' editor and defines options with labels in the 'items' property.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Multiselect field\",\n    \"description\": \"My multiselect field\",\n    \"type\": \"array\",\n    \"editor\": \"select\",\n    \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\"value1\", \"value2\", \"value3\"],\n        \"enumTitles\": [\"Label of value1\", \"Label of value2\", \"Label of value3\"]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Data with JavaScript Variables\nDESCRIPTION: Demonstrates how to save queried elements to variables and extract specific product information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/using_devtools.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst products = document.querySelectorAll('.product-item');\nconst subwoofer = products[2];\n```\n\n----------------------------------------\n\nTITLE: Complete Browser Lifecycle Management\nDESCRIPTION: Full example showing how to launch a browser, create a new page, and properly close the browser instance. This demonstrates proper resource cleanup using the browser.close() method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nawait browser.newPage();\n\n// code will be here in the future\n\nawait browser.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nawait browser.newPage();\n\n// code will be here in the future\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Configuring Date Selection Inputs for Apify Actor\nDESCRIPTION: This snippet demonstrates three different date input configurations: absolute date, relative date, and a combination of both. It uses the 'datepicker' editor with various options and validation patterns for each type.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"absoluteDate\": {\n        \"title\": \"Date\",\n        \"type\": \"string\",\n        \"description\": \"Select absolute date in format YYYY-MM-DD\",\n        \"editor\": \"datepicker\",\n        \"pattern\": \"^(\\\\d{4})-(0[1-9]|1[0-2])-(0[1-9]|[12]\\\\d|3[01])$\"\n    },\n    \"relativeDate\": {\n        \"title\": \"Relative date\",\n        \"type\": \"string\",\n        \"description\": \"Select relative date in format: {number} {unit}\",\n        \"editor\": \"datepicker\",\n        \"dateType\": \"relative\",\n        \"pattern\": \"^(\\\\d+)\\\\s*(day|week|month|year)s?$\"\n    },\n    \"anyDate\": {\n        \"title\": \"Any date\",\n        \"type\": \"string\",\n        \"description\": \"Select date in format YYYY-MM-DD or {number} {unit}\",\n        \"editor\": \"datepicker\",\n        \"dateType\": \"absoluteOrRelative\",\n        \"pattern\": \"^(\\\\d{4})-(0[1-9]|1[0-2])-(0[1-9]|[12]\\\\d|3[01])$|^(\\\\d+)\\\\s*(day|week|month|year)s?$\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Multiple Elements with querySelectorAll in JavaScript\nDESCRIPTION: Shows how to select all button elements on a page using document.querySelectorAll(). Returns a NodeList containing all matching elements.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/querying_css_selectors.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst buttons = document.querySelectorAll('button');\n```\n\n----------------------------------------\n\nTITLE: Page Loading Events in Browser\nDESCRIPTION: The three main events that occur during page loading: DOMContentLoaded for initial HTML, load for JavaScript execution, and networkidle for XHR/Fetch requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/dynamic_pages.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n1. DOMContentLoaded - The initial HTML document is loaded\n2. load - The page's JavaScript is executed\n3. networkidle - Network XHR/Fetch requests are sent and loaded\n```\n\n----------------------------------------\n\nTITLE: Analyzing Crash Information from Logs in JavaScript\nDESCRIPTION: This code checks the Actor's log for ReferenceError and TypeError occurrences, which should not appear in production code.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/automated_tests.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait expectAsync(runResult).withLog((log) => {\n    // Neither ReferenceError or TypeErrors should ever occur\n    // in production code – they mean the code is over-optimistic\n    // The errors must be dealt with gracefully and displayed with a helpful message to the user\n    expect(log)\n        .withContext(runResult.format('ReferenceError'))\n        .not.toContain('ReferenceError');\n\n    expect(log)\n        .withContext(runResult.format('TypeError'))\n        .not.toContain('TypeError');\n});\n```\n\n----------------------------------------\n\nTITLE: Example Sitemap XML Structure\nDESCRIPTION: This snippet shows the typical structure of a sitemap.xml file with URL entries containing location, last modification date, and change frequency information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n    <url>\n        <loc>http://www.brewbound.com/advertise</loc>\n        <lastmod>2015-03-19</lastmod>\n        <changefreq>daily</changefreq>\n    </url>\n    <url>\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining Link Selector in CSS for Finding Actor Links\nDESCRIPTION: CSS selector that targets HTML elements containing URLs to Actor detail pages. This selector is used to find all div.item > a elements, which contain links to individual Actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_2\n\nLANGUAGE: css\nCODE:\n```\ndiv.item > a\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Call via Direct API\nDESCRIPTION: Function that uses direct API calls with axios to run a task and retrieve dataset items.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/using_api_and_client.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst withAPI = async () => {\n    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;\n    const url = new URL(uri);\n\n    url.search = new URLSearchParams({\n        memory,\n        format: 'csv',\n        limit: maxItems,\n        fields: fields.join(','),\n        token: process.env.APIFY_TOKEN,\n    });\n\n    const { data } = await axios.post(url.toString());\n\n    return Actor.setValue('OUTPUT', data, { contentType: 'text/csv' });\n};\n```\n\n----------------------------------------\n\nTITLE: Specifying TypeScript Library Files in Configuration\nDESCRIPTION: This configuration snippet demonstrates how to specify which TypeScript library files to include in the project using the 'lib' option in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"lib\": [\"ES2015\", \"ES2016\", \"ES2018\", \"ES2019.Object\", \"ES2018.AsyncIterable\", \"ES2020.String\", \"ES2019.Array\"],\n        \"outDir\": \"dist/\"\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Apify Proxy Configuration in JavaScript\nDESCRIPTION: This code demonstrates how to create a proxy configuration using Apify Proxy, which provides access to pools of residential and datacenter IP addresses.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    countryCode: 'US',\n});\n```\n\n----------------------------------------\n\nTITLE: Stock Units Scraping Solution\nDESCRIPTION: Solution for extracting product stock units, including handling of sold out items and string cleaning.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    units_text = (\n        product\n        .select_one(\".product-item__inventory\")\n        .text\n        .removeprefix(\"In stock,\")\n        .removeprefix(\"Only\")\n        .removesuffix(\" left\")\n        .removesuffix(\"units\")\n        .strip()\n    )\n    if \"Sold out\" in units_text:\n        units = 0\n    else:\n        units = int(units_text)\n\n    print(title, units)\n```\n\n----------------------------------------\n\nTITLE: Actor Lifecycle States Flowchart with Mermaid\nDESCRIPTION: Comprehensive flowchart showing the lifecycle states of Actor builds and runs, including transitional and terminal states.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/index.md#2025-04-18_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph Transitional states\n        RUNNING\n        TIMING-OUT\n        ABORTING\n    end\n\n    subgraph Terminal states\n        SUCCEEDED\n        FAILED\n        TIMED-OUT\n        ABORTED\n    end\n\n    READY --> RUNNING\n              RUNNING --> SUCCEEDED\n              RUNNING --> FAILED\n              RUNNING --> TIMING-OUT --> TIMED-OUT\n              RUNNING --> ABORTING --> ABORTED\n```\n\n----------------------------------------\n\nTITLE: Extracting Title Using Puppeteer in Node.js Context\nDESCRIPTION: Uses Puppeteer's page.$eval method to extract the title of an Actor from its detail page. The function selects the h1 element within a header tag and returns its text content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Using Puppeteer\nasync function pageFunction(context) {\n    const { page } = context;\n    const title = await page.$eval(\n        'header h1',\n        ((el) => el.textContent),\n    );\n\n    return {\n        title,\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Connection String in JavaScript\nDESCRIPTION: Example of creating an Apify Proxy connection string using environment variables in JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/usage.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst { APIFY_PROXY_HOSTNAME, APIFY_PROXY_PORT, APIFY_PROXY_PASSWORD } = process.env;\nconst connectionString = `http://auto:${APIFY_PROXY_PASSWORD}@${APIFY_PROXY_HOSTNAME}:${APIFY_PROXY_PORT}`;\n```\n\n----------------------------------------\n\nTITLE: Actor Execution with Additional Settings\nDESCRIPTION: Example of running an Actor with additional parameters like memory allocation and build version specified in the URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_1\n\nLANGUAGE: cURL\nCODE:\n```\nhttps://api.apify.com/v2/acts/ACTOR_NAME_OR_ID/runs?token=YOUR_TOKEN&memory=8192&build=beta\n```\n\n----------------------------------------\n\nTITLE: Handling Readiness Probe in JavaScript Actor Standby\nDESCRIPTION: This snippet demonstrates how to handle the readiness probe in a JavaScript Actor using Standby mode. It distinguishes between normal requests and readiness probe requests, providing different responses for each.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/actor_standby.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport http from 'http';\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst server = http.createServer((req, res) => {\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    if (req.headers['x-apify-container-server-readiness-probe']) {\n        console.log('Readiness probe');\n        res.end('Hello, readiness probe!\\n');\n    } else {\n        console.log('Normal request');\n        res.end('Hello from Actor Standby!\\n');\n    }\n});\n\nserver.listen(Actor.config.get('standbyPort'));\n```\n\n----------------------------------------\n\nTITLE: Downloading a binary file to memory using request-promise\nDESCRIPTION: Use request-promise with specific options to download a binary file and store it as a Buffer in memory.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_form_with_file_attachment.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst fileData = await request({\n    uri: 'https://example.com/file.pdf',\n    encoding: null,\n});\n```\n\n----------------------------------------\n\nTITLE: Launching Browser with UI (Non-Headless Mode)\nDESCRIPTION: Example showing how to launch a browser with visible UI by setting headless: false in the launch options. Also demonstrates creating a new page in the browser.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nawait browser.newPage();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nawait browser.newPage();\n```\n\n----------------------------------------\n\nTITLE: Converting Type to Interface in TypeScript\nDESCRIPTION: This snippet shows how to convert the 'Person' type into an interface using the 'interface' keyword. It demonstrates the slight syntax difference where interfaces don't require an equals sign.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/interfaces.md#2025-04-18_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Person {\n    name: string;\n    age: number;\n    hobbies: string[];\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Text Content from Multiple Elements in JavaScript\nDESCRIPTION: Example of selecting all button elements and extracting their text content using querySelectorAll() and forEach(). Demonstrates how to iterate over a NodeList of elements.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/querying_css_selectors.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst buttons = document.querySelectorAll('button');\nconst buttonTexts = buttons.forEach((button) => button.textContent);\n```\n\n----------------------------------------\n\nTITLE: Extracting Title, Description, and Modified Date Using Puppeteer\nDESCRIPTION: Completes the data extraction by adding the Actor's last modification date. The code selects a time element, extracts its datetime attribute which contains a timestamp, then converts it to a JavaScript Date object.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nasync function pageFunction(context) {\n    const { page } = context;\n    const title = await page.$eval(\n        'header h1',\n        ((el) => el.textContent),\n    );\n    const description = await page.$eval(\n        'header span.actor-description',\n        ((el) => el.textContent),\n    );\n\n    const modifiedTimestamp = await page.$eval(\n        'ul.ActorHeader-stats time',\n        (el) => el.getAttribute('datetime'),\n    );\n    const modifiedDate = new Date(Number(modifiedTimestamp));\n\n    return {\n        title,\n        description,\n        modifiedDate,\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Regular Function with Type Annotations\nDESCRIPTION: Demonstrates type annotations in a regular function declaration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nfunction totalLengthIsGreaterThan10(string1: string, string2: string): boolean {\n    return (string1 + string2).length > 10;\n}\n```\n\n----------------------------------------\n\nTITLE: Manual Actor Reboot During Migration - Python\nDESCRIPTION: Example showing how to manually reboot an Actor after saving state during migration using Actor.reboot() in Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor, Event\n\nasync def actor_migrate(_event_data):\n    # ...\n    # save state\n    # ...\n    await Actor.reboot()\n\nasync def main():\n    async with Actor:\n        # ...\n        Actor.on(Event.MIGRATING, actor_migrate)\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Using Python 2 urllib with Apify Proxy Authentication\nDESCRIPTION: Demonstrates how to use Python 2's urllib with the six compatibility library to make requests through Apify's proxy. It configures a ProxyHandler with authentication credentials and creates an opener to make HTTP requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport six\nfrom six.moves.urllib import request\n\n# Replace <YOUR_PROXY_PASSWORD> below with your password\n# found at https://console.apify.com/proxy\npassword = \"<YOUR_PROXY_PASSWORD>\"\nproxy_url = (\n    \"http://auto:%s@proxy.apify.com:8000\" %\n    (password)\n)\nproxy_handler = request.ProxyHandler({\n    \"http\": proxy_url,\n    \"https\": proxy_url,\n})\nopener = request.build_opener(proxy_handler)\nprint(opener.open(\"http://proxy.apify.com/?format=json\").read())\n```\n\n----------------------------------------\n\nTITLE: Listing Key-Value Store Keys\nDESCRIPTION: Gets a list of all keys stored in an actor's key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_8\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('actor-runs/<RUN_ID>/key-value-store/keys');\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Configuring Actor Input Parameters\nDESCRIPTION: JSON configuration object that defines the input parameters for an Apify Actor. Includes memory allocation, client usage toggle, field selection, and item limit settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/apify_api_and_client.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"memory\": 4096,\n    \"useClient\": false,\n    \"fields\": [\"title\", \"itemUrl\", \"offer\"],\n    \"maxItems\": 10\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Lists required Python packages for the project including beautifulsoup4 for HTML parsing and requests for making HTTP requests. The file follows standard pip requirements.txt format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/examples/python-data-scraper/requirements.txt#2025-04-18_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbeautifulsoup4\nrequests\n```\n\n----------------------------------------\n\nTITLE: Structuring Input Schema JSON in Apify\nDESCRIPTION: Example of how to structure an input schema JSON object for an Apify Actor. This snippet shows the basic structure with various field types supported by the Apify platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/product_optimization/how_to_create_a_great_input_schema.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"title\": \"Input schema title\",\n  \"description\": \"Description of the input schema\",\n  \"type\": \"object\",\n  \"schemaVersion\": 1,\n  \"properties\": {\n    \"fieldName\": {\n      \"title\": \"Field Title\",\n      \"type\": \"string\",\n      \"description\": \"Tooltip text\",\n      \"prefill\": \"Default value\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Object Type with Type Keyword in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a custom object type using the 'type' keyword in TypeScript. It defines a 'Person' type with name, age, and hobbies properties.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/interfaces.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ntype Person = {\n    name: string;\n    age: number;\n    hobbies: string[];\n};\n```\n\n----------------------------------------\n\nTITLE: Specifying Property Types and Editor Types in JSON\nDESCRIPTION: Enhances the input schema by defining the data type and UI editor component for each property. This example specifies that 'numbers' is an array and should use the JSON editor in the UI.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/input_schema.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Adding Actor input\",\n    \"description\": \"Add all values in list of numbers with an arbitrary length.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"numbers\": {\n            \"title\": \"Number list\",\n            \"description\": \"The list of numbers to add up.\",\n            \"type\": \"array\",\n            \"editor\": \"json\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Charging for Events in Pay-per-event (PPE) Model using JavaScript SDK\nDESCRIPTION: This snippet shows how to use the Apify JavaScript SDK to charge for events in the Pay-per-event (PPE) pricing model. It demonstrates using the Actor.charge() method and handling the ChargeResult.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/monetize.mdx#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst Apify = require('apify');\n\nApify.main(async () => {\n    const chargeResult = await Apify.Actor.charge({\n        eventName: 'ACTOR_START',\n        amount: 1, // charge $1 for starting the Actor\n        idempotencyKey: 'unique-key-for-this-run'\n    });\n\n    if (chargeResult.error) {\n        console.error('Charging failed:', chargeResult.error);\n        return;\n    }\n\n    if (chargeResult.maxReached) {\n        console.log('Maximum cost per run reached. Stopping the Actor.');\n        return;\n    }\n\n    // Continue with Actor logic\n});\n```\n\n----------------------------------------\n\nTITLE: Visualizing Actor Run Lifecycle with Mermaid Diagram\nDESCRIPTION: A flowchart diagram showing the lifecycle states of an Actor run, from the initial READY state through transitional states like RUNNING, TIMING-OUT, and ABORTING to terminal states like SUCCEEDED, FAILED, TIMED-OUT, and ABORTED.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/runs_and_builds.md#2025-04-18_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph Transitional states\n        RUNNING\n        TIMING-OUT\n        ABORTING\n    end\n\n    subgraph Terminal states\n        SUCCEEDED\n        FAILED\n        TIMED-OUT\n        ABORTED\n    end\n\n    READY --> RUNNING\n              RUNNING --> SUCCEEDED\n              RUNNING --> FAILED\n              RUNNING --> TIMING-OUT --> TIMED-OUT\n              RUNNING --> ABORTING --> ABORTED\n```\n\n----------------------------------------\n\nTITLE: Manual Actor Reboot During Migration - JavaScript\nDESCRIPTION: Example showing how to manually reboot an Actor after saving state during migration using Actor.reboot() in JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nActor.on('migrating', async () => {\n    // ...\n    // save state\n    // ...\n    await Actor.reboot();\n});\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Inspecting Dataset Information in JavaScript\nDESCRIPTION: This code examines the Actor's dataset, checking the number of items and validating the structure of individual data entries.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/automated_tests.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait expectAsync(runResult).withDataset(({ dataset, info }) => {\n    // If you're sure, always set this number to be your exact maxItems\n    expect(info.cleanItemCount)\n        .withContext(runResult.format('Dataset cleanItemCount'))\n        .toBe(3); // or toBeGreaterThan(1) or toBeWithinRange(1,3)\n\n    // Make sure the dataset isn't empty\n    expect(dataset.items)\n        .withContext(runResult.format('Dataset items array'))\n        .toBeNonEmptyArray();\n\n    const results = dataset.items;\n\n    // Check dataset items to have the expected data format\n    for (const result of results) {\n        expect(result.directUrl)\n            .withContext(runResult.format('Direct url'))\n            .toStartWith('https://www.yelp.com/biz/');\n\n        expect(result.bizId)\n            .withContext(runResult.format('Biz ID'))\n            .toBeNonEmptyString();\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Constants as an Object in TypeScript\nDESCRIPTION: This snippet shows how to define a constant object with file extensions. It demonstrates the limitation of TypeScript inferring the values as strings and provides two solutions: using a type annotation or the 'as const' assertion.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/enums.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst fileExtensions: {\n    JAVASCRIPT: '.js';\n    TYPESCRIPT: '.ts';\n    RUST: '.rs';\n    PYTHON: '.py';\n} = {\n    JAVASCRIPT: '.js',\n    TYPESCRIPT: '.ts',\n    RUST: '.rs',\n    PYTHON: '.py',\n};\n\n// Or, we can just do this\nconst fileExtensions = {\n    JAVASCRIPT: '.js',\n    TYPESCRIPT: '.ts',\n    RUST: '.rs',\n    PYTHON: '.py',\n} as const;\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration File\nDESCRIPTION: Default TypeScript configuration file (tsconfig.json) with compiler options for module system, target ES version, type checking, and other build settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compilerOptions\": {\n    /* Visit https://aka.ms/tsconfig.json to read more about this file */\n\n    /* Projects */\n    // \"incremental\": true,                              /* Enable incremental compilation */\n    // \"composite\": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */\n    // \"tsBuildInfoFile\": \"./\",                          /* Specify the folder for .tsbuildinfo incremental compilation files. */\n    // \"disableSourceOfProjectReferenceRedirect\": true,  /* Disable preferring source files instead of declaration files when referencing composite projects */\n    // \"disableSolutionSearching\": true,                 /* Opt a project out of multi-project reference checking when editing. */\n    // \"disableReferencedProjectLoad\": true,             /* Reduce the number of projects loaded automatically by TypeScript. */\n\n    /* Language and Environment */\n    \"target\": \"es2016\",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\n    // \"lib\": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */\n    // \"jsx\": \"preserve\",                                /* Specify what JSX code is generated. */\n    // \"experimentalDecorators\": true,                   /* Enable experimental support for TC39 stage 2 draft decorators. */\n    // \"emitDecoratorMetadata\": true,                    /* Emit design-type metadata for decorated declarations in source files. */\n    // \"jsxFactory\": \"\",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h' */\n    // \"jsxFragmentFactory\": \"\",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */\n    // \"jsxImportSource\": \"\",                            /* Specify module specifier used to import the JSX factory functions when using `jsx: react-jsx*`.` */\n    // \"reactNamespace\": \"\",                             /* Specify the object invoked for `createElement`. This only applies when targeting `react` JSX emit. */\n    // \"noLib\": true,                                    /* Disable including any library files, including the default lib.d.ts. */\n    // \"useDefineForClassFields\": true,                  /* Emit ECMAScript-standard-compliant class fields. */\n\n    /* Modules */\n    \"module\": \"commonjs\",                                /* Specify what module code is generated. */\n    // \"rootDir\": \"./\",                                  /* Specify the root folder within your source files. */\n    // \"moduleResolution\": \"node\",                       /* Specify how TypeScript looks up a file from a given module specifier. */\n    // \"baseUrl\": \"./\",                                  /* Specify the base directory to resolve non-relative module names. */\n    // \"paths\": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */\n    // \"rootDirs\": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */\n    // \"typeRoots\": [],                                  /* Specify multiple folders that act like `./node_modules/@types`. */\n    // \"types\": [],                                      /* Specify type package names to be included without being referenced in a source file. */\n    // \"allowUmdGlobalAccess\": true,                     /* Allow accessing UMD globals from modules. */\n    // \"resolveJsonModule\": true,                        /* Enable importing .json files */\n    // \"noResolve\": true,                                /* Disallow `import`s, `require`s or `<reference>`s from expanding the number of files TypeScript should add to a project. */\n\n    /* JavaScript Support */\n    // \"allowJs\": true,                                  /* Allow JavaScript files to be a part of your program. Use the `checkJS` option to get errors from these files. */\n    // \"checkJs\": true,                                  /* Enable error reporting in type-checked JavaScript files. */\n    // \"maxNodeModuleJsDepth\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from `node_modules`. Only applicable with `allowJs`. */\n\n    /* Emit */\n    // \"declaration\": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\n    // \"declarationMap\": true,                           /* Create sourcemaps for d.ts files. */\n    // \"emitDeclarationOnly\": true,                      /* Only output d.ts files and not JavaScript files. */\n    // \"sourceMap\": true,                                /* Create source map files for emitted JavaScript files. */\n    // \"outFile\": \"./\",                                  /* Specify a file that bundles all outputs into one JavaScript file. If `declaration` is true, also designates a file that bundles all .d.ts output. */\n    // \"outDir\": \"./\",                                   /* Specify an output folder for all emitted files. */\n    // \"removeComments\": true,                           /* Disable emitting comments. */\n    // \"noEmit\": true,                                   /* Disable emitting files from a compilation. */\n    // \"importHelpers\": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */\n    // \"importsNotUsedAsValues\": \"remove\",               /* Specify emit/checking behavior for imports that are only used for types */\n    // \"downlevelIteration\": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */\n    // \"sourceRoot\": \"\",                                 /* Specify the root path for debuggers to find the reference source code. */\n    // \"mapRoot\": \"\",                                    /* Specify the location where debugger should locate map files instead of generated locations. */\n    // \"inlineSourceMap\": true,                          /* Include sourcemap files inside the emitted JavaScript. */\n    // \"inlineSources\": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */\n    // \"emitBOM\": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */\n    // \"newLine\": \"crlf\",                                /* Set the newline character for emitting files. */\n    // \"stripInternal\": true,                            /* Disable emitting declarations that have `@internal` in their JSDoc comments. */\n    // \"noEmitHelpers\": true,                            /* Disable generating custom helper functions like `__extends` in compiled output. */\n    // \"noEmitOnError\": true,                            /* Disable emitting files if any type checking errors are reported. */\n    // \"preserveConstEnums\": true,                       /* Disable erasing `const enum` declarations in generated code. */\n    // \"declarationDir\": \"./\",                           /* Specify the output directory for generated declaration files. */\n    // \"preserveValueImports\": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */\n\n    /* Interop Constraints */\n    // \"isolatedModules\": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */\n    // \"allowSyntheticDefaultImports\": true,             /* Allow 'import x from y' when a module doesn't have a default export. */\n    \"esModuleInterop\": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables `allowSyntheticDefaultImports` for type compatibility. */\n    // \"preserveSymlinks\": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */\n    \"forceConsistentCasingInFileNames\": true,            /* Ensure that casing is correct in imports. */\n\n    /* Type Checking */\n    \"strict\": true,                                      /* Enable all strict type-checking options. */\n    // \"noImplicitAny\": true,                            /* Enable error reporting for expressions and declarations with an implied `any` type.. */\n    // \"strictNullChecks\": true,                         /* When type checking, take into account `null` and `undefined`. */\n    // \"strictFunctionTypes\": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */\n    // \"strictBindCallApply\": true,                      /* Check that the arguments for `bind`, `call`, and `apply` methods match the original function. */\n    // \"strictPropertyInitialization\": true,             /* Check for class properties that are declared but not set in the constructor. */\n    // \"noImplicitThis\": true,                           /* Enable error reporting when `this` is given the type `any`. */\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining CSS Selectors using JavaScript\nDESCRIPTION: Demonstrates how to combine multiple selectors to target specific elements more precisely using querySelectorAll()\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/css_selectors.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst highlightedParagraph = document.querySelectorAll('p.highlight');\n```\n\n----------------------------------------\n\nTITLE: Opening Request Queues in Python SDK\nDESCRIPTION: Shows how to open default and named request queues using the Apify Python SDK. It demonstrates the usage of Actor.open_request_queue() method for managing request queues in a Python Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Open the default request queue associated with the Actor run\n        queue = await Actor.open_request_queue()\n\n        # Open the 'my-queue' request queue\n        queue_with_name = await Actor.open_request_queue(name='my-queue')\n\n        # ...\n```\n\n----------------------------------------\n\nTITLE: GraphQL Media Query\nDESCRIPTION: A query to fetch the 1000 most recent articles from the Cheddar API, including title and publication date, using cursor-based relay pagination.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/introspection.md#2025-04-18_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\nquery {\n    organization {\n        media(first: 1000) {\n            edges {\n                node {\n                    title\n                    public_at\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Saving Image File\nDESCRIPTION: Python script for downloading and saving binary image content using HTTPX and pathlib\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nimport httpx\n\nurl = \"https://warehouse-theme-metal.myshopify.com/cdn/shop/products/sonyxbr55front_f72cc8ff-fcd6-4141-b9cc-e1320f867785.jpg\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\nPath(\"tv.jpg\").write_bytes(response.content)\n```\n\n----------------------------------------\n\nTITLE: Error Handling with HTTPX\nDESCRIPTION: Python script demonstrating error handling for HTTP requests using HTTPX's raise_for_status()\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\n\nurl = \"https://warehouse-theme-metal.myshopify.com/does/not/exist\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Updating routes.js to Use ASINTracker in JavaScript\nDESCRIPTION: Updates the routes.js file to utilize the ASINTracker utility class for tracking the number of offers for each product ASIN. It includes handlers for different labels and uses the tracker to increment ASIN counts.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// routes.js\nimport { createCheerioRouter } from '@crawlee/cheerio';\nimport { BASE_URL, OFFERS_URL, labels } from './constants';\nimport tracker from './asinTracker';\nimport { dataset } from './main.js';\n\nexport const router = createCheerioRouter();\n\nrouter.addHandler(labels.START, async ({ $, crawler, request }) => {\n    const { keyword } = request.userData;\n\n    const products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\n    for (const product of products) {\n        const element = $(product);\n        const titleElement = $(element.find('.a-text-normal[href]'));\n\n        const url = `${BASE_URL}${titleElement.attr('href')}`;\n\n        // For each product, add it to the ASIN tracker\n        // and initialize its collected offers count to 0\n        tracker.incrementASIN(element.attr('data-asin'));\n\n        await crawler.addRequest([{\n            url,\n            label: labels.PRODUCT,\n            userData: {\n                data: {\n                    title: titleElement.first().text().trim(),\n                    asin: element.attr('data-asin'),\n                    itemUrl: url,\n                    keyword,\n                },\n            },\n        }]);\n    }\n});\n\nrouter.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\n    const { data } = request.userData;\n\n    const element = $('div#productDescription');\n\n    await crawler.addRequests([{\n        url: OFFERS_URL(data.asin),\n        label: labels.OFFERS,\n        userData: {\n            data: {\n                ...data,\n                description: element.text().trim(),\n            },\n        },\n    }]);\n});\n\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    const { asin } = data;\n\n    for (const offer of $('#aod-offer')) {\n        // For each offer, add 1 to the ASIN's\n        // offer count\n        tracker.incrementASIN(asin);\n\n        const element = $(offer);\n\n        await dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n        });\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Accessing Key-Value Store Information\nDESCRIPTION: Retrieves metadata about an actor run's key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_7\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('actor-runs/<RUN_ID>/key-value-store');\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Installing Puppeteer with npm\nDESCRIPTION: Command to install Puppeteer library using npm package manager.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/index.md#2025-04-18_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install puppeteer\n```\n\n----------------------------------------\n\nTITLE: Configuring Colors Array Input for Apify Actor in JSON\nDESCRIPTION: This snippet demonstrates how to configure a colors array input for an Apify Actor. It uses the 'json' editor and includes prefill values for colors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Colors\",\n    \"type\": \"array\",\n    \"description\": \"Enter colors you know\",\n    \"prefill\": [\"Red\", \"White\"],\n    \"editor\": \"json\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Enum in TypeScript\nDESCRIPTION: This snippet demonstrates how to create an enum in TypeScript using the 'enum' keyword. It defines file extensions as enum values.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/enums.md#2025-04-18_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nenum FileExtension {\n    // Use an \"=\" sign instead of a \":\"\n    JAVASCRIPT = '.js',\n    TYPESCRIPT = '.ts',\n    RUST = '.rs',\n    PYTHON = '.py',\n}\n```\n\n----------------------------------------\n\nTITLE: Using Python SDK with requests Library and Proxy Configuration\nDESCRIPTION: Demonstrates how to use the Apify Python SDK with the requests library to make HTTP requests through a proxy. It creates a proxy configuration with a specific session ID to maintain the same IP address across multiple requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\nimport requests, asyncio\n\nasync def main():\n    async with Actor:\n        proxy_configuration = await Actor.create_proxy_configuration()\n        proxy_url = await proxy_configuration.new_url('my_session')\n        proxies = {\n            'http': proxy_url,\n            'https': proxy_url,\n        }\n\n        # each request uses the same IP address\n        for _ in range(10):\n            response = requests.get('https://api.apify.com/v2/browser-info', proxies=proxies)\n            print(response.text)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Product Detail HTML Structure\nDESCRIPTION: HTML markup showing the structure of a product detail page, highlighting the vendor information section that will be scraped.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"product-meta\">\n  <h1 class=\"product-meta__title heading h1\">\n    Sony XBR-950G BRAVIA 4K HDR Ultra HD TV\n  </h1>\n  <div class=\"product-meta__label-list\">\n    ...\n  </div>\n  <div class=\"product-meta__reference\">\n    <a class=\"product-meta__vendor link link--accented\" href=\"/collections/sony\">\n        Sony\n    </a>\n    <span class=\"product-meta__sku\">\n      SKU:\n      <span class=\"product-meta__sku-number\">SON-985594-XBR-65</span>\n    </span>\n  </div>\n  <a href=\"#product-reviews\" class=\"product-meta__reviews-badge link\" data-offset=\"30\">\n    <div class=\"rating\">\n      <div class=\"rating__stars\" role=\"img\" aria-label=\"4.0 out of 5.0 stars\">\n        ...\n      </div>\n      <span class=\"rating__caption\">3 reviews</span>\n    </div>\n  </a>\n  ...\n</div>\n```\n\n----------------------------------------\n\nTITLE: Link Selection with Attribute Filter\nDESCRIPTION: Selecting only anchor tags that have an href attribute using CSS attribute selectors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/filtering_links.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('a[href]');\n```\n\nLANGUAGE: javascript\nCODE:\n```\n$('a[href]');\n```\n\n----------------------------------------\n\nTITLE: Union Types in TypeScript\nDESCRIPTION: Shows how to use union types to allow a variable to hold multiple different types of values.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nlet value: number | string;\n\n// Totally ok\nvalue = 10;\n\n// Totally ok\nvalue = 'hello academy!';\n\n// This will throw a compiler error, because we didn't include\n// number arrays in our union type.\nvalue = [1, 2, 3];\n```\n\n----------------------------------------\n\nTITLE: Updating Actor Configuration\nDESCRIPTION: Example of updating an actor's default run options using the client.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait actor.update({\n    defaultRunOptions: {\n        build: 'latest',\n        memoryMbytes: 256,\n        timeoutSecs: 20,\n    },\n});\n```\n\nLANGUAGE: python\nCODE:\n```\nactor.update(default_run_build='latest', default_run_memory_mbytes=256, default_run_timeout_secs=20)\n```\n\n----------------------------------------\n\nTITLE: Using Idempotency Keys with Webhooks in JavaScript\nDESCRIPTION: Example showing how to prevent duplicate webhooks by using an idempotency key parameter in JavaScript. It uses the Actor run ID as the unique key to ensure only one webhook is created.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nawait Actor.addWebhook({\n    eventTypes: ['ACTOR.RUN.FAILED'],\n    requestUrl: 'https://example.com/run-failed',\n    idempotencyKey: process.env.APIFY_ACTOR_RUN_ID,\n});\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Client ID Scraping Implementation\nDESCRIPTION: JavaScript module to scrape SoundCloud client ID using Puppeteer\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/handling_pagination.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nexport const scrapeClientId = async () => {\n    const browser = await puppeteer.launch({ headless: true });\n    const page = await browser.newPage();\n\n    let clientId = null;\n\n    page.on('response', async (res) => {\n        const id = new URL(res.url()).searchParams.get('client_id') ?? null;\n        if (id) clientId = id;\n    });\n\n    await page.goto('https://soundcloud.com/tiesto/tracks');\n    await page.waitForSelector('.profileHeader__link');\n    await browser.close();\n\n    return clientId;\n};\n```\n\n----------------------------------------\n\nTITLE: Constructing Amazon Search URL in JavaScript\nDESCRIPTION: Demonstrates how to construct the URL for Amazon product search results using a keyword.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/index.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=KEYWORD\n```\n\n----------------------------------------\n\nTITLE: Sample Actor Input JSON\nDESCRIPTION: Example JSON input file structure for the Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"numbers\": [5, 5, 5, 5]\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Assistant Response to Web Scraping Question\nDESCRIPTION: An example of how the assistant might respond to a question about web scraping with Apify, using knowledge from the vector store data that was previously loaded.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_17\n\nLANGUAGE: plaintext\nCODE:\n```\nAssistant response:\nYou can scrape a website using Apify by following these steps:\n1. Visit the [Apify website](https://apify.com) and create an account.\n2. Go to the [Apify Store](https://apify.com/store) and choose a web scraper.\n3. Configure the web scraper with the URL of the website you want to scrape.\n4. Run the web scraper and download the data.\n```\n\n----------------------------------------\n\nTITLE: Formatting HTML in Input Schema Descriptions\nDESCRIPTION: Example of using basic HTML formatting in the description field of an input schema. This can be used to add links, paragraphs, and other formatting to improve readability.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/product_optimization/how_to_create_a_great_input_schema.md#2025-04-18_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<p>This is a description with <strong>bold text</strong> and a <a href=\\\"https://example.com\\\" target=\\\"_blank\\\">link</a>.</p>\n<p>You can add multiple paragraphs.</p>\n```\n\n----------------------------------------\n\nTITLE: Defining Input JSON for Amazon Scraper\nDESCRIPTION: Shows the expected input format for the Amazon scraper, specifying the search keyword.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/index.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"keyword\": \"iphone\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Input Schema for Integration-Ready Actor in JSON\nDESCRIPTION: This snippet shows an example of an input schema for an Actor that uploads a dataset to a database table. It includes fields for dataset ID, connection string, and table name.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"datasetId\": \"{{resource.defaultDatasetId}}\",\n    \"connectionString\": \"****\",\n    \"tableName\": \"results\"\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Dataset Fields in API Request\nDESCRIPTION: Example of how to use the fields parameter to retrieve only specific fields from dataset items. This URL demonstrates requesting only the 'hotel' and 'cafe' fields with proper URL encoding.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/datasets/{DATASET_ID}/items?format=json&fields=hotel%2Ccafe\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Clean Code in JavaScript\nDESCRIPTION: This snippet illustrates the difference between poorly written and well-written code for a function that converts Fahrenheit to Celsius. It emphasizes descriptive naming and comments.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/best_practices.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst x = (y) => (y - 32) * (5 / 9);\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Converts a fahrenheit value to celsius\nconst fahrenheitToCelsius = (celsius) => (celsius - 32) * (5 / 9);\n```\n\n----------------------------------------\n\nTITLE: Running Actor with Completion Wait\nDESCRIPTION: Runs an actor with waitForFinish parameter to wait for completion before proceeding.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_10\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->post('acts/mhamas~html-string-to-pdf/runs', [\n    'json' => [\n        'htmlString' => '<html><body><h1>Hi World</h1></body></html>'\n    ],\n    'query' => [ 'waitForFinish' => 60 ]\n]);\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n\n$runId = $data['id'];\n$response = $client->get(sprintf('actor-runs/%s/key-value-store/records/OUTPUT', $runId));\nfile_put_contents(__DIR__ . '/hi-world.pdf', $response->getBody());\n```\n\n----------------------------------------\n\nTITLE: Paginating Dataset Results\nDESCRIPTION: Shows how to paginate through dataset items using offset and limit parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_5\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('datasets/<DATASET_ID>/items', [\n    'query' => [\n        'offset' => 20,\n        'limit' => 10,\n    ]\n]);\n$parsedResponse = \\json_decode($response->getBody(), true);\necho \\json_encode($parsedResponse, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Making Fields Required in Input Schema JSON\nDESCRIPTION: Completes the input schema by adding a 'required' array that lists which properties must be provided. This ensures that the 'numbers' field will be validated as mandatory for the Actor to run.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/input_schema.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Adding Actor input\",\n    \"description\": \"Add all values in list of numbers with an arbitrary length.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"numbers\": {\n            \"title\": \"Number list\",\n            \"description\": \"The list of numbers to add up.\",\n            \"type\": \"array\",\n            \"editor\": \"json\"\n        }\n    },\n    \"required\": [\"numbers\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Resurrections in ASINTracker with Apify SDK in JavaScript\nDESCRIPTION: Adds an initialize method to the ASINTracker class to handle resurrections. It checks the key-value store for a previous state and updates the class state if found.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\n// asinTracker.js\nimport { Actor } from 'apify';\nimport { ASIN_TRACKER } from './constants';\n\nclass ASINTracker {\n    constructor() {\n        this.state = {};\n\n        Actor.on('persistState', async () => {\n            await Actor.setValue(ASIN_TRACKER, this.state);\n        });\n\n        setInterval(() => console.log(this.state), 10000);\n    }\n\n    async initialize() {\n        // Read the data from the key-value store. If it\n        // doesn't exist, it will be undefined\n        const data = await Actor.getValue(ASIN_TRACKER);\n\n        // If the data does exist, replace the current state\n        // (initialized as an empty object) with the data\n        if (data) this.state = data;\n    }\n\n    incrementASIN(asin) {\n        if (this.state[asin] === undefined) {\n            this.state[asin] = 0;\n            return;\n        }\n\n        this.state[asin] += 1;\n    }\n}\n\nmodule.exports = new ASINTracker();\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Assistant Instructions\nDESCRIPTION: This code defines the instructions for the OpenAI Assistant, specifying its behavior and responsibilities in providing web-based information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nINSTRUCTIONS = \"\"\" You are a smart and helpful assistant. Maintain an expert, friendly, and informative tone in your responses.\n   Your task is to answer questions based on information from the internet.\n   Always call call_rag_web_browser function to retrieve the latest and most relevant online results.\n   Never provide answers based solely on your own knowledge.\n   For each answer, always include relevant sources whenever possible.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Agent Output for TikTok Profile Analysis\nDESCRIPTION: Shows the expected output format from the LangGraph agent when analyzing a TikTok profile. Includes the agent's tool calls and the formatted analysis results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n================================ Human Message =================================\n\nSearch the web for OpenAI TikTok profile and analyze their profile.\n================================== AI Message ==================================\nTool Calls:\n  apify_actor_apify_rag-web-browser (call_y2rbmQ6gYJYC2lHzWJAoKDaq)\n Call ID: call_y2rbmQ6gYJYC2lHzWJAoKDaq\n  Args:\n    run_input: {\"query\":\"OpenAI TikTok profile\",\"maxResults\":1}\n\n...\n\n================================== AI Message ==================================\n\nThe OpenAI TikTok profile is titled \"OpenAI (@openai) Official.\" Here are some key details about the profile:\n\n- **Followers**: 592.3K\n- **Likes**: 3.3M\n- **Description**: The profile features \"low key research previews\" and includes videos that showcase their various projects and research developments.\n\n### Profile Overview:\n- **Profile URL**: [OpenAI TikTok Profile](https://www.tiktok.com/@openai?lang=en)\n- **Content Focus**: The posts primarily involve previews of OpenAI's research and various AI-related innovations.\n\n...\n```\n\n----------------------------------------\n\nTITLE: Setting API Tokens for Apify and OpenAI\nDESCRIPTION: Sets environment variables for the Apify API token and OpenAI API key, which are required for the integration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"APIFY_API_TOKEN\"] = \"YOUR-APIFY-API-TOKEN\"\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR-OPENAI-API-KEY\"\n```\n\n----------------------------------------\n\nTITLE: Calling Milvus Integration Actor in Python\nDESCRIPTION: Python code to call Apify's Milvus integration Actor and store scraped data in the Milvus Vector Database.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/milvus.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmilvus_integration_inputs = {\n    \"milvusUri\": MILVUS_URI,\n    \"milvusToken\": MILVUS_TOKEN,\n    \"milvusCollectionName\": MILVUS_COLLECTION_NAME,\n    \"datasetFields\": [\"text\"],\n    \"datasetId\": actor_call[\"defaultDatasetId\"],\n    \"deltaUpdatesPrimaryDatasetFields\": [\"url\"],\n    \"expiredObjectDeletionPeriodDays\": 30,\n    \"embeddingsApiKey\": OPENAI_API_KEY,\n    \"embeddingsProvider\": \"OpenAI\",\n}\nactor_call = client.actor(\"apify/milvus-integration\").call(run_input=milvus_integration_inputs)\n```\n\n----------------------------------------\n\nTITLE: System Event JSON Structure in Apify\nDESCRIPTION: This JSON structure represents the format of system events sent to Apify Actors through a WebSocket connection. It includes the event name, creation time, and optional payload data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/system_events.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": String,\n    \"createdAt\": String,\n    \"data\": Object\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a RequestList from Sitemap URLs with Crawlee\nDESCRIPTION: This code creates a RequestList that filters URLs from a sitemap using a regular expression to match only beer product URLs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst requestList = await RequestList.open(null, [{\n    requestsFromUrl: 'https://www.brewbound.com/sitemap.xml',\n    regex: /http(s)?:\\/\\/www\\.brewbound\\.com\\/breweries\\/[^/<]+\\/[^/<]+/gm,\n}]);\n```\n\n----------------------------------------\n\nTITLE: Querying Multiple Elements with JavaScript\nDESCRIPTION: Shows how to use document.querySelectorAll() to find all elements matching a CSS selector on a webpage.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/using_devtools.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.product-item');\n```\n\n----------------------------------------\n\nTITLE: Explicit Type Annotation for Variables\nDESCRIPTION: Demonstrates how to explicitly declare a variable's type using type annotation syntax.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nlet value: number;\n\n// Totally ok\nvalue = 10;\n\n// This will throw a compiler error\nvalue = 'hello academy!';\n```\n\n----------------------------------------\n\nTITLE: Defining a Secret Input Field in Actor Input Schema (JSON)\nDESCRIPTION: This snippet shows how to add a 'isSecret: true' setting to an input field in the Actor's input schema to mark it as a secret input. This is applicable only for string inputs with textfield or textarea editors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/secret_input.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"properties\": {\n        \"password\": {\n            \"title\": \"Password\",\n            \"type\": \"string\",\n            \"description\": \"A secret, encrypted input field\",\n            \"editor\": \"textfield\",\n            \"isSecret\": true\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Input Schema for Number Addition Actor in JSON\nDESCRIPTION: This JSON schema defines the structure and validation rules for the Actor's input. It specifies that the Actor requires two integer inputs (num1 and num2) and provides titles and descriptions for generating a user interface on the Apify platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/inputs_outputs.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Number adder\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"num1\": {\n            \"title\": \"1st Number\",\n            \"type\": \"integer\",\n            \"description\": \"First number.\",\n            \"editor\": \"number\"\n        },\n        \"num2\": {\n            \"title\": \"2nd Number\",\n            \"type\": \"integer\",\n            \"description\": \"Second number.\",\n            \"editor\": \"number\"\n        }\n    },\n    \"required\": [\"num1\", \"num2\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Request Queue Payload Structure\nDESCRIPTION: Example JSON payload structure for adding or updating requests in a queue, showing required fields like uniqueKey, url, and method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\\n    \"uniqueKey\": \"http://example.com\",\\n    \"url\": \"http://example.com\",\\n    \"method\": \"GET\"\\n}\n```\n\n----------------------------------------\n\nTITLE: Importing modules and setting up credentials in Python\nDESCRIPTION: Python code to import necessary modules and set up credentials for Apify, OpenAI, and Milvus.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/milvus.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify_client import ApifyClient\n\nAPIFY_API_TOKEN = \"YOUR-APIFY-TOKEN\"\nOPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\"\n\nMILVUS_COLLECTION_NAME = \"YOUR-MILVUS-COLLECTION-NAME\"\nMILVUS_URI = \"YOUR-MILVUS-URI\"\nMILVUS_TOKEN = \"YOUR-MILVUS-TOKEN\"\nclient = ApifyClient(APIFY_API_TOKEN)\n```\n\n----------------------------------------\n\nTITLE: Price Range Handling in Python\nDESCRIPTION: Shows how to handle price ranges by separating minimum and exact prices into different variables.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprice_text = product.select_one(\".price\").contents[-1]\nif price_text.startswith(\"From \"):\n    min_price = price_text.removeprefix(\"From \")\n    price = None\nelse:\n    min_price = price_text\n    price = min_price\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Standby Mode in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a simple HTTP server for an Actor in Standby mode using JavaScript. It listens on the port specified by the Actor configuration and responds with a basic message.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/actor_standby.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport http from 'http';\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst server = http.createServer((req, res) => {\n    res.writeHead(200, { 'Content-Type': 'text/plain' });\n    res.end('Hello from Actor Standby!\\n');\n});\n\nserver.listen(Actor.config.get('containerPort'));\n```\n\n----------------------------------------\n\nTITLE: URL Console Logging Loop\nDESCRIPTION: Loop to print all product detail page URLs to the console using DevTools.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/filtering_links.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nfor (const a of document.querySelectorAll('a.product-item__title')) {\n    console.log(a.href);\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Number Addition in Python\nDESCRIPTION: A Python function that takes a list of numbers as input and returns their sum using a for loop. The function demonstrates basic arithmetic operations that will be converted into an Apify Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/index.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# index.py\ndef add_all_numbers (nums):\n    total = 0\n\n    for num in nums:\n        total += num\n\n    return total\n\nprint(add_all_numbers([1, 2, 3, 4])) # -> 10\n\n```\n\n----------------------------------------\n\nTITLE: Selecting Element by ID using JavaScript\nDESCRIPTION: Demonstrates how to select a single element by its ID using the ID selector with querySelector()\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/css_selectors.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst header = document.querySelector(`#header`);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating String Splitting in JavaScript Obfuscation\nDESCRIPTION: This snippet illustrates the string splitting technique used in data obfuscation for fingerprinting scripts. It typically involves concatenating multiple substrings and is often used with eval() or document.write() functions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/fingerprinting.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\neval('ale' + 'rt(\"Hello, World!\")')\n```\n\n----------------------------------------\n\nTITLE: Orchestrator Actor Input Schema Configuration\nDESCRIPTION: JSON schema defining the input structure for the Orchestrator Actor, including parameters for parallel run count, target Actor ID, and configuration options.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n// OrchestratorActorInputSchemaJson reference\n```\n\n----------------------------------------\n\nTITLE: Basic Link Selection with QuerySelector\nDESCRIPTION: Simple examples of selecting all anchor tags from a webpage using both DevTools and Cheerio.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/filtering_links.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('a');\n```\n\nLANGUAGE: javascript\nCODE:\n```\n$('a');\n```\n\n----------------------------------------\n\nTITLE: Deleting Storage with Apify SDK and API Client\nDESCRIPTION: Methods for deleting storage resources using JavaScript SDK, Python SDK and API clients. Each storage type (Dataset, Key-value store, Request queue) provides a drop() or delete() method for removal.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/usage.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// JavaScript SDK\ndataset.drop()\nkeyValueStore.drop()\nrequestQueue.drop()\n\n// JavaScript API client\ndatasetClient.delete()\nkeyValueStoreClient.delete()\nrequestQueueClient.delete()\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Python SDK\ndataset.drop()\nkey_value_store.drop()\nrequest_queue.drop()\n\n# Python API client\ndataset_client.delete()\nkey_value_store_client.delete()\nrequest_queue_client.delete()\n```\n\n----------------------------------------\n\nTITLE: Final Price Scraping Implementation\nDESCRIPTION: Complete implementation using Decimal for accurate price representation and proper string cleaning.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    print(title, min_price, price, sep=\" | \")\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Client in Python\nDESCRIPTION: Setup code for initializing the Apify client with API tokens and configuration for Pinecone and OpenAI integration\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/pinecone.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify_client import ApifyClient\n\nAPIFY_API_TOKEN = \"YOUR-APIFY-TOKEN\"\nOPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\"\nPINECONE_API_KEY = \"YOUR-PINECONE-API-KEY\"\nPINECONE_INDEX_NAME = \"YOUR-PINECONE-INDEX-NAME\"\n\nclient = ApifyClient(APIFY_API_TOKEN)\n```\n\n----------------------------------------\n\nTITLE: Accessing Payload and Input in Integration-Ready Actor\nDESCRIPTION: This JavaScript code shows how to access both the payload (containing dynamic data) and user-provided input in an integration-ready Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst { payload, connectionString, tableName } = await Actor.getInput();\nconst datasetId = payload.resource.defaultDatasetId;\n```\n\n----------------------------------------\n\nTITLE: Node.js Actor with Output Handling\nDESCRIPTION: Example showing how to write output to a dataset using the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// index.js\n\n// This is our example project code from earlier.\n// We will use the Apify input as its input.\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst { numbers } = await Actor.getInput();\n\nconst addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);\n\nconst solution = addAllNumbers(...numbers);\n\n// And save its output to the default dataset\nawait Actor.pushData({ solution });\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Loading Sessions in Apify Actor Main Function\nDESCRIPTION: Loads previously saved sessions from the key-value store or initializes an empty object if no sessions were saved.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nApify.main(async () => {\n    sessions = (await Apify.getValue('SESSIONS')) || {};\n    // ...the rest of your code\n});\n```\n\n----------------------------------------\n\nTITLE: Defining package.json for Apify Actor Node.js Project\nDESCRIPTION: This package.json file specifies the project dependencies, scripts, and metadata for an Apify Actor Node.js project.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/source_code.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"getting-started-node\",\n    \"version\": \"0.0.1\",\n    \"type\": \"module\",\n    \"description\": \"This is an example of an Apify Actor.\",\n    \"dependencies\": {\n        \"apify\": \"^3.0.0\"\n    },\n    \"devDependencies\": {},\n    \"scripts\": {\n        \"start\": \"node src/main.js\",\n        \"test\": \"echo \\\"Error: oops, the Actor has no tests yet, sad!\\\" && exit 1\"\n    },\n    \"author\": \"It's not you; it's me\",\n    \"license\": \"ISC\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running an Apify Actor from Slack\nDESCRIPTION: Shows how to use the Slack slash command to run an Apify Actor or task directly from Slack. This command allows users to execute Actors without leaving their Slack workspace.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/workflows-and-notifications/slack.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n/apify call [Actor or task ID]\n```\n\n----------------------------------------\n\nTITLE: Setting Output Directory for Compiled Files in TypeScript\nDESCRIPTION: This configuration snippet demonstrates how to specify the output directory for compiled TypeScript files using the 'outDir' option in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"outDir\": \"dist/\"\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Location Constants\nDESCRIPTION: Setting up constant values for target locations with their BBC Weather IDs to be scraped.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Locations which to scrape and their BBC Weather IDs\nLOCATIONS = [\n    ('Prague', '3067696'),\n    ('Honolulu', '5856195'),\n    ('New York', '5128581'),\n]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Persisted State - JavaScript\nDESCRIPTION: Example showing how to retrieve previously saved state when an Actor starts using getValue() in JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nconst previousCrawlingState = await Actor.getValue('my-crawling-state') || {};\n// ...\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Adding Path Documentation in OpenAPI YAML\nDESCRIPTION: This YAML snippet shows how to add path references to the OpenAPI specification file for request queues endpoints.\nSOURCE: https://github.com/apify/apify-docs/blob/master/CONTRIBUTING.md#2025-04-18_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n'/requests-queues':\n  $ref: './paths/request-queues/request-queues.yaml'\n'/requests-queues/{queueId}':\n  $ref: './paths/request-queues/request-queues@{queueId}.yaml'\n```\n\n----------------------------------------\n\nTITLE: Server Startup Configuration\nDESCRIPTION: Starts the Express.js server on the specified container port.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\napp.listen(APIFY_CONTAINER_PORT, () => {\n    console.log(`Application is listening at URL ${APIFY_CONTAINER_URL}.`);\n});\n```\n\n----------------------------------------\n\nTITLE: Resulting Input Object for Website Content Crawler Actor\nDESCRIPTION: This JSON object represents the actual input passed to the Actor based on the input schema defined above. It includes the user-specified values for various configuration options.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/index.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"debugMode\": false,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"saveHtml\": false,\n    \"saveMarkdown\": false,\n    \"saveScreenshots\": false,\n    \"startUrls\": [\n        {\n            \"url\": \"https://docs.apify.com/\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Proxy Country in Python\nDESCRIPTION: Example of how to specify both the residential proxy group and a specific country (Japan) in the username parameter when using Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nusername = \"groups-RESIDENTIAL,country-JP\"\n```\n\n----------------------------------------\n\nTITLE: Variable Declaration Without Initial Value\nDESCRIPTION: Shows how TypeScript handles variable declaration without an initial value, defaulting to 'any' type.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nlet value;\n\nvalue = 10;\n```\n\n----------------------------------------\n\nTITLE: Complete PageFunction Testing with Mocked Context in Browser Console\nDESCRIPTION: A comprehensive approach to test a full pageFunction in the browser console by creating a mock of the Apify context object. This allows testing the entire function without needing to run it in Apify.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    // this is your pageFunction\n}\n// Now you will call it with mocked context\npageFunction({\n    request: {\n        url: window.location.href,\n        userData: { label: 'paste-a-label-if-you-use-one' },\n    },\n    async waitFor(ms) {\n        console.log('(waitFor)');\n        await new Promise((res) => setTimeout(res, ms));\n    },\n    enqueueRequest() { console.log('(enqueuePage)', arguments); },\n    skipLinks() { console.log('(skipLinks)', arguments); },\n    jQuery: $,\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Title and Description with jQuery\nDESCRIPTION: Extracts both the title and description from a webpage. The description is selected using a CSS selector targeting a span element with the class 'actor-description' inside the header.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { jQuery: $ } = context;\n\n    // ... rest of the code\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify Platform\nDESCRIPTION: Command to authenticate with the Apify platform using an API token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input for Actor\nDESCRIPTION: This code snippet demonstrates how the input JSON would look when entered through the generated UI. It includes the startUrls array and pageFunction string as defined in the input schema.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"startUrls\": [\n    {\n        \"url\": \"http://example.com\"\n    },\n    {\n        \"url\": \"http://example.com/some-path\"\n    }\n    ],\n    \"pageFunction\": \"async () => { return $('title').text(); }\"\n}\n```\n\n----------------------------------------\n\nTITLE: Julia Actor Dockerfile Example\nDESCRIPTION: A simple example Dockerfile for a Julia Actor. It uses the official Julia Alpine image, sets the working directory, copies the source code, runs an installation script, and specifies how to run the main script.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_5\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM julia:1.7.1-alpine\n\nWORKDIR /app\nCOPY . .\n\nRUN julia install.jl\n\nCMD [\"julia\", \"main.jl\"]\n```\n\n----------------------------------------\n\nTITLE: Using Apify CLI to Deploy Actors\nDESCRIPTION: The Apify CLI command for pushing local code to the Apify platform. This command uploads the current directory contents to the platform where it will be automatically built. It's recommended to use .gitignore to exclude unnecessary files like node_modules and storage directories.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/deploying.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Creating an Apify Document Loader with Website Content Crawler\nDESCRIPTION: Configures a document loader using Apify's Website Content Crawler to extract content from a specified website and convert it to Haystack Document objects.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndocument_loader = ApifyDatasetFromActorCall(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\n        \"maxCrawlPages\": 3,  # limit the number of pages to crawl\n        \"startUrls\": [{\"url\": \"https://haystack.deepset.ai/\"}],\n    },\n    dataset_mapping_function=lambda item: Document(content=item[\"text\"] or \"\", meta={\"url\": item[\"url\"]}),\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Page Setup with Playwright/Puppeteer\nDESCRIPTION: Initial setup code for launching a browser and opening a page using either Playwright or Puppeteer. Opens Tiësto's SoundCloud following list page for demonstration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Our code will go here\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Our code will go here\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: JSON File Storage Path\nDESCRIPTION: Displays the storage path where the exported JSON file will be saved in the key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n./storage/key-value-stores/default/results.json\n```\n\n----------------------------------------\n\nTITLE: Default Dockerfile for Apify Actors\nDESCRIPTION: This Dockerfile is used as the default configuration for Apify Actors when no custom Dockerfile is provided. It uses the apify/actor-node:20 base image and sets up the Node.js environment.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/docker.md#2025-04-18_snippet_0\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM apify/actor-node:20\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n && npm install --only=prod --no-optional \\\n && echo \"Installed NPM packages:\" \\\n && (npm list --only=prod --no-optional --all || true) \\\n && echo \"Node.js version:\" \\\n && node --version \\\n && echo \"NPM version:\" \\\n && npm --version\n\nCOPY . ./\n```\n\n----------------------------------------\n\nTITLE: Using Enums as Types in Function Parameters\nDESCRIPTION: This snippet demonstrates how to use enums as type annotations in function parameters. It shows a function that creates a file name using an enum value as the file extension.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/enums.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nenum FileExtension {\n    JAVASCRIPT = '.js',\n    TYPESCRIPT = '.ts',\n    RUST = '.rs',\n    PYTHON = '.py',\n}\n\nconst createFileName = (name: string, extension: FileExtension) => {\n    return name + extension;\n};\n\n// Call the function and use the enum to populate the second parameter\nconst fileName = createFileName('hello', FileExtension.TYPESCRIPT);\n\nconsole.log(fileName);\n```\n\n----------------------------------------\n\nTITLE: Sample API Response Structure\nDESCRIPTION: Example JSON structure returned by the Dummy JSON API endpoint showing product data format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id\": 1,\n    \"title\": \"iPhone 9\",\n    \"description\": \"An apple mobile which is nothing like apple\",\n    \"price\": 549,\n    \"discountPercentage\": 12.96,\n    \"rating\": 4.69,\n    \"stock\": 94,\n    \"brand\": \"Apple\",\n    \"category\": \"smartphones\",\n    \"thumbnail\": \"https://dummyjson.com/image/i/products/1/thumbnail.jpg\",\n    \"images\": [\n        \"https://dummyjson.com/image/i/products/1/1.jpg\",\n        \"https://dummyjson.com/image/i/products/1/2.jpg\",\n        \"https://dummyjson.com/image/i/products/1/3.jpg\",\n        \"https://dummyjson.com/image/i/products/1/4.jpg\",\n        \"https://dummyjson.com/image/i/products/1/thumbnail.jpg\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Elements by Attribute using JavaScript\nDESCRIPTION: Shows how to select elements based on their attribute values using attribute selectors with querySelectorAll()\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/css_selectors.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst customElements = document.querySelectorAll('[data-custom=\"yes\"]');\n```\n\n----------------------------------------\n\nTITLE: Python API Client Usage\nDESCRIPTION: Example showing how to initialize and use the Python API client to access request queues.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmy_queue_client = apify_client.request_queue('jane-doe/my-request-queue')\n```\n\n----------------------------------------\n\nTITLE: Authenticated Proxy Configuration\nDESCRIPTION: Demonstrates the correct way to implement authenticated proxies in both frameworks, showing how to properly pass username and password credentials.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/proxies.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst proxy = 'proxy.example.com:3001';\nconst username = 'someUsername';\nconst password = 'password123';\n\nconst browser = await chromium.launch({\n    headless: false,\n    proxy: {\n        server: proxy,\n        username,\n        password,\n    },\n});\n// Proxy will now be authenticated\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst proxy = 'proxy.example.com:3001';\nconst username = 'someUsername';\nconst password = 'password123';\n\nconst browser = await puppeteer.launch({\n    headless: false,\n    args: [`--proxy-server=${proxy}`],\n});\n\nconst page = await browser.newPage();\n\nawait page.authenticate({ username, password });\n// Proxy will now be authenticated\n```\n\n----------------------------------------\n\nTITLE: Selecting Elements by Tag Name using JavaScript\nDESCRIPTION: Demonstrates how to select all paragraph elements on a page using the element selector with querySelectorAll()\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/css_selectors.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst paragraphs = document.querySelectorAll('p');\n```\n\n----------------------------------------\n\nTITLE: Browser Fingerprint JSON Structure\nDESCRIPTION: Example JSON structure showing a complete browser fingerprint including user agent, codecs support, hardware info, and system capabilities.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/fingerprinting.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n  \"cookiesEnabled\": true,\n  \"timezone\": \"Europe/Prague\",\n  \"timezoneOffset\": -60,\n  \"audioCodecs\": {\n    \"ogg\": \"probably\",\n    \"mp3\": \"maybe\",\n    \"wav\": \"probably\",\n    \"m4a\": \"maybe\",\n    \"aac\": \"maybe\"\n  },\n  \"videoCodecs\": {\n    \"ogg\": \"probably\",\n    \"h264\": \"probably\",\n    \"webm\": \"probably\"\n  },\n  \"videoCard\": [\n    \"Intel Open Source Technology Center\",\n    \"Mesa DRI Intel(R) HD Graphics 4600 (HSW GT2)\"\n  ],\n  \"productSub\": \"20100101\",\n  \"hardwareConcurrency\": 8,\n  \"multimediaDevices\": {\n    \"speakers\": 0,\n    \"micros\": 0,\n    \"webcams\": 0\n  },\n  \"platform\": \"Linux x86_64\",\n  \"pluginsSupport\": true,\n  \"screenResolution\": [ 1920, 1080 ],\n  \"availableScreenResolution\": [ 1920, 1080 ],\n  \"colorDepth\": 24,\n  \"touchSupport\": {\n    \"maxTouchPoints\": 0,\n    \"touchEvent\": false,\n    \"touchStart\": false\n  },\n  \"languages\": [ \"en-US\", \"en\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Limiting Proxy Location to United States\nDESCRIPTION: Configures the proxy to only use IP addresses from the United States by setting the countryCode parameter to 'US'. This is useful for geo-restricted content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'US',\n});\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output with Full URLs\nDESCRIPTION: Example of the JSON output produced by the scraper, showing properly formed full URLs for each product instead of relative paths.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"title\": \"JBL Flip 4 Waterproof Portable Bluetooth Speaker\",\n    \"min_price\": \"74.95\",\n    \"price\": \"74.95\",\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/jbl-flip-4-waterproof-portable-bluetooth-speaker\"\n  },\n  {\n    \"title\": \"Sony XBR-950G BRAVIA 4K HDR Ultra HD TV\",\n    \"min_price\": \"1398.00\",\n    \"price\": null,\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/sony-xbr-65x950g-65-class-64-5-diag-bravia-4k-hdr-ultra-hd-tv\"\n  },\n  ...\n]\n```\n\n----------------------------------------\n\nTITLE: JSON Payload Example for Adding Items to Dataset\nDESCRIPTION: Example of a JSON array payload that can be sent to the dataset API to add multiple items. Each object in the array represents a single item to be stored in the dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"foo\": \"bar\"\n    },\n    {\n        \"foo\": \"hotel\"\n    },\n    {\n        \"foo\": \"cafe\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: AliExpress HTML Scraping\nDESCRIPTION: Solution for downloading HTML from AliExpress search results page\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\n\nurl = \"https://www.aliexpress.com/w/wholesale-darth-vader.html\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Actor Exit Operations in Python\nDESCRIPTION: Demonstrates different ways to terminate an Actor in Python, including success and failure scenarios.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Actor will finish with 'SUCCEEDED' status\n        await Actor.exit(status_message='Succeeded, crawled 50 pages')\n        # INFO  Exiting actor ({\"exit_code\": 0})\n        # INFO  [Terminal status message]: Succeeded, crawled 50 pages\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Exit right away without calling `exit` handlers at all\n        await Actor.exit(event_listeners_timeout_secs=0, status_message='Done right now')\n        # INFO  Exiting actor ({\"exit_code\": 0})\n        # INFO  [Terminal status message]: Done right now\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # Actor will finish with 'FAILED' status\n        await Actor.exit(status_message='Could not finish the crawl, try increasing memory', exit_code=1)\n        # INFO  Exiting actor ({\"exit_code\": 1})\n        # INFO  [Terminal status message]: Could not finish the crawl, try increasing memory\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # ... or nicer way using this syntactic sugar:\n        await Actor.fail(status_message='Could not finish the crawl. Try increasing memory')\n        # INFO  Exiting actor ({\"exit_code\": 1})\n        # INFO  [Terminal status message]: Could not finish the crawl. Try increasing memory\n```\n\n----------------------------------------\n\nTITLE: Actor Run Event Data Format in JSON\nDESCRIPTION: The JSON structure provided in event data for Actor run events. Contains identifiers for the Actor, task (if applicable), and the specific run that triggered the event.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/events.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorId\": \"ID of the triggering Actor.\",\n    \"actorTaskId\": \"If task was used, its ID.\",\n    \"actorRunId\": \"ID of the triggering Actor run.\",\n}\n```\n\n----------------------------------------\n\nTITLE: Running PDF Conversion Actor\nDESCRIPTION: Example of running an actor that converts HTML to PDF and stores result in key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_6\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->post('acts/mhamas~html-string-to-pdf/runs', [\n    'json' => [\n        'htmlString' => '<html><body><h1>Hello World</h1></body></html>'\n    ],\n]);\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Creating an Actor from Template with Apify CLI\nDESCRIPTION: These shell commands demonstrate how to create a new Actor from a template using the Apify CLI. The first command creates a new Actor project from the 'getting_started_node' template, and the second command runs the Actor locally.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/creating_actors.md#2025-04-18_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\napify create my-actor -t getting_started_node\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncd my-actor\napify run\n```\n\n----------------------------------------\n\nTITLE: Final TypeScript Solution with Type Conversion\nDESCRIPTION: The final TypeScript solution that converts string prices to numbers when passing them to the addPrices function, ensuring type safety and correct calculation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst products = [\n    {\n        title: 'iPhone',\n        price: '1000',\n    },\n    {\n        title: 'iPad',\n        price: '1099',\n    },\n];\n\nconst addPrices = (price1: number, price2: number) => {\n    return price1 + price2;\n};\n\n// Convert the values to numbers as they are passed in\nconsole.log(addPrices(+products[0].price, +products[1].price));\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Index from Crawled Documents\nDESCRIPTION: Initializes a vector store index using OpenAI embeddings from the crawled documents.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=InMemoryVectorStore,\n    embedding=OpenAIEmbeddings()\n).from_loaders([loader])\n```\n\n----------------------------------------\n\nTITLE: Basic HTTPX Import Test\nDESCRIPTION: Simple Python script to verify HTTPX installation and project setup\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\n\nprint(\"OK\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Schema in actor.json\nDESCRIPTION: Demonstrates how to directly define dataset schema validation rules in the actor.json configuration file using JSON Schema draft-07.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"fields\": {\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\n                        \"type\": \"string\"\n                    }\n                },\n                \"required\": [\"name\"]\n            },\n            \"views\": {}\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Enums in TypeScript\nDESCRIPTION: This snippet shows how to use enums in TypeScript. It demonstrates accessing enum values using dot notation and logging the result.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/enums.md#2025-04-18_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nenum FileExtension {\n    JAVASCRIPT = '.js',\n    TYPESCRIPT = '.ts',\n    RUST = '.rs',\n    PYTHON = '.py',\n}\n\nconst value = FileExtension.JAVASCRIPT;\n\nconsole.log(value); // => \".js\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Element Text Content\nDESCRIPTION: Shows how to extract the text content from a selected DOM element.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/using_devtools.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nsubwoofer.textContent;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Apify API Endpoint for Actor Run\nDESCRIPTION: Example of an Apify API endpoint URL for running an Actor synchronously and retrieving dataset items. The URL includes placeholders for your username and API token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_api.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync?token=YOUR_TOKEN\n```\n\n----------------------------------------\n\nTITLE: URL Resolution Example\nDESCRIPTION: Example showing how to resolve relative URLs to absolute URLs using the URL constructor in Node.js.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/relative_urls.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst websiteUrl = 'https://warehouse-theme-metal.myshopify.com';\nconst relativeUrl = '/products/denon-ah-c720-in-ear-headphones';\n\nconst absoluteUrl = new URL(relativeUrl, websiteUrl);\nconsole.log(absoluteUrl.href);\n```\n\n----------------------------------------\n\nTITLE: Running Actors with Python Client\nDESCRIPTION: Example of running the Google Maps Scraper Actor using the Python API client. Demonstrates client initialization, actor execution, and result retrieval.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/index.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom apify_client import ApifyClient\n\napify_client = ApifyClient('MY-API-TOKEN')\n\n# Start the Google Maps Scraper Actor and wait for it to finish.\nactor_run = apify_client.actor('compass/crawler-google-places').call(\n    run_input={ 'queries': 'apify' }\n)\n\n# Fetch scraped results from the Actor's dataset.\ndataset_items = apify_client.dataset(actor_run['defaultDatasetId']).list_items().items\nprint(dataset_items)\n```\n\n----------------------------------------\n\nTITLE: Selecting Single Element with querySelector in JavaScript\nDESCRIPTION: Demonstrates how to select the first button element on a page using document.querySelector(). Returns the first matching element or null if no match is found.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/querying_css_selectors.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst firstButton = document.querySelector('button');\n```\n\n----------------------------------------\n\nTITLE: EACCES Error Message in Actor Build Logs\nDESCRIPTION: Example error message displayed when permission issues occur during Actor builds with a custom Dockerfile.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/avoid_eacces_error_in_actor_builds.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nMissing write access to ...\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies Example - Python\nDESCRIPTION: Example of installing required dependencies using pip, mentioned in requirements section\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/index.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dependencies\n```\n\n----------------------------------------\n\nTITLE: Single Configuration File for Actor in JSON\nDESCRIPTION: This JSON configuration demonstrates a single file setup for an Actor, including the actor specification, name, title, version, and dataset schema structure. It shows the basic structure for defining views and transformations within the dataset configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/index.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"this-is-book-library-scraper\",\n    \"title\": \"Book Library scraper\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"fields\": {},\n            \"views\": {\n                \"overview\": {\n                    \"title\": \"Overview\",\n                    \"transformation\": {},\n                    \"display\": {}\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Module Resolution in TypeScript\nDESCRIPTION: This configuration snippet demonstrates how to set module resolution and type for a Node.js project using TypeScript, including options for moduleResolution, skipLibCheck, and module in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"lib\": [\"ES2015\", \"ES2016\", \"ES2018\", \"ES2019.Object\", \"ES2018.AsyncIterable\", \"ES2020.String\", \"ES2019.Array\"],\n        \"outDir\": \"dist/\",\n        \"removeComments\": true,\n        \"noEmitOnError\": true,\n        \"strict\": true,\n        \"moduleResolution\": \"node\",\n        \"skipLibCheck\": true,\n        \"module\": \"commonjs\"\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Markdown and HTML Usage\nDESCRIPTION: Demonstrates the interchangeable use of Markdown and HTML in Actor README files. Shows that while both Markdown and basic HTML are supported, CSS styling is not permitted.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/actor_basics/how-to-create-actor-readme.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Heading 2\n### Heading 3\n[Link text](url)\n![Image alt text](image-url)\n```\n\nLANGUAGE: html\nCODE:\n```\n<h2>Heading 2</h2>\n<h3>Heading 3</h3>\n<a href=\"url\">Link text</a>\n<img src=\"image-url\" alt=\"Image alt text\">\n```\n\n----------------------------------------\n\nTITLE: Extracting Hourly Weather Data from BBC Weather Page\nDESCRIPTION: This code snippet extracts hourly weather data from the BBC Weather page, including datetime and temperature information, and appends it to a list of weather data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n        # Go through the elements for each displayed time slot of the displayed day\n        slot_container = soup.find(class_='wr-time-slot-container__slots')\n        for slot in slot_container.find_all(class_='wr-time-slot'):\n            # Find out the date and time of the displayed element from the day offset and the displayed hour.\n            # The times displayed for each day are from 6:00 AM that day to 5:00 AM the next day,\n            # so anything between midnight and 6 AM actually represents the next day\n            slot_hour = int(slot.find(class_='wr-time-slot-primary__hours').text)\n            slot_datetime = datetime.combine(first_displayed_date, time(hour=slot_hour), tzinfo=location_timezone)\n            slot_datetime += timedelta(days=day_offset)\n            if slot_hour < 6:\n                slot_datetime += timedelta(days=1)\n\n            # Parse the temperature from the right element\n            slot_temperature = int(slot.find(class_='wr-value--temperature--c').text[:-1])\n\n            # Add the parsed data to the result list\n            weather_data.append({\n                'datetime': slot_datetime,\n                'location': location_name,\n                'temperature': slot_temperature,\n            })\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Apify CLI\nDESCRIPTION: Command to log in to your Apify account using the CLI tool.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/deployment/index.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Collecting Scraped Product Data in Python\nDESCRIPTION: Modifies the existing scraping code to collect product data into a list of dictionaries instead of printing each item.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\ndata = []\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    data.append({\"title\": title, \"min_price\": min_price, \"price\": price})\n\nprint(data)\n```\n\n----------------------------------------\n\nTITLE: Using select_one() for Single Element Selection\nDESCRIPTION: Demonstrates the use of select_one() method to simplify code when targeting single elements instead of lists.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/06_locating_elements.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text\n    price = product.select_one(\".price\").text\n    print(title, price)\n```\n\n----------------------------------------\n\nTITLE: Running a JavaScript file with Node.js in Shell\nDESCRIPTION: This command executes a JavaScript file named 'hello.js' using Node.js. It's used to run the 'Hello World' script and verify the Node.js setup.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/computer_preparation.md#2025-04-18_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nnode hello.js\n```\n\n----------------------------------------\n\nTITLE: String Interpolation in Payload Template\nDESCRIPTION: This snippet shows how to use string interpolation in the payload template. It allows for creating valid JSON while still using variables, which is useful when integrating Actors or tasks.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/actions.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"userId\": \"{{userId}}\",\n    \"createdAt\": \"{{createdAt}}\",\n    \"eventType\": \"{{eventType}}\",\n    \"eventData\": \"{{eventData}}\",\n    \"resource\": \"{{resource}}\"\n}\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Actor Detail Pages from JSON Data\nDESCRIPTION: This snippet demonstrates how to parse JSON data from a script tag, extract actor information, and enqueue detail pages for further scraping.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n// We're not in DevTools anymore,\n// so we use Cheerio to get the data.\nconst dataJson = $('#__NEXT_DATA__').html();\n// We requested HTML, but the data are actually JSON.\nconst data = JSON.parse(dataJson);\n\nfor (const item of data.props.pageProps.items) {\n    const { name, username } = item;\n    const actorDetailUrl = `https://apify.com/${username}/${name}`;\n    await context.enqueueRequest({\n        url: actorDetailUrl,\n        userData: {\n            // Don't forget the label.\n            label: 'DETAIL',\n        },\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Start URL for Apify Store Scraper\nDESCRIPTION: Specifies the starting point URL for the web scraper to begin crawling the Apify Store. This is the main entry point for the scraper.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://apify.com/store\n```\n\n----------------------------------------\n\nTITLE: Using Default Persistent Context in Puppeteer\nDESCRIPTION: Shows how to use the default persistent browser context in Puppeteer, which stores cache, cookies, and storage on disk by default.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// This page will be under the default context, which is persistent.\n// Cache, cookies, etc. will be stored on disk and persisted\nconst page = await browser.newPage();\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: GraphQL Introspection Query\nDESCRIPTION: A comprehensive introspection query that retrieves the complete schema information including types, queries, mutations, and directives from a GraphQL API.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/introspection.md#2025-04-18_snippet_0\n\nLANGUAGE: graphql\nCODE:\n```\nquery {\n  __schema {\n    queryType {\n      name\n    }\n    mutationType {\n      name\n    }\n    subscriptionType {\n      name\n    }\n    types {\n      ...FullType\n    }\n    directives {\n      name\n      description\n      locations\n      args {\n        ...InputValue\n      }\n    }\n  }\n}\nfragment FullType on __Type {\n  kind\n  name\n  description\n  fields(includeDeprecated: true) {\n    name\n    description\n    args {\n      ...InputValue\n    }\n    type {\n      ...TypeRef\n    }\n    isDeprecated\n    deprecationReason\n  }\n  inputFields {\n    ...InputValue\n  }\n  interfaces {\n    ...TypeRef\n  }\n  enumValues(includeDeprecated: true) {\n    name\n    description\n    isDeprecated\n    deprecationReason\n  }\n  possibleTypes {\n    ...TypeRef\n  }\n}\nfragment InputValue on __InputValue {\n  name\n  description\n  type {\n    ...TypeRef\n  }\n  defaultValue\n}\nfragment TypeRef on __Type {\n  kind\n  name\n  ofType {\n    kind\n    name\n    ofType {\n      kind\n      name\n      ofType {\n        kind\n        name\n        ofType {\n          kind\n          name\n          ofType {\n            kind\n            name\n            ofType {\n              kind\n              name\n              ofType {\n                kind\n                name\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Default Payload Output in JSON\nDESCRIPTION: This snippet demonstrates an example of the default payload output for an Apify webhook. It shows how the variables are replaced with actual values when the webhook is triggered.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/actions.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"userId\": \"abf6vtB2nvQZ4nJzo\",\n    \"createdAt\": \"2019-01-09T15:59:56.408Z\",\n    \"eventType\": \"ACTOR.RUN.SUCCEEDED\",\n    \"eventData\": {\n        \"actorId\": \"fW4MyDhgwtMLrB987\",\n        \"actorRunId\": \"uPBN9qaKd2iLs5naZ\"\n    },\n    \"resource\": {\n        \"id\": \"uPBN9qaKd2iLs5naZ\",\n        \"actId\": \"fW4MyDhgwtMLrB987\",\n        \"userId\": \"abf6vtB2nvQZ4nJzo\",\n        \"startedAt\": \"2019-01-09T15:59:40.750Z\",\n        \"finishedAt\": \"2019-01-09T15:59:56.408Z\",\n        \"status\": \"SUCCEEDED\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Elements by Tag Name\nDESCRIPTION: JavaScript code showing how to select multiple DOM elements using getElementsByTagName() method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/html_elements.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst myElements = document.getElementsByTagName('p');\n```\n\n----------------------------------------\n\nTITLE: Extracting URL and Unique Identifier in JavaScript\nDESCRIPTION: This snippet demonstrates how to extract the URL and a unique identifier from the request object in an Apify scraper.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst { url } = request;\nconst uniqueIdentifier = url.split('/').slice(-2).join('/');\n```\n\n----------------------------------------\n\nTITLE: Accessing Datasets with JavaScript API Client\nDESCRIPTION: This snippet demonstrates how to access a dataset using the JavaScript API client. It creates a dataset client instance that can be used to interact with a dataset belonging to another user.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_22\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst otherDatasetClient = apifyClient.dataset('jane-doe/old-dataset');\n```\n\n----------------------------------------\n\nTITLE: Initial Price Text Cleaning in Python\nDESCRIPTION: Demonstrates basic string manipulation to clean price text by removing 'From' prefix using Python's string methods.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprice_text = product.select_one(\".price\").contents[-1]\nprice = price_text.removeprefix(\"From \")\n```\n\n----------------------------------------\n\nTITLE: Configuring Comment Removal in TypeScript Compilation\nDESCRIPTION: This snippet shows how to configure TypeScript to remove comments from compiled files using the 'removeComments' option in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"lib\": [\"ES2015\", \"ES2016\", \"ES2018\", \"ES2019.Object\", \"ES2018.AsyncIterable\", \"ES2020.String\", \"ES2019.Array\"],\n        \"outDir\": \"dist/\",\n        \"removeComments\": true\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Preventing Compilation on Errors in TypeScript\nDESCRIPTION: This configuration snippet demonstrates how to prevent TypeScript from compiling when there are errors by using the 'noEmitOnError' option in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"lib\": [\"ES2015\", \"ES2016\", \"ES2018\", \"ES2019.Object\", \"ES2018.AsyncIterable\", \"ES2020.String\", \"ES2019.Array\"],\n        \"outDir\": \"dist/\",\n        \"removeComments\": true,\n        \"noEmitOnError\": true\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Actor Run Process with Mermaid\nDESCRIPTION: Flowchart demonstrating how a build combines with input to create an Actor run.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/index.md#2025-04-18_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph AD [+]\n        direction LR\n        Build\n        Input\n    end\n\n    AD -- \"start Actor\" --> Run\n```\n\n----------------------------------------\n\nTITLE: Initializing Dependencies and Constants for Shopify Scraper in JavaScript\nDESCRIPTION: This snippet imports the required libraries (got-scraping and cheerio) and defines the base URL constant for the Shopify store to be scraped.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\n```\n\n----------------------------------------\n\nTITLE: Setting up Webhooks for Apify Actors in PHP\nDESCRIPTION: This snippet shows how to set up webhooks for Apify Actors using PHP. It encodes webhook data as a base64 JSON string and includes it in the API request to run an Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_11\n\nLANGUAGE: php\nCODE:\n```\n// Webhooks need to be passed as a base64-encoded JSON string\n$webhooks = \\base64_encode(\\json_encode([\n    [\n        // The webhook can be sent on multiple events\n        // this one fires when the run succeeds\n        'eventTypes' => ['ACTOR.RUN.SUCCEEDED'],\n        // Set this to some url that you can react to\n        // To see what is sent to the URL,\n        // you can set up a temporary request bin at https://requestbin.com/r\n        'requestUrl' => '<WEBHOOK_ENDPOINT_URL>',\n    ],\n]));\n$response = $client->post('acts/mhamas~html-string-to-pdf/runs', [\n    'json' => [\n        'htmlString' => '<html><body><h1>Hello World</h1></body></html>'\n    ],\n    'query' => [ 'webhooks' => $webhooks ]\n]);\n```\n\n----------------------------------------\n\nTITLE: Opening Datasets from Other Runs with Apify SDK in JavaScript\nDESCRIPTION: This code demonstrates how to open a dataset from another run using the Apify SDK in JavaScript. The Actor.openDataset() method allows accessing a dataset by name.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_20\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst otherDataset = await Actor.openDataset('old-dataset');\n// ...\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Precise Price Extraction using contents\nDESCRIPTION: Shows how to precisely extract price information by accessing specific nodes using the contents property.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/06_locating_elements.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text\n    price = product.select_one(\".price\").contents[-1]\n    print(title, price)\n```\n\n----------------------------------------\n\nTITLE: Custom Payload Template Example in JSON\nDESCRIPTION: This example demonstrates how to create a custom payload template using specific resource properties. It includes the run ID, run status, and a custom property.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/actions.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"runId\": {{resource.id}},\n    \"runStatus\": {{resource.status}},\n    \"myProp\": \"hello world\"\n}\n```\n\n----------------------------------------\n\nTITLE: TypeScript Example with Type Annotations\nDESCRIPTION: A TypeScript version of the example using type annotations to ensure only numbers are accepted by the addPrices function. This prevents type-related errors at compile-time.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst products = [\n    {\n        title: 'iPhone',\n        price: '1000',\n    },\n    {\n        title: 'iPad',\n        price: '1099',\n    },\n];\n\n// Add type annotations so that now the function will\n// only accept numbers.\nconst addPrices = (price1: number, price2: number) => {\n    return price1 + price2;\n};\n\nconsole.log(addPrices(products[0].price, products[1].price));\n```\n\n----------------------------------------\n\nTITLE: Regular Expression for Matching Beer URLs\nDESCRIPTION: This regular expression pattern matches beer URLs from the sitemap while excluding other pages like brewery pages or contact pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_3\n\nLANGUAGE: RegExp\nCODE:\n```\nhttp(s)?:\\/\\/www\\.brewbound\\.com\\/breweries\\/[^\\/]+\\/[^\\/<]+\n```\n\n----------------------------------------\n\nTITLE: Input Schema Configuration\nDESCRIPTION: JSON schema defining the input parameters for the Actor including memory, client usage flag, fields and maximum items.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/using_api_and_client.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"title\": \"Actor Caller\",\n  \"type\": \"object\",\n  \"schemaVersion\": 1,\n  \"properties\": {\n    \"memory\": {\n      \"title\": \"Memory\",\n      \"type\": \"integer\",\n      \"description\": \"Select memory in megabytes.\",\n      \"default\": 4096,\n      \"maximum\": 32768,\n      \"unit\": \"MB\"\n    },\n    \"useClient\": {\n      \"title\": \"Use client?\",\n      \"type\": \"boolean\",\n      \"description\": \"Specifies whether the Apify JS client, or the pure Apify API should be used.\",\n      \"default\": true\n    },\n    \"fields\": {\n      \"title\": \"Fields\",\n      \"type\": \"array\",\n      \"description\": \"Enter the dataset fields to export to CSV\",\n      \"prefill\": [\"title\", \"url\", \"price\"],\n      \"editor\": \"stringList\"\n    },\n    \"maxItems\": {\n      \"title\": \"Max items\",\n      \"type\": \"integer\",\n      \"description\": \"Fill the maximum number of items to export.\",\n      \"default\": 10\n    }\n  },\n  \"required\": [\"useClient\", \"memory\", \"fields\", \"maxItems\"]\n}\n```\n\n----------------------------------------\n\nTITLE: API Endpoint Examples for Actor/Task Execution\nDESCRIPTION: Basic cURL endpoint examples for running Actors and tasks via the Apify API with authentication token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_0\n\nLANGUAGE: cURL\nCODE:\n```\nhttps://api.apify.com/v2/acts/ACTOR_NAME_OR_ID/runs?token=YOUR_TOKEN\n```\n\nLANGUAGE: cURL\nCODE:\n```\nhttps://api.apify.com/v2/actor-tasks/TASK_NAME_OR_ID/runs?token=YOUR_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Getting Actor Input in Python\nDESCRIPTION: Shows how to retrieve the Actor's input object in Python implementation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        actor_input: dict = await Actor.get_input() or {}\n        Actor.log.info(actor_input)\n        # prints: {'option1': 'aaa', 'option2': 456}\n```\n\n----------------------------------------\n\nTITLE: Extracting Title, Description and Modified Date with jQuery\nDESCRIPTION: Extracts title, description, and modified date from a webpage. The date is extracted from a time element's datetime attribute, converted from a Unix timestamp string to a JavaScript Date object.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { jQuery: $ } = context;\n\n    // ... rest of the code\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Dataset Information via API\nDESCRIPTION: API endpoint for retrieving metadata about a specific dataset, such as its creation time and item count. The DATASET_ID placeholder should be replaced with your actual dataset identifier.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/datasets/{DATASET_ID}\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Latest Version Workflow\nDESCRIPTION: GitHub Actions workflow configuration for testing and building the latest version of an Actor. Triggers on pushes to master/main branches and includes steps for running tests and building the latest version.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/deployment/continuous_integration.md#2025-04-18_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: Test and build latest version\non:\n  push:\n    branches:\n      - master\n      - main\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      # Install dependencies and run tests\n      - uses: actions/checkout@v2\n      - run: npm install && npm run test\n      # Build latest version\n      - uses: distributhor/workflow-webhook@v1\n        env:\n          webhook_url: ${{ secrets.LATEST_BUILD_URL }}\n          webhook_secret: ${{ secrets.APIFY_TOKEN }}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Price with JavaScript\nDESCRIPTION: Demonstrates extracting a product's price by selecting the price element and accessing its text content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/03_devtools_extracting_data.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nprice = subwoofer.querySelector('.price');\nprice.textContent;\n```\n\n----------------------------------------\n\nTITLE: Options Configuration with Group Caption in JSON5\nDESCRIPTION: Example showing how to configure boolean options with group captions and descriptions. Demonstrates verbose logging and lightspeed options with prefill values.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_6\n\nLANGUAGE: json5\nCODE:\n```\n{\n    \"verboseLog\": {\n        \"title\": \"Verbose log\",\n        \"type\": \"boolean\",\n        \"description\": \"Debug messages will be included in the log.\",\n        \"default\": true,\n        \"groupCaption\": \"Options\",\n        \"groupDescription\": \"Various options for this Actor\"\n    },\n    \"lightspeed\": {\n        \"title\": \"Lightspeed\",\n        \"type\": \"boolean\",\n        \"description\": \"If checked then actors runs at the\\n            speed of light.\",\n        \"prefill\": true\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Packages for CrewAI-Apify Integration\nDESCRIPTION: Python import statements for required modules and packages\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom crewai import Agent, Task, Crew\nfrom crewai_tools import ApifyActorsTool\nfrom langchain_openai import ChatOpenAI\n```\n\n----------------------------------------\n\nTITLE: Function with Parameter Type Annotations\nDESCRIPTION: Demonstrates how to add type annotations to function parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst totalLengthIsGreaterThan10 = (string1: string, string2: string) => {\n    return (string1 + string2).length > 10;\n};\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Function to Save Puppeteer Screenshots to Apify KV Store\nDESCRIPTION: A utility function that takes a Puppeteer page instance and saves a full-page screenshot to the Apify key-value store under a specified key. The function captures the entire page and stores it as a PNG image.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/how_to_save_screenshots_puppeteer.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n/**\n* Store screen from puppeteer page to Apify key-value store\n* @param page - Instance of puppeteer Page class https://pptr.dev/api/puppeteer.page\n* @param [key] - Function stores your screen in Apify key-value store under this key\n* @return {Promise<void>}\n*/\nconst saveScreen = async (page, key = 'debug-screen') => {\n    const screenshotBuffer = await page.screenshot({ fullPage: true });\n    await Apify.setValue(key, screenshotBuffer, { contentType: 'image/png' });\n};\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Beta Version Workflow\nDESCRIPTION: GitHub Actions workflow configuration for testing and building the beta version of an Actor. Triggers on pushes to the develop branch and includes steps for running tests and building the beta version.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/deployment/continuous_integration.md#2025-04-18_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: Test and build beta version\non:\n  push:\n    branches:\n      - develop\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      # Install dependencies and run tests\n      - uses: actions/checkout@v2\n      - run: npm install && npm run test\n      # Build latest version\n      - uses: distributhor/workflow-webhook@v1\n        env:\n          webhook_url: ${{ secrets.BETA_BUILD_URL }}\n          webhook_secret: ${{ secrets.APIFY_TOKEN }}\n```\n\n----------------------------------------\n\nTITLE: Installing Web Scraping Dependencies\nDESCRIPTION: Installs got-scraping for HTTP requests and cheerio for HTML parsing using npm.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/project_setup.md#2025-04-18_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install got-scraping cheerio\n```\n\n----------------------------------------\n\nTITLE: Using Idempotency Keys with Webhooks in Python\nDESCRIPTION: Example showing how to prevent duplicate webhooks by using an idempotency key parameter in Python. It uses the Actor run ID environment variable as the unique key.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        await Actor.add_webhook(\n            event_types=['ACTOR.RUN.FAILED'],\n            request_url='https://example.com/run-failed',\n            idempotency_key=os.environ['APIFY_ACTOR_RUN_ID'],\n        )\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Complete Product Scraping Script\nDESCRIPTION: Full implementation of product data extraction including title and price handling using Beautiful Soup and HTTPX.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text\n\n    price_text = product.select_one(\".price\").contents[-1]\n    if price_text.startswith(\"From \"):\n        min_price = price_text.removeprefix(\"From \")\n        price = None\n    else:\n        min_price = price_text\n        price = min_price\n\n    print(title, min_price, price, sep=\" | \")\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Record Retrieval Endpoint\nDESCRIPTION: HTTP GET endpoint for retrieving a record from a key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_6\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.apify.com/v2/key-value-stores/:storeId/records/:recordKey\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Dataset in JavaScript\nDESCRIPTION: Example array showing how offset pagination works with a simple dataset\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/handling_pagination.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst myAwesomeDataset = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15];\n```\n\n----------------------------------------\n\nTITLE: Creating a Schedule via API Request with JSON Payload\nDESCRIPTION: This JSON payload example demonstrates how to create a monthly SEO audit schedule for the Apify domain. The request includes schedule configuration settings like cronExpression, timezone, and defines an action to run a specific Actor task with custom input parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/schedules.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"apify-domain-monthly-seo-audit\",\n    \"userId\": \"7AxwNO4kCDZxsMHip\",\n    \"isEnabled\": true,\n    \"isExclusive\": true,\n    \"cronExpression\": \"@monthly\",\n    \"timezone\": \"UTC\",\n    \"description\": \"A monthly audit of the Apify domain's SEO\",\n    \"actions\": [\n        {\n            \"type\": \"RUN_ACTOR_TASK\",\n            \"actorTaskId\": \"6rHoK2zjYJkmYhSug\",\n            \"input\": {\n                \"startUrl\": \"https://apify.com\"\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for Weather Data Processing Actor\nDESCRIPTION: Specifies the required third-party packages (matplotlib and pandas) in the requirements.txt file for the Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Add your dependencies here.\n# See https://pip.pypa.io/en/latest/cli/pip_install/#requirements-file-format\n# for how to format them\n\nmatplotlib\npandas\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Try-Catch in JavaScript\nDESCRIPTION: Shows how to use a try-catch block to handle errors in web scraping, providing a custom error message for better debugging.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/analyzing_pages_and_fixing_errors.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ntry {\n    // Sensitive code block\n    // ...\n} catch (error) {\n    // You know where the code crashed so you can explain here\n    throw new Error('Request failed during login with an error', { cause: error });\n}\n```\n\n----------------------------------------\n\nTITLE: Pay-Per-Event Cost Structure Example\nDESCRIPTION: Example of how pay-per-event costs are structured for different Actor operations including memory usage, page scraping, and proxy usage.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/store_basics/how_actor_monetization_works.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nActor start per 1 GB of memory at $0.005\nPages scraped at $0.002\nPage opened with residential proxy at $0.002\nPage opened with a browser at $0.002\n```\n\n----------------------------------------\n\nTITLE: API Authentication Header\nDESCRIPTION: Example of how to authenticate API requests using an API token in the Authorization header.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nAuthorization: Bearer YOUR_API_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Downloading Product Listing HTML\nDESCRIPTION: Python script using HTTPX to download HTML content from a product listing page\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nprint(response.text)\n```\n\n----------------------------------------\n\nTITLE: Selecting Guardian Article Elements with CSS Selector\nDESCRIPTION: Uses descendant combinator to select all article list items within the main content area of Guardian's F1 page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('main li')\n```\n\n----------------------------------------\n\nTITLE: Basic Apify Proxy Connection String Format\nDESCRIPTION: The standard format for connecting to Apify Proxy using HTTP proxy protocol.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/usage.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttp://<username>:<password>@<hostname>:<port>\n```\n\n----------------------------------------\n\nTITLE: Excluding Files and Folders in TypeScript Configuration\nDESCRIPTION: This snippet demonstrates how to exclude specific files or folders from TypeScript compilation using the 'exclude' option in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {},\n    \"exclude\": [\"node_modules\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CrewAI Tasks\nDESCRIPTION: Setting up tasks for the agents to search for and analyze TikTok profiles\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsearch_task = Task(\n    description=\"Search the web for the OpenAI TikTok profile URL.\",\n    agent=search_agent,\n    expected_output=\"A URL linking to the OpenAI TikTok profile.\"\n)\n\nanalysis_task = Task(\n    description=\"Extract data from the OpenAI TikTok profile URL and provide a profile summary and details about the latest post.\",\n    agent=analysis_agent,\n    context=[search_task],\n    expected_output=\"A summary of the OpenAI TikTok profile including followers and likes, plus details about their most recent post.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Incorrect COPY Instruction Without Ownership Specification\nDESCRIPTION: Example of a problematic Dockerfile COPY instruction that doesn't specify file ownership, leading to permission errors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/avoid_eacces_error_in_actor_builds.md#2025-04-18_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nCOPY . ./\n```\n\n----------------------------------------\n\nTITLE: Storing Scraped ASINs and Offer Counts in JSON\nDESCRIPTION: This JSON structure demonstrates the format for storing scraped Amazon Standard Identification Numbers (ASINs) as keys and the number of offers scraped for each ASIN as values. This object needs to be persisted to handle Actor migrations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/migrations_maintaining_state.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"B079ZJ1BPR\": 3,\n    \"B07D4R4258\": 21\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Fix Attempt with Type Checking\nDESCRIPTION: An attempt to fix the logical error in JavaScript by adding type checking within the function. This solution works but doesn't prevent incorrect types from being passed initially.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst products = [\n    {\n        title: 'iPhone',\n        price: '1000',\n    },\n    {\n        title: 'iPad',\n        price: '1099',\n    },\n];\n\nconst addPrices = (price1, price2) => {\n    // If they are numbers just add them together\n    if (typeof price1 === 'number' && typeof price2 === 'number') {\n        return price1 + price2;\n    }\n\n    // Otherwise, convert them to numbers and add them together\n    return +price1 + +price2;\n};\n\nconsole.log(addPrices(products[0].price, products[1].price));\n```\n\n----------------------------------------\n\nTITLE: Actor Build Event Data Format in JSON\nDESCRIPTION: The JSON structure provided in event data for Actor build events. Contains identifiers for the Actor and the specific build that triggered the event.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/events.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorId\": \"ID of the triggering Actor.\",\n    \"actorBuildId\": \"ID of the triggering Actor build.\",\n}\n```\n\n----------------------------------------\n\nTITLE: Querying DOM for All Product Cards with JavaScript\nDESCRIPTION: JavaScript code using document.querySelectorAll() to select all product card elements on the page based on their CSS class.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.product-item');\n```\n\n----------------------------------------\n\nTITLE: Key-Value Store Record Storage Endpoint\nDESCRIPTION: HTTP PUT endpoint for storing data in a key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_10\n\nLANGUAGE: http\nCODE:\n```\nPUT https://api.apify.com/v2/key-value-stores/:storeId/records/:recordKey\n```\n\n----------------------------------------\n\nTITLE: Opening Dataset in Actor\nDESCRIPTION: Code to access a dataset using its ID passed as input to the Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { datasetId } = await Actor.getInput();\nconst dataset = await Actor.openDataset(datasetId);\n// ...\n```\n\n----------------------------------------\n\nTITLE: Calling Website Content Crawler Actor in Python\nDESCRIPTION: Python code to call the Website Content Crawler Actor to crawl Milvus documentation and Zilliz website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/milvus.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nactor_call = client.actor(\"apify/website-content-crawler\").call(\n    run_input={\"maxCrawlPages\": 10, \"startUrls\": [{\"url\": \"https://milvus.io/\"}, {\"url\": \"https://zilliz.com/\"}]}\n)\n```\n\n----------------------------------------\n\nTITLE: Reading Encrypted Secret Input Directly (JavaScript)\nDESCRIPTION: This snippet shows how to read the INPUT key from the Actor run's default key-value store directly. The secret field remains encrypted when accessed this way.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/secret_input.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n> await Actor.getValue('INPUT');\n{\n    username: 'username',\n    password: 'ENCRYPTED_VALUE:Hw/uqRMRNHmxXYYDJCyaQX6xcwUnVYQnH4fWIlKZL2Vhtq1rZmtoGXQSnhIXmF58+DjKlMZpTlK2zN3YUXk1ylzU6LfXyysOG/PISAfwm27FUgy3IfdgMyQggQ4MydLzdlzefX0mPRyixBviRcFhRTC+K7nK9lkATt3wJpj91YAZm104ZYkcd5KmsU2JX39vxN0A0lX53NjIenzs3wYPaPYLdjKIe+nqG9fHlL7kALyi7Htpy91ZgnQJ1s9saJRkKfWXvmLYIo5db69zU9dGCeJzUc0ca154O+KYYP7QTebJxqZNQsC8EH6sVMQU3W0qYKjuN8fUm1fRzyw/kKFacQ==:VfQd2ZbUt3S0RZ2ciywEWYVBbTTZOTiy'\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Logic for Filter Pages in JavaScript\nDESCRIPTION: This snippet shows how to implement the request handler logic for filter pages, including pagination and splitting filters when necessary.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-with-search.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\n// Doesn't matter what Crawler class we choose\nconst crawler = new CheerioCrawler({\n    // Crawler options here\n    // ...\n    async requestHandler({ request, $ }) {\n        const { label } = request;\n        if (label === 'FILTER') {\n            // Of course, change the selectors and make it more robust\n            const numberOfProducts = Number($('.product-count').text());\n\n            // The filter is either good enough of we have to split it\n            if (numberOfProducts <= MAX_PRODUCTS_PAGINATION) {\n                // We pass the URL for scraping, we could optimize it so the page is not opened again\n                await crawler.addRequests([{\n                    url: `${request.url}&page=1`,\n                    userData: { label: 'PAGINATION' },\n                }]);\n            } else {\n                // Here we have to split the filter\n                // To be continued...\n            }\n        }\n        if (label === 'PAGINATION') {\n            // We know we are under the limit here\n            // Enqueue next page as long as possible\n            // Enqueue or scrape products normally\n        }\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: REST API Request Examples\nDESCRIPTION: Examples of typical REST API endpoint requests showing different HTTP methods and URL patterns.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/index.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nGET https://api.example.com/users/123\nGET https://api.example.com/comments/abc123?limit=100\nPOST https://api.example.com/orders\n```\n\n----------------------------------------\n\nTITLE: CSV Parsing Operation\nDESCRIPTION: Basic operation to convert results array to CSV format using json2csv parse function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/save_to_csv.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst csv = parse(results);\n```\n\n----------------------------------------\n\nTITLE: Running an Apify Actor\nDESCRIPTION: Demonstrates how to run the Contact Details Scraper actor with custom input parameters and timeout settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_2\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->post('acts/vdrmota~contact-info-scraper/runs', [\n  'json' => [\n    'startUrls' => [\n        ['url' => 'https://www.apify.com/contact']\n    ],\n    'maxDepth' => 0,\n  ],\n  'query' => [ 'timeout' => 30 ],\n]);\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Selecting Element by CSS Selector\nDESCRIPTION: JavaScript code showing how to select a DOM element using querySelector() method and CSS selector.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/html_elements.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst myElement = document.querySelector('#myId');\n```\n\n----------------------------------------\n\nTITLE: Build Actor API Endpoint URL Format\nDESCRIPTION: cURL format for the Builds Actor API endpoint URL that needs to be added to GitHub secrets. It includes placeholders for actor name and token with configuration parameters.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/deployment/continuous_integration.md#2025-04-18_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\nhttps://api.apify.com/v2/acts/YOUR-ACTOR-NAME/builds?token=YOUR-TOKEN-HERE&version=0.0&tag=beta&waitForFinish=60\n```\n\n----------------------------------------\n\nTITLE: Basic Crawlee Setup\nDESCRIPTION: Initial setup code to verify Crawlee installation and import CheerioCrawler\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconsole.log('Crawlee works!');\n```\n\n----------------------------------------\n\nTITLE: Configuring Apify Actor input for RAG Web Browser\nDESCRIPTION: JSON input configuration for the RAG Web Browser Apify Actor to search Google with a specific query and limit results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langflow.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"query\": \"what is monero?\", \"maxResults\": 3}\n```\n\n----------------------------------------\n\nTITLE: XML Output from JavaScript Object Conversion\nDESCRIPTION: Shows the XML output resulting from the conversion of the JavaScript object, illustrating how object properties become XML tags and their values become children of these tags.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_17\n\nLANGUAGE: xml\nCODE:\n```\n<name>Rashida Jones</name>\n<address>\n    <type>home</type>\n    <street>21st</street>\n    <city>Chicago</city>\n</address>\n<address>\n    <type>office</type>\n    <street/>\n    <city/>\n</address>\n```\n\n----------------------------------------\n\nTITLE: Installing json2csv Package\nDESCRIPTION: Command to install the json2csv NPM package for converting JSON data to CSV format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/save_to_csv.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i json2csv\n```\n\n----------------------------------------\n\nTITLE: Sharing Request Queues Between Runs in JavaScript\nDESCRIPTION: This code shows how to access a request queue from another Actor or task run in JavaScript. You can open a request queue using its name or ID and then use it like any other request queue.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_13\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst otherQueue = await Actor.openRequestQueue('old-queue');\n// ...\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: HTML Structure of a Product Card\nDESCRIPTION: The HTML markup for a product card element on the e-commerce website. It demonstrates the use of multiple CSS classes for styling and layout purposes.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"product-item product-item--vertical 1/3--tablet-and-up 1/4--desk\">\n  ...\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Resource Input for Apify Actor in JSON\nDESCRIPTION: This snippet demonstrates how to configure a dataset resource input for an Apify Actor. It uses the 'resourceType' property to specify a dataset input.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Dataset\",\n    \"type\": \"string\",\n    \"description\": \"Select a dataset\",\n    \"resourceType\": \"dataset\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using TypeScript Watch Mode Command\nDESCRIPTION: Command to enable watch mode for automatic recompilation of TypeScript files when changes are detected. Uses the --watch or -w flag with the TypeScript compiler.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntsc FILE_NAME --watch\n```\n\n----------------------------------------\n\nTITLE: Defining Blocked File Extensions in JavaScript\nDESCRIPTION: Creates an array of file extensions to be blocked when making requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n```\n\n----------------------------------------\n\nTITLE: Complete Dockerfile for Node.js Apify Actor\nDESCRIPTION: A complete Dockerfile for a Node.js Actor. It uses the Apify Node.js base image, installs production npm dependencies, and copies the source code. It avoids copying unnecessary files to keep the build fast and the image small.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n\n# Second, copy just package.json and package-lock.json since they are the only files\n# that affect npm install in the next step\nCOPY package*.json ./\n\n# Install npm packages, skip optional and development dependencies to keep the\n# image small. Avoid logging too much and print the dependency tree for debugging\nRUN npm --quiet set progress=false \\\n && npm install --only=prod --no-optional \\\n && echo \"Installed npm packages:\" \\\n && (npm list --all || true) \\\n && echo \"Node.js version:\" \\\n && node --version \\\n && echo \"npm version:\" \\\n && npm --version\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after npm install, quick build will be really fast\n# for simple source file changes.\nCOPY . ./\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Fields from Dataset in Python\nDESCRIPTION: Shows how to use the 'fields' option in the get_data() method to retrieve specific data fields from a dataset in Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        dataset = await Actor.open_dataset()\n\n        # Only get the 'hotel' and 'cafe' fields\n        hotel_and_cafe_data = await dataset.get_data(fields=['hotel', 'cafe'])\n```\n\n----------------------------------------\n\nTITLE: Downloading a text file to memory using request-promise\nDESCRIPTION: Use request-promise to download a text file and store its content in memory.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_form_with_file_attachment.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst fileData = await request('https://example.com/file.txt');\n```\n\n----------------------------------------\n\nTITLE: Scraping Title and Description with Cheerio\nDESCRIPTION: Extends the previous function to also extract the actor description from a span element with class actor-description.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { $ } = context;\n    // ... rest of your code can come here\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching the Sales Page URL in JavaScript\nDESCRIPTION: This code snippet demonstrates how to construct the sales page URL and fetch its HTML content using got-scraping.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for LangGraph and Apify Integration\nDESCRIPTION: Installs the necessary Python packages to work with LangGraph and Apify, including langgraph, langchain-apify, and langchain-openai.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install langgraph langchain-apify langchain-openai\n```\n\n----------------------------------------\n\nTITLE: Modifying Request URL with Playwright\nDESCRIPTION: Demonstrates how to intercept and modify a request URL in Playwright, changing the destination from one SoundCloud profile to another.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Only listen for requests matching this regular expression\npage.route(/soundcloud.com\\/tiesto/, async (route) => {\n    // Continue  the route, but replace \"tiesto\" in the URL with \"mestomusic\"\n    return route.continue({ url: route.request().url().replace('tiesto', 'mestomusic') });\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: XML Output from JavaScript Object with Special Properties\nDESCRIPTION: Demonstrates the XML output resulting from the conversion of a JavaScript object using '@' and '#' properties, showing how they translate to XML attributes and element values.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_19\n\nLANGUAGE: xml\nCODE:\n```\n<address type=\"home\">\n    <street>21st</street>\n    <city>Chicago</city>\n</address>\n<address type=\"office\">unknown</address>\n```\n\n----------------------------------------\n\nTITLE: Creating and Running the CrewAI Crew\nDESCRIPTION: Assembling and executing the crew with defined agents and tasks\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncrew = Crew(\n    agents=[search_agent, analysis_agent],\n    tasks=[search_task, analysis_task],\n    process=\"sequential\"\n)\n\nresult = crew.kickoff()\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for GraphQL Scraping\nDESCRIPTION: Sets up a new Node.js project and installs necessary dependencies for GraphQL scraping.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm init -y && npm install graphql-tag puppeteer got-scraping\n```\n\n----------------------------------------\n\nTITLE: Accessing Request Queue via API Endpoint\nDESCRIPTION: Examples of API endpoints used to interact with request queues. Shows URL structure for basic operations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/request-queues\\nhttps://api.apify.com/v2/request-queues/{QUEUE_ID}\\nhttps://api.apify.com/v2/request-queues/{QUEUE_ID}/requests/{REQUEST_ID}\\nhttps://api.apify.com/v2/request-queues/{QUEUE_ID}/requests\n```\n\n----------------------------------------\n\nTITLE: Actor Input Schema Configuration\nDESCRIPTION: JSON schema defining the expected input format for the filter Actor, requiring a datasetId parameter.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Amazon Filter Actor\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"datasetId\": {\n            \"title\": \"Dataset ID\",\n            \"type\": \"string\",\n            \"description\": \"Enter the ID of the dataset.\",\n            \"editor\": \"textfield\"\n        }\n    },\n    \"required\": [\"datasetId\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring User Object Input for Apify Actor in JSON\nDESCRIPTION: This snippet demonstrates how to configure a user object input for an Apify Actor. It uses the 'json' editor and includes prefill values for name and email.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"User object\",\n    \"type\": \"object\",\n    \"description\": \"Enter object representing user\",\n    \"prefill\": {\n        \"name\": \"John Doe\",\n        \"email\": \"janedoe@gmail.com\"\n    },\n    \"editor\": \"json\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Crawlee Project with CLI\nDESCRIPTION: Command to initialize a new Crawlee project named amazon-crawler using the Crawlee CLI tool.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/initializing_and_setting_up.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpx crawlee create amazon-crawler\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Actor Run Endpoint\nDESCRIPTION: HTTP POST endpoint for running an actor asynchronously, ideal for longer-running operations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPOST https://api.apify.com/v2/acts/:actorId/runs\n```\n\n----------------------------------------\n\nTITLE: Basic Dockerfile FROM Statement for Node.js Apify Actor\nDESCRIPTION: Simple example of using the Apify Node.js base image in a Dockerfile. This specifies that the Docker image should be based on Apify's Node.js 16 image, which contains the Node.js runtime and other dependencies needed for Actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM apify/actor-node:16\n```\n\n----------------------------------------\n\nTITLE: Initializing Git Repository and Pushing to GitHub\nDESCRIPTION: Commands to initialize a local Git repository and push files to a remote GitHub repository. These steps are typically performed after creating a new repository on GitHub's website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/managing_source_code.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit init\ngit add .\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/username/repo-name.git\ngit push -u origin main\n```\n\n----------------------------------------\n\nTITLE: Short Form TypeScript Watch Mode Command\nDESCRIPTION: Alternative shorter command using -w flag to enable watch mode for TypeScript compilation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntsc FILE_NAME -w\n```\n\n----------------------------------------\n\nTITLE: API Endpoint Call for Running Actors\nDESCRIPTION: Example of the HTTP endpoint URL used to run an Apify Actor via API. Requires authentication via API token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/index.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/acts/compass~crawler-google-places/runs?token=<YOUR_API_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Filtering Amazon Product Data\nDESCRIPTION: Implementation of filtering logic using Array.reduce() to process Amazon product data and find lowest prices per ASIN.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst filtered = items.reduce((acc, curr) => {\n    const prevPrice = acc?.[curr.asin] ? +acc[curr.asin].offer.slice(1) : null;\n    const price = +curr.offer.slice(1);\n\n    if (!acc[curr.asin] || prevPrice > price) acc[curr.asin] = curr;\n\n    return acc;\n}, {});\n```\n\n----------------------------------------\n\nTITLE: Configuring Input JSON\nDESCRIPTION: Input configuration file that specifies the search keyword for the Amazon product crawler.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/initializing_and_setting_up.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"keyword\": \"iphone\"\n}\n```\n\n----------------------------------------\n\nTITLE: Sample URL Formats\nDESCRIPTION: Examples showing the difference between absolute and relative URL formats on a web page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/relative_urls.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones\n```\n\nLANGUAGE: text\nCODE:\n```\n/products/denon-ah-c720-in-ear-headphones\n```\n\n----------------------------------------\n\nTITLE: Installing Apify Python SDK\nDESCRIPTION: Command to install the Apify Python SDK using pip, which is required for programmatic interaction with Apify services.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/qdrant.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install apify-client\n```\n\n----------------------------------------\n\nTITLE: Initializing Guzzle Client for Apify API\nDESCRIPTION: Sets up a guzzle HTTP client instance configured with Apify API base URL and authentication token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_0\n\nLANGUAGE: php\nCODE:\n```\nrequire 'vendor/autoload.php';\n\n$client = new \\GuzzleHttp\\Client([\n    'base_uri' => 'https://api.apify.com/v2/',\n    'headers' => [\n        // Replace <YOUR_APIFY_API_TOKEN> with your actual token\n        'Authorization' => 'Bearer <YOUR_APIFY_API_TOKEN>',\n    ]\n]);\n```\n\n----------------------------------------\n\nTITLE: JSON to CSV Import Statement\nDESCRIPTION: Import statement to get the parse function from json2csv library.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/save_to_csv.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { parse } from 'json2csv';\n```\n\n----------------------------------------\n\nTITLE: Ad-hoc Webhook Configuration in JavaScript\nDESCRIPTION: Example of a webhook configuration array that should be base64 encoded and passed to the API. It shows two webhook definitions - one for run failures and one for successful runs with a custom payload template.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n[\n    {\n        eventTypes: ['ACTOR.RUN.FAILED'],\n        requestUrl: 'https://example.com/run-failed',\n    },\n    {\n        eventTypes: ['ACTOR.RUN.SUCCEEDED'],\n        requestUrl: 'https://example.com/run-succeeded',\n        payloadTemplate: '{\"hello\": \"world\", \"resource\":{{resource}}}',\n    },\n];\n```\n\n----------------------------------------\n\nTITLE: Selecting Wikipedia Main Page Headings with CSS Selector\nDESCRIPTION: Uses querySelector to find all h2 elements with mp-h2 class on Wikipedia's main page, representing section headings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('h2')\n```\n\n----------------------------------------\n\nTITLE: Locating Local Dataset Storage in Apify\nDESCRIPTION: Specifies the file path for locally stored datasets in Apify. The path includes the local storage directory, dataset ID or name, and the JSON file index.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n{APIFY_LOCAL_STORAGE_DIR}/datasets/{DATASET_ID}/{INDEX}.json\n```\n\n----------------------------------------\n\nTITLE: Enabling Request Interception in Puppeteer\nDESCRIPTION: This snippet shows how to enable request interception in Puppeteer, which is necessary for intercepting file download requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.setRequestInterception(true);\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI on Other Platforms\nDESCRIPTION: Command to install the Apify CLI tool on platforms other than macOS/Linux using npm.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_web_ide.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -g install apify-cli\n```\n\n----------------------------------------\n\nTITLE: Logging into Apify CLI with Personal API Token\nDESCRIPTION: This command logs you into the Apify CLI using your personal API token, which can be found in the Integrations tab of your Apify account settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/tools/apify_cli.md#2025-04-18_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\napify login -t YOUR_TOKEN_HERE\n```\n\n----------------------------------------\n\nTITLE: Accessing Specific Product Card with JavaScript\nDESCRIPTION: JavaScript code that selects all product cards, then accesses a specific card (the subwoofer) using array indexing and assigns it to a variable.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nproducts = document.querySelectorAll('.product-item');\nsubwoofer = products[2];\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler with Infinite Scroll\nDESCRIPTION: Final implementation using PuppeteerCrawler with infinite scroll to handle lazy-loaded images and extract all product data correctly.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PuppeteerCrawler, Dataset, utils } from 'crawlee';\n\nconst BASE_URL = 'https://demo-webstore.apify.org';\n\nconst crawler = new PuppeteerCrawler({\n    requestHandler: async ({ page, parseWithCheerio, request }) => {\n        // Scroll to the bottom of the page to load all products\n        await utils.puppeteer.infiniteScroll(page, { timeoutSecs: 10 });\n\n        // Now that everything is loaded, we can grab the HTML content\n        const $ = await parseWithCheerio();\n\n        const products = $('a[href*=\"/product/\"]');\n\n        const results = [...products].map((product) => {\n            const elem = $(product);\n\n            const title = elem.find('h3').text();\n            const price = elem.find('div[class*=\"price\"]').text();\n            const image = elem.find('img[src]').attr('src');\n\n            return {\n                title,\n                price,\n                image: new URL(image, BASE_URL).href,\n            };\n        });\n\n        // Save the results to the dataset\n        await Dataset.pushData(results);\n    },\n});\n\nawait crawler.run([{ url: 'https://demo-webstore.apify.org/search/new-arrivals' }]);\n```\n\n----------------------------------------\n\nTITLE: Initializing an Actor Configuration File\nDESCRIPTION: The Apify CLI command that generates an .actor/actor.json file at the root of your project. This configuration file is required before you can push your code to the Apify platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/deploying.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify init .\n```\n\n----------------------------------------\n\nTITLE: Running Actor Locally\nDESCRIPTION: Command to execute the Actor locally for testing and debugging purposes.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify run\n```\n\n----------------------------------------\n\nTITLE: Selecting Page Navigation Elements in JavaScript\nDESCRIPTION: CSS selector for targeting the 'next' pagination button in form submit navigation scenarios, specifically used for Maxwell Materials website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n'li.page-item.next a';\n```\n\n----------------------------------------\n\nTITLE: Index Route Handler Implementation\nDESCRIPTION: Defines the root route handler that displays a form for URL submission and shows thumbnails of processed screenshots.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\napp.get('/', (req, res) => {\n    let listItems = '';\n\n    // For each of the processed\n    processedUrls.forEach((url, index) => {\n        const imageUrl = `https://api.apify.com/v2/key-value-stores/${APIFY_DEFAULT_KEY_VALUE_STORE_ID}/records/${index}.jpg`;\n\n        // Display the screenshots below the form\n        listItems += `<li>\n    <a href=\"${imageUrl}\" target=\"_blank\">\n        <img src=\"${imageUrl}\" width=\"300px\" />\n        <br />\n        ${url}\n    </a>\n</li>`;\n    });\n\n    const pageHtml = `<html>\n    <head><title>Example</title></head>\n    <body>\n        <form method=\"POST\" action=\"/add-url\">\n            URL: <input type=\"text\" name=\"url\" placeholder=\"http://example.com\" />\n            <input type=\"submit\" value=\"Add\" />\n            <hr />\n            <ul>${listItems}</ul>\n        </form>\n    </body>\n</html>`;\n\n    res.send(pageHtml);\n});\n```\n\n----------------------------------------\n\nTITLE: Filling Google Search Form with Keywords in Web Scraper\nDESCRIPTION: This code retrieves the keyword from the userData object and inputs it into the Google search form. It demonstrates how to handle form submissions in a Web Scraper page function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/processing_multiple_pages_web_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const $ = context.jQuery;\n\n    if (context.request.userData.label === 'enqueue') {\n        // copy from the previous part\n    } else if (context.request.userData.label === 'fill-form') {\n        // retrieve the keyword\n        const { keyword } = context.request.userData;\n\n        // input the keyword into the search bar\n        $('#lst-ib').val(keyword);\n\n        // submit the form\n        $('#tsf').submit();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Injecting jQuery into Browser Page with Puppeteer Scraper in JavaScript\nDESCRIPTION: Shows how to inject jQuery into the browser page before it loads using Apify SDK's preGotoFunction. This allows using jQuery in subsequent page.evaluate() calls.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function preGotoFunction({ request, page, Apify }) {\n    await Apify.utils.puppeteer.injectJQuery(page);\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst bodyText = await context.page.evaluate(() => {\n    return $('body').text();\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Card Grid Layout in Markdown\nDESCRIPTION: This code snippet uses the imported CardGrid and Card components to create a structured layout presenting information about running Actors, development, and publishing/monetization. Each Card component contains a title, description, and link.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/index.mdx#2025-04-18_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<CardGrid>\n    <Card\n        title=\"Running Actors\"\n        desc=\"In this section, you learn how to run Apify Actors. You will learn about their configuration, versioning, data retention, usage, and pricing.\"\n        to=\"/platform/actors/running\"\n    />\n    <Card\n        title=\"Development\"\n        desc=\"Read about the technical part of building Apify Actors. Learn to define Actor inputs, build new versions, persist Actor state, and choose base Docker images.\"\n        to=\"/platform/actors/development\"\n    />\n    <Card\n        title=\"Publishing and monetization\"\n        desc=\"Learn about publishing, and monetizing your Actors on the Apify platform.\"\n        to=\"/platform/actors/publishing\"\n    />\n</CardGrid>\n```\n\n----------------------------------------\n\nTITLE: Finding Last Page Number - Playwright Implementation\nDESCRIPTION: Script using Playwright to determine the total number of pages by extracting the last page number from GitHub's pagination.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\nconst browser = await chromium.launch({ headless: false });\nconst firstPage = await browser.newPage();\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageElement = firstPage.locator('a[aria-label*=\"Page \"]:nth-last-child(2)');\nconst lastPageLabel = await lastPageElement.getAttribute('aria-label');\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\nconsole.log(lastPageNumber);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Downloading Key-Value Store Record\nDESCRIPTION: Downloads a specific record (PDF file) from a key-value store and saves it locally.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_9\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('actor-runs/<RUN_ID>/key-value-store/records/OUTPUT');\nfile_put_contents(__DIR__ . '/hello-world.pdf', $response->getBody());\n```\n\n----------------------------------------\n\nTITLE: Extracting Links Using Browser DevTools\nDESCRIPTION: JavaScript code snippet for extracting all link URLs from a webpage using the browser's DevTools console.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/finding_links.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Select all the <a> elements.\nconst links = document.querySelectorAll('a');\n// For each of the links...\nfor (const link of links) {\n    // get the value of its 'href' attribute...\n    const url = link.href;\n    // and print it to console.\n    console.log(url);\n}\n```\n\n----------------------------------------\n\nTITLE: Function with Return Type Annotation\nDESCRIPTION: Shows how to explicitly declare a function's return type using type annotation syntax.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst totalLengthIsGreaterThan10 = (string1: string, string2: string): boolean => {\n    return (string1 + string2).length > 10;\n};\n```\n\n----------------------------------------\n\nTITLE: Fetching Dataset Items\nDESCRIPTION: Retrieves actual data items from an actor's dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_4\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('actor-runs/<RUN_ID>/dataset/items');\n$data = \\json_decode($response->getBody(), true);\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Webhook API Endpoint Example\nDESCRIPTION: The URL format for setting up an ad-hoc webhook through the Apify API. It requires the Actor ID and your API token, plus a base64 encoded webhooks parameter.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/acts/[ACTOR_ID]/runs?token=[YOUR_API_TOKEN]&webhooks=[AD_HOC_WEBHOOKS]\n```\n\n----------------------------------------\n\nTITLE: Starting Flowise Locally with npx\nDESCRIPTION: This command starts Flowise locally using npx. It makes Flowise available on https://localhost:3000 for building LLM flows.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/flowise.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx flowise start\n```\n\n----------------------------------------\n\nTITLE: Example Page Function Return Value in JavaScript\nDESCRIPTION: Example of a JavaScript object returned by the page function. This demonstrates the format of data that will be saved to the dataset, including URL and page title.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_6\n\nLANGUAGE: js\nCODE:\n```\nasync function pageFunction(context) {\n    // ... rest of your code\n    return {\n        url: 'https://apify.com',\n        title: 'Web Scraping, Data Extraction and Automation - Apify',\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Browser-Side Code with Puppeteer\nDESCRIPTION: This snippet demonstrates how to correctly execute browser-side code using page.evaluate() in Puppeteer, changing the background color of a web page.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nawait page.evaluate(() => {\n    document.body.style.background = 'green';\n});\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Adding User Data Label to Pseudo URL in JSON Format\nDESCRIPTION: JSON object that adds metadata to all requests that match the Pseudo URL pattern. The label 'DETAIL' helps identify detail pages in the page function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"label\": \"DETAIL\"\n}\n```\n\n----------------------------------------\n\nTITLE: Time-based Wait Using Puppeteer\nDESCRIPTION: Demonstrates how to pause execution for a specific duration in milliseconds using Puppeteer's page.waitFor method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/waiting_for_dynamic_content.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait page.waitFor(10000)\n```\n\n----------------------------------------\n\nTITLE: URL Storage Initialization\nDESCRIPTION: Creates an array to store processed URLs for tracking screenshots.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst processedUrls = [];\n```\n\n----------------------------------------\n\nTITLE: Configuring robots.txt for web crawlers with sitemap reference\nDESCRIPTION: A robots.txt configuration that allows all user agents to access the site and specifies the location of the XML sitemap. This helps search engines and other crawlers to index the website efficiently.\nSOURCE: https://github.com/apify/apify-docs/blob/master/static/robots.txt#2025-04-18_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\nSitemap: https://docs.apify.com/sitemap.xml\n```\n\n----------------------------------------\n\nTITLE: HTML Whitespace Examples\nDESCRIPTION: Demonstrates how HTML treats whitespace with equivalent formatting examples.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/01_devtools_inspecting.md#2025-04-18_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<strong>\n  The Free Encyclopedia\n</strong>\n```\n\nLANGUAGE: html\nCODE:\n```\n  <strong>The Free\nEncyclopedia\n</strong>\n```\n\n----------------------------------------\n\nTITLE: Extracting Title and Description Using Puppeteer\nDESCRIPTION: Extends the previous example by also extracting the Actor's description. It uses page.$eval to select and extract text from the span element with class actor-description within the header.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nasync function pageFunction(context) {\n    const { page } = context;\n    const title = await page.$eval(\n        'header h1',\n        ((el) => el.textContent),\n    );\n    const description = await page.$eval(\n        'header span.actor-description',\n        ((el) => el.textContent),\n    );\n\n    return {\n        title,\n        description,\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: JavaScript Example with Logical Error\nDESCRIPTION: A JavaScript snippet demonstrating a logical error where string concatenation occurs instead of number addition due to lack of type checking.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst products = [\n    {\n        title: 'iPhone',\n        price: '1000',\n    },\n    {\n        title: 'iPad',\n        price: '1099',\n    },\n];\n\nconst addPrices = (price1, price2) => {\n    return price1 + price2;\n};\n\nconsole.log(addPrices(products[0].price, products[1].price));\n```\n\n----------------------------------------\n\nTITLE: Importing React Components in Markdown\nDESCRIPTION: This code snippet imports custom React components (Card and CardGrid) to be used within the Markdown document. These components are likely used to create a structured layout for presenting information about Actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/index.mdx#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport Card from \"@site/src/components/Card\";\nimport CardGrid from \"@site/src/components/CardGrid\";\n```\n\n----------------------------------------\n\nTITLE: Implementing URL Handling in Main Scraper Loop\nDESCRIPTION: The main scraper code that downloads a webpage, selects product elements, and passes the base URL to the parse_product function to ensure full URLs are created for each product.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlisting_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product, listing_url)\n    data.append(item)\n```\n\n----------------------------------------\n\nTITLE: Accessing Request Queues via API Client in Python\nDESCRIPTION: This code shows how to access a request queue using the Python API client. You can access a request queue by its name or ID, including request queues from other users.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nother_queue_client = apify_client.request_queue('jane-doe/old-queue')\n```\n\n----------------------------------------\n\nTITLE: Modified SoundCloud API Endpoint with Extended Limit\nDESCRIPTION: Modified version of the SoundCloud API endpoint that attempts to retrieve all tracks at once by setting a very high limit parameter value. Shows how to manipulate API parameters for different results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/locating_and_learning.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://api-v2.soundcloud.com/users/141707/tracks?client_id=zdUqm51WRIAByd0lVLntcaWRKzuEIB4X&limit=99999\n```\n\n----------------------------------------\n\nTITLE: Creating New Actor with Apify CLI\nDESCRIPTION: Command to create a new Orchestrator Actor using Apify CLI with the Empty TypeScript Actor template.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\napify create orchestrator-actor\n```\n\n----------------------------------------\n\nTITLE: Generating Actor Status Badge URL in Plain Text\nDESCRIPTION: Demonstrates the URL template for generating an Actor status badge. Replace <USERNAME> and <ACTOR> with the appropriate values for your specific Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/badge.mdx#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://apify.com/actor-badge?actor=<USERNAME>/<ACTOR>\n```\n\n----------------------------------------\n\nTITLE: External Apify Proxy Connection Example\nDESCRIPTION: Example connection string for connecting to Apify Proxy from outside the Apify Platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/usage.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttp://auto:apify_proxy_EaAFg6CFhc4eKk54Q1HbGDEiUTrk480uZv03@proxy.apify.com:8000\n```\n\n----------------------------------------\n\nTITLE: Structured Logging Example for Web Scraping\nDESCRIPTION: Demonstrates a structured log message format for web scraping, including page type, key metrics, and the current URL.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/analyzing_pages_and_fixing_errors.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n[CATEGORY]: Products: 20, Unique products: 4, Next page: true --- https://apify.com/store\n```\n\n----------------------------------------\n\nTITLE: Storing Additional Request Statistics in JSON\nDESCRIPTION: This snippet shows the structure for storing additional statistics about each request in the dataset results. It includes the date handled, number of retries, and current pending requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/saving_useful_stats.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"dateHandled\": \"date-here\", // the date + time at which the request was handled\n    \"numberOfRetries\": 4, // the number of retries of the request before running successfully\n    \"currentPendingRequests\": 24 // the current number of requests left pending in the request queue\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required LangChain and Apify Packages\nDESCRIPTION: Import statements for the necessary Python packages to integrate LangChain with Apify, including vector store, embeddings, and chat components.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_apify import ApifyWrapper\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n```\n\n----------------------------------------\n\nTITLE: Formatted GraphQL Query\nDESCRIPTION: This snippet shows the GraphQL query extracted from the JSON payload and formatted for better readability. It illustrates the structure of the query, including fragments and variables.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/modifying_variables.md#2025-04-18_snippet_1\n\nLANGUAGE: graphql\nCODE:\n```\nquery SearchQuery($query: String!, $count: Int!, $cursor: String) {\n    organization {\n        ...SearchList_organization\n        id\n    }\n    }\n    fragment SearchList_organization on Organization {\n    media(\n        first: $count\n        after: $cursor\n        query: $query\n        recency_weight: 0.6\n        recency_days: 30\n        include_private: false\n        include_unpublished: false\n    ) {\n        hitCount\n        edges {\n        node {\n            _score\n            id\n            ...StandardListCard_video\n            __typename\n        }\n        cursor\n        }\n        pageInfo {\n        endCursor\n        hasNextPage\n        }\n    }\n    }\n    fragment StandardListCard_video on Slugable {\n    ...Thumbnail_video\n    ...StandardTextCard_media\n    slug\n    id\n    __typename\n    }\n    fragment Thumbnail_video on Slugable {\n    original_thumbnails: thumbnails(aspect_ratio: ORIGINAL) {\n        small\n        medium\n        large\n    }\n    sd_thumbnails: thumbnails(aspect_ratio: SD) {\n        small\n        medium\n        large\n    }\n    hd_thumbnails: thumbnails(aspect_ratio: HD) {\n        small\n        medium\n        large\n    }\n    film_thumbnails: thumbnails(aspect_ratio: FILM) {\n        small\n        medium\n        large\n    }\n    square_thumbnails: thumbnails(aspect_ratio: SQUARE) {\n        small\n        medium\n        large\n    }\n    }\n    fragment StandardTextCard_media on Slugable {\n    public_at\n    updated_at\n    title\n    hero_video {\n        duration\n    }\n    description\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Page Navigation in Web Scraper vs Puppeteer Scraper in JavaScript\nDESCRIPTION: Compares navigation handling in Web Scraper (which throws an error) and Puppeteer Scraper (which handles it gracefully). Demonstrates waiting for navigation after clicking a button.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait context.waitFor('button');\n$('button').click();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait context.page.waitFor('button');\nawait Promise.all([\n    context.page.waitForNavigation(),\n    context.page.click('button'),\n]);\n```\n\nLANGUAGE: javascript\nCODE:\n```\nawait context.page.goto('https://some-new-page.com');\n```\n\n----------------------------------------\n\nTITLE: Run Log Retrieval Endpoint\nDESCRIPTION: HTTP GET endpoint for retrieving logs from a specific actor run or build.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_7\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.apify.com/v2/logs/:buildOrRunId\n```\n\n----------------------------------------\n\nTITLE: Pulling Existing Actor from Apify Platform\nDESCRIPTION: Commands to pull an existing Actor from Apify platform to local machine. Supports optional version specification.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/deployment/index.md#2025-04-18_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify pull [ACTORID]\n```\n\nLANGUAGE: bash\nCODE:\n```\napify pull [ACTORID] --version=1.2\n```\n\n----------------------------------------\n\nTITLE: CSS Selector for Show More Button\nDESCRIPTION: CSS selector example for targeting a show more button in pagination implementation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ndiv.show-more > button\n```\n\n----------------------------------------\n\nTITLE: Determining Actor Standby Mode in JavaScript\nDESCRIPTION: This snippet demonstrates how to determine if an Actor was started in Standby mode or standard mode using JavaScript. It checks the 'metaOrigin' configuration option to make this determination.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/actor_standby.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nif (Actor.config.get('metaOrigin') === 'STANDBY') {\n    // Start your Standby server here\n} else {\n    // Perform the standard Actor operations here\n}\n```\n\n----------------------------------------\n\nTITLE: Pulling a Specific Version of an Actor\nDESCRIPTION: Command to pull a specific version of an Actor's source code from the Apify platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_web_ide.md#2025-04-18_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napify pull your-actor-name --version [version_number]\n```\n\n----------------------------------------\n\nTITLE: Handling Product Description Extraction in Crawlee\nDESCRIPTION: This snippet demonstrates how to extract product descriptions from Amazon using Crawlee's router handler. It shows the basic structure for processing product pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nrouter.addHandler(labels.PRODUCT, async ({ $ }) => {\n    const element = $('div#productDescription');\n\n    const description = element.text().trim();\n\n    console.log(description); // works!\n});\n```\n\n----------------------------------------\n\nTITLE: Examining JSON Payload for GraphQL Query\nDESCRIPTION: This snippet shows the full JSON payload for a GraphQL query, including the query string and variables. It demonstrates the structure of a typical GraphQL request.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/modifying_variables.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"query\": \"query SearchQuery($query: String!, $count: Int!, $cursor: String) {\\n    organization {\\n        ...SearchList_organization\\n        id\\n    }\\n    }\\n    fragment SearchList_organization on Organization {\\n    media(\\n        first: $count\\n        after: $cursor\\n        query: $query\\n        recency_weight: 0.6\\n        recency_days: 30\\n        include_private: false\\n        include_unpublished: false\\n    ) {\\n        hitCount\\n        edges {\\n        node {\\n            _score\\n            id\\n            ...StandardListCard_video\\n            __typename\\n        }\\n        cursor\\n        }\\n        pageInfo {\\n        endCursor\\n        hasNextPage\\n        }\\n    }\\n    }\\n    fragment StandardListCard_video on Slugable {\\n    ...Thumbnail_video\\n    ...StandardTextCard_media\\n    slug\\n    id\\n    __typename\\n    }\\n    fragment Thumbnail_video on Slugable {\\n    original_thumbnails: thumbnails(aspect_ratio: ORIGINAL) {\\n        small\\n        medium\\n        large\\n    }\\n    sd_thumbnails: thumbnails(aspect_ratio: SD) {\\n        small\\n        medium\\n        large\\n    }\\n    hd_thumbnails: thumbnails(aspect_ratio: HD) {\\n        small\\n        medium\\n        large\\n    }\\n    film_thumbnails: thumbnails(aspect_ratio: FILM) {\\n        small\\n        medium\\n        large\\n    }\\n    square_thumbnails: thumbnails(aspect_ratio: SQUARE) {\\n        small\\n        medium\\n        large\\n    }\\n    }\\n    fragment StandardTextCard_media on Slugable {\\n    public_at\\n    updated_at\\n    title\\n    hero_video {\\n        duration\\n    }\\n    description\\n    }\",\n    \"variables\": { \"query\": \"test\",\"count\": 10,\"cursor\": null },\n    \"operationName\": \"SearchQuery\"\n}\n```\n\n----------------------------------------\n\nTITLE: Tuple Implementation in TypeScript\nDESCRIPTION: Shows how to implement and use tuples in TypeScript, defining fixed-length arrays with specific types for each position.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types_continued.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst course: {\n    name: string;\n    currentLesson: string;\n    typesLearned: string[];\n    courseInfo: [number, string];\n    learningBasicTypes?: boolean;\n} = {\n    name: 'Switching to TypeScript',\n    currentLesson: 'Using types - II',\n    typesLearned: ['number', 'boolean', 'string', 'object'],\n    courseInfo: [7, 'advanced'],\n};\n```\n\n----------------------------------------\n\nTITLE: Using Build-time Environment Variables in Dockerfile\nDESCRIPTION: Shows how to use environment variables during the Actor's build process in a Dockerfile using the ARG instruction.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nARG MY_BUILD_VARIABLE\nRUN echo $MY_BUILD_VARIABLE\n```\n\n----------------------------------------\n\nTITLE: Importing request-promise module in Node.js\nDESCRIPTION: Import the request-promise module to handle HTTP requests and file downloads.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_form_with_file_attachment.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst request = require('request-promise');\n```\n\n----------------------------------------\n\nTITLE: Defining Constants for Amazon Scraping in JavaScript\nDESCRIPTION: This code snippet defines constants used throughout the Amazon scraping project, including the base URL and labels for different types of pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/modularity.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// constants.js\nexport const BASE_URL = 'https://www.amazon.com';\n\nexport const labels = {\n    START: 'START',\n    PRODUCT: 'PRODUCT',\n    OFFERS: 'OFFERS',\n};\n```\n\n----------------------------------------\n\nTITLE: Pressing a Keyboard Key\nDESCRIPTION: Shows how to simulate pressing the Enter key using both Puppeteer and Playwright.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Press enter\nawait page.keyboard.press('Enter');\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Title with JavaScript\nDESCRIPTION: Shows how to extract a product title by selecting the title element within a product card and accessing its text content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/03_devtools_extracting_data.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ntitle = subwoofer.querySelector('.product-item__title');\ntitle.textContent;\n```\n\n----------------------------------------\n\nTITLE: Pagination Implementation\nDESCRIPTION: Complete implementation of paginated API scraping for SoundCloud tracks\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/handling_pagination.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport { scrapeClientId } from './scrapeClientId';\n\nconst scrape100Items = async () => {\n    let nextHref = 'https://api-v2.soundcloud.com/users/141707/tracks?limit=20&offset=0';\n    const items = [];\n\n    const clientId = await scrapeClientId();\n\n    while (items.flat().length < 100) {\n        if (!nextHref) return items.flat();\n\n        const nextURL = new URL(nextHref);\n        nextURL.searchParams.set('client_id', clientId);\n\n        const res = await gotScraping(nextURL);\n        const json = JSON.parse(res.body);\n        items.push(json.collection);\n\n        nextHref = json.next_href;\n    }\n\n    return items.flat();\n};\n\n(async () => {\n    const data = await scrape100Items();\n    console.log(data.length);\n})();\n```\n\n----------------------------------------\n\nTITLE: Encoded API Response Example\nDESCRIPTION: Example of an API response containing Base64 encoded data in JSON format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/index.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Scraping Academy Message\",\n    \"message\": \"SGVsbG8hIFlvdSBoYXZlIHN1Y2Nlc3NmdWxseSBkZWNvZGVkIHRoaXMgYmFzZTY0IGVuY29kZWQgbWVzc2FnZSEgV2UgaG9wZSB5b3UncmUgbGVhcm5pbmcgYSBsb3QgZnJvbSB0aGUgQXBpZnkgU2NyYXBpbmcgQWNhZGVteSE=\"\n}\n```\n\n----------------------------------------\n\nTITLE: Terms Summary Table in Markdown\nDESCRIPTION: Markdown table summarizing each section of the terms and conditions with descriptions of their contents.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/old/general-terms-and-conditions-2022.md#2025-04-18_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Section                                                                                                                            | What can you find there?                                                                                                                                                                                                                                                                               |\n|------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [1. Acceptance of these Terms](#1-acceptance-of-these-terms)                                                                       | These Terms become a binding contract at the moment you sign-up on our Website.                                                                                                                                                                                                                        |\n```\n\n----------------------------------------\n\nTITLE: Demonstrating the 'any' Type in TypeScript\nDESCRIPTION: This snippet shows how the 'any' type can lead to potential runtime errors by allowing assignment of any value type without compiler checks.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nlet userInput: any;\nlet savedInput: string;\n\nuserInput = 5;\n\nsavedInput = userInput;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Apify API Endpoint with CSV Format Parameter\nDESCRIPTION: Example of an Apify API endpoint URL with additional parameters to specify the output format as CSV. This URL is used to run an Actor and retrieve its results formatted as CSV data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_api.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync-get-dataset-items?token=YOUR_TOKEN_HERE&format=csv\n```\n\n----------------------------------------\n\nTITLE: React Card Import and Usage in Integrations Page\nDESCRIPTION: JSX code showing the import and usage of Card and CardGrid components to display integration options in a grid layout. The components are used to create clickable cards with titles, descriptions and icons for different integration services.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/index.mdx#2025-04-18_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Card from '@site/src/components/Card';\nimport CardGrid from '@site/src/components/CardGrid';\n\n<CardGrid>\n    <Card\n        title=\"API\"\n        desc=\"Control the Apify platform programmatically from your code.\"\n        to=\"/platform/integrations/api\"\n        smallImage\n    />\n    <Card\n        title=\"Actors and tasks\"\n        desc=\"Trigger other Actors or tasks when your Actor run fails or succeeds.\"\n        to=\"/platform/integrations/actors\"\n        smallImage\n    />\n</CardGrid>\n```\n\n----------------------------------------\n\nTITLE: Basic Actor Input Handling with Node.js\nDESCRIPTION: Simple example showing how to initialize an Actor and get input using the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// index.js\nimport { Actor } from 'apify';\n\n// We must initialize and exit the Actor. The rest of our code\n// goes in between these two.\nawait Actor.init();\n\nconst input = await Actor.getInput();\nconsole.log(input);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Apify and LlamaIndex Integration\nDESCRIPTION: Command to install all required Python packages for integrating Apify with LlamaIndex, including the apify-client, llama-index-core, and llama-index-readers-apify packages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/llama.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apify-client llama-index-core llama-index-readers-apify\n```\n\n----------------------------------------\n\nTITLE: Embedding Actor Status Badge in HTML\nDESCRIPTION: Shows how to embed the Actor status badge in HTML documentation. The image is wrapped in a link that directs to the Actor's page on the Apify platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/badge.mdx#2025-04-18_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<a href=\"https://apify.com/apify/website-content-crawler\">\n  <img src=\"https://apify.com/actor-badge?actor=apify/website-content-crawler\">\n</a>\n```\n\n----------------------------------------\n\nTITLE: Calling Website Content Crawler Actor\nDESCRIPTION: Python code to invoke the Website Content Crawler Actor for crawling Pinecone documentation pages\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/pinecone.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nactor_call = client.actor(\"apify/website-content-crawler\").call(\n    run_input={\"startUrls\": [{\"url\": \"https://docs.pinecone.io/home\"}]}\n)\n\nprint(\"Website Content Crawler Actor has finished\")\nprint(actor_call)\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries\nDESCRIPTION: Import statements for required Python libraries including datetime handling, web requests, and BeautifulSoup for HTML parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime, time, timedelta, timezone\nimport os\nimport re\n\nfrom apify_client import ApifyClient\nfrom bs4 import BeautifulSoup\nimport requests\n```\n\n----------------------------------------\n\nTITLE: Deploying Actor to Apify Platform\nDESCRIPTION: Command to deploy the Actor code and configuration to the Apify cloud platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Logging Best Practices in JavaScript\nDESCRIPTION: This example demonstrates the difference between poor and good logging practices. It shows how to create meaningful log messages that provide context and are understandable to outsiders.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/best_practices.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n300  https://example.com/1234  1234\n```\n\nLANGUAGE: text\nCODE:\n```\nIndex 1234 --- https://example.com/1234 --- took 300 ms\n```\n\n----------------------------------------\n\nTITLE: Deploying Actor using Apify CLI\nDESCRIPTION: Command to deploy and push your Actor code to the Apify platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/deployment/index.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Simple Message Return Function\nDESCRIPTION: Defines a basic function that returns a static message string.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst returnMessage = () => 'Apify academy!';\n```\n\n----------------------------------------\n\nTITLE: Determining Actor Standby Mode in Python\nDESCRIPTION: This snippet shows how to determine if an Actor was started in Standby mode or standard mode using Python. It checks the 'meta_origin' configuration attribute to make this determination.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/actor_standby.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main() -> None:\n    async with Actor:\n        if Actor.config.meta_origin == 'STANDBY':\n            # Start your Standby server here\n        else:\n            # Perform the standard Actor operations here\n```\n\n----------------------------------------\n\nTITLE: Specifying Files to Compile in TypeScript Configuration\nDESCRIPTION: This snippet shows how to use the 'include' option in tsconfig.json to specify which files or directories should be included in the TypeScript compilation process.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {},\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up MITM Proxy for Puppeteer in JavaScript\nDESCRIPTION: This code sets up a man-in-the-middle proxy using the http-mitm-proxy library. It creates necessary certificate directories, initializes the proxy with wildcard matching and gzip support, and adds the CA certificate to Chromium's trusted certificates.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/using_proxy_to_intercept_requests_puppeteer.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst { promisify } = require('util');\nconst { exec } = require('child_process');\nconst Proxy = require('http-mitm-proxy');\nconst Promise = require('bluebird');\n\nconst execPromise = promisify(exec);\n\nconst wait = (timeout) => new Promise((resolve) => setTimeout(resolve, timeout));\n\nconst setupProxy = async (port) => {\n    // Setup chromium certs directory\n    // WARNING: this only works in debian docker images\n    // modify it for any other use cases or local usage.\n    await execPromise('mkdir -p $HOME/.pki/nssdb');\n    await execPromise('certutil -d sql:$HOME/.pki/nssdb -N');\n    const proxy = Proxy();\n    proxy.use(Proxy.wildcard);\n    proxy.use(Proxy.gunzip);\n    return new Promise((resolve, reject) => {\n        proxy.listen({ port, silent: true }, (err) => {\n            if (err) return reject(err);\n            // Add CA certificate to chromium and return initialize proxy object\n            execPromise('certutil -d sql:$HOME/.pki/nssdb -A -t \"C,,\" -n mitm-ca -i ./.http-mitm-proxy/certs/ca.pem')\n                .then(() => resolve(proxy))\n                .catch(reject);\n        });\n    });\n};\n```\n\n----------------------------------------\n\nTITLE: Expected Output JSON Format\nDESCRIPTION: Example of the expected output format in the default dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"solution\": 20\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dataset Information\nDESCRIPTION: Fetches metadata about an actor run's dataset using the run ID.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_3\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('actor-runs/<RUN_ID>/dataset');\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Basic Variable Declaration in TypeScript\nDESCRIPTION: Demonstrates automatic type inference by TypeScript when declaring a variable with an initial value.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst value = 10;\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for OpenAI Assistant and Apify Integration\nDESCRIPTION: This snippet shows how to install the required Python packages for working with OpenAI's API and Apify's client.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apify-client openai\n```\n\n----------------------------------------\n\nTITLE: Installing Axios Package\nDESCRIPTION: Command to install the Axios HTTP client package for making API requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i axios\n```\n\n----------------------------------------\n\nTITLE: Accessing Actor Input in JavaScript\nDESCRIPTION: This code snippet demonstrates how to retrieve input values in an Actor's JavaScript code using the Actor.getInput() method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst { datasetId, connectionString, tableName } = await Actor.getInput();\n```\n\n----------------------------------------\n\nTITLE: Guardian F1 News URL Constant\nDESCRIPTION: URL string constant for The Guardian's Formula One news page, to be used in a web scraping exercise.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nhttps://www.theguardian.com/sport/formulaone\n```\n\n----------------------------------------\n\nTITLE: Installing Apify-Haystack Dependencies with pip\nDESCRIPTION: Command to install the required Python packages for integrating Apify with Haystack.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apify-haystack haystack-ai\n```\n\n----------------------------------------\n\nTITLE: Initializing CheerioCrawler for Web Scraping\nDESCRIPTION: Sets up a basic CheerioCrawler to scrape a web page. This initial setup doesn't handle dynamic content correctly.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request }) => {\n        // We'll put our logic here in a minute\n    },\n});\n\nawait crawler.addRequests([{ url: 'https://demo-webstore.apify.org/search/new-arrivals' }]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: API Endpoint Examples with cURL\nDESCRIPTION: Collection of cURL commands demonstrating various Apify API endpoints for running actors, checking run status, and retrieving data from datasets and key-value stores.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nhttps://api.apify.com/v2/acts/apify~web-scraper/runs?token=YOUR_TOKEN&waitForFinish=60\n```\n\nLANGUAGE: shell\nCODE:\n```\nhttps://api.apify.com/v2/acts/ACTOR_NAME_OR_ID/runs/RUN_ID\n```\n\nLANGUAGE: shell\nCODE:\n```\nhttps://api.apify.com/v2/datasets/DATASET_ID/items\n```\n\nLANGUAGE: shell\nCODE:\n```\nhttps://api.apify.com/v2/datasets/DATASET_ID/items?format=csv&offset=250000\n```\n\nLANGUAGE: shell\nCODE:\n```\nhttps://api.apify.com/v2/key-value-stores/STORE_ID/records/RECORD_KEY\n```\n\nLANGUAGE: shell\nCODE:\n```\nhttps://api.apify.com/v2/key-value-stores/STORE_ID/keys?exclusiveStartKey=myLastRecordKey\n```\n\n----------------------------------------\n\nTITLE: Saving Plot to Apify Key-Value Store\nDESCRIPTION: Saves the generated plot as a PNG image to the Actor's default key-value store and prints the URL for accessing the result.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Get the resource sub-client for working with the default key-value store of the run\nkey_value_store_client = client.key_value_store(os.environ['APIFY_DEFAULT_KEY_VALUE_STORE_ID'])\n\n# Save the resulting plot to the key-value store through an in-memory buffer\nprint('Saving plot to key-value store...')\nwith BytesIO() as buf:\n    axes.figure.savefig(buf, format='png', dpi=200, facecolor='w')\n    buf.seek(0)\n    key_value_store_client.set_record('prediction.png', buf, 'image/png')\n\nprint(f'Result is available at {os.environ[\"APIFY_API_PUBLIC_BASE_URL\"]}'\n      + f'/v2/key-value-stores/{os.environ[\"APIFY_DEFAULT_KEY_VALUE_STORE_ID\"]}/records/prediction.png')\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI with npm\nDESCRIPTION: This command installs the Apify CLI globally on your system using npm. This requires Node.js to be installed on your computer first.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/tools/apify_cli.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i -g apify-cli\n```\n\n----------------------------------------\n\nTITLE: Creating New Apify Actor\nDESCRIPTION: Command to initiate the creation of a new Actor project, which will prompt for name, programming language, and template selection.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify create\n```\n\n----------------------------------------\n\nTITLE: JavaScript Console Commands\nDESCRIPTION: JavaScript commands for interacting with HTML elements in the browser console.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/01_devtools_inspecting.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\ntemp1.textContent;\n```\n\nLANGUAGE: javascript\nCODE:\n```\ntemp1.outerHTML;\n```\n\nLANGUAGE: javascript\nCODE:\n```\ntemp1.textContent = 'Hello World!';\n```\n\n----------------------------------------\n\nTITLE: Finding Last Page Number - Puppeteer Implementation\nDESCRIPTION: Script using Puppeteer to determine the total number of pages by extracting the last page number from GitHub's pagination.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\nconst browser = await puppeteer.launch({ headless: false });\nconst firstPage = await browser.newPage();\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageLabel = await firstPage.$eval(\n    'a[aria-label*=\"Page \"]:nth-last-child(2)',\n    (element) => element.getAttribute('aria-label'),\n);\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\nconsole.log(lastPageNumber);\n\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Running Apify Website Content Crawler\nDESCRIPTION: Initializes and runs the Apify website content crawler to fetch documents from a specified URL, converting the results into LangChain Document format.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napify = ApifyWrapper()\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nloader = apify.call_actor(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\"startUrls\": [{\"url\": \"https://python.langchain.com/docs/get_started/introduction\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"},\n    dataset_mapping_function=lambda item: Document(\n        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Keyword Replacement in JavaScript Obfuscation\nDESCRIPTION: This example demonstrates the keyword replacement technique used in data obfuscation. It allows the script to mask accessed properties, making detection more difficult and enabling random ordering of substrings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/fingerprinting.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nvar _0x1234 = ['navigator', 'userAgent'];\\nvar _0x5678 = window[_0x1234[0]][_0x1234[1]];\n```\n\n----------------------------------------\n\nTITLE: Optional Properties in TypeScript Objects\nDESCRIPTION: Shows how to define optional properties in object types using the ? operator, allowing a property to be undefined or of a specified type.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types_continued.md#2025-04-18_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst course: {\n    name: string;\n    currentLesson: string;\n    learningBasicTypes?: boolean;\n} = {\n    name: 'Switching to TypeScript',\n    currentLesson: 'Using types - II',\n};\n\ncourse.learningBasicTypes = true;\n```\n\n----------------------------------------\n\nTITLE: Selecting Elements by Class using JavaScript\nDESCRIPTION: Shows how to select all elements with a specific class name using the class selector with querySelectorAll()\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/css_selectors.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst highlightedElements = document.querySelectorAll('.highlight');\n```\n\n----------------------------------------\n\nTITLE: Markdown Navigation Link\nDESCRIPTION: A markdown header and link directing users to the next lesson about Actor inputs and outputs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/creating_actors.md#2025-04-18_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## Next up {#next}\n\nWe've created an Actor, but how can we give it more complex inputs and make it do stuff based on these inputs? This is exactly what we'll be discussing in the [next lesson](./inputs_outputs.md)'s activity.\n```\n\n----------------------------------------\n\nTITLE: Selecting Product Elements with JavaScript\nDESCRIPTION: Demonstrates how to select product items from a page using querySelectorAll and access a specific product element.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/03_devtools_extracting_data.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nproducts = document.querySelectorAll('.product-item');\nsubwoofer = products[2];\n```\n\n----------------------------------------\n\nTITLE: Google Search Query for Finding Relevant Quora Questions\nDESCRIPTION: A search query format to find relevant Quora questions related to your Actor's keyword. This helps identify opportunities for promoting your Actor through helpful answers.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/promote_your_actor/parasite_seo.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsite:quora.com <your keyword>\n```\n\n----------------------------------------\n\nTITLE: Wikipedia Page URL Constant\nDESCRIPTION: URL string constant for the Wikipedia page listing African countries, to be used in a web scraping exercise.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nhttps://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_in_Africa\n```\n\n----------------------------------------\n\nTITLE: Storing Scraped Weather Data in Apify Dataset\nDESCRIPTION: This code snippet initializes an ApifyClient, creates a dataset client, and pushes the scraped weather data to the default dataset of the Actor run.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the main ApifyClient instance\nclient = ApifyClient(os.environ['APIFY_TOKEN'], api_url=os.environ['APIFY_API_BASE_URL'])\n\n# Get the resource subclient for working with the default dataset of the Actor run\ndefault_dataset_client = client.dataset(os.environ['APIFY_DEFAULT_DATASET_ID'])\n\n# Finally, push all the results into the dataset\ndefault_dataset_client.push_items(weather_data)\n\nprint(f'Results have been saved to the dataset with ID {os.environ[\"APIFY_DEFAULT_DATASET_ID\"]}')\n```\n\n----------------------------------------\n\nTITLE: Defining Default Payload Template in JSON\nDESCRIPTION: This snippet shows the default payload template structure for Apify webhooks. It includes userId, createdAt, eventType, eventData, and resource fields, using variables enclosed in double curly braces.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/actions.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"userId\": {{userId}},\n    \"createdAt\": {{createdAt}},\n    \"eventType\": {{eventType}},\n    \"eventData\": {{eventData}},\n    \"resource\": {{resource}}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing TypeScript Globally with npm\nDESCRIPTION: Use npm to install TypeScript globally on your machine. This command installs the TypeScript compiler, which can be used to compile .ts files into JavaScript.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install -g typescript\n```\n\n----------------------------------------\n\nTITLE: Synchronous Actor Run Endpoint\nDESCRIPTION: HTTP POST endpoint for running an actor synchronously, suitable for shorter runs requiring immediate results.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nPOST https://api.apify.com/v2/acts/:actorId/run-sync\n```\n\n----------------------------------------\n\nTITLE: Default package.json for Apify Actors\nDESCRIPTION: This is the default package.json configuration for Apify Actors. It specifies the main script as 'main.js' and includes dependencies for apify and crawlee.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/docker.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"description\": \"Anonymous Actor on the Apify platform\",\n    \"version\": \"0.0.1\",\n    \"license\": \"UNLICENSED\",\n    \"main\": \"main.js\",\n    \"scripts\": {\n        \"start\": \"node main.js\"\n    },\n    \"dependencies\": {\n        \"apify\": \"^3.0.0\",\n        \"crawlee\": \"^3.0.0\"\n    },\n    \"repository\": {}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Library Imports\nDESCRIPTION: Basic test script to verify the correct installation and importing of got-scraping and cheerio libraries.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/project_setup.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconsole.log('it works!');\n```\n\n----------------------------------------\n\nTITLE: Downloading a Binary File using request-promise in JavaScript\nDESCRIPTION: This snippet shows how to download a binary file (e.g., PDF) using request-promise in JavaScript. It specifies the encoding as null to ensure the data is treated as binary.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst fileData = await request({\n    uri: 'https://some-site.com/file.pdf',\n    encoding: null,\n});\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Google Search Pages with Different Keywords in Web Scraper\nDESCRIPTION: This code handles the initial page that enqueues multiple requests to Google, each with a different keyword. It uses random uniqueKey values to prevent URL deduplication and passes keywords via userData.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/processing_multiple_pages_web_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const $ = context.jQuery;\n\n    if (context.request.userData.label === 'enqueue') {\n    // parse input keywords\n        const keywords = context.customData;\n\n        // process all the keywords\n        for (const keyword of keywords) {\n        // enqueue the page and pass the keyword in\n        // the interceptRequestData attribute\n            await context.enqueueRequest({\n                url: 'https://google.com',\n                uniqueKey: `${Math.random()}`,\n                userData: {\n                    label: 'fill-form',\n                    keyword,\n                },\n            });\n        }\n        // No return here because we don't extract any data yet\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Request Queue Endpoint in YAML\nDESCRIPTION: This YAML snippet defines the GET operation for a specific request queue, including tags, summary, description, and responses.\nSOURCE: https://github.com/apify/apify-docs/blob/master/CONTRIBUTING.md#2025-04-18_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nget:\n    tags:\n        - Request Queues\n    summary: Get a Request Queue\n    operationId: requestQueues_get\n    description: |\n        You can have a markdown description here.\n    parameters:\n    responses:\n        '200':\n        '401':\n    x-code-samples:\n        -   lang: PHP\n            source:\n                $ref: ../code_samples/PHP/customers/get.php\n```\n\n----------------------------------------\n\nTITLE: Querying DOM for First Product Card with JavaScript\nDESCRIPTION: JavaScript code using document.querySelector() to select the first product card element on the page based on its CSS class.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelector('.product-item');\n```\n\n----------------------------------------\n\nTITLE: NPM Initialize and Install Command\nDESCRIPTION: Shell command to initialize a new Node.js project and install required dependencies\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/handling_pagination.md#2025-04-18_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm init -y && npm i puppeteer got-scraping\n```\n\n----------------------------------------\n\nTITLE: Selecting Element by ID\nDESCRIPTION: JavaScript code demonstrating how to select a DOM element using getElementById() method.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/html_elements.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst myElement = document.getElementById('myId');\n```\n\n----------------------------------------\n\nTITLE: Successful Apify CLI Login Message\nDESCRIPTION: This shows the success message displayed after successfully logging into the Apify CLI with your personal API token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/tools/apify_cli.md#2025-04-18_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nSuccess: You are logged in to Apify as YOUR_USERNAME!\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Commands to install the necessary Python packages for CrewAI integration with Apify\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'crewai[tools]' langchain-apify langchain-openai\n```\n\n----------------------------------------\n\nTITLE: Scraping Title, Description and Modified Date with Cheerio\nDESCRIPTION: Adds functionality to extract and parse the modification date from a time element's datetime attribute.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    const { $ } = context;\n    // ... rest of your code can come here\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n    };\n}\n```\n\n----------------------------------------\n\nTITLE: Version History Table in Markdown\nDESCRIPTION: A markdown table showing the version history and effective dates of Apify Store publishing terms and conditions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/old/store-publishing-terms-and-conditions-2022.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version                                                            | Effective from   | Effective until |\n|--------------------------------------------------------------------|------------------|------------------|\n| [Latest](../latest/terms/store-publishing-terms-and-conditions.md) | May 13, 2024     |                 |\n| December 2022 (This document)                                      | December 1, 2022 | June 12, 2024   |\n```\n\n----------------------------------------\n\nTITLE: AWS Bedrock Agent Reasoning Example\nDESCRIPTION: Sample JSON output showing the agent's rationale for using RAG Web Browser to process a user query.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/aws_bedrock.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"rationale\": {\n        \"text\": \"To answer this question about the latest news for AWS Bedrock, I'll need to use the RAG Web Browser function to search for and retrieve the most recent information. I'll craft a search query that specifically targets AWS Bedrock news.\",\n        \"traceId\": \"845d524a-b82c-445b-9e36-66d887b3b25e-0\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Authenticated Proxy URL Structure\nDESCRIPTION: Illustrates how to include authentication credentials in a proxy URL with username and password.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/proxies.md#2025-04-18_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttp://USERNAME:PASSWORD@proxy.example.com:8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Modern JavaScript in package.json\nDESCRIPTION: Adds the type module configuration to package.json to enable modern JavaScript features like ES modules.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/project_setup.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n\"type\": \"module\"\n```\n\n----------------------------------------\n\nTITLE: Rendering Course Categories and Sections using React in JSX\nDESCRIPTION: This code snippet maps over the entries of a homepageContent object to render course categories and their corresponding sections. It uses custom Card and CardGrid components to display the content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/index.mdx#2025-04-18_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<>\n{\n    Object.entries(homepageContent).map(([categoryName, sections]) =>\n        <>\n            <h2>{categoryName}</h2>\n            <CardGrid>\n                {\n                    sections.map((section, i) =>\n                    <Card\n                        title={section.title}\n                        desc={section.description}\n                        imageUrl={section.imageUrl}\n                        to={section.link}\n                        key={i}\n                    />)\n                }\n            </CardGrid>\n        </>\n    )}\n</>\n```\n\n----------------------------------------\n\nTITLE: Incorrect Payment Verification Pattern in JavaScript\nDESCRIPTION: Shows an anti-pattern of payment verification that incorrectly relies on element absence, which can lead to false positives.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nasync function isPaymentSuccessful() {\n    const $paymentAmount = await page.$('#PaymentAmount');\n\n    if (!$paymentAmount) return OUTPUT.paymentSuccess;\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy URL Structure\nDESCRIPTION: Demonstrates the basic structure of a proxy URL with hostname and port number.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/proxies.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttp://proxy.example.com:8080\n```\n\n----------------------------------------\n\nTITLE: Selecting Shein Product Elements with CSS Selector\nDESCRIPTION: Selects all product card elements on Shein's jewelry page using the product-card class selector.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/02_devtools_locating_elements.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\ndocument.querySelectorAll('.product-card')\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cookies with Playwright\nDESCRIPTION: This code snippet demonstrates how to retrieve cookies from the default browser context in Playwright after logging in.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// Grab the cookies from the default browser context,\n// which was used to log in\nconst cookies = await browser.contexts()[0].cookies();\n```\n\n----------------------------------------\n\nTITLE: Unreliable Payment Submission Implementation in JavaScript\nDESCRIPTION: Shows an incorrect payment submission implementation that lacks proper outcome verification.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nasync function submitPayment() {\n    await Promise.all([\n        page.click('submitPayment'),\n        page.waitForNavigation(),\n    ]);\n\n    return OUTPUT.paymentSuccess;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting Apify Documentation\nDESCRIPTION: These bash commands demonstrate how to install dependencies and start the Apify documentation development server.\nSOURCE: https://github.com/apify/apify-docs/blob/master/CONTRIBUTING.md#2025-04-18_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install\nnpm start\n```\n\n----------------------------------------\n\nTITLE: HTML Link Structure Example\nDESCRIPTION: Demonstrates the basic structure of an HTML anchor element used for creating links.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/finding_links.md#2025-04-18_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<a href=\"https://example.com\">This is a link to example.com</a>\n```\n\n----------------------------------------\n\nTITLE: Logging in to Apify Platform via CLI\nDESCRIPTION: Command to log in to the Apify platform using the Apify CLI.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_web_ide.md#2025-04-18_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\napify login\n```\n\n----------------------------------------\n\nTITLE: Adding User Data Label to Start URL in JSON Format\nDESCRIPTION: JSON object that adds metadata to the Start URL request. The label 'START' helps distinguish this URL from others during processing in the page function.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"label\": \"START\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Langflow using uv package manager\nDESCRIPTION: Commands to install Langflow using the uv Python package and project manager, and then start the Langflow platform.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langflow.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install langflow\n```\n\nLANGUAGE: bash\nCODE:\n```\nuv run langflow run\n```\n\n----------------------------------------\n\nTITLE: Installing Apify Python SDK\nDESCRIPTION: Command to install the Apify Python SDK using pip.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/milvus.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apify-client\n```\n\n----------------------------------------\n\nTITLE: Calculating Pay-Per-Result Profit Formula\nDESCRIPTION: Mathematical formula showing how profit is calculated in the pay-per-result pricing model, accounting for revenue share and platform costs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/store_basics/how_actor_monetization_works.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nprofit = (0.8 * revenue) - platform usage costs\n```\n\n----------------------------------------\n\nTITLE: Complete Google Search Automation with Puppeteer (with Error)\nDESCRIPTION: A full example of automating a Google search using Puppeteer, which demonstrates a common error when not waiting for page navigation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n// This code will throw an error!\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nawait page.click('button + button');\n\nawait page.type('textarea[title]', 'hello world');\n\nawait page.keyboard.press('Enter');\n\n// Click the first result\nawait page.click('.g a');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Run Status Monitoring Endpoint\nDESCRIPTION: HTTP GET endpoint for checking the status of an actor run.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.apify.com/v2/actor-runs/:runId\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI via Homebrew\nDESCRIPTION: Command to install the Apify CLI tool using the Homebrew package manager on MacOS/Linux systems.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install apify-cli\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Keys\nDESCRIPTION: Configuration of environment variables for OpenAI and Apify API authentication.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n```\n\n----------------------------------------\n\nTITLE: Installing HTTPX Package\nDESCRIPTION: Command to install the HTTPX library using pip package manager\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ pip install httpx\n...\nSuccessfully installed ... httpx-0.0.0\n```\n\n----------------------------------------\n\nTITLE: Pushing Actor to Apify Platform\nDESCRIPTION: Command to deploy the Actor code to the Apify platform, making it available for execution in the cloud.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\napify push\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for File Handling in JavaScript\nDESCRIPTION: This snippet shows how to import the necessary modules for file handling and HTTP requests in JavaScript. It uses the fs/promises module for file operations and request-promise for making HTTP requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as fs from 'fs/promises';\nimport request from 'request-promise';\n```\n\n----------------------------------------\n\nTITLE: Injecting jQuery into Browser Page for Web Scraper Debugging\nDESCRIPTION: Injects jQuery into the current page to enable jQuery-based scraping code in the browser console. This allows testing selectors and scrapers before implementing them in Apify Web Scraper.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst jq = document.createElement('script');\njq.src = 'https://ajax.googleapis.com/ajax/libs/jquery/2.2.2/jquery.min.js';\ndocument.getElementsByTagName('head')[0].appendChild(jq);\n```\n\n----------------------------------------\n\nTITLE: HTML Iframe Embedding YouTube Video\nDESCRIPTION: HTML code for embedding a YouTube video about anti-scraping measures in an iframe with specific dimensions and security settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/index.md#2025-04-18_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/aXil0K-M-Vs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n```\n\n----------------------------------------\n\nTITLE: IP-Based Proxy URL Structure\nDESCRIPTION: Shows how to format a proxy URL using an IP address instead of a hostname.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/proxies.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttp://123.456.789.10:8080\n```\n\n----------------------------------------\n\nTITLE: Permission Denied Error in Actor Build Logs\nDESCRIPTION: Alternative error message shown when permission issues prevent file operations during Actor builds.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/avoid_eacces_error_in_actor_builds.md#2025-04-18_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nEACCES: permission denied\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI on macOS/Linux\nDESCRIPTION: Command to install the Apify CLI tool on macOS or Linux systems using Homebrew.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_web_ide.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install apify-cli\n```\n\n----------------------------------------\n\nTITLE: Dataset Item Storage Endpoint\nDESCRIPTION: HTTP POST endpoint for storing data items in a dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_9\n\nLANGUAGE: http\nCODE:\n```\nPOST https://api.apify.com/v2/datasets/:datasetId/items\n```\n\n----------------------------------------\n\nTITLE: Basic Proxy Setup - Initial Code\nDESCRIPTION: Basic boilerplate code showing initial setup for using proxies with Playwright and Puppeteer to visit Google.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/proxies.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\n// our proxy server\nconst proxy = '103.214.9.13:3128';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\n// our proxy server\nconst proxy = '103.214.9.13:3128';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Submitting a form with file attachment using request-promise\nDESCRIPTION: Use request-promise to submit a form with both text fields and a file attachment. The Content-Type header is automatically set to multipart/form-data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_form_with_file_attachment.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait request({\n    uri: 'https://example.com/submit-form.php',\n    method: 'POST',\n\n    formData: {\n        // set any form values\n        name: 'John',\n        surname: 'Doe',\n        email: 'john.doe@example.com',\n\n        // add the attachment\n        attachment: {\n            value: fileData,\n            options: {\n                filename: 'file.pdf',\n                contentType: 'application/pdf',\n            },\n        },\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Stats Utility Class in JavaScript\nDESCRIPTION: Defines a Stats class to store and manage run statistics, including error tracking and successful request counting. The class provides methods for initialization, adding errors, and updating success count.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport Actor from 'apify';\n\nclass Stats {\n    constructor() {\n        this.state = {\n            errors: {},\n            totalSaved: 0,\n        };\n    }\n\n    async initialize() {\n        const data = await Actor.getValue('STATS');\n\n        if (data) this.state = data;\n\n        Actor.on('persistState', async () => {\n            await Actor.setValue('STATS', this.state);\n        });\n\n        setInterval(() => console.log(this.state), 10000);\n    }\n\n    addError(url, errorMessage) {\n        if (!this.state.errors?.[url]) this.state.errors[url] = [];\n        this.state.errors[url].push(errorMessage);\n    }\n\n    success() {\n        this.state.totalSaved += 1;\n    }\n}\n\nmodule.exports = new Stats();\n```\n\n----------------------------------------\n\nTITLE: Sample Output Format for F1 Articles\nDESCRIPTION: Example of the expected output format showing author names and article titles\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nDaniel Harris: Sports quiz of the week: Johan Neeskens, Bond and airborne antics\nColin Horgan: The NHL is getting its own Drive to Survive. But could it backfire?\nReuters: US GP ticket sales 'took off' after Max Verstappen stopped winning in F1\nGiles Richards: Liam Lawson gets F1 chance to replace Pérez alongside Verstappen at Red Bull\nPA Media: Lewis Hamilton reveals lifelong battle with depression after school bullying\n...\n```\n\n----------------------------------------\n\nTITLE: Installing Apify Client Package\nDESCRIPTION: Commands to install the apify-client package using npm for Node.js and pip for Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install apify-client\n```\n\nLANGUAGE: shell\nCODE:\n```\npip install apify-client\n```\n\n----------------------------------------\n\nTITLE: Base64 Decoding in Node.js\nDESCRIPTION: JavaScript code demonstrating how to decode Base64 encoded strings using Node.js Buffer class.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/index.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst value = 'SGVsbG8hIFlvdSBoYXZlIHN1Y2Nlc3NmdWxseSBkZWNvZGVkIHRoaXMgYmFzZTY0IGVuY29kZWQgbWVzc2FnZSEgV2UgaG9wZSB5b3UncmUgbGVhcm5pbmcgYSBsb3QgZnJvbSB0aGUgQXBpZnkgU2NyYXBpbmcgQWNhZGVteSE=';\n\nconst decoded = Buffer.from(value, 'base64').toString('utf-8');\n\nconsole.log(decoded);\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: This snippet lists the required Python packages for the project. It includes matplotlib for plotting and visualization, and pandas for data manipulation and analysis.\nSOURCE: https://github.com/apify/apify-docs/blob/master/examples/python-data-parser/requirements.txt#2025-04-18_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmatplotlib\npandas\n```\n\n----------------------------------------\n\nTITLE: Account Verification Endpoint\nDESCRIPTION: HTTP GET endpoint for verifying API credentials and account details.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.apify.com/v2/users/me\n```\n\n----------------------------------------\n\nTITLE: Embedding Actor Status Badge in Markdown\nDESCRIPTION: Demonstrates how to embed the Actor status badge in Markdown documentation. This example uses the Markdown image syntax combined with a link.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/badge.mdx#2025-04-18_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n[![Website Content Crawler Actor](https://apify.com/actor-badge?actor=apify/website-content-crawler)](https://apify.com/apify/website-content-crawler)\n```\n\n----------------------------------------\n\nTITLE: Running Node.js Script\nDESCRIPTION: Command to execute the Node.js script and verify the setup.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/project_setup.md#2025-04-18_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnode main.js\n```\n\n----------------------------------------\n\nTITLE: Separate Configuration Files for Actor in JSON\nDESCRIPTION: These JSON configurations show how to split the Actor and dataset schema configurations into separate files. The first file (actor.json) contains the basic Actor information and references the second file (dataset_schema.json) for the dataset schema details.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/index.md#2025-04-18_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"name\": \"this-is-book-library-scraper\",\n    \"title\": \"Book Library scraper\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": \"./dataset_schema.json\"\n    }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"actorSpecification\": 1,\n    \"fields\": {},\n    \"views\": {\n        \"overview\": {\n            \"title\": \"Overview\",\n            \"transformation\": {},\n            \"display\": {}\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Apify SDK\nDESCRIPTION: Command to install the Apify SDK package using npm.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install apify\n```\n\n----------------------------------------\n\nTITLE: Example module syntax reference\nDESCRIPTION: Reference to Node.js modularity patterns, included as a prerequisite concept\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/index.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n[]\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n()\n```\n\n----------------------------------------\n\nTITLE: Initializing TypeScript Configuration\nDESCRIPTION: Command to create a new TypeScript configuration file (tsconfig.json) with default settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ntsc --init\n```\n\n----------------------------------------\n\nTITLE: Keyboard Shortcuts for Apify Console Navigation\nDESCRIPTION: A markdown table showing keyboard shortcuts for navigating different sections of the Apify Console. These shortcuts allow for quick access to various parts of the interface.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/console/index.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n|Shortcut| Tab |\n|:---|:----|\n|Show shortcuts | Shift? |\n|Home| GH  |\n|Store| GO  |\n|Actors| GA  |\n|Development| GD |\n|Saved tasks| GT  |\n|Runs| GR  |\n|Integrations | GI |\n|Schedules| GU  |\n|Storage| GE  |\n|Proxy| GP  |\n|Settings| GS  |\n|Billing| GB  |\n```\n\n----------------------------------------\n\nTITLE: CSV File Storage Path\nDESCRIPTION: Displays the storage path where the exported CSV file will be saved in the key-value store.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n./storage/key-value-stores/default/results.csv\n```\n\n----------------------------------------\n\nTITLE: Terms of Use Section in Markdown\nDESCRIPTION: Section detailing the permitted uses and restrictions for the platform and services, focusing on data extraction from public websites\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/old/general-terms-and-conditions-2022.md#2025-04-18_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## 5. Terms of Use of the Website, Platform, and Services\n\nYou may use the Platform and other Services solely for the purposes of data extraction from publicly accessible websites...\n```\n\n----------------------------------------\n\nTITLE: Defining URL Pattern for Actor Detail Pages\nDESCRIPTION: Text pattern (Pseudo URL) that matches the structure of Actor detail pages. This pattern uses regular expressions to match URLs following the format of Actor detail pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nhttps://apify.com/[.+]/[.+]\n```\n\n----------------------------------------\n\nTITLE: Installing Apify CLI via NPM\nDESCRIPTION: Command to globally install the Apify CLI tool using Node Package Manager (NPM) for platforms other than MacOS/Linux.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm -g install apify-cli\n```\n\n----------------------------------------\n\nTITLE: Package JSON Configuration\nDESCRIPTION: Required package.json configuration to enable ESM imports for Crawlee\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\": \"my-scraping-project\",\n    \"type\": \"module\",\n    \"dependencies\": {\n        \"crawlee\": \"^3.0.0\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: CSS Styling Example\nDESCRIPTION: Shows how to style HTML elements using CSS class selectors to modify text color and transformation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/01_devtools_inspecting.md#2025-04-18_snippet_1\n\nLANGUAGE: css\nCODE:\n```\n.heading {\n  color: blue;\n  text-transform: uppercase;\n}\n```\n\n----------------------------------------\n\nTITLE: Frontmatter Configuration for Storage Documentation\nDESCRIPTION: YAML frontmatter configuration for the storage documentation page, defining metadata like title, description, sidebar position and category.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/index.md#2025-04-18_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Storage\ndescription: Store anything from images and key-value pairs to structured output data. Learn how to access and manage your stored data from the Apify platform or via API.\nsidebar_position: 9\ncategory: platform\nslug: /storage\n---\n```\n\n----------------------------------------\n\nTITLE: Implementing Task Call via JavaScript Client\nDESCRIPTION: Function that uses Apify JavaScript client to call a task, download dataset items and save them as CSV.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/using_api_and_client.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst withClient = async () => {\n    const client = Actor.newClient();\n    const task = client.task(TASK);\n\n    const { id } = await task.call({ memory });\n\n    const dataset = client.run(id).dataset();\n\n    const items = await dataset.downloadItems('csv', {\n        limit: maxItems,\n        fields,\n    });\n\n    // If the content type is anything other than JSON, it must\n    // be specified within the third options parameter\n    return Actor.setValue('OUTPUT', items, { contentType: 'text/csv' });\n};\n```\n\n----------------------------------------\n\nTITLE: Modified Variable Declaration for Browser Console Testing\nDESCRIPTION: The modified version of the code snippet without const declaration, suitable for repeatedly running in a browser console during debugging.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nresults = [];\n```\n\n----------------------------------------\n\nTITLE: Using Apify SDK's Built-in saveSnapshot Utility with Puppeteer\nDESCRIPTION: A complete example of an Apify Actor that navigates to a URL specified in the input and saves a screenshot using the built-in puppeteerUtils.saveSnapshot function. This demonstrates the recommended approach for debugging Puppeteer-based crawlers.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/how_to_save_screenshots_puppeteer.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { puppeteerUtils, launchPuppeteer } from 'crawlee';\n\nActor.main(async () => {\n    const input = await Actor.getValue('INPUT');\n\n    console.log('Launching Puppeteer...');\n    const browser = await launchPuppeteer();\n\n    const page = await browser.newPage();\n    await page.goto(input.url);\n\n    await puppeteerUtils.saveSnapshot(page, { key: 'test-screen' });\n\n    console.log('Closing Puppeteer...');\n    await browser.close();\n\n    console.log('Done.');\n});\n```\n\n----------------------------------------\n\nTITLE: Workflow Diagram in Mermaid\nDESCRIPTION: A flowchart showing the basic workflow between HTTP Client, Apify API, Actor Run and storage systems (Dataset and Key-Value Store).\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    A[HTTP Client] <--> B[Apify API]\n    B --> C[Actor Run]\n    C --> D1[Dataset]\n    C --> D2[Key-Value Store]\n```\n\n----------------------------------------\n\nTITLE: Running Linting Commands for Apify Documentation\nDESCRIPTION: These bash commands show how to run linting checks for Markdown and code files in the Apify documentation project.\nSOURCE: https://github.com/apify/apify-docs/blob/master/CONTRIBUTING.md#2025-04-18_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm run lint:md # Checks for any issues using markdownlint\nnpm run lint:md:fix # Applies fixes\n\nnpm run lint:code # Checks .js & .ts files\nnpm run lint:code:fix # Applies fixes\n```\n\n----------------------------------------\n\nTITLE: Initializing NPM Project\nDESCRIPTION: Creates a new npm project by generating a package.json file with default settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/project_setup.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm init -y\n```\n\n----------------------------------------\n\nTITLE: Evaluating In-Browser Code with Puppeteer Scraper in JavaScript\nDESCRIPTION: Demonstrates how to execute JavaScript code in the browser environment and retrieve the result using Puppeteer Scraper. This snippet shows accessing the page's body HTML.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst bodyHTML = await context.page.evaluate(() => {\n    console.log('This will be printed in browser console.');\n    return document.body.innerHTML;\n});\n```\n\n----------------------------------------\n\nTITLE: Typing Text into an Input Field\nDESCRIPTION: Illustrates how to type 'hello world' into Google's search box using both Puppeteer and Playwright.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Type the query into the search box\nawait page.type('textarea[title]', 'hello world');\n```\n\n----------------------------------------\n\nTITLE: Exit Event Handlers in JavaScript\nDESCRIPTION: Shows how to register and handle exit events in JavaScript implementation of the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Register a handler to be called on exit.\n// Note that the handler has `timeoutSecs` to finish its job.\nActor.on('exit', ({ statusMessage, exitCode, timeoutSecs }) => {\n    // Perform cleanup...\n});\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Warranty Section in Markdown\nDESCRIPTION: Section containing warranty disclaimers and limitations regarding platform functionality and service quality\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/old/general-terms-and-conditions-2022.md#2025-04-18_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n## 7. Warranty\n\nWE MAKE NO REPRESENTATION, WARRANTY, OR GUARANTY AS TO THE RELIABILITY, TIMELINESS, QUALITY, SUITABILITY, AVAILABILITY, ACCURACY OR COMPLETENESS...\n```\n\n----------------------------------------\n\nTITLE: Pulling an Actor from Apify Platform\nDESCRIPTION: Command to pull an Actor's source code from the Apify platform to the local machine.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_web_ide.md#2025-04-18_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\napify pull your-actor-name\n```\n\n----------------------------------------\n\nTITLE: Printing 'Hello World' in JavaScript\nDESCRIPTION: This simple JavaScript code prints 'Hello World' to the console. It's used to verify that Node.js is correctly installed and can execute JavaScript files.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/computer_preparation.md#2025-04-18_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconsole.log('Hello World');\n```\n\n----------------------------------------\n\nTITLE: Status Flag Examples in Markdown\nDESCRIPTION: Example status flags used in the Apify system to mark Actor states during testing phases.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/testing.mdx#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`under maintenance`\n```\n\nLANGUAGE: markdown\nCODE:\n```\n`deprecate`\n```\n\n----------------------------------------\n\nTITLE: Saving Plot to Apify Key-Value Store\nDESCRIPTION: This code saves the generated plot as a PNG image to the Apify key-value store and prints the URL where the result can be accessed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkey_value_store_client = client.key_value_store(os.environ['APIFY_DEFAULT_KEY_VALUE_STORE_ID'])\n\nprint('Saving plot to key-value store...')\nwith BytesIO() as buf:\n    axes.figure.savefig(buf, format='png', dpi=200, facecolor='w')\n    buf.seek(0)\n    key_value_store_client.set_record('prediction.png', buf, 'image/png')\n\nprint(f'Result is available at {os.environ[\"APIFY_API_PUBLIC_BASE_URL\"]}'\n      + f'/v2/key-value-stores/{os.environ[\"APIFY_DEFAULT_KEY_VALUE_STORE_ID\"]}/records/prediction.png')\n```\n\n----------------------------------------\n\nTITLE: Scraping Weather Data from BBC Weather in Python\nDESCRIPTION: This code snippet iterates through a list of locations, scrapes weather forecast data from BBC Weather for each location, and processes the data to extract timezone, date, and hourly temperature information.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Scrape each location separately\nfor (location_name, location_id) in LOCATIONS:\n    print(f'Scraping weather from {location_name}...')\n    location_timezone = None\n    first_displayed_date = None\n    for day_offset in range(14):\n        # Get the BBC Weather page for the given location and day and parse it with BeautifulSoup\n        response = requests.get(f'https://www.bbc.com/weather/{location_id}/day{day_offset}')\n        soup = BeautifulSoup(response.content, 'html.parser')\n```\n\n----------------------------------------\n\nTITLE: React Component Import Declaration\nDESCRIPTION: Import statements for React card components used in the documentation layout.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/index.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport Card from \"@site/src/components/Card\";\nimport CardGrid from \"@site/src/components/CardGrid\";\n```\n\n----------------------------------------\n\nTITLE: RegExp Pattern Matching for URLs\nDESCRIPTION: Example of filtering URLs using regular expressions to match specific product page patterns.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/filtering_links.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nfor (const a of document.querySelectorAll('a')) {\n    const regExp = /https?:\\/\\/warehouse-theme-metal\\.myshopify\\.com\\/products\\/[\\w-]+/;\n    const url = a.href;\n    if (regExp.test(url)) console.log(url);\n}\n```\n\n----------------------------------------\n\nTITLE: Input Section Reference Example\nDESCRIPTION: Example text for referring to the Actor's input configuration options.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/actor_basics/how-to-create-actor-readme.md#2025-04-18_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nTwitter Scraper has the following input options. Click on the input tab for more information.\n```\n\n----------------------------------------\n\nTITLE: Robust Payment Submission with Verification in JavaScript\nDESCRIPTION: Implements a robust payment submission process with proper verification using Promise.all for navigation and waitForFunction for success confirmation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nasync function submitPayment() {\n    await Promise.all([\n        page.click('submitPayment'),\n        page.waitForNavigation(),\n    ]);\n\n    try {\n        await page.waitForFunction(\n            (selector) => document.querySelector(selector).innerText.includes('Payment Success'),\n            { polling: 'mutation' },\n            '#PaymentOutcome',\n        );\n    } catch (error) {\n        return OUTPUT.paymentFailure;\n    }\n\n    return OUTPUT.paymentSuccess;\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Dataset API Endpoint with API Token\nDESCRIPTION: Example of the API endpoint URL structure used to retrieve a list of datasets. This requires authentication using an API token from the Integrations tab of your Apify account settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/datasets\n```\n\n----------------------------------------\n\nTITLE: Displaying Version History Table in Markdown\nDESCRIPTION: A markdown table showing the version history of Apify's General Terms and Conditions, including version links, effective dates, and expiration dates.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/latest/terms/general-terms-and-conditions.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version                                                    | Effective from  | Effective until    |\n|------------------------------------------------------------|-----------------|-----------------|\n| Latest (this document)                                     | May 14, 2024    |                    |\n| [Oct 2022](../../old/general-terms-and-conditions-2022.md) | October 1, 2022 | June 13, 2024      |\n| Older T&Cs available upon request                          |                 | September 30, 2022 |\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: YAML front matter configuration for the documentation page, specifying metadata like title, description, sidebar position and display settings.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/index.mdx#2025-04-18_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Apify Terms and Policies\ndescription: This is an index of Apify's public facing policies, terms of use and legal documents.\nsidebar_position: 0\ndisplayed_sidebar: legal\nslug: /\nhide_table_of_contents: true\n---\n```\n\n----------------------------------------\n\nTITLE: Corrected COPY Instruction With Ownership Specification\nDESCRIPTION: Fixed version of the COPY instruction using the --chown flag to specify the correct user and group for copied files.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/avoid_eacces_error_in_actor_builds.md#2025-04-18_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nCOPY --chown=myuser:myuser . ./\n```\n\n----------------------------------------\n\nTITLE: URL Pattern Examples for Breweries and Beers\nDESCRIPTION: These examples show the typical URL structure for brewery and beer pages that can be targeted for scraping.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttp://www.brewbound.com/breweries/[BREWERY_NAME]\n```\n\nLANGUAGE: text\nCODE:\n```\nhttp://www.brewbound.com/breweries/[BREWERY_NAME]/[BEER_NAME]\n```\n\n----------------------------------------\n\nTITLE: Dataset Items Retrieval Endpoint\nDESCRIPTION: HTTP GET endpoint for retrieving items from a dataset, with support for various output formats and pagination.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/api/getting-started.mdx#2025-04-18_snippet_5\n\nLANGUAGE: http\nCODE:\n```\nGET https://api.apify.com/v2/datasets/:datasetId/items\n```\n\n----------------------------------------\n\nTITLE: Accessing Shadow DOM Content with jQuery\nDESCRIPTION: This snippet demonstrates how to access content within a shadow DOM using jQuery. It finds the shadow root element, creates a copy of its HTML, and extracts links from it.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_shadow_doms.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Find element that is shadow root of menu DOM tree.\nconst { shadowRoot } = document.getElementById('top-navbar-view');\n\n// Create a copy of its HTML and use jQuery find links.\nconst links = $(shadowRoot.innerHTML).find('a');\n\n// Get URLs from link elements.\nconst urls = links.map((obj, el) => el.href);\n```\n\n----------------------------------------\n\nTITLE: Apify Console Navigation Tabs Description\nDESCRIPTION: A markdown table describing the purpose of each tab in the Apify Console navigation menu. This table explains what features and functionalities are available in each section.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/console/index.md#2025-04-18_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Tab name | Description |\n|:---|:---|\n| [Store](/sources/platform/console/store.md)| Search for Actors that suit your web-scraping needs. |\n| [Actors](/sources/platform/actors/index.mdx)| View your recent runs, Actors developed by you, rented Actors, analytics & issues, as well as:<br/><br/> &bull; Saved tasks - where you can view your saved tasks or change any settings within them.<br/> &bull; Runs - where you can check on your previous runs. |\n| [Schedules](/sources/platform/schedules.md)| Schedule Actor runs & tasks to run at specified time. |\n| [Storage](/sources/platform/storage/index.md)| View stored results of your runs in various data formats. |\n| [Proxy](/sources/platform/proxy/index.md)| View your proxy usage & credentials |\n| [Settings](/sources/platform/console/settings.md)| Settings of your account. |\n| [Billing](/sources/platform/console/billing.md)| Billing information, statistics and invoices. |\n```\n\n----------------------------------------\n\nTITLE: Version History Table in Markdown\nDESCRIPTION: Markdown table showing different versions of Apify's terms and conditions with their effective dates.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/old/general-terms-and-conditions-2022.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version                                                   | Effective from  | Effective until    |\n|-----------------------------------------------------------|-----------------|--------------------|\\n| [Latest](../latest/terms/general-terms-and-conditions.md) | May 13, 2024    |                    |\n| Oct 2022 (This document)                                  | October 1, 2022 | June 12, 2024      |\n| Older T&Cs available upon request                         |                 | September 30, 2022 |\n```\n\n----------------------------------------\n\nTITLE: Python Actor Integration Example\nDESCRIPTION: Complete example showing how to run an Actor and retrieve results using the Python Apify client library.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom apify_client import ApifyClient\nclient = ApifyClient(token='YOUR_API_TOKEN')\n\nrun_input = {\n    \"queries\": \"Food in NYC\",\n}\n\n# Run the Actor and wait for it to finish\n# .call method waits infinitely long using smart polling\n# Get back the run API object\nrun = client.actor(\"apify/google-search-scraper\").call(run_input=run_input)\n\n# Fetch and print Actor results from the run's dataset (if there are any)\nfor item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n    print(item)\n```\n\n----------------------------------------\n\nTITLE: Basic Function Without Type Annotations\nDESCRIPTION: Example of a function written without TypeScript type annotations.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types.md#2025-04-18_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst totalLengthIsGreaterThan10 = (string1, string2) => {\n    // Returns true if the total length of both strings is greater\n    // than 10, and false if it's less than 10.\n    return (string1 + string2).length > 10;\n};\n```\n\n----------------------------------------\n\nTITLE: HTML Image Element with Attributes\nDESCRIPTION: Shows how to create an image element with src and alt attributes to specify the image source and alternative text.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/html_elements.md#2025-04-18_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<img src=\"image.jpg\" alt=\"A description of the image\">\n```\n\n----------------------------------------\n\nTITLE: Dataset Storage Path Configuration\nDESCRIPTION: Shows the default storage path where Crawlee saves the extracted data as JSON files.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n./storage/datasets/default/*.json\n```\n\n----------------------------------------\n\nTITLE: Google Search Query for Finding Relevant Reddit Threads\nDESCRIPTION: A search query format to find relevant Reddit threads related to your Actor's keyword. This helps identify opportunities for promoting your Actor through helpful comments.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/promote_your_actor/parasite_seo.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nsite:reddit.com <your keyword>\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Dependencies\nDESCRIPTION: Command to install Playwright package as a dependency for the web scraping project.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/headless_browser.md#2025-04-18_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install playwright\n```\n\n----------------------------------------\n\nTITLE: Installing Apify Python SDK\nDESCRIPTION: Command to install the Apify Python SDK using pip package manager\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/pinecone.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install apify-client\n```\n\n----------------------------------------\n\nTITLE: Markdown Informational Block for Actor Usage Cost\nDESCRIPTION: A markdown info block explaining how to estimate Actor usage costs and referencing the platform's free plan for testing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/store.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::info Estimating Actor usage cost\n\nWith this model, it's very easy to see how many platform resources each Actor run consumed, but it is quite difficult to estimate their usage beforehand. The best way to find the costs of free Actors upfront is to try out the Actor on a limited scope (for example, on a small number of pages) and evaluate the consumption. You can easily do that using our [free plan](https://apify.com/pricing).\n\n_For more information on platform usage cost see the [usage and resources](./usage_and_resources.md) page._\n\n:::\n```\n\n----------------------------------------\n\nTITLE: Basic HTML Paragraph Element\nDESCRIPTION: Demonstrates the basic structure of an HTML paragraph element with opening and closing tags.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/glossary/concepts/html_elements.md#2025-04-18_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<p>This is a paragraph of text.</p>\n```\n\n----------------------------------------\n\nTITLE: Basic HTML Element Structure Example\nDESCRIPTION: Demonstrates the basic structure of HTML elements including tags, attributes, and nested elements with text content.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/01_devtools_inspecting.md#2025-04-18_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<article id=\"article-123\">\n  <h1 class=\"heading\">First Level Heading</h1>\n  <p>Paragraph with <em>emphasized text</em>.</p>\n</article>\n```\n\n----------------------------------------\n\nTITLE: Calculating Profit in Pay-per-result Pricing Model\nDESCRIPTION: Formula for calculating profit in the pay-per-result pricing model, where profit equals 80% of revenue minus platform usage costs.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/monetizing_your_actor.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`(0.8 * revenue) - costs = profit`\n```\n\n----------------------------------------\n\nTITLE: Implementing ACTOR_MAX_PAID_DATASET_ITEMS Check\nDESCRIPTION: Reference to implementing a check for ACTOR_MAX_PAID_DATASET_ITEMS in Actors to prevent excess result generation, which is a best practice for pay-per-result Actors.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/monetizing_your_actor.md#2025-04-18_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n`ACTOR_MAX_PAID_DATASET_ITEMS`\n```\n\n----------------------------------------\n\nTITLE: Legal Boilerplate Text for Web Scraping\nDESCRIPTION: Standard disclaimer text about the legality and ethics of web scraping, including GDPR considerations and personal data handling.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/actor_basics/how-to-create-actor-readme.md#2025-04-18_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nOur scrapers are ethical and do not extract any private user data, such as email addresses, gender, or location. They only extract what the user has chosen to share publicly. We therefore believe that our scrapers, when used for ethical purposes by Apify users, are safe. However, you should be aware that your results could contain personal data. Personal data is protected by the GDPR in the European Union and by other regulations around the world. You should not scrape personal data unless you have a legitimate reason to do so. If you're unsure whether your reason is legitimate, consult your lawyers. You can also read our blog post on the legality of web scraping\n```\n\n----------------------------------------\n\nTITLE: Splitting Price Filters in JavaScript\nDESCRIPTION: This utility function splits a price filter into two new filters, handling cases where the maximum price is null (open-ended range).\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-with-search.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// utils.js\nexport function splitFilter(filter) {\n    const { min, max } = filter;\n    // Don't forget that max can be null and we have to handle that situation\n    if (max && min > max) {\n        throw new Error(`WRONG FILTER - min(${min}) is greater than max(${max})`);\n    }\n\n    // We crate a middle value for the split. If max in null, we will use double min as the middle value\n    const middle = max\n        ? min + Math.floor((max - min) / 2)\n        : min * 2;\n\n    // We have to do the Math.max and Math.min to prevent having min > max\n    const filterMin = {\n        min,\n        max: Math.max(middle, min),\n    };\n    const filterMax = {\n        min: max ? Math.min(middle + 1, max) : middle + 1,\n        max,\n    };\n    // We return 2 new filters\n    return [filterMin, filterMax];\n}\n```\n\n----------------------------------------\n\nTITLE: Wikipedia Calling Codes Scraper\nDESCRIPTION: Solution for extracting calling codes from Wikipedia pages of African countries. Demonstrates crawling through country links and parsing infobox data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n    return BeautifulSoup(response.text, \"html.parser\")\n\ndef parse_calling_code(soup):\n    for label in soup.select(\"th.infobox-label\"):\n        if label.text.strip() == \"Calling code\":\n            data = label.parent.select_one(\"td.infobox-data\")\n            return data.text.strip()\n    return None\n\nlisting_url = \"https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_in_Africa\"\nlisting_soup = download(listing_url)\nfor name_cell in listing_soup.select(\".wikitable tr td:nth-child(3)\"):\n    link = name_cell.select_one(\"a\")\n    country_url = urljoin(listing_url, link[\"href\"])\n    country_soup = download(country_url)\n    calling_code = parse_calling_code(country_soup)\n    print(country_url, calling_code)\n```\n\n----------------------------------------\n\nTITLE: Implementing CheerioCrawler for Data Extraction\nDESCRIPTION: Expands on the initial CheerioCrawler setup to extract product data. This version still doesn't handle dynamic content correctly.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioCrawler } from 'crawlee';\n\nconst BASE_URL = 'https://demo-webstore.apify.org';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request }) => {\n        const products = $('a[href*=\"/product/\"]');\n\n        const results = [...products].map((product) => {\n            const elem = $(product);\n\n            const title = elem.find('h3').text();\n            const price = elem.find('div[class*=\"price\"]').text();\n            const image = elem.find('img[src]').attr('src');\n\n            return {\n                title,\n                price,\n                image: new URL(image, BASE_URL).href,\n            };\n        });\n\n        console.log(results);\n    },\n});\n\nawait crawler.run([{ url: 'https://demo-webstore.apify.org/search/new-arrivals' }]);\n```\n\n----------------------------------------\n\nTITLE: Complete Web Scraping with CSV Export\nDESCRIPTION: Full implementation of web scraping with CSV conversion, including cheerio for HTML parsing and got-scraping for HTTP requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/save_to_csv.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv'; // <---- added a new import\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n\n    const priceElement = $(product).find('span.price');\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n\nconst csv = parse(results); // <---- added parsing of results to CSV\nconsole.log(csv);\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Split Filters in JavaScript\nDESCRIPTION: This snippet demonstrates how to use the splitFilter function to create new filter requests and enqueue them for further processing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-with-search.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst { min, max } = getFiltersFromUrl(request.url);\n// Our generic splitFilter function doesn't account for decimal values so we will have to convert to cents and back to dollars\nconst newFilters = splitFilter({ min: min * 100, max: max * 100 });\n\n// And we enqueue those 2 new filters so the process will recursively repeat until all pages get to the PAGINATION phase\nconst requestsToEnqueue = [];\nfor (const filter of newFilters) {\n    requestsToEnqueue.push({\n        // Remember that we have to convert back from cents to dollars\n        url: createFilterUrl({ min: filter.min / 100, max: filter.max / 100 }),\n        label: 'FILTER',\n    });\n}\n\nawait crawler.addRequests(requestsToEnqueue);\n```\n\n----------------------------------------\n\nTITLE: Handling Readiness Probe in Python Actor Standby\nDESCRIPTION: This snippet shows how to handle the readiness probe in a Python Actor using Standby mode. It checks for the presence of a specific header to distinguish between normal requests and readiness probe requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/actor_standby.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom http.server import HTTPServer, SimpleHTTPRequestHandler\nfrom apify import Actor\n\n\nclass GetHandler(SimpleHTTPRequestHandler):\n    def do_GET(self) -> None:\n        self.send_response(200)\n        self.end_headers()\n        if self.headers['x-apify-container-server-readiness-probe']:\n            print('Readiness probe')\n            self.wfile.write(b'Hello, readiness probe!')\n        else:\n            print('Normal request')\n            self.wfile.write(b'Hello, normal request!')\n\n\nasync def main() -> None:\n    async with Actor:\n        with HTTPServer(('', Actor.config.standby_port), GetHandler) as http_server:\n            http_server.serve_forever()\n```\n\n----------------------------------------\n\nTITLE: Implementing Page Function for Apify Store Scraper in JavaScript\nDESCRIPTION: JavaScript function that processes each page the scraper visits. It handles different page types based on their labels, logs information, and returns scraped data from detail pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_5\n\nLANGUAGE: js\nCODE:\n```\nasync function pageFunction(context) {\n    const { request, log, skipLinks } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        log.info(`Scraping ${request.url}`);\n        await skipLinks();\n        // Do some scraping.\n        return {\n            // Scraped data.\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nginx Server for Apify Documentation\nDESCRIPTION: This nginx configuration sets up a server to proxy requests to different Apify documentation repositories running on various ports.\nSOURCE: https://github.com/apify/apify-docs/blob/master/CONTRIBUTING.md#2025-04-18_snippet_0\n\nLANGUAGE: nginx\nCODE:\n```\nserver {\n  listen       80;\n  server_name  docs.apify.loc;\n  location / {\n    proxy_pass http://localhost:3000;\n  }\n  location /api/client/js {\n    proxy_pass http://localhost:3001;\n  }\n  location /api/client/python {\n    proxy_pass http://localhost:3002;\n  }\n  location /sdk/js {\n    proxy_pass http://localhost:3003;\n  }\n  location /sdk/python {\n    proxy_pass http://localhost:3004;\n  }\n  location /cli {\n    proxy_pass http://localhost:3005;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Amazon Offer Data with Crawlee\nDESCRIPTION: This snippet demonstrates how to extract offer data from Amazon product pages. It iterates through offer elements, extracting seller names and prices, then pushes the data to a dataset.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    for (const offer of $('#aod-offer')) {\n        const element = $(offer);\n\n        await Dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n        });\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Replicating Intercepted Request with request-promise in Node.js\nDESCRIPTION: This snippet shows how to convert an intercepted Puppeteer request into a request-promise options object and resend it to download the file. It includes adding cookies to the request.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport request from 'request-promise';\n\nconst options = {\n    encoding: null,\n    method: xRequest._method,\n    uri: xRequest._url,\n    body: xRequest._postData,\n    headers: xRequest._headers,\n};\n\n// Add the cookies\nconst cookies = await page.cookies();\noptions.headers.Cookie = cookies.map((ck) => `${ck.name}=${ck.value}`).join(';');\n\n// Resend the request\nconst response = await request(options);\n```\n\n----------------------------------------\n\nTITLE: Handling System Events in Python Apify Actor\nDESCRIPTION: This snippet shows how to handle system events in a Python Apify Actor. It demonstrates adding event handlers, removing all handlers for a specific event, and removing a specific event handler.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/system_events.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\nfrom apify_shared.consts import ActorEventTypes\n\ndef handler_foo(arg: dict):\n    Actor.log.info(f'handler_foo: arg = {arg}')\n\ndef handler_boo(arg: dict):\n    pass\n\nasync def main():\n    async with Actor:\n        # Add event handler\n        Actor.on(ActorEventTypes.ABORTING, handler_foo)\n\n        # Remove all handlers for a specific event\n        Actor.off('systemInfo')\n\n        # Remove a specific event handler\n        Actor.off('systemInfo', handler_boo)\n```\n\n----------------------------------------\n\nTITLE: Configuring Pinecone Integration\nDESCRIPTION: Python code for setting up and executing the Pinecone integration with various configuration parameters for data storage and chunking\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/pinecone.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npinecone_integration_inputs = {\n    \"pineconeApiKey\": PINECONE_API_KEY,\n    \"pineconeIndexName\": PINECONE_INDEX_NAME,\n    \"datasetFields\": [\"text\"],\n    \"datasetId\": actor_call[\"defaultDatasetId\"],\n    \"enableDeltaUpdates\": True,\n    \"deltaUpdatesPrimaryDatasetFields\": [\"url\"],\n    \"deleteExpiredObjects\": True,\n    \"expiredObjectDeletionPeriodDays\": 30,\n    \"embeddingsApiKey\": OPENAI_API_KEY,\n    \"embeddingsProvider\": \"OpenAI\",\n    \"performChunking\": True,\n    \"chunkSize\": 1000,\n    \"chunkOverlap\": 0,\n}\n\nactor_call = client.actor(\"apify/pinecone-integration\").call(run_input=pinecone_integration_inputs)\nprint(\"Apify's Pinecone Integration has finished\")\nprint(actor_call)\n```\n\n----------------------------------------\n\nTITLE: Initializing PuppeteerCrawler with Custom Launch Function\nDESCRIPTION: Sets up a PuppeteerCrawler instance with a custom launch function that manages session creation and user agent configuration. The function picks a session and embeds the session name in the user agent string.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new Apify.PuppeteerCrawler({\n    launchPuppeteerFunction: async () => {\n        const session = pickSession(sessions);\n        return Apify.launchPuppeteer({\n            useApifyProxy: true,\n            userAgent: `${session.userAgent} s=${session.name}`,\n            apifyProxySession: session.name,\n        });\n    },\n    // handlePageFunction etc.\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Actor Standby Mode in Python\nDESCRIPTION: This snippet shows how to create a simple HTTP server for an Actor in Standby mode using Python. It uses the HTTPServer class and listens on the port specified by the Actor configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/actor_standby.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom http.server import HTTPServer, SimpleHTTPRequestHandler\nfrom apify import Actor\n\nclass GetHandler(SimpleHTTPRequestHandler):\n    def do_GET(self):\n        self.send_response(200)\n        self.end_headers()\n        self.wfile.write(b'Hello from Actor Standby!')\n\nasync def main() -> None:\n    async with Actor:\n        with HTTPServer(('', Actor.config.web_server_port), GetHandler) as http_server:\n            http_server.serve_forever()\n```\n\n----------------------------------------\n\nTITLE: Basic SoundCloud API Endpoint\nDESCRIPTION: Example of a SoundCloud API endpoint URL that returns 20 latest tracks for a specific user. Shows default parameters including client_id, limit, offset, and other configuration options.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/locating_and_learning.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nhttps://api-v2.soundcloud.com/users/141707/tracks?representation=&client_id=zdUqm51WRIAByd0lVLntcaWRKzuEIB4X&limit=20&offset=0&linked_partitioning=1&app_version=1646987254&app_locale=en\n```\n\n----------------------------------------\n\nTITLE: Python Actor Implementation\nDESCRIPTION: Implementation of an Actor using Python with custom input handling and Apify Client.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# index.py\nfrom apify_client import ApifyClient\nfrom os import environ\nimport json\n\nclient = ApifyClient(token='YOUR_TOKEN')\n\n# If being run on the platform, the \"APIFY_IS_AT_HOME\" environment variable\n# will be \"1\". Otherwise, it will be undefined/None\ndef is_on_apify ():\n    return 'APIFY_IS_AT_HOME' in environ\n\n# Get the input\ndef get_input ():\n    if not is_on_apify():\n        with open('./apify_storage/key_value_stores/default/INPUT.json') as actor_input:\n            return json.load(actor_input)\n\n    kv_store = client.key_value_store(environ.get('APIFY_DEFAULT_KEY_VALUE_STORE_ID'))\n    return kv_store.get_record('INPUT')['value']\n\ndef add_all_numbers (nums):\n    total = 0\n\n    for num in nums:\n        total += num\n\n    return total\n\nactor_input = get_input()['numbers']\n\nsolution = add_all_numbers(actor_input)\n\nprint(solution)\n```\n\n----------------------------------------\n\nTITLE: Handling System Events in JavaScript Apify Actor\nDESCRIPTION: This snippet demonstrates how to handle system events in a JavaScript Apify Actor. It shows how to add event handlers, remove specific handlers, and remove all handlers for a particular event.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/system_events.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// Add event handler\nActor.on('cpuInfo', (data) => {\n    if (data.isCpuOverloaded) console.log('Oh no, we need to slow down!');\n});\n\n// Remove all handlers for a specific event\nActor.off('systemInfo');\n\n// Remove a specific event handler\nActor.off('systemInfo', handler);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Handling Different Request Labels in handlePageFunction\nDESCRIPTION: Demonstrates how to implement conditional logic in the handlePageFunction based on request labels. This allows for executing different code blocks depending on the type of page being processed.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_1\n\nLANGUAGE: js\nCODE:\n```\nif (request.userData.label === 'START') {\n    // your code for the first request for example\n    // enqueue the items of a shop\n} else if (request.userData.label === 'ITEM') {\n    // other code for the item of a shop\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Product Data with Node.js and Cheerio\nDESCRIPTION: This code snippet shows how to loop through product elements, extract title and price information, and store the results in an array using Node.js and Cheerio.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_continued.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = [];\n\nfor (const product of products) {\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n\n    const priceElement = $(product).find('span.price');\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n```\n\n----------------------------------------\n\nTITLE: Using Apify Proxy with JavaScript SDK and PuppeteerCrawler\nDESCRIPTION: This code snippet demonstrates how to use Apify Proxy with the JavaScript SDK and PuppeteerCrawler. It creates a proxy configuration and uses it with a crawler to access a website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/index.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ page }) {\n        console.log(await page.content());\n    },\n});\n\nawait crawler.run(['https://proxy.apify.com/?format=json']);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Input Schema Structure in JSON\nDESCRIPTION: The initial structure of an INPUT_SCHEMA.json file that defines the title and description of the schema. This specifies what the input schema is for and provides basic information about the Actor's purpose.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/input_schema.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Adding Actor input\",\n    \"description\": \"Add all values in list of numbers with an arbitrary length.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Actor in JavaScript\nDESCRIPTION: Demonstrates how to initialize an Apify Actor using both the init()/exit() pattern and the main() function wrapper. The main() function is recommended for environments without top-level await support.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nawait Actor.init();\nconsole.log('Actor starting...');\n// ...\nawait Actor.exit();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Actor } from 'apify';\n\nActor.main(async () => {\n    console.log('Actor starting...');\n    // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Constants for Amazon Scraper\nDESCRIPTION: This code defines constants used throughout the Amazon scraper, including the base URL and labels for different types of pages to be scraped.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nexport const BASE_URL = 'https://www.amazon.com';\n\nexport const labels = {\n    START: 'START',\n    PRODUCT: 'PRODUCT',\n    OFFERS: 'OFFERS',\n};\n```\n\n----------------------------------------\n\nTITLE: Privacy Policy Section Headers in Markdown\nDESCRIPTION: Markdown headers defining the structure of the privacy policy document, including sections for data disclosure, retention, and user rights.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/latest/policies/privacy-policy.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## How We Disclose Your Personal Data\n## How We Retain and Dispose Your Personal Data\n## Your Rights and Your Choices\n### Correcting, Updating, and Accessing\n### Removal and Objection\n## Third-Party Links and Features\n## International Transfer of Your Personal Data\n## How We Protect Your Personal Data\n## Children and Privacy\n## Aggregate Data\n## Territory-Specific Terms\n### EEA and the UK\n```\n\n----------------------------------------\n\nTITLE: Markdown HTML Comment\nDESCRIPTION: A markdown comment indicating a TODO item for adding a screenshot from Apify Store on Web\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/store.md#2025-04-18_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[//]: # (TODO: also show the screenshot from Apify Store on Web)\n```\n\n----------------------------------------\n\nTITLE: Simplified Web Scraper Code Snippet for Browser Console Testing\nDESCRIPTION: A simple example showing how to modify a Web Scraper code snippet for testing in browser console by removing const declarations to allow re-running the same code multiple times.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = [];\n// Scraping something to fill the results\n```\n\n----------------------------------------\n\nTITLE: Markdown Header Configuration\nDESCRIPTION: YAML frontmatter configuration for the documentation page, defining metadata like title, description, sidebar details and category.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/latest/policies/whistleblowing-policy.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Apify Whistleblowing Policy\ndescription: Apify's whistleblowing policy describes how illegal activities can be reported, as required by law.\nsidebar_label: Whistleblowing Policy\nsidebar_position: 5\ncategory: legal\nslug: /whistleblowing-policy\n---\n```\n\n----------------------------------------\n\nTITLE: Initializing NPM Project\nDESCRIPTION: Command to create a new npm project with default settings using the -y flag for automatic confirmation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm init -y\n```\n\n----------------------------------------\n\nTITLE: Defining API Schema in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define an API schema for a resource with an ID property, using OpenAPI specification.\nSOURCE: https://github.com/apify/apify-docs/blob/master/CONTRIBUTING.md#2025-04-18_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntype: object\nproperties:\n  id:\n    description: The resource ID\n    readOnly: true\n    allOf:\n      -$ref: ./ResourceId.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Flowise Globally with npm\nDESCRIPTION: This command installs Flowise globally on your device using npm. It allows you to use Flowise from any directory in your terminal.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/flowise.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g flowise\n```\n\n----------------------------------------\n\nTITLE: Initializing Apify Actor in Python\nDESCRIPTION: Shows how to initialize an Apify Actor in Python using an asynchronous context manager with the 'with' keyword.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        Actor.log.info('Actor starting...')\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Checking Run Status in JavaScript\nDESCRIPTION: This snippet demonstrates how to verify that an Actor run has succeeded using the Actor Testing Actor.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/automated_tests.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait expectAsync(runResult).toHaveStatus('SUCCEEDED');\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: Setting up API keys for OpenAI and Apify as environment variables\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n```\n\n----------------------------------------\n\nTITLE: Basic Playwright Web Scraper Implementation\nDESCRIPTION: Implementation of a web scraper using PlaywrightCrawler to extract product information from an e-commerce site. Demonstrates browser automation with visible UI and Cheerio parsing.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/headless_browser.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    headless: false,\n    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            return;\n        }\n\n        const $ = await parseWithCheerio();\n\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();\n```\n\n----------------------------------------\n\nTITLE: Managing Actor Run State for Continuity\nDESCRIPTION: Code to initialize and maintain state for parallel Actor runs, handling resurrection of previous runs or starting new ones based on the current state, ensuring continuity in case of Actor restarts.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Actor, log } from 'apify';\n\nconst { apifyClient } = Actor;\nconst state = await Actor.useState<State>('actor-state', { parallelRunIds: [], isInitialized: false });\n\nif (state.isInitialized) {\n    for (const runId of state.parallelRunIds) {\n        const runClient = apifyClient.run(runId);\n        const run = await runClient.get();\n\n        // This should happen if the run was deleted or the state was incorectly saved.\n        if (!run) throw new Error(`The run ${runId} from state does not exists.`);\n\n        if (run.status === 'RUNNING') {\n            log.info('Parallel run is already running.', { runId });\n        } else {\n            log.info(`Parallel run was in state ${run.status}, resurrecting.`, { runId });\n            await runClient.resurrect(targetActorRunOptions);\n        }\n    }\n} else {\n    for (let i = 0; i < parallelRunsCount; i++) {\n        const run = await Actor.start(targetActorId, {\n            ...targetActorInput,\n            datasetId: dataset.id,\n            requestQueueId: requestQueue.id,\n        }, targetActorRunOptions);\n        log.info(`Started parallel run with ID: ${run.id}`, { runId: run.id });\n        state.parallelRunIds.push(run.id);\n    }\n    state.isInitialized = true;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Proxy Server\nDESCRIPTION: Implementation showing how to properly configure proxy servers in both Playwright and Puppeteer using their respective configuration options.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/proxies.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { chromium } from 'playwright';\n\nconst proxy = '103.214.9.13:3128';\n\nconst browser = await chromium.launch({\n    headless: false,\n    // Using the \"proxy\" option\n    proxy: {\n        // Pass in the server URL\n        server: proxy,\n\n    },\n});\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport puppeteer from 'puppeteer';\n\nconst proxy = '103.214.9.13:3128';\n\n// Using the \"args\" option, which is an array of Chromium command\n// line switches, we pass the server URL in with \"--proxy-server\"\nconst browser = await puppeteer.launch({\n    headless: false,\n    args: [`--proxy-server=${proxy}`],\n});\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();\n```\n\n----------------------------------------\n\nTITLE: Wikipedia Subtitle HTML Example\nDESCRIPTION: Shows the HTML structure of Wikipedia's subtitle element with class and data attributes.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/01_devtools_inspecting.md#2025-04-18_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">\n  The Free Encyclopedia\n</strong>\n```\n\n----------------------------------------\n\nTITLE: Liability Section in Markdown\nDESCRIPTION: Section outlining the platform's liability limitations and user responsibilities\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/legal/old/general-terms-and-conditions-2022.md#2025-04-18_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## 6. Liability\n\nWe are not obliged to verify the manner in which you or other users use the Website, Platform, Configuration or Services...\n```\n\n----------------------------------------\n\nTITLE: JavaScript Object with '@' and '#' Properties for XML Conversion\nDESCRIPTION: Illustrates a JavaScript object using '@' and '#' properties, which are specially handled in XML conversion. '@' properties become XML attributes, and '#' provides the element value when there are no child elements.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n    address: [\n        {\n            '@': {\n                type: 'home',\n            },\n            street: '21st',\n            city: 'Chicago',\n        },\n        {\n            '@': {\n                type: 'office',\n            },\n            '#': 'unknown',\n        },\n    ],\n}\n```\n\n----------------------------------------\n\nTITLE: Scraping F1 Drivers Count with Python and Beautiful Soup\nDESCRIPTION: This exercise solution shows how to scrape and count the number of F1 drivers listed on the Formula 1 website.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://www.formula1.com/en/teams\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nprint(len(soup.select(\".f1-grid\")))\n```\n\n----------------------------------------\n\nTITLE: Setting Download Path with Chrome DevTools Protocol in Puppeteer\nDESCRIPTION: This snippet demonstrates how to set up a download path for Puppeteer using the Chrome DevTools Protocol. It allows specifying a folder where downloaded files will be saved.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst client = await page.target().createCDPSession();\nawait client.send('Page.setDownloadBehavior', { behavior: 'allow', downloadPath: './my-downloads' });\n```\n\n----------------------------------------\n\nTITLE: Creating Residential Proxy Configuration in Apify SDK (Python)\nDESCRIPTION: Shows how to configure the Apify SDK in Python to use residential proxies by setting the 'RESIDENTIAL' group in the proxy configuration.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # ...\n        proxy_configuration = await Actor.create_proxy_configuration(groups=['RESIDENTIAL'])\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Navigating to Actor Directory\nDESCRIPTION: Command to change directory to the newly created Actor project folder.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/quick_start/start_locally.md#2025-04-18_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd your-actor-name\n```\n\n----------------------------------------\n\nTITLE: Handling OpenAI Assistant Run Status and Tool Output Submission\nDESCRIPTION: This code checks the run status of the OpenAI Assistant and submits tool outputs if required, then prints the assistant's response.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nif run.status == \"requires_action\":\n    run = submit_tool_outputs(run)\n\nprint(\"Assistant response:\")\nfor m in client.beta.threads.messages.list(thread_id=run.thread_id):\n    print(m.content[0].text.value)\n```\n\n----------------------------------------\n\nTITLE: Request Listening Implementation\nDESCRIPTION: Code to listen for specific network requests by filtering URLs containing 'followings'. Demonstrates basic request monitoring.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Listen for all requests\npage.on('request', (req) => {\n    // If the URL doesn't include our keyword, ignore it\n    if (!req.url().includes('followings')) return;\n\n    console.log('Request for followers was made!');\n});\n```\n\n----------------------------------------\n\nTITLE: Querying the Vector Index\nDESCRIPTION: Performs a query against the vector index to retrieve relevant information with source attribution.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What is LangChain?\"\nresult = index.query_with_sources(query, llm=llm)\n\nprint(\"answer:\", result[\"answer\"])\nprint(\"source:\", result[\"sources\"])\n```\n\n----------------------------------------\n\nTITLE: Using Session with Puppeteer in Apify\nDESCRIPTION: Demonstrates how to use the picked session with Puppeteer in an Apify Actor, setting up the browser with the selected proxy session and user agent.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst session = pickSession(sessions);\nconst browser = await Apify.launchPuppeteer({\n    useApifyProxy: true,\n    apifyProxySession: session.name,\n    userAgent: session.userAgent,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Target ECMAScript Version in TypeScript\nDESCRIPTION: This snippet shows how to set the target ECMAScript version for TypeScript compilation using the 'target' option in tsconfig.json.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"outDir\": \"dist/\"\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}\n```\n\n----------------------------------------\n\nTITLE: API Endpoint Format for Fetching Actor Run Results\nDESCRIPTION: Example URL format to fetch items from a dataset generated during an Actor run. Uses the actorRunId from the event data and requires an authentication token.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/events.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/actor-runs/[ACTOR_RUN_ID]/dataset/items?token=[TOKEN]\n```\n\n----------------------------------------\n\nTITLE: Email Object for Multiple Sends\nDESCRIPTION: This code snippet defines an array of email objects to be sent. Each object contains recipient, subject, and body information for three different emails.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst emailsToSend = [\n    {\n        to: 'alice@example.com',\n        subject: 'Hello',\n        body: 'This is a message.',\n    },\n    {\n        to: 'bob@example.com',\n        subject: 'Testing',\n        body: 'I love the academy!',\n    },\n    {\n        to: 'carol@example.com',\n        subject: 'Apify is awesome!',\n        body: 'Some content.',\n    },\n];\n```\n\n----------------------------------------\n\nTITLE: Creating Intersection Types in TypeScript\nDESCRIPTION: This snippet demonstrates how to achieve similar functionality to interface extension using intersection types with the 'type' keyword. It creates an 'Employee' type that combines the 'Person' type with an additional 'occupation' property.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/interfaces.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\ntype Employee = Person & {\n    occupation: string;\n};\n```\n\n----------------------------------------\n\nTITLE: Basic Input Schema Structure in JSON\nDESCRIPTION: This snippet shows the basic structure of an Actor input schema. It includes the required fields like title, type, schemaVersion, and properties, as well as the optional required array.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"title\": \"Cheerio Crawler input\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": { /* define input fields here */ },\n    \"required\": []\n}\n```\n\n----------------------------------------\n\nTITLE: Saving Sessions to Key-Value Store in Node.js\nDESCRIPTION: Sets up an interval to periodically save the sessions object to a key-value store, ensuring persistence across Actor restarts.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nsetInterval(async () => {\n    await Apify.setValue('SESSIONS', sessions);\n}, 30 * 1000);\n```\n\n----------------------------------------\n\nTITLE: PuppeteerCrawler Pool Access Example\nDESCRIPTION: Demonstrates how to access PuppeteerPool through handlePageFunction for proxy management.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/handle_blocked_requests_puppeteer.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new PuppeteerCrawler({\n    requestList: someInitializedRequestList,\n    launchPuppeteerOptions: {\n        useApifyProxy: true,\n    },\n    handlePageFunction: async ({ request, page, puppeteerPool }) => {\n        // you are on the page now\n    },\n\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Beautiful Soup in Python\nDESCRIPTION: This snippet shows how to install the Beautiful Soup library using pip, which is necessary for parsing HTML in Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ pip install beautifulsoup4\n...\nSuccessfully installed beautifulsoup4-4.0.0 soupsieve-0.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Residential Proxy URL in JavaScript\nDESCRIPTION: Example of how to set up a residential proxy URL in JavaScript using the got-scraping library. The URL includes the required 'groups-RESIDENTIAL' parameter in the username portion.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst proxyUrl = 'http://groups-RESIDENTIAL:<YOUR_PROXY_PASSWORD>@proxy.apify.com:8000';\n```\n\n----------------------------------------\n\nTITLE: Rendering Card Components in JSX for Apify Documentation\nDESCRIPTION: This code snippet uses JSX to render a grid of Card components, which display various sections of the Apify documentation. It imports Card and CardGrid components, as well as content from a JSON file, to dynamically generate the documentation layout.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/index.mdx#2025-04-18_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport Card from \"@site/src/components/Card\";\nimport CardGrid from \"@site/src/components/CardGrid\";\nimport homepageContent from \"./homepage_content.json\";\n\n<CardGrid>\n    {\n        homepageContent.map(({ title, description, to }, i) => (\n            <Card\n                title={title}\n                desc={description}\n                to={to}\n                key={i}\n            />)\n        )\n    }\n</CardGrid>\n```\n\n----------------------------------------\n\nTITLE: Refactored Page Function with Separate Handlers\nDESCRIPTION: Shows how to restructure the page function code by separating different page type handlers into their own functions for better maintainability and readability.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nasync function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error('Unknown request label.');\n    }\n\n    async function handleStart({ log, page }) {\n        log.info('Store opened!');\n        let timeout; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await page.waitFor(buttonSelector, { timeout });\n                // 2 sec timeout after the first.\n                timeout = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            await page.click(buttonSelector);\n        }\n    }\n\n    async function handleDetail({\n        request,\n        log,\n        skipLinks,\n        page,\n    }) {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Get attributes in parallel to speed up the process.\n        const titleP = page.$eval(\n            'header h1',\n            (el) => el.textContent,\n        );\n        const descriptionP = page.$eval(\n            'header span.actor-description',\n            (el) => el.textContent,\n        );\n        const modifiedTimestampP = page.$eval(\n            'ul.ActorHeader-stats time',\n            (el) => el.getAttribute('datetime'),\n        );\n        const runCountTextP = page.$eval(\n            'ul.ActorHeader-stats > li:nth-of-type(3)',\n            (el) => el.textContent,\n        );\n\n        const [\n            title,\n            description,\n            modifiedTimestamp,\n            runCountText,\n        ] = await Promise.all([\n            titleP,\n            descriptionP,\n            modifiedTimestampP,\n            runCountTextP,\n        ]);\n\n        const modifiedDate = new Date(Number(modifiedTimestamp));\n        const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n        return {\n            url,\n            uniqueIdentifier,\n            title,\n            description,\n            modifiedDate,\n            runCount,\n        };\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Ad-hoc Webhook in Python SDK\nDESCRIPTION: Code snippet demonstrating how to create a webhook from within an Actor's Python code using the Apify SDK. This registers a webhook that triggers when the Actor run fails.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        await Actor.add_webhook(\n            event_types=['ACTOR.RUN.FAILED'],\n            request_url='https://example.com/run-failed',\n        )\n        # ...\n```\n\n----------------------------------------\n\nTITLE: Creating an ASPX Form Submission Utility Function in JavaScript\nDESCRIPTION: A utility function for Web Scraper that handles form submission on ASP.NET pages. The function serializes form data, adds submit button information, and manages AJAX posts for proper navigation through .ASPX pages.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_forms_on_aspx_pages.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst enqueueAspxForm = async function (request, formSelector, submitButtonSelector, async) {\n    request.payload = $(formSelector).serialize();\n    if ($(submitButtonSelector).length) {\n        request.payload += decodeURIComponent(`&${$(submitButtonSelector).attr('name')}=${$(submitButtonSelector).attr('value')}`);\n    }\n    request.payload += decodeURIComponent(`&__ASYNCPOST=${async.toString()}`);\n    request.method = 'POST';\n    request.uniqueKey = Math.random();\n    await context.enqueueRequest(request);\n    return request;\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Response Caching in Puppeteer\nDESCRIPTION: Basic implementation of response caching in Puppeteer that stores responses with cache-control headers in memory. Includes request interception and response handling logic to cache and serve cached responses.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/caching_responses_in_puppeteer.md#2025-04-18_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst cache = {};\n\nawait page.setRequestInterception(true);\n\npage.on('request', async (request) => {\n    const url = request.url();\n    if (cache[url] && cache[url].expires > Date.now()) {\n        await request.respond(cache[url]);\n        return;\n    }\n    request.continue();\n});\n\npage.on('response', async (response) => {\n    const url = response.url();\n    const headers = response.headers();\n    const cacheControl = headers['cache-control'] || '';\n    const maxAgeMatch = cacheControl.match(/max-age=(\\d+)/);\n    const maxAge = maxAgeMatch && maxAgeMatch.length > 1 ? parseInt(maxAgeMatch[1], 10) : 0;\n    if (maxAge) {\n        if (cache[url] && cache[url].expires > Date.now()) return;\n\n        let buffer;\n        try {\n            buffer = await response.buffer();\n        } catch (error) {\n            return;\n        }\n\n        cache[url] = {\n            status: response.status(),\n            headers: response.headers(),\n            body: buffer,\n            expires: Date.now() + (maxAge * 1000),\n        };\n    }\n});\n```\n\n----------------------------------------\n\nTITLE: Example async-await syntax reference\nDESCRIPTION: Reference to asynchronous programming syntax in JavaScript, mentioned as a prerequisite for the course\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/index.md#2025-04-18_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nasync...await\n```\n\n----------------------------------------\n\nTITLE: Authentication Request URL with Token Parameter\nDESCRIPTION: Example of how to authenticate a request to an Actor in Standby mode by including the token as a URL query parameter.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/actor_standby.md#2025-04-18_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nhttps://rag-web-browser.apify.actor/search?query=apify&token=my_apify_token\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Required Python packages specified in requirements.txt for web scraping with Beautiful Soup and making HTTP requests.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Add your dependencies here.\n# See https://pip.pypa.io/en/latest/cli/pip_install/#requirements-file-format\n# for how to format them\n\nbeautifulsoup4\nrequests\n```\n\n----------------------------------------\n\nTITLE: Implementing PuppeteerCrawler Navigation Function\nDESCRIPTION: Defines a goto function that extracts the session information from the user agent string, stores it in userData, and restores the original user agent. This enables session tracking across page navigation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst gotoFunction = async ({ request, page }) => {\n    const userAgentWithSession = await page.browser().userAgent();\n    const match = userAgentWithSession.match(/(.+) s=(.+)/);\n    const session = {\n        name: match[2],\n        userAgent: match[1],\n    };\n    request.userData.session = session;\n    await page.setUserAgent(session.userAgent);\n    return page.goto(request.url, { timeout: 60000 });\n};\n```\n\n----------------------------------------\n\nTITLE: Generating Fingerprints with fingerprint-generator (JavaScript)\nDESCRIPTION: This code snippet shows how to use the FingerprintGenerator class from the fingerprint-generator package to create browser fingerprints. It configures the generator with specific browser, device, and OS options, then generates a fingerprint.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { FingerprintGenerator } from 'fingerprint-generator';\n\n// Instantiate the fingerprint generator with\n// configuration options\nconst fingerprintGenerator = new FingerprintGenerator({\n    browsers: [\n        { name: 'firefox', minVersion: 80 },\n    ],\n    devices: [\n        'desktop',\n    ],\n    operatingSystems: [\n        'windows',\n    ],\n});\n\n// Grab a fingerprint from the fingerprint generator\nconst generated = fingerprintGenerator.getFingerprint({\n    locales: ['en-US', 'en'],\n});\n```\n\n----------------------------------------\n\nTITLE: Using Apify Proxy with Automatic Selection in PHP\nDESCRIPTION: This code demonstrates how to use Apify's proxy service with automatic proxy selection in PHP. It sets up a Guzzle HTTP client with the proxy configuration and makes a request through the proxy.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_12\n\nLANGUAGE: php\nCODE:\n```\n$client = new \\GuzzleHttp\\Client([\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    'proxy' => 'http://auto:<YOUR_PROXY_PASSWORD>@proxy.apify.com:8000'\n]);\n\n// This request will be made through an automatically chosen proxy\n$response = $client->get(\"http://proxy.apify.com/?format=json\");\necho $response->getBody();\n```\n\n----------------------------------------\n\nTITLE: Importing modules and setting up credentials for Apify-Qdrant integration\nDESCRIPTION: Python code that imports the ApifyClient and sets up the necessary API tokens and connection details for both Apify and Qdrant services.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/qdrant.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom apify_client import ApifyClient\n\nAPIFY_API_TOKEN = \"YOUR-APIFY-TOKEN\"\nOPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\"\n\nQDRANT_URL = \"YOUR-QDRANT-URL\"\nQDRANT_API_KEY = \"YOUR-QDRANT-API-KEY\"\nQDRANT_COLLECTION_NAME = \"YOUR-QDRANT-COLLECTION-NAME\"\n\nclient = ApifyClient(APIFY_API_TOKEN)\n```\n\n----------------------------------------\n\nTITLE: Implementing Proxy Configuration in Crawlee Scraper\nDESCRIPTION: This code demonstrates how to integrate the proxy configuration into a CheerioCrawler instance for automatic proxy rotation.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        if (request.label === 'START') {\n            await enqueueLinks({\n                selector: 'a[href*=\"/product/\"]',\n            });\n            return;\n        }\n\n        const title = $('h3').text().trim();\n        const price = $('h3 + div').text().trim();\n        const description = $('div[class*=\"Text_body\"]').text().trim();\n\n        await Dataset.pushData({\n            title,\n            description,\n            price,\n        });\n    },\n});\n```\n\n----------------------------------------\n\nTITLE: Clicking an Element with Puppeteer\nDESCRIPTION: Shows how to click the 'Accept all' button on Google's cookie policy using Puppeteer's CSS selector.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Click the \"Accept all\" button\nawait page.click('button + button');\n```\n\n----------------------------------------\n\nTITLE: Creating OpenAI Assistant with Web Browser Capabilities\nDESCRIPTION: This code creates an OpenAI Assistant with the specified instructions and RAG Web Browser function, using the GPT-4o-mini model.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmy_assistant = client.beta.assistants.create(\n    instructions=INSTRUCTIONS,\n    name=\"OpenAI Assistant with Web Browser\",\n    tools=[rag_web_browser_function],\n    model=\"gpt-4o-mini\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using Apify Proxy with Python SDK and requests library\nDESCRIPTION: This code snippet shows how to use Apify Proxy with the Python SDK and the requests library. It creates a proxy configuration, generates a proxy URL, and uses it to make an HTTP request.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/index.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests, asyncio\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        proxy_configuration = await Actor.create_proxy_configuration()\n        proxy_url = await proxy_configuration.new_url()\n\n        proxies = {\n            'http': proxy_url,\n            'https': proxy_url,\n        }\n\n        response = requests.get('https://api.apify.com/v2/browser-info', proxies=proxies)\n        print(response.text)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using the 'unknown' Type for Safer Type Handling\nDESCRIPTION: This example demonstrates how the 'unknown' type prevents direct assignment to other types without proper type checking, improving type safety.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nlet userInput: unknown;\nlet savedInput: string;\n\nuserInput = 'hello world!';\n\nsavedInput = userInput;\n```\n\n----------------------------------------\n\nTITLE: Looping Over Product Elements and Extracting Titles with JavaScript\nDESCRIPTION: This code uses a for...of loop to iterate through all product elements, extract the title of each product, and log it to the console.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nfor (const product of products) {\n    const titleElement = product.querySelector('a.product-item__title');\n    const title = titleElement.textContent.trim();\n    console.log(title);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright with npm\nDESCRIPTION: Command to install Playwright library using npm package manager.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/index.md#2025-04-18_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnpm install playwright\n```\n\n----------------------------------------\n\nTITLE: Selector-based Wait Using Web Scraper\nDESCRIPTION: Shows how to wait for a specific HTML element to appear on the page using Web Scraper's context.waitFor method with a CSS selector.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/waiting_for_dynamic_content.md#2025-04-18_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait context.waitFor('my-selector')\n```\n\n----------------------------------------\n\nTITLE: Data Fetching Implementation\nDESCRIPTION: Implementation of data fetching function using Axios with proper TypeScript type assertions.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport axios from 'axios';\nimport type { ResponseData } from './types';\n\nconst fetchData = async () => {\n    const { data } = await axios('https://dummyjson.com/products?limit=100');\n    return data as ResponseData;\n};\n```\n\n----------------------------------------\n\nTITLE: Fetching User Data from Apify API\nDESCRIPTION: Makes an API call to retrieve current user information and parse the JSON response.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_1\n\nLANGUAGE: php\nCODE:\n```\n$response = $client->get('users/me');\n$parsedResponse = \\json_decode($response->getBody(), true);\n$data = $parsedResponse['data'];\n\necho \\json_encode($data, JSON_PRETTY_PRINT);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Dataset Items via API\nDESCRIPTION: API endpoint for fetching the actual data items stored in a dataset. This endpoint can be customized with query parameters to filter the returned data.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nhttps://api.apify.com/v2/datasets/{DATASET_ID}/items\n```\n\n----------------------------------------\n\nTITLE: Visualizing Actor Build Process with Mermaid\nDESCRIPTION: Flowchart showing how Actor definition files (Dockerfile, actor.json, and main.js) are processed into a build through the build process.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/index.md#2025-04-18_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph AD [Actor Definition files]\n        direction LR\n        Dockerfile\n        .actor/actor.json\n        src/main.js\n    end\n\n    AD -- \"build process\" --> Build\n```\n\n----------------------------------------\n\nTITLE: Complete Node.js Actor Implementation\nDESCRIPTION: Full implementation of an Actor that processes number arrays using the Apify SDK.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// index.js\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\nconst { numbers } = await Actor.getInput();\n\nconst addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);\n\nconst solution = addAllNumbers(...numbers);\n\nconsole.log(solution);\n\nawait Actor.exit();\n```\n\n----------------------------------------\n\nTITLE: JSON Response Format for Rate Limit Exceeded\nDESCRIPTION: Example JSON response when a client exceeds the API rate limit of requests per second. The API endpoints respond with HTTP status code 429 and this error message structure.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/storage/usage.md#2025-04-18_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"error\": {\n        \"type\": \"rate-limit-exceeded\",\n        \"message\": \"You have exceeded the rate limit of ... requests per second\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Persisted State - Python\nDESCRIPTION: Example showing how to retrieve previously saved state when an Actor starts using get_value() in Python.\nSOURCE: https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom apify import Actor\n\nasync def main():\n    async with Actor:\n        # ...\n        previous_crawling_state = await Actor.get_value('my-crawling-state')\n        # ...\n```"
  }
]
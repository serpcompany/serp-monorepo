[
  {
    "owner": "qwenlm",
    "repo": "qwen-agent",
    "content": "TITLE: Configuring and Using LLMs with Function Calling in Qwen-Agent\nDESCRIPTION: This example demonstrates how to initialize an LLM using the get_chat_model interface, configure it with specific parameters, and utilize function calling capabilities with streaming output. The code shows setting up a weather query with defined function parameters.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/llm.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom qwen_agent.llm import get_chat_model\n\nllm_cfg = {\n            # Use the model service provided by DashScope:\n            # 'model_type': 'qwen_dashscope',\n            'model': 'qwen-max',\n            'model_server': 'dashscope',\n            # Use your own model service compatible with OpenAI API:\n            # 'model': 'Qwen',\n            # 'model_server': 'http://127.0.0.1:7905/v1',\n            # (Optional) LLM hyper-paramters:\n            'generate_cfg': {\n                'top_p': 0.8\n            }\n          }\nllm = get_chat_model(llm_cfg)\nmessages = [{\n    'role': 'user',\n    'content': \"What's the weather like in San Francisco?\"\n}]\nfunctions = [{\n    'name': 'get_current_weather',\n    'description': 'Get the current weather in a given location',\n    'parameters': {\n        'type': 'object',\n        'properties': {\n            'location': {\n                'type': 'string',\n                'description':\n                'The city and state, e.g. San Francisco, CA',\n            },\n            'unit': {\n                'type': 'string',\n                'enum': ['celsius', 'fahrenheit']\n            },\n        },\n        'required': ['location'],\n    },\n}]\n\n# The streaming output responses\nresponses = []\nfor responses in llm.chat(messages=messages,\n                          functions=functions,\n                          stream=True):\n    print(responses)\n```\n\n----------------------------------------\n\nTITLE: DocQA Agent Class Implementation in Python\nDESCRIPTION: Implements a DocQA agent class that handles document question-answering by managing message flows and prompt templates. It processes messages with knowledge input and language specification, constructing appropriate system prompts before calling the language model.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/agent.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DocQA(Agent):\n\n    def _run(self,\n             messages: List[Message],\n             knowledge: str = '',\n             lang: str = 'en',\n             **kwargs) -> Iterator[List[Message]]:\n        messages = copy.deepcopy(messages)\n        system_prompt = PROMPT_TEMPLATE[lang].format(ref_doc=knowledge)\n        if messages and messages[0][ROLE] == SYSTEM:\n            messages[0][CONTENT] += system_prompt\n        else:\n            messages.insert(0, Message(SYSTEM, system_prompt))\n\n        return self._call_llm(messages=messages)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using LLM with Qwen-Agent in Python\nDESCRIPTION: This snippet demonstrates how to configure and use an LLM (Language Model) with Qwen-Agent. It shows setting up the model configuration, initializing the chat model, and using it to generate responses with function calling capabilities.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/llm_cn.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom qwen_agent.llm import get_chat_model\n\nllm_cfg = {\n            # Use the model service provided by DashScope:\n            # 'model_type': 'qwen_dashscope',\n            'model': 'qwen-max',\n            'model_server': 'dashscope',\n            # Use your own model service compatible with OpenAI API:\n            # 'model': 'Qwen',\n            # 'model_server': 'http://127.0.0.1:7905/v1',\n            # (Optional) LLM hyper-paramters:\n            'generate_cfg': {\n                'top_p': 0.8\n            }\n          }\nllm = get_chat_model(llm_cfg)\nmessages = [{\n    'role': 'user',\n    'content': \"What's the weather like in San Francisco?\"\n}]\nfunctions = [{\n    'name': 'get_current_weather',\n    'description': 'Get the current weather in a given location',\n    'parameters': {\n        'type': 'object',\n        'properties': {\n            'location': {\n                'type': 'string',\n                'description':\n                'The city and state, e.g. San Francisco, CA',\n            },\n            'unit': {\n                'type': 'string',\n                'enum': ['celsius', 'fahrenheit']\n            },\n        },\n        'required': ['location'],\n    },\n}]\n\n# 此处演示流式输出效果\nresponses = []\nfor responses in llm.chat(messages=messages,\n                          functions=functions,\n                          stream=True):\n    print(responses)\n```\n\n----------------------------------------\n\nTITLE: DocQA Agent Class Implementation in Python\nDESCRIPTION: This class extends the Agent base class to implement document-based question answering functionality. It customizes the system prompt with reference documentation and handles message processing before calling the language model. The agent supports both Chinese and English languages through template selection.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/agent_cn.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass DocQA(Agent):\n\n    def _run(self,\n             messages: List[Message],\n             knowledge: str = '',\n             lang: str = 'en',\n             **kwargs) -> Iterator[List[Message]]:\n        messages = copy.deepcopy(messages)\n        system_prompt = PROMPT_TEMPLATE[lang].format(ref_doc=knowledge)\n        if messages and messages[0][ROLE] == SYSTEM:\n            messages[0][CONTENT] += system_prompt\n        else:\n            messages.insert(0, Message(SYSTEM, system_prompt))\n\n        return self._call_llm(messages=messages)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Image Generation Tool in Python\nDESCRIPTION: This code defines and registers a custom image generation tool named 'my_image_gen'. It specifies the tool's description, parameters, and implements the call() method to generate image URLs based on text prompts.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/tool.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.parse\nimport json5\nimport json\nfrom qwen_agent.tools.base import BaseTool, register_tool\n# Add a custom tool named my_image_gen：\n@register_tool('my_image_gen')\nclass MyImageGen(BaseTool):\n    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'\n    parameters = [{\n        'name': 'prompt',\n        'type': 'string',\n        'description':\n        'Detailed description of the desired image content, in English',\n        'required': True\n    }]\n\n    def call(self, params: str, **kwargs) -> str:\n        prompt = json5.loads(params)['prompt']\n        prompt = urllib.parse.quote(prompt)\n        return json.dumps(\n            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},\n            ensure_ascii=False)\n```\n\n----------------------------------------\n\nTITLE: Defining Standalone Image Generation Tool Class in Python\nDESCRIPTION: This snippet shows how to create a standalone tool class without using the registration method. The tool can be directly passed to an Agent as an object.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/tool.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport urllib.parse\nimport json5\nimport json\nfrom qwen_agent.tools.base import BaseTool\n\nclass MyImageGen(BaseTool):\n    name = 'my_image_gen'\n    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'\n    parameters = [{\n        'name': 'prompt',\n        'type': 'string',\n        'description':\n        'Detailed description of the desired image content, in English',\n        'required': True\n    }]\n\n    def call(self, params: str, **kwargs) -> str:\n        prompt = json5.loads(params)['prompt']\n        prompt = urllib.parse.quote(prompt)\n        return json.dumps(\n            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},\n            ensure_ascii=False)\n```\n\n----------------------------------------\n\nTITLE: Calling Image Generation Tool in Python\nDESCRIPTION: This snippet demonstrates how to directly call an image generation tool using the .call() method with the necessary parameters.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/tool.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom qwen_agent.tools import ImageGen\n\ntool = ImageGen()\nres = tool.call(params = {'prompt': 'a cute cat'})\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Running Comprehensive Benchmark Evaluation\nDESCRIPTION: Shell command to run the benchmark evaluation for a specific model, which will test the model's performance on various code interpreter tasks.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython inference_and_execute.py --model {model_name}\n```\n\n----------------------------------------\n\nTITLE: Prompt Template Dictionary Definition\nDESCRIPTION: Defines a dictionary mapping language codes to prompt templates for different languages. This enables multilingual support in the DocQA system.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/docs/agent.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nPROMPT_TEMPLATE = {\n    'zh': PROMPT_TEMPLATE_ZH,\n    'en': PROMPT_TEMPLATE_EN,\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Local Database Service with Custom Model Server\nDESCRIPTION: Command to start the database service using a custom model server instead of DashScope. Users need to specify the model, API base URL, and API key for their own model deployment.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/browser_qwen.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Specify the model service, and start the database service.\n# Example: Assuming Qwen1.5-72B-Chat is deployed at http://localhost:8000/v1 using vLLM, you can specify the model service as:\n#   --llm Qwen1.5-72B-Chat --model_server http://localhost:8000/v1 --api_key EMPTY\npython run_server.py --llm {MODEL} --model_server {API_BASE} --workstation_port 7864 --api_key {API_KEY}\n```\n\n----------------------------------------\n\nTITLE: Deploying Local Database Service with DashScope Model\nDESCRIPTION: Command to start the database service using DashScope's model service. Users can choose from different Qwen models (7B/14B/72B or turbo/plus/max) and must provide their DashScope API key.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/browser_qwen.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Start the database service, specifying the model on DashScope by using the --llm flag.\n# The value of --llm can be one of the following, in increasing order of resource consumption:\n#   - qwen1.5-7b/14b/72b-chat (the same as the open-sourced Qwen1.5 7B/14B/72B Chat model)\n#   - qwen-turbo, qwen-plus, qwen-max (qwen-max is recommended)\n# \"YOUR_DASHSCOPE_API_KEY\" is a placeholder. The user should replace it with their actual key.\npython run_server.py --llm qwen-max --model_server dashscope --workstation_port 7864 --api_key YOUR_DASHSCOPE_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Running Specific Task Evaluation - Code Correctness Rate\nDESCRIPTION: Shell command to evaluate the correctness rate of code generated by the model for specific tasks like visualization or math.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython inference_and_execute.py --task {task_name} --model {model_name}\n```\n\n----------------------------------------\n\nTITLE: Running Specific Task Evaluation - Code Executable Rate\nDESCRIPTION: Shell command to evaluate the executable rate of code generated by the model for specific tasks like general problem-solving.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython inference_and_execute.py --task {task_name} --model {model_name}\n```\n\n----------------------------------------\n\nTITLE: Installing SimHei Font for Matplotlib Visualization\nDESCRIPTION: Python code snippet to properly install the SimHei font for matplotlib visualizations, which is needed for proper display when evaluating visualization tasks.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport matplotlib\ntarget_font_path = os.path.join(\n    os.path.abspath(\n        os.path.join(matplotlib.matplotlib_fname(), os.path.pardir)),\n        'fonts', 'ttf', 'simhei.ttf')\nos.system(f'cp simhei.ttf {target_font_path}')\nfont_list_cache = os.path.join(matplotlib.get_cachedir(), 'fontlist-*.json')\nos.system(f'rm -f {font_list_cache}')\n```\n\n----------------------------------------\n\nTITLE: Downloading Benchmark Dataset\nDESCRIPTION: Shell commands to download and extract the benchmark dataset, then move it to the evaluation directory.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd benchmark\nwget https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/benchmark_code_interpreter_data.zip\nunzip benchmark_code_interpreter_data.zip\nmkdir eval_data\nmv eval_code_interpreter_v1.jsonl eval_data/\n```\n\n----------------------------------------\n\nTITLE: Installing the Code Interpreter Benchmark\nDESCRIPTION: Shell commands to clone the GitHub repository and install the required dependencies for the benchmark.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/QwenLM/Qwen-Agent.git\ncd benchmark\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: Lists required Python packages and version constraints for the Qwen Agent project. Includes ML libraries like transformers, accelerate, and data science packages like numpy and pandas.\nSOURCE: https://github.com/qwenlm/qwen-agent/blob/main/benchmark/code_interpreter/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\naccelerate>=0.20.3\nfunc_timeout\njson5\nmatplotlib\nnumpy\nopenai\npandas\nPrettyTable\nscipy\nseaborn\nsympy\ntransformers==4.33.1\ntransformers_stream_generator\n```"
  }
]
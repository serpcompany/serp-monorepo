[
  {
    "owner": "delta-io",
    "repo": "delta",
    "content": "TITLE: Implementing Change Data Capture (CDC) with Delta Lake in Python\nDESCRIPTION: This snippet demonstrates how to apply change data capture (CDC) operations to a Delta table using merge in Python. It processes a DataFrame containing updates, deletes, and inserts from an external system, finds the latest change for each key, and applies those changes to the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndeltaTable = ... # DeltaTable with schema (key, value)\n\n# DataFrame with changes having following columns\n# - key: key of the change\n# - time: time of change for ordering between changes (can replaced by other ordering id)\n# - newValue: updated or inserted value if key was not deleted\n# - deleted: true if the key was deleted, false if the key was inserted or updated\nchangesDF = spark.table(\"changes\")\n\n# Find the latest change for each key based on the timestamp\n# Note: For nested structs, max on struct is computed as\n# max on first struct field, if equal fall back to second fields, and so on.\nlatestChangeForEachKey = changesDF \\\n  .selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\") \\\n  .groupBy(\"key\") \\\n  .agg(max(\"otherCols\").alias(\"latest\")) \\\n  .select(\"key\", \"latest.*\") \\\n\ndeltaTable.alias(\"t\").merge(\n    latestChangeForEachKey.alias(\"s\"),\n    \"s.key = t.key\") \\\n  .whenMatchedDelete(condition = \"s.deleted = true\") \\\n  .whenMatchedUpdate(set = {\n    \"key\": \"s.key\",\n    \"value\": \"s.newValue\"\n  }) \\\n  .whenNotMatchedInsert(\n    condition = \"s.deleted = false\",\n    values = {\n      \"key\": \"s.key\",\n      \"value\": \"s.newValue\"\n    }\n  ).execute()\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession for Delta Lake (Python)\nDESCRIPTION: This code snippet shows how to configure a SparkSession for Delta Lake operations in Python. It sets the necessary configurations for enabling Delta Lake integration with Apache Spark DataSourceV2 and Catalog APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n  .builder \\\n  .appName(\"...\") \\\n  .master(\"...\") \\\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n  .getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Streaming Upsert Implementation in Scala\nDESCRIPTION: Implementation of streaming upserts using Delta table merge operation in Scala. The code demonstrates how to update or insert records into a Delta table using foreachBatch with streaming aggregation queries. Includes merge operation with matched and not matched conditions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_29\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables.*\n\nval deltaTable = DeltaTable.forPath(spark, \"/data/aggregates\")\n\n// Function to upsert microBatchOutputDF into Delta table using merge\ndef upsertToDelta(microBatchOutputDF: DataFrame, batchId: Long) {\n  deltaTable.as(\"t\")\n    .merge(\n      microBatchOutputDF.as(\"s\"),\n      \"s.key = t.key\")\n    .whenMatched().updateAll()\n    .whenNotMatched().insertAll()\n    .execute()\n}\n\n// Write the output of a streaming aggregation query into Delta table\nstreamingAggregatesDF.writeStream\n  .format(\"delta\")\n  .foreachBatch(upsertToDelta _)\n  .outputMode(\"update\")\n  .start()\n```\n\n----------------------------------------\n\nTITLE: Implementing Idempotent Writes in Delta Tables using Scala\nDESCRIPTION: Scala example of implementing idempotent writes to a Delta table using txnVersion and txnAppId options to prevent duplicate data during job restarts.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_24\n\nLANGUAGE: scala\nCODE:\n```\nval appId = ... // A unique string that is used as an application ID.\nversion = ... // A monotonically increasing number that acts as transaction version.\n\ndataFrame.write.format(...).option(\"txnVersion\", version).option(\"txnAppId\", appId).save(...)\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Python\nDESCRIPTION: Python code to create a Delta table from a DataFrame. This creates a table with a single column containing values 0-4 at the specified location.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndata = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"/tmp/delta-table\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Update, Delete, and Upsert Operations in Delta Lake Tables\nDESCRIPTION: Demonstrates how to perform conditional update, delete, and upsert (merge) operations on Delta Lake tables using SQL, Python, Scala, and Java. These operations allow for efficient data manipulation without overwriting the entire table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\n-- Update every even value by adding 100 to it\nUPDATE delta.`/tmp/delta-table` SET id = id + 100 WHERE id % 2 == 0;\n\n-- Delete every even value\nDELETE FROM delta.`/tmp/delta-table` WHERE id % 2 == 0;\n\n-- Upsert (merge) new data\nCREATE TEMP VIEW newData AS SELECT col1 AS id FROM VALUES 1,3,5,7,9,11,13,15,17,19;\n\nMERGE INTO delta.`/tmp/delta-table` AS oldData\nUSING newData\nON oldData.id = newData.id\nWHEN MATCHED\n  THEN UPDATE SET id = newData.id\nWHEN NOT MATCHED\n  THEN INSERT (id) VALUES (newData.id);\n\nSELECT * FROM delta.`/tmp/delta-table`;\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n\n# Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = { \"id\": expr(\"id + 100\") })\n\n# Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n# Upsert (merge) new data\nnewData = spark.range(0, 20)\n\ndeltaTable.alias(\"oldData\") \\\n  .merge(\n    newData.alias(\"newData\"),\n    \"oldData.id = newData.id\") \\\n  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n  .execute()\n\ndeltaTable.toDF().show()\n```\n\nLANGUAGE: Scala\nCODE:\n```\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\nval deltaTable = DeltaTable.forPath(\"/tmp/delta-table\")\n\n// Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = Map(\"id\" -> expr(\"id + 100\")))\n\n// Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n// Upsert (merge) new data\nval newData = spark.range(0, 20).toDF\n\ndeltaTable.as(\"oldData\")\n  .merge(\n    newData.as(\"newData\"),\n    \"oldData.id = newData.id\")\n  .whenMatched\n  .update(Map(\"id\" -> col(\"newData.id\")))\n  .whenNotMatched\n  .insert(Map(\"id\" -> col(\"newData.id\")))\n  .execute()\n\ndeltaTable.toDF.show()\n```\n\nLANGUAGE: Java\nCODE:\n```\nimport io.delta.tables.*;\nimport org.apache.spark.sql.functions;\nimport java.util.HashMap;\n\nDeltaTable deltaTable = DeltaTable.forPath(\"/tmp/delta-table\");\n\n// Update every even value by adding 100 to it\ndeltaTable.update(\n  functions.expr(\"id % 2 == 0\"),\n  new HashMap<String, Column>() {{\n    put(\"id\", functions.expr(\"id + 100\"));\n  }}\n);\n\n// Delete every even value\ndeltaTable.delete(condition = functions.expr(\"id % 2 == 0\"));\n\n// Upsert (merge) new data\nDataset<Row> newData = spark.range(0, 20).toDF();\n\ndeltaTable.as(\"oldData\")\n  .merge(\n    newData.as(\"newData\"),\n    \"oldData.id = newData.id\")\n  .whenMatched()\n  .update(\n    new HashMap<String, Column>() {{\n      put(\"id\", functions.col(\"newData.id\"));\n    }})\n  .whenNotMatched()\n  .insertExpr(\n    new HashMap<String, Column>() {{\n      put(\"id\", functions.col(\"newData.id\"));\n    }})\n  .execute();\n\ndeltaTable.toDF().show();\n```\n\n----------------------------------------\n\nTITLE: Writing Streaming Data to Delta Lake Tables\nDESCRIPTION: Shows how to write streaming data to a Delta Lake table using Structured Streaming. This example uses a rate source to generate streaming data and writes it to a Delta table in append mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nstreamingDf = spark.readStream.format(\"rate\").load()\nstream = streamingDf.selectExpr(\"value as id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\")\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval streamingDf = spark.readStream.format(\"rate\").load()\nval stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\")\n```\n\nLANGUAGE: Java\nCODE:\n```\nimport org.apache.spark.sql.streaming.StreamingQuery;\n\nDataset<Row> streamingDf = spark.readStream().format(\"rate\").load();\nStreamingQuery stream = streamingDf.selectExpr(\"value as id\").writeStream().format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\");\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from Delta Table using SQL\nDESCRIPTION: Demonstrates how to delete records from a Delta table using SQL syntax. The example shows deleting records where birthDate is before 1955.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nDELETE FROM people10m WHERE birthDate < '1955-01-01'\n\nDELETE FROM delta.`/tmp/delta/people-10m` WHERE birthDate < '1955-01-01'\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Delta Table\nDESCRIPTION: Examples of how to append new data to an existing Delta table using different programming interfaces. Demonstrates both file-based and table-based writes.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO default.people10m SELECT * FROM morePeople\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/people10m\")\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.people10m\")\n```\n\nLANGUAGE: scala\nCODE:\n```\ndf.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/people10m\")\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.people10m\")\n\nimport io.delta.implicits._\ndf.write.mode(\"append\").delta(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table and Engine in Java\nDESCRIPTION: This snippet demonstrates how to create a Table object representing a Delta table and initialize the default Engine. It sets up the necessary objects to interact with a Delta table using Delta Kernel.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.*;\nimport io.delta.kernel.defaults.*;\nimport org.apache.hadoop.conf.Configuration;\n\nString myTablePath = <my-table-path>; // fully qualified table path. Ex: file:/user/tables/myTable\nConfiguration hadoopConf = new Configuration();\nEngine myEngine = DefaultEngine.create(hadoopConf);\nTable myTable = Table.forPath(myEngine, myTablePath);\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table with DeltaTableBuilder API\nDESCRIPTION: Use the DeltaTableBuilder API to create Delta tables with more granular control over table properties, column comments, and generated columns. This API provides a fluent interface for table creation.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create table in the metastore\nDeltaTable.createIfNotExists(spark) \\\n  .tableName(\"default.people10m\") \\\n  .addColumn(\"id\", \"INT\") \\\n  .addColumn(\"firstName\", \"STRING\") \\\n  .addColumn(\"middleName\", \"STRING\") \\\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n  .addColumn(\"gender\", \"STRING\") \\\n  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n  .addColumn(\"ssn\", \"STRING\") \\\n  .addColumn(\"salary\", \"INT\") \\\n  .execute()\n\n# Create or replace table with path and add properties\nDeltaTable.createOrReplace(spark) \\\n  .addColumn(\"id\", \"INT\") \\\n  .addColumn(\"firstName\", \"STRING\") \\\n  .addColumn(\"middleName\", \"STRING\") \\\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n  .addColumn(\"gender\", \"STRING\") \\\n  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n  .addColumn(\"ssn\", \"STRING\") \\\n  .addColumn(\"salary\", \"INT\") \\\n  .property(\"description\", \"table with people data\") \\\n  .location(\"/tmp/delta/people10m\") \\\n  .execute()\n```\n\nLANGUAGE: scala\nCODE:\n```\n// Create table in the metastore\nDeltaTable.createOrReplace(spark)\n  .tableName(\"default.people10m\")\n  .addColumn(\"id\", \"INT\")\n  .addColumn(\"firstName\", \"STRING\")\n  .addColumn(\"middleName\", \"STRING\")\n  .addColumn(\n    DeltaTable.columnBuilder(\"lastName\")\n      .dataType(\"STRING\")\n      .comment(\"surname\")\n      .build())\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\")\n  .addColumn(\"gender\", \"STRING\")\n  .addColumn(\"birthDate\", \"TIMESTAMP\")\n  .addColumn(\"ssn\", \"STRING\")\n  .addColumn(\"salary\", \"INT\")\n  .execute()\n\n// Create or replace table with path and add properties\nDeltaTable.createOrReplace(spark)\n  .addColumn(\"id\", \"INT\")\n  .addColumn(\"firstName\", \"STRING\")\n  .addColumn(\"middleName\", \"STRING\")\n  .addColumn(\n    DeltaTable.columnBuilder(\"lastName\")\n      .dataType(\"STRING\")\n      .comment(\"surname\")\n      .build())\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\")\n  .addColumn(\"gender\", \"STRING\")\n  .addColumn(\"birthDate\", \"TIMESTAMP\")\n  .addColumn(\"ssn\", \"STRING\")\n  .addColumn(\"salary\", \"INT\")\n  .property(\"description\", \"table with people data\")\n  .location(\"/tmp/delta/people10m\")\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Implementing Idempotent Writes in Delta Tables using Python\nDESCRIPTION: Python example of implementing idempotent writes to a Delta table using txnVersion and txnAppId options to prevent duplicate data during job restarts.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\napp_id = ... # A unique string that is used as an application ID.\nversion = ... # A monotonically increasing number that acts as transaction version.\n\ndataFrame.write.format(...).option(\"txnVersion\", version).option(\"txnAppId\", app_id).save(...)\n```\n\n----------------------------------------\n\nTITLE: Reading from a Delta Table with Python\nDESCRIPTION: Python code to read data from a Delta table and display the results. This loads the table into a DataFrame and then prints its contents.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Reading Streaming Changes from Delta Lake Tables\nDESCRIPTION: Demonstrates how to read a stream of changes from a Delta Lake table using Structured Streaming. This example reads all changes from the table and writes them to the console for demonstration purposes.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nstream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start()\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval stream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start()\n```\n\nLANGUAGE: Java\nCODE:\n```\nStreamingQuery stream2 = spark.readStream().format(\"delta\").load(\"/tmp/delta-table\").writeStream().format(\"console\").start();\n```\n\n----------------------------------------\n\nTITLE: Updating Delta Table using SQL\nDESCRIPTION: Shows how to update records in a Delta table using SQL syntax. The example demonstrates updating gender abbreviations to full words.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE people10m SET gender = 'Female' WHERE gender = 'F';\nUPDATE people10m SET gender = 'Male' WHERE gender = 'M';\n\nUPDATE delta.`/tmp/delta/people-10m` SET gender = 'Female' WHERE gender = 'F';\nUPDATE delta.`/tmp/delta/people-10m` SET gender = 'Male' WHERE gender = 'M';\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Generated Columns in Python\nDESCRIPTION: This snippet demonstrates how to create a Delta table with generated columns using Python. It includes various column types and a generated column for date of birth based on a timestamp.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nDeltaTable.create(spark) \\\n  .tableName(\"default.people10m\") \\\n  .addColumn(\"id\", \"INT\") \\\n  .addColumn(\"firstName\", \"STRING\") \\\n  .addColumn(\"middleName\", \"STRING\") \\\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n  .addColumn(\"gender\", \"STRING\") \\\n  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n  .addColumn(\"dateOfBirth\", DateType(), generatedAlwaysAs=\"CAST(birthDate AS DATE)\") \\\n  .addColumn(\"ssn\", \"STRING\") \\\n  .addColumn(\"salary\", \"INT\") \\\n  .partitionedBy(\"gender\") \\\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Creating Continuous Delta Source with Time Travel in Java\nDESCRIPTION: Creates a continuous Delta source that starts reading from a specific historical point and monitors for changes. Suitable for streaming applications.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createContinuousDeltaSourceWithTimeTravel(\n        StreamExecutionEnvironment env,\n        String deltaTablePath) {\n\n    DeltaSource<RowData> deltaSource = DeltaSource\n        .forContinuousRowData(\n            new Path(deltaTablePath),\n            new Configuration())\n        // could also use `.startingVersion(314159)`\n        .startingTimestamp(\"2022-06-28 04:55:00\")\n        .build();\n\n    return env.fromSource(deltaSource, WatermarkStrategy.noWatermarks(), \"delta-source\");\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Generated Columns in Scala\nDESCRIPTION: This snippet shows how to create a Delta table with generated columns using Scala. It includes various column types and a generated column for date of birth based on a timestamp.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nDeltaTable.create(spark)\n  .tableName(\"default.people10m\")\n  .addColumn(\"id\", \"INT\")\n  .addColumn(\"firstName\", \"STRING\")\n  .addColumn(\"middleName\", \"STRING\")\n  .addColumn(\n    DeltaTable.columnBuilder(\"lastName\")\n      .dataType(\"STRING\")\n      .comment(\"surname\")\n      .build())\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\")\n  .addColumn(\"gender\", \"STRING\")\n  .addColumn(\"birthDate\", \"TIMESTAMP\")\n  .addColumn(\n    DeltaTable.columnBuilder(\"dateOfBirth\")\n     .dataType(DateType)\n     .generatedAlwaysAs(\"CAST(dateOfBirth AS DATE)\")\n     .build())\n  .addColumn(\"ssn\", \"STRING\")\n  .addColumn(\"salary\", \"INT\")\n  .partitionedBy(\"gender\")\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Configuring Idempotent Writes in Delta Tables using SQL\nDESCRIPTION: SQL example of configuring idempotent writes to a Delta table by setting session configurations for txnAppId, txnVersion, and enabling automatic version reset.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_25\n\nLANGUAGE: sql\nCODE:\n```\nSET spark.databricks.delta.write.txnAppId = ...;\nSET spark.databricks.delta.write.txnVersion = ...;\nSET spark.databricks.delta.write.txnVersion.autoReset.enabled = true; -- if set to true, this will reset txnVersion after every write\n```\n\n----------------------------------------\n\nTITLE: Overwriting Delta Table Data\nDESCRIPTION: Examples of how to completely overwrite data in a Delta table using different programming interfaces. Shows both file-based and table-based overwrites.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nINSERT OVERWRITE TABLE default.people10m SELECT * FROM morePeople\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/people10m\")\ndf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.people10m\")\n```\n\nLANGUAGE: scala\nCODE:\n```\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/people10m\")\ndf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.people10m\")\n\nimport io.delta.implicits._\ndf.write.mode(\"overwrite\").delta(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table with SQL DDL\nDESCRIPTION: Use SQL DDL commands to create Delta tables in the metastore or by path. This snippet shows how to create a table with specified columns and data types using the DELTA format.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE IF NOT EXISTS default.people10m (\n  id INT,\n  firstName STRING,\n  middleName STRING,\n  lastName STRING,\n  gender STRING,\n  birthDate TIMESTAMP,\n  ssn STRING,\n  salary INT\n) USING DELTA\n```\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE TABLE delta.`/tmp/delta/people10m` (\n  id INT,\n  firstName STRING,\n  middleName STRING,\n  lastName STRING,\n  gender STRING,\n  birthDate TIMESTAMP,\n  ssn STRING,\n  salary INT\n) USING DELTA\n```\n\n----------------------------------------\n\nTITLE: Implementing SCD Type 2 Operations with Delta Lake in Scala\nDESCRIPTION: This snippet demonstrates how to perform a Slowly Changing Dimension (SCD) Type 2 operation using Delta Lake's merge functionality in Scala. It maintains a history of customer addresses by marking previous addresses as inactive and inserting new addresses as active records, complete with effective date ranges.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_25\n\nLANGUAGE: scala\nCODE:\n```\nval customersTable: DeltaTable = ...   // table with schema (customerId, address, current, effectiveDate, endDate)\n\nval updatesDF: DataFrame = ...          // DataFrame with schema (customerId, address, effectiveDate)\n\n// Rows to INSERT new addresses of existing customers\nval newAddressesToInsert = updatesDF\n  .as(\"updates\")\n  .join(customersTable.toDF.as(\"customers\"), \"customerid\")\n  .where(\"customers.current = true AND updates.address <> customers.address\")\n\n// Stage the update by unioning two sets of rows\n// 1. Rows that will be inserted in the whenNotMatched clause\n// 2. Rows that will either update the current addresses of existing customers or insert the new addresses of new customers\nval stagedUpdates = newAddressesToInsert\n  .selectExpr(\"NULL as mergeKey\", \"updates.*\")   // Rows for 1.\n  .union(\n    updatesDF.selectExpr(\"updates.customerId as mergeKey\", \"*\")  // Rows for 2.\n  )\n\n// Apply SCD Type 2 operation using merge\ncustomersTable\n  .as(\"customers\")\n  .merge(\n    stagedUpdates.as(\"staged_updates\"),\n    \"customers.customerId = mergeKey\")\n  .whenMatched(\"customers.current = true AND customers.address <> staged_updates.address\")\n  .updateExpr(Map(                                      // Set current to false and endDate to source's effective date.\n    \"current\" -> \"false\",\n    \"endDate\" -> \"staged_updates.effectiveDate\"))\n  .whenNotMatched()\n  .insertExpr(Map(\n    \"customerid\" -> \"staged_updates.customerId\",\n    \"address\" -> \"staged_updates.address\",\n    \"current\" -> \"true\",\n    \"effectiveDate\" -> \"staged_updates.effectiveDate\",  // Set current to true along with the new address and its effective date.\n    \"endDate\" -> \"null\"))\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Creating Tables with Liquid Clustering in SQL, Python, and Scala\nDESCRIPTION: Examples of how to create Delta tables with liquid clustering enabled using SQL CREATE TABLE statements, and the DeltaTable API in Python and Scala. This demonstrates specifying clustering columns during table creation.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Create an empty table\nCREATE TABLE table1(col0 int, col1 string) USING DELTA CLUSTER BY (col0);\n\n-- Using a CTAS statement (Delta 3.3+)\nCREATE EXTERNAL TABLE table2 CLUSTER BY (col0)  -- specify clustering after table name, not in subquery\nLOCATION 'table_location'\nAS SELECT * FROM table1;\n```\n\nLANGUAGE: python\nCODE:\n```\n# Create an empty table\nDeltaTable.create()\n  .tableName(\"table1\")\n  .addColumn(\"col0\", dataType = \"INT\")\n  .addColumn(\"col1\", dataType = \"STRING\")\n  .clusterBy(\"col0\")\n  .execute()\n```\n\nLANGUAGE: scala\nCODE:\n```\n// Create an empty table\nDeltaTable.create()\n  .tableName(\"table1\")\n  .addColumn(\"col0\", dataType = \"INT\")\n  .addColumn(\"col1\", dataType = \"STRING\")\n  .clusterBy(\"col0\")\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Time Travel Examples in Python\nDESCRIPTION: Provides practical examples of using time travel functionality in Delta Lake, including fixing accidental deletes, correcting updates, and querying historical data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nyesterday = spark.sql(\"SELECT CAST(date_sub(current_date(), 1) AS STRING)\").collect()[0][0]\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", yesterday).load(\"/tmp/delta/events\")\ndf.where(\"userId = 111\").write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/events\")\n\nyesterday = spark.sql(\"SELECT CAST(date_sub(current_date(), 1) AS STRING)\").collect()[0][0]\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", yesterday).load(\"/tmp/delta/events\")\ndf.createOrReplaceTempView(\"my_table_yesterday\")\nspark.sql('''\nMERGE INTO delta.`/tmp/delta/events` target\n  USING my_table_yesterday source\n  ON source.userId = target.userId\n  WHEN MATCHED THEN UPDATE SET *\n''')\n\nlast_week = spark.sql(\"SELECT CAST(date_sub(current_date(), 7) AS STRING)\").collect()[0][0]\ndf = spark.read.format(\"delta\").option(\"timestampAsOf\", last_week).load(\"/tmp/delta/events\")\nlast_week_count = df.select(\"userId\").distinct().count()\ncount = spark.read.format(\"delta\").load(\"/tmp/delta/events\").select(\"userId\").distinct().count()\nnew_customers_count = count - last_week_count\n```\n\n----------------------------------------\n\nTITLE: Implementing SCD Type 2 Operations with Delta Lake in Python\nDESCRIPTION: This snippet demonstrates how to perform a Slowly Changing Dimension (SCD) Type 2 operation using Delta Lake's merge functionality in Python. It maintains a history of customer addresses by marking previous addresses as inactive and inserting new addresses as active records, complete with effective date ranges.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncustomersTable = ...  # DeltaTable with schema (customerId, address, current, effectiveDate, endDate)\n\nupdatesDF = ...       # DataFrame with schema (customerId, address, effectiveDate)\n\n# Rows to INSERT new addresses of existing customers\nnewAddressesToInsert = updatesDF \\\n  .alias(\"updates\") \\\n  .join(customersTable.toDF().alias(\"customers\"), \"customerid\") \\\n  .where(\"customers.current = true AND updates.address <> customers.address\")\n\n# Stage the update by unioning two sets of rows\n# 1. Rows that will be inserted in the whenNotMatched clause\n# 2. Rows that will either update the current addresses of existing customers or insert the new addresses of new customers\nstagedUpdates = (\n  newAddressesToInsert\n  .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n  .union(updatesDF.selectExpr(\"updates.customerId as mergeKey\", \"*\"))  # Rows for 2.\n)\n\n# Apply SCD Type 2 operation using merge\ncustomersTable.alias(\"customers\").merge(\n  stagedUpdates.alias(\"staged_updates\"),\n  \"customers.customerId = mergeKey\") \\\n.whenMatchedUpdate(\n  condition = \"customers.current = true AND customers.address <> staged_updates.address\",\n  set = {                                      # Set current to false and endDate to source's effective date.\n    \"current\": \"false\",\n    \"endDate\": \"staged_updates.effectiveDate\"\n  }\n).whenNotMatchedInsert(\n  values = {\n    \"customerid\": \"staged_updates.customerId\",\n    \"address\": \"staged_updates.address\",\n    \"current\": \"true\",\n    \"effectiveDate\": \"staged_updates.effectiveDate\",  # Set current to true along with the new address and its effective date.\n    \"endDate\": \"null\"\n  }\n).execute()\n```\n\n----------------------------------------\n\nTITLE: Updating Delta Table using Python\nDESCRIPTION: Shows how to update records in a Delta table using Python API. Includes examples using both SQL-formatted string predicates and Spark SQL functions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, '/tmp/delta/people-10m')\n\n# Declare the predicate by using a SQL-formatted string.\ndeltaTable.update(\n  condition = \"gender = 'F'\",\n  set = { \"gender\": \"'Female'\" }\n)\n\n# Declare the predicate by using Spark SQL functions.\ndeltaTable.update(\n  condition = col('gender') == 'M',\n  set = { 'gender': lit('Male') }\n)\n```\n\n----------------------------------------\n\nTITLE: Reading Change Data Feed in Python Batch Queries\nDESCRIPTION: Python examples demonstrating how to read change data using DataFrame APIs in batch mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-change-data-feed.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark.read.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingVersion\", 0) \\\n  .option(\"endingVersion\", 10) \\\n  .table(\"myDeltaTable\")\n\nspark.read.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n  .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n  .table(\"myDeltaTable\")\n\nspark.read.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingVersion\", 0) \\\n  .table(\"myDeltaTable\")\n\nspark.read.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n  .load(\"pathToMyDeltaTable\")\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Tables as Stream Sources in Scala\nDESCRIPTION: Shows two methods to read a Delta table as a stream source: using the standard format method and using the implicit delta method. Both approaches load data from the specified Delta table path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .load(\"/tmp/delta/events\")\n\nimport io.delta.implicits._\nspark.readStream.delta(\"/tmp/delta/events\")\n```\n\n----------------------------------------\n\nTITLE: Selective Data Overwrite with Predicate\nDESCRIPTION: Examples showing how to selectively overwrite data matching specific conditions in a Delta table. Includes configuration for constraint checking.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nreplace_data.write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"replaceWhere\", \"start_date >= '2017-01-01' AND end_date <= '2017-01-31'\") \\\n  .save(\"/tmp/delta/events\")\n\nspark.conf.set(\"spark.databricks.delta.replaceWhere.constraintCheck.enabled\", False)\n```\n\nLANGUAGE: scala\nCODE:\n```\nreplace_data.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"replaceWhere\", \"start_date >= '2017-01-01' AND end_date <= '2017-01-31'\")\n  .save(\"/tmp/delta/events\")\n\nspark.conf.set(\"spark.databricks.delta.replaceWhere.constraintCheck.enabled\", false)\n```\n\n----------------------------------------\n\nTITLE: Merging Data into Delta Table using Scala\nDESCRIPTION: This Scala snippet demonstrates how to merge data from a source Delta table into a target Delta table using the Delta Lake API. It updates matched rows and inserts unmatched rows based on the 'id' column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\nval deltaTablePeople = DeltaTable.forPath(spark, \"/tmp/delta/people-10m\")\nval deltaTablePeopleUpdates = DeltaTable.forPath(spark, \"tmp/delta/people-10m-updates\")\nval dfUpdates = deltaTablePeopleUpdates.toDF()\n\ndeltaTablePeople\n  .as(\"people\")\n  .merge(\n    dfUpdates.as(\"updates\"),\n    \"people.id = updates.id\")\n  .whenMatched\n  .updateExpr(\n    Map(\n      \"id\" -> \"updates.id\",\n      \"firstName\" -> \"updates.firstName\",\n      \"middleName\" -> \"updates.middleName\",\n      \"lastName\" -> \"updates.lastName\",\n      \"gender\" -> \"updates.gender\",\n      \"birthDate\" -> \"updates.birthDate\",\n      \"ssn\" -> \"updates.ssn\",\n      \"salary\" -> \"updates.salary\"\n    ))\n  .whenNotMatched\n  .insertExpr(\n    Map(\n      \"id\" -> \"updates.id\",\n      \"firstName\" -> \"updates.firstName\",\n      \"middleName\" -> \"updates.middleName\",\n      \"lastName\" -> \"updates.lastName\",\n      \"gender\" -> \"updates.gender\",\n      \"birthDate\" -> \"updates.birthDate\",\n      \"ssn\" -> \"updates.ssn\",\n      \"salary\" -> \"updates.salary\"\n    ))\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Tables in SQL, Python, and Scala\nDESCRIPTION: Shows different methods to read Delta tables using SQL, PySpark, and Scala. Includes examples of querying tables by name from the metastore and by path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM default.people10m   -- query table in the metastore\n\nSELECT * FROM delta.`/tmp/delta/people10m`  -- query table by path\n```\n\nLANGUAGE: python\nCODE:\n```\nspark.table(\"default.people10m\")    # query table in the metastore\n\nspark.read.format(\"delta\").load(\"/tmp/delta/people10m\")  # query table by path\n```\n\nLANGUAGE: scala\nCODE:\n```\nspark.table(\"default.people10m\")      // query table in the metastore\n\nspark.read.format(\"delta\").load(\"/tmp/delta/people10m\")  // create table by path\n\nimport io.delta.implicits._\nspark.read.delta(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Merging Data into Delta Table using SQL\nDESCRIPTION: This SQL snippet demonstrates how to merge data from a source table 'people10mupdates' into a target table 'people10m'. It updates matched rows and inserts unmatched rows based on the 'id' column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO people10m\nUSING people10mupdates\nON people10m.id = people10mupdates.id\nWHEN MATCHED THEN\n  UPDATE SET\n    id = people10mupdates.id,\n    firstName = people10mupdates.firstName,\n    middleName = people10mupdates.middleName,\n    lastName = people10mupdates.lastName,\n    gender = people10mupdates.gender,\n    birthDate = people10mupdates.birthDate,\n    ssn = people10mupdates.ssn,\n    salary = people10mupdates.salary\nWHEN NOT MATCHED\n  THEN INSERT (\n    id,\n    firstName,\n    middleName,\n    lastName,\n    gender,\n    birthDate,\n    ssn,\n    salary\n  )\n  VALUES (\n    people10mupdates.id,\n    people10mupdates.firstName,\n    people10mupdates.middleName,\n    people10mupdates.lastName,\n    people10mupdates.gender,\n    people10mupdates.birthDate,\n    people10mupdates.ssn,\n    people10mupdates.salary\n  )\n```\n\n----------------------------------------\n\nTITLE: Installing PySpark for Delta Lake\nDESCRIPTION: Command to install the PySpark version compatible with Delta Lake using pip. This is the first step for setting up the Python environment for Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pyspark==<compatible-spark-version>\n```\n\n----------------------------------------\n\nTITLE: Enabling Change Data Feed in SQL\nDESCRIPTION: SQL commands to enable Change Data Feed for new and existing Delta tables using table properties.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-change-data-feed.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n```\n\nLANGUAGE: sql\nCODE:\n```\nset spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n```\n\n----------------------------------------\n\nTITLE: Implementing Change Data Capture (CDC) with Delta Lake in Scala\nDESCRIPTION: This snippet demonstrates how to apply change data capture (CDC) operations to a Delta table using merge in Scala. It processes a DataFrame containing updates, deletes, and inserts from an external system, finds the latest change for each key, and applies those changes to the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_27\n\nLANGUAGE: scala\nCODE:\n```\nval deltaTable: DeltaTable = ... // DeltaTable with schema (key, value)\n\n// DataFrame with changes having following columns\n// - key: key of the change\n// - time: time of change for ordering between changes (can replaced by other ordering id)\n// - newValue: updated or inserted value if key was not deleted\n// - deleted: true if the key was deleted, false if the key was inserted or updated\nval changesDF: DataFrame = ...\n\n// Find the latest change for each key based on the timestamp\n// Note: For nested structs, max on struct is computed as\n// max on first struct field, if equal fall back to second fields, and so on.\nval latestChangeForEachKey = changesDF\n  .selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\" )\n  .groupBy(\"key\")\n  .agg(max(\"otherCols\").as(\"latest\"))\n  .selectExpr(\"key\", \"latest.*\")\n\ndeltaTable.as(\"t\")\n  .merge(\n    latestChangeForEachKey.as(\"s\"),\n    \"s.key = t.key\")\n  .whenMatched(\"s.deleted = true\")\n  .delete()\n  .whenMatched()\n  .updateExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\n  .whenNotMatched(\"s.deleted = false\")\n  .insertExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Scanning a Delta Table with Filters in Java\nDESCRIPTION: Demonstrates how to perform a simple table scan with filters using the Delta Kernel API. This snippet shows the process of creating an engine, defining a table, getting a snapshot, building a scan, and transforming physical data.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/README.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nEngine myEngine = DefaultEngine.create() ;                  // define a engine (more details below)\nTable myTable = Table.forPath(\"/delta/table/path\");         // define what table to scan\nSnapshot mySnapshot = myTable.getLatestSnapshot(myEngine);  // define which version of table to scan\nScan myScan = mySnapshot.getScanBuilder(myEngine)           // specify the scan details\n  .withFilters(myEngine, scanFilter)\n  .build();\nCloseableIterator<ColumnarBatch> physicalData =             // read the Parquet data files\n  .. read from Parquet data files ...\nScan.transformPhysicalData(...)                             // returns the table data\n```\n\n----------------------------------------\n\nTITLE: Filtering Delta Table Scans with Expressions in Java\nDESCRIPTION: This snippet demonstrates how to use Delta Kernel's expression framework to filter scans and skip files based on query predicates. It shows creating a filter for a partition column and applying it to a scan builder.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.expressions.*;\n\nimport io.delta.kernel.defaults.engine.*;\n\nEngine myEngine = DefaultEngine.create(new Configuration());\n\nPredicate filter = new Predicate(\n  \"=\",\n  Arrays.asList(new Column(\"columnX\"), Literal.ofInt(1)));\n\nScan myFilteredScan = mySnapshot.getScanBuilder().withFilter(filter).build()\n\n// Subset of the given filter that is not guaranteed to be satisfied by\n// Delta Kernel when it returns data. This filter is used by Delta Kernel\n// to do data skipping as much as possible. The connector should use this filter\n// on top of the data returned by Delta Kernel in order for further filtering.\nOptional<Predicate> remainingFilter = myFilteredScan.getRemainingFilter();\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table History in SQL\nDESCRIPTION: SQL commands to retrieve the history of operations performed on a Delta table, with options to specify limits or filter results. Shows operations in reverse chronological order.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table Data in Java\nDESCRIPTION: Demonstrates the complete process of reading data from a Delta table, including iterating through scan files, reading physical data from Parquet files, and transforming it into logical data. It also shows how to handle selection vectors and access column data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nCloserableIterator<FilteredColumnarBatch> fileIter = scanObject.getScanFiles(myEngine);\n\nRow scanStateRow = scanObject.getScanState(myEngine);\n\nwhile(fileIter.hasNext()) {\n  FilteredColumnarBatch scanFileColumnarBatch = fileIter.next();\n\n  // Get the physical read schema of columns to read from the Parquet data files\n  StructType physicalReadSchema =\n    ScanStateRow.getPhysicalDataReadSchema(engine, scanStateRow);\n\n  try (CloseableIterator<Row> scanFileRows = scanFileColumnarBatch.getRows()) {\n    while (scanFileRows.hasNext()) {\n      Row scanFileRow = scanFileRows.next();\n\n      // From the scan file row, extract the file path, size and modification time metadata\n      // needed to read the file.\n      FileStatus fileStatus = InternalScanFileUtils.getAddFileStatus(scanFileRow);\n\n      // Open the scan file which is a Parquet file using connector's own\n      // Parquet reader or default Parquet reader provided by the Kernel (which\n      // is used in this example).\n      CloseableIterator<ColumnarBatch> physicalDataIter =\n        engine.getParquetHandler().readParquetFiles(\n          singletonCloseableIterator(fileStatus),\n          physicalReadSchema,\n          Optional.empty() /* optional predicate the connector can apply to filter data from the reader */\n        );\n\n      // Now the physical data read from the Parquet data file is converted to a table\n      // logical data. Logical data may include the addition of partition columns and/or\n      // subset of rows deleted\n      try (\n         CloseableIterator<FilteredColumnarBatch> transformedData =\n           Scan.transformPhysicalData(\n             engine,\n             scanStateRow,\n             scanFileRow,\n             physicalDataIter)) {\n        while (transformedData.hasNext()) {\n          FilteredColumnarBatch logicalData = transformedData.next();\n          ColumnarBatch dataBatch = logicalData.getData();\n\n          // Not all rows in `dataBatch` are in the selected output.\n          // An optional selection vector determines whether a row with a\n          // specific row index is in the final output or not.\n          Optional<ColumnVector> selectionVector = dataReadResult.getSelectionVector();\n\n          // access the data for the column at ordinal 0\n          ColumnVector column0 = dataBatch.getColumnVector(0);\n          for (int rowIndex = 0; rowIndex < column0.getSize(); rowIndex++) {\n            // check if the row is selected or not\n            if (!selectionVector.isPresent() || // there is no selection vector, all records are selected\n               (!selectionVector.get().isNullAt(rowId) && selectionVector.get().getBoolean(rowId)))  {\n              // Assuming the column type is String.\n              // If it is a different type, call the relevant function on the `ColumnVector`\n              System.out.println(column0.getString(rowIndex));\n            }\n          }\n\n\t  // access the data for column at ordinal 1\n\t  ColumnVector column1 = dataBatch.getColumnVector(1);\n\t  for (int rowIndex = 0; rowIndex < column1.getSize(); rowIndex++) {\n            // check if the row is selected or not\n            if (!selectionVector.isPresent() || // there is no selection vector, all records are selected\n               (!selectionVector.get().isNullAt(rowId) && selectionVector.get().getBoolean(rowId)))  {\n              // Assuming the column type is Long.\n              // If it is a different type, call the relevant function on the `ColumnVector`\n              System.out.println(column1.getLong(rowIndex));\n            }\n          }\n\t  // .. more ..\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Storage Credentials for Delta Lake in Scala\nDESCRIPTION: This code snippet illustrates how to pass Azure storage account credentials through DataFrame options when reading from and writing to Delta Lake tables in Scala. It demonstrates reading from two different sources with different credentials and then writing the combined data with a third set of credentials.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_45\n\nLANGUAGE: scala\nCODE:\n```\nval df1 = spark.read.format(\"delta\")\n  .option(\"fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\", \"<storage-account-access-key-1>\")\n  .read(\"...\")\nval df2 = spark.read.format(\"delta\")\n  .option(\"fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\", \"<storage-account-access-key-2>\")\n  .read(\"...\")\ndf1.union(df2).write.format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\", \"<storage-account-access-key-3>\")\n  .save(\"...\")\n```\n\n----------------------------------------\n\nTITLE: Compacting Delta Table Files in Scala and Python\nDESCRIPTION: Demonstrates how to compact a Delta table by repartitioning it into a smaller number of files. The operation uses the 'dataChange' option set to false to minimize impact on concurrent operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/best-practices.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval path = \"...\"\nval numFiles = 16\n\nspark.read\n .format(\"delta\")\n .load(path)\n .repartition(numFiles)\n .write\n .option(\"dataChange\", \"false\")\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(path)\n```\n\nLANGUAGE: python\nCODE:\n```\npath = \"...\"\nnumFiles = 16\n\n(spark.read\n .format(\"delta\")\n .load(path)\n .repartition(numFiles)\n .write\n .option(\"dataChange\", \"false\")\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(path))\n```\n\n----------------------------------------\n\nTITLE: Performing File Compaction with OPTIMIZE in SQL\nDESCRIPTION: Demonstrates how to use the OPTIMIZE command to compact small files into larger ones in Delta Lake tables. The command can be used with path-based tables, named tables, or with a WHERE clause to optimize specific partitions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nOPTIMIZE '/path/to/delta/table' -- Optimizes the path-based Delta Lake table\n\nOPTIMIZE delta_table_name;\n\nOPTIMIZE delta.`/path/to/delta/table`;\n\n-- If you have a large amount of data and only want to optimize a subset of it, you can specify an optional partition predicate using `WHERE`:\nOPTIMIZE delta_table_name WHERE date >= '2017-01-01'\n```\n\n----------------------------------------\n\nTITLE: Reading Change Data Feed in Streaming Queries\nDESCRIPTION: Python and Scala examples showing how to read change data using streaming DataFrame APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-change-data-feed.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nspark.readStream.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingVersion\", 0) \\\n  .table(\"myDeltaTable\")\n\nspark.readStream.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n  .load(\"/pathToMyDeltaTable\")\n\nspark.readStream.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .table(\"myDeltaTable\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingVersion\", 0)\n  .table(\"myDeltaTable\")\n\nspark.readStream.format(\"delta\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n  .load(\"/pathToMyDeltaTable\")\n\nspark.readStream.format(\"delta\")\n  .option(\"readChangeFeed\", \"true\")\n  .table(\"myDeltaTable\")\n```\n\n----------------------------------------\n\nTITLE: Starting PySpark Shell with Delta Lake\nDESCRIPTION: Command to launch the PySpark shell with Delta Lake package and necessary configurations. This enables interactive Python development with Delta Lake functionality.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npyspark --packages io.delta:delta-spark_2.12:3.3.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Liquid Clustering on Existing Tables with ALTER TABLE\nDESCRIPTION: SQL syntax for enabling liquid clustering on an existing unpartitioned Delta table. This command adds clustering capabilities to tables that weren't originally created with clustering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name>\nCLUSTER BY (<clustering_columns>)\n```\n\n----------------------------------------\n\nTITLE: Merging Data into Delta Table using Python\nDESCRIPTION: This Python snippet shows how to merge data from a source Delta table into a target Delta table using the Delta Lake API. It updates matched rows and inserts unmatched rows based on the 'id' column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTablePeople = DeltaTable.forPath(spark, '/tmp/delta/people-10m')\ndeltaTablePeopleUpdates = DeltaTable.forPath(spark, '/tmp/delta/people-10m-updates')\n\ndfUpdates = deltaTablePeopleUpdates.toDF()\n\ndeltaTablePeople.alias('people') \\\n  .merge(\n    dfUpdates.alias('updates'),\n    'people.id = updates.id'\n  ) \\\n  .whenMatchedUpdate(set =\n    {\n      \"id\": \"updates.id\",\n      \"firstName\": \"updates.firstName\",\n      \"middleName\": \"updates.middleName\",\n      \"lastName\": \"updates.lastName\",\n      \"gender\": \"updates.gender\",\n      \"birthDate\": \"updates.birthDate\",\n      \"ssn\": \"updates.ssn\",\n      \"salary\": \"updates.salary\"\n    }\n  ) \\\n  .whenNotMatchedInsert(values =\n    {\n      \"id\": \"updates.id\",\n      \"firstName\": \"updates.firstName\",\n      \"middleName\": \"updates.middleName\",\n      \"lastName\": \"updates.lastName\",\n      \"gender\": \"updates.gender\",\n      \"birthDate\": \"updates.birthDate\",\n      \"ssn\": \"updates.ssn\",\n      \"salary\": \"updates.salary\"\n    }\n  ) \\\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Merge with Specific Column Insert (SQL)\nDESCRIPTION: Illustrates a merge operation that inserts specific columns into the target table. This example shows how schema evolution handles adding a new column during insertion.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_22\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO target_table t\nUSING source_table s\nON t.key = s.key\nWHEN NOT MATCHED\n  THEN INSERT (key, new_value) VALUES (s.key, s.new_value)\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table with DataFrameWriter API\nDESCRIPTION: Use the Spark DataFrameWriter API to create Delta tables and simultaneously insert data. This method supports creating tables in the metastore or by path, with options for overwriting existing data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create table in the metastore using DataFrame's schema and write data to it\ndf.write.format(\"delta\").saveAsTable(\"default.people10m\")\n\n# Create or replace partitioned table with path using DataFrame's schema and write/overwrite data to it\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/people10m\")\n```\n\nLANGUAGE: scala\nCODE:\n```\n// Create table in the metastore using DataFrame's schema and write data to it\ndf.write.format(\"delta\").saveAsTable(\"default.people10m\")\n\n// Create table with path using DataFrame's schema and write data to it\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Restoring Delta Tables Using Multiple Languages\nDESCRIPTION: Examples of restoring Delta tables to specific versions or timestamps using different programming languages. Supports both path-based and Hive metastore-based tables. Can restore to version numbers or specific timestamps in 'yyyy-MM-dd HH:mm:ss' format.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nRESTORE TABLE db.target_table TO VERSION AS OF <version>\nRESTORE TABLE delta.`/data/target/` TO TIMESTAMP AS OF <timestamp>\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, <path-to-table>)  # path-based tables, or\ndeltaTable = DeltaTable.forName(spark, <table-name>)    # Hive metastore-based tables\n\ndeltaTable.restoreToVersion(0) # restore table to oldest version\n\ndeltaTable.restoreToTimestamp('2019-02-14') # restore to a specific timestamp\n```\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, <path-to-table>)\nval deltaTable = DeltaTable.forName(spark, <table-name>)\n\ndeltaTable.restoreToVersion(0) // restore table to oldest version\n\ndeltaTable.restoreToTimestamp(\"2019-02-14\") // restore to a specific timestamp\n```\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, <path-to-table>);\nDeltaTable deltaTable = DeltaTable.forName(spark, <table-name>);\n\ndeltaTable.restoreToVersion(0) // restore table to oldest version\n\ndeltaTable.restoreToTimestamp(\"2019-02-14\") // restore to a specific timestamp\n```\n\n----------------------------------------\n\nTITLE: Querying Delta Tables in Batch and Streaming Modes\nDESCRIPTION: These SQL snippets show how to query Delta tables in both batch (default) and streaming modes using SELECT statements. The streaming mode uses a hint to specify the mode option.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM testTable;\n\nSELECT * FROM testTable /*+ OPTIONS('mode' = 'streaming') */;\n\nSELECT col1, col2, col3 FROM testTable;\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Table Manifests in Apache Spark\nDESCRIPTION: This code snippet demonstrates how to generate manifest files for a Delta table using different programming languages in Apache Spark. The 'generate' operation creates symlink format manifests for the specified Delta table path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/snowflake-integration.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGENERATE symlink_format_manifest FOR TABLE delta.`<path-to-delta-table>`\n```\n\nLANGUAGE: scala\nCODE:\n```\nval deltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\nLANGUAGE: java\nCODE:\n```\nDeltaTable deltaTable = DeltaTable.forPath(<path-to-delta-table>);\ndeltaTable.generate(\"symlink_format_manifest\");\n```\n\nLANGUAGE: python\nCODE:\n```\ndeltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\n----------------------------------------\n\nTITLE: Creating External Delta Table in Hive SQL\nDESCRIPTION: SQL command to create an external Hive table that points to a Delta table, using the Delta Storage Handler.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/hive/README.md#2025-04-22_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE EXTERNAL TABLE deltaTable(col1 INT, col2 STRING)\nSTORED BY 'io.delta.hive.DeltaStorageHandler'\nLOCATION '/delta/table/path'\n```\n\n----------------------------------------\n\nTITLE: Idempotent Delta Table Writes in foreachBatch (Python)\nDESCRIPTION: This example demonstrates how to perform idempotent writes to Delta tables using foreachBatch. It uses txnAppId and txnVersion options to identify and ignore duplicate writes.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\napp_id = ... # A unique string that is used as an application ID.\n\ndef writeToDeltaLakeTableIdempotent(batch_df, batch_id):\n  batch_df.write.format(...).option(\"txnVersion\", batch_id).option(\"txnAppId\", app_id).save(...) # location 1\n  batch_df.write.format(...).option(\"txnVersion\", batch_id).option(\"txnAppId\", app_id).save(...) # location 2\n```\n\n----------------------------------------\n\nTITLE: Time Travel Queries on Delta Lake Tables\nDESCRIPTION: Demonstrates how to query previous versions of a Delta Lake table using time travel. This feature allows access to historical data by specifying the version number or timestamp of the desired snapshot.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM delta.`/tmp/delta-table` VERSION AS OF 0;\n```\n\nLANGUAGE: Python\nCODE:\n```\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\ndf.show()\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\ndf.show()\n```\n\nLANGUAGE: Java\nCODE:\n```\nDataset<Row> df = spark.read().format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\");\ndf.show();\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Stream with ignoreDeletes Option in Scala\nDESCRIPTION: Demonstrates how to configure a Delta streaming source to ignore delete operations at partition boundaries. This allows the stream to continue even when data is deleted from the source table based on partition columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .option(\"ignoreDeletes\", \"true\")\n  .load(\"/tmp/delta/user_events\")\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Delta Table Files in Java\nDESCRIPTION: This code shows the process of writing data to Delta table files on worker nodes. It includes deserializing transaction state, transforming logical data to physical data, writing Parquet files, and generating append actions for the Delta log.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_41\n\nLANGUAGE: java\nCODE:\n```\nRow txnState = ... deserialize from JSON string sent by the driver ...\n\nCloseableIterator<FilteredColumnarBatch> data = ... generate data ...\n\n// If the table is un-partitioned then this is an empty map\nMap<String, Literal> partitionValues = ... prepare the partition values ...\n\n\n// First transform the logical data to physical data that needs to be written\n// to the Parquet files\nCloseableIterator<FilteredColumnarBatch> physicalData =\n  Transaction.transformLogicalData(engine, txnState, data, partitionValues);\n\n// Get the write context\nDataWriteContext writeContext = Transaction.getWriteContext(engine, txnState, partitionValues);\n\n// Now write the physical data to Parquet files\nCloseableIterator<DataFileStatus> dataFiles =\n  engine.getParquetHandler()\n    .writeParquetFiles(\n      writeContext.getTargetDirectory(),\n      physicalData,\n      writeContext.getStatisticsColumns());\n\n// Now convert the data file status to data actions that needs to be written to the Delta table log\nCloseableIterator<Row> partitionDataActions =\n  Transaction.generateAppendActions(\n    engine,\n    txnState,\n    dataFiles,\n    writeContext);\n\n.... serialize `partitionDataActions` and send them to driver node\n```\n\n----------------------------------------\n\nTITLE: Performing File Compaction with OPTIMIZE in Python\nDESCRIPTION: Shows how to use the Delta Lake Python API to perform file compaction (optimize) on Delta tables. The API supports both path-based and Hive metastore-based tables, with an optional where clause for partition filtering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)  # For path-based tables\n# For Hive metastore-based tables: deltaTable = DeltaTable.forName(spark, tableName)\n\ndeltaTable.optimize().executeCompaction()\n\n# If you have a large amount of data and only want to optimize a subset of it, you can specify an optional partition predicate using `where`\ndeltaTable.optimize().where(\"date='2021-11-18'\").executeCompaction()\n```\n\n----------------------------------------\n\nTITLE: Time Travel Queries in SQL\nDESCRIPTION: Demonstrates how to perform time travel queries on Delta tables using SQL. Examples show querying a table at a specific timestamp or version.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM default.people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\nSELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n```\n\n----------------------------------------\n\nTITLE: Merging Data into Delta Table using Java\nDESCRIPTION: This Java snippet shows how to merge data from a source Delta table into a target Delta table using the Delta Lake API. It updates matched rows and inserts unmatched rows based on the 'id' column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\nimport org.apache.spark.sql.functions;\nimport java.util.HashMap;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, \"/tmp/delta/people-10m\")\nDataset<Row> dfUpdates = spark.read(\"delta\").load(\"/tmp/delta/people-10m-updates\")\n\ndeltaTable\n  .as(\"people\")\n  .merge(\n    dfUpdates.as(\"updates\"),\n    \"people.id = updates.id\")\n  .whenMatched()\n  .updateExpr(\n    new HashMap<String, String>() {{\n      put(\"id\", \"updates.id\");\n      put(\"firstName\", \"updates.firstName\");\n      put(\"middleName\", \"updates.middleName\");\n      put(\"lastName\", \"updates.lastName\");\n      put(\"gender\", \"updates.gender\");\n      put(\"birthDate\", \"updates.birthDate\");\n      put(\"ssn\", \"updates.ssn\");\n      put(\"salary\", \"updates.salary\");\n    }})\n  .whenNotMatched()\n  .insertExpr(\n    new HashMap<String, String>() {{\n      put(\"id\", \"updates.id\");\n      put(\"firstName\", \"updates.firstName\");\n      put(\"middleName\", \"updates.middleName\");\n      put(\"lastName\", \"updates.lastName\");\n      put(\"gender\", \"updates.gender\");\n      put(\"birthDate\", \"updates.birthDate\");\n      put(\"ssn\", \"updates.ssn\");\n      put(\"salary\", \"updates.salary\");\n    }})\n  .execute();\n```\n\n----------------------------------------\n\nTITLE: Reading from a Delta Table with Scala\nDESCRIPTION: Scala code to read data from a Delta table and display the results. This loads the table into a DataFrame and then prints its contents.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\nval df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Partitioning Delta Tables\nDESCRIPTION: Partition Delta tables to optimize query performance. This can be done using SQL DDL, DataFrameWriter API, or DeltaTableBuilder API. The example shows partitioning by the 'gender' column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE default.people10m (\n  id INT,\n  firstName STRING,\n  middleName STRING,\n  lastName STRING,\n  gender STRING,\n  birthDate TIMESTAMP,\n  ssn STRING,\n  salary INT\n)\nUSING DELTA\nPARTITIONED BY (gender)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.write.format(\"delta\").partitionBy(\"gender\").saveAsTable(\"default.people10m\")\n\nDeltaTable.create(spark) \\\n  .tableName(\"default.people10m\") \\\n  .addColumn(\"id\", \"INT\") \\\n  .addColumn(\"firstName\", \"STRING\") \\\n  .addColumn(\"middleName\", \"STRING\") \\\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n  .addColumn(\"gender\", \"STRING\") \\\n  .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n  .addColumn(\"ssn\", \"STRING\") \\\n  .addColumn(\"salary\", \"INT\") \\\n  .partitionedBy(\"gender\") \\\n  .execute()\n```\n\nLANGUAGE: scala\nCODE:\n```\ndf.write.format(\"delta\").partitionBy(\"gender\").saveAsTable(\"default.people10m\")\n\nDeltaTable.createOrReplace(spark)\n  .tableName(\"default.people10m\")\n  .addColumn(\"id\", \"INT\")\n  .addColumn(\"firstName\", \"STRING\")\n  .addColumn(\"middleName\", \"STRING\")\n  .addColumn(\n    DeltaTable.columnBuilder(\"lastName\")\n      .dataType(\"STRING\")\n      .comment(\"surname\")\n      .build())\n  .addColumn(\"lastName\", \"STRING\", comment = \"surname\")\n  .addColumn(\"gender\", \"STRING\")\n  .addColumn(\"birthDate\", \"TIMESTAMP\")\n  .addColumn(\"ssn\", \"STRING\")\n  .addColumn(\"salary\", \"INT\")\n  .partitionedBy(\"gender\")\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Streaming from Delta Sharing Tables\nDESCRIPTION: Demonstrates how to set up streaming reads from Delta Sharing tables using Spark Structured Streaming. Requires history sharing to be enabled by the data provider.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-sharing.md#2025-04-22_snippet_3\n\nLANGUAGE: Scala\nCODE:\n```\nval tablePath = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\"\n\nspark.readStream.format(\"deltaSharing\").load(tablePath)\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Table Stream in Complete Mode (Python)\nDESCRIPTION: This snippet shows how to write a stream to a Delta table in complete mode, replacing the entire table with each batch. It includes an aggregation example.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n(spark.readStream\n  .format(\"delta\")\n  .load(\"/tmp/delta/events\")\n  .groupBy(\"customerId\")\n  .count()\n  .writeStream\n  .format(\"delta\")\n  .outputMode(\"complete\")\n  .option(\"checkpointLocation\", \"/tmp/delta/eventsByCustomer/_checkpoints/\")\n  .start(\"/tmp/delta/eventsByCustomer\")\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Iceberg Table to Delta Format\nDESCRIPTION: Demonstrates how to convert an Iceberg table to a Delta table in-place, assuming the underlying file format is Parquet. This feature is available from Delta Lake 2.3 and above.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\n-- Convert the Iceberg table in the path <path-to-table>.\nCONVERT TO DELTA iceberg.`<path-to-table>`\n\n-- Convert the Iceberg table in the path <path-to-table> without collecting statistics.\nCONVERT TO DELTA iceberg.`<path-to-table>` NO STATISTICS\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Merge with Specific Column Update (SQL)\nDESCRIPTION: Shows a merge operation that updates a specific column in the target table. This example demonstrates how schema evolution handles adding a new column to the target table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO target_table t\nUSING source_table s\nON t.key = s.key\nWHEN MATCHED\n  THEN UPDATE SET new_value = s.new_value\n```\n\n----------------------------------------\n\nTITLE: Idempotent Delta Table Writes in foreachBatch (Scala)\nDESCRIPTION: This snippet shows how to achieve idempotent writes to Delta tables using foreachBatch in Scala. It uses txnAppId and txnVersion options to handle duplicate writes correctly.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_11\n\nLANGUAGE: scala\nCODE:\n```\nval appId = ... // A unique string that is used as an application ID.\nstreamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>\n  batchDF.write.format(...).option(\"txnVersion\", batchId).option(\"txnAppId\", appId).save(...)  // location 1\n  batchDF.write.format(...).option(\"txnVersion\", batchId).option(\"txnAppId\", appId).save(...)  // location 2\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Java\nDESCRIPTION: Java code to create a Delta table from a Dataset. This creates a table with a single column containing values 0-4 at the specified location.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\n\nSparkSession spark = ...   // create SparkSession\n\nDataset<Row> data = spark.range(0, 5);\ndata.write().format(\"delta\").save(\"/tmp/delta-table\");\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Table Manifests in Scala\nDESCRIPTION: Scala code to generate symlink format manifests for a Delta table using the DeltaTable API. This creates manifest files containing the names of data files to be read for querying the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/presto-integration.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval deltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\n----------------------------------------\n\nTITLE: Dynamic Partition Overwrites\nDESCRIPTION: Examples demonstrating dynamic partition overwrite mode for partitioned Delta tables. Shows configuration and usage across different APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_21\n\nLANGUAGE: sql\nCODE:\n```\nSET spark.sql.sources.partitionOverwriteMode=dynamic;\nINSERT OVERWRITE TABLE default.people10m SELECT * FROM morePeople;\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"partitionOverwriteMode\", \"dynamic\") \\\n  .saveAsTable(\"default.people10m\")\n```\n\nLANGUAGE: scala\nCODE:\n```\ndf.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"partitionOverwriteMode\", \"dynamic\")\n  .saveAsTable(\"default.people10m\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession and Querying Delta Shared Tables in Scala\nDESCRIPTION: This code snippet demonstrates how to configure a SparkSession for Delta Sharing and perform batch, CDF, and streaming queries on shared Delta tables. It includes setting up necessary configurations and options for reading tables with advanced features like Deletion Vectors and Column Mapping.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-sharing.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession\n        .builder()\n        .appName(\"...\")\n        .master(\"...\")\n        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n        .getOrCreate()\n\nval tablePath = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\"\n\n// Batch query\nspark.read.format(\"deltaSharing\").load(tablePath)\n\n// CDF query\nspark.read.format(\"deltaSharing\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"responseFormat\", \"delta\")\n  .option(\"startingVersion\", 1)\n  .load(tablePath)\n\n// Streaming query\nspark.readStream.format(\"deltaSharing\").option(\"responseFormat\", \"delta\").load(tablePath)\n```\n\n----------------------------------------\n\nTITLE: Advanced Merge Operation with Conditions using Scala\nDESCRIPTION: This Scala snippet demonstrates an advanced merge operation with conditions on the WHEN NOT MATCHED BY SOURCE clause and specifies values to update in unmatched target rows.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\ntargetDF\n  .merge(sourceDF, \"source.key = target.key\")\n  .whenMatched()\n  .updateExpr(Map(\"target.lastSeen\" -> \"source.timestamp\"))\n  .whenNotMatched()\n  .insertExpr(Map(\n    \"target.key\" -> \"source.key\",\n    \"target.lastSeen\" -> \"source.timestamp\",\n    \"target.status\" -> \"'active'\",\n    )\n  )\n  .whenNotMatchedBySource(\"target.lastSeen >= (current_date() - INTERVAL '5' DAY)\")\n  .updateExpr(Map(\"target.status\" -> \"'inactive'\"))\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Time Travel Queries using DataFrameReader in Python\nDESCRIPTION: Shows how to perform time travel queries on Delta tables using DataFrameReader options in PySpark. Examples include querying a table at a specific timestamp or version.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf1 = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_string).load(\"/tmp/delta/people10m\")\ndf2 = spark.read.format(\"delta\").option(\"versionAsOf\", version).load(\"/tmp/delta/people10m\")\n\nhistory = spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta/people10m`\")\nlatest_version = history.selectExpr(\"max(version)\").collect()\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version[0][0]).load(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Merge with Schema Evolution (SQL)\nDESCRIPTION: Demonstrates a merge operation with schema evolution enabled. This example shows how to handle cases where the source table has additional columns or the target table has missing columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_20\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO target_table t\nUSING source_table s\nON t.key = s.key\nWHEN MATCHED\n  THEN UPDATE SET *\nWHEN NOT MATCHED\n  THEN INSERT *\n```\n\n----------------------------------------\n\nTITLE: Enabling Deletion Vectors in Delta Lake Tables with SQL\nDESCRIPTION: SQL command to enable deletion vectors on a Delta Lake table by setting the appropriate table property. When enabled, this feature allows for marking rows as deleted without rewriting entire Parquet files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-deletion-vectors.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> SET TBLPROPERTIES ('delta.enableDeletionVectors' = true);\n```\n\n----------------------------------------\n\nTITLE: Replacing Delta Table Content with Schema Changes\nDESCRIPTION: Demonstrates how to replace the entire content of a Delta table, including schema changes. This approach is atomic and preserves the old version for potential recovery using Time Travel.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/best-practices.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataframe.write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"overwriteSchema\", \"true\") \\\n  .partitionBy(<your-partition-columns>) \\\n  .saveAsTable(\"<your-table>\") # Managed table\ndataframe.write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"overwriteSchema\", \"true\") \\\n  .option(\"path\", \"<your-table-path>\") \\\n  .partitionBy(<your-partition-columns>) \\\n  .saveAsTable(\"<your-table>\") # External table\n```\n\nLANGUAGE: sql\nCODE:\n```\nREPLACE TABLE <your-table> USING DELTA PARTITIONED BY (<your-partition-columns>) AS SELECT ... -- Managed table\nREPLACE TABLE <your-table> USING DELTA PARTITIONED BY (<your-partition-columns>) LOCATION \"<your-table-path>\" AS SELECT ... -- External table\n```\n\nLANGUAGE: scala\nCODE:\n```\ndataframe.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", \"true\")\n  .partitionBy(<your-partition-columns>)\n  .saveAsTable(\"<your-table>\") // Managed table\ndataframe.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", \"true\")\n  .option(\"path\", \"<your-table-path>\")\n  .partitionBy(<your-partition-columns>)\n  .saveAsTable(\"<your-table>\") // External table\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Identity Columns in Python\nDESCRIPTION: This snippet demonstrates how to create a Delta table with identity columns using Python. It shows various ways to configure identity columns, including always generated and generated by default.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import DeltaTable, IdentityGenerator\nfrom pyspark.sql.types import LongType\n\nDeltaTable.create()\n  .tableName(\"table_name\")\n  .addColumn(\"id_col1\", dataType=LongType(), generatedAlwaysAs=IdentityGenerator())\n  .addColumn(\"id_col2\", dataType=LongType(), generatedAlwaysAs=IdentityGenerator(start=-1, step=1))\n  .addColumn(\"id_col3\", dataType=LongType(), generatedByDefaultAs=IdentityGenerator())\n  .addColumn(\"id_col4\", dataType=LongType(), generatedByDefaultAs=IdentityGenerator(start=-1, step=1))\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with SQL\nDESCRIPTION: SQL command to create a Delta table from a simple VALUES query. This creates a new Delta table at the specified location with a 'id' column containing values 0-4.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE delta.`/tmp/delta-table` USING DELTA AS SELECT col1 as id FROM VALUES 0,1,2,3,4;\n```\n\n----------------------------------------\n\nTITLE: Creating a DeltaSink with Multi-Cluster Support for S3 Storage in Java\nDESCRIPTION: This example demonstrates how to create a DeltaSink with multi-cluster configuration for AWS S3 storage. It configures the required parameters for S3 access and DynamoDB log store implementation.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createDeltaSink(\n        DataStream<RowData> stream,\n        String deltaTablePath) {\n    String[] partitionCols = { \"surname\" };\n\n    Configuration configuration = new Configuration();\n    configuration.set(\"spark.hadoop.fs.s3a.access.key\", \"USE_YOUR_S3_ACCESS_KEY_HERE\");\n    configuration.set(\"spark.hadoop.fs.s3a.secret.key\", \"USE_YOUR_S3_SECRET_KEY_HERE\");\n    configuration.set(\"spark.delta.logStore.s3a.impl\", \"io.delta.storage.S3DynamoDBLogStore\");\n    configuration.set(\"spark.io.delta.storage.S3DynamoDBLogStore.ddb.region\", \"eu-central-1\");\n        \n    DeltaSink<RowData> deltaSink = DeltaSink\n        .forRowData(\n            new Path(deltaTablePath),\n            configuration,\n            rowType)\n        .build();\n    stream.sinkTo(deltaSink);\n    return stream;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Table Manifest File\nDESCRIPTION: Demonstrates how to generate a manifest file for a Delta table that can be used by other processing engines. The manifest is generated using SQL, Python, Scala, or Java APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nGENERATE symlink_format_manifest FOR TABLE delta.`<path-to-delta-table>`\n```\n\nLANGUAGE: python\nCODE:\n```\ndeltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nval deltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\nLANGUAGE: java\nCODE:\n```\nDeltaTable deltaTable = DeltaTable.forPath(<path-to-delta-table>);\ndeltaTable.generate(\"symlink_format_manifest\");\n```\n\n----------------------------------------\n\nTITLE: Modifying Unmatched Rows in Delta Table using SQL\nDESCRIPTION: This SQL snippet demonstrates how to use the WHEN NOT MATCHED BY SOURCE clause to update or delete records in the target table that do not have corresponding records in the source table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO target\nUSING source\nON source.key = target.key\nWHEN MATCHED\n  UPDATE SET *\nWHEN NOT MATCHED\n  INSERT *\nWHEN NOT MATCHED BY SOURCE\n  DELETE\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Partitioned Generated Columns in Python\nDESCRIPTION: This example demonstrates creating a Delta table with generated columns used for partitioning. It shows how to create date-based partitions from a timestamp column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nDeltaTable.create(spark) \\\n  .tableName(\"default.events\") \\\n  .addColumn(\"eventId\", \"BIGINT\") \\\n  .addColumn(\"data\", \"STRING\") \\\n  .addColumn(\"eventType\", \"STRING\") \\\n  .addColumn(\"eventTime\", \"TIMESTAMP\") \\\n  .addColumn(\"eventDate\", \"DATE\", generatedAlwaysAs=\"CAST(eventTime AS DATE)\") \\\n  .partitionedBy(\"eventType\", \"eventDate\") \\\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Renaming Column in Delta Table (Python)\nDESCRIPTION: This code snippet shows how to rename a column in a Delta table using PySpark. It reads a table, renames the 'dateOfBirth' column to 'birthDate', and overwrites the table with the updated schema.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nspark.read.table(...) \\\n  .withColumnRenamed(\"dateOfBirth\", \"birthDate\") \\\n  .write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"overwriteSchema\", \"true\") \\\n  .saveAsTable(...)\n```\n\n----------------------------------------\n\nTITLE: Committing Delta Table Transaction in Java\nDESCRIPTION: This snippet demonstrates how to finalize a Delta table transaction by committing the data actions received from worker nodes. It includes creating an iterable from the actions, committing the transaction, and optionally checkpointing the table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_42\n\nLANGUAGE: java\nCODE:\n```\n// Create a iterable out of the data actions. If the contents are too big to fit in memory,\n// the connector may choose to write the data actions to a temporary file and return an\n// iterator that reads from the file.\nCloseableIterable<Row> dataActionsIterable = CloseableIterable.inMemoryIterable(\n\ttoCloseableIterator(dataActions.iterator()));\n\n// Commit the transaction.\nTransactionCommitResult commitResult = txn.commit(engine, dataActionsIterable);\n\n// Optional step\nif (commitResult.isReadyForCheckpoint()) {\n  // Checkpoint the table\n  Table.forPath(engine, tablePath).checkpoint(engine, commitResult.getVersion());\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Obsolete Files with Delta Vacuum in Scala\nDESCRIPTION: Scala examples for running vacuum operations on Delta tables to remove obsolete files, supporting both default and custom retention periods. Requires io.delta.tables package.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, pathToTable)\n\ndeltaTable.vacuum()        // vacuum files not required by versions older than the default retention period\n\ndeltaTable.vacuum(100)     // vacuum files not required by versions more than 100 hours old\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession with Delta Lake in Python\nDESCRIPTION: Python code to configure a SparkSession with Delta Lake using the pip-installed package. This utility function sets up all necessary configurations for Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyspark\nfrom delta import *\n\nbuilder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Table Stream in Append Mode (Scala)\nDESCRIPTION: This example shows how to write a stream to a Delta table in append mode using Scala. It includes two methods: using the format method and using Delta-specific implicits.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nevents.writeStream\n  .format(\"delta\")\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/tmp/delta/events/_checkpoints/\")\n  .start(\"/tmp/delta/events\")\n\nimport io.delta.implicits._\nevents.writeStream\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/tmp/delta/events/_checkpoints/\")\n  .delta(\"/tmp/delta/events\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Idempotent Writes to Delta Table in Java\nDESCRIPTION: This code shows how to configure a transaction for idempotent writes by setting a transaction identifier. This ensures that data with the same identifier is not written multiple times, which is particularly useful for incremental processing systems.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_21\n\nLANGUAGE: java\nCODE:\n```\n// Set the transaction identifiers for idempotent writes\n// Delta/Kernel makes sure that there exists only one transaction in the Delta log\n// with the given application id and txn version\ntxnBuilder =\n  txnBuilder.withTransactionId(\n    engine,\n    \"my app id\", /* application id */\n    100 /* monotonically increasing txn version with each new data insert */\n  );\n```\n\n----------------------------------------\n\nTITLE: Creating a DeltaSink for Partitioned Tables in Java\nDESCRIPTION: This example shows how to create a DeltaSink for writing to a partitioned Delta table. It configures the sink to use 'surname' as a partitioning column.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.flink.sink.DeltaBucketAssigner;\nimport io.delta.flink.sink.DeltaSinkBuilder;\n\npublic DataStream<RowData> createDeltaSink(\n        DataStream<RowData> stream,\n        String deltaTablePath) {\n    String[] partitionCols = { \"surname\" };\n    DeltaSink<RowData> deltaSink = DeltaSink\n        .forRowData(\n            new Path(deltaTablePath),\n            new Configuration(),\n            rowType)\n        .withPartitionColumns(partitionCols)\n        .build();\n    stream.sinkTo(deltaSink);\n    return stream;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding a CHECK Constraint to a Delta Table in SQL\nDESCRIPTION: This example demonstrates how to add a CHECK constraint to an existing Delta table. It adds a constraint named 'dateWithinRange' to ensure that the 'birthDate' column values are after January 1, 1900.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE default.people10m ADD CONSTRAINT dateWithinRange CHECK (birthDate > '1900-01-01');\n```\n\n----------------------------------------\n\nTITLE: Converting Parquet Table to Delta Format\nDESCRIPTION: Shows how to convert a Parquet table to a Delta table in-place, including options for partitioned tables and statistics collection. Examples are provided in SQL, Python, Scala, and Java.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n-- Convert unpartitioned Parquet table at path '<path-to-table>'\nCONVERT TO DELTA parquet.`<path-to-table>`\n\n-- Convert unpartitioned Parquet table and disable statistics collection\nCONVERT TO DELTA parquet.`<path-to-table>` NO STATISTICS\n\n-- Convert partitioned Parquet table at path '<path-to-table>' and partitioned by integer columns named 'part' and 'part2'\nCONVERT TO DELTA parquet.`<path-to-table>` PARTITIONED BY (part int, part2 int)\n\n-- Convert partitioned Parquet table and disable statistics collection\nCONVERT TO DELTA parquet.`<path-to-table>` NO STATISTICS PARTITIONED BY (part int, part2 int)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\n# Convert unpartitioned Parquet table at path '<path-to-table>'\ndeltaTable = DeltaTable.convertToDelta(spark, \"parquet.`<path-to-table>`\")\n\n# Convert partitioned parquet table at path '<path-to-table>' and partitioned by integer column named 'part'\npartitionedDeltaTable = DeltaTable.convertToDelta(spark, \"parquet.`<path-to-table>`\", \"part int\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\n// Convert unpartitioned Parquet table at path '<path-to-table>'\nval deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`<path-to-table>`\")\n\n// Convert partitioned Parquet table at path '<path-to-table>' and partitioned by integer columns named 'part' and 'part2'\nval partitionedDeltaTable = DeltaTable.convertToDelta(spark, \"parquet.`<path-to-table>`\", \"part int, part2 int\")\n```\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\n\n// Convert unpartitioned Parquet table at path '<path-to-table>'\nDeltaTable deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`<path-to-table>`\");\n\n// Convert partitioned Parquet table at path '<path-to-table>' and partitioned by integer columns named 'part' and 'part2'\nDeltaTable deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`<path-to-table>`\", \"part int, part2 int\");\n```\n\n----------------------------------------\n\nTITLE: Specifying Delta Protocol with Reader and Writer Features (JSON)\nDESCRIPTION: This JSON snippet shows a Delta protocol definition that includes both reader and writer features. It specifies different versions and features for readers and writers, allowing for more granular control over table compatibility.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"protocol\": {\n    \"readerVersion\":3,\n    \"writerVersion\":7,\n    \"readerFeatures\":[\"columnMapping\"],\n    \"writerFeatures\":[\"columnMapping\",\"identityColumns\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Optimized Deduplication with Date Range in Delta Lake MERGE\nDESCRIPTION: This snippet shows an optimized MERGE INTO operation for deduplicating data within a specific date range. It limits the matching and insertion to records within the last 7 days, improving performance for large datasets.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_24\n\nLANGUAGE: SQL\nCODE:\n```\nMERGE INTO logs\nUSING newDedupedLogs\nON logs.uniqueId = newDedupedLogs.uniqueId AND logs.date > current_date() - INTERVAL 7 DAYS\nWHEN NOT MATCHED AND newDedupedLogs.date > current_date() - INTERVAL 7 DAYS\n  THEN INSERT *\n```\n\nLANGUAGE: Python\nCODE:\n```\ndeltaTable.alias(\"logs\").merge(\n    newDedupedLogs.alias(\"newDedupedLogs\"),\n    \"logs.uniqueId = newDedupedLogs.uniqueId AND logs.date > current_date() - INTERVAL 7 DAYS\") \\\n  .whenNotMatchedInsertAll(\"newDedupedLogs.date > current_date() - INTERVAL 7 DAYS\") \\\n  .execute()\n```\n\nLANGUAGE: Scala\nCODE:\n```\ndeltaTable.as(\"logs\").merge(\n    newDedupedLogs.as(\"newDedupedLogs\"),\n    \"logs.uniqueId = newDedupedLogs.uniqueId AND logs.date > current_date() - INTERVAL 7 DAYS\")\n  .whenNotMatched(\"newDedupedLogs.date > current_date() - INTERVAL 7 DAYS\")\n  .insertAll()\n  .execute()\n```\n\nLANGUAGE: Java\nCODE:\n```\ndeltaTable.as(\"logs\").merge(\n    newDedupedLogs.as(\"newDedupedLogs\"),\n    \"logs.uniqueId = newDedupedLogs.uniqueId AND logs.date > current_date() - INTERVAL 7 DAYS\")\n  .whenNotMatched(\"newDedupedLogs.date > current_date() - INTERVAL 7 DAYS\")\n  .insertAll()\n  .execute();\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Azure Data Lake Storage Gen2 in Scala\nDESCRIPTION: Demonstrates writing and reading Delta tables using Azure Data Lake Storage Gen2. Requires specifying the container name and storage account name in the path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(5).write.format(\"delta\").save(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-delta-table>\")\n\nspark.read.format(\"delta\").load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Creating a DeltaSink for Non-Partitioned Tables in Java\nDESCRIPTION: This example demonstrates how to create a DeltaSink instance and connect it to an existing DataStream. It configures a basic sink for writing RowData to a non-partitioned Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.flink.sink.DeltaSink;\nimport org.apache.flink.core.fs.Path;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.types.logical.RowType;\nimport org.apache.hadoop.conf.Configuration;\n\npublic DataStream<RowData> createDeltaSink(\n        DataStream<RowData> stream,\n        String deltaTablePath,\n        RowType rowType) {\n    DeltaSink<RowData> deltaSink = DeltaSink\n        .forRowData(\n            new Path(deltaTablePath),\n            new Configuration(),\n            rowType)\n        .build();\n    stream.sinkTo(deltaSink);\n    return stream;\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Change Data Feed in Batch SQL Queries\nDESCRIPTION: SQL examples showing how to read change data using version numbers and timestamps in batch mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-change-data-feed.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM table_changes('tableName', 0, 10)\n\nSELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\nSELECT * FROM table_changes('tableName', 0)\n\nSELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\nSELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table with Identity Column in Python\nDESCRIPTION: Demonstrates how to create a Delta table with an identity column using PySpark. The example includes adding columns with specific data types and an identity generator, as well as inserting records with existing IDs and generating new IDs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import DeltaTable, IdentityGenerator\nfrom pyspark.sql.types import LongType, DateType\n\nDeltaTable.create(spark)\n  .tableName(\"new_table\")\n  .addColumn(\"id\", dataType=LongType(), generatedByDefaultAs=IdentityGenerator(start=5, step=1))\n  .addColumn(\"event_date\", dataType=DateType())\n  .addColumn(\"some_value\", dataType=LongType())\n  .execute()\n\n# Insert records including existing IDs\nold_table_df = spark.table(\"old_table\").select(\"id\", \"event_date\", \"some_value\")\nold_table_df.write\n  .format(\"delta\")\n  .mode(\"append\")\n  .saveAsTable(\"new_table\")\n\n# Insert records and generate new IDs\nnew_records_df = spark.table(\"new_records\").select(\"event_date\", \"some_value\")\nnew_records_df.write\n  .format(\"delta\")\n  .mode(\"append\")\n  .saveAsTable(\"new_table\")\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Delta Table Using Flink SQL\nDESCRIPTION: Examples of SQL INSERT queries for appending data to Delta tables using Flink SQL. Includes static inserts, table-to-table inserts, partitioned inserts, and continuous streaming inserts.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO sinkTable VALUES\n            ('a', 'b', 1),\n            ('c', 'd', 2),\n            ('e', 'f', 3);\n```\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO sinkTable SELECT * FROM sourceTable;\n```\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO sinkTable PARTITION (region='europe') SELECT * FROM sourceTable;\n```\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO sinkTable SELECT * FROM sourceTable /*+ OPTIONS('mode' = 'streaming') */;\n```\n\n----------------------------------------\n\nTITLE: Writing Delta Table Stream in Append Mode (Python)\nDESCRIPTION: This snippet demonstrates how to write a stream to a Delta table in append mode using Python. It specifies the Delta format, append output mode, and checkpoint location.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nevents.writeStream\n  .format(\"delta\")\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/tmp/delta/_checkpoints/\")\n  .start(\"/delta/events\")\n```\n\n----------------------------------------\n\nTITLE: Renaming Nested Fields in Delta Tables using SQL\nDESCRIPTION: SQL example showing how to rename nested fields in a Delta table schema using ALTER TABLE with the RENAME COLUMN syntax. Requires column mapping to be enabled.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_35\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name RENAME COLUMN col_name.old_nested_field TO new_nested_field\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Identity Columns in Scala\nDESCRIPTION: This snippet shows how to create a Delta table with identity columns using Scala. It demonstrates various ways to configure identity columns, including always generated and generated by default.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables.DeltaTable\nimport org.apache.spark.sql.types.LongType\n\nDeltaTable.create(spark)\n  .tableName(\"table_name\")\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"id_col1\")\n      .dataType(LongType)\n      .generatedAlwaysAsIdentity().build())\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"id_col2\")\n      .dataType(LongType)\n      .generatedAlwaysAsIdentity(start = -1L, step = 1L).build())\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"id_col3\")\n      .dataType(LongType)\n      .generatedByDefaultAsIdentity().build())\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"id_col4\")\n      .dataType(LongType)\n      .generatedByDefaultAsIdentity(start = -1L, step = 1L).build())\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from Delta Table using Java\nDESCRIPTION: Shows how to delete records from a Delta table using Java API. Includes examples using both SQL-formatted string predicates and Spark SQL functions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\nimport org.apache.spark.sql.functions;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, \"/tmp/delta/people-10m\");\n\n// Declare the predicate by using a SQL-formatted string.\ndeltaTable.delete(\"birthDate < '1955-01-01'\");\n\n// Declare the predicate by using Spark SQL functions.\ndeltaTable.delete(functions.col(\"birthDate\").lt(functions.lit(\"1955-01-01\")));\n```\n\n----------------------------------------\n\nTITLE: Deduplicating Data with MERGE INTO in Delta Lake\nDESCRIPTION: This snippet demonstrates how to use MERGE INTO to deduplicate data when writing logs into a Delta table. It matches records based on a unique ID and only inserts new, non-duplicate records.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_23\n\nLANGUAGE: SQL\nCODE:\n```\nMERGE INTO logs\nUSING newDedupedLogs\nON logs.uniqueId = newDedupedLogs.uniqueId\nWHEN NOT MATCHED\n  THEN INSERT *\n```\n\nLANGUAGE: Python\nCODE:\n```\ndeltaTable.alias(\"logs\").merge(\n    newDedupedLogs.alias(\"newDedupedLogs\"),\n    \"logs.uniqueId = newDedupedLogs.uniqueId\") \\\n  .whenNotMatchedInsertAll() \\\n  .execute()\n```\n\nLANGUAGE: Scala\nCODE:\n```\ndeltaTable\n  .as(\"logs\")\n  .merge(\n    newDedupedLogs.as(\"newDedupedLogs\"),\n    \"logs.uniqueId = newDedupedLogs.uniqueId\")\n  .whenNotMatched()\n  .insertAll()\n  .execute()\n```\n\nLANGUAGE: Java\nCODE:\n```\ndeltaTable\n  .as(\"logs\")\n  .merge(\n    newDedupedLogs.as(\"newDedupedLogs\"),\n    \"logs.uniqueId = newDedupedLogs.uniqueId\")\n  .whenNotMatched()\n  .insertAll()\n  .execute();\n```\n\n----------------------------------------\n\nTITLE: Reading from a Delta Table with Java\nDESCRIPTION: Java code to read data from a Delta table and display the results. This loads the table into a Dataset and then prints its contents.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\nDataset<Row> df = spark.read().format(\"delta\").load(\"/tmp/delta-table\");\ndf.show();\n```\n\n----------------------------------------\n\nTITLE: Basic Delta Table Operations on S3 in Scala\nDESCRIPTION: Scala code snippets demonstrating how to create and read a Delta table on S3 using Spark.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\n// Create a Delta table on S3:\nspark.range(5).write.format(\"delta\").save(\"s3a://<your-s3-bucket>/<path-to-delta-table>\")\n\n// Read a Delta table on S3:\nspark.read.format(\"delta\").load(\"s3a://<your-s3-bucket>/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Multiple Generated Partition Columns in Python\nDESCRIPTION: This example shows how to create a Delta table with multiple generated columns used for partitioning. It demonstrates creating year, month, and day partitions from a timestamp column.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nDeltaTable.create(spark) \\\n  .tableName(\"default.events\") \\\n  .addColumn(\"eventId\", \"BIGINT\") \\\n  .addColumn(\"data\", \"STRING\") \\\n  .addColumn(\"eventType\", \"STRING\") \\\n  .addColumn(\"eventTime\", \"TIMESTAMP\") \\\n  .addColumn(\"year\", \"INT\", generatedAlwaysAs=\"YEAR(eventTime)\") \\\n  .addColumn(\"month\", \"INT\", generatedAlwaysAs=\"MONTH(eventTime)\") \\\n  .addColumn(\"day\", \"INT\", generatedAlwaysAs=\"DAY(eventTime)\") \\\n  .partitionedBy(\"eventType\", \"year\", \"month\", \"day\") \\\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Implementing Delta Table Checkpointing in Java\nDESCRIPTION: This code demonstrates how to implement checkpointing for a Delta table after a transaction is committed. Checkpoints are optimizations that store the state of the table at a specific version to enable faster table state reconstruction.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_22\n\nLANGUAGE: java\nCODE:\n```\nTransactionCommitResult commitResult = txn.commit(engine, dataActionsIterable);\n\nif (commitResult.isReadyForCheckpoint()) {\n  // Checkpoint the table\n  Table.forPath(engine, tablePath).checkpoint(engine, commitResult.getVersion());\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing a Transaction for Delta Table Creation in Java\nDESCRIPTION: This code snippet demonstrates how to construct a Transaction object for creating a Delta table. It includes setting up the table path, engine, schema, and partition columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\npackage io.delta.kernel.examples;\n\nimport io.delta.kernel.*;\nimport io.delta.kernel.types.*;\nimport io.delta.kernel.utils.CloseableIterable;\n\nString myTablePath = <my-table-path>; \nConfiguration hadoopConf = new Configuration();\nEngine myEngine = DefaultEngine.create(hadoopConf);\nTable myTable = Table.forPath(myEngine, myTablePath);\n\nStructType mySchema = new StructType()\n  .add(\"id\", IntegerType.INTEGER)\n  .add(\"name\", StringType.STRING)\n  .add(\"city\", StringType.STRING)\n  .add(\"salary\", DoubleType.DOUBLE);\n\n// Partition columns are optional. Use it only if you are creating a partitioned table.\nList<String> myPartitionColumns = Collections.singletonList(\"city\");\n\nTransactionBuilder txnBuilder =\n  myTable.createTransactionBuilder(\n    myEngine,\n    \"Examples\", /* engineInfo - connector can add its own identifier which is noted in the Delta Log */ \n    Operation.WRITE /* What is the operation we are trying to perform? This is noted in the Delta Log */\n  );\n\n// Set the schema of the new table on the transaction builder\ntxnBuilder = txnBuilder\n  .withSchema(engine, mySchema);\n\n// Set the partition columns of the new table only if you are creating\n// a partitioned table; otherwise, this step can be skipped.\ntxnBuilder = txnBuilder\n  .withPartitionColumns(engine, examplePartitionColumns);\n\n// Build the transaction\nTransaction txn = txnBuilder.build(engine);\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Engine Implementation in Delta Kernel\nDESCRIPTION: Java example showing how to extend DefaultEngine to create a custom Engine implementation. This demonstrates overriding just one subinterface (FileSystemClient) while inheriting other implementations from DefaultEngine.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_25\n\nLANGUAGE: java\nCODE:\n```\nclass MyEngine extends DefaultEngine {\n\n  FileSystemClient getFileSystemClient() {\n    // Build a new implementation from scratch\n    return new MyFileSystemClient();\n  }\n  \n  // For all other sub-clients, use the default implementations provided by the `DefaultEngine`.\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table with Identity Column in Scala\nDESCRIPTION: Shows how to create a Delta table with an identity column using Scala. The example includes adding columns with specific data types and an identity generator, as well as inserting records with existing IDs and generating new IDs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.types._\nimport io.delta.tables.DeltaTable\n\nDeltaTable.createOrReplace(spark)\n  .tableName(\"new_table\")\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"id\")\n      .dataType(LongType)\n      .generatedByDefaultAsIdentity(start = 5L, step = 1L)\n      .build())\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"event_date\")\n      .dataType(DateType)\n      .nullable(true)\n      .build())\n  .addColumn(\n    DeltaTable.columnBuilder(spark, \"some_value\")\n      .dataType(LongType)\n      .nullable(true)\n      .build())\n  .execute()\n\n// Insert records including existing IDs\nval oldTableDF = spark.table(\"old_table\").select(\"id\", \"event_date\", \"some_value\")\noldTableDF.write\n  .format(\"delta\")\n  .mode(\"append\")\n  .saveAsTable(\"new_table\")\n\n// Insert records and generate new IDs\nval newRecordsDF = spark.table(\"new_records\").select(\"event_date\", \"some_value\")\nnewRecordsDF.write\n  .format(\"delta\")\n  .mode(\"append\")\n  .saveAsTable(\"new_table\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table History in Python\nDESCRIPTION: Python code examples to retrieve the history of operations performed on a Delta table, including options to get the full history or limit to a specific number of latest operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)\n\nfullHistoryDF = deltaTable.history()    # get the full history of the table\n\nlastOperationDF = deltaTable.history(1) # get the last operation\n```\n\n----------------------------------------\n\nTITLE: Updating Delta Table using Java\nDESCRIPTION: Shows how to update records in a Delta table using Java API. Includes examples using both SQL-formatted string predicates and Spark SQL functions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\nimport org.apache.spark.sql.functions;\nimport java.util.HashMap;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, \"/data/events/\");\n\n// Declare the predicate by using a SQL-formatted string.\ndeltaTable.updateExpr(\n  \"gender = 'F'\",\n  new HashMap<String, String>() {{\n    put(\"gender\", \"'Female'\");\n  }}\n);\n\n// Declare the predicate by using Spark SQL functions.\ndeltaTable.update(\n  functions.col(gender).eq(\"M\"),\n  new HashMap<String, Column>() {{\n    put(\"gender\", functions.lit(\"Male\"));\n  }}\n);\n```\n\n----------------------------------------\n\nTITLE: Performing File Compaction with OPTIMIZE in Scala\nDESCRIPTION: Demonstrates using the Delta Lake Scala API to perform file compaction on Delta tables. The API supports both path-based and Hive metastore-based tables, with optional partition filtering using the where method.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, pathToTable)  // For path-based tables\n// For Hive metastore-based tables: val deltaTable = DeltaTable.forName(spark, tableName)\n\ndeltaTable.optimize().executeCompaction()\n\n// If you have a large amount of data and only want to optimize a subset of it, you can specify an optional partition predicate using `where`\ndeltaTable.optimize().where(\"date='2021-11-18'\").executeCompaction()\n```\n\n----------------------------------------\n\nTITLE: Defining Delta Table Schema and Partition Columns in Java\nDESCRIPTION: This code demonstrates how to define the schema and partition columns for a new Delta table using the TransactionBuilder. It shows setting up a StructType for the schema and specifying partition columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nStructType mySchema = new StructType()\n  .add(\"id\", IntegerType.INTEGER)\n  .add(\"name\", StringType.STRING)\n  .add(\"city\", StringType.STRING)\n  .add(\"salary\", DoubleType.DOUBLE);\n\n// Partition columns are optional. Use it only if you are creating a partitioned table.\nList<String> myPartitionColumns = Collections.singletonList(\"city\");\n\n// Set the schema of the new table on the transaction builder\ntxnBuilder = txnBuilder\n  .withSchema(engine, mySchema);\n\n// Set the partition columns of the new table only if you are creating\n// a partitioned table; otherwise, this step can be skipped.\ntxnBuilder = txnBuilder\n  .withPartitionColumns(engine, examplePartitionColumns);\n```\n\n----------------------------------------\n\nTITLE: Triggering Clustering with OPTIMIZE Command\nDESCRIPTION: Example of using the OPTIMIZE command to trigger incremental clustering on a Delta table. This rewrites data as necessary to maintain the clustering organization.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nOPTIMIZE table_name;\n```\n\n----------------------------------------\n\nTITLE: Advanced Merge Operation with Conditions using SQL\nDESCRIPTION: This SQL snippet shows an advanced merge operation with conditions on the WHEN NOT MATCHED BY SOURCE clause and specifies values to update in unmatched target rows.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nMERGE INTO target\nUSING source\nON source.key = target.key\nWHEN MATCHED THEN\n  UPDATE SET target.lastSeen = source.timestamp\nWHEN NOT MATCHED THEN\n  INSERT (key, lastSeen, status) VALUES (source.key,  source.timestamp, 'active')\nWHEN NOT MATCHED BY SOURCE AND target.lastSeen >= (current_date() - INTERVAL '5' DAY) THEN\n  UPDATE SET target.status = 'inactive'\n```\n\n----------------------------------------\n\nTITLE: Reading from a Delta Table with SQL\nDESCRIPTION: SQL query to read all data from a Delta table at the specified location. This returns all rows in the Delta table as a result set.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM delta.`/tmp/delta-table`;\n```\n\n----------------------------------------\n\nTITLE: Generating Manifest Files for Delta Tables in Different Languages\nDESCRIPTION: Shows how to generate symlink format manifest files for a Delta table using SQL, Scala, Java, and Python. This command creates manifest files that Redshift can use to identify which data files to read.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/redshift-spectrum-integration.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGENERATE symlink_format_manifest FOR TABLE delta.`<path-to-delta-table>`\n```\n\nLANGUAGE: scala\nCODE:\n```\nval deltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\nLANGUAGE: java\nCODE:\n```\nDeltaTable deltaTable = DeltaTable.forPath(<path-to-delta-table>);\ndeltaTable.generate(\"symlink_format_manifest\");\n```\n\nLANGUAGE: python\nCODE:\n```\ndeltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\n----------------------------------------\n\nTITLE: Purging Soft-Deleted Data with REORG TABLE in Delta Lake\nDESCRIPTION: Example SQL commands demonstrating how to use the REORG TABLE command to purge soft-deleted data (like rows marked by deletion vectors) from Delta Lake tables. Includes variations with partition predicates to limit the scope of the operation.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-deletion-vectors.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nREORG TABLE events APPLY (PURGE);\n\n -- If you have a large amount of data and only want to purge a subset of it, you can specify an optional partition predicate using `WHERE`:\nREORG TABLE events WHERE date >= '2022-01-01' APPLY (PURGE);\n\nREORG TABLE events\n  WHERE date >= current_timestamp() - INTERVAL '1' DAY\n  APPLY (PURGE);\n```\n\n----------------------------------------\n\nTITLE: Setting User-defined Commit Metadata in Delta Tables using Python\nDESCRIPTION: Python example of setting user-defined metadata for commits to Delta tables using the userMetadata option, which can be read in the table history.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf.write.format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n  .save(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table from Azure Data Lake Gen2 with Partition Filtering\nDESCRIPTION: Example showing how to read a Delta table from Azure Data Lake Store Gen2 with optional partition filtering. Demonstrates usage of HierarchicalNavigation and custom partition filtering for specific year and month values.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_8\n\nLANGUAGE: powerquery\nCODE:\n```\nlet\n    Source = AzureStorage.DataLake(\"https://gbadls01.dfs.core.windows.net/public/powerbi_delta/DimProduct.delta\", [HierarchicalNavigation = false]),\n    DeltaTable = fn_ReadDeltaTable(Source, [PartitionFilterFunction=(x) => x[Year] = 2021 and x[Month] = \"Jan\"])\nin\n    DeltaTable\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Stream with ignoreChanges Option in Scala\nDESCRIPTION: Shows how to configure a Delta streaming source to ignore both updates and deletes in the source table. This option allows the stream to continue when files are rewritten due to UPDATE, MERGE INTO, DELETE, or OVERWRITE operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .option(\"ignoreChanges\", \"true\")\n  .load(\"/tmp/delta/user_events\")\n```\n\n----------------------------------------\n\nTITLE: Compacting Specific Partition in Delta Table\nDESCRIPTION: Shows how to compact a specific partition of a Delta table based on a predicate. This is useful for partitioned tables when you want to repartition only a subset of the data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/best-practices.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nval path = \"...\"\nval partition = \"year = '2019'\"\nval numFilesPerPartition = 16\n\nspark.read\n .format(\"delta\")\n .load(path)\n .where(partition)\n .repartition(numFilesPerPartition)\n .write\n .option(\"dataChange\", \"false\")\n .format(\"delta\")\n .mode(\"overwrite\")\n .option(\"replaceWhere\", partition)\n .save(path)\n```\n\nLANGUAGE: python\nCODE:\n```\npath = \"...\"\npartition = \"year = '2019'\"\nnumFilesPerPartition = 16\n\n(spark.read\n .format(\"delta\")\n .load(path)\n .where(partition)\n .repartition(numFilesPerPartition)\n .write\n .option(\"dataChange\", \"false\")\n .format(\"delta\")\n .mode(\"overwrite\")\n .option(\"replaceWhere\", partition)\n .save(path))\n```\n\n----------------------------------------\n\nTITLE: Enabling Type Widening During Table Creation\nDESCRIPTION: SQL command to create a new Delta table with type widening enabled through table properties.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-type-widening.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE <table_name> USING DELTA TBLPROPERTIES('delta.enableTypeWidening' = 'true')\n```\n\n----------------------------------------\n\nTITLE: Removing Obsolete Files with Delta Vacuum in SQL\nDESCRIPTION: Examples of vacuum commands in SQL to remove files no longer referenced by a Delta table and older than the retention threshold. Includes various modes like FULL, LITE, and options for custom retention periods and dry runs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nVACUUM eventsTable   -- This runs VACUUM in 'FULL' mode and deletes data files outside of the retention duration and all files in the table directory not referenced by the table.\n\nVACUUM eventsTable LITE   -- This VACUUM in 'LITE' mode runs faster.  \n                          -- Instead of finding all files in the table directory, `VACUUM LITE` uses the Delta transaction log to identify and remove files no longer referenced by any table versions within the retention duration.  \n                          -- If `VACUUM LITE` cannot be completed because the Delta log has been pruned a `DELTA_CANNOT_VACUUM_LITE` exception is raised.  \n                          -- This mode is available only in Delta 3.3 and above.\n\nVACUUM '/data/events' -- vacuum files in path-based table\n\nVACUUM delta.`/data/events/`\n\nVACUUM delta.`/data/events/` RETAIN 100 HOURS  -- vacuum files not required by versions more than 100 hours old\n\nVACUUM eventsTable DRY RUN    -- do dry run to get the list of files to be deleted\n\nVACUUM eventsTable USING INVENTORY inventoryTable  —- vacuum files based on a provided reservoir of files as a delta table\n\nVACUUM eventsTable USING INVENTORY (select * from inventoryTable)  —- vacuum files based on a provided reservoir of files as spark SQL query\n```\n\n----------------------------------------\n\nTITLE: Implementing Idempotent Writes in Delta Tables using Java\nDESCRIPTION: Example showing how to implement idempotent writes by setting transaction identifiers to ensure data is written only once.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\n// Set the transaction identifiers for idempotent writes\n// Delta/Kernel makes sure that there exists only one transaction in the Delta log\n// with the given application id and txn version\ntxnBuilder =\n  txnBuilder.withTransactionId(\n    engine,\n    \"my app id\", /* application id */\n    100 /* monotonically increasing txn version with each new data insert */\n  );\n```\n\n----------------------------------------\n\nTITLE: Generating and Committing Delta Log Actions in Java\nDESCRIPTION: This code shows how to generate Delta log actions from written data files and commit them to the Delta table. It includes converting DataFileStatus to log actions and committing the transaction.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\nCloseableIterator<Row> dataActions =\n  Transaction.generateAppendActions(engine, txnState, dataFiles, writeContext);\n\n// Create a iterable out of the data actions. If the contents are too big to fit in memory,\n// the connector may choose to write the data actions to a temporary file and return an\n// iterator that reads from the file.\nCloseableIterable<Row> dataActionsIterable = CloseableIterable.inMemoryIterable(dataActions);\n\nTransactionCommitStatus commitStatus = txn.commit(engine, dataActionsIterable)\n```\n\n----------------------------------------\n\nTITLE: Querying Data from a Clustered Table\nDESCRIPTION: Example of how to query data from a clustered table for optimal performance by filtering on clustering columns. This leverages the data organization provided by clustering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM table_name WHERE clustering_column_name = \"some_value\";\n```\n\n----------------------------------------\n\nTITLE: Inserting All Columns in Delta Lake Merge Operation (Scala)\nDESCRIPTION: Shows how to insert all columns of the target Delta table with corresponding columns from the source dataset using whenNotMatched(...).insertAll(). This is equivalent to inserting each column individually.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nwhenNotMatched(...).insertExpr(Map(\"col1\" -> \"source.col1\", \"col2\" -> \"source.col2\", ...))\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Lake S3 Multi-Cluster Settings\nDESCRIPTION: Essential Spark configuration settings for using Delta Lake with S3 in multi-cluster mode including DynamoDB logstore implementation.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n\"spark.delta.logStore.s3.impl\" = \"io.delta.storage.S3DynamoDBLogStore\"\n\"spark.io.delta.storage.S3DynamoDBLogStore.ddb.region\" = \"<region>\"\n\"spark.io.delta.storage.S3DynamoDBLogStore.ddb.tableName\" = \"<dynamodb_table_name>\"\n\"spark.hadoop.fs.s3.impl\"=\"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n# and any other config  key name that has `s3a` in it should be changed to `s3`\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Table Manifests in SQL\nDESCRIPTION: SQL command to generate symlink format manifests for a Delta table. This creates manifest files containing the names of data files to be read for querying the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/presto-integration.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nGENERATE symlink_format_manifest FOR TABLE delta.`<path-to-delta-table>`\n```\n\n----------------------------------------\n\nTITLE: Z-Ordering Data with OPTIMIZE in SQL\nDESCRIPTION: Shows how to use Z-Ordering to collocate related information in the same files for better data skipping. The example demonstrates using OPTIMIZE with ZORDER BY to organize data by specific columns, with an optional WHERE clause for partition filtering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nOPTIMIZE events ZORDER BY (eventType)\n\n-- If you have a large amount of data and only want to optimize a subset of it, you can specify an optional partition predicate by using \"where\".\nOPTIMIZE events WHERE date = '2021-11-18' ZORDER BY (eventType)\n```\n\n----------------------------------------\n\nTITLE: Creating Continuous Delta Source with Specific Columns in Java\nDESCRIPTION: Creates a continuous Delta source that reads only specified columns and monitors for changes. Optimizes streaming applications by filtering columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createContinuousDeltaSourceUserColumns(\n        StreamExecutionEnvironment env,\n        String deltaTablePath,\n        String[] columnNames) {\n\n    DeltaSource<RowData> deltaSource = DeltaSource\n        .forContinuousRowData(\n            new Path(deltaTablePath),\n            new Configuration())\n        .columnNames(columnNames)\n        .build();\n\n    return env.fromSource(deltaSource, WatermarkStrategy.noWatermarks(), \"delta-source\");\n}\n```\n\n----------------------------------------\n\nTITLE: Building a Scan Object for Delta Table Reading in Java\nDESCRIPTION: This snippet illustrates how to create a Scan object using a ScanBuilder. The Scan object contains the necessary metadata to start reading the table, including scan files and scan state information.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nScan myScan = mySnapshot.getScanBuilder(myEngine).build()\n\n// Common information about scanning for all data files to read.\nRow scanState = myScan.getScanState(myEngine)\n\n// Information about the list of scan files to read\nCloseableIterator<FilteredColumnarBatch> scanFiles = myScan.getScanFiles(myEngine)\n```\n\n----------------------------------------\n\nTITLE: Reclustering an Entire Table with OPTIMIZE FULL\nDESCRIPTION: Command to force reclustering of all records in a Delta table. This is useful when changing clustering columns or when initially clustering an existing table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nOPTIMIZE table_name FULL;\n```\n\n----------------------------------------\n\nTITLE: Shallow Cloning Parquet or Iceberg Tables to Delta Lake using SQL\nDESCRIPTION: SQL commands to shallow clone Parquet or Iceberg data sources into Delta tables. The 'REPLACE' clause requires the target table to be emptied before application. This functionality is available from Delta Lake 2.3 and above.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_19\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE TABLE <target_table_name> SHALLOW CLONE parquet.`/path/to/data`;\n\nCREATE OR REPLACE TABLE <target_table_name> SHALLOW CLONE iceberg.`/path/to/data`;\n```\n\n----------------------------------------\n\nTITLE: Adding Another CHECK Constraint to a Delta Table in SQL\nDESCRIPTION: This example adds another CHECK constraint to the 'people10m' table. It creates a constraint named 'validIds' to ensure that the 'id' column values are between 1 and 99999999.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE default.people10m ADD CONSTRAINT validIds CHECK (id > 1 and id < 99999999);\n```\n\n----------------------------------------\n\nTITLE: Converting Parquet to Delta with CONVERT Command\nDESCRIPTION: Demonstrates two approaches for converting Parquet tables to Delta format using SQL commands - either converting files directly or creating a Parquet table first.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCONVERT TO DELTA parquet.`/data-pipeline/`\nCREATE TABLE events USING DELTA LOCATION '/data-pipeline/'\n```\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE events USING PARQUET OPTIONS (path '/data-pipeline/')\nCONVERT TO DELTA events\n```\n\n----------------------------------------\n\nTITLE: Using Default Values in Delta Lake SQL INSERT\nDESCRIPTION: Demonstrates how to use the DEFAULT keyword in a Delta Lake SQL INSERT statement to utilize default column values.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_44\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO t VALUES (42, DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Dropping a NOT NULL Constraint in SQL\nDESCRIPTION: This example shows how to drop a NOT NULL constraint from a column using the ALTER TABLE CHANGE COLUMN command. It removes the NOT NULL constraint from the 'middleName' column in the 'people10m' table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE default.people10m CHANGE COLUMN middleName DROP NOT NULL;\n```\n\n----------------------------------------\n\nTITLE: Using DEFAULT Keyword in INSERT Statement with Delta Tables\nDESCRIPTION: Example SQL INSERT command demonstrating how to use the DEFAULT keyword to leverage the default value for a column in a Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-default-columns.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO t VALUES (16, DEFAULT);\n```\n\n----------------------------------------\n\nTITLE: Adding Columns to Delta Table Schema using SQL\nDESCRIPTION: SQL example showing how to add new columns to a Delta table schema using ALTER TABLE with the ADD COLUMNS syntax. This permits adding columns at specific positions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_29\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)\n```\n\n----------------------------------------\n\nTITLE: Streaming Delta Table with Schema Tracking in Python\nDESCRIPTION: This example shows how to use schema tracking when reading from a Delta table in a streaming context. It specifies a schema tracking location to handle non-additive schema changes.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_path = \"/path/to/checkpointLocation\"\n\n(spark.readStream\n  .option(\"schemaTrackingLocation\", checkpoint_path)\n  .table(\"delta_source_table\")\n  .writeStream\n  .option(\"checkpointLocation\", checkpoint_path)\n  .toTable(\"output_table\")\n)\n```\n\n----------------------------------------\n\nTITLE: Dropping Columns from Delta Tables using SQL\nDESCRIPTION: SQL example showing how to drop a single column from a Delta table schema using ALTER TABLE with the DROP COLUMN syntax. Requires column mapping to be enabled for metadata-only operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_36\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name DROP COLUMN col_name\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Data Files\nDESCRIPTION: Writes the physical data to Parquet files using the engine's ParquetHandler, collecting file statistics as specified by the write context.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_16\n\nLANGUAGE: java\nCODE:\n```\nCloseableIterator<DataFileStatus> dataFiles = engine.getParquetHandler()\n  .writeParquetFiles(\n    writeContext.getTargetDirectory(),\n    physicalData,\n    writeContext.getStatisticsColumns()\n  );\n```\n\n----------------------------------------\n\nTITLE: Building and Committing Delta Table Transaction in Java\nDESCRIPTION: This snippet shows how to build a Transaction from a TransactionBuilder and commit it to create a new Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n// Build the transaction\nTransaction txn = txnBuilder.build(engine);\n\n// Commit the transaction.\n// As we are just creating the table and not adding any data, the `dataActions` is empty.\nTransactionCommitResult commitResult =\n  txn.commit(\n    engine,\n    CloseableIterable.emptyIterable() /* dataActions */\n  );\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Azure Data Lake Storage Gen1 (Scala)\nDESCRIPTION: Scala code demonstrating how to write and read Delta tables using Azure Data Lake Storage Gen1. This shows the basic usage pattern for Delta Lake with ADLS Gen1.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(5).write.format(\"delta\").save(\"adl://<your-adls-account>.azuredatalakestore.net/<path-to-delta-table>\")\n\nspark.read.format(\"delta\").load(\"adl://<your-adls-account>.azuredatalakestore.net/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Specifying Starting Version for Delta Stream in Scala\nDESCRIPTION: Demonstrates how to specify a starting version for a Delta streaming source using the startingVersion option. This allows the stream to begin processing from a specific Delta table version rather than from the beginning.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .option(\"startingVersion\", \"5\")\n  .load(\"/tmp/delta/user_events\")\n```\n\n----------------------------------------\n\nTITLE: Advanced Merge Operation with Conditions using Python\nDESCRIPTION: This Python snippet shows an advanced merge operation with conditions on the WHEN NOT MATCHED BY SOURCE clause and specifies values to update in unmatched target rows.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n(targetDF\n  .merge(sourceDF, \"source.key = target.key\")\n  .whenMatchedUpdate(\n    set = {\"target.lastSeen\": \"source.timestamp\"}\n  )\n  .whenNotMatchedInsert(\n    values = {\n      \"target.key\": \"source.key\",\n      \"target.lastSeen\": \"source.timestamp\",\n      \"target.status\": \"'active'\"\n    }\n  )\n  .whenNotMatchedBySourceUpdate(\n    condition=\"target.lastSeen >= (current_date() - INTERVAL '5' DAY)\",\n    set = {\"target.status\": \"'inactive'\"}\n  )\n  .execute()\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table Instance in Java\nDESCRIPTION: This code snippet shows how to create a Delta Table instance using the Delta Kernel API. It demonstrates initializing the table with a specified path and engine configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npackage io.delta.kernel.examples;\n\nimport io.delta.kernel.*;\nimport io.delta.kernel.types.*;\nimport io.delta.kernel.utils.CloseableIterable;\n\nString myTablePath = <my-table-path>; \nConfiguration hadoopConf = new Configuration();\nEngine myEngine = DefaultEngine.create(hadoopConf);\nTable myTable = Table.forPath(myEngine, myTablePath);\n```\n\n----------------------------------------\n\nTITLE: Enabling UniForm for Iceberg on Existing Table in SQL\nDESCRIPTION: SQL command to enable or upgrade UniForm Iceberg on an existing Delta table by setting the required table properties.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-uniform.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET TBLPROPERTIES(\n  'delta.enableIcebergCompatV2' = 'true',\n  'delta.universalFormat.enabledFormats' = 'iceberg');\n```\n\n----------------------------------------\n\nTITLE: Reading Change Data Feed (CDF) from Delta Sharing Tables\nDESCRIPTION: Shows how to query change data feed from Delta Sharing tables using version ranges or timestamps. Requires CDF and history sharing to be enabled on the original Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-sharing.md#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE mytable USING deltaSharing LOCATION '<profile-file-path>#<share-name>.<schema-name>.<table-name>';\n\nSELECT * FROM table_changes('mytable', 0, 10)\n\nSELECT * FROM table_changes('mytable', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\nSELECT * FROM table_changes('mytable', 0)\n```\n\nLANGUAGE: Python\nCODE:\n```\ntable_path = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\"\n\nspark.read.format(\"deltaSharing\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingVersion\", 0) \\\n  .option(\"endingVersion\", 10) \\\n  .load(tablePath)\n\nspark.read.format(\"deltaSharing\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n  .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n  .load(tablePath)\n\nspark.read.format(\"deltaSharing\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingVersion\", 0) \\\n  .load(tablePath)\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval tablePath = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\"\n\nspark.read.format(\"deltaSharing\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingVersion\", 0)\n  .option(\"endingVersion\", 10)\n  .load(tablePath)\n\nspark.read.format(\"deltaSharing\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingTimestamp\", \"2024-01-18 05:45:46\")\n  .option(\"endingTimestamp\", \"2024-01-18 12:00:00\")\n  .load(tablePath)\n\nspark.read.format(\"deltaSharing\")\n  .option(\"readChangeFeed\", \"true\")\n  .option(\"startingVersion\", 0)\n  .load(tablePath)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table History in Java\nDESCRIPTION: Java code examples to retrieve and analyze the operation history of a Delta table, with options to get the complete history or only the most recent operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable);\n\nDataFrame fullHistoryDF = deltaTable.history();       // get the full history of the table\n\nDataFrame lastOperationDF = deltaTable.history(1);    // fetch the last operation on the DeltaTable\n```\n\n----------------------------------------\n\nTITLE: Removing Obsolete Files with Delta Vacuum in Python\nDESCRIPTION: Python examples for using vacuum operations on Delta tables to remove files no longer needed, with options for both default and custom retention periods. Requires delta.tables module.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)  # path-based tables, or\ndeltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables\n\ndeltaTable.vacuum()        # vacuum files not required by versions older than the default retention period\n\ndeltaTable.vacuum(100)     # vacuum files not required by versions more than 100 hours old\n```\n\n----------------------------------------\n\nTITLE: Defining Delta Lake Metadata Action in JSON\nDESCRIPTION: This JSON snippet demonstrates the structure of a metaData action in Delta Lake. It includes the table ID, format specification, schema string, partition columns, and configuration options. This example shows a table with Parquet format and an appendOnly configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metaData\":{\n    \"id\":\"af23c9d7-fff1-4a5a-a2c8-55c59bd782aa\",\n    \"format\":{\"provider\":\"parquet\",\"options\":{}},\n    \"schemaString\":\"...\",\n    \"partitionColumns\":[],\n    \"configuration\":{\n      \"appendOnly\": \"true\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing Multiple Columns in Delta Tables using SQL\nDESCRIPTION: SQL example showing how to replace multiple columns in a Delta table schema using ALTER TABLE with the REPLACE COLUMNS syntax. This changes both the structure and ordering of columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_33\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name REPLACE COLUMNS (col_name1 col_type1 [COMMENT col_comment1], ...)\n```\n\n----------------------------------------\n\nTITLE: Setting User-defined Commit Metadata in Delta Tables using SQL\nDESCRIPTION: SQL example of setting user-defined metadata for commits to Delta tables using SparkSession configuration, which can be read in the table history.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_26\n\nLANGUAGE: sql\nCODE:\n```\nSET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\nINSERT OVERWRITE default.people10m SELECT * FROM morePeople\n```\n\n----------------------------------------\n\nTITLE: Building a Delta Table Transaction in Java\nDESCRIPTION: This snippet illustrates how to create a TransactionBuilder for a Delta table and set up a transaction for creating a new table. It includes setting the operation type and engine information.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nTransactionBuilder txnBuilder =\n  myTable.createTransactionBuilder(\n    myEngine,\n    \"Examples\", /* engineInfo - connector can add its own identifier which is noted in the Delta Log */ \n    Operation.CREATE_TABLE /* What is the operation we are trying to perform. This is noted in the Delta Log */\n  );\n```\n\n----------------------------------------\n\nTITLE: Showing Delta Table Properties in SQL\nDESCRIPTION: This snippet demonstrates how to view the table properties of a Delta table, which include CHECK constraints. It uses the SHOW TBLPROPERTIES command on the 'people10m' table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSHOW TBLPROPERTIES default.people10m;\n```\n\n----------------------------------------\n\nTITLE: Updating Delta Table using Scala\nDESCRIPTION: Shows how to update records in a Delta table using Scala API. Includes examples using both SQL-formatted string predicates and Spark SQL functions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, \"/tmp/delta/people-10m\")\n\n// Declare the predicate by using a SQL-formatted string.\ndeltaTable.updateExpr(\n  \"gender = 'F'\",\n  Map(\"gender\" -> \"'Female'\")\n\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\n// Declare the predicate by using Spark SQL functions and implicits.\ndeltaTable.update(\n  col(\"gender\") === \"M\",\n  Map(\"gender\" -> lit(\"Male\")));\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession for Delta Lake (Scala)\nDESCRIPTION: This Scala code snippet demonstrates how to configure a SparkSession for Delta Lake operations. It sets the necessary configurations for enabling Delta Lake integration with Apache Spark DataSourceV2 and Catalog APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_42\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession\n  .builder()\n  .appName(\"...\")\n  .master(\"...\")\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Modifying Unmatched Rows in Delta Table using Scala\nDESCRIPTION: This Scala snippet shows how to use the WHEN NOT MATCHED BY SOURCE clause to update or delete records in the target table that do not have corresponding records in the source table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\ntargetDF\n  .merge(sourceDF, \"source.key = target.key\")\n  .whenMatched()\n  .updateAll()\n  .whenNotMatched()\n  .insertAll()\n  .whenNotMatchedBySource()\n  .delete()\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Performing Blind Append into an Existing Delta Table in Java\nDESCRIPTION: This code demonstrates how to insert data into an existing Delta table without providing schema information. It handles partitioning data, transforming logical data to physical data, writing to Parquet files, and committing the transaction using Delta Kernel's API.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_20\n\nLANGUAGE: java\nCODE:\n```\n// Create a `Table` object with the given destination table path\nTable table = Table.forPath(engine, tablePath);\n\n// Create a transaction builder to build the transaction\nTransactionBuilder txnBuilder =\n  table.createTransactionBuilder(\n    engine,\n    \"Examples\", /* engineInfo */\n    Operation.WRITE\n  );\n\n/ Build the transaction - no need to provide the schema as the table already exists.\nTransaction txn = txnBuilder.build(engine);\n\n// Get the transaction state\nRow txnState = txn.getTransactionState(engine);\n\nList<Row> dataActions = new ArrayList<>();\n\n// Generate the sample data for three partitions. Process each partition separately.\n// This is just an example. In a real-world scenario, the data may come from different\n// partitions. Connectors already have the capability to partition by partition values\n// before writing to the table\n\n// In the test data `city` is a partition column\nfor (String city : Arrays.asList(\"San Francisco\", \"Campbell\", \"San Jose\")) {\n  FilteredColumnarBatch batch1 = generatedPartitionedDataBatch(\n\t    5 /* offset */, city /* partition value */);\n  FilteredColumnarBatch batch2 = generatedPartitionedDataBatch(\n\t    5 /* offset */, city /* partition value */);\n  FilteredColumnarBatch batch3 = generatedPartitionedDataBatch(\n\t    10 /* offset */, city /* partition value */);\n\n    CloseableIterator<FilteredColumnarBatch> data =\n\t    toCloseableIterator(Arrays.asList(batch1, batch2, batch3).iterator());\n\n    // Create partition value map\n    Map<String, Literal> partitionValues =\n\t    Collections.singletonMap(\n\t\t    \"city\", // partition column name\n\t\t    // partition value. Depending upon the parition column type, the\n\t\t    // partition value should be created. In this example, the partition\n\t\t    // column is of type StringType, so we are creating a string literal.\n\t\t    Literal.ofString(city));\n\n\n    // First transform the logical data to physical data that needs to be written\n    // to the Parquet\n    // files\n    CloseableIterator<FilteredColumnarBatch> physicalData =\n\t    Transaction.transformLogicalData(engine, txnState, data, partitionValues);\n\n    // Get the write context\n    DataWriteContext writeContext =\n\t    Transaction.getWriteContext(engine, txnState, partitionValues);\n\n\n    // Now write the physical data to Parquet files\n    CloseableIterator<DataFileStatus> dataFiles = engine.getParquetHandler()\n\t    .writeParquetFiles(\n\t\t    writeContext.getTargetDirectory(),\n\t\t    physicalData,\n\t\t    writeContext.getStatisticsColumns());\n\n\n    // Now convert the data file status to data actions that needs to be written to the Delta\n    // table log\n    CloseableIterator<Row> partitionDataActions = Transaction.generateAppendActions(\n\t    engine,\n\t    txnState,\n\t    dataFiles,\n\t    writeContext);\n\n    // Now add all the partition data actions to the main data actions list. In a\n    // distributed query engine, the partition data is written to files at tasks on executor\n    // nodes. The data actions are collected at the driver node and then written to the\n    // Delta table log using the `Transaction.commit`\n    while (partitionDataActions.hasNext()) {\n\tdataActions.add(partitionDataActions.next());\n    }\n}\n\n// Create a iterable out of the data actions. If the contents are too big to fit in memory,\n// the connector may choose to write the data actions to a temporary file and return an\n// iterator that reads from the file.\nCloseableIterable<Row> dataActionsIterable = CloseableIterable.inMemoryIterable(\n\ttoCloseableIterator(dataActions.iterator()));\n\n// Commit the transaction.\nTransactionCommitResult commitResult = txn.commit(engine, dataActionsIterable);\n```\n\n----------------------------------------\n\nTITLE: Describing Delta Table Details in SQL\nDESCRIPTION: This command shows how to view detailed information about a Delta table, including its constraints. It uses the DESCRIBE DETAIL command on the 'people10m' table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE DETAIL default.people10m;\n```\n\n----------------------------------------\n\nTITLE: Performing a Simple Table Scan with Filter in Delta Kernel Java\nDESCRIPTION: This snippet demonstrates how to perform a basic table scan with a filter using Delta Kernel's Java API. It shows the process of creating an engine, defining a table, getting the latest snapshot, specifying scan details, and reading the table data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nEngine myEngine = DefaultEngine.create() ;                  // define a engine (more details below)\nTable myTable = Table.forPath(\"/delta/table/path\");         // define what table to scan\nSnapshot mySnapshot = myTable.getLatestSnapshot(myEngine);  // define which version of table to scan\nScan myScan = mySnapshot.getScanBuilder()           // specify the scan details\n  .withFilters(scanFilter)\n  .build();\nCloseableIterator<ColumnarBatch> physicalData =             // read the Parquet data files\n  .. read from Parquet data files ...\nScan.transformPhysicalData(...)                             // returns the table data\n```\n\n----------------------------------------\n\nTITLE: Modifying Unmatched Rows in Delta Table using Python\nDESCRIPTION: This Python snippet demonstrates how to use the WHEN NOT MATCHED BY SOURCE clause to update or delete records in the target table that do not have corresponding records in the source table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n(targetDF\n  .merge(sourceDF, \"source.key = target.key\")\n  .whenMatchedUpdateAll()\n  .whenNotMatchedInsertAll()\n  .whenNotMatchedBySourceDelete()\n  .execute()\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling UniForm for Iceberg with REORG in SQL\nDESCRIPTION: SQL command to enable UniForm Iceberg and rewrite underlying data files using the REORG command. This is useful for tables with deletion vectors or previously enabled IcebergCompatV1.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-uniform.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nREORG TABLE table_name APPLY (UPGRADE UNIFORM(ICEBERG_COMPAT_VERSION=2));\n```\n\n----------------------------------------\n\nTITLE: Removing Obsolete Files with Delta Vacuum in Java\nDESCRIPTION: Java examples demonstrating the vacuum operation on Delta tables to remove files no longer referenced and beyond the retention period. Requires io.delta.tables package.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\nimport org.apache.spark.sql.functions;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable);\n\ndeltaTable.vacuum();        // vacuum files not required by versions older than the default retention period\n\ndeltaTable.vacuum(100);     // vacuum files not required by versions more than 100 hours old\n```\n\n----------------------------------------\n\nTITLE: Viewing Clustering Information with DESCRIBE DETAIL\nDESCRIPTION: SQL command to view detailed information about a table, including its clustering columns. This helps understand how a table is currently clustered.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE DETAIL table_name;\n```\n\n----------------------------------------\n\nTITLE: SQL Command for Dropping Delta Table Features\nDESCRIPTION: SQL command syntax for dropping a table feature from a Delta table. The command requires Delta Lake 3.0.0 or above and may include an optional TRUNCATE HISTORY clause for final protocol downgrade.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-drop-feature.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table-name> DROP FEATURE <feature-name> [TRUNCATE HISTORY]\n```\n\n----------------------------------------\n\nTITLE: Changing Nested Column Comment or Ordering in Delta Tables using SQL\nDESCRIPTION: SQL example showing how to change nested column comments or reorder nested columns in a Delta table schema using ALTER TABLE with the ALTER COLUMN syntax.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_32\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name ALTER [COLUMN] col_name.nested_col_name nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]\n```\n\n----------------------------------------\n\nTITLE: Basic Delta Table Operations on S3\nDESCRIPTION: Example Scala code demonstrating basic Delta table operations including creating and reading tables on S3.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\n// Create a Delta table on S3:\nspark.range(5).write.format(\"delta\").save(\"s3a://<your-s3-bucket>/<path-to-delta-table>\")\n\n// Read a Delta table on S3:\nspark.read.format(\"delta\").load(\"s3a://<your-s3-bucket>/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Creating New Tables with Row Tracking in SQL\nDESCRIPTION: SQL commands for creating new Delta tables with row tracking enabled, including basic creation, CTAS (Create Table As Select), LIKE statement for copying configuration, and CLONE statement.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-row-tracking.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Create an empty table\nCREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES ('delta.enableRowTracking' = 'true');\n\n-- Using a CTAS statement\nCREATE TABLE course_new\nTBLPROPERTIES ('delta.enableRowTracking' = 'true')\nAS SELECT * FROM course_old;\n\n-- Using a LIKE statement to copy configuration\nCREATE TABLE graduate LIKE student;\n\n-- Using a CLONE statement to copy configuration\nCREATE TABLE graduate CLONE student;\n```\n\n----------------------------------------\n\nTITLE: Upgrading Delta Lake Protocol Version using Python\nDESCRIPTION: This Python code demonstrates how to use the DeltaTable.upgradeTableProtocol method to upgrade the reader version to 1 and the writer version to 3 for a Delta Lake table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/versioning.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import DeltaTable\ndelta = DeltaTable.forPath(spark, \"path_to_table\") # or DeltaTable.forName\ndelta.upgradeTableProtocol(1, 3) # upgrades to readerVersion=1, writerVersion=3\n```\n\n----------------------------------------\n\nTITLE: Building and Committing a Delta Table Transaction in Java\nDESCRIPTION: This snippet shows how to build a Transaction object from a TransactionBuilder and commit the transaction to create a new Delta table. It demonstrates the final steps in table creation without adding initial data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n// Build the transaction\nTransaction txn = txnBuilder.build(engine);\n\n// Commit the transaction.\n// As we are just creating the table and not adding any data, the `dataActions` is empty.\nTransactionCommitResult commitResult =\n  txn.commit(\n    engine,\n    CloseableIterable.emptyIterable() /* dataActions */\n  );\n```\n\n----------------------------------------\n\nTITLE: Committing Delta Transaction\nDESCRIPTION: Code to finalize and commit the Delta transaction with collected data actions from worker nodes, including optional checkpointing.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_36\n\nLANGUAGE: java\nCODE:\n```\n// Create a iterable out of the data actions. If the contents are too big to fit in memory,\n// the connector may choose to write the data actions to a temporary file and return an\n// iterator that reads from the file.\nCloseableIterable<Row> dataActionsIterable = CloseableIterable.inMemoryIterable(\n\ttoCloseableIterator(dataActions.iterator()));\n\n// Commit the transaction.\nTransactionCommitResult commitResult = txn.commit(engine, dataActionsIterable);\n\n// Optional step\nif (commitResult.isReadyForCheckpoint()) {\n  // Checkpoint the table\n  Table.forPath(engine, tablePath).checkpoint(engine, commitResult.getVersion());\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Catalog in SQL\nDESCRIPTION: SQL query to create and configure a Delta Catalog, which is required for Flink SQL support. Supports both in-memory and Hive catalog types.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nCREATE CATALOG <catalog_name> WITH (\n  'type' = 'delta-catalog',\n  'catalog-type' = '<decorated-catalog>',\n   '<config_key_1>' = '<config_value_1>',\n   '<config_key_2>' = '<config_value_2>'\n);\nUSE CATALOG <catalog_name>;\n```\n\n----------------------------------------\n\nTITLE: Renaming a Column in Delta Tables with Column Mapping\nDESCRIPTION: SQL command to rename a column in a Delta table with column mapping enabled. This operation doesn't require rewriting the underlying Parquet files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-column-mapping.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> RENAME COLUMN old_col_name TO new_col_name\n```\n\n----------------------------------------\n\nTITLE: Specifying Starting Timestamp for Delta Stream in Scala\nDESCRIPTION: Shows how to specify a starting timestamp for a Delta streaming source using the startingTimestamp option. This allows the stream to begin processing from a specific point in time rather than from the beginning of the table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .option(\"startingTimestamp\", \"2018-10-18\")\n  .load(\"/tmp/delta/user_events\")\n```\n\n----------------------------------------\n\nTITLE: Committing Transaction\nDESCRIPTION: Finalizes the transaction by committing the data actions to the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\nTransactionCommitStatus commitStatus = txn.commit(engine, dataActionsIterable)\n```\n\n----------------------------------------\n\nTITLE: Adding a File to a Partitioned Delta Lake Table\nDESCRIPTION: This JSON snippet demonstrates an 'add' action for adding a file to a partitioned Delta Lake table. It includes file metadata, partition information, and statistics.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"add\": {\n    \"path\": \"date=2017-12-10/part-000...c000.gz.parquet\",\n    \"partitionValues\": {\"date\": \"2017-12-10\"},\n    \"size\": 841454,\n    \"modificationTime\": 1512909768000,\n    \"dataChange\": true,\n    \"baseRowId\": 4071,\n    \"defaultRowCommitVersion\": 41,\n    \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"val...\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Changing Clustering Columns with ALTER TABLE\nDESCRIPTION: SQL command to modify the clustering columns of an existing table. This allows the data layout to evolve as query patterns change over time.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name CLUSTER BY (new_column1, new_column2);\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Storage Credentials for Delta Lake (Scala)\nDESCRIPTION: Scala code to configure Azure Blob Storage credentials for use with Delta Lake. This sets up authentication using either a SAS token or an account access key.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\n  \"fs.azure.sas.<your-container-name>.<your-storage-account-name>.blob.core.windows.net\",\n   \"<complete-query-string-of-your-sas-for-the-container>\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\n  \"fs.azure.account.key.<your-storage-account-name>.blob.core.windows.net\",\n   \"<your-storage-account-access-key>\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Storage with Account Access Key\nDESCRIPTION: Sets up authentication for Azure Blob Storage using an account access key in the Hadoop configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nconf.set(\n  \"fs.azure.account.key.<your-storage-account-name>.blob.core.windows.net\",\n  \"<your-storage-account-access-key>\");\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table Details in Python\nDESCRIPTION: Python code to retrieve detailed information about a Delta table using the DeltaTable class.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)\n\ndetailDF = deltaTable.detail()\n```\n\n----------------------------------------\n\nTITLE: Adding Delta Lake Dependency in SBT Project\nDESCRIPTION: SBT configuration for including Delta Lake in a Scala project. This line should be added to your build.sbt file to include the Delta Lake library.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies += \"io.delta\" %% \"delta-spark\" % \"3.3.0\"\n```\n\n----------------------------------------\n\nTITLE: Dropping a CHECK Constraint from a Delta Table in SQL\nDESCRIPTION: This snippet shows how to drop a CHECK constraint from a Delta table. It removes the previously added 'dateWithinRange' constraint from the 'people10m' table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE default.people10m DROP CONSTRAINT dateWithinRange;\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from Delta Table using Scala\nDESCRIPTION: Shows how to delete records from a Delta table using Scala API. Includes examples using both SQL-formatted string predicates and Spark SQL functions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, \"/tmp/delta/people-10m\")\n\n// Declare the predicate by using a SQL-formatted string.\ndeltaTable.delete(\"birthDate < '1955-01-01'\")\n\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\n// Declare the predicate by using Spark SQL functions and implicits.\ndeltaTable.delete(col(\"birthDate\") < \"1955-01-01\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Engine by Extending DefaultEngine in Java\nDESCRIPTION: Example Java code showing how to create a custom Engine implementation by extending the DefaultEngine class and overriding specific methods to provide custom implementations for certain components.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_20\n\nLANGUAGE: java\nCODE:\n```\nclass MyEngine extends DefaultEngine {\n\n  FileSystemClient getFileSystemClient() {\n    // Build a new implementation from scratch\n    return new MyFileSystemClient();\n  }\n  \n  // For all other sub-clients, use the default implementations provided by the `DefaultEngine`.\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Storage Credentials for Delta Lake in Python\nDESCRIPTION: This code snippet demonstrates how to pass Azure storage account credentials through DataFrame options when reading from and writing to Delta Lake tables in Python. It shows examples of reading from two different sources with different credentials and then writing the combined data with a third set of credentials.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndf1 = spark.read.format(\"delta\") \\\n  .option(\"fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\", \"<storage-account-access-key-1>\") \\\n  .read(\"...\")\ndf2 = spark.read.format(\"delta\") \\\n  .option(\"fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\", \"<storage-account-access-key-2>\") \\\n  .read(\"...\")\ndf1.union(df2).write.format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\", \"<storage-account-access-key-3>\") \\\n  .save(\"...\")\n```\n\n----------------------------------------\n\nTITLE: Time Travel Queries on Delta Sharing Tables\nDESCRIPTION: Demonstrates how to query older snapshots of a shared table using timestamp or version-based time travel. Requires history sharing to be enabled by the data provider.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-sharing.md#2025-04-22_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM mytable TIMESTAMP AS OF timestamp_expression\nSELECT * FROM mytable VERSION AS OF version\n```\n\nLANGUAGE: Python\nCODE:\n```\nspark.read.format(\"deltaSharing\").option(\"timestampAsOf\", timestamp_string).load(tablePath)\n\nspark.read.format(\"deltaSharing\").option(\"versionAsOf\", version).load(tablePath)\n```\n\nLANGUAGE: Scala\nCODE:\n```\nspark.read.format(\"deltaSharing\").option(\"timestampAsOf\", timestamp_string).load(tablePath)\n\nspark.read.format(\"deltaSharing\").option(\"versionAsOf\", version).load(tablePath)\n```\n\n----------------------------------------\n\nTITLE: Committing Changes to Delta Table in Java\nDESCRIPTION: Demonstrates how to commit changes to a Delta table by adding new files and removing old files using Delta Standalone APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.standalone.Operation;\nimport io.delta.standalone.actions.RemoveFile;\nimport io.delta.standalone.exceptions.DeltaConcurrentModificationException;\nimport io.delta.standalone.types.StructType;\n\nList<RemoveFile> removeOldFiles = filteredFiles.stream()\n    .map(path -> addFileMap.get(path).remove())\n    .collect(Collectors.toList());\n\nList<AddFile> addNewFiles = dataWriteResult.getNewFiles()\n    .map(file ->\n        new AddFile(\n            file.getPath(),\n            file.getPartitionValues(),\n            file.getSize(),\n            System.currentTimeMillis(),\n            true, // isDataChange\n            null, // stats\n            null  // tags\n        );\n    ).collect(Collectors.toList());\n\nList<Action> totalCommitFiles = new ArrayList<>();\ntotalCommitFiles.addAll(removeOldFiles);\ntotalCommitFiles.addAll(addNewFiles);\n\ntry {\n    txn.commit(totalCommitFiles, new Operation(Operation.Name.UPDATE), \"Zippy/1.0.0\");\n} catch (DeltaConcurrentModificationException e) {\n    // handle exception here\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from Delta Table using Python\nDESCRIPTION: Shows how to delete records from a Delta table using Python API. Includes examples using both SQL-formatted string predicates and Spark SQL functions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, '/tmp/delta/people-10m')\n\n# Declare the predicate by using a SQL-formatted string.\ndeltaTable.delete(\"birthDate < '1955-01-01'\")\n\n# Declare the predicate by using Spark SQL functions.\ndeltaTable.delete(col('birthDate') < '1960-01-01')\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with NOT NULL Constraints in SQL\nDESCRIPTION: This snippet demonstrates how to create a Delta table with NOT NULL constraints on specific columns. It creates a 'people10m' table with various columns, where 'id' and 'middleName' are set as NOT NULL.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE default.people10m (\n    id INT NOT NULL,\n    firstName STRING,\n    middleName STRING NOT NULL,\n    lastName STRING,\n    gender STRING,\n    birthDate TIMESTAMP,\n    ssn STRING,\n    salary INT\n  ) USING DELTA;\n```\n\n----------------------------------------\n\nTITLE: Transforming Logical Data to Physical Data\nDESCRIPTION: Converts logical data to physical format suitable for Parquet file writing, handling partition values and data transformations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\n// Transform the logical data to physical data that needs to be written to the Parquet\n// files\nCloseableIterator<FilteredColumnarBatch> physicalData =\n  Transaction.transformLogicalData(engine, txnState, data, partitionValues);\n```\n\n----------------------------------------\n\nTITLE: Upgrading Delta Lake Protocol Version using SQL\nDESCRIPTION: This SQL command upgrades the reader protocol version to 1 and the writer protocol version to 3 for a specified Delta Lake table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/versioning.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n-- Upgrades the reader protocol version to 1 and the writer protocol version to 3.\nALTER TABLE <table_identifier> SET TBLPROPERTIES('delta.minReaderVersion' = '1', 'delta.minWriterVersion' = '3')\n```\n\n----------------------------------------\n\nTITLE: Conflict-Safe Merge Operation in Delta Lake with Partition Constraints (Scala)\nDESCRIPTION: Improved merge operation example that explicitly includes partition constraints in the merge condition. This approach prevents concurrency conflicts by ensuring each operation works on distinct partitions when run concurrently with different date/country values.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/concurrency-control.md#2025-04-22_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\n// Target 'deltaTable' is partitioned by date and country\ndeltaTable.as(\"t\").merge(\n    source.as(\"s\"),\n    \"s.user_id = t.user_id AND s.date = t.date AND s.country = t.country AND t.date = '\" + <date> + \"' AND t.country = '\" + <country> + \"'\")\n  .whenMatched().updateAll()\n  .whenNotMatched().insertAll()\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Limiting Records Per File\nDESCRIPTION: Examples showing how to limit the number of records written to a single file in a Delta table using the maxRecordsPerFile option.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf.write.format(\"delta\") \\\n  .mode(\"append\") \\\n  .option(\"maxRecordsPerFile\", \"10000\") \\\n  .save(\"/tmp/delta/people10m\")\n```\n\nLANGUAGE: scala\nCODE:\n```\ndf.write.format(\"delta\")\n  .mode(\"append\")\n  .option(\"maxRecordsPerFile\", \"10000\")\n  .save(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Z-Ordering Data with OPTIMIZE in Scala\nDESCRIPTION: Shows how to use the Scala API to perform Z-Ordering on Delta Lake tables. Z-Ordering is a technique to improve query performance by colocating related data in the same files, with support for optional partition filtering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, pathToTable)  // path-based table\n// For Hive metastore-based tables: val deltaTable = DeltaTable.forName(spark, tableName)\n\ndeltaTable.optimize().executeZOrderBy(eventType)\n\n// If you have a large amount of data and only want to optimize a subset of it, you can specify an optional partition predicate by using \"where\".\ndeltaTable.optimize().where(\"date='2021-11-18'\").executeZOrderBy(eventType)\n```\n\n----------------------------------------\n\nTITLE: Changing Column Type in Delta Table (Python)\nDESCRIPTION: This snippet demonstrates how to change a column's data type in a Delta table using PySpark. It reads a table, casts the 'birthDate' column to 'date' type, and overwrites the table with the updated schema.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nspark.read.table(...) \\\n  .withColumn(\"birthDate\", col(\"birthDate\").cast(\"date\")) \\\n  .write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"overwriteSchema\", \"true\") \\\n  .saveAsTable(...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom LogStore Implementation in Spark\nDESCRIPTION: Set Spark configuration to use a custom LogStore implementation for a specific storage scheme.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\nspark.delta.logStore.<scheme>.impl=<full-qualified-class-name>\n```\n\n----------------------------------------\n\nTITLE: Initializing Azure Data Lake Storage Gen2 File System for Delta Lake in Scala\nDESCRIPTION: Initializes the Azure Data Lake Storage Gen2 file system for use with Delta Lake. This step is optional and may be needed in some cases.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/\")\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n```\n\n----------------------------------------\n\nTITLE: Building a Scan Object for Delta Table Reading in Java\nDESCRIPTION: Creates a Scan object using a ScanBuilder, which is necessary for reading table data. It also demonstrates how to retrieve scan state and scan files information.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nScan myScan = mySnapshot.getScanBuilder().build()\n\n// Common information about scanning for all data files to read.\nRow scanState = myScan.getScanState(myEngine)\n\n// Information about the list of scan files to read\nCloseableIterator<FilteredColumnarBatch> scanFiles = myScan.getScanFiles(myEngine)\n```\n\n----------------------------------------\n\nTITLE: Enabling UniForm for Hudi on Existing Table in SQL\nDESCRIPTION: SQL command to enable UniForm Hudi on an existing Delta table by setting the required table property.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-uniform.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET TBLPROPERTIES ('delta.universalFormat.enabledFormats' = 'hudi');\n```\n\n----------------------------------------\n\nTITLE: Specifying Delta Protocol Version (JSON)\nDESCRIPTION: This JSON snippet shows how to define the minimum reader and writer versions required for a Delta table. It helps ensure compatibility between clients and the table's features.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"protocol\":{\n    \"minReaderVersion\":1,\n    \"minWriterVersion\":2\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Data Transformations during JDBC to Delta Lake Import (Scala)\nDESCRIPTION: Illustrates how to use JDBCImport in a Scala project to specify custom transformations that will be applied during the import process. It includes examples of converting timestamp columns to strings and casting an ID column to string type.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/sql-delta-import/readme.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport io.delta.connectors.spark.jdbc._\n  \n  implicit val spark: SparkSession = SparkSession\n    .builder()\n    .master(\"local[*]\")\n    .getOrCreate()\n\n // All additional possible jdbc connector properties described here -\n // https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-reference-configuration-properties.html\n  \n  val jdbcUrl = \"jdbc:mysql://hostName:port/database\"\n\n  val config = ImportConfig(\n    source = \"table\",\n    destination = \"target_database.table\",\n    splitBy = \"id\",\n    chunks = 10)\n\n  // define a transform to convert all timestamp columns to strings\n  val timeStampsToStrings : DataFrame => DataFrame = source => {\n    val tsCols = source.schema.fields.filter(_.dataType == DataTypes.TimestampType).map(_.name)\n     tsCols.foldLeft(source)((df, colName) =>\n       df.withColumn(colName, from_unixtime(unix_timestamp(col(colName)), \"yyyy-MM-dd HH:mm:ss.S\")))\n}\n\n  // Whatever functions are passed to below transform will be applied during import\n  val transforms = new DataTransforms(Seq(\n      df => df.withColumn(\"id\", col(\"id\").cast(types.StringType)), // cast id column to string\n      timeStampsToStrings // use transform defined above for timestamp conversion\n    ))\n\n  new JDBCImport(jdbcUrl = jdbcUrl, importConfig = config, dataTransform = transforms)\n    .run()\n```\n\n----------------------------------------\n\nTITLE: Altering Column Type Manually\nDESCRIPTION: SQL command to manually change a column's type using ALTER COLUMN when type widening is enabled.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-type-widening.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> ALTER COLUMN <col_name> TYPE <new_type>\n```\n\n----------------------------------------\n\nTITLE: Defining Clustered Table in Delta Lake\nDESCRIPTION: Outlines the criteria and requirements for defining a table as a clustered table in Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_32\n\nLANGUAGE: markdown\nCODE:\n```\n- When the feature `clustering` exists in the table `protocol`'s `writerFeatures`, then we say that the table is a clustered table.\n  The feature `domainMetadata` is required in the table `protocol`'s `writerFeatures`.\n\nEnablement:\n- The table must be on Writer Version 7.\n- The feature `clustering` must exist in the table `protocol`'s `writerFeatures`, either during its creation or at a later stage, provided the table does not have partition columns.\n```\n\n----------------------------------------\n\nTITLE: Creating DynamoDB Table for Delta Lake\nDESCRIPTION: AWS CLI command to create a DynamoDB table with required schema for Delta Lake multi-cluster coordination.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\naws dynamodb create-table \\\n  --region us-east-1 \\\n  --table-name delta_log \\\n  --attribute-definitions AttributeName=tablePath,AttributeType=S \\\n                          AttributeName=fileName,AttributeType=S \\\n  --key-schema AttributeName=tablePath,KeyType=HASH \\\n               AttributeName=fileName,KeyType=RANGE \\\n  --billing-mode PAY_PER_REQUEST\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Scala\nDESCRIPTION: Scala code to create a Delta table from a DataFrame. This creates a table with a single column containing values 0-4 at the specified location.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval data = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"/tmp/delta-table\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Manifest Updates for Delta Tables\nDESCRIPTION: SQL command to enable automatic manifest updates for a Delta table. When enabled, all write operations on the table will automatically update the manifests, ensuring they stay current with the table data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/presto-integration.md#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE delta.`<path-to-delta-table>` SET TBLPROPERTIES(delta.compatibility.symlinkFormatManifest.enabled=true)\n```\n\n----------------------------------------\n\nTITLE: Changing Column Comment or Ordering in Delta Tables using SQL\nDESCRIPTION: SQL example showing how to change column comments or reorder columns in a Delta table schema using ALTER TABLE with the ALTER COLUMN syntax.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_31\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name ALTER [COLUMN] col_name col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]\n```\n\n----------------------------------------\n\nTITLE: Setting User-defined Commit Metadata in Delta Tables using Scala\nDESCRIPTION: Scala example of setting user-defined metadata for commits to Delta tables using the userMetadata option, which can be read in the table history.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_28\n\nLANGUAGE: scala\nCODE:\n```\ndf.write.format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n  .save(\"/tmp/delta/people10m\")\n```\n\n----------------------------------------\n\nTITLE: Reading UniForm Tables as Hudi in Spark\nDESCRIPTION: Scala code to read a UniForm table as a Hudi table in Apache Spark, enabling Hudi metadata for improved performance.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-uniform.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nspark.read.format(\"hudi\").option(\"hoodie.metadata.enable\", \"true\").load(\"PATH_TO_UNIFORM_TABLE_DIRECTORY\")\n```\n\n----------------------------------------\n\nTITLE: Delta Data Type to Parquet Type Mapping Table\nDESCRIPTION: A reference table showing how Delta Lake data types are mapped to Parquet physical types and logical types when storing data in Parquet files. This mapping is essential for both writers and readers of Delta Lake tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_53\n\nLANGUAGE: markdown\nCODE:\n```\nDelta Type Name | Parquet Physical Type | Parquet Logical Type\n-|-|-\nboolean| `boolean` |\nbyte| `int32` | `INT(bitwidth = 8, signed = true)`\nshort| `int32` | `INT(bitwidth = 16, signed = true)`\nint| `int32` | `INT(bitwidth = 32, signed = true)`\nlong| `int64` | `INT(bitwidth = 64, signed = true)`\ndate| `int32` | `DATE`\ntimestamp| `int96` or `int64` | `TIMESTAMP(isAdjustedToUTC = true, units = microseconds)`\ntimestamp without time zone| `int96` or `int64` | `TIMESTAMP(isAdjustedToUTC = false, units = microseconds)`\nfloat| `float` |\ndouble| `double` |\ndecimal| `int32`, `int64` or `fixed_length_binary` | `DECIMAL(scale, precision)`\nstring| `binary` | `string (UTF-8)`\nbinary| `binary` |\narray| either as `2-level` or `3-level` representation. Refer to [Parquet documentation](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists) for further details | `LIST`\nmap| either as `2-level` or `3-level` representation. Refer to [Parquet documentation](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#maps) for further details | `MAP`\nstruct| `group` |\n```\n\n----------------------------------------\n\nTITLE: Reading Single-JVM Parquet Data with Delta Standalone\nDESCRIPTION: Shows how to read Parquet data directly in a single JVM using Delta Standalone's Snapshot.open() method. Demonstrates accessing specific columns from each row record.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.standalone.data.RowRecord;\n\nCloseableIterator<RowRecord> dataIter = log.update().open();\n\ntry {\n    while (dataIter.hasNext()) {\n        RowRecord row = dataIter.next();\n        int year = row.getInt(\"year\");\n        String customer = row.getString(\"customer\");\n        float totalCost = row.getFloat(\"total_cost\");\n    }\n} finally {\n    dataIter.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Parquet to Delta with Save As Pattern\nDESCRIPTION: Shows how to read Parquet data into a DataFrame and save it as a new Delta table in a different location. Includes creating a table reference to the saved Delta files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = spark.read.format(\"parquet\").load(\"/data-pipeline\")\ndata.write.format(\"delta\").save(\"/tmp/delta/data-pipeline/\")\n```\n\nLANGUAGE: python\nCODE:\n```\nspark.sql(\"CREATE TABLE events USING DELTA LOCATION '/tmp/delta/data-pipeline/'\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession for Delta Lake in Python\nDESCRIPTION: Initialize a SparkSession with Delta Lake configurations in Python. Adds required extensions and catalog configurations for Delta Lake 0.7+ compatibility.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n  .builder \\\n  .appName(\"...\") \\\n  .master(\"...\") \\\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n  .getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Building a Table Scan with Schema and Filters in Java\nDESCRIPTION: This snippet shows how to build a scan operation with specific read schema and filter predicates. It demonstrates converting engine-specific schema and expressions to Delta Kernel objects and creating a scan builder with filters and schema.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_34\n\nLANGUAGE: Java\nCODE:\n```\nimport io.delta.kernel.expressions.*;\nimport io.delta.kernel.types.*;\n\nStructType readSchema = ... ;  // convert engine schema\nPredicate filterExpr = ... ;   // convert engine filter expression\n\nScan myScan = mySnapshot.getScanBuilder().withFilter(filterExpr).withReadSchema(readSchema).build();\n```\n\n----------------------------------------\n\nTITLE: Reading Row Tracking Metadata Fields\nDESCRIPTION: Commands to access the row tracking metadata fields (_metadata.row_id and _metadata.row_commit_version) when reading a Delta table, shown in SQL, Python, and Scala.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-row-tracking.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT _metadata.row_id, _metadata.row_commit_version, * FROM table_name;\n```\n\nLANGUAGE: python\nCODE:\n```\nspark.read.table(\"table_name\") \\\n  .select(\"_metadata.row_id\", \"_metadata.row_commit_version\", \"*\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nspark.read.table(\"table_name\")\n  .select(\"_metadata.row_id\", \"_metadata.row_commit_version\", \"*\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table and Transaction Builder in Java\nDESCRIPTION: Code to initialize a Delta table object, resolve schema, and create a transaction builder. Handles both new and existing table scenarios with schema validation.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_33\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.*;\nimport io.delta.kernel.defaults.engine.*;\n\nEngine myEngine = new MyEngine();\nTable myTable = Table.forPath(myTablePath);\n\nStructType writeOperatorSchema = // ... derived from the query operator tree ...\nStructType dataSchema;\nboolean isNewTable = false;\n\ntry {\n  Snapshot mySnapshot = myTable.getLatestSnapshot(myEngine);\n  dataSchema = mySnapshot.getSchema(myEngine);\n\n  // .. check dataSchema and writeOperatorSchema match ...\n} catch(TableNotFoundException e) {\n  isNewTable = true;\n  dataSchema = writeOperatorSchema;\n}\n\nTransactionBuilder txnBuilder =\n  myTable.createTransactionBuilder(\n    myEngine,\n    \"Examples\", /* engineInfo - connector can add its own identifier which is noted in the Delta Log */ \n    Operation /* What is the operation we are trying to perform? This is noted in the Delta Log */\n  );\n\nif (isNewTable) {\n  // For a new table set the table schema in the transaction builder\n  txnBuilder = txnBuilder.withSchema(engine, dataSchema)\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Scan Files and State for Delta Table Reading\nDESCRIPTION: This code demonstrates how to access scan state and files from a Scan object. It retrieves the common scan state row and iterates through batches of scan file information, which is needed to read the actual data from the table files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_35\n\nLANGUAGE: Java\nCODE:\n```\nRow myScanStateRow = myScan.getScanState();\nCloseableIterator<FilteredColumnarBatch> myScanFilesAsBatches = myScan.getScanFiles();\n\nwhile (myScanFilesAsBatches.hasNext()) {\n  FilteredColumnarBatch scanFileBatch = myScanFilesAsBatches.next();\n\n  CloseableIterator<Row> myScanFilesAsRows = scanFileBatch.getRows();\n}\n```\n\n----------------------------------------\n\nTITLE: Running Python Delta Lake Examples with spark-submit\nDESCRIPTION: Command to run a Python example file with Delta Lake using spark-submit. Requires specifying the Delta Lake version in the packages parameter.\nSOURCE: https://github.com/delta-io/delta/blob/master/examples/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nspark-submit --packages io.delta:delta-spark_2.12:{Delta Lake version} PATH/TO/EXAMPLE\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Azure Blob Storage (Scala)\nDESCRIPTION: Scala code demonstrating how to write and read Delta tables using Azure Blob Storage. This shows the basic usage pattern for Delta Lake with Azure Blob Storage.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(5).write.format(\"delta\").save(\"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<path-to-delta-table>\")\nspark.read.format(\"delta\").load(\"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Starting Spark SQL Shell with Delta Lake\nDESCRIPTION: Command to start the Spark SQL shell with Delta Lake package and configuration. This configures necessary session extensions and catalog settings for Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/spark-sql --packages io.delta:delta-spark_2.12:3.3.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table Stream with Event Time Ordering in Scala\nDESCRIPTION: This snippet demonstrates how to read a Delta table stream with event time ordering. It uses the 'withEventTimeOrder' option to ensure no data drop during initial snapshot processing.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-streaming.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nspark.readStream.format(\"delta\")\n  .option(\"withEventTimeOrder\", \"true\")\n  .load(\"/tmp/delta/user_events\")\n  .withWatermark(\"event_time\", \"10 seconds\")\n```\n\n----------------------------------------\n\nTITLE: Creating Bounded Delta Source with Time Travel in Java\nDESCRIPTION: Creates a bounded Delta source that reads all columns from a specific historical version using time travel. Supports both timestamp and version-based time travel.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createBoundedDeltaSourceWithTimeTravel(\n        StreamExecutionEnvironment env,\n        String deltaTablePath) {\n\n    DeltaSource<RowData> deltaSource = DeltaSource\n        .forBoundedRowData(\n            new Path(deltaTablePath),\n            new Configuration())\n        // could also use `.versionAsOf(314159)`\n        .timestampAsOf(\"2022-06-28 04:55:00\")\n        .build();\n\n    return env.fromSource(deltaSource, WatermarkStrategy.noWatermarks(), \"delta-source\");\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession for Delta Lake in Scala\nDESCRIPTION: Initialize a SparkSession with Delta Lake configurations in Scala. Adds required extensions and catalog configurations for Delta Lake 0.7+ compatibility.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession\n  .builder()\n  .appName(\"...\")\n  .master(\"...\")\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate()\n```\n\n----------------------------------------\n\nTITLE: Implementing Blind Append Operations in Delta Tables using Java\nDESCRIPTION: Code demonstrating how to perform blind append operations into an existing Delta table using TransactionBuilder. Handles partitioned data writing and transaction commits.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\n// Create a `Table` object with the given destination table path\nTable table = Table.forPath(engine, tablePath);\n\n// Create a transaction builder to build the transaction\nTransactionBuilder txnBuilder =\n  table.createTransactionBuilder(\n    engine,\n    \"Examples\", /* engineInfo */\n    Operation.WRITE\n  );\n\n/ Build the transaction - no need to provide the schema as the table already exists.\nTransaction txn = txnBuilder.build(engine);\n\n// Get the transaction state\nRow txnState = txn.getTransactionState(engine);\n\nList<Row> dataActions = new ArrayList<>();\n\n// Generate the sample data for three partitions. Process each partition separately.\nfor (String city : Arrays.asList(\"San Francisco\", \"Campbell\", \"San Jose\")) {\n  FilteredColumnarBatch batch1 = generatedPartitionedDataBatch(\n\t    5 /* offset */, city /* partition value */);\n  FilteredColumnarBatch batch2 = generatedPartitionedDataBatch(\n\t    5 /* offset */, city /* partition value */);\n  FilteredColumnarBatch batch3 = generatedPartitionedDataBatch(\n\t    10 /* offset */, city /* partition value */);\n\n    CloseableIterator<FilteredColumnarBatch> data =\n\t    toCloseableIterator(Arrays.asList(batch1, batch2, batch3).iterator());\n\n    Map<String, Literal> partitionValues =\n\t    Collections.singletonMap(\n\t\t    \"city\",\n\t\t    Literal.ofString(city));\n\n    CloseableIterator<FilteredColumnarBatch> physicalData =\n\t    Transaction.transformLogicalData(engine, txnState, data, partitionValues);\n\n    DataWriteContext writeContext =\n\t    Transaction.getWriteContext(engine, txnState, partitionValues);\n\n    CloseableIterator<DataFileStatus> dataFiles = engine.getParquetHandler()\n\t    .writeParquetFiles(\n\t\t    writeContext.getTargetDirectory(),\n\t\t    physicalData,\n\t\t    writeContext.getStatisticsColumns());\n\n    CloseableIterator<Row> partitionDataActions = Transaction.generateAppendActions(\n\t    engine,\n\t    txnState,\n\t    dataFiles,\n\t    writeContext);\n\n    while (partitionDataActions.hasNext()) {\n\t    dataActions.add(partitionDataActions.next());\n    }\n}\n\nCloseableIterable<Row> dataActionsIterable = CloseableIterable.inMemoryIterable(\n\ttoCloseableIterator(dataActions.iterator()));\n\nTransactionCommitResult commitResult = txn.commit(engine, dataActionsIterable);\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table Snapshot in Java\nDESCRIPTION: Shows how to initialize Engine object, create Table instance and retrieve latest snapshot with schema. Used for resolving table snapshots during query planning.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_28\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.*;\nimport io.delta.kernel.defaults.engine.*;\n\nEngine myEngine = new MyEngine();\nTable myTable = Table.forPath(myTablePath);\nSnapshot mySnapshot = myTable.getLatestSnapshot(myEngine);\nStructType mySchema = mySnapshot.getSchema(myEngine);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Schema and Partition Information\nDESCRIPTION: Retrieves the schema and partition column information from the transaction object for data preparation and validation.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nStructType dataSchema = txn.getSchema(engine)\n\n// Optional for un-partitioned tables\nList<String> partitionColumnNames = txn.getPartitionColumns(engine)\n```\n\n----------------------------------------\n\nTITLE: Implementing ParquetHandler.writeParquetFileAtomically Method in Delta Kernel\nDESCRIPTION: Method for atomically writing a single Parquet file that ensures all content is written or nothing is written. Uses LogStore implementations for atomicity.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\nwriteParquetFileAtomically(String filePath, CloseableIterator<FilteredColumnarBatch> data)\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with IBM Cloud Object Storage in Scala\nDESCRIPTION: Demonstrates writing and reading Delta tables using IBM Cloud Object Storage. Requires specifying the COS bucket and service name in the path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_27\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(5).write.format(\"delta\").save(\"cos://<your-cos-bucket>.service/<path-to-delta-table>\")\nspark.read.format(\"delta\").load(\"cos://<your-cos-bucket>.service/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Creating an External Table in Redshift to Read Delta Manifests\nDESCRIPTION: SQL statement to create an external table in Redshift that uses the generated manifest files to read Delta table data. Uses SymlinkTextInputFormat to process the manifest files instead of direct directory listings.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/redshift-spectrum-integration.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE mytable ([(col_name1 col_datatype1, ...)])\n[PARTITIONED BY (col_name2 col_datatype2, ...)]\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION '<path-to-delta-table>/_symlink_format_manifest/'  -- location of the generated manifest\n```\n\n----------------------------------------\n\nTITLE: Enabling Default Column Values in Delta Tables using SQL\nDESCRIPTION: SQL command to enable default column values feature for a Delta table. This setting upgrades the table's protocol version to support table features and requires Delta 3.1.0 or above for subsequent write operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-default-columns.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> SET TBLPROPERTIES (\n  'delta.feature.allowColumnDefaults' = 'enabled'\n)\n```\n\n----------------------------------------\n\nTITLE: Fixing Invalid Partition Column Names\nDESCRIPTION: Code examples in Python and Scala for renaming partition columns containing invalid characters when upgrading to Delta 1.1 or above.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nspark.read \\\n  .format(\"delta\") \\\n  .load(\"/the/delta/table/path\") \\\n  .withColumnRenamed(\"column name\", \"column-name\") \\\n  .write \\\n  .format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"overwriteSchema\", \"true\") \\\n  .save(\"/the/delta/table/path\")\n```\n\nLANGUAGE: scala\nCODE:\n```\nspark.read\n  .format(\"delta\")\n  .load(\"/the/delta/table/path\")\n  .withColumnRenamed(\"column name\", \"column-name\")\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"overwriteSchema\", \"true\")\n  .save(\"/the/delta/table/path\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SBT Dependencies for Delta Standalone\nDESCRIPTION: SBT build configuration showing how to add Delta Standalone and Hadoop client dependencies for Scala projects.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"io.delta\" %% \"delta-standalone\" % \"0.5.0\",\n  \"org.apache.hadoop\" % \"hadoop-client\" % \"3.1.0)\n```\n\n----------------------------------------\n\nTITLE: Enabling Row Tracking on Existing Tables in SQL\nDESCRIPTION: SQL command for enabling row tracking on existing Delta tables using the ALTER TABLE command. Available from Delta 3.3 and above.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-row-tracking.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE grade SET TBLPROPERTIES ('delta.enableRowTracking' = 'true');\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Catalog with Hive Metastore in Flink SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a Delta Catalog backed by Hive catalog using the 'hadoop-conf-dir' option. It includes creating the catalog and switching to it.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nCREATE CATALOG <catalog_name> WITH (\n  'type' = 'delta-catalog',\n  'catalog-type' = 'hive',\n  'hadoop-conf-dir' = '<some-path>'\n);\nUSE CATALOG <catalog_name>;\n```\n\n----------------------------------------\n\nTITLE: Getting Transaction State\nDESCRIPTION: Retrieves the transaction state which contains information for data transformation based on table protocol and features.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nRow txnState = txn.getTransactionState(engine);\n```\n\n----------------------------------------\n\nTITLE: Creating Bounded Delta Source with Specific Columns in Java\nDESCRIPTION: Creates a bounded Delta source that reads only specified columns from the latest table version. Allows column filtering for optimized data reading.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createBoundedDeltaSourceUserColumns(\n        StreamExecutionEnvironment env,\n        String deltaTablePath,\n        String[] columnNames) {\n\n    DeltaSource<RowData> deltaSource = DeltaSource\n        .forBoundedRowData(\n            new Path(deltaTablePath),\n            new Configuration())\n        .columnNames(columnNames)\n        .build();\n\n    return env.fromSource(deltaSource, WatermarkStrategy.noWatermarks(), \"delta-source\");\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table and Transaction in Java\nDESCRIPTION: This snippet demonstrates how to create a Table object, resolve the schema, and initialize a TransactionBuilder for Delta Lake operations. It handles both existing and new table scenarios.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_38\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.*;\nimport io.delta.kernel.defaults.engine.*;\n\nEngine myEngine = new MyEngine();\nTable myTable = Table.forPath(myTablePath);\n\nStructType writeOperatorSchema = // ... derived from the query operator tree ...\nStructType dataSchema;\nboolean isNewTable = false;\n\ntry {\n  Snapshot mySnapshot = myTable.getLatestSnapshot(myEngine);\n  dataSchema = mySnapshot.getSchema(myEngine);\n\n  // .. check dataSchema and writeOperatorSchema match ...\n} catch(TableNotFoundException e) {\n  isNewTable = true;\n  dataSchema = writeOperatorSchema;\n}\n\nTransactionBuilder txnBuilder =\n  myTable.createTransactionBuilder(\n    myEngine,\n    \"Examples\", /* engineInfo - connector can add its own identifier which is noted in the Delta Log */ \n    Operation /* What is the operation we are trying to perform? This is noted in the Delta Log */\n  );\n\nif (isNewTable) {\n  // For a new table set the table schema in the transaction builder\n  txnBuilder = txnBuilder.withSchema(engine, dataSchema)\n}\n```\n\n----------------------------------------\n\nTITLE: Checking UniForm Metadata Generation Status in SQL\nDESCRIPTION: SQL command to view table properties, including UniForm metadata generation status, for a Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-uniform.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSHOW TBLPROPERTIES <table-name>;\n```\n\n----------------------------------------\n\nTITLE: Setting a Default Value for an Existing Column in Delta Tables\nDESCRIPTION: SQL command to assign or update a default value for an existing column in a Delta table using the ALTER TABLE statement.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-default-columns.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE t ALTER COLUMN c SET DEFAULT 16;\n```\n\n----------------------------------------\n\nTITLE: Adding Nested Columns to Delta Table Schema using SQL\nDESCRIPTION: SQL example showing how to add nested columns to a struct field in a Delta table schema using ALTER TABLE with the ADD COLUMNS syntax.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_30\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name ADD COLUMNS (col_name.nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)\n```\n\n----------------------------------------\n\nTITLE: Creating External Table for Presto/Trino/Athena to Read Delta Manifests\nDESCRIPTION: SQL command to create an external table in the Hive metastore for Presto/Trino/Athena to read Delta table manifests. This table uses SymlinkTextInputFormat to read the manifest files instead of directly listing data files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/presto-integration.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTERNAL TABLE mytable ([(col_name1 col_datatype1, ...)])\n[PARTITIONED BY (col_name2 col_datatype2, ...)]\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION '<path-to-delta-table>/_symlink_format_manifest/'\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned Delta Table in Flink SQL\nDESCRIPTION: This SQL snippet shows how to create a partitioned Delta table using the CREATE TABLE statement with the PARTITIONED BY clause. It includes column definitions, partition columns, and required connector properties.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE testTable (\n    id BIGINT,\n    data STRING,\n    part_a STRING,\n    part_b STRING\n  )\n  PARTITIONED BY (part_a, part_b);\n  WITH (\n    'connector' = 'delta',\n    'table-path' = '<path-to-table>',\n    '<arbitrary-user-define-table-property' = '<value>',\n    '<delta.*-properties>' = '<value'>);\n```\n\n----------------------------------------\n\nTITLE: Defining Column Schema with Type Change History in Delta Lake (JSON)\nDESCRIPTION: Example JSON schema definition for a column that underwent two type changes, from short to integer and then from integer to long. The type changes are recorded in the column's metadata under the delta.typeChanges key.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : \"long\",\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.typeChanges\": [\n        {\n          \"fromType\": \"short\",\n          \"toType\": \"integer\"\n        },\n        {\n          \"fromType\": \"integer\",\n          \"toType\": \"long\"\n        }\n      ]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing Row Objects in Delta Lake Connector\nDESCRIPTION: Code showing how to serialize Row objects at the driver and deserialize them at worker nodes. This is necessary for distributing scan metadata from the query planning machine to task execution machines in a distributed engine.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_31\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.utils.*;\n\n// In the driver where query planning is being done\nByte[] scanStateRowBytes = RowUtils.serialize(scanStateRow);\nByte[] scanFileRowBytes = RowUtils.serialize(scanFileRow);\n\n// Optionally the connector adds a split info to the task (scan file, scan state) to\n// split reading of a Parquet file into multiple tasks. The task gets split info\n// along with the scan file row and scan state row.\nSplit split = ...; // connector specific class, not related to Kernel\n\n// Send these over to the worker\n\n// In the worker when data will be read, after rowBytes have been sent over\nRow scanStateRow = RowUtils.deserialize(scanStateRowBytes);\nRow scanFileRow = RowUtils.deserialize(scanFileRowBytes);\nSplit split = ... deserialize split info ...;\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Files for Delta Table Data in Java\nDESCRIPTION: This snippet illustrates how to write physical data to Parquet files for a Delta table. It includes getting the write context and using the ParquetHandler to write the files.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// Get the write context\nDataWriteContext writeContext = Transaction.getWriteContext(engine, txnState, partitionValues);\n\nCloseableIterator<DataFileStatus> dataFiles = engine.getParquetHandler()\n  .writeParquetFiles(\n    writeContext.getTargetDirectory(),\n    physicalData,\n    writeContext.getStatisticsColumns()\n  );\n```\n\n----------------------------------------\n\nTITLE: Dropping a Delta Table in Flink SQL\nDESCRIPTION: This SQL snippet demonstrates how to drop a Delta table using the DROP TABLE statement. Note that this only removes the metastore entry and not the actual table files.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_16\n\nLANGUAGE: sql\nCODE:\n```\nDROP TABLE sample;\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Delta Table with Python\nDESCRIPTION: Python code to completely overwrite data in a Delta table. This replaces the existing data with new values 5-9 using the overwrite mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndata = spark.range(5, 10)\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n```\n\n----------------------------------------\n\nTITLE: Reading Distributed Parquet Data with Delta Standalone\nDESCRIPTION: Demonstrates how to read Parquet data in a distributed manner using Delta Standalone's scan API with partition pruning and filtering. Uses the Snapshot.scan() method to efficiently iterate through files matching specified predicates.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_14\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.standalone.Snapshot;\n\nDeltaLog log = DeltaLog.forTable(new Configuration(), \"/data/sales\");\nSnapshot latestSnapshot = log.update();\nStructType schema = latestSnapshot.getMetadata().getSchema();\nDeltaScan scan = latestSnapshot.scan(\n    new And(\n        new And(\n            new EqualTo(schema.column(\"year\"), Literal.of(2021)),\n            new EqualTo(schema.column(\"month\"), Literal.of(11))),\n        new EqualTo(schema.column(\"customer\"), Literal.of(\"XYZ\"))\n    )\n);\n\nCloseableIterator<AddFile> iter = scan.getFiles();\n\ntry {\n    while (iter.hasNext()) {\n        AddFile addFile = iter.next();\n\n        // Zappy engine to handle reading data in `addFile.getPath()` and apply any `scan.getResidualPredicate()`\n    }\n} finally {\n    iter.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table Transaction\nDESCRIPTION: Sets up a new transaction for table creation by configuring schema, engine, and optional partition columns. Demonstrates core setup requirements for Delta table operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_10\n\nLANGUAGE: java\nCODE:\n```\npackage io.delta.kernel.examples;\n\nimport io.delta.kernel.*;\nimport io.delta.kernel.types.*;\nimport io.delta.kernel.utils.CloseableIterable;\n\nString myTablePath = <my-table-path>; \nConfiguration hadoopConf = new Configuration();\nEngine myEngine = DefaultEngine.create(hadoopConf);\nTable myTable = Table.forPath(myEngine, myTablePath);\n\nStructType mySchema = new StructType()\n  .add(\"id\", IntegerType.INTEGER)\n  .add(\"name\", StringType.STRING)\n  .add(\"city\", StringType.STRING)\n  .add(\"salary\", DoubleType.DOUBLE);\n\n// Partition columns are optional. Use it only if you are creating a partitioned table.\nList<String> myPartitionColumns = Collections.singletonList(\"city\");\n\nTransactionBuilder txnBuilder =\n  myTable.createTransactionBuilder(\n    myEngine,\n    \"Examples\", /* engineInfo - connector can add its own identifier which is noted in the Delta Log */ \n    Operation.WRITE /* What is the operation we are trying to perform? This is noted in the Delta Log */\n  );\n\n// Set the schema of the new table on the transaction builder\ntxnBuilder = txnBuilder\n  .withSchema(engine, mySchema);\n\n// Set the partition columns of the new table only if you are creating\n// a partitioned table; otherwise, this step can be skipped.\ntxnBuilder = txnBuilder\n  .withPartitionColumns(engine, examplePartitionColumns);\n\n// Build the transaction\nTransaction txn = txnBuilder.build(engine);\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Blob Storage with SAS Token\nDESCRIPTION: Sets up authentication for Azure Blob Storage using a Shared Access Signature (SAS) token in the Hadoop configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nconf.set(\n  \"fs.azure.sas.<your-container-name>.<your-storage-account-name>.blob.core.windows.net\",\n  \"<complete-query-string-of-your-sas-for-the-container>\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table for CHECK Constraint Example in SQL\nDESCRIPTION: This snippet creates a Delta table named 'people10m' with various columns to be used in the CHECK constraint examples. It defines the table structure without any constraints initially.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-constraints.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE default.people10m (\n   id INT,\n   firstName STRING,\n   middleName STRING,\n   lastName STRING,\n   gender STRING,\n   birthDate TIMESTAMP,\n   ssn STRING,\n   salary INT\n ) USING DELTA;\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession for Delta Lake in Java\nDESCRIPTION: Initialize a SparkSession with Delta Lake configurations in Java. Adds required extensions and catalog configurations for Delta Lake 0.7+ compatibility.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.spark.sql.SparkSession;\n\nSparkSession spark = SparkSession\n  .builder()\n  .appName(\"...\")\n  .master(\"...\")\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate();\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake External Stage for Delta Table\nDESCRIPTION: This SQL command creates or replaces an external stage in Snowflake that points to the location of the Delta table. It is a prerequisite for defining external tables on the manifest files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/snowflake-integration.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace stage my_staged_table url='<path-to-delta-table>'\n```\n\n----------------------------------------\n\nTITLE: Enabling Column Mapping in Delta Tables with SQL\nDESCRIPTION: SQL command to upgrade Delta table version and enable column mapping by setting required protocol versions and the column mapping mode to 'name'.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-column-mapping.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> SET TBLPROPERTIES (\n  'delta.minReaderVersion' = '2',\n  'delta.minWriterVersion' = '5',\n  'delta.columnMapping.mode' = 'name'\n)\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Checkpoint Schema Structure\nDESCRIPTION: Detailed schema structure for Delta Lake checkpoints, including metadata, protocol, transaction information, add/remove actions, and sidecar files for a table with partition columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_49\n\nLANGUAGE: txt\nCODE:\n```\n|-- metaData: struct\n|    |-- id: string\n|    |-- name: string\n|    |-- description: string\n|    |-- format: struct\n|    |    |-- provider: string\n|    |    |-- options: map<string,string>\n|    |-- schemaString: string\n|    |-- partitionColumns: array<string>\n|    |-- createdTime: long\n|    |-- configuration: map<string, string>\n|-- protocol: struct\n|    |-- minReaderVersion: int\n|    |-- minWriterVersion: int\n|    |-- readerFeatures: array[string]\n|    |-- writerFeatures: array[string]\n|-- txn: struct\n|    |-- appId: string\n|    |-- version: long\n|-- add: struct\n|    |-- path: string\n|    |-- partitionValues: map<string,string>\n|    |-- size: long\n|    |-- modificationTime: long\n|    |-- dataChange: boolean\n|    |-- stats: string\n|    |-- tags: map<string,string>\n|    |-- baseRowId: long\n|    |-- defaultRowCommitVersion: long\n|    |-- partitionValues_parsed: struct\n|    |    |-- date: date\n|    |    |-- region: string\n|    |-- stats_parsed: struct\n|    |    |-- numRecords: long\n|    |    |-- minValues: struct\n|    |    |    |-- asset: string\n|    |    |    |-- quantity: double\n|    |    |-- maxValues: struct\n|    |    |    |-- asset: string\n|    |    |    |-- quantity: double\n|    |    |-- nullCounts: struct\n|    |    |    |-- asset: long\n|    |    |    |-- quantity: long\n|-- remove: struct\n|    |-- path: string\n|    |-- deletionTimestamp: long\n|    |-- dataChange: boolean\n|-- checkpointMetadata: struct\n|    |-- version: long\n|    |-- tags: map<string,string>\n|-- sidecar: struct\n|    |-- path: string\n|    |-- sizeInBytes: long\n|    |-- modificationTime: long\n|    |-- tags: map<string,string>\n```\n\n----------------------------------------\n\nTITLE: Setting Up S3 Credentials in Spark Configuration\nDESCRIPTION: Scala code to set S3 access and secret keys in Spark's Hadoop configuration for authentication.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"<your-s3-access-key>\")\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"<your-s3-secret-key>\")\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Delta Table with Scala\nDESCRIPTION: Scala code to completely overwrite data in a Delta table. This replaces the existing data with new values 5-9 using the overwrite mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\nval data = spark.range(5, 10)\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Delta Table with SQL\nDESCRIPTION: SQL command to completely overwrite data in a Delta table. This replaces the existing data with new values 5-9.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\nINSERT OVERWRITE delta.`/tmp/delta-table` SELECT col1 as id FROM VALUES 5,6,7,8,9;\n```\n\n----------------------------------------\n\nTITLE: Adding a File to a Clustered Delta Lake Table\nDESCRIPTION: This JSON snippet shows an 'add' action for adding a file to a clustered Delta Lake table. It includes clustering-specific information along with file metadata and statistics.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"add\": {\n    \"path\": \"date=2017-12-10/part-000...c000.gz.parquet\",\n    \"partitionValues\": {},\n    \"size\": 841454,\n    \"modificationTime\": 1512909768000,\n    \"dataChange\": true,\n    \"baseRowId\": 4071,\n    \"defaultRowCommitVersion\": 41,\n    \"clusteringProvider\": \"liquid\",\n    \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"val...\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a TransactionBuilder for Delta Table Operations in Java\nDESCRIPTION: This snippet illustrates how to create a TransactionBuilder object from a Delta Table, which is used to construct transactions for table operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nTransactionBuilder txnBuilder =\n  myTable.createTransactionBuilder(\n    myEngine,\n    \"Examples\", /* engineInfo - connector can add its own identifier which is noted in the Delta Log */ \n    Operation.CREATE_TABLE /* What is the operation we are trying to perform. This is noted in the Delta Log */\n  );\n```\n\n----------------------------------------\n\nTITLE: Running Queries on 1GB TPC-DS Delta Tables\nDESCRIPTION: Shell command to execute the TPC-DS query suite against the previously loaded 1GB Delta tables. This benchmark measures query performance on the Delta format.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --cloud-provider <CLOUD_PROVIDER> \\\n    --benchmark tpcds-1gb-delta\n```\n\n----------------------------------------\n\nTITLE: Removing Type Widening Feature\nDESCRIPTION: SQL command to remove the type widening feature from a Delta table, which rewrites Parquet files if necessary to ensure type consistency.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-type-widening.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> DROP FEATURE 'typeWidening-preview' [TRUNCATE HISTORY]\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Lake using spark-submit\nDESCRIPTION: Command-line configuration for Delta Lake when using spark-submit. Specifies required extensions and catalog configurations via command-line parameters.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nspark-submit --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"  ...\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake Partition Elimination in PowerQuery\nDESCRIPTION: Example demonstrating how to use partition elimination with a filter function to efficiently load only data from Sales Territories where SalesTerritoryKey ≥ 5.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_6\n\nLANGUAGE: m\nCODE:\n```\nlet\n    Source = AzureStorage.Blobs(\"https://gbadls01.blob.core.windows.net/public\"),\n    #\"Filtered Rows\" = Table.SelectRows(Source, each Text.StartsWith([Name], \"powerbi_delta/FactInternetSales_part.delta/\")),\n    DeltaTable = fn_ReadDeltaTable(#\"Filtered Rows\", [PartitionFilterFunction = (x) => Record.Field(x, \"SalesTerritoryKey\") >= 5])\nin\n    DeltaTable\n```\n\n----------------------------------------\n\nTITLE: Implementing Delta Table Checkpointing using Java\nDESCRIPTION: Code showing how to implement checkpointing functionality in Delta tables to optimize table state construction.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\nTransactionCommitResult commitResult = txn.commit(engine, dataActionsIterable);\n\nif (commitResult.isReadyForCheckpoint()) {\n  // Checkpoint the table\n  Table.forPath(engine, tablePath).checkpoint(engine, commitResult.getVersion());\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Data and Statistics Schema in JSON\nDESCRIPTION: Examples of JSON schema definitions for data and corresponding statistics in Delta Lake. Shows how statistics mirror the structure of the actual data schema.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_46\n\nLANGUAGE: json\nCODE:\n```\n|\n|-- a: struct\n|    |-- b: struct\n|    |    |-- c: long\n```\n\nLANGUAGE: json\nCODE:\n```\n|\n|-- stats: struct\n|    |-- numRecords: long\n|    |-- tightBounds: boolean\n|    |-- minValues: struct\n|    |    |-- a: struct\n|    |    |    |-- b: struct\n|    |    |    |    |-- c: long\n|    |-- maxValues: struct\n|    |    |-- a: struct\n|    |    |    |-- b: struct\n|    |    |    |    |-- c: long\n```\n\n----------------------------------------\n\nTITLE: Setting Hive Input Formats for Delta Tables in Hive CLI\nDESCRIPTION: SQL commands to set the Delta Hive InputFormat for both MapReduce and Tez execution engines in Hive CLI before reading Delta tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/hive/README.md#2025-04-22_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSET hive.input.format=io.delta.hive.HiveInputFormat;\nSET hive.tez.input.format=io.delta.hive.HiveInputFormat;\n```\n\n----------------------------------------\n\nTITLE: Dropping Multiple Columns from Delta Tables using SQL\nDESCRIPTION: SQL example showing how to drop multiple columns from a Delta table schema using ALTER TABLE with the DROP COLUMNS syntax. Requires column mapping to be enabled for metadata-only operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_37\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name DROP COLUMNS (col_name_1, col_name_2)\n```\n\n----------------------------------------\n\nTITLE: Creating Shallow Clones of Delta Tables\nDESCRIPTION: SQL commands for creating shallow clones of Delta tables with various options including version specification, timestamp-based cloning, and table property overrides. Available in Delta Lake 2.3 and above.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_17\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE delta.`/data/target/` SHALLOW CLONE delta.`/data/source/` -- Create a shallow clone of /data/source at /data/target\n\nCREATE OR REPLACE TABLE db.target_table SHALLOW CLONE db.source_table -- Replace the target. target needs to be emptied\n\nCREATE TABLE IF NOT EXISTS delta.`/data/target/` SHALLOW CLONE db.source_table -- No-op if the target table exists\n\nCREATE TABLE db.target_table SHALLOW CLONE delta.`/data/source`\n\nCREATE TABLE db.target_table SHALLOW CLONE delta.`/data/source` VERSION AS OF version\n\nCREATE TABLE db.target_table SHALLOW CLONE delta.`/data/source` TIMESTAMP AS OF timestamp_expression -- timestamp can be like \"2019-01-01\" or like date_sub(current_date(), 1)\n```\n\n----------------------------------------\n\nTITLE: Overwriting a Delta Table with Java\nDESCRIPTION: Java code to completely overwrite data in a Delta table. This replaces the existing data with new values 5-9 using the overwrite mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\nDataset<Row> data = spark.range(5, 10);\ndata.write().format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\");\n```\n\n----------------------------------------\n\nTITLE: Adding a CDC File to a Delta Lake Table\nDESCRIPTION: This JSON snippet demonstrates a 'cdc' action for adding a Change Data Capture file to a Delta Lake table. It includes the file path, partition information, and size.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"cdc\": {\n    \"path\": \"_change_data/cdc-00001-c…..snappy.parquet\",\n    \"partitionValues\": {},\n    \"size\": 1213,\n    \"dataChange\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Delta Table Schema and Partition Columns in Java\nDESCRIPTION: This code demonstrates how to define the schema and partition columns for a Delta table using the TransactionBuilder.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nStructType mySchema = new StructType()\n  .add(\"id\", IntegerType.INTEGER)\n  .add(\"name\", StringType.STRING)\n  .add(\"city\", StringType.STRING)\n  .add(\"salary\", DoubleType.DOUBLE);\n\n// Partition columns are optional. Use it only if you are creating a partitioned table.\nList<String> myPartitionColumns = Collections.singletonList(\"city\");\n\n// Set the schema of the new table on the transaction builder\ntxnBuilder = txnBuilder\n  .withSchema(engine, mySchema);\n\n// Set the partition columns of the new table only if you are creating\n// a partitioned table; otherwise, this step can be skipped.\ntxnBuilder = txnBuilder\n  .withPartitionColumns(engine, examplePartitionColumns);\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Lifecycle Rule for Delta Lake Temp Files (JSON)\nDESCRIPTION: JSON configuration for an S3 lifecycle rule to automatically expire temporary files created by Delta Lake after 30 days. This helps manage storage without manual intervention.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Rules\":[\n    {\n      \"ID\":\"expire_tmp_files\",\n      \"Filter\":{\n        \"Prefix\":\"path/to/table/_delta_log/.tmp/\"\n      },\n      \"Status\":\"Enabled\",\n      \"Expiration\":{\n        \"Days\":30\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing ExpressionHandler.createSelectionVector Method in Delta Kernel\nDESCRIPTION: Method for creating a ColumnVector of boolean type from a boolean array. Allows connectors to maintain ColumnVectors in the desired memory format.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_25\n\nLANGUAGE: Java\nCODE:\n```\ncreateSelectionVector(boolean[] values, int from, int to)\n```\n\n----------------------------------------\n\nTITLE: Configuring Delta Lake for PySpark shell\nDESCRIPTION: Command-line configuration for Delta Lake when launching PySpark shell. Specifies required extensions and catalog configurations via command-line parameters.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npyspark --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"  ...\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns in Delta Tables using SQL\nDESCRIPTION: SQL example showing how to rename columns in a Delta table schema using ALTER TABLE with the RENAME COLUMN syntax. Requires column mapping to be enabled for metadata-only operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_34\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name RENAME COLUMN old_col_name TO new_col_name\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-part Checkpointing in Delta Lake (SQL)\nDESCRIPTION: Sets the SQL configuration to enable multi-part checkpointing in Delta Lake. The value 'n' determines the limit of actions at which Delta Lake will start parallelizing the checkpoint.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nspark.databricks.delta.checkpoint.partSize=<n>\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Canonicalization Example for Delta Lake Checksum\nDESCRIPTION: An example demonstrating the JSON canonicalization process for Delta Lake checksum generation, showing the original JSON, its canonicalized form according to Delta's rules, and the resulting MD5 checksum.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_52\n\nLANGUAGE: text\nCODE:\n```\nJson: {\"k0\":\"'v 0'\", \"checksum\": \"adsaskfljadfkjadfkj\", \"k1\":{\"k2\": 2, \"k3\": [\"v3\", [1, 2], {\"k4\": \"v4\", \"k5\": [\"v5\", \"v6\", \"v7\"]}]}}\nCanonicalized form: \"k0\"=\"%27v%200%27\",\"k1\"+\"k2\"=2,\"k1\"+\"k3\"+0=\"v3\",\"k1\"+\"k3\"+1+0=1,\"k1\"+\"k3\"+1+1=2,\"k1\"+\"k3\"+2+\"k4\"=\"v4\",\"k1\"+\"k3\"+2+\"k5\"+0=\"v5\",\"k1\"+\"k3\"+2+\"k5\"+1=\"v6\",\"k1\"+\"k3\"+2+\"k5\"+2=\"v7\"\nChecksum is 6a92d155a59bf2eecbd4b4ec7fd1f875\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Lake Storage Gen1 for Delta Lake (Scala)\nDESCRIPTION: Scala code to set up Azure Data Lake Storage Gen1 credentials for use with Delta Lake. This configures OAuth 2.0 authentication for ADLS Gen1.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\")\nspark.conf.set(\"dfs.adls.oauth2.client.id\", \"<your-oauth2-client-id>\")\nspark.conf.set(\"dfs.adls.oauth2.credential\", \"<your-oauth2-credential>\")\nspark.conf.set(\"dfs.adls.oauth2.refresh.url\", \"https://login.microsoftonline.com/<your-directory-id>/oauth2/token\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Manifest Updates for Delta Tables\nDESCRIPTION: SQL command to configure a Delta table to automatically update manifests whenever write operations occur. This eliminates the need to manually regenerate manifests after data changes.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/redshift-spectrum-integration.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE delta.`<path-to-delta-table>` SET TBLPROPERTIES(delta.compatibility.symlinkFormatManifest.enabled=true)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table History in Scala\nDESCRIPTION: Scala examples for retrieving the history of operations on a Delta table, including methods to get the complete history or limit to recent operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, pathToTable)\n\nval fullHistoryDF = deltaTable.history()    // get the full history of the table\n\nval lastOperationDF = deltaTable.history(1) // get the last operation\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table History in SQL\nDESCRIPTION: SQL command to describe the detail of a Delta table. This can be used with either a file path or a table name.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nDESCRIBE DETAIL '/data/events/'\n\nDESCRIBE DETAIL eventsTable\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Log Actions\nDESCRIPTION: Converts data file status information into Delta log actions for table updates.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_17\n\nLANGUAGE: java\nCODE:\n```\nCloseableIterator<Row> dataActions =\n  Transaction.generateAppendActions(engine, txnState, dataFiles, writeContext);\n```\n\n----------------------------------------\n\nTITLE: Defining Map Column with Key Type Change in Delta Lake (JSON)\nDESCRIPTION: Example JSON schema definition for a map column after changing the type of its key from float to double. The type change is recorded in the column's metadata with a fieldPath value of \"key\" to indicate the change applies to the map key.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : {\n      \"type\": \"map\",\n      \"keyType\": \"double\",\n      \"valueType\": \"integer\",\n      \"valueContainsNull\": true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.typeChanges\": [\n        {\n          \"fromType\": \"float\",\n          \"toType\": \"double\",\n          \"fieldPath\": \"key\"\n        }\n      ]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Implementing ExpressionHandler.getEvaluator Method in Delta Kernel\nDESCRIPTION: Method for generating an ExpressionEvaluator that can evaluate expressions on columnar data. Takes batch schema, expression, and output data type as inputs.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_23\n\nLANGUAGE: Java\nCODE:\n```\ngetEvaluator(StructType batchSchema, Expression expresion, DataType outputType)\n```\n\n----------------------------------------\n\nTITLE: Enabling UniForm for Iceberg in SQL\nDESCRIPTION: SQL commands to enable UniForm support for Iceberg when creating a new Delta table. This includes setting the required table properties and enabling column mapping.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-uniform.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE T(c1 INT) USING DELTA TBLPROPERTIES(\n  'delta.enableIcebergCompatV2' = 'true',\n  'delta.universalFormat.enabledFormats' = 'iceberg');\n```\n\n----------------------------------------\n\nTITLE: Disabling Clustering with ALTER TABLE\nDESCRIPTION: SQL command to turn off clustering for a table by setting clustering columns to NONE. This prevents future OPTIMIZE operations from using clustering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-clustering.md#2025-04-22_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name CLUSTER BY NONE;\n```\n\n----------------------------------------\n\nTITLE: Merge Operation with Potential Conflict in Delta Lake (Scala)\nDESCRIPTION: Example of a merge operation on a partitioned Delta table that may cause concurrency conflicts. This snippet demonstrates a merge operation without explicit partition constraints, which can lead to ConcurrentAppendException when run concurrently.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/concurrency-control.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\n// Target 'deltaTable' is partitioned by date and country\ndeltaTable.as(\"t\").merge(\n    source.as(\"s\"),\n    \"s.user_id = t.user_id AND s.date = t.date AND s.country = t.country\")\n  .whenMatched().updateAll()\n  .whenNotMatched().insertAll()\n  .execute()\n```\n\n----------------------------------------\n\nTITLE: Defining In-Commit Timestamp Field in CommitInfo Action\nDESCRIPTION: This snippet shows the required field 'inCommitTimestamp' that must be included in the commitInfo action when In-Commit Timestamps are enabled. It specifies the data type and calculation method for the timestamp.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/in-commit-timestamps.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n3. The `commitInfo` action must include a field named `inCommitTimestamp`, of type `long` (see [Primitive Types](#primitive-types)), which represents the time (in milliseconds since the Unix epoch) when the commit is considered to have succeeded. It is the larger of two values:\n   - The time, in milliseconds since the Unix epoch, at which the writer attempted the commit\n   - One millisecond later than the previous commit's `inCommitTimestamp`\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Column Mapping Schema Definition\nDESCRIPTION: JSON schema definition for a Delta Lake table using column mapping, showing how logical column names are mapped to physical names through metadata fields.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_50\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"asset\",\n    \"type\" : \"string\",\n    \"nullable\" : true,\n    \"metadata\" : {\n      \"delta.columnMapping.id\": 1,\n      \"delta.columnMapping.physicalName\": \"col-b96921f0-2329-4cb3-8d79-184b2bdab23b\"\n    }\n  }, {\n    \"name\" : \"quantity\",\n    \"type\" : \"double\",\n    \"nullable\" : true,\n    \"metadata\" : {\n      \"delta.columnMapping.id\": 2,\n      \"delta.columnMapping.physicalName\": \"col-04ee4877-ee53-4cb9-b1fb-1a4eb74b508c\"\n    }\n  }, {\n    \"name\" : \"date\",\n    \"type\" : \"date\",\n    \"nullable\" : true,\n    \"metadata\" : {\n      \"delta.columnMapping.id\": 3,\n      \"delta.columnMapping.physicalName\": \"col-798f4abc-c63f-444c-9a04-e2cf1ecba115\"\n    }\n  }, {\n    \"name\" : \"region\",\n    \"type\" : \"string\",\n    \"nullable\" : true,\n    \"metadata\" : {\n      \"delta.columnMapping.id\": 4,\n      \"delta.columnMapping.physicalName\": \"col-19034dc3-8e3d-4156-82fc-8e05533c088e\"\n    }\n  } ]\n}\n```\n\n----------------------------------------\n\nTITLE: Dropping Columns in Delta Tables with Column Mapping\nDESCRIPTION: SQL commands to drop one or more columns in a Delta table with column mapping enabled. These operations can be performed without rewriting the underlying Parquet files.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-column-mapping.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name DROP COLUMN col_name\nALTER TABLE table_name DROP COLUMNS (col_name_1, col_name_2, ...)\n```\n\n----------------------------------------\n\nTITLE: Enabling Type Widening on Existing Table\nDESCRIPTION: SQL command to enable type widening on an existing Delta table by setting the delta.enableTypeWidening table property to true.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-type-widening.md#2025-04-22_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> SET TBLPROPERTIES ('delta.enableTypeWidening' = 'true')\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Table Manifests in Java\nDESCRIPTION: Java code to generate symlink format manifests for a Delta table using the DeltaTable API. This creates manifest files containing the names of data files to be read for querying the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/presto-integration.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nDeltaTable deltaTable = DeltaTable.forPath(<path-to-delta-table>);\ndeltaTable.generate(\"symlink_format_manifest\");\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Delta-Hudi Table\nDESCRIPTION: SQL command to insert a single record into the Delta table with Hudi format enabled.\nSOURCE: https://github.com/delta-io/delta/blob/master/hudi/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nINSERT INTO delta_table_with_hudi VALUES (1);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table Details in Java\nDESCRIPTION: Java code to retrieve detailed information about a Delta table using the DeltaTable class.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.tables.*;\n\nDeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable);\n\nDataFrame detailDF = deltaTable.detail();\n```\n\n----------------------------------------\n\nTITLE: Running Simple Test Workload with Delta Benchmark Framework\nDESCRIPTION: Shell command to execute a simple write-read test workload using the benchmark framework. It requires cluster hostname, PEM file, SSH user, benchmark path, and cloud provider parameters.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --cloud-provider <CLOUD_PROVIDER> \\\n    --benchmark test\n```\n\n----------------------------------------\n\nTITLE: Complex SQL Query for Variant Data Generation\nDESCRIPTION: SQL query that generates test data with various nested structures including JSON variants, arrays, structs, and maps. Creates a range of records with complex data types and nested combinations.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/kernel-defaults/src/test/resources/spark-variant-checkpoint/info.txt#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nwith jsonStrings as (\n    select\n      id,\n      format_string('{\"key\": %s}', id) as jsonString\n    from\n      range(0, 100)\n  )\n  select\n    id,\n    parse_json(jsonString) as v,\n    array(\n      parse_json(jsonString),\n      null,\n      parse_json(jsonString),\n      null,\n      parse_json(jsonString)\n    ) as array_of_variants,\n    named_struct('v', parse_json(jsonString)) as struct_of_variants,\n    map(\n      cast(id as string),\n      parse_json(jsonString),\n      'nullKey',\n      null\n    ) as map_of_variants,\n    array(\n      named_struct('v', parse_json(jsonString)),\n      named_struct('v', null),\n      null,\n      named_struct(\n        'v',\n        parse_json(jsonString)\n      ),\n      null,\n      named_struct(\n        'v',\n        parse_json(jsonString)\n      )\n    ) as array_of_struct_of_variants,\n    named_struct(\n      'v',\n      array(\n        null,\n        parse_json(jsonString)\n      )\n    ) as struct_of_array_of_variants\n  from\n    jsonStrings\n```\n\n----------------------------------------\n\nTITLE: Creating a Snapshot and Retrieving Table Metadata in Java\nDESCRIPTION: Demonstrates how to create a Snapshot object from the Table, which represents a consistent state of the table at a specific version. It also shows how to retrieve the version and schema of the snapshot.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nSnapshot mySnapshot = myTable.getLatestSnapshot(myEngine);\n\nlong version = mySnapshot.getVersion();\nStructType tableSchema = mySnapshot.getSchema();\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 DynamoDB LogStore for Multi-Cluster Setup\nDESCRIPTION: Sets the S3DynamoDBLogStore implementation for the S3 scheme to enable concurrent writes to S3 from multiple clusters using DynamoDB for mutual exclusion.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nconf.set(\"delta.logStore.s3.impl\", \"io.delta.storage.S3DynamoDBLogStore\");\n```\n\n----------------------------------------\n\nTITLE: Adding Delta Lake Dependency in Maven Project\nDESCRIPTION: Maven configuration for including Delta Lake in a Java/Scala project. This XML snippet should be added to the dependencies section of your pom.xml file.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-spark_2.12</artifactId>\n  <version>3.3.0</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Altering Column Default Value in Delta Lake SQL\nDESCRIPTION: Shows how to alter an existing column to set a default value using Delta Lake SQL.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_45\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE t ALTER COLUMN c SET DEFAULT 42;\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Shell with Delta and S3 Support\nDESCRIPTION: Command to start a Spark shell with Delta and S3 support, including necessary packages and S3 credentials.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbin/spark-shell \\\n --packages io.delta:delta-spark_2.12:3.3.0,org.apache.hadoop:hadoop-aws:3.3.4 \\\n --conf spark.hadoop.fs.s3a.access.key=<your-s3-access-key> \\\n --conf spark.hadoop.fs.s3a.secret.key=<your-s3-secret-key>\n```\n\n----------------------------------------\n\nTITLE: Disabling Type Widening\nDESCRIPTION: SQL command to disable type widening on a Delta table by setting the property to false. This prevents future type changes but doesn't remove existing changes.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-type-widening.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE <table_name> SET TBLPROPERTIES ('delta.enableTypeWidening' = 'false')\n```\n\n----------------------------------------\n\nTITLE: Setting Default Table Properties in SQL\nDESCRIPTION: Demonstrates how to set default table properties for new Delta tables created in a Spark session using SQL. This example sets the appendOnly property to true for all new Delta tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nSET spark.databricks.delta.properties.defaults.appendOnly = true\n```\n\n----------------------------------------\n\nTITLE: Defining Transaction Identifier in Delta Protocol (JSON)\nDESCRIPTION: This JSON snippet demonstrates the structure of a transaction identifier action in the Delta protocol. It includes an 'appId' for uniquely identifying the application and a 'version' for tracking progress.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"txn\": {\n    \"appId\":\"3ba13872-2d47-4e17-86a0-21afd2a22395\",\n    \"version\":364475\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Delta Table Schema Details\nDESCRIPTION: Shows the output schema for the DESCRIBE DETAIL command on a Delta table, including format, ID, name, location, timestamps, partition columns, file counts, size, properties, and version information.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n+------+--------------------+------------------+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n|format|                  id|              name|description|            location|           createdAt|       lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n+------+--------------------+------------------+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n| delta|d31f82d2-a69f-42e...|default.deltatable|       null|file:/Users/tuor/...|2020-06-05 12:20:...|2020-06-05 12:20:20|              []|      10|      12345|        []|               1|               2|\n+------+--------------------+------------------+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n```\n\n----------------------------------------\n\nTITLE: Setting Up Delta Kernel Maven Dependencies in Java Projects\nDESCRIPTION: Maven configuration for adding Delta Kernel dependencies to a connector project, showing both the required API dependency and the optional defaults implementation.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_18\n\nLANGUAGE: xml\nCODE:\n```\n<!-- Must have dependency -->\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-kernel-api</artifactId>\n  <version>${delta-kernel.version}</version>\n</dependency>\n\n<!-- Optional depdendency -->\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-kernel-defaults</artifactId>\n  <version>${delta-kernel.version}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Schema and Partition Columns from Delta Transaction in Java\nDESCRIPTION: This snippet shows how to retrieve the schema and partition column names from a Delta transaction object. This information is used to plan the query and generate data.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nStructType dataSchema = txn.getSchema(engine)\n\n// Optional for un-partitioned tables\nList<String> partitionColumnNames = txn.getPartitionColumns(engine)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data and Partition Values\nDESCRIPTION: Sets up data iterator and partition value mapping for data insertion. Shows how to structure data for both partitioned and unpartitioned tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_13\n\nLANGUAGE: java\nCODE:\n```\n// The data generated by the connector to write into a table\nCloseableIterator<FilteredColumnarBatch> data = ... \n\n// Create partition value map\nMap<String, Literal> partitionValues =\n  Collections.singletonMap(\n    \"city\", // partition column name\n     // partition value. Depending upon the partition column type, the\n     // partition value should be created. In this example, the partition\n     // column is of type StringType, so we are creating a string literal.\n     Literal.ofString(city)\n  );\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Google Cloud Storage in Scala\nDESCRIPTION: Demonstrates writing and reading Delta tables using Google Cloud Storage. Requires specifying the bucket name in the path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_22\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(5, 10).write.format(\"delta\").mode(\"append\").save(\"gs://<bucket-name>/<path-to-delta-table>\")\n\nspark.read.format(\"delta\").load(\"gs://<bucket-name>/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Setting up Delta Kernel Dependencies in Maven\nDESCRIPTION: XML configuration for adding the required and optional Delta Kernel dependencies to a Maven project. The delta-kernel-api is required for core functionality, while delta-kernel-defaults is optional and provides default implementations.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_23\n\nLANGUAGE: xml\nCODE:\n```\n<!-- Must have dependency -->\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-kernel-api</artifactId>\n  <version>${delta-kernel.version}</version>\n</dependency>\n\n<!-- Optional depdendency -->\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-kernel-defaults</artifactId>\n  <version>${delta-kernel.version}</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Listing Delta Log Entries in JSON Format\nDESCRIPTION: Examples of Delta log entry file names in JSON format, including both standard and managed commit file naming conventions.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/managed-commits.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  \"./_delta_log/00000000000000000000.json\",\n  \"./_delta_log/_commits/00000000000000000000.3a0d65cd-4056-49b8-937b-95f9e3ee90e5.json\",\n  \"./_delta_log/_commits/00000000000000000001.7d17ac10-5cc3-401b-bd1a-9c82dd2ea032.json\",\n  \"./_delta_log/_commits/00000000000000000001.016ae953-37a9-438e-8683-9a9a4a79a395.json\",\n  \"./_delta_log/_commits/00000000000000000002.3ae45b72-24e1-865a-a211-34987ae02f2a.json\"\n]\n```\n\n----------------------------------------\n\nTITLE: Enabling Fast S3A Listing for Delta Metadata\nDESCRIPTION: Spark shell command with configuration to enable fast S3A listing for Delta metadata files, which is an experimental performance optimization.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/spark-shell \\\n  --packages io.delta:delta-spark_2.12:3.3.0,org.apache.hadoop:hadoop-aws:3.3.4 \\\n  --conf spark.hadoop.fs.s3a.access.key=<your-s3-access-key> \\\n  --conf spark.hadoop.fs.s3a.secret.key=<your-s3-secret-key> \\\n  --conf \"spark.hadoop.delta.enableFastS3AListFrom=true\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table Components and Fetching Schema in Java\nDESCRIPTION: This snippet demonstrates how to initialize the Engine object, create a Table reference for a specific path, get the latest snapshot, and retrieve the schema. This is typically done during the logical plan analysis phase of query processing.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_33\n\nLANGUAGE: Java\nCODE:\n```\nimport io.delta.kernel.*;\nimport io.delta.kernel.defaults.engine.*;\n\nEngine myEngine = new MyEngine();\nTable myTable = Table.forPath(myTablePath);\nSnapshot mySnapshot = myTable.getLatestSnapshot(myEngine);\nStructType mySchema = mySnapshot.getSchema(myEngine);\n```\n\n----------------------------------------\n\nTITLE: Reading Hudi Table with Spark\nDESCRIPTION: Scala code to read the converted Hudi table using Spark DataFrame API. Enables Hudi metadata table for optimized reading.\nSOURCE: https://github.com/delta-io/delta/blob/master/hudi/README.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval df = spark.read.format(\"hudi\").option(\"hoodie.metadata.enable\", \"true\").load(\"/tmp/delta-table-with-hudi\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Delta Table Scans with Expressions in Java\nDESCRIPTION: This snippet demonstrates how to use Delta Kernel's expression framework to filter table scans and improve performance by skipping unnecessary files.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.expressions.*;\n\nimport io.delta.kernel.defaults.engine.*;\n\nEngine myEngine = DefaultEngine.create(new Configuration());\n\nPredicate filter = new Predicate(\n  \"=\",\n  Arrays.asList(new Column(\"columnX\"), Literal.ofInt(1)));\n\nScan myFilteredScan = mySnapshot.getScanBuilder().withFilter(filter).build()\n\n// Subset of the given filter that is not guaranteed to be satisfied by\n// Delta Kernel when it returns data. This filter is used by Delta Kernel\n// to do data skipping as much as possible. The connector should use this filter\n// on top of the data returned by Delta Kernel in order for further filtering.\nOptional<Predicate> remainingFilter = myFilteredScan.getRemainingFilter();\n```\n\n----------------------------------------\n\nTITLE: Enabling Row Tracking in Delta Lake Configuration\nDESCRIPTION: Shows how to enable Row Tracking by setting a specific configuration property in the table's metadata.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_28\n\nLANGUAGE: markdown\nCODE:\n```\nWriters can enable Row Tracking by setting `delta.enableRowTracking` to `true` in the `configuration` of the table's `metaData`.\n```\n\n----------------------------------------\n\nTITLE: Starting Transaction and Finding Relevant Files in Java\nDESCRIPTION: Demonstrates how to start a transaction, apply filters, and find relevant files in a Delta table using Delta Standalone APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.standalone.DeltaLog;\nimport io.delta.standalone.DeltaScan;\nimport io.delta.standalone.OptimisticTransaction;\nimport io.delta.standalone.actions.AddFile;\nimport io.delta.standalone.data.CloseableIterator;\nimport io.delta.standalone.expressions.And;\nimport io.delta.standalone.expressions.EqualTo;\nimport io.delta.standalone.expressions.Literal;\n\nDeltaLog log = DeltaLog.forTable(new Configuration(), \"/data/sales\");\nOptimisticTransaction txn = log.startTransaction();\n\nDeltaScan scan = txn.markFilesAsRead(\n    new And(\n        new And(\n            new EqualTo(schema.column(\"year\"), Literal.of(2021)),  // partition filter\n            new EqualTo(schema.column(\"month\"), Literal.of(11))),  // partition filter\n        new EqualTo(schema.column(\"customer\"), Literal.of(\"XYZ\"))  // non-partition filter\n    )\n);\n\nCloseableIterator<AddFile> iter = scan.getFiles();\nMap<String, AddFile> addFileMap = new HashMap<String, AddFile>();  // partition filtered files: year=2021, month=11\nwhile (iter.hasNext()) {\n    AddFile addFile = iter.next();\n    addFileMap.put(addFile.getPath(), addFile);\n}\niter.close();\n\nList<String> filteredFiles = ZappyReader.filterFiles( // fully filtered files: year=2021, month=11, customer=XYZ\n    addFileMap.keySet(),\n    toZappyExpression(scan.getResidualPredicate())\n);\n```\n\n----------------------------------------\n\nTITLE: Selection Vector Creation Method Signature\nDESCRIPTION: Method for creating a column vector from boolean values representing row selection status.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_30\n\nLANGUAGE: Java\nCODE:\n```\ncreateSelectionVector(boolean[] values, int from, int to)\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Delta Flink Connector\nDESCRIPTION: Maven POM configuration for including Delta Flink connector and its dependencies. Includes Scala 2.12 version specifications and optional dependencies for Table/SQL API and AWS S3 support.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_19\n\nLANGUAGE: xml\nCODE:\n```\n<project>\n    <properties>\n        <scala.main.version>2.12</scala.main.version>\n        <delta-connectors-version>3.0.0</delta-connectors-version>\n        <flink-version>1.16.1</flink-version>\n        <hadoop-version>3.1.0</hadoop-version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>io.delta</groupId>\n            <artifactId>delta-flink</artifactId>\n            <version>${delta-connectors-version}</version>\n        </dependency>\n        <dependency>\n            <groupId>io.delta</groupId>\n            <artifactId>delta-standalone_${scala.main.version}</artifactId>\n            <version>${delta-connectors-version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-clients_${scala.main.version}</artifactId>\n            <version>${flink-version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-parquet_${scala.main.version}</artifactId>\n            <version>${flink-version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-table-common</artifactId>\n            <version>${flink-version}</version>\n            <scope>provided</scope>\n        </dependency>\n\n      <!-- Needed for Flink Table/SQL API -->\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-client</artifactId>\n        <version>${hadoop-version}</version>\n      </dependency>\n      <dependency>\n          <groupId>org.apache.flink</groupId>\n          <artifactId>flink-table-runtime_${scala.main.version}</artifactId>\n          <version>${flink-version}</version>\n          <scope>provided</scope>\n      </dependency>\n      <!-- End needed for Flink Table/SQL API -->\n\n      <!-- Needed for AWS S3 support -->\n      <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-aws</artifactId>\n        <version>${hadoop-version}</version>\n      </dependency>\n      <!-- End needed for AWS S3 support -->\n  </dependencies>\n</project>\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table and Engine in Java\nDESCRIPTION: This code snippet shows how to initialize a Delta Table object and create a default Engine instance for interacting with Delta tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npackage io.delta.kernel.examples;\n\nimport io.delta.kernel.*;\nimport io.delta.kernel.types.*;\nimport io.delta.kernel.utils.CloseableIterable;\n\nString myTablePath = <my-table-path>; \nConfiguration hadoopConf = new Configuration();\nEngine myEngine = DefaultEngine.create(hadoopConf);\nTable myTable = Table.forPath(myEngine, myTablePath);\n```\n\n----------------------------------------\n\nTITLE: Serializing Delta Transaction State in Java\nDESCRIPTION: This snippet demonstrates how to serialize the transaction state for distribution to worker nodes in a distributed processing environment. It converts the transaction state to JSON format.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_40\n\nLANGUAGE: java\nCODE:\n```\nRow txnState = txn.getState(engine);\n\nString jsonTxnState = serializeToJson(txnState);\n```\n\n----------------------------------------\n\nTITLE: Sample Result Output from Delta Test Benchmark\nDESCRIPTION: Example of successful benchmark completion results, showing query execution times for various test operations like creating Delta tables, running SQL queries, and reading data back.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nRESULT:\n{\n  \"benchmarkSpecs\" : {\n    \"benchmarkPath\" : ...,\n    \"benchmarkId\" : \"20220126-191336-test\"\n  },\n  \"queryResults\" : [ {\n    \"name\" : \"sql-test\",\n    \"durationMs\" : 11075\n  }, {\n    \"name\" : \"db-list-test\",\n    \"durationMs\" : 208\n  }, {\n    \"name\" : \"db-create-test\",\n    \"durationMs\" : 4070\n  }, {\n    \"name\" : \"db-use-test\",\n    \"durationMs\" : 41\n  }, {\n    \"name\" : \"table-drop-test\",\n    \"durationMs\" : 74\n  }, {\n    \"name\" : \"table-create-test\",\n    \"durationMs\" : 33812\n  }, {\n    \"name\" : \"table-query-test\",\n    \"durationMs\" : 4795\n  } ]\n}\nFILE UPLOAD: Uploaded /home/hadoop/20220126-191336-test-report.json to s3:// ...\nSUCCESS\n```\n\n----------------------------------------\n\nTITLE: Implementing JsonHandler.parseJson Method in Delta Kernel\nDESCRIPTION: Method for parsing JSON strings from a ColumnVector into structured data according to the output schema. Handles missing columns and supports selective parsing via selection vector.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nparseJson(ColumnVector jsonStringVector, StructType outputSchema, java.util.Optional<ColumnVector> selectionVector)\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Credentials for Single-Cluster Setup\nDESCRIPTION: Sets S3 access and secret keys in the Hadoop configuration for authentication when using Delta Standalone with Amazon S3 in single-cluster mode.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nconf.set(\"fs.s3a.access.key\", \"<your-s3-access-key>\");\nconf.set(\"fs.s3a.secret.key\", \"<your-s3-secret-key>\");\n```\n\n----------------------------------------\n\nTITLE: Defining Snowflake External Table for Delta Parquet Files\nDESCRIPTION: This SQL command creates an external table in Snowflake that reads all the Parquet files in the Delta table. It defines columns based on the Parquet file structure and includes a column for the Parquet filename.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/snowflake-integration.md#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE EXTERNAL TABLE my_parquet_data_table(\n    id INT AS (VALUE:id::INT),\n    part INT AS (VALUE:part::INT),\n    ...\n    parquet_filename VARCHAR AS split_part(metadata$filename, '/', -1)\n  )\n  WITH LOCATION = @my_staged_table/\n  FILE_FORMAT = (TYPE = PARQUET)\n  PATTERN = '.*[/]part-[^/]*[.]parquet'\n  AUTO_REFRESH = true;\n```\n\n----------------------------------------\n\nTITLE: Defining Column Mapping in Delta Lake JSON Schema\nDESCRIPTION: This JSON snippet demonstrates how column mapping is defined in a Delta Lake table schema. It shows the structure for nested columns with physical names and column IDs assigned for mapping purposes.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : {\n      \"type\" : \"array\",\n      \"elementType\" : {\n        \"type\" : \"struct\",\n        \"fields\" : [ {\n          \"name\" : \"d\",\n          \"type\" : \"integer\",\n          \"nullable\" : false,\n          \"metadata\" : { \n            \"delta.columnMapping.id\": 5,\n            \"delta.columnMapping.physicalName\": \"col-a7f4159c-53be-4cb0-b81a-f7e5240cfc49\"\n          }\n        } ]\n      },\n      \"containsNull\" : true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.columnMapping.id\": 4,\n      \"delta.columnMapping.physicalName\": \"col-5f422f40-de70-45b2-88ab-1d5c90e94db1\"\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Defining Clustering Columns with Domain Metadata in Delta Lake\nDESCRIPTION: Example of a domainMetadata action definition for a clustered table that uses column mapping. It shows how clustering columns are specified in the table's metadata using physical column names.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"domainMetadata\": {\n    \"domain\": \"delta.clustering\",\n    \"configuration\": \"{\\\"clusteringColumns\\\":[\\\"col-daadafd7-7c20-4697-98f8-bff70199b1f9\\\", \\\"col-5abe0e80-cf57-47ac-9ffc-a861a3d1077e\\\"]}\",\n    \"removed\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table Data Using Scan Object in Java\nDESCRIPTION: This extensive code snippet demonstrates the process of reading all table data using the Scan object. It iterates through scan files, reads physical data from Parquet files, transforms it into logical data, and processes the data row by row and column by column.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nCloserableIterator<FilteredColumnarBatch> fileIter = scanObject.getScanFiles(myEngine);\n\nRow scanStateRow = scanObject.getScanState(myEngine);\n\nwhile(fileIter.hasNext()) {\n  FilteredColumnarBatch scanFileColumnarBatch = fileIter.next();\n\n  // Get the physical read schema of columns to read from the Parquet data files\n  StructType physicalReadSchema =\n    ScanStateRow.getPhysicalDataReadSchema(engine, scanStateRow);\n\n  try (CloseableIterator<Row> scanFileRows = scanFileColumnarBatch.getRows()) {\n    while (scanFileRows.hasNext()) {\n      Row scanFileRow = scanFileRows.next();\n\n      // From the scan file row, extract the file path, size and modification time metadata\n      // needed to read the file.\n      FileStatus fileStatus = InternalScanFileUtils.getAddFileStatus(scanFileRow);\n\n      // Open the scan file which is a Parquet file using connector's own\n      // Parquet reader or default Parquet reader provided by the Kernel (which\n      // is used in this example).\n      CloseableIterator<ColumnarBatch> physicalDataIter =\n        engine.getParquetHandler().readParquetFiles(\n          singletonCloseableIterator(fileStatus),\n          physicalReadSchema,\n          Optional.empty() /* optional predicate the connector can apply to filter data from the reader */\n        );\n\n      // Now the physical data read from the Parquet data file is converted to a table\n      // logical data. Logical data may include the addition of partition columns and/or\n      // subset of rows deleted\n      try (\n         CloseableIterator<FilteredColumnarBatch> transformedData =\n           Scan.transformPhysicalData(\n             engine,\n             scanStateRow,\n             scanFileRow,\n             physicalDataIter)) {\n        while (transformedData.hasNext()) {\n          FilteredColumnarBatch logicalData = transformedData.next();\n          ColumnarBatch dataBatch = logicalData.getData();\n\n          // Not all rows in `dataBatch` are in the selected output.\n          // An optional selection vector determines whether a row with a\n          // specific row index is in the final output or not.\n          Optional<ColumnVector> selectionVector = dataReadResult.getSelectionVector();\n\n          // access the data for the column at ordinal 0\n          ColumnVector column0 = dataBatch.getColumnVector(0);\n          for (int rowIndex = 0; rowIndex < column0.getSize(); rowIndex++) {\n            // check if the row is selected or not\n            if (!selectionVector.isPresent() || // there is no selection vector, all records are selected\n               (!selectionVector.get().isNullAt(rowId) && selectionVector.get().getBoolean(rowId)))  {\n              // Assuming the column type is String.\n              // If it is a different type, call the relevant function on the `ColumnVector`\n              System.out.println(column0.getString(rowIndex));\n            }\n          }\n\n\t  // access the data for column at ordinal 1\n\t  ColumnVector column1 = dataBatch.getColumnVector(1);\n\t  for (int rowIndex = 0; rowIndex < column1.getSize(); rowIndex++) {\n            // check if the row is selected or not\n            if (!selectionVector.isPresent() || // there is no selection vector, all records are selected\n               (!selectionVector.get().isNullAt(rowId) && selectionVector.get().getBoolean(rowId)))  {\n              // Assuming the column type is Long.\n              // If it is a different type, call the relevant function on the `ColumnVector`\n              System.out.println(column1.getLong(rowIndex));\n            }\n          }\n\t  // .. more ..\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON File Reading Method Signature\nDESCRIPTION: Method for reading JSON files into columnar batches with optional predicate filtering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\nreadJsonFiles(CloseableIterator<FileStatus> fileIter, StructType physicalSchema, java.util.Optional<Predicate> predicate)\n```\n\n----------------------------------------\n\nTITLE: Configuring SBT Dependencies for Delta Flink Connector\nDESCRIPTION: SBT configuration for including Delta Flink connector and its dependencies. Includes library dependencies for Delta, Flink, and Hadoop components.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"io.delta\" %% \"delta-flink\" % deltaConnectorsVersion,\n  \"io.delta\" %% \"delta-standalone\" % deltaConnectorsVersion,\n  \"org.apache.flink\" %% \"flink-clients\" % flinkVersion,\n  \"org.apache.flink\" %% \"flink-parquet\" % flinkVersion,\n  \"org.apache.hadoop\" % \"hadoop-client\" % hadoopVersion,\n  \"org.apache.flink\" % \"flink-table-common\" % flinkVersion % \"provided\",\n  \"org.apache.flink\" %% \"flink-table-runtime\" % flinkVersion % \"provided\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Snapshot and Retrieving Table Information in Java\nDESCRIPTION: This code shows how to create a Snapshot object from a Table, which represents a consistent state of the table at a specific version. It also demonstrates how to retrieve the version and schema of the snapshot.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nSnapshot mySnapshot = myTable.getLatestSnapshot(myEngine);\n\nlong version = mySnapshot.getVersion();\nStructType tableSchema = mySnapshot.getSchema();\n```\n\n----------------------------------------\n\nTITLE: Creating Bounded Delta Source with All Columns in Java\nDESCRIPTION: Creates a bounded Delta source that reads all columns from the latest table version. This approach is suitable for batch processing jobs.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createBoundedDeltaSourceAllColumns(\n        StreamExecutionEnvironment env,\n        String deltaTablePath) {\n\n    DeltaSource<RowData> deltaSource = DeltaSource\n        .forBoundedRowData(\n            new Path(deltaTablePath),\n            new Configuration())\n        .build();\n\n    return env.fromSource(deltaSource, WatermarkStrategy.noWatermarks(), \"delta-source\");\n}\n```\n\n----------------------------------------\n\nTITLE: Destroying Terraform-managed GCP Resources\nDESCRIPTION: Command to destroy all resources created by Terraform once benchmarks are completed. Note that buckets containing objects won't be deleted automatically without setting force_destroy to true.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/gcp/terraform/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nterraform destroy\n```\n\n----------------------------------------\n\nTITLE: Transforming Logical Data to Physical Data for Delta Table in Java\nDESCRIPTION: This code demonstrates how to transform logical data to physical data that can be written to Parquet files in a Delta table. It includes getting the transaction state and preparing partition values.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nRow txnState = txn.getTransactionState(engine);\n\n// The data generated by the connector to write into a table\nCloseableIterator<FilteredColumnarBatch> data = ... \n\n// Create partition value map\nMap<String, Literal> partitionValues =\n  Collections.singletonMap(\n    \"city\", // partition column name\n     // partition value. Depending upon the partition column type, the\n     // partition value should be created. In this example, the partition\n     // column is of type StringType, so we are creating a string literal.\n     Literal.ofString(city)\n  );\n\n// Transform the logical data to physical data that needs to be written to the Parquet\n// files\nCloseableIterator<FilteredColumnarBatch> physicalData =\n  Transaction.transformLogicalData(engine, txnState, data, partitionValues);\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Table from Azure Data Lake Store Gen1 in PowerQuery\nDESCRIPTION: Example showing how to read a Delta table from Azure Data Lake Store Gen1 with the required options UseFileBuffer and IterateFolderContent set to true.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_7\n\nLANGUAGE: m\nCODE:\n```\nlet\n    Source = DataLake.Contents(\"adl://myadlsgen1.azuredatalakestore.net/DeltaSamples/FactInternetSales_part.delta\", [PageSize=null]),\n    DeltaTable = fn_ReadDeltaTable(Source, [UseFileBuffer = true, IterateFolderContent = true])\nin\n    DeltaTable\n```\n\n----------------------------------------\n\nTITLE: Defining Row Tracking High Water Mark in Delta Lake\nDESCRIPTION: Specifies how to store the high water mark for Row Tracking using a domainMetadata action with a specific configuration format.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_27\n\nLANGUAGE: markdown\nCODE:\n```\nThe high water mark must be stored in a `domainMetadata` action with `delta.rowTracking` as the `domain`\nand a `configuration` containing a single key-value pair with `highWaterMark` as the key and the highest assigned fresh row id as the value.\n```\n\n----------------------------------------\n\nTITLE: Enabling Row Tracking by Default for All New Tables\nDESCRIPTION: Commands to set the configuration parameter to enable row tracking by default for all new tables in the current session, shown in SQL, Python, and Scala.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-row-tracking.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSET spark.databricks.delta.properties.defaults.enableRowTracking = true;\n```\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set(\"spark.databricks.delta.properties.defaults.enableRowTracking\", True)\n```\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\"spark.databricks.delta.properties.defaults.enableRowTracking\", true)\n```\n\n----------------------------------------\n\nTITLE: Reading Columnar Data from Parquet Files in Delta Lake\nDESCRIPTION: Code demonstrating how to read physical data from Parquet files and convert it to logical data using Delta Lake Kernel APIs. This includes extracting file information, reading data with optional splits and predicates, and transforming it to match the table's logical representation.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_32\n\nLANGUAGE: java\nCODE:\n```\nRow scanStateRow = ... ;\nRow scanFileRow = ... ;\nSplit split = ...;\n\n// Additional option predicate such as dynamic filters the connector wants to\n// pass to the reader when reading files.\nPredicate optPredicate = ...;\n\n// Get the physical read schema of columns to read from the Parquet data files\nStructType physicalReadSchema =\n  ScanStateRow.getPhysicalDataReadSchema(engine, scanStateRow);\n\n// From the scan file row, extract the file path, size and modification metadata\n// needed to read the file.\nFileStatus fileStatus = InternalScanFileUtils.getAddFileStatus(scanFileRow);\n\n// Open the scan file which is a Parquet file using connector's own\n// Parquet reader which supports reading specific parts (split) of the file.\n// If the connector doesn't have its own Parquet reader, it can use the\n// default Parquet reader provider which at the moment doesn't support reading\n// a specific part of the file, but reads the entire file from the beginning.\nCloseableIterator<ColumnarBatch> physicalDataIter =\n  connectParquetReader.readParquetFile(\n    fileStatus\n    physicalReadSchema,\n    split, // what part of the Parquet file to read data from\n    optPredicate /* additional predicate the connector can apply to filter data from the reader */\n  );\n\n// Now the physical data read from the Parquet data file is converted to logical data\n// the table represents.\n// Logical data may include the addition of partition columns and/or\n// subset of rows deleted\nCloseableIterator<FilteredColumnarBatch> transformedData =\n  Scan.transformPhysicalData(\n    engine,\n    scanState,\n    scanFileRow,\n    physicalDataIter));\n```\n\n----------------------------------------\n\nTITLE: Installing Delta Lake Python Package\nDESCRIPTION: Command to install the Delta Lake package using pip package manager. This installs the delta-spark package which contains Python APIs for Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/python/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install delta-spark\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table with Hudi UniForm Support\nDESCRIPTION: SQL command to create a Delta table with Hudi UniForm format enabled. Specifies table properties for universal format and storage location.\nSOURCE: https://github.com/delta-io/delta/blob/master/hudi/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE `delta_table_with_hudi` (col1 INT) USING DELTA TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'hudi') LOCATION '/tmp/delta-table-with-hudi';\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkSession for Delta Lake (Java)\nDESCRIPTION: This Java code snippet shows how to configure a SparkSession for Delta Lake operations. It sets the necessary configurations for enabling Delta Lake integration with Apache Spark DataSourceV2 and Catalog APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_43\n\nLANGUAGE: java\nCODE:\n```\nimport org.apache.spark.sql.SparkSession;\n\nSparkSession spark = SparkSession\n  .builder()\n  .appName(\"...\")\n  .master(\"...\")\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate();\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table Based on Another Table in Flink SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a new Delta table with the same schema, partitioning, and table properties as an existing table using the CREATE TABLE LIKE statement.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE testTable (\n    id BIGINT,\n    data STRING\n  ) WITH (\n    'connector' = 'delta',\n    'table-path' = '<path-to-table>'\n);\n\nCREATE TABLE likeTestTable\n  WITH (\n    'connector' = 'delta',\n    'table-path' = '%s'\n) LIKE testTable;\n```\n\n----------------------------------------\n\nTITLE: Documenting Reader Requirements in Markdown\nDESCRIPTION: Specifies the requirements for readers when working with tables that have checkpointProtection enabled.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/checkpoint-protection.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## Recommendations for Readers of Tables with Checkpoint Protection feature\n\nFor tables with `checkpointProtection` supported in the protocol, readers do not need to understand or change anything new; they just need to acknowledge the feature exists.\n```\n\n----------------------------------------\n\nTITLE: Loading 1GB TPC-DS Data as Delta Tables on GCP\nDESCRIPTION: Shell command for loading 1GB TPC-DS data into Delta tables specifically on Google Cloud Platform. It requires an additional source-path parameter to locate the raw Parquet input files.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --source-path <SOURCE_PATH> \\\n    --cloud-provider gcp \\\n    --benchmark tpcds-1gb-delta-load\n```\n\n----------------------------------------\n\nTITLE: Building Delta Table Scan with Filters in Java\nDESCRIPTION: Demonstrates how to build a scan object with read schema and filter expressions. Used for resolving files to scan based on query parameters.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_29\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.expressions.*;\nimport io.delta.kernel.types.*;\n\nStructType readSchema = ... ;  // convert engine schema\nPredicate filterExpr = ... ;   // convert engine filter expression\n\nScan myScan = mySnapshot.getScanBuilder().withFilter(filterExpr).withReadSchema(readSchema).build();\n```\n\n----------------------------------------\n\nTITLE: Running Queries on 3TB TPC-DS Delta Tables\nDESCRIPTION: Shell command to run the TPC-DS query suite against the previously loaded 3TB Delta tables. This benchmark evaluates Delta Lake performance on large-scale datasets.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --cloud-provider <CLOUD_PROVIDER> \\\n    --benchmark tpcds-3tb-delta\n```\n\n----------------------------------------\n\nTITLE: Applying S3 Lifecycle Configuration for Delta Lake (Bash)\nDESCRIPTION: AWS CLI command to apply an S3 lifecycle configuration for managing Delta Lake temporary files. This automates the cleanup process for improved storage management.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket my-bucket \\\n  --lifecycle-configuration file://lifecycle.json\n```\n\n----------------------------------------\n\nTITLE: Defining CHECK Constraints in Delta Lake\nDESCRIPTION: Shows how to define a CHECK constraint in Delta Lake. The example constraint 'birthDateCheck' ensures that the 'birthDate' column value is greater than '1900-01-01'.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_43\n\nLANGUAGE: sql\nCODE:\n```\ndelta.constraints.birthDateCheck, birthDate > '1900-01-01'\n```\n\n----------------------------------------\n\nTITLE: Reading Columnar Data from Parquet Files in Delta IO\nDESCRIPTION: Illustrates the process of reading columnar data from Parquet files using Delta IO. This includes extracting physical read schema, file metadata, and transforming physical data into logical data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_37\n\nLANGUAGE: java\nCODE:\n```\nRow scanStateRow = ... ;\nRow scanFileRow = ... ;\nSplit split = ...;\n\n// Additional option predicate such as dynamic filters the connector wants to\n// pass to the reader when reading files.\nPredicate optPredicate = ...;\n\n// Get the physical read schema of columns to read from the Parquet data files\nStructType physicalReadSchema =\n  ScanStateRow.getPhysicalDataReadSchema(engine, scanStateRow);\n\n// From the scan file row, extract the file path, size and modification metadata\n// needed to read the file.\nFileStatus fileStatus = InternalScanFileUtils.getAddFileStatus(scanFileRow);\n\n// Open the scan file which is a Parquet file using connector's own\n// Parquet reader which supports reading specific parts (split) of the file.\n// If the connector doesn't have its own Parquet reader, it can use the\n// default Parquet reader provider which at the moment doesn't support reading\n// a specific part of the file, but reads the entire file from the beginning.\nCloseableIterator<ColumnarBatch> physicalDataIter =\n  connectParquetReader.readParquetFile(\n    fileStatus\n    physicalReadSchema,\n    split, // what part of the Parquet file to read data from\n    optPredicate /* additional predicate the connector can apply to filter data from the reader */\n  );\n\n// Now the physical data read from the Parquet data file is converted to logical data\n// the table represents.\n// Logical data may include the addition of partition columns and/or\n// subset of rows deleted\nCloseableIterator<FilteredColumnarBatch> transformedData =\n  Scan.transformPhysicalData(\n    engine,\n    scanState,\n    scanFileRow,\n    physicalDataIter));\n```\n\n----------------------------------------\n\nTITLE: Example JSON Schema for Column with Multiple Type Changes\nDESCRIPTION: Demonstrates the schema definition for a column that has undergone two type widening operations, from short to integer and then to long. The delta.typeChanges metadata array tracks the history of these changes.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/type-widening.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : \"long\",\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.typeChanges\": [\n        {\n          \"fromType\": \"short\",\n          \"toType\": \"integer\"\n        },\n        {\n          \"fromType\": \"integer\",\n          \"toType\": \"long\"\n        }\n      ]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Creating Snowflake View for Consistent Delta Table Snapshot\nDESCRIPTION: This SQL command creates a view in Snowflake that filters the Parquet data table to include only the rows from files listed in the manifest table, providing a consistent snapshot of the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/snowflake-integration.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE VIEW my_delta_table AS\n    SELECT id, part, ...\n    FROM my_parquet_data_table\n    WHERE parquet_filename IN (SELECT filename FROM delta-manifest-table);\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Compaction Read Support in Delta Lake (SQL)\nDESCRIPTION: Sets the SQL configuration to enable or disable read support for log compaction files in Delta Lake. The value can be 'true' or 'false'.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nspark.databricks.delta.deltaLog.minorCompaction.useForReads=<value>\n```\n\n----------------------------------------\n\nTITLE: Altering Delta Table Properties in Flink SQL\nDESCRIPTION: This SQL snippet shows how to alter a Delta table's properties and rename the table using ALTER TABLE statements.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE sourceTable SET ('userCustomProp'='myVal1')\nALTER TABLE sourceTable RENAME TO newSourceTable\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Extensions in conf.py\nDESCRIPTION: Shows how to add the metadata extension to the Sphinx configuration file. This is required to enable the extension for processing substitution references in document metadata.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/metadata/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nextensions = ['metadata']\n```\n\n----------------------------------------\n\nTITLE: Setting up Delta Kernel dependencies in Maven POM file\nDESCRIPTION: This XML snippet shows how to include Delta Kernel API and defaults dependencies in a Maven project. It specifies the required artifact IDs and versions for Delta Kernel integration.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<dependencies>\n  <dependency>\n    <groupId>io.delta</groupId>\n    <artifactId>delta-kernel-api</artifactId>\n    <version>${delta-kernel.version}</version>\n  </dependency>\n\n  <dependency>\n    <groupId>io.delta</groupId>\n    <artifactId>delta-kernel-defaults</artifactId>\n    <version>${delta-kernel.version}</version>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom LogStore Implementation for Storage Systems\nDESCRIPTION: Sets the LogStore implementation class for a specific storage scheme in the Hadoop configuration to enable transactional guarantees on storage systems without built-in support.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_3\n\nLANGUAGE: java\nCODE:\n```\ndelta.logStore.<scheme>.impl=<full-qualified-class-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Oracle Cloud Infrastructure LogStore for Delta Lake in INI\nDESCRIPTION: Sets up the LogStore implementation for Oracle Cloud Infrastructure. This configuration is required when using Delta Lake with OCI.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_23\n\nLANGUAGE: ini\nCODE:\n```\nspark.delta.logStore.oci.impl=io.delta.storage.OracleCloudLogStore\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Input Formats for Delta Tables in XML\nDESCRIPTION: XML configuration to be added to hive-site.xml file to set up the Delta Hive InputFormat for both MapReduce and Tez execution engines.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/hive/README.md#2025-04-22_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.input.format</name>\n  <value>io.delta.hive.HiveInputFormat</value>\n</property>\n<property>\n  <name>hive.tez.input.format</name>\n  <value>io.delta.hive.HiveInputFormat</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Parallelized Data Import with Optimizations (Shell)\nDESCRIPTION: Shows how to control the degree of import parallelism using the 'chunks' parameter and Spark executor configuration. It also enables Delta's auto compaction and optimization features.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/sql-delta-import/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nspark-submit --num-executors 15 --executor-cores 4 \\\n--conf spark.databricks.delta.optimizeWrite.enabled=true \\\n--conf spark.databricks.delta.autoCompact.enabled=true \\\n--class \"io.delta.connectors.spark.jdbc.ImportRunner\" sql-delta-import.jar \\\n--jdbc-url jdbc:mysql://hostName:port/database \\\n--source source.table\n--destination destination.table\n--split-by id\n--chunks 500\n```\n\n----------------------------------------\n\nTITLE: Collations Domain Metadata Example\nDESCRIPTION: Shows the structure of domain metadata for collations, including read and write version specifications for different collation providers.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/collated-string-type.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"readVersions\": {\n    \"ICU.en_US\": \"73\"\n  },\n  \"writeVersions\": {\n    \"ICU.en_US\": [\"72\", \"73\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Manifest Updates for Delta Table\nDESCRIPTION: This SQL command sets a table property on the Delta table to enable automatic updates of the manifest files whenever write operations occur on the table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/snowflake-integration.md#2025-04-22_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE delta.`<path-to-delta-table>` SET TBLPROPERTIES(delta.compatibility.symlinkFormatManifest.enabled=true)\n```\n\n----------------------------------------\n\nTITLE: Initializing Delta Table and Engine in Java\nDESCRIPTION: Creates a Table object representing a Delta table and initializes the default Engine. This is the first step in accessing a Delta table using the Delta Kernel API.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.*;\nimport io.delta.kernel.defaults.*;\nimport org.apache.hadoop.conf.Configuration;\n\nString myTablePath = <my-table-path>; // fully qualified table path. Ex: file:/user/tables/myTable\nConfiguration hadoopConf = new Configuration();\nEngine myEngine = DefaultEngine.create(hadoopConf);\nTable myTable = Table.forPath(myEngine, myTablePath);\n```\n\n----------------------------------------\n\nTITLE: Setting Partition Columns in Delta Transaction\nDESCRIPTION: Code to determine and set partition columns for the Delta table transaction, handling both new and existing table scenarios.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_34\n\nLANGUAGE: java\nCODE:\n```\nTransactionBuilder txnBuilder = ... from the last step ...\nTransaction txn;\n\nList<String> partitionColumns = ...\nif (newTable) {\n  partitionColumns = ... derive from the query parameters (ex. PARTITION BY clause in SQL) ...\n  txnBuilder = txnBuilder.withPartitionColumns(engine, partitionColumns);\n  txn = txnBuilder.build(engine);\n} else {\n  txn = txnBuilder.build(engine);\n  partitionColumns = txn.getPartitionColumns(engine);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Metadata Extension in ReStructuredText\nDESCRIPTION: Demonstrates how to use substitution references in document metadata when writing documentation in ReStructuredText format. The |DEFINITION| placeholder will be replaced with its defined value.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/metadata/README.md#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. meta::\n    :title: Title text with |DEFINITION| included\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Column Metadata Table Structure\nDESCRIPTION: Markdown table defining the column metadata fields and their purposes in Delta Lake, including mappings, identity columns, invariants, generation expressions, and type changes.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/type-widening.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nField Name | Description\n-|-\ndelta.columnMapping.*| These keys are used to store information about the mapping between the logical column name to  the physical name. See [Column Mapping](#column-mapping) for details.\ndelta.identity.*| These keys are for defining identity columns. See [Identity Columns](#identity-columns) for details.\ndelta.invariants| JSON string contains SQL expression information. See [Column Invariants](#column-invariants) for details.\ndelta.generationExpression| SQL expression string. See [Generated Columns](#generated-columns) for details.\ndelta.typeChanges| JSON string containing information about previous type changes applied to this column. See [Type Change Metadata](#type-change-metadata) for details.\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing Row Objects in Java for Delta IO\nDESCRIPTION: Demonstrates how to serialize and deserialize Row objects for Delta IO operations. This code shows the process in both the driver and worker contexts, including handling of scan state and scan file data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_36\n\nLANGUAGE: java\nCODE:\n```\nimport io.delta.kernel.utils.*;\n\n// In the driver where query planning is being done\nByte[] scanStateRowBytes = RowUtils.serialize(scanStateRow);\nByte[] scanFileRowBytes = RowUtils.serialize(scanFileRow);\n\n// Optionally the connector adds a split info to the task (scan file, scan state) to\n// split reading of a Parquet file into multiple tasks. The task gets split info\n// along with the scan file row and scan state row.\nSplit split = ...; // connector specific class, not related to Kernel\n\n// Send these over to the worker\n\n// In the worker when data will be read, after rowBytes have been sent over\nRow scanStateRow = RowUtils.deserialize(scanStateRowBytes);\nRow scanFileRow = RowUtils.deserialize(scanFileRowBytes);\nSplit split = ... deserialize split info ...;\n```\n\n----------------------------------------\n\nTITLE: Connecting to Azure Blob Storage for Delta Table Content in PowerQuery\nDESCRIPTION: PowerQuery code to connect to an Azure Blob Storage account and filter for a specific Delta table path. This creates the content required as input for the Delta table reading function.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_0\n\nLANGUAGE: m\nCODE:\n```\nlet\n    Source = AzureStorage.Blobs(\"https://gbadls01.blob.core.windows.net/public\"),\n    #\"Filtered Rows\" = Table.SelectRows(Source, each Text.StartsWith([Name], \"powerbi_delta/FactInternetSales_part.delta/\"))\nin\n    #\"Filtered Rows\"\n```\n\n----------------------------------------\n\nTITLE: Adding Partitions to Redshift External Table for Delta Manifests\nDESCRIPTION: SQL command to add partition information to a Redshift external table that reads Delta manifest files. This step is required for partitioned Delta tables since the manifest itself is also partitioned.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/redshift-spectrum-integration.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE mytable.redshiftdeltatable ADD IF NOT EXISTS PARTITION (col_name=col_value) LOCATION '<path-to-delta-table>/_symlink_format_manifest/col_name=col_value'\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Lake Storage Gen2 Credentials for Delta Lake in Scala\nDESCRIPTION: Sets up Azure Data Lake Storage Gen2 credentials for use with Delta Lake. Requires storage account name, application ID, password, and directory ID from a service principal.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account-name>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account-name>.dfs.core.windows.net\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account-name>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account-name>.dfs.core.windows.net\",\"<password>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account-name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\n----------------------------------------\n\nTITLE: Launching Spark Shell with Delta and S3 Support\nDESCRIPTION: Command to start Spark shell with Delta Lake, S3, and DynamoDB dependencies for multi-cluster setup.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbin/spark-shell \\\n --packages io.delta:delta-spark_2.12:3,org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-storage-s3-dynamodb:3.3.0 \\\n --conf spark.hadoop.fs.s3a.access.key=<your-s3-access-key> \\\n --conf spark.hadoop.fs.s3a.secret.key=<your-s3-secret-key> \\\n --conf spark.delta.logStore.s3a.impl=io.delta.storage.S3DynamoDBLogStore \\\n --conf spark.io.delta.storage.S3DynamoDBLogStore.ddb.region=us-west-2\n```\n\n----------------------------------------\n\nTITLE: Updating DynamoDB TTL Configuration\nDESCRIPTION: AWS CLI commands to update DynamoDB TTL configuration when migrating from Delta 1.2.1/2.0.0/2.1.0 to 2.0.1/2.1.1 or above, changing from commitTime to expireTime attribute.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/porting.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naws dynamodb update-time-to-live \\\n  --region <region> \\\n  --table-name <table-name> \\\n  --time-to-live-specification \"Enabled=false, AttributeName=commitTime\"\n\naws dynamodb update-time-to-live \\\n  --region <region> \\\n  --table-name <table-name> \\\n  --time-to-live-specification \"Enabled=true, AttributeName=expireTime\"\n```\n\n----------------------------------------\n\nTITLE: Defining Snowflake External Table for Delta Manifests\nDESCRIPTION: This SQL command creates an external table in Snowflake that reads the file names specified in the manifest files generated for the Delta table. It extracts the filename from the full path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/snowflake-integration.md#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE EXTERNAL TABLE delta_manifest_table(\n    filename VARCHAR AS split_part(VALUE:c1, '/', -1)\n  )\n  WITH LOCATION = @my_staged_table/_symlink_format_manifest/\n  FILE_FORMAT = (TYPE = CSV)\n  PATTERN = '.*[/]manifest'\n  AUTO_REFRESH = true;\n```\n\n----------------------------------------\n\nTITLE: Simplified Delta Lake Partition Filter Function in PowerQuery\nDESCRIPTION: A more concise version of the partition filter function without explicit type definitions, which performs the same filtering for Year=2021 and Month=\"Jan\".\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_2\n\nLANGUAGE: m\nCODE:\n```\n(x) => Record.Field(x, \"Year\") = 2021 and Record.Field(x, \"Month\") = \"Jan\"\n```\n\n----------------------------------------\n\nTITLE: Processing Delta Table Scan Results in Java\nDESCRIPTION: Shows how to retrieve and process scan state and file information from a scan object. Used for accessing individual file data during query execution.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_30\n\nLANGUAGE: java\nCODE:\n```\nRow myScanStateRow = myScan.getScanState();\nCloseableIterator<FilteredColumnarBatch> myScanFilesAsBatches = myScan.getScanFiles();\n\nwhile (myScanFilesAsBatches.hasNext()) {\n  FilteredColumnarBatch scanFileBatch = myScanFilesAsBatches.next();\n\n  CloseableIterator<Row> myScanFilesAsRows = scanFileBatch.getRows();\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Schema for Map with Key Type Change\nDESCRIPTION: Demonstrates the schema definition for a map column where the key type has been widened from float to double. The fieldPath property is used to specify that the change applies to the map key.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/type-widening.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : {\n      \"type\": \"map\",\n      \"keyType\": \"double\",\n      \"valueType\": \"integer\",\n      \"valueContainsNull\": true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.typeChanges\": [\n        {\n          \"fromType\": \"float\",\n          \"toType\": \"double\",\n          \"fieldPath\": \"key\"\n        }\n      ]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Determining Partition Columns for Delta Table in Java\nDESCRIPTION: This code snippet shows how to determine the partition columns for a Delta table, handling both new and existing table scenarios. It builds on the TransactionBuilder from the previous step.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_39\n\nLANGUAGE: java\nCODE:\n```\nTransactionBuilder txnBuilder = ... from the last step ...\nTransaction txn;\n\nList<String> partitionColumns = ...\nif (newTable) {\n  partitionColumns = ... derive from the query parameters (ex. PARTITION BY clause in SQL) ...\n  txnBuilder = txnBuilder.withPartitionColumns(engine, partitionColumns);\n  txn = txnBuilder.build(engine);\n} else {\n  txn = txnBuilder.build(engine);\n  partitionColumns = txn.getPartitionColumns(engine);\n}\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Partition Filter Function Example in PowerQuery\nDESCRIPTION: Example of a partition filter function that can be used with the Delta Lake reader to filter data based on partition values. This function filters for records where Year=2021 and Month=\"Jan\".\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_1\n\nLANGUAGE: m\nCODE:\n```\n(PartitionValues as record) as logical =>\n    Record.Field(PartitionValues, \"Year\") = 2021 and Record.Field(PartitionValues, \"Month\") = \"Jan\"\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Table Manifests in Python\nDESCRIPTION: Python code to generate symlink format manifests for a Delta table using the DeltaTable API. This creates manifest files containing the names of data files to be read for querying the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/presto-integration.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndeltaTable = DeltaTable.forPath(<path-to-delta-table>)\ndeltaTable.generate(\"symlink_format_manifest\")\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake with Oracle Cloud Infrastructure in Scala\nDESCRIPTION: Demonstrates writing and reading Delta tables using Oracle Cloud Infrastructure. Requires specifying the OCI bucket and namespace in the path.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_24\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(5).write.format(\"delta\").save(\"oci://<ociBucket>@<ociNameSpace>/<path-to-delta-table>\")\n\nspark.read.format(\"delta\").load(\"oci://<ociBucket>@<ociNameSpace>/<path-to-delta-table>\").show()\n```\n\n----------------------------------------\n\nTITLE: Compiling Delta Lake Project using SBT\nDESCRIPTION: SBT command to clean and compile the Delta Lake project. This is a troubleshooting step to ensure all generated sources are up to date.\nSOURCE: https://github.com/delta-io/delta/blob/master/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbuild/sbt clean compile\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Delta Standalone (Scala 2.11)\nDESCRIPTION: Maven configuration snippet showing how to add Delta Standalone and Hadoop client dependencies for Scala 2.11 projects.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-standalone_2.11</artifactId>\n  <version>0.5.0</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>3.1.0</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Delta Protocol with Writer Features (JSON)\nDESCRIPTION: This JSON snippet demonstrates how to specify a Delta protocol version with writer features. It includes reader and writer versions, along with specific writer features like column mapping and identity columns.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"protocol\":{\n    \"readerVersion\":2,\n    \"writerVersion\":7,\n    \"writerFeatures\":[\"columnMapping\",\"identityColumns\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Getting Write Context for Data Files\nDESCRIPTION: Obtains the write context that specifies target directory and statistics columns for data file writing.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_15\n\nLANGUAGE: java\nCODE:\n```\n// Get the write context\nDataWriteContext writeContext = Transaction.getWriteContext(engine, txnState, partitionValues);\n```\n\n----------------------------------------\n\nTITLE: Defining Table Name Variable\nDESCRIPTION: Declares a variable to store the target Delta table name or path.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/kernel-defaults/src/test/resources/spark-variant-checkpoint/info.txt#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nval tableName = \"<REPLACE WITH THE TABLE NAME OR PATH>\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning up S3 Temp Files for Delta Lake (Bash)\nDESCRIPTION: AWS CLI command to manually delete all but the latest temporary file in an S3 bucket used by Delta Lake. This helps manage storage and cleanup old artifacts.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\naws s3 ls s3://<bucket>/<delta_table_path>/_delta_log/.tmp/ --recursive | awk 'NF>1{print $4}' | grep . | sort | head -n -1  | while read -r line ; do\n    echo \"Removing ${line}\"\n    aws s3 rm s3://<bucket>/<delta_table_path>/_delta_log/.tmp/${line}\ndone\n```\n\n----------------------------------------\n\nTITLE: Disabling Row Tracking in SQL\nDESCRIPTION: SQL command to disable row tracking on a Delta table. This reduces storage overhead but does not remove the table feature or downgrade the protocol version.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-row-tracking.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE table_name SET TBLPROPERTIES (delta.enableRowTracking = false);\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Stats Filter Function Example in PowerQuery\nDESCRIPTION: Example of a stats filter function that filters files based on min/max values in the delta log before reading. This filters for ProductKey ≤ 220 and OrderDateKey ≥ 20080731.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_4\n\nLANGUAGE: m\nCODE:\n```\n= (\n    minValues as record,\n    maxValues as record\n) as logical =>\n\nRecord.Field(minValues, \"ProductKey\") <= 220 and Record.Field(maxValues, \"OrderDateKey\") >= 20080731\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Lake Storage Gen1 in Java\nDESCRIPTION: Sets up Azure Data Lake Storage Gen1 credentials by configuring the Hadoop Configuration object with OAuth2 settings.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nconf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\");\nconf.set(\"dfs.adls.oauth2.client.id\", \"<your-oauth2-client-id>\");\nconf.set(\"dfs.adls.oauth2.credential\", \"<your-oauth2-credential>\");\nconf.set(\"dfs.adls.oauth2.refresh.url\", \"https://login.microsoftonline.com/<your-directory-id>/oauth2/token\");\n```\n\n----------------------------------------\n\nTITLE: Example Delta Log Commit File 5\nDESCRIPTION: Sample content of commit file 00000000000000000005.json showing commit info, add actions and transaction identifier\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"commitInfo\":{...}}\n{\"add\":{\"path\":\"f3\",...}}\n{\"add\":{\"path\":\"f4\",...}}\n{\"txn\":{\"appId\":\"3ae45b72-24e1-865a-a211-34987ae02f2a\",\"version\":4389}}\n```\n\n----------------------------------------\n\nTITLE: Upgrading Delta Lake Protocol Version using Scala\nDESCRIPTION: This Scala code shows how to use the DeltaTable.upgradeTableProtocol method to upgrade the reader version to 1 and the writer version to 3 for a Delta Lake table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/versioning.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables.DeltaTable\nval delta = DeltaTable.forPath(spark, \"path_to_table\") // or DeltaTable.forName\ndelta.upgradeTableProtocol(1, 3) // Upgrades to readerVersion=1, writerVersion=3.\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Data Lake Storage Gen2 in Java\nDESCRIPTION: Sets up Azure Data Lake Storage Gen2 credentials by configuring the Hadoop Configuration object with OAuth settings for a specific storage account.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nconf.set(\"fs.azure.account.auth.type.<storage-account-name>.dfs.core.windows.net\", \"OAuth\");\nconf.set(\"fs.azure.account.oauth.provider.type.<storage-account-name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\");\nconf.set(\"fs.azure.account.oauth2.client.id.<storage-account-name>.dfs.core.windows.net\", \"<application-id>\");\nconf.set(\"fs.azure.account.oauth2.client.secret.<storage-account-name>.dfs.core.windows.net\",\"<password>\");\nconf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account-name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\");\n```\n\n----------------------------------------\n\nTITLE: Removing a File from a Delta Lake Table\nDESCRIPTION: This JSON snippet illustrates a 'remove' action for removing a file from a Delta Lake table. It includes the file path, deletion timestamp, and row-related metadata.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"remove\": {\n    \"path\": \"part-00001-9…..snappy.parquet\",\n    \"deletionTimestamp\": 1515488792485,\n    \"baseRowId\": 4071,\n    \"defaultRowCommitVersion\": 41,\n    \"dataChange\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Delta Sharing Table Snapshots\nDESCRIPTION: Shows how to read the most recent snapshot of a Delta Sharing table using different programming languages. The table path consists of the profile file path followed by the fully qualified table name.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-sharing.md#2025-04-22_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE mytable USING deltaSharing LOCATION '<profile-file-path>#<share-name>.<schema-name>.<table-name>';\nSELECT * FROM mytable;\n```\n\nLANGUAGE: Python\nCODE:\n```\ntable_path = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\"\ndf = spark.read.format(\"deltaSharing\").load(table_path)\n```\n\nLANGUAGE: Scala\nCODE:\n```\nval tablePath = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\"\nval df = spark.read.format(\"deltaSharing\").load(tablePath)\n```\n\nLANGUAGE: Java\nCODE:\n```\nString tablePath = \"<profile-file-path>#<share-name>.<schema-name>.<table-name>\";\nDataset<Row> df = spark.read.format(\"deltaSharing\").load(tablePath);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Delta Table Details in Scala\nDESCRIPTION: Scala code to retrieve detailed information about a Delta table using the DeltaTable class.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, pathToTable)\n\nval detailDF = deltaTable.detail()\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Cloud Storage LogStore for Delta Lake in INI\nDESCRIPTION: Sets up the LogStore implementation for Google Cloud Storage. This configuration is required for Delta Lake versions 1.2.0 and below when using GCS.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_21\n\nLANGUAGE: ini\nCODE:\n```\nspark.delta.logStore.gs.impl=io.delta.storage.GCSLogStore\n```\n\n----------------------------------------\n\nTITLE: Creating a Non-Partitioned Delta Table in Flink SQL\nDESCRIPTION: This SQL snippet demonstrates how to create a non-partitioned Delta table using the CREATE TABLE statement. It includes basic column definitions and required connector properties.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE testTable (\n    id BIGINT,\n    data STRING\n  ) WITH (\n    'connector' = 'delta',\n    'table-path' = '<path-to-table>',\n    '<arbitrary-user-define-table-property' = '<value>',\n    '<delta.*-properties>' = '<value'>);\n```\n\n----------------------------------------\n\nTITLE: Implementing Deletion Vector with Relative Path\nDESCRIPTION: JSON example showing a Deletion Vector configuration using a relative path with random prefix. The DV is stored relative to an s3://mytable/ directory.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"storageType\" : \"u\",\n  \"pathOrInlineDv\" : \"ab^-aqEH.-t@S}K{vb[*k^\",\n  \"offset\" : 4,\n  \"sizeInBytes\" : 40,\n  \"cardinality\" : 6\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing Delta-Flink Connector SNAPSHOT Version Locally\nDESCRIPTION: Shell commands to build and publish the Delta-Flink connector SNAPSHOT version to the local Maven repository. This is required if using a SNAPSHOT version in Maven or SBT examples.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/examples/flink-example/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbuild/sbt standaloneCosmetic/publishM2\nbuild/sbt flink/publishM2\n```\n\n----------------------------------------\n\nTITLE: Destroying Terraform-managed AWS Infrastructure\nDESCRIPTION: Command to destroy all AWS resources created by Terraform for the benchmarks. This should be run after benchmarks are completed to clean up and avoid unnecessary costs.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/aws/terraform/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nterraform destroy\n```\n\n----------------------------------------\n\nTITLE: Delta Table Directory Structure Example\nDESCRIPTION: Example of a Delta table with three entries in the commit log, showing the directory structure and file naming conventions for Delta tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/mytable/_delta_log/00000000000000000000.json\n/mytable/_delta_log/00000000000000000001.json\n/mytable/_delta_log/00000000000000000003.json\n/mytable/_delta_log/00000000000000000003.checkpoint.parquet\n/mytable/_delta_log/_last_checkpoint\n/mytable/_change_data/cdc-00000-924d9ac7-21a9-4121-b067-a0a6517aa8ed.c000.snappy.parquet\n/mytable/part-00000-3935a07c-416b-4344-ad97-2a38342ee2fc.c000.snappy.parquet\n/mytable/deletion_vector-0c6cbaaf-5e04-4c9d-8959-1088814f58ef.bin\n```\n\n----------------------------------------\n\nTITLE: Setting IBM Cloud Object Storage Credentials for Delta Lake in Scala\nDESCRIPTION: Configures the IBM Cloud Object Storage credentials using access keys. This setup is required for using Delta Lake with IBM COS.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_26\n\nLANGUAGE: scala\nCODE:\n```\nsc.hadoopConfiguration.set(\"fs.cos.service.endpoint\", \"<your-cos-endpoint>\")\nsc.hadoopConfiguration.set(\"fs.cos.service.access.key\", \"<your-cos-access-key>\")\nsc.hadoopConfiguration.set(\"fs.cos.service.secret.key\", \"<your-cos-secret-key>\")\n```\n\n----------------------------------------\n\nTITLE: Setting Table Properties in Delta Table (SQL)\nDESCRIPTION: This SQL snippet demonstrates how to set custom table properties for a Delta table using ALTER TABLE statement. It sets the 'department' and 'delta.appendOnly' properties for the 'default.people10m' table.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_40\n\nLANGUAGE: sql\nCODE:\n```\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n----------------------------------------\n\nTITLE: Recording Commit Provenance Information in Delta Protocol (JSON)\nDESCRIPTION: This JSON snippet demonstrates how to store commit provenance information in the Delta protocol. It includes details such as timestamp, user information, operation type, and execution environment, providing a comprehensive audit trail for table modifications.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"commitInfo\":{\n    \"timestamp\":1515491537026,\n    \"userId\":\"100121\",\n    \"userName\":\"michael@databricks.com\",\n    \"operation\":\"INSERT\",\n    \"operationParameters\":{\"mode\":\"Append\",\"partitionBy\":\"[]\"},\n    \"notebook\":{\n      \"notebookId\":\"4443029\",\n      \"notebookPath\":\"Users/michael@databricks.com/actions\"},\n      \"clusterId\":\"1027-202406-pooh991\"\n  }  \n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Delta Standalone (Scala 2.12)\nDESCRIPTION: Maven configuration snippet showing how to add Delta Standalone and Hadoop client dependencies for Scala 2.12 projects.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-standalone_2.12</artifactId>\n  <version>0.5.0</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.hadoop</groupId>\n  <artifactId>hadoop-client</artifactId>\n  <version>3.1.0</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Checking Partition Existence in Delta Tables\nDESCRIPTION: Use SQL or DataFrame operations to check if a specific partition exists in a Delta table. This example demonstrates how to check for the existence of a 'gender' partition with value 'M'.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-batch.md#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*) > 0 AS `Partition exists` FROM default.people10m WHERE gender = \"M\"\n```\n\nLANGUAGE: python\nCODE:\n```\ndisplay(spark.sql(\"SELECT COUNT(*) > 0 AS `Partition exists` FROM default.people10m WHERE gender = 'M'\"))\n```\n\nLANGUAGE: scala\nCODE:\n```\ndisplay(spark.sql(\"SELECT COUNT(*) > 0 AS `Partition exists` FROM default.people10m WHERE gender = 'M'\"))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Test Execution Times with PySpark\nDESCRIPTION: This PySpark code reads CSV files containing test execution data, filters and aggregates the data to calculate total execution time per test suite, and displays the top 50 slowest test suites. It uses PySpark SQL functions and custom schema definition.\nSOURCE: https://github.com/delta-io/delta/blob/master/project/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.sql.functions import col, sum\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType\n\nschema = StructType([\n    StructField(\"test_suite\", StringType(), True),\n    StructField(\"test_name\", StringType(), True),\n    StructField(\"execution_time_ms\", LongType(), True),\n    StructField(\"result\", StringType(), True)\n])\n\ncsv_dir = \"...\"\n\nspark.read.csv(csv_dir, schema=schema) \\\n    .filter(col(\"execution_time_ms\") != -1) \\\n    .groupBy(\"test_suite\") \\\n    .agg((sum(\"execution_time_ms\") / 60000).alias(\"execution_time_mins\")) \\\n    .orderBy(col(\"execution_time_mins\").desc()) \\\n    .limit(50) \\\n    .select(\"test_suite\", \"execution_time_mins\") \\\n    .show(50, truncate=False)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Delta Kernel Maven Dependencies\nDESCRIPTION: Maven POM file configuration showing the required dependencies for Delta Kernel. It includes delta-kernel-api (core module abstracting the Delta protocol) and delta-kernel-defaults (provides default Engine implementation).\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependencies>\n  <dependency>\n    <groupId>io.delta</groupId>\n    <artifactId>delta-kernel-api</artifactId>\n    <version>${delta-kernel.version}</version>\n  </dependency>\n\n  <dependency>\n    <groupId>io.delta</groupId>\n    <artifactId>delta-kernel-defaults</artifactId>\n    <version>${delta-kernel.version}</version>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Enabling VACUUM Protocol Check in Delta Lake\nDESCRIPTION: Specifies the requirements for enabling the VACUUM Protocol Check feature in Delta Lake tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_31\n\nLANGUAGE: markdown\nCODE:\n```\n- The table must be on Writer Version 7 and Reader Version 3.\n- The feature `vacuumProtocolCheck` must exist in the table `protocol`'s `writerFeatures` and `readerFeatures`.\n```\n\n----------------------------------------\n\nTITLE: Configuring IBM Cloud Object Storage LogStore for Delta Lake in INI\nDESCRIPTION: Sets up the LogStore implementation and Stocator configuration for IBM Cloud Object Storage. This configuration is required when using Delta Lake with IBM COS.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_25\n\nLANGUAGE: ini\nCODE:\n```\nspark.delta.logStore.cos.impl=io.delta.storage.IBMCOSLogStore\n\nfs.stocator.scheme.list=cos\nfs.cos.impl=com.ibm.stocator.fs.ObjectStoreFileSystem\nfs.stocator.cos.impl=com.ibm.stocator.fs.cos.COSAPIClient\nfs.stocator.cos.scheme=cos\nfs.cos.atomic.write=true\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Definition for Delta Table with Variant Types\nDESCRIPTION: Example schema showing how Variant data types are defined in a Delta table schema, including both a simple variant field and an array of variants.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/variant-type.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"raw_data\",\n    \"type\" : \"variant\",\n    \"nullable\" : true,\n    \"metadata\" : { }\n  }, {\n    \"name\" : \"variant_array\",\n    \"type\" : {\n      \"type\" : \"array\",\n      \"elementType\" : {\n        \"type\" : \"variant\"\n      },\n      \"containsNull\" : false\n    },\n    \"nullable\" : false,\n    \"metadata\" : { }\n  } ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Domain Metadata Action in Delta Lake Protocol\nDESCRIPTION: JSON example of a domainMetadata action that defines a system-controlled domain with configuration values. This action represents how metadata for Delta table features is stored in the transaction log.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_18\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"domainMetadata\": {\n    \"domain\": \"delta.deltaTableFeatureX\",\n    \"configuration\": \"{\\\"key1\\\":\\\"value1\\\"}\",\n    \"removed\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Data Import from JDBC to Delta Lake using Spark Submit (Shell)\nDESCRIPTION: Demonstrates how to use spark-submit to run a basic import job, splitting the source table by 'id' into 10 chunks and importing it into a destination Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/sql-delta-import/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nspark-submit \\\n--class \"io.delta.connectors.spark.jdbc.ImportRunner\" sql-delta-import.jar \\\n--jdbc-url jdbc:mysql://hostName:port/database \\\n--source source.table\n--destination destination.table\n--split-by id\n```\n\n----------------------------------------\n\nTITLE: Schema Example for Nested Field Identifiers\nDESCRIPTION: JSON example demonstrating the storage of identifiers for nested fields in ArrayType and MapType within a complex schema structure.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"col1\",\n    \"type\": {\n      \"type\": \"array\",\n      \"elementType\": {\n        \"type\": \"array\",\n        \"elementType\": \"int\"\n      }\n    },\n    \"metadata\": {\n      \"parquet.field.nested.ids\": {\n        \"col1.element\": 100,\n        \"col1.element.element\": 101\n      }\n    }\n  },\n  {\n    \"name\": \"col2\",\n    \"type\": {\n      \"type\": \"map\",\n      \"keyType\": \"int\",\n      \"valueType\": {\n        \"type\": \"array\",\n        \"elementType\": \"int\"\n      }\n    },\n    \"metadata\": {\n      \"parquet.field.nested.ids\": {\n        \"col2.key\": 102,\n        \"col2.value\": 103,\n        \"col2.value.element\": 104\n      }\n    }\n  },\n  {\n    \"name\": \"col3\",\n    \"type\": {\n      \"type\": \"map\",\n      \"keyType\": \"int\",\n      \"valueType\": {\n        \"type\": \"struct\",\n        \"fields\": [\n          {\n            \"name\": \"subcol1\",\n            \"type\": {\n              \"type\": \"array\",\n              \"elementType\": \"int\"\n            },\n            \"metadata\": {\n              \"parquet.field.nested.ids\": {\n                \"subcol1.element\": 107\n              }\n            }\n          }\n        ]\n      }\n    },\n    \"metadata\": {\n      \"parquet.field.nested.ids\": {\n        \"col3.key\": 105,\n        \"col3.value\": 106\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Delta Lake Checkpoint Schema with Column Mapping\nDESCRIPTION: Structure of the 'add' portion of a Delta Lake checkpoint schema when column mapping is used, showing how physical column names are used in statistics and partition values.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_51\n\nLANGUAGE: txt\nCODE:\n```\n|-- add: struct\n|    |-- path: string\n|    |-- partitionValues: map<string,string>\n|    |-- size: long\n|    |-- modificationTime: long\n|    |-- dataChange: boolean\n|    |-- stats: string\n|    |-- tags: map<string,string>\n|    |-- baseRowId: long\n|    |-- defaultRowCommitVersion: long\n|    |-- partitionValues_parsed: struct\n|    |    |-- col-798f4abc-c63f-444c-9a04-e2cf1ecba115: date\n|    |    |-- col-19034dc3-8e3d-4156-82fc-8e05533c088e: string\n|    |-- stats_parsed: struct\n|    |    |-- numRecords: long\n|    |    |-- minValues: struct\n|    |    |    |-- col-b96921f0-2329-4cb3-8d79-184b2bdab23b: string\n|    |    |    |-- col-04ee4877-ee53-4cb9-b1fb-1a4eb74b508c: double\n|    |    |-- maxValues: struct\n|    |    |    |-- col-b96921f0-2329-4cb3-8d79-184b2bdab23b: string\n|    |    |    |-- col-04ee4877-ee53-4cb9-b1fb-1a4eb74b508c: double\n|    |    |-- nullCounts: struct\n|    |    |    |-- col-b96921f0-2329-4cb3-8d79-184b2bdab23b: long\n|    |    |    |-- col-04ee4877-ee53-4cb9-b1fb-1a4eb74b508c: long\n```\n\n----------------------------------------\n\nTITLE: Loading 3TB TPC-DS Data as Delta Tables on GCP\nDESCRIPTION: Shell command for loading the 3TB TPC-DS dataset into Delta tables specifically on Google Cloud Platform. Includes the additional source-path parameter needed for GCP deployments.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --source-path <SOURCE_PATH> \\\n    --cloud-provider gcp \\\n    --benchmark tpcds-3tb-delta-load\n```\n\n----------------------------------------\n\nTITLE: Running Scala Delta Lake Examples with SBT\nDESCRIPTION: Command to run Scala examples using SBT build tool. The example demonstrates how to run the Quickstart class from the example package.\nSOURCE: https://github.com/delta-io/delta/blob/master/examples/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/sbt \"runMain example.{Example class name}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring LogStore Implementation\nDESCRIPTION: Configuration setting for specifying the S3DynamoDBLogStore implementation in Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_9\n\nLANGUAGE: ini\nCODE:\n```\nspark.delta.logStore.s3.impl=io.delta.storage.S3DynamoDBLogStore\n```\n\n----------------------------------------\n\nTITLE: Engine Interface Definition in Delta Kernel\nDESCRIPTION: Java interface showing the core Engine methods that need to be implemented when integrating Delta Kernel with a connector. This interface defines handlers for expressions, JSON parsing, file system operations, and Parquet handling.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_19\n\nLANGUAGE: java\nCODE:\n```\ninterface Engine {\n  /**\n   * Get the connector provided {@link ExpressionHandler}.\n   * @return An implementation of {@link ExpressionHandler}.\n  */\n  ExpressionHandler getExpressionHandler();\n\n  /**\n   * Get the connector provided {@link JsonHandler}.\n   * @return An implementation of {@link JsonHandler}.\n   */\n  JsonHandler getJsonHandler();\n\n  /**\n   * Get the connector provided {@link FileSystemClient}.\n   * @return An implementation of {@link FileSystemClient}.\n   */\n  FileSystemClient getFileSystemClient();\n\n  /**\n   * Get the connector provided {@link ParquetHandler}.\n   * @return An implementation of {@link ParquetHandler}.\n   */\n  ParquetHandler getParquetHandler();\n}\n```\n\n----------------------------------------\n\nTITLE: Example Sidecar File Reference Action in Delta Lake Protocol\nDESCRIPTION: JSON example of a sidecar action that references a parquet file containing checkpoint file actions. This action is only allowed in checkpoints following the V2 spec and provides metadata about the sidecar file.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"sidecar\":{\n    \"path\": \"016ae953-37a9-438e-8683-9a9a4a79a395.parquet\",\n    \"sizeInBytes\": 2304522,\n    \"modificationTime\": 1512909768000,\n    \"tags\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Delta Source Example with Maven\nDESCRIPTION: Maven command to package and execute a Delta Source example. It demonstrates how to run a specific class and pass necessary parameters.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/examples/flink-example/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/flink-example/\n\nmvn package exec:java -Dexec.cleanupDaemonThreads=false -Dexec.mainClass=org.example.source.bounded.DeltaBoundedSourceExample -Dstaging.repo.url={maven_repo} -Dconnectors.version={version}\n```\n\n----------------------------------------\n\nTITLE: Plain JSON Format of Clustering Columns Configuration in Delta Lake\nDESCRIPTION: Example of the plain JSON format for the clustering columns configuration, showing the unescaped representation of the clusteringColumns array with physical column names.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_34\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"clusteringColumns\": [\n    \"col-daadafd7-7c20-4697-98f8-bff70199b1f9\",\n    \"col-5abe0e80-cf57-47ac-9ffc-a861a3d1077e\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Checkpoint Schema Structure\nDESCRIPTION: Shows the schema structure for a checkpoint with partitioned data, demonstrating how partition values are stored in their corresponding data types.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_39\n\nLANGUAGE: text\nCODE:\n```\n|-- add: struct\n|    |-- partitionValues_parsed: struct\n|    |    |-- year: int\n|    |    |-- month: int\n|    |    |-- event: string\n```\n\n----------------------------------------\n\nTITLE: Packaging Flink Example for Cluster Deployment\nDESCRIPTION: Maven command to package the Flink example as a fat JAR for deployment on a Flink cluster. It includes profile and property settings.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/examples/flink-example/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/flink-example/\nmvn -P cluster clean package -Dstaging.repo.url={maven_repo} -Dconnectors.version={version}\n```\n\n----------------------------------------\n\nTITLE: Implementation of the Engine Interface in Delta Kernel\nDESCRIPTION: Java code showing the Engine interface definition with its subinterfaces for handling expressions, JSON, file systems, and Parquet operations. This interface design allows connectors to customize how Delta tables are accessed.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_24\n\nLANGUAGE: java\nCODE:\n```\ninterface Engine {\n  /**\n   * Get the connector provided {@link ExpressionHandler}.\n   * @return An implementation of {@link ExpressionHandler}.\n  */\n  ExpressionHandler getExpressionHandler();\n\n  /**\n   * Get the connector provided {@link JsonHandler}.\n   * @return An implementation of {@link JsonHandler}.\n   */\n  JsonHandler getJsonHandler();\n\n  /**\n   * Get the connector provided {@link FileSystemClient}.\n   * @return An implementation of {@link FileSystemClient}.\n   */\n  FileSystemClient getFileSystemClient();\n\n  /**\n   * Get the connector provided {@link ParquetHandler}.\n   * @return An implementation of {@link ParquetHandler}.\n   */\n  ParquetHandler getParquetHandler();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Commit Owner Interface in Java\nDESCRIPTION: Interface definition for CommitStore that handles table commits and backfills. Includes methods for committing actions, retrieving un-backfilled commits, and managing backfill operations for Delta tables.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/managed-commits.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ninterface CommitStore {\n    /**\n     * Commits the given set of `actions` to the given commit `version`.\n     *\n     * @param version The version we want to commit.\n     * @param actions Actions that need to be committed.\n     *\n     * @return CommitResponse which has details around the new committed delta file.\n     */\n    def commit(\n        version: Long,\n        actions: Iterator[String]): CommitResponse\n\n    /**\n     * API to get the un-backfilled commits for the table represented by the given `tablePath` where\n     * `startVersion` <= version <= endVersion.\n     * If endVersion is -1, then it means that we want to get all the commits starting from `startVersion`\n     * till the latest version tracked by commit-owner.\n     * The returned commits are contiguous and in ascending version order.\n     * Note that the first version returned by this API may not be equal to the `startVersion`. This\n     * happens when few versions starting from `startVersion` are already backfilled and so\n     * CommitStore may have stopped tracking them.\n     * The returned latestTableVersion is the maximum commit version ratified by the Commit-Owner.\n     * Note that returning latestTableVersion as -1 is acceptable only if the commit-owner never\n     * ratified any version i.e. it never accepted any un-backfilled commit.\n     *\n     * @return GetCommitsResponse which contains a list of `Commit`s and the latestTableVersion\n     *         tracked by the commit-owner.\n     */\n    def getCommits(\n        startVersion: Long,\n        endVersion: Long): GetCommitsResponse\n\n    /**\n     * API to ask the commit-owner to backfill all commits <= given `version`.\n     */\n    def backfillToVersion(version: Long): Unit\n}\n```\n\n----------------------------------------\n\nTITLE: Serializing Delta Transaction State\nDESCRIPTION: Code to serialize transaction state for distribution to worker nodes in a distributed processing environment.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_35\n\nLANGUAGE: java\nCODE:\n```\nRow txnState = txn.getState(engine);\n\nString jsonTxnState = serializeToJson(txnState);\n```\n\n----------------------------------------\n\nTITLE: Example Checkpoint Metadata Action in Delta Lake Protocol\nDESCRIPTION: JSON example of a checkpointMetadata action that describes details about a checkpoint. This action is only allowed in checkpoints following the V2 spec and provides version information and optional tags.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_20\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"checkpointMetadata\":{\n    \"version\":1,\n    \"tags\":{}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Lightweight Delta Lake Partition Filter Function in PowerQuery\nDESCRIPTION: The most concise version of the partition filter function using record field access notation to filter for Year=2021 and Month=\"Jan\".\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_3\n\nLANGUAGE: m\nCODE:\n```\n(x) => x[Year] = 2021 and x[Month] = \"Jan\"\n```\n\n----------------------------------------\n\nTITLE: Clone Use Cases Examples\nDESCRIPTION: Practical examples of using shallow clones for machine learning workflows and production table experiments, including table property overrides for archiving.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-utility.md#2025-04-22_snippet_18\n\nLANGUAGE: sql\nCODE:\n```\n-- Trained model on version 15 of Delta table\nCREATE TABLE delta.`/model/dataset` SHALLOW CLONE entire_dataset VERSION AS OF 15\n```\n\nLANGUAGE: sql\nCODE:\n```\n-- Perform shallow clone\nCREATE OR REPLACE TABLE my_test SHALLOW CLONE my_prod_table;\n\nUPDATE my_test WHERE user_id is null SET invalid=true;\n-- Run a bunch of validations. Once happy:\n\n-- This should leverage the update information in the clone to prune to only\n-- changed files in the clone if possible\nMERGE INTO my_prod_table\nUSING my_test\nON my_test.user_id <=> my_prod_table.user_id\nWHEN MATCHED AND my_test.user_id is null THEN UPDATE *;\n\nDROP TABLE my_test;\n```\n\nLANGUAGE: sql\nCODE:\n```\nCREATE OR REPLACE TABLE archive.my_table SHALLOW CLONE prod.my_table\nTBLPROPERTIES (\n  delta.logRetentionDuration = '3650 days',\n  delta.deletedFileRetentionDuration = '3650 days'\n)\nLOCATION 'xx://archive/my_table'\n```\n\n----------------------------------------\n\nTITLE: Delta Table Schema with Variant Data Types\nDESCRIPTION: Example of a JSON-encoded Delta table schema that includes the Variant data type. It shows both a simple Variant field and an array of Variant elements with their respective nullable settings.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"raw_data\",\n    \"type\" : \"variant\",\n    \"nullable\" : true,\n    \"metadata\" : { }\n  }, {\n    \"name\" : \"variant_array\",\n    \"type\" : {\n      \"type\" : \"array\",\n      \"elementType\" : {\n        \"type\" : \"variant\"\n      },\n      \"containsNull\" : false\n    },\n    \"nullable\" : false,\n    \"metadata\" : { }\n  } ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Local Flink Cluster and Submitting Job\nDESCRIPTION: Shell commands to start a local Flink cluster and submit a Flink job using the packaged example JAR. It demonstrates how to specify the main class and JAR file.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/examples/flink-example/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd <local-flink-cluster-dir>\n./bin/start-cluster.sh\n./bin/flink run -c org.example.sink.DeltaSinkExampleCluster <connectors-repo-local-dir>/flink-example/target/flink-example-<version>-jar-with-dependencies.jar\n```\n\n----------------------------------------\n\nTITLE: Setting Row Tracking Preservation Flag in Delta Lake Commit Info\nDESCRIPTION: Explains how to indicate that all stable Row IDs and Row Commit Versions were preserved during a commit operation.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_30\n\nLANGUAGE: markdown\nCODE:\n```\nWriters should set `delta.rowTracking.preserved` in the `tags` of the `commitInfo` action to `true` whenever all the stable Row IDs of rows that are updated or copied and all the stable Row Commit Versions of rows that are copied were preserved.\n```\n\n----------------------------------------\n\nTITLE: Example Delta Lake Table Schema in JSON Format\nDESCRIPTION: JSON representation of a Delta Lake table schema defining the same structure as the text format above, with detailed type information and metadata for each field.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_48\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\" : \"struct\",\n  \"fields\" : [ {\n    \"name\" : \"a\",\n    \"type\" : \"integer\",\n    \"nullable\" : false,\n    \"metadata\" : { }\n  }, {\n    \"name\" : \"b\",\n    \"type\" : {\n      \"type\" : \"struct\",\n      \"fields\" : [ {\n        \"name\" : \"d\",\n        \"type\" : \"integer\",\n        \"nullable\" : false,\n        \"metadata\" : { }\n      } ]\n    },\n    \"nullable\" : true,\n    \"metadata\" : { }\n  }, {\n    \"name\" : \"c\",\n    \"type\" : {\n      \"type\" : \"array\",\n      \"elementType\" : \"integer\",\n      \"containsNull\" : false\n    },\n    \"nullable\" : true,\n    \"metadata\" : { }\n  }, {\n    \"name\" : \"e\",\n    \"type\" : {\n      \"type\" : \"array\",\n      \"elementType\" : {\n        \"type\" : \"struct\",\n        \"fields\" : [ {\n          \"name\" : \"d\",\n          \"type\" : \"integer\",\n          \"nullable\" : false,\n          \"metadata\" : { }\n        } ]\n      },\n      \"containsNull\" : true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { }\n  }, {\n    \"name\" : \"f\",\n    \"type\" : {\n      \"type\" : \"map\",\n      \"keyType\" : \"string\",\n      \"valueType\" : \"string\",\n      \"valueContainsNull\" : true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { }\n  } ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Delta Lake using SBT\nDESCRIPTION: Commands for compiling, generating artifacts, and running tests for Delta Lake using SBT (Scala Build Tool).\nSOURCE: https://github.com/delta-io/delta/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbuild/sbt compile\nbuild/sbt package\nbuild/sbt test\nbuild/sbt spark/'testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSQLSuite'\nbuild/sbt spark/'testOnly *.OptimizeCompactionSQLSuite -- -z \"optimize command: on partitioned table - all partitions\"'\n```\n\n----------------------------------------\n\nTITLE: Example Directory Structure for Managed-Commit Delta Table\nDESCRIPTION: This snippet shows the directory structure of a Delta table with managed commits, including backfilled and un-backfilled commits.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/managed-commits.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n_delta_log/00000000000000000000.json\n_delta_log/00000000000000000001.json\n_delta_log/00000000000000000002.json\n_delta_log/00000000000000000002.checkpoint.parquet\n_delta_log/00000000000000000003.json\n_delta_log/00000000000000000003.00000000000000000005.compacted.json\n_delta_log/00000000000000000004.json\n_delta_log/00000000000000000005.json\n_delta_log/00000000000000000006.json\n_delta_log/00000000000000000007.json\n_delta_log/_commits/00000000000000000006.3a0d65cd-4056-49b8-937b-95f9e3ee90e5.json\n_delta_log/_commits/00000000000000000007.016ae953-37a9-438e-8683-9a9a4a79a395.json\n_delta_log/_commits/00000000000000000008.7d17ac10-5cc3-401b-bd1a-9c82dd2ea032.json\n_delta_log/_commits/00000000000000000008.b91807ba-fe18-488c-a15e-c4807dbd2174.json\n_delta_log/_commits/00000000000000000009.41bf693a-f5b9-4478-9434-af7475d5a9f0.json\n_delta_log/_commits/00000000000000000010.0f707846-cd18-4e01-b40e-84ee0ae987b0.json\n_delta_log/_commits/00000000000000000010.7a980438-cb67-4b89-82d2-86f73239b6d6.json\n```\n\n----------------------------------------\n\nTITLE: Building Delta Hive Connector Uber JAR with SBT\nDESCRIPTION: Commands to compile the project, run tests, and generate the uber JAR for the Delta Hive Connector using SBT build tool.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/hive/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nbuild/sbt hive/compile\nbuild/sbt hiveMR/test hiveTez/test\nbuild/sbt hive2MR/test hive2Tez/test\nbuild/sbt hiveAssembly/assembly\n```\n\n----------------------------------------\n\nTITLE: Creating Continuous Delta Source with All Columns in Java\nDESCRIPTION: Creates a continuous Delta source that reads all columns and monitors for changes. Suitable for streaming applications requiring full table access.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic DataStream<RowData> createContinuousDeltaSourceAllColumns(\n        StreamExecutionEnvironment env,\n        String deltaTablePath) {\n\n    DeltaSource<RowData> deltaSource = DeltaSource\n        .forContinuousRowData(\n            new Path(deltaTablePath),\n            new Configuration())\n        .build();\n\n    return env.fromSource(deltaSource, WatermarkStrategy.noWatermarks(), \"delta-source\");\n}\n```\n\n----------------------------------------\n\nTITLE: Example V2 Spec Checkpoint Content with Sidecar References\nDESCRIPTION: Shows the JSON structure of a V2 spec checkpoint that references sidecar files. This example demonstrates how checkpoint metadata, table metadata, protocol information, transaction data, and sidecar references are formatted.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\"checkpointMetadata\":{\"version\":364475,\"tags\":{}}}\n{\"metaData\":{...}}\n{\"protocol\":{...}}\n{\"txn\":{\"appId\":\"3ba13872-2d47-4e17-86a0-21afd2a22395\",\"version\":364475}}\n{\"txn\":{\"appId\":\"3ae45b72-24e1-865a-a211-34987ae02f2a\",\"version\":4389}}\n{\"sidecar\":{\"path\":\"3a0d65cd-4056-49b8-937b-95f9e3ee90e5.parquet\",\"sizeInBytes\":2341330,\"modificationTime\":1512909768000,\"tags\":{}}}\n{\"sidecar\":{\"path\":\"016ae953-37a9-438e-8683-9a9a4a79a395.parquet\",\"sizeInBytes\":8468120,\"modificationTime\":1512909848000,\"tags\":{}}}\n```\n\n----------------------------------------\n\nTITLE: Generating Full Delta Lake Documentation Including API Docs\nDESCRIPTION: Python command to generate all Delta Lake documentation, including API docs. This command should be run from the Delta repository root directory and will provide a URL for previewing the docs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 docs/generate_docs.py --livehtml --api-docs\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies for Delta Kernel in XML\nDESCRIPTION: Shows how to set up Maven dependencies for the Delta Kernel project. It includes the required 'delta-kernel-api' dependency and the optional 'delta-kernel-defaults' dependency.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/README.md#2025-04-22_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<!-- Must have dependency -->\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-kernel-api</artifactId>\n  <version>VERSION</version>\n</dependency>\n\n<!-- Optional dependency -->\n<dependency>\n  <groupId>io.delta</groupId>\n  <artifactId>delta-kernel-defaults</artifactId>\n  <version>VERSION</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Column Invariants in Delta Lake JSON Schema\nDESCRIPTION: Demonstrates how to add column invariants to a Delta Lake table schema using JSON. The example shows an integer column 'x' with an invariant that its value must be greater than 3.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_42\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"type\": \"struct\",\n    \"fields\": [\n        {\n            \"name\": \"x\",\n            \"type\": \"integer\",\n            \"nullable\": true,\n            \"metadata\": {\n                \"delta.invariants\": \"{\\\"expression\\\": { \\\"expression\\\": \\\"x > 3\\\"} }\"\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Managed Commit Configuration in Delta Table Metadata\nDESCRIPTION: This JSON snippet shows the configuration for enabling managed commits in a Delta table's metadata, including the commit owner and additional configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/managed-commits.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"metaData\":{\n      \"id\":\"af23c9d7-fff1-4a5a-a2c8-55c59bd782aa\",\n      \"format\":{\"provider\":\"parquet\",\"options\":{}},\n      \"schemaString\":\"...\",\n      \"partitionColumns\":[],\n      \"configuration\":{\n         \"appendOnly\": \"true\",\n         \"delta.managedCommit.commitOwner\": \"commit-owner-1\",\n         \"delta.managedCommit.commitOwnerConf\":\n             \"{\\\"endpoint\\\":\\\"http://sample-url.com/commit\\\", \\\"authenticationMode\\\":\\\"oauth2\\\"}\"\n      }\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Delta Lake Table Schema in Text Format\nDESCRIPTION: Text representation of a Delta Lake table schema with various data types including integer, struct, array, and map types with different nullability settings.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_47\n\nLANGUAGE: txt\nCODE:\n```\n|-- a: integer (nullable = false)\n|-- b: struct (nullable = true)\n|    |-- d: integer (nullable = false)\n|-- c: array (nullable = true)\n|    |-- element: integer (containsNull = false)\n|-- e: array (nullable = true)\n|    |-- element: struct (containsNull = true)\n|    |    |-- d: integer (nullable = false)\n|-- f: map (nullable = true)\n|    |-- key: string\n|    |-- value: string (valueContainsNull = true)\n```\n\n----------------------------------------\n\nTITLE: Executing Terraform Commands for Infrastructure Creation\nDESCRIPTION: Sequence of Terraform commands to initialize the working directory, validate the configuration, and apply the infrastructure changes to create the required AWS resources for benchmarks.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/aws/terraform/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nterraform init\nterraform validate\nterraform apply\n```\n\n----------------------------------------\n\nTITLE: Implementing ParquetHandler.writeParquetFiles Method in Delta Kernel\nDESCRIPTION: Method for writing data as Parquet files that accepts columnar data, a directory path, and columns for statistics collection. Returns DataFileStatus objects for each written file.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nwriteParquetFiles(String directoryPath, CloseableIterator<FilteredColumnarBatch> dataIter, java.util.List<Column> statsColumns)\n```\n\n----------------------------------------\n\nTITLE: Schema Definition with Column Mapping for IcebergWriterCompatV1\nDESCRIPTION: Example of a compliant schema definition showing the required column mapping format where physical names must follow the pattern 'col-[column id]'. The schema demonstrates proper metadata configuration for integer and string fields.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/iceberg-writer-compat-v1.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"struct\",\n  \"fields\": [\n    {\n      \"name\": \"a\",\n      \"type\": \"integer\",\n      \"nullable\": false,\n      \"metadata\": {\n        \"delta.columnMapping.id\": 1,\n        \"delta.columnMapping.physicalName\": \"col-1\"\n      }\n    },\n    {\n      \"name\": \"b\",\n      \"type\": \"string\",\n      \"nullable\": false,\n      \"metadata\": {\n        \"delta.columnMapping.id\": 2,\n        \"delta.columnMapping.physicalName\": \"col-2\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Delta Lake Documentation Excluding API Docs\nDESCRIPTION: Python command to generate Delta Lake documentation without API docs. This is faster and useful for generating content for the main documentation website.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 docs/generate_docs.py --livehtml\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Auxiliary JARs Path in XML\nDESCRIPTION: XML configuration to be added to hive-site.xml file to specify the path to the Delta Hive Connector uber JAR.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/hive/README.md#2025-04-22_snippet_3\n\nLANGUAGE: XML\nCODE:\n```\n<property>\n  <name>hive.aux.jars.path</name>\n  <value>path_to_uber_jar</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Example V2 Spec Checkpoint Content without Sidecars\nDESCRIPTION: Shows the JSON structure of a V2 spec checkpoint without sidecar files. This example includes checkpoint metadata, table metadata, protocol information, transaction data, and embedded add/remove file actions.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\"checkpointMetadata\":{\"version\":364475,\"tags\":{}}}\n{\"metaData\":{...}}\n{\"protocol\":{...}}\n{\"txn\":{\"appId\":\"3ba13872-2d47-4e17-86a0-21afd2a22395\",\"version\":364475}}\n{\"add\":{\"path\":\"date=2017-12-10/part-000...c000.gz.parquet\",...}\n{\"add\":{\"path\":\"date=2017-12-09/part-000...c000.gz.parquet\",...}\n{\"remove\":{\"path\":\"date=2017-12-08/part-000...c000.gz.parquet\",...}\n```\n\n----------------------------------------\n\nTITLE: Setting up Python test environment using Conda\nDESCRIPTION: Instructions for creating a Conda environment and running Python tests for Delta Lake locally.\nSOURCE: https://github.com/delta-io/delta/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda env create --name delta_python_tests --file=<absolute_path_to_delta_repo>/python/environment.yml\nconda activate delta_python_tests\npython3 <delta-root>/python/run-tests.py\n```\n\n----------------------------------------\n\nTITLE: Example JSON Schema for Nested Complex Type Change\nDESCRIPTION: Demonstrates the schema definition for an array containing maps, where the map value type has been widened from decimal(6,2) to decimal(10,4). The fieldPath uses dot notation to identify the nested path element.value.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/type-widening.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : {\n      \"type\": \"array\",\n      \"elementType\": {\n        \"type\": \"map\",\n        \"keyType\": \"string\",\n        \"valueType\": \"decimal(10, 4)\",\n        \"valueContainsNull\": true\n      },\n      \"containsNull\": true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.typeChanges\": [\n        {\n          \"fromType\": \"decimal(6, 2)\",\n          \"toType\": \"decimal(10, 4)\",\n          \"fieldPath\": \"element.value\"\n        }\n      ]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Example TimestampNtz Format in Delta Lake\nDESCRIPTION: Example of the TimestampNtz format in Delta Lake, which represents timestamps without timezone information.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_26\n\nLANGUAGE: markdown\nCODE:\n```\n`1970-01-01 00:00:00`, or `1970-01-01 00:00:00.123456`\n```\n\n----------------------------------------\n\nTITLE: Configuring Terraform Variables for GCP Delta Lake Benchmarks\nDESCRIPTION: Creates a terraform.tfvars file that configures the GCP project, credentials, SSH keys, region, bucket name, and worker count for the Delta Lake benchmark infrastructure. Labels can be added for resource organization.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/gcp/terraform/README.md#2025-04-22_snippet_0\n\nLANGUAGE: tf\nCODE:\n```\nproject          = \"<PROJECT_ID>\"\ncredentials_file = \"<CREDENTIALS_FILE>\"\npublic_key_path  = \"<PUBLIC_KEY_PATH>\"\nregion           = \"<REGION>\"\nzone             = \"<ZONE>\"\nbucket_name      = \"<BUCKET_NAME>\"\ndataproc_workers = WORKERS_COUNT\nlabels           = {\n  key1 = \"value1\"\n  key2 = \"value2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading 1GB TPC-DS Data as Delta Tables\nDESCRIPTION: Shell command to load 1GB TPC-DS dataset into Delta tables. This is the first step in the TPC-DS benchmark process, creating the Delta tables needed for subsequent query testing.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --cloud-provider <CLOUD_PROVIDER> \\\n    --benchmark tpcds-1gb-delta-load\n```\n\n----------------------------------------\n\nTITLE: Commit-Owner Tracking Example for Managed-Commit Delta Table\nDESCRIPTION: This JSON snippet demonstrates how a commit-owner tracks un-backfilled commits in a managed-commit Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/managed-commits.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  6  : \"00000000000000000006.3a0d65cd-4056-49b8-937b-95f9e3ee90e5.json\",\n  7  : \"00000000000000000007.016ae953-37a9-438e-8683-9a9a4a79a395.json\",\n  8  : \"00000000000000000008.7d17ac10-5cc3-401b-bd1a-9c82dd2ea032.json\",\n  9  : \"00000000000000000009.41bf693a-f5b9-4478-9434-af7475d5a9f0.json\"\n}\n```\n\n----------------------------------------\n\nTITLE: Predicate Evaluator Creation Method Signature\nDESCRIPTION: Method to create a predicate evaluator for boolean expressions on columnar data.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\ngetPredicateEvaluator(StructType inputSchema, Predicate predicate)\n```\n\n----------------------------------------\n\nTITLE: Creating Iterable for Data Actions\nDESCRIPTION: Creates a closeable iterable from data actions for transaction commit processing, with options for memory management.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_18\n\nLANGUAGE: java\nCODE:\n```\n// Create a iterable out of the data actions. If the contents are too big to fit in memory,\n// the connector may choose to write the data actions to a temporary file and return an\n// iterator that reads from the file.\nCloseableIterable<Row> dataActionsIterable = CloseableIterable.inMemoryIterable(dataActions);\n```\n\n----------------------------------------\n\nTITLE: Defining Nested Array/Map Structure with Type Change in Delta Lake (JSON)\nDESCRIPTION: Example JSON schema definition for a complex column with an array of maps after changing the type of the map value from decimal(6,2) to decimal(10,4). The type change is recorded with a nested fieldPath to identify the exact location of the changed type.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_38\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"name\" : \"e\",\n    \"type\" : {\n      \"type\": \"array\",\n      \"elementType\": {\n        \"type\": \"map\",\n        \"keyType\": \"string\",\n        \"valueType\": \"decimal(10, 4)\",\n        \"valueContainsNull\": true\n      },\n      \"containsNull\": true\n    },\n    \"nullable\" : true,\n    \"metadata\" : { \n      \"delta.typeChanges\": [\n        {\n          \"fromType\": \"decimal(6, 2)\",\n          \"toType\": \"decimal(10, 4)\",\n          \"fieldPath\": \"element.value\"\n        }\n      ]\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Enabling DynamoDB TTL for Delta Log Store (Bash)\nDESCRIPTION: AWS CLI command to enable Time to Live (TTL) on a DynamoDB table used for Delta Lake metadata storage. This helps cleanup old entries automatically.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-storage.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\naws dynamodb update-time-to-live \\\n  --region us-east-1 \\\n  --table-name delta_log \\\n  --time-to-live-specification \"Enabled=true, AttributeName=expireTime\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Deletion Vector with Absolute Path\nDESCRIPTION: JSON example demonstrating a Deletion Vector configuration using an absolute path reference.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"storageType\" : \"p\",\n  \"pathOrInlineDv\" : \"s3://mytable/deletion_vector_d2c639aa-8816-431a-aaf6-d3fe2512ff61.bin\",\n  \"offset\" : 4,\n  \"sizeInBytes\" : 40,\n  \"cardinality\" : 6\n}\n```\n\n----------------------------------------\n\nTITLE: Terraform Deployment Commands for GCP Infrastructure\nDESCRIPTION: A series of commands to initialize Terraform, validate the configuration, and apply the infrastructure changes to create the required GCP resources for benchmarking.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/gcp/terraform/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nterraform init\nterraform validate\nterraform apply\n```\n\n----------------------------------------\n\nTITLE: Loading 3TB TPC-DS Data as Delta Tables\nDESCRIPTION: Shell command to load the full 3TB TPC-DS dataset into Delta tables. This prepares the environment for large-scale benchmark testing.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./run-benchmark.py \\\n    --cluster-hostname <HOSTNAME> \\\n    -i <PEM_FILE> \\\n    --ssh-user <SSH_USER> \\\n    --benchmark-path <BENCHMARK_PATH> \\\n    --cloud-provider <CLOUD_PROVIDER> \\\n    --benchmark tpcds-3tb-delta-load\n```\n\n----------------------------------------\n\nTITLE: Delta Table Creation SQL\nDESCRIPTION: SQL statement to create or replace a Delta table with specific checkpoint interval property set to 2.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/kernel-defaults/src/test/resources/spark-variant-checkpoint/info.txt#2025-04-22_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncreate or replace table $tableName\n  USING DELTA TBLPROPERTIES (delta.checkpointInterval = 2)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Engine Exceptions in Java for Delta Kernel\nDESCRIPTION: Example of wrapping unchecked exceptions from the Engine implementation with KernelEngineException, providing additional context for debugging. This approach is used to clearly indicate the origin of exceptions and enhance troubleshooting.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/EXCEPTION_PRINCIPLES.md#2025-04-22_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nDeltaErrors.wrapEngineException(\n    () -> engine.readFiles(inputFiles, readOptions),\n    \"Error reading input files\",\n    \"inputFiles\", inputFiles,\n    \"readOptions\", readOptions);\n```\n\n----------------------------------------\n\nTITLE: UUID-named Checkpoint Files Example\nDESCRIPTION: Example of UUID-named V2 checkpoint files for a Delta table, showing the main checkpoint file and its associated sidecar files stored in the _sidecars directory.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n00000000000000000010.checkpoint.80a083e8-7026-4e79-81be-64bd76c43a11.json\n_sidecars/3a0d65cd-4056-49b8-937b-95f9e3ee90e5.parquet\n_sidecars/016ae953-37a9-438e-8683-9a9a4a79a395.parquet\n_sidecars/7d17ac10-5cc3-401b-bd1a-9c82dd2ea032.parquet\n```\n\n----------------------------------------\n\nTITLE: Defining Table Properties for In-Commit Timestamps Enablement\nDESCRIPTION: This snippet outlines the table properties that must be set when enabling the In-Commit Timestamps feature. It specifies the properties for tracking the version and timestamp when the feature was enabled.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/accepted/in-commit-timestamps.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n4. If the table has commits from a period when this feature was not enabled, provenance information around when this feature was enabled must be tracked in table properties:\n   - The property `delta.inCommitTimestampEnablementVersion` must be used to track the version of the table when this feature was enabled.\n   - The property `delta.inCommitTimestampEnablementTimestamp` must be the same as the `inCommitTimestamp` of the commit when this feature was enabled.\n```\n\n----------------------------------------\n\nTITLE: Delta Source Console Output Example\nDESCRIPTION: Sample console output from running a Delta Source example. It shows the format of the logged data rows read from the Delta table.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/examples/flink-example/README.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\norg.utils.ConsoleSink [] - Delta table row content: f1 -> [f1_val15], f2 -> [f2_val15], f3 -> [15]\norg.utils.ConsoleSink [] - Delta table row content: f1 -> [f1_val6], f2 -> [f2_val6], f3 -> [6]\norg.utils.ConsoleSink [] - Delta table row content: f1 -> [f1_val19], f2 -> [f2_val19], f3 -> [19]\n```\n\n----------------------------------------\n\nTITLE: Example Schema with Collation Information\nDESCRIPTION: Demonstrates how collations are stored in the Delta table schema, showing the structure and metadata for string fields with different collation settings.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/collated-string-type.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"type\":\"struct\",\n   \"fields\":[\n      {\n         \"name\":\"col1\",\n         \"type\":\"string\",\n         \"metadata\":{\n            \"__COLLATIONS\":{\n               \"col1\":\"ICU.de_DE\"\n            }\n         }\n      },\n      {\n         \"name\":\"col2\",\n         \"type\":{\n            \"type\":\"array\",\n            \"elementType\":{\n               \"type\":\"map\",\n               \"keyType\":\"string\",\n               \"valueType\":{\n                  \"type\":\"struct\",\n                  \"fields\":[\n                     {\n                        \"name\":\"f1\",\n                        \"type\":\"string\",\n                        \"metadata\":{\n                           \"__COLLATIONS\":{\n                              \"f1\":\"ICU.de_DE\"\n                           }\n                        }\n                     }\n                  ]\n               }\n            }\n         },\n         \"metadata\":{\n            \"__COLLATIONS\":{\n               \"col2.element.key\":\"ICU.en_US\"\n            }\n         }\n      }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Delta Log Commit File 6\nDESCRIPTION: Sample content of commit file 00000000000000000006.json showing commit info, remove action and transaction identifier\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"commitInfo\":{...}}\n{\"remove\":{\"path\":\"f3\",...}}\n{\"txn\":{\"appId\":\"3ae45b72-24e1-865a-a211-34987ae02f2a\",\"version\":4390}}\n```\n\n----------------------------------------\n\nTITLE: Handling Checked Exceptions in Java for Delta Kernel\nDESCRIPTION: Demonstration of wrapping checked exceptions in a KernelEngineException when they cannot be directly thrown. This approach is used in scenarios where checked exceptions need to be converted to unchecked exceptions.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/EXCEPTION_PRINCIPLES.md#2025-04-22_snippet_1\n\nLANGUAGE: java\nCODE:\n```\ntry {\n    // Code that may throw a checked exception\n} catch (IOException e) {\n    throw new KernelEngineException(e);\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Non-Partitioned Delta Table with Multiple Data Types in Spark/Scala\nDESCRIPTION: This code snippet creates a Delta table with 5 rows and 10 columns of different data types. It converts a range of values (0-4) into various data types including numeric types, timestamp, string, and boolean, then writes the data to a Delta table without partitioning.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/src/test/resources/test-data/test-non-partitioned-delta-table-alltypes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(0, 5)\n.map(x => (\n    x.toByte, x.toShort, x.toInt, x.toDouble, x.toFloat, BigInt(x), BigDecimal(x), Timestamp.valueOf(java.time.LocalDateTime.now), s\"test-${x}\", true)\n    )\n.toDF(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\")\n.write\n.mode(\"append\")\n.format(\"delta\")\n.save(table)\n```\n\n----------------------------------------\n\nTITLE: Setting Java Home Environment Variable in Bash\nDESCRIPTION: Command to set the JAVA_HOME environment variable to Java 1.8 on macOS. This is a troubleshooting step to ensure the correct Java version is being used.\nSOURCE: https://github.com/delta-io/delta/blob/master/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport JAVA_HOME=`/usr/libexec/java_home -v 1.8`\n```\n\n----------------------------------------\n\nTITLE: Atomic Parquet File Writing Method Signature\nDESCRIPTION: Method for atomically writing data to a single Parquet file. Ensures complete write or no file creation.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nwriteParquetFileAtomically(String filePath, CloseableIterator<FilteredColumnarBatch> data)\n```\n\n----------------------------------------\n\nTITLE: Defining SBT Configuration for Delta Standalone Project\nDESCRIPTION: Specifies the SBT project configuration including Scala version and dependencies for Delta Standalone and Hadoop client.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_10\n\nLANGUAGE: scala\nCODE:\n```\n// <project-root>/build.sbt\n\nscalaVersion := \"2.12.8\"\n\nlibraryDependencies ++= Seq(\n  \"io.delta\" %% \"delta-standalone\" % \"0.5.0\",\n  \"org.apache.hadoop\" % \"hadoop-client\" % \"3.1.0\")\n```\n\n----------------------------------------\n\nTITLE: Statistics Schema Example\nDESCRIPTION: Illustrates the schema structure for storing column statistics with collation information, including both regular and collation-specific statistics.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/collated-string-type.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"stats\": {\n        \"numRecords\": \"long\",\n        \"tightBounds\": \"boolean\",\n        \"minValues\": {\n            \"a\": {\n                \"b\": {\n                    \"c\": \"long\"\n                }\n            }\n        },\n        \"maxValues\": {\n            \"a\": {\n                \"b\": {\n                    \"c\": \"long\"\n                }\n            }\n        },\n        \"statsWithCollation\": {\n            \"ICU.en_US.72\": {\n                \"minValues\": {\n                    \"d\": {\n                        \"e\": \"string\"\n                    }\n                },\n                \"maxValues\": {\n                    \"d\": {\n                        \"e\": \"string\"\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Development Dependencies for Delta Lake\nDESCRIPTION: Specifies the Python packages required for development work on the Delta Lake project. Includes linting tools (mypy and flake8), code formatting (black), and an optional protobuf generation plugin for Spark and Delta Connect integration. Each package has a pinned version to ensure consistency across development environments.\nSOURCE: https://github.com/delta-io/delta/blob/master/dev/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Linter\nmypy==0.982\nflake8==3.9.0\n\n# Code Formatter\nblack==23.9.1\n\n# Spark and Delta Connect python proto generation plugin (optional)\nmypy-protobuf==3.3.0\n```\n\n----------------------------------------\n\nTITLE: Java 9+ Compatibility Fix in build.sbt\nDESCRIPTION: Shows how to modify build.sbt to add JVM options that resolve IllegalAccessError issues when using Java 9 or later versions with Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/examples/scala/README.md#2025-04-22_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\nlazy val root = (project in file(\".\")\n  .settings(\n    run / fork := true,\n+   run / javaOptions ++= Seq(\n+     \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED\"\n+   ),\n```\n\n----------------------------------------\n\nTITLE: Creating a Delta Table with Variant Data Types in Spark SQL\nDESCRIPTION: This snippet creates a Delta table with variant data types including JSON strings, arrays, structs, and maps. The table is configured with a checkpoint interval of 2 to ensure checkpoint files are generated after a small number of operations.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/kernel-defaults/src/test/resources/spark-variant-stable-feature-checkpoint/_delta_log/info.txt#2025-04-22_snippet_0\n\nLANGUAGE: Scala\nCODE:\n```\nval tableName = \"<REPLACE WITH THE TABLE NAME OR PATH>\"\nval query = \"\"\"\n  with jsonStrings as (\n    select\n      id,\n      format_string('{\"key\": %s}', id) as jsonString\n    from\n      range(0, 100)\n  )\n  select\n    id,\n    parse_json(jsonString) as v,\n    array(\n      parse_json(jsonString),\n      null,\n      parse_json(jsonString),\n      null,\n      parse_json(jsonString)\n    ) as array_of_variants,\n    named_struct('v', parse_json(jsonString)) as struct_of_variants,\n    map(\n      cast(id as string),\n      parse_json(jsonString),\n      'nullKey',\n      null\n    ) as map_of_variants,\n    array(\n      named_struct('v', parse_json(jsonString)),\n      named_struct('v', null),\n      null,\n      named_struct(\n        'v',\n        parse_json(jsonString)\n      ),\n      null,\n      named_struct(\n        'v',\n        parse_json(jsonString)\n      )\n    ) as array_of_struct_of_variants,\n    named_struct(\n      'v',\n      array(\n        null,\n        parse_json(jsonString)\n      )\n    ) as struct_of_array_of_variants\n  from\n    jsonStrings\n\"\"\"\n\nval writeToTableSql = s\"\"\"\n  create or replace table $tableName\n  USING DELTA TBLPROPERTIES (delta.checkpointInterval = 2)\n\"\"\"\n\nspark.sql(s\"${writeToTableSql}\\n${query}\")\n// Write two additional rows to create a checkpoint.\n(0 until 2).foreach { v =>\n  spark\n    .sql(query)\n    .where(s\"id = $v\")\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .insertInto(tableName)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hive Metastore Connection in core-site.xml\nDESCRIPTION: This XML snippet shows the minimum configuration required in the core-site.xml file to connect to a Hive metastore. It specifies the metastore URI.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/README.md#2025-04-22_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<configuration>\n  <property>\n    <name>hive.metastore.uris</name>\n    <value>thrift://hive-metastore:9083</value>\n    <description>IP address (or fully-qualified domain name) and port of the metastore host</description>\n  </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Expression Evaluator Generation Method Signature\nDESCRIPTION: Method to create an expression evaluator for processing columnar data batches based on given schema and expression.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\ngetEvaluator(StructType batchSchema, Expression expresion, DataType outputType)\n```\n\n----------------------------------------\n\nTITLE: Creating Terraform Variables File for AWS Infrastructure\nDESCRIPTION: Example of a Terraform variables file (terraform.tfvars) used to configure the AWS infrastructure for benchmarks. It includes settings for region, availability zones, S3 buckets, database credentials, and EMR cluster configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/aws/terraform/README.md#2025-04-22_snippet_3\n\nLANGUAGE: hcl\nCODE:\n```\nregion                 = \"<REGION>\"\navailability_zone1     = \"<AVAILABILITY_ZONE1>\"\navailability_zone2     = \"<AVAILABILITY_ZONE2>\"\nbenchmarks_bucket_name = \"<BUCKET_NAME>\"\nsource_bucket_name     = \"<SOURCE_BUCKET_NAME>\"\nmysql_user             = \"<MYSQL_USER>\"\nmysql_password         = \"<MYSQL_PASSWORD>\"\nemr_public_key_path    = \"<EMR_PUBLIC_KEY_PATH>\"\nuser_ip_address        = \"<MY_IP>\"\nemr_workers            = WORKERS_COUNT\ntags                   = {\n  key1 = \"value1\"\n  key2 = \"value2\"\n}\n```\n\n----------------------------------------\n\nTITLE: Delta Table Population and Checkpoint Generation\nDESCRIPTION: Scala code that executes the table creation and performs additional writes to generate checkpoint files. Includes initial data load and two additional single-row writes.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/kernel-defaults/src/test/resources/spark-variant-checkpoint/info.txt#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nspark.sql(s\"${writeToTableSql}\\n${query}\")\n// Write two additional rows to create a checkpoint.\n(0 until 2).foreach { v =>\n  spark\n    .sql(query)\n    .where(s\"id = $v\")\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .insertInto(tableName)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark SQL with Delta and Hudi Dependencies\nDESCRIPTION: Command to launch spark-sql with required Delta dependencies and configurations. Includes Delta Spark package and custom assembly jar for Hudi integration.\nSOURCE: https://github.com/delta-io/delta/blob/master/hudi/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nspark-sql --packages io.delta:delta-spark_2.12:3.2.0-SNAPSHOT --jars delta-hudi-assembly_2.12-3.2.0-SNAPSHOT.jar --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n```\n\n----------------------------------------\n\nTITLE: Git Commit Sign-off Format for Delta Lake Contributions\nDESCRIPTION: The required format for signing off on Git commits when contributing to the Delta Lake project. Contributors must add a 'Signed-off-by' line with their real name and email to certify compliance with the Developer Certificate of Origin.\nSOURCE: https://github.com/delta-io/delta/blob/master/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\nSigned-off-by: Jane Smith <jane.smith@email.com>\nUse your real name (sorry, no pseudonyms or anonymous contributions.)\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Delta Lake Version for Documentation\nDESCRIPTION: Bash command to set an environment variable for the Delta Lake release version. This version is used in the documentation generation process.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport _DELTA_LAKE_RELEASE_VERSION_=3.3.0\n```\n\n----------------------------------------\n\nTITLE: Using Delta Lake Time Travel in PowerQuery\nDESCRIPTION: Example of how to use the Delta Lake time travel feature to read a specific version (Version 1) of a Delta table from Azure Blob Storage.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/powerbi/README.md#2025-04-22_snippet_5\n\nLANGUAGE: m\nCODE:\n```\nlet\n    Source = AzureStorage.Blobs(\"https://gbadls01.blob.core.windows.net/public\"),\n    #\"Filtered Rows\" = Table.SelectRows(Source, each Text.StartsWith([Name], \"powerbi_delta/FactInternetSales_part.delta/\")),\n    DeltaTable = fn_ReadDeltaTable(#\"Filtered Rows\", [Version=1])\nin\n    DeltaTable\n```\n\n----------------------------------------\n\nTITLE: Example Compacted Log File Content\nDESCRIPTION: Sample content of compacted log file combining actions from version 4 to 6 after reconciliation\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\"add\":{\"path\":\"f2\",...}}\n{\"add\":{\"path\":\"f4\",...}}\n{\"remove\":{\"path\":\"f1\",...}}\n{\"remove\":{\"path\":\"f3\",...}}\n{\"txn\":{\"appId\":\"3ae45b72-24e1-865a-a211-34987ae02f2a\",\"version\":4390}}\n```\n\n----------------------------------------\n\nTITLE: Exporting AWS Credentials as Environment Variables\nDESCRIPTION: Alternative method to provide AWS credentials by exporting them as environment variables for Terraform to use.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/aws/terraform/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport AWS_ACCESS_KEY_ID=\"anaccesskey\"\nexport AWS_SECRET_ACCESS_KEY=\"asecretkey\"\n```\n\n----------------------------------------\n\nTITLE: Writing Parquet Files Method Signature\nDESCRIPTION: Method for writing data into Parquet files with support for collecting column statistics. Takes a directory path, data iterator, and list of columns for statistics collection.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nwriteParquetFiles(String directoryPath, CloseableIterator<FilteredColumnarBatch> dataIter, java.util.List<Column> statsColumns)\n```\n\n----------------------------------------\n\nTITLE: Starting Spark Scala Shell with Delta Lake\nDESCRIPTION: Command to start the Spark Scala shell with Delta Lake package and configuration. This enables interactive Scala development with Delta Lake functionality.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/quick-start.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbin/spark-shell --packages io.delta:delta-spark_2.12:3.3.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n```\n\n----------------------------------------\n\nTITLE: Creating Delta Table Versions with Spark DataFrame Operations\nDESCRIPTION: Scala code that creates a non-partitioned Delta table by generating data with spark.range(), mapping to a 3-column DataFrame, and writing in append mode. This code was executed four times with different range values to create four Delta table versions.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/src/test/resources/test-data/test-non-partitioned-delta-table-4-versions/README.md#2025-04-22_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nspark.range(0, 5) \n      .map(x => (x, x % 5, s\"test-${x % 2}\"))\n      .toDF(\"col1\", \"col2\", \"col3\")\n      .write\n      .mode(\"append\")\n      .format(\"delta\")\n      .save(table)\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Delta Lake Documentation\nDESCRIPTION: Command to create a Conda environment named 'delta_docs' from the environment.yml file in the Delta Lake repository. The file path must be absolute.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda env create --name delta_docs --file=<absolute_path_to_delta_repo>/docs/environment.yml\n```\n\n----------------------------------------\n\nTITLE: Using Code Language Tabs in Markdown (Mixed Form)\nDESCRIPTION: This example shows the mixed form syntax for the 'code-language-tabs' directive in Markdown. It demonstrates how to include arbitrary content for different programming languages using the 'lang' subdirective.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/code_language_tabs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n.. code-language-tabs::\n\n    .. lang:: python\n        \n        some arbitrary python related content\n\n    .. lang:: java\n        \n        some arbitrary java related content\n\n    .. lang:: dotnet\n        \n        some arbitrary .net related content\n```\n\n----------------------------------------\n\nTITLE: Main Class Selection Output\nDESCRIPTION: Shows the available main classes that can be executed via sbt run command, including examples for Quickstart, SQL operations, and Streaming.\nSOURCE: https://github.com/delta-io/delta/blob/master/examples/scala/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nMultiple main classes detected. Select one to run:\n [1] example.Quickstart\n [2] example.QuickstartSQL\n [3] example.QuickstartSQLOnPaths\n [4] example.Streaming\n [5] example.Utilities\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Delta Test Benchmark Execution\nDESCRIPTION: Example console output showing the successful launch of a benchmark job, with details about the generated files and execution status.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/README.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n>>> Benchmark script generated and uploaded\n\n...\nThere is a screen on:\n12001..ip-172-31-21-247\t(Detached)\n\nFiles for this benchmark:\n20220126-191336-test-benchmarks.jar\n20220126-191336-test-cmd.sh\n20220126-191336-test-out.txt\n>>> Benchmark script started in a screen. Stdout piped into 20220126-191336-test-out.txt.Final report will be generated on completion in 20220126-191336-test-report.json.\n```\n\n----------------------------------------\n\nTITLE: Instantiating DeltaSqlBaseParser in Scala\nDESCRIPTION: Example of instantiating a DeltaSqlBaseParser object, which may cause errors if the parser is not found. This snippet is used to demonstrate another aspect of the setup issue.\nSOURCE: https://github.com/delta-io/delta/blob/master/README.md#2025-04-22_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval parser = new DeltaSqlBaseParser(tokenStream)\n```\n\n----------------------------------------\n\nTITLE: JSON Parsing Method Signature\nDESCRIPTION: Method for parsing JSON string vectors into structured columnar data with optional row selection.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_32\n\nLANGUAGE: Java\nCODE:\n```\nparseJson(ColumnVector jsonStringVector, StructType outputSchema, java.util.Optional<ColumnVector> selectionVector)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Credentials in Shared Credentials File\nDESCRIPTION: Example of how to set up AWS credentials in the shared credentials file (~/.aws/credentials) for Terraform to use when creating infrastructure.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/aws/terraform/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n[default]\naws_access_key_id = anaccesskey\naws_secret_access_key = asecretkey\n```\n\n----------------------------------------\n\nTITLE: Using Code Language Tabs in Markdown (Short Form)\nDESCRIPTION: This example demonstrates the short form syntax for using the 'code-language-tabs' directive in Markdown. It shows how to include code blocks for multiple programming languages within the directive.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/code_language_tabs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n.. code-language-tabs::\n\n    ```python\n    python code\n    ```\n\n    ```java\n    java code\n    ```\n\n    ```dotnet\n    dotnet code\n    ```\n```\n\n----------------------------------------\n\nTITLE: Running Quickstart Scala Example with SBT\nDESCRIPTION: Specific example showing how to run the Quickstart class from the example package using SBT.\nSOURCE: https://github.com/delta-io/delta/blob/master/examples/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./build/sbt \"runMain example.Quickstart\"\n```\n\n----------------------------------------\n\nTITLE: Displaying RFC Tables in Markdown\nDESCRIPTION: This snippet shows how to create tables in Markdown to list proposed, accepted, and rejected RFCs. It includes columns for date, RFC file, GitHub issue, and RFC title.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Date proposed | RFC file                                                                                                                         | Github issue                                  | RFC title                              |\n|:--------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------|:---------------------------------------|\n| 2023-02-14    | [managed-commits.md](https://github.com/delta-io/delta/blob/master/protocol_rfcs/managed-commits.md)                             | https://github.com/delta-io/delta/issues/2598 | Managed Commits                        |\n| 2023-02-26    | [column-mapping-usage.tracking.md](https://github.com/delta-io/delta/blob/master/protocol_rfcs/column-mapping-usage-tracking.md) | https://github.com/delta-io/delta/issues/2682 | Column Mapping Usage Tracking          |\n| 2023-04-24    | [variant-type.md](https://github.com/delta-io/delta/blob/master/protocol_rfcs/variant-type.md)                                   | https://github.com/delta-io/delta/issues/2864 | Variant Data Type                      |\n| 2024-04-30    | [collated-string-type.md](https://github.com/delta-io/delta/blob/master/protocol_rfcs/collated-string-type.md)                   | https://github.com/delta-io/delta/issues/2894 | Collated String Type                   |\n| 2025-03-13    | [checkpoint-protection.md](https://github.com/delta-io/delta/blob/master/protocol_rfcs/checkpoint-protection.md)                 | https://github.com/delta-io/delta/issues/4152 | Checkpoint Protection |\n| 2025-03-18    | [iceberg-writer-compat-v1.md](https://github.com/delta-io/delta/blob/master/protocol_rfcs/iceberg-writer-compat-v1.md)           | https://github.com/delta-io/delta/issues/4284 | IcebergWriterCompatV1                  |\n```\n\n----------------------------------------\n\nTITLE: Custom IAM Policy for EC2 Key Pair Management\nDESCRIPTION: JSON policy definition for granting permissions to manage EC2 key pairs specifically for benchmarks. This policy allows creating, importing, and deleting a specific key pair.\nSOURCE: https://github.com/delta-io/delta/blob/master/benchmarks/infrastructure/aws/terraform/README.md#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:ImportKeyPair\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\"\n      ],\n      \"Resource\": \"arn:aws:ec2:*:*:key-pair/benchmarks_key_pair\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Importing DeltaSqlBaseParser in Scala\nDESCRIPTION: Example of import statement causing errors due to missing DeltaSqlBaseParser object. This snippet is used to illustrate a common issue during setup.\nSOURCE: https://github.com/delta-io/delta/blob/master/README.md#2025-04-22_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport io.delta.sql.parser.DeltaSqlBaseParser._\n```\n\n----------------------------------------\n\nTITLE: Resolving Paths with FileSystemClient in Delta IO Java\nDESCRIPTION: The FileSystemClient.resolvePath method now handles non-existent paths differently. It no longer throws a FileNotFoundException, instead resolving the given path into a fully qualified path regardless of whether the path exists or not.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-kernel-java.md#2025-04-22_snippet_43\n\nLANGUAGE: Java\nCODE:\n```\nFileSystemClient.resolvePath(\"path/to/resolve\")\n```\n\n----------------------------------------\n\nTITLE: Running Delta Source Example with SBT\nDESCRIPTION: SBT commands to set up environment variables and run a Delta Source example. It shows how to specify the connector version and additional Maven repositories.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/examples/flink-example/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/\nexport STANDALONE_VERSION=x.y.z  # update to desired version\nexport EXTRA_MAVEN_REPO={staged_repo}  # include staged repo if desired\n\nbuild/sbt \"flinkExample/runMain org.example.source.bounded.DeltaBoundedSourceExample\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Language Tabs Extension in Sphinx\nDESCRIPTION: This snippet shows how to enable the 'code-language-tabs' extension in the Sphinx configuration file (conf.py). It adds the extension to the list of enabled extensions.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/code_language_tabs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nextensions = ['code_language_tabs']\n```\n\n----------------------------------------\n\nTITLE: Implementing ExpressionHandler.getPredicateEvaluator Method in Delta Kernel\nDESCRIPTION: Method for creating a PredicateEvaluator for boolean predicate expressions. Returns a selection vector indicating which rows meet the predicate condition.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_24\n\nLANGUAGE: Java\nCODE:\n```\ngetPredicateEvaluator(StructType inputSchema, Predicate predicate)\n```\n\n----------------------------------------\n\nTITLE: Using Metadata Extension in Markdown\nDESCRIPTION: Shows how to use substitution references in document metadata when writing documentation in Markdown format. The <DEFINITION> placeholder will be replaced with its defined value.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/metadata/README.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n    ---\n    title: Title text with <DEFINITION> included\n    ---\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx for Markdown Support in conf.py\nDESCRIPTION: Configuration settings to enable Markdown support in Sphinx documentation by setting the source suffix mapping and adding the markdown_reader extension.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/markdown_reader/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsource_suffix = {'.md': 'markdown'}\nextensions = ['markdown_reader']\n```\n\n----------------------------------------\n\nTITLE: Documenting Writer Requirements in Markdown\nDESCRIPTION: Defines the requirements for writers when working with tables that have checkpointProtection enabled, including rules for checkpoint cleanup, creation, and version history management.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/checkpoint-protection.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Writer Requirements for Checkpoint Protection\n\nFor tables with `checkpointProtection` supported in the protocol:\n\na) Writers must not clean up any checkpoints for table versions before the version given by table property `delta.requireCheckpointProtectionBeforeVersion`.\n\nb) Writers must not create new checkpoints for table versions before the version given by table property `delta.requireCheckpointProtectionBeforeVersion` unless they support all of the features in the table protocol at that version.\n\nc) Writers must not clean up version history for table versions for which they do not support the protocol. A writer is allowed to clean up a range of versions if it supports all table features for every version that is being cleaned up. If a writer does not support the protocol for some of the versions that are being cleaned up, then the cleanup is allowed if and only if the cleanup includes _all_ table versions before the version given by  `delta.requireCheckpointProtectionBeforeVersion`. In this case, a single cleanup operation should truncate the history up to that boundary version in one go as opposed to several cleanup operations truncating in chunks.\n\nd) In version history cleanup, writers must remove commits _before_ removing the associated checkpoints, so that requirement (a) is satisfied even during the cleanup.\n```\n\n----------------------------------------\n\nTITLE: Implementing JsonHandler.readJsonFiles Method in Delta Kernel\nDESCRIPTION: Method for reading JSON files into columnar format. Takes file statuses, physical schema, and optional predicate as inputs, returning data as columnar batches.\nSOURCE: https://github.com/delta-io/delta/blob/master/kernel/USER_GUIDE.md#2025-04-22_snippet_26\n\nLANGUAGE: Java\nCODE:\n```\nreadJsonFiles(CloseableIterator<FileStatus> fileIter, StructType physicalSchema, java.util.Optional<Predicate> predicate)\n```\n\n----------------------------------------\n\nTITLE: Running Sphinx Build with Custom Code Language Tabs Mode\nDESCRIPTION: This command demonstrates how to run the Sphinx build process with a custom output mode for the 'code-language-tabs' extension. It sets the mode to 'markdown' using the command-line option.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/shared/extensions/code_language_tabs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-build -D code-language-tabs-mode=markdown <other_options>\n```\n\n----------------------------------------\n\nTITLE: Documenting Delta Table Feature in Markdown\nDESCRIPTION: This snippet provides a template structure for documenting a new Delta table feature. It includes sections for the feature name, associated GitHub issue, general description, and proposed protocol modifications.\nSOURCE: https://github.com/delta-io/delta/blob/master/protocol_rfcs/template.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Table feature name / meaningful name\n**Associated Github issue for discussions: https://github.com/delta-io/delta/issues/XXXX**\n<!-- Remove this: Replace XXXX with the actual github issue number -->\n\n\n<!--  Give a general description / context of the protocol change, and remove this comment. -->\n\n--------\n\n<!-- Remove this: Add your proposed protocol.md modifications here, and remove this comment. -->\n```\n\n----------------------------------------\n\nTITLE: PlantUML File Example Instructions\nDESCRIPTION: Instructions for creating and updating PlantUML diagram files. This includes guidance on file creation, syntax usage, and diagram generation using IntelliJ IDEA's PlantUML plugin and Graphviz library.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/flink/uml/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n# Create or update .puml files\n# Follow PlantUML class diagram syntax\n# Save with .puml extension\n# Use IntelliJ PlantUML plugin to view and export\n```\n\n----------------------------------------\n\nTITLE: Developer Certificate of Origin for Delta Lake Contributions\nDESCRIPTION: The Developer Certificate of Origin (DCO) text that contributors must agree to when submitting contributions to the Delta Lake project. This certifies that contributors have the right to submit their code under the project's open source license.\nSOURCE: https://github.com/delta-io/delta/blob/master/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\nDeveloper Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n```\n```\n\n----------------------------------------\n\nTITLE: Defining reStructuredText Substitutions for Delta Lake and Apache Spark\nDESCRIPTION: Sets up reStructuredText substitution variables for 'Delta Lake' and 'Apache Spark' to be used throughout the documentation for consistent naming.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/shared/replacements.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. <Delta> replace:: Delta Lake\n.. <AS> replace:: Apache Spark\n```\n\n----------------------------------------\n\nTITLE: Streaming Upsert Implementation in Python\nDESCRIPTION: Python implementation of streaming upserts using Delta table merge operation. The code shows how to perform upserts into a Delta table using foreachBatch with streaming aggregation queries, including merge operation handling for both matched and unmatched records.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, \"/data/aggregates\")\n\n# Function to upsert microBatchOutputDF into Delta table using merge\ndef upsertToDelta(microBatchOutputDF, batchId):\n  deltaTable.alias(\"t\").merge(\n      microBatchOutputDF.alias(\"s\"),\n      \"s.key = t.key\") \\\n    .whenMatchedUpdateAll() \\\n    .whenNotMatchedInsertAll() \\\n    .execute()\n}\n\n# Write the output of a streaming aggregation query into Delta table\nstreamingAggregatesDF.writeStream \\\n  .format(\"delta\") \\\n  .foreachBatch(upsertToDelta) \\\n  .outputMode(\"update\") \\\n  .start()\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Boilerplate Notice Template\nDESCRIPTION: Standard boilerplate notice text to be included in files when applying the Apache License 2.0 to a project. The template includes placeholders for copyright year and owner that should be replaced with actual information.\nSOURCE: https://github.com/delta-io/delta/blob/master/connectors/licenses/LICENSE-apache-spark.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Z-Ordering Data with OPTIMIZE in Python\nDESCRIPTION: Demonstrates how to perform Z-Ordering in Delta Lake using the Python API. Z-Ordering collocates related data in the same files to improve query performance through data skipping, with support for optional partition filtering.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)  # path-based table\n# For Hive metastore-based tables: deltaTable = DeltaTable.forName(spark, tableName)\n\ndeltaTable.optimize().executeZOrderBy(eventType)\n\n# If you have a large amount of data and only want to optimize a subset of it, you can specify an optional partition predicate using `where`\ndeltaTable.optimize().where(\"date='2021-11-18'\").executeZOrderBy(eventType)\n```\n\n----------------------------------------\n\nTITLE: Implementing Inline Deletion Vector\nDESCRIPTION: JSON example showing an inline Deletion Vector configuration that encodes row indexes 3, 4, 7, 11, 18, 29.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_24\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"storageType\" : \"i\",\n  \"pathOrInlineDv\" : \"wi5b=000010000siXQKl0rr91000f55c8Xg0@@D72lkbi5=-{L\",\n  \"sizeInBytes\" : 40,\n  \"cardinality\" : 6\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Materialized Row ID Column in Delta Lake\nDESCRIPTION: Demonstrates how to specify the column name for materialized Row IDs in the table's metadata configuration.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_29\n\nLANGUAGE: markdown\nCODE:\n```\nThe column name for the materialized Row IDs and Row Commit Versions have been assigned and added to the `configuration` in the table's `metaData` action using the keys `delta.rowTracking.materializedRowIdColumnName` and `delta.rowTracking.materializedRowCommitVersionColumnName` respectively.\n```\n\n----------------------------------------\n\nTITLE: Classic Checkpoint File Example\nDESCRIPTION: Example of a classic checkpoint file for a Delta table, showing the filename format with the version number padded to 20 digits.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n00000000000000000010.checkpoint.parquet\n```\n\n----------------------------------------\n\nTITLE: Multi-part Checkpoint Files Example\nDESCRIPTION: Example of multi-part checkpoint files for a Delta table, showing the filename format for a version with three parts. Note that this format is deprecated in favor of UUID-named V2 checkpoints.\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n00000000000000000010.checkpoint.0000000001.0000000003.parquet\n00000000000000000010.checkpoint.0000000002.0000000003.parquet\n00000000000000000010.checkpoint.0000000003.0000000003.parquet\n```\n\n----------------------------------------\n\nTITLE: Example Delta Log Commit File 4\nDESCRIPTION: Sample content of commit file 00000000000000000004.json showing commit info, add and remove actions\nSOURCE: https://github.com/delta-io/delta/blob/master/PROTOCOL.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\"commitInfo\":{...}}\n{\"add\":{\"path\":\"f2\",...}}\n{\"remove\":{\"path\":\"f1\",...}}\n```\n\n----------------------------------------\n\nTITLE: Writing Updated Parquet Data with Zappy Engine in Java\nDESCRIPTION: Shows how to write updated Parquet data using a fictitious Zappy engine, as Delta Standalone does not provide data-writing APIs.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-standalone.md#2025-04-22_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nZappyDataFrame correctedSaleIdToTotalCost = ...;\n\nZappyDataFrame invalidSales = ZappyReader.readParquet(filteredFiles);\nZappyDataFrame correctedSales = invalidSales.join(correctedSaleIdToTotalCost, \"id\");\n\nZappyWriteResult dataWriteResult = ZappyWritter.writeParquet(\"/data/sales\", correctedSales);\n```\n\n----------------------------------------\n\nTITLE: Updating All Columns in Delta Lake Merge Operation (Scala)\nDESCRIPTION: Demonstrates how to update all columns of the target Delta table with corresponding columns from the source dataset using whenMatched(...).updateAll(). This is equivalent to updating each column individually.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/delta-update.md#2025-04-22_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nwhenMatched(...).updateExpr(Map(\"col1\" -> \"source.col1\", \"col2\" -> \"source.col2\", ...))\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimized Writes in Delta Lake (SQL)\nDESCRIPTION: Sets the SQL configuration to enable optimized writes in Delta Lake. This can be set at the table, SQL session, or DataFrameWriter level.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\ndelta.autoOptimize.optimizeWrite\n```\n\nLANGUAGE: sql\nCODE:\n```\nspark.databricks.delta.optimizeWrite.enabled\n```\n\nLANGUAGE: sql\nCODE:\n```\noptimizeWrite\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Optimized Writes in Delta Lake (SQL)\nDESCRIPTION: Advanced SQL configurations for fine-tuning the number and size of files written during optimized writes in Delta Lake.\nSOURCE: https://github.com/delta-io/delta/blob/master/docs/source/optimizations-oss.md#2025-04-22_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nspark.databricks.delta.optimizeWrite.binSize\n```\n\nLANGUAGE: sql\nCODE:\n```\nspark.databricks.delta.optimizeWrite.numShuffleBlocks\n```\n\nLANGUAGE: sql\nCODE:\n```\nspark.databricks.delta.optimizeWrite.maxShufflePartitions\n```"
  }
]
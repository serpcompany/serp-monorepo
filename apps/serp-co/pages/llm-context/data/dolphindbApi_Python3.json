[
  {
    "owner": "dolphindb",
    "repo": "api_python3",
    "content": "TITLE: Establishing a DolphinDB connection using Python API\nDESCRIPTION: This code demonstrates establishing a session with the DolphinDB server by creating a session object, connecting to the server with hostname and port, and then closing the session. It shows the basic process of session creation, connection, and termination.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(\"localhost\", 8848)\n# output\nTrue\n\ns.close()   # 关闭会话\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB Session with Connection Parameters in Python\nDESCRIPTION: Demonstrates how to create a DolphinDB session by specifying the server host, port, and optionally user credentials. These parameters enable the client to connect and log in to the DolphinDB server. The snippet shows default connection using 'localhost' and port 8848, and also illustrates specifying user credentials for authentication.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 创建session，同时连接地址为localhost，端口为8848的DolphinDB\ns = ddb.session(\"localhost\", 8848)\n\n# 创建session，同时连接地址为localhost，端口为8848的DolphinDB，登录用户名为admin，密码为123456的账户\ns = ddb.session(\"localhost\", 8848, \"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Connecting and Closing DolphinDB Session (Python)\nDESCRIPTION: Demonstrates how to establish a connection to a DolphinDB server using the `dolphindb.session()` class and its `connect()` method, and subsequently how to close the session using the `close()` method. Requires the `dolphindb` library and a running DolphinDB server instance at the specified address and port. The `connect` method returns True on success.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(\"localhost\", 8848)\n# output\nTrue\n\ns.close()  #close session\n```\n\n----------------------------------------\n\nTITLE: Connecting to DolphinDB server with authentication and high availability\nDESCRIPTION: This snippet illustrates how to connect to DolphinDB with user credentials and optional high availability setup. It includes variables for hostname, port, user ID, password, and an example of high availability node list, facilitating secure and load-balanced connections.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\nsites=[\"192.168.1.2:24120\", \"192.168.1.3:24120\", \"192.168.1.4:24120\"]\ns.connect(host=\"192.168.1.2\", port=24120, userid=\"admin\", password=\"123456\", highAvailability=True, highAvailabilitySites=sites)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to streaming data tables in DolphinDB Python API\nDESCRIPTION: Subscribes to a DolphinDB streaming data table using the subscribe method with extensive configuration options including host, port, callback handler, subscription table and action names, offset, filter conditions, batch processing, and serialization choices. The method allows precise control of subscription behavior such as starting offset (default, persisted, or current), automatic resubscription on disconnect, filtering by data columns, and controlling message batching and throttling. A user-defined callback function processes each batch or message. The snippet also shows a minimal example of a callback handler printing received messages. Required dependencies include the dolphindb Python package and optionally numpy for filter arrays.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.4_Subscription/2.4_Subscription.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef handler(msg):\n    print(msg)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.subscribe(host, port, handler, tableName, actionName=None, offset=-1, resub=False, \n          filter=None, msgAsTable=False, batchSize=0, throttle=1.0,\n          userName=None, password=None, streamDeserializer=None)\n```\n\n----------------------------------------\n\nTITLE: Running DolphinDB scripts from Python\nDESCRIPTION: Utilizes the 'run' method to execute DolphinDB scripts or queries; returns the result as a Python object if there is an output. Supports multi-line scripts with triple quotes for improved readability.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848)\na=s.run(\"`IBM`GOOG`YHOO\")\nrepr(a)\n\n# output\n\"array(['IBM','GOOG','YHOO'], dtype=object)\"\n\n# Running a custom function\ns.run(\"def getTypeStr(input){ \\nreturn typestr(input)\\}\")\n```\n\n----------------------------------------\n\nTITLE: Example of subscribing to filtered streaming data with callback and offset in DolphinDB Python API\nDESCRIPTION: Demonstrates a typical usage scenario where streaming is enabled, a callback handler is defined to print received data, and a subscription is initiated to a DolphinDB streaming table from a specified host and port. The subscription filters data to only include rows with symbol \"000905\", starts from the current end of the table (offset -1), and ensures the main thread does not exit by blocking with an Event. This includes batch processing and offset considerations to control which data events are received and handled. Dependencies include the dolphindb Python client, numpy for filter arguments, and threading for blocking the main thread.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.4_Subscription/2.4_Subscription.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\ns.enableStreaming(0) # DolphinDB 的版本为 1.30.x 或小于 2.00.9 时，需指定端口\n\ndef handler(lst):\n    print(lst)\n\ns.subscribe(\"192.168.1.113\", 8848, handler, \"trades\", \"action\", offset=-1, filter=np.array([\"000905\"]))\n\nfrom threading import Event\nEvent().wait()          # 阻塞主线程，保持进程不退出\n```\n\n----------------------------------------\n\nTITLE: Establishing DolphinDB Connection with Python API\nDESCRIPTION: Demonstrates creating a DolphinDB session and connecting to a server using host and port, with optional user credentials. Also shows how to login separately if session expires or credentials are not provided at initial connection.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.2_Connect.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\n\n# Connect to DolphinDB on localhost with default port 8848\ns.connect(\"localhost\", 8848)\n\n# Connect with user credentials\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n# If session expires or no login info at connect, login separately\ns.connect(\"localhost\", 8848)\n# Login after connection\ns.login(\"admin\",\"123456\")\n```\n\n----------------------------------------\n\nTITLE: Thread-Safe Data Insertion into DolphinDB MTW from Multiple Python Threads\nDESCRIPTION: This example demonstrates creating a MultithreadedTableWriter (MTW) object and using a Python threading.Thread to insert data concurrently. The insert_MTW function writes 100 randomly generated records. After starting the client thread, the main thread waits for its completion, then waits for MTW worker threads to finish via waitForThreadCompletion. Dependencies: dolphindb Python API, threading module, random module. Inputs: None directly, uses random values. Outputs: Write status and database row count. Constraints: Proper synchronization and finalization of threads are demonstrated for safe parallel usage.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_90\n\nLANGUAGE: Python\nCODE:\n```\n# 创建MTW对象\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\n\ndef insert_MTW(writer):\n    try:\n        # 插入100行正确数据 \n        for i in range(100):\n            res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\n    except Exception as ex:\n        # MTW 抛出异常\n        print(\"MTW exit with exception %s\" % ex)\n\n# 创建线程，在线程中将数据写入MTW\nthread=threading.Thread(target=insert_MTW, args=(writer,))\nthread.setDaemon(True)\nthread.start()\n# 完成其他任务，此处用 sleep 模拟\ntime.sleep(1)\n\n# 现在需要结束任务\n# 1 - 等待线程退出\nthread.join()\n# 2 - 结束MTW工作线程\nwriter.waitForThreadCompletion()\n# 3 - 检查输出结果\nwriteStatus=writer.getStatus()\nprint(\"writeStatus:\\n\", writeStatus)\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Standard workflow for MultithreadedTableWriter in Python\nDESCRIPTION: Shows a best-practice workflow pattern for using MTW, including error handling, status checking, and data recovery. This example demonstrates how to handle unwritten data and recover from errors.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# 准备数据 data\nprepre_Data()\n# 构造 MTW 对象\nwriter = ddb.MultithreadedTableWriter(...)\n\ntry:\n    for data in datas:\n        # 插入数据\n        res = writer.insert(data)\n        # 判断数据是否成功放入转换队列\n        if res.hasError():\n            print(res.errorInfo)\n    # 等待 MTW 工作完成\n    writer.waitForThreadCompletion()\nexcept Exception as e:\n    # 获取 MTW 工作状态，通过 writeStatus.hasError() 和 writeStatus.succeed() 判断是否正常执行完成\n    writeStatus = writer.getStatus()\n    print(e)\n    if writeStatus.hasError():\n        # 获取并修正失败数据后将失败数据重新写入MTW\n        print(writeStatus)\n        unwrittendata = writer.getUnwrittenData()\n        # 调用预先定义的修正函数来修复错误数据\n        unwrittendata = revise(unwrittendata)\n        newwriter = ddb.MultithreadedTableWriter(...)\n        newwriter.insertUnwrittenData(unwrittendata)\n```\n\n----------------------------------------\n\nTITLE: Streaming Subscription: Batch Mode (msgAsTable=True)\nDESCRIPTION: This snippet shows a batch subscription with `msgAsTable=True`. It connects to the DolphinDB server, creates a stream table, enables streaming, defines a handler and subscribes to the stream. The handler processes messages in batches (using the `batchSize` and `throttle` parameters). When `msgAsTable=True`, the handler receives data in a DataFrame. After the subscription is set up, the script inserts data into the stream table. After 3 seconds, it unsubscribes.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.3_SubscriptionOptions/3.3_SubscriptionOptions.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport time\n\ns = ddb.session()\ns.connect(\"192.168.1.113\", 8848, \"admin\", \"123456\")\n\ns.run(\"\"\"\nshare streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,DOUBLE,INT]) as trades\n\"\"\")\n\ns.enableStreaming()\n\ndef handler(lsts):\n    print(lsts)\n\ns.subscribe(\"192.168.1.113\", 8848, handler, \"trades\", \"MultiMode2\", offset=-1, batchSize=1000, throttle=0.1, msgAsTable=True)\n\ns.run(\"n=1500;insert into trades values(take(now(), n), take(\\`000905\\`600001\\`300201\\`000908\\`600002, n), rand(1000,n)/10.0, 1..n)\")\n\ntime.sleep(3)\n\ns.unsubscribe(\"192.168.1.113\", 8848, \"trades\", \"MultiMode2\")\n```\n\n----------------------------------------\n\nTITLE: Standard Workflow for MultithreadedTableWriter in Python\nDESCRIPTION: A common workflow pattern for using MultithreadedTableWriter, including data insertion, waiting for completion, status checking, and handling failed writes by retrieving unwritten data and resubmitting it after correction.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_84\n\nLANGUAGE: Python\nCODE:\n```\n# 准备数据 data\n# 插入数据或其他任务\nwriter.insert(data)\n...\n# 等待 MTW 工作完成\nwriter.waitForThreadCompletion()\n# 获取 MTW 工作状态，通过 writeStatus.hasError() 和 writeStatus.succeed() 判断是否正常执行完成\nwriteStatus=writer.getStatus()\nif writeStatus.hasError():\n    # 获取并修正失败数据后将失败数据重新写入MTW\n    unwrittendata = writer.getUnwrittenData()\n    unwrittendata = revise(unwrittendata)\n    newwriter.insertUnwrittenData(unwrittendata)\nelse\n    print(\"Write successfully!\")\n```\n\n----------------------------------------\n\nTITLE: Writing Stream Data to Shared Streaming Table Asynchronously with DolphinDB Python API\nDESCRIPTION: This example sets up a shared streaming table on the DolphinDB server via an inline script, then uses Python to asynchronously insert large volumes of randomly generated streaming data into that table. The session is configured for asynchronous mode. The snippet includes generating a pandas DataFrame with date, symbol, price, and ID columns, then repeatedly calling session.run() to insert the data without waiting for server responses. It demonstrates improvement in throughput by avoiding synchronous network waits.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.1_SessionAsyncMode.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport datetime\n\ns = ddb.session(enableASYNC=True)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nn = 100\n\nscript = \"\"\"\n    share streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,DOUBLE,INT]) as trades\n\"\"\"\ns.run(script) # 此处的脚本可以在服务端直接运行\n\n# 生成一个 DataFrame\ntime_list = [np.datetime64(datetime.date(2020, random.randint(1, 12), random.randint(1, 20))) for _ in range(n)]\nsym_list = np.random.choice(['IBN', 'GTYU', 'FHU', 'DGT', 'FHU', 'YUG', 'EE', 'ZD', 'FYU'], n)\nprice_list = [round(np.random.uniform(1, 100), 1) for _ in range(n)]\nid_list = np.random.choice([1, 2, 3, 4, 5], n)\n\ntb = pd.DataFrame({\n    'time': time_list,\n    'sym': sym_list,\n    'price': price_list,\n    'id': id_list,\n})\n\nfor _ in range(50000):\n    s.run(\"tableInsert{trades}\", tb)\n```\n\n----------------------------------------\n\nTITLE: Uploading Python List to DolphinDB\nDESCRIPTION: This snippet demonstrates uploading a Python list to a DolphinDB server using the `upload` method. The uploaded list becomes a variable in the DolphinDB session. The list's type will become ANY VECTOR if containing multiple data types.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\na = [1,2,3.0]\ns.upload({'a':a})\na_new = s.run(\"a\")\nprint(a_new)\n# output\n[1, 2, 3.0]\n\na_type = s.run(\"typestr(a)\")\nprint(a_type)\n# output\nANY VECTOR\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic DBConnectionPool in Python\nDESCRIPTION: Demonstrates initializing a `DBConnectionPool` by specifying the mandatory host and port. It also shows an example including user credentials (userid, password) and a specific number of connections (threadNum). Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 连接地址为localhost，端口为8848的DolphinDB，连接数为10\npool = ddb.DBConnectionPool(\"localhost\", 8848)\n\n# 连接地址为localhost，端口为8848的DolphinDB，登录用户名为admin，密码为123456的账户，连接数为8\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, \"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Using tableAppender to Append DataFrame Data to DolphinDB Table\nDESCRIPTION: This snippet demonstrates the creation of a DolphinDB session, table creation, instantiation of a tableAppender object, and appending a pandas DataFrame to the table. It highlights automatic type conversion for timestamp columns and verifies the insertion through querying the table and its schema.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.1_TableAppender.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ns.run(\"share table(1000:0, `sym`timestamp`qty, [SYMBOL, TIMESTAMP, INT]) as t\")\nappender = ddb.tableAppender(tableName=\"t\", ddbSession=s)\ndata = pd.DataFrame({\n    'sym': ['A1', 'A2', 'A3', 'A4', 'A5'], \n    'timestamp': np.array(['2012-06-13 13:30:10.008', 'NaT','2012-06-13 13:30:10.008', '2012-06-13 15:30:10.008', 'NaT'], dtype=\"datetime64[ms]\"), \n    'qty': np.arange(1, 6).astype(\"int32\"),\n})\nnum = appender.append(data)\nprint(\"append rows: \", num)\nt = s.run(\"t\")\nprint(t)\nschema = s.run(\"schema(t)\")\nprint(schema[\"colDefs\"])\n\n```\n\n----------------------------------------\n\nTITLE: Using DolphinDB DBConnectionPool for Concurrent Script Execution in Python\nDESCRIPTION: This snippet illustrates usage of the DolphinDB Python API's DBConnectionPool to enable concurrent execution of DolphinDB scripts across multiple threads asynchronously. It initializes a connection pool with 20 threads to a local DolphinDB server, then defines an asynchronous task `test_run` which runs a sleep command with a simple arithmetic operation via the pool. Multiple asyncio tasks are created and executed simultaneously inside an event loop, and results of all tasks are printed. The approach allows parallel execution of DolphinDB scripts with independent sessions managed by the pool. Dependencies include dolphindb, asyncio, and standard threading and datetime libraries. This method overcomes the serial execution limitation of `session.run` by distributing workload across pooled connections. It requires Python 3.7+ for asyncio support and proper DolphinDB server setup.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport datetime\nimport time\nimport asyncio\nimport threading\nimport sys\nimport numpy\nimport pandas\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, 20)\n\n# define a task function and simulate the runtime with function sleep\nasync def test_run():\n    try:\n        return await pool.run(\"sleep(1000);1+2\")\n    except Exception as e:\n        print(e)\n\n# define the tasks\ntasks = [\n    asyncio.ensure_future(test_run()),\n    asyncio.ensure_future(test_run()),\n    asyncio.ensure_future(test_run()),\n    asyncio.ensure_future(test_run()),\n]\n\n# create an event loop to run the tasks until all tasks are completed\nloop = asyncio.get_event_loop()\ntry:\n    loop.run_until_complete(asyncio.wait(tasks))\nexcept Exception as e:\n    print(\"catch e:\")\n    print(e)\n\nfor i in tasks:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Initializing Data Transfer Protocol in DolphinDB Python API - Python\nDESCRIPTION: This snippet demonstrates how to create a DolphinDB session in Python with a user-specified data serialization protocol. The example shows selecting among PROTOCOL_DDB, PROTOCOL_PICKLE, and PROTOCOL_ARROW, which are defined in the dolphindb.settings module. To use this code, the dolphindb package (version 1.30.21.1 or newer) must be installed. The session object s will communicate with the DolphinDB server using the selected serialization protocol for all supported data types and forms. 'protocol' is an optional keyword argument that configures how data is transferred between Python and DolphinDB.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.0_TypeCasting.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n# 使用协议 PROTOCOL_DDB\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\n# 使用协议 PROTOCOL_PICKLE\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\n# 使用协议 PROTOCOL_ARROW\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using DBConnectionPool with Coroutines in Python\nDESCRIPTION: A basic example showing how to create a connection pool and execute multiple tasks concurrently using Python's asyncio with the run method. Each task is executed asynchronously and we measure the total execution time.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.2_AsyncMethodsAndOthers.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport time\nimport asyncio\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8)\n```\n\nLANGUAGE: python\nCODE:\n```\nasync def test_run(i):\n    try:\n        return await pool.run(f\"sleep(2000);1+{i}\")\n    except Exception as e:\n        print(e)\n```\n\nLANGUAGE: python\nCODE:\n```\ntasks = [\n    asyncio.ensure_future(test_run(1)),\n    asyncio.ensure_future(test_run(3)),\n    asyncio.ensure_future(test_run(5)),\n    asyncio.ensure_future(test_run(7)),\n]\n\nloop = asyncio.get_event_loop()\ntry:\n    time_st = time.time()\n    loop.run_until_complete(asyncio.wait(tasks))\n    time_ed = time.time()\nexcept Exception as e:\n    print(\"catch e:\")\n    print(e)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(\"time: \", time_ed-time_st)\n\nfor task in tasks:\n    print(task.result())\n\npool.shutDown()\n```\n\n----------------------------------------\n\nTITLE: Uploading NumPy Array to DolphinDB\nDESCRIPTION: This snippet demonstrates uploading a NumPy array to a DolphinDB server using the `upload` method. The uploaded NumPy array becomes a variable in the DolphinDB session. Ensure that the data type is consistent.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\narr = np.array([1,2,3.0],dtype=np.double)\ns.upload({'arr':arr})\narr_new = s.run(\"arr\")\nprint(arr_new)\n# output\n[1. 2. 3.]\n\narr_type = s.run(\"typestr(arr)\")\nprint(arr_type)\n# output\nFAST DOUBLE VECTOR\n```\n\n----------------------------------------\n\nTITLE: Configuring Serialization Protocols in DolphinDB Python API - Python\nDESCRIPTION: Demonstrates how to create a DolphinDB session in Python and specify the desired serialization protocol (native DDB, Pickle, or Arrow) via the protocol argument using settings from dolphindb.settings. No additional dependencies beyond dolphindb are required. This configuration determines the data encoding used for client-server communication, affecting type compatibility and performance.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\n```\n\n----------------------------------------\n\nTITLE: Streaming Subscription: Heterogeneous Table\nDESCRIPTION: This snippet demonstrates a heterogeneous stream table subscription. It uses a `streamDeserializer` to handle data from multiple tables with different schemas. It first defines the output table in DolphinDB. It then defines two partitioned tables as inputs. The data from partitioned tables are replayed to a stream table with the replayDS function. The Python code then defines the stream deserializer, connects to the DolphinDB server, and subscribes to the stream. The handler function distinguishes data from different source tables using the keys specified in `sym2table` within the `streamDeserializer`. The `msgAsTable` parameter should be false, and the handler receives a list of data, with an extra column identifying the data's origin.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.3_SubscriptionOptions/3.3_SubscriptionOptions.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 异构流表反序列化器返回的数据末尾为异构流表反序列化器中 sym2table 指定的 key\ndef streamDeserializer_handler(lst):\n    if lst[-1]==\"msg1\":\n        print(\"Msg1: \", lst)\n    elif lst[-1]=='msg2':\n        print(\"Msg2: \", lst)\n    else:\n        print(\"Error: \", lst)\n\ns = ddb.session()\ns.connect(\"192.168.1.113\", 8848, \"admin\", \"123456\")\ns.enableStreaming()\n\n# 填入分区表数据库路径和表名的 list，以获取对应表结构\nsd = ddb.streamDeserializer({\n    'msg1': [\"dfs://test_StreamDeserializer_pair\", \"pt1\"],\n    'msg2': [\"dfs://test_StreamDeserializer_pair\", \"pt2\"],\n}, session=s)\ns.subscribe(host=\"192.168.1.113\", port=8848, handler=streamDeserializer_handler, tableName=\"outTables\", actionName=\"action\", offset=0, resub=False,\n            msgAsTable=False, streamDeserializer=sd, userName=\"admin\", password=\"123456\")\n\nfrom threading import Event\nEvent().wait()\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with Chained WHERE Conditions in Python\nDESCRIPTION: Loads data from a CSV, selects specific columns, applies multiple filtering conditions sequentially using chained `where()` calls, sorts the result by 'vol' descending using `sort()`, and executes the query on the server, saving the result to a server variable 't1' via `executeAs()`. The proxy object `t1` is then used to fetch the data as a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_116\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\n\n# use chaining WHERE conditions and save result to DolphinDB server variable \"t1\" through function \"executeAs\"\nt1=trade.select(['date','bid','ask','prc','vol']).where('TICKER=`AMZN').where('bid!=NULL').where('ask!=NULL').where('vol>10000000').sort('vol desc').executeAs(\"t1\")\nprint(t1.toDF())\n# output\n#          date    bid      ask     prc        vol\n# 0  2007.04.25  56.80  56.8100  56.810  104463043\n# 1  1999.09.29  80.75  80.8125  80.750   80380734\n# ...\nprint(t1.rows)\n# output\n# 765\n```\n\n----------------------------------------\n\nTITLE: Session Constructor Signature for DolphinDB Python API\nDESCRIPTION: Shows the full session class constructor signature with all optional parameters and their default values. This highlights the customizable options available when creating a session instance for connecting and interacting with DolphinDB servers, including host, port, user credentials, SSL, async mode, compression, protocol selection, keepAlive timings, chunk granularity, and display output options.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsession(host=None, port=None, userid=\"\", password=\"\",\n        enableSSL=False, enableASYNC=False,\n        keepAliveTime=30, enableChunkGranularityConfig=False, compress=False,\n        enablePickle=None, protocol=PROTOCOL_DEFAULT,\n        python=False, show_output=True)\n```\n\n----------------------------------------\n\nTITLE: Selecting Data Protocol for DolphinDB Python Session\nDESCRIPTION: Shows configuring the data serialization protocol between DolphinDB and the Python API using either enablePickle (legacy) or protocol (recommended for versions >=1.30.21.1). The protocols supported include PROTOCOL_PICKLE, PROTOCOL_DDB, and PROTOCOL_ARROW, affecting the format of data returned by DolphinDB scripts. Avoid setting both parameters simultaneously to prevent conflicts.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# 使用协议 PROTOCOL_PICKLE\ns = ddb.session(enablePickle=True)\n\n# 使用协议 PROTOCOL_DDB\ns = ddb.session(enablePickle=False)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\n# 使用协议 PROTOCOL_DDB\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\n\n# 使用协议 PROTOCOL_PICKLE\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\n\n# 使用协议 PROTOCOL_ARROW\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\n```\n\n----------------------------------------\n\nTITLE: Retrying Failed Writes with DolphinDB MultithreadedTableWriter in Python\nDESCRIPTION: Shows how to recover and re-insert data that failed to write using `MultithreadedTableWriter`. After detecting an error using `writeStatus.hasError()`, it retrieves the unwritten data rows with `writer.getUnwrittenData()`. Since the original writer's threads are terminated, a *new* `MultithreadedTableWriter` instance (`newwriter`) is created with the same configuration. The failed data is then potentially corrected (example modifies the second column) and re-inserted into the new writer using `newwriter.insertUnwrittenData()`. A `try...finally` block ensures `newwriter.waitForThreadCompletion()` is called before checking the status of the retry attempt. Assumes `writeStatus` and `writer` are from a previous failed write attempt.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# Assuming 'writeStatus' indicates an error from a previous 'writer' object\nif writeStatus.hasError():\n    print(\"Error in writing:\")\n    unwrittendata = writer.getUnwrittenData() # Get failed data from the original writer\n    print(\"Unwrittendata: %d\" % len(unwrittendata))\n    # 重新构造新的 MTW 对象\n    newwriter = ddb.MultithreadedTableWriter(\n        \"localhost\", 8848, \"admin\", \"123456\", dbPath=\"dfs://valuedb3\", tableName=\"pdatetest\",\n        batchSize=10000, throttle=1, threadCount=5, partitionCol=\"id\", compressMethods=[\"LZ4\",\"LZ4\",\"DELTA\"]\n    )\n    try:\n        # 修正失败数据后将失败数据重新写入 MTW\n        for row in unwrittendata:\n            # Example correction: ensure the second column is a string\n            row[1] = \"aaaaa\" \n        res = newwriter.insertUnwrittenData(unwrittendata)\n        if res.hasError():\n            print(\"Failed to write data again: \\n\", res)\n    except Exception as ex:\n        # MTW 抛出异常\n        print(\"MTW exit with exception %s\" % ex)\n    finally:\n        # 确保 newwriter工作线程结束运行\n        newwriter.waitForThreadCompletion()\n        writeStatus_retry = newwriter.getStatus()\n        print(\"Write again:\\n\", writeStatus_retry)\nelse:\n    print(\"Write successfully:\\n\", writeStatus) # Status from the original writer\n\n# Assuming 's' is a connected dolphindb session\n# print(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB Session and Performing As-of Join with Python\nDESCRIPTION: This snippet demonstrates creating a DolphinDB database with value partitioning, loading trade and quote data from CSV files into partitioned tables, and performing an asof join (aj) using the merge_asof method. It highlights the requirement that the right table be ordered by the join columns and focuses on joining on 'Symbol' and 'Time' columns for non-simultaneous temporal joining. The output presents a merged view combining trade prices and bid/offer quotes for the nearest previous time in the right table. Dependencies include DolphinDB Python API and access to the trades.csv and quotes.csv datasets located in the specified working directory.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_135\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\nWORK_DIR = \"C:/DolphinDB/Data\"\nif s.existsDatabase(WORK_DIR+\"/tickDB\"):\n    s.dropDatabase(WORK_DIR+\"/tickDB\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AAPL\",\"FB\"], dbPath=WORK_DIR+\"/tickDB\")\ntrades = s.loadTextEx(\"mydb\",  tableName='trades',partitionColumns=[\"Symbol\"], remoteFilePath=WORK_DIR + \"/trades.csv\")\nquotes = s.loadTextEx(\"mydb\",  tableName='quotes',partitionColumns=[\"Symbol\"], remoteFilePath=WORK_DIR + \"/quotes.csv\")\n\nprint(trades.top(5).toDF())\n\nprint(quotes.where(\"second(Time)>=09:29:59\").top(5).toDF())\n\nprint(trades.merge_asof(quotes,on=[\"Symbol\",\"Time\"]).select([\"Symbol\",\"Time\",\"Trade_Volume\",\"Trade_Price\",\"Bid_Price\", \"Bid_Size\",\"Offer_Price\", \"Offer_Size\"]).top(5).toDF())\n```\n\n----------------------------------------\n\nTITLE: Releasing DolphinDB Table Object using undef\nDESCRIPTION: Demonstrates releasing a DolphinDB server-side table object referenced by the alias 't1' using the `session.undef()` method. This removes the variable 't1' from the server session. Assumes 's' is a connected DolphinDB session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ns.undef(\"t1\", \"VAR\")\n```\n\n----------------------------------------\n\nTITLE: Enabling PROTOCOL_PICKLE in DolphinDB Python API for Session and DBConnectionPool - Python\nDESCRIPTION: This snippet demonstrates how to enable the PROTOCOL_PICKLE serialization protocol when creating a DolphinDB session and a database connection pool using the Python API. It imports the necessary DolphinDB modules, sets the protocol parameter to PROTOCOL_PICKLE, and connects to a local DolphinDB server instance with specified credentials. This setup is necessary to use DolphinDB's Python-specific Pickle-based serialization mechanism for transferring objects. Dependencies include the dolphinDB Python package and access to a running DolphinDB server. Inputs are connection parameters (host, port, username, password), and the output is connected session and connection pool objects ready for executing queries with PROTOCOL_PICKLE serialization.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.2_PROTOCOL_PICKLE.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, \"admin\", \"123456\", 10, protocol=keys.PROTOCOL_PICKLE)\n```\n\n----------------------------------------\n\nTITLE: Specifying Communication Protocols for DBConnectionPool in Python\nDESCRIPTION: Illustrates how to explicitly set the data transfer protocol (PROTOCOL_DDB, PROTOCOL_PICKLE, or PROTOCOL_ARROW) using the `protocol` parameter during `DBConnectionPool` initialization (API version >= 1.30.21.1). The chosen protocol affects data type handling, performance, and compatibility with features like compression. Requires `dolphindb` and `dolphindb.settings`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\n# 使用协议 PROTOCOL_DDB\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10, protocol=keys.PROTOCOL_DDB)\n\n# 使用协议 PROTOCOL_PICKLE\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10, protocol=keys.PROTOCOL_PICKLE)\n\n# 使用协议 PROTOCOL_ARROW\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10, protocol=keys.PROTOCOL_ARROW)\n```\n\n----------------------------------------\n\nTITLE: Enabling Asynchronous Mode in Session (Python)\nDESCRIPTION: Shows how to initialize a DolphinDB session with asynchronous mode enabled by setting the `enableASYNC` parameter to `True`. This mode improves client throughput in high-frequency data writing scenarios by not waiting for server execution confirmation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ns=ddb.session(enableASYNC=True)\n```\n\n----------------------------------------\n\nTITLE: Initializing DBConnectionPool in Python for DolphinDB\nDESCRIPTION: Demonstrates how to create an instance of DBConnectionPool to manage multiple connections to a DolphinDB server for concurrent task execution. This pool allows submitting tasks that can run in parallel on the server side. Requires the 'dolphindb' library and connection details (host, port, optional credentials, thread count).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_93\n\nLANGUAGE: Python\nCODE:\n```\npool = ddb.DBConnectionPool(host, port, threadNum=10, userid=None, password=None, loadBalance=False, highAvailability=False, compress=False,reConnectFlag=False, python=False)\n# userid 和 password 可以省略\n```\n\n----------------------------------------\n\nTITLE: Activating compression for DolphinDB session\nDESCRIPTION: This code shows how to enable data compression during the session for large data transfers by setting 'compress' to True. It also disables pickling to optimize performance, appropriate for large query or data upload scenarios.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ns=ddb.session(compress=True, enablePickle=False)\n```\n\n----------------------------------------\n\nTITLE: Left Join using merge (how='left') - DolphinDB Python\nDESCRIPTION: Performs a left join between the 'trade' table and auxiliary table 't1', retaining all records from 'trade' and matching available records from 't1'. Applies an additional filter for ticker and date range. Left join is indicated via 'how=\"left\"'. Requires DolphinDB Python API and proper table initialization.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_133\n\nLANGUAGE: Python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = s.table(data={'TICKER': ['AMZN', 'AMZN', 'AMZN'], 'date': ['2015.12.31', '2015.12.30', '2015.12.29'], 'open': [695, 685, 674]}, tableAliasName=\"t1\")\ns.run(\"\"\"t1 = select TICKER,date(date) as date,open from t1\"\"\")\nprint(trade.merge(t1,how=\"left\", on=[\"TICKER\",\"date\"]).where('TICKER=`AMZN').where('2015.12.23<=date<=2015.12.31').toDF())\n```\n\n----------------------------------------\n\nTITLE: Defining protocol parameter examples for DolphinDB Python session\nDESCRIPTION: Examples demonstrating how to create a DolphinDB Python session instance with different serialization protocols, including DolphinDB native, Pickle, and Arrow. These examples highlight initializing the session with specific protocols to control data serialization method used during client-server communication.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\n```\n\n----------------------------------------\n\nTITLE: Uploading Data and Retrieving as Pandas DataFrame\nDESCRIPTION: Uploads data (obtained from a hypothetical `createDemoDict()` function) to DolphinDB server using `s.table()` with the alias 't1'. It then retrieves the uploaded table as a Pandas DataFrame using the `.toDF()` method on the returned table object. Assumes 's' is a connected session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nt1=s.table(data=createDemoDict(), tableAliasName=\"t1\")\nprint(t1.toDF())\n\n#output\n#    id       date ticker    price\n# 0   1 2021-05-06   AAPL   129.74\n# 1   2 2021-05-07   AAPL   130.21\n# 2   2 2021-05-06   AMZN  3306.37\n# 3   3 2021-05-07   AMZN  3291.61\n```\n\n----------------------------------------\n\nTITLE: Updating data in a DolphinDB table in Python\nDESCRIPTION: This snippet updates specific columns in a DolphinDB table where certain conditions are met, using the update-where-execute pattern. It demonstrates updating within in-memory tables and distributed tables, highlighting the required execute call for persistence.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_101\n\nLANGUAGE: Python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+/example.csv)\ntrade = trade.update([\"VOL\"],[\"999999\"]).where(\"TICKER=`AMZN\").where([\"date=2015.12.16\"]).execute()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Thread Management for Data Writing with DolphinDB in Python\nDESCRIPTION: This snippet shows how to create and manage a single thread in Python to write data concurrently using a predefined function `insert_MTW` and a MultithreadedTableWriter instance. The thread is set as a daemon to auto-close with the main program. It demonstrates starting a thread, waiting for a short sleep period, joining the thread to ensure completion, and then calling writer's synchronization methods to wait for thread completion and retrieve write status. Finally, it prints the write status and runs a DolphinDB query to count rows in a table named 'pt'. Key dependencies include Python threading, time module, and the DolphinDB Python API writer object. The snippet validates thread completion and synchronous data batch writes to DolphinDB.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n# create a thread to write data to MTW\nthread=threading.Thread(target=insert_MTW, args=(writer,))\nthread.setDaemon(True)\nthread.start()\n\ntime.sleep(1)\n\nthread.join()\nwriter.waitForThreadCompletion()\nwriteStatus=writer.getStatus()\nprint(\"writeStatus:\\n\", writeStatus)\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Aggregating Data using GROUP BY in Python\nDESCRIPTION: Loads the 'trade' table from the 'dfs://valuedb' database. It then calculates the sum of 'vol' and the sum of 'prc' for each unique 'ticker' using `select()` with aggregate functions and `groupby(['ticker'])`. The aggregated result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_120\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\nprint(trade.select(['sum(vol)','sum(prc)']).groupby(['ticker']).toDF())\n```\n\n----------------------------------------\n\nTITLE: Initializing Import and Session for DolphinDB Python\nDESCRIPTION: This snippet prepares the environment for subsequent database operations by importing DolphinDB modules, NumPy, and pandas, and establishing a session connection. Credentials and host parameters must match the DolphinDB server configuration. The session is reused in database and table setup routines.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\nimport numpy as np\nimport pandas as pd\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n```\n\n----------------------------------------\n\nTITLE: Join on Different Column Names using merge - DolphinDB Python\nDESCRIPTION: Executes an inner join where the join keys differ in name between the two tables ('TICKER'/'date' in left, 'TICKER1'/'date1' in right), using 'left_on' and 'right_on' to align join logic. Prepares and registers temporary tables with the server. Requires the DolphinDB Python API.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_132\n\nLANGUAGE: Python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = s.table(data={'TICKER1': ['AMZN', 'AMZN', 'AMZN'], 'date1': ['2015.12.31', '2015.12.30', '2015.12.29'], 'open': [695, 685, 674]}, tableAliasName=\"t1\")\ns.run(\"\"\"t1 = select TICKER1,date(date1) as date1,open from t1\"\"\")\nprint(trade.merge(t1,left_on=[\"TICKER\",\"date\"], right_on=[\"TICKER1\",\"date1\"]).toDF())\n```\n\n----------------------------------------\n\nTITLE: Enabling Load Balancing for DBConnectionPool in Python\nDESCRIPTION: Illustrates how to create a `DBConnectionPool` with the load balancing feature enabled by setting the `loadBalance` parameter to `True`. This aims to distribute connections evenly across available data nodes when high availability is not active. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 创建连接池；开启负载均衡\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, loadBalance=True)\n```\n\n----------------------------------------\n\nTITLE: Checking Database Existence with existsDatabase\nDESCRIPTION: The `existsDatabase` function checks whether a database exists in the DolphinDB server. The `dbUrl` parameter specifies the database URL. The function returns a boolean value indicating the existence of the database.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> s.existsDatabase(dbUrl=\"dfs://testDB\")\nFalse\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with loadTableBySQL Function in Python\nDESCRIPTION: Demonstrates how to use loadTableBySQL to load filtered data from a DolphinDB database. The function applies an SQL filter condition when loading data into memory, reducing the amount of data transferred.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport dolphindb.settings as keys\n\nif s.existsDatabase(\"dfs://valuedb\"  or os.path.exists(\"dfs://valuedb\")):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\", \"NVDA\"], dbPath=\"dfs://valuedb\")\nt = s.loadTextEx(dbPath=\"mydb\",  tableName='trade',partitionColumns=[\"TICKER\"], remoteFilePath=WORK_DIR + \"/example.csv\")\n\ntrade = s.loadTableBySQL(tableName=\"trade\", dbPath=\"dfs://valuedb\", sql=\"select * from trade where date>2010.01.01\")\nprint(trade.rows)\n```\n\n----------------------------------------\n\nTITLE: Inner Table Join using merge - DolphinDB Python\nDESCRIPTION: Performs an inner join between DolphinDB's 'trade' table and an in-memory table 't1' based on common columns 'TICKER' and 'date'. The snippet creates t1, registers it with the server, and joins it to fetch matched records with an additional column. Requires DolphinDB Python API and NumPy.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_131\n\nLANGUAGE: Python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = s.table(data={'TICKER': ['AMZN', 'AMZN', 'AMZN'], 'date': np.array(['2015-12-31', '2015-12-30', '2015-12-29'], dtype='datetime64[D]'), 'open': [695, 685, 674]}, tableAliasName=\"t1\")\ns.run(\"\"\"t1 = select TICKER,date(date) as date,open from t1\"\"\")\nprint(trade.merge(t1,on=[\"TICKER\",\"date\"]).toDF())\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB MultithreadedTableWriter in Python\nDESCRIPTION: Initializes a `MultithreadedTableWriter` object for asynchronous data writing to a DolphinDB table. It manages threads and buffering for efficient batch insertion. Requires connection details, table information, and configuration options for thread count, batching, and partitioning.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nMultithreadedTableWriter(host, port, userId, password, dbPath, tableName, useSSL, enableHighAvailability, highAvailabilitySites, batchSize, throttle, threadCount, partitionCol, compressMethods, mode, modeOption)\n```\n\n----------------------------------------\n\nTITLE: Grouping and Filtering with contextby - DolphinDB Python\nDESCRIPTION: Loads a table from a DolphinDB database, groups data by the 'ticker' column using 'contextby', filters groups where the sum of 'VOL' exceeds 40,000,000,000, and converts the result to a pandas DataFrame. Requires a DolphinDB server connection and the python3 DolphinDB API. Expects a valid path and table name; outputs a grouped, filtered DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_125\n\nLANGUAGE: Python\nCODE:\n```\ndf= s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\").contextby('ticker').having(\"sum(VOL)>40000000000\").toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Appending Data to PartitionedTable (Python)\nDESCRIPTION: This snippet demonstrates how to use the `append` method of the `PartitionedTableAppender` class. It takes a pandas DataFrame as input, typically containing the data to be written into the DolphinDB partitioned table. The method leverages the connection pool to handle the concurrent writing of data, making this method suitable for appending data to partitioned tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.3_PartitionedTableAppender.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nappend(table)\n```\n\n----------------------------------------\n\nTITLE: Controlling Script Output Display with DBConnectionPool in Python\nDESCRIPTION: Shows how to use the `show_output` parameter in `DBConnectionPool` to control whether `print` statements within executed DolphinDB scripts are displayed on the client side. Includes examples demonstrating the difference between `show_output=True` (default, prints '1') and `show_output=False` (does not print '1'), while the script's return value ('2') is retrieved separately. Requires `dolphindb` and `time`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# 启用 show_output\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, show_output=True)\ntaskid = 12\npool.addTask(\"print(1);2\", taskId=taskid)\nwhile True:\n    if pool.isFinished(taskId=taskid):\n        break\n    time.sleep(0.01)\n\nres = pool.getData(taskId=taskid)\nprint(res)\n\n# output:\n1\n2\n\n\n# 不启用 show_output\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, show_output=False)\ntaskid = 12\npool.addTask(\"print(1);2\", taskId=taskid)\nwhile True:\n    if pool.isFinished(taskId=taskid):\n        break\n    time.sleep(0.01)\n\nres = pool.getData(taskId=taskid)\nprint(res)\n\n# output:\n2\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns from a DolphinDB Table in Python\nDESCRIPTION: Loads data from a specified CSV file into a DolphinDB table object using `s.loadText`. It then selects a subset of columns ('ticker', 'date', 'bid', 'ask', 'prc', 'vol') using the `select()` method with a list of column names and converts the result into a Pandas DataFrame using `toDF()` for display.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_108\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\nprint(trade.select(['ticker','date','bid','ask','prc','vol']).toDF())\n```\n\n----------------------------------------\n\nTITLE: Concurrent DolphinDB Script Execution using Asyncio and DBConnectionPool\nDESCRIPTION: Illustrates using DBConnectionPool with Python's asyncio library to execute multiple DolphinDB scripts concurrently. An async function `test_run` calls `pool.run` (which is a coroutine), and tasks are created and run using `asyncio.ensure_future` and `loop.run_until_complete`. Requires the 'dolphindb' and 'asyncio' libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_94\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport datetime\nimport time\nimport asyncio\nimport threading\nimport sys\nimport numpy\nimport pandas\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, 20)\n\n# 创建一个任务函数，用 sleep 模拟一段运行的时间\nasync def test_run():\n    try:\n        return await pool.run(\"sleep(1000);1+2\")\n    except Exception as e:\n        print(e)\n\n# 定义任务列表\ntasks = [\n    asyncio.ensure_future(test_run()),\n    asyncio.ensure_future(test_run()),\n    asyncio.ensure_future(test_run()),\n    asyncio.ensure_future(test_run()),\n]\n\n# 创建一个事件循环对象，运行任务列表直到全部任务完成\nloop = asyncio.get_event_loop()\ntry:\n    loop.run_until_complete(asyncio.wait(tasks))\nexcept Exception as e:\n    print(\"catch e:\")\n    print(e)\n\nfor i in tasks:\n    print(i)\n\npool.shutDown() # 关闭进程池对象\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative Sum within Groups using CONTEXT BY in Python\nDESCRIPTION: Loads the 'trade' table. It groups the data by 'TICKER' and the month of the 'date' using `contextby(\"TICKER,month(date)\")`. Within each group, it calculates the cumulative sum of 'VOL' using `cumsum(VOL)` in the `select()` clause. The result includes the ticker, month, and the calculated cumulative volume for each row within its group, converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_123\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\").select(\"TICKER, month(date) as month, cumsum(VOL)\").contextby(\"TICKER,month(date)\").toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Typical MultithreadedTableWriter Usage Sequence in Python\nDESCRIPTION: Illustrates the standard pattern for using the `MultithreadedTableWriter`: first, call `insert` (or `insertUnwrittenData`) one or more times to add data to the buffer, and then call `waitForThreadCompletion` to ensure all data is processed asynchronously before continuing or exiting.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\n# insert data\nwriter.insert(data)\n...\nwriter.waitForThreadCompletion()\n```\n\n----------------------------------------\n\nTITLE: Enabling Compression for DBConnectionPool in Python\nDESCRIPTION: Demonstrates enabling network data compression by setting `compress=True` when creating a `DBConnectionPool`. It highlights the requirement of using `PROTOCOL_DDB` for compression, showing examples for different API versions (>= 1.30.21.1 requiring explicit protocol setting, <= 1.30.19.4 using it by default). This is useful for large data transfers but increases CPU load. Requires the `dolphindb` and `dolphindb.settings` modules.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n\n# api version >= 1.30.21.1，开启压缩，需指定协议为PROTOCOL_DDB\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, compress=True, protocol=keys.PROTOCOL_DDB)\n\n# api version <= 1.30.19.4，开启压缩，默认使用协议为PROTOCOL_DDB，即enablePickle=False\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, compress=True)\n```\n\n----------------------------------------\n\nTITLE: Using tableAppender with Distributed Table in Python\nDESCRIPTION: Provides an example of using the `tableAppender` object to append data from a Pandas DataFrame to a DolphinDB distributed table. The script connects to DolphinDB, creates a RANGE-partitioned database and table, initializes `tableAppender`, prepares a large Pandas DataFrame with various data types (including time types and NaT), and appends the data using `appender.append()`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript='''\ndbPath = \"dfs://tableAppender\"\nif(existsDatabase(dbPath))\n    dropDatabase(dbPath)\nt = table(1000:0, `sym`date`month`time`minute`second`datetime`timestamp`nanotimestamp`qty, [SYMBOL, DATE,MONTH,TIME,MINUTE,SECOND,DATETIME,TIMESTAMP,NANOTIMESTAMP, INT])\ndb=database(dbPath,RANGE,100000 200000 300000 400000 600001)\npt = db.createPartitionedTable(t, `pt, `qty)\n'''\ns.run(script)\nappender = ddb.tableAppender(\"dfs://tableAppender\",\"pt\", s)\nsym = list(map(str, np.arange(100000, 600000)))\ndate = np.array(np.tile(['2012-01-01', 'NaT', '1965-07-25', 'NaT', '2020-12-23', '1970-01-01', 'NaT', 'NaT', 'NaT', '2009-08-05'],50000), dtype=\"datetime64[D]\")\nmonth = np.array(np.tile(['1965-08', 'NaT','2012-02', '2012-03', 'NaT'],100000), dtype=\"datetime64\")\ntime = np.array(np.tile(['2012-01-01T00:00:00.000', '2015-08-26T05:12:48.426', 'NaT', 'NaT', '2015-06-09T23:59:59.999'],100000), dtype=\"datetime64\")\nsecond = np.array(np.tile(['2012-01-01T00:00:00', '2015-08-26T05:12:48', 'NaT', 'NaT', '2015-06-09T23:59:59'],100000), dtype=\"datetime64\")\nnanotime = np.array(np.tile(['2012-01-01T00:00:00.000000000', '2015-08-26T05:12:48.008007006', 'NaT', 'NaT', '2015-06-09T23:59:59.999008007'],100000), dtype=\"datetime64\")\nqty = np.arange(100000, 600000)\ndata = pd.DataFrame({'sym': sym, 'date': date, 'month':month, 'time':time, 'minute':time, 'second':second, 'datetime':second, 'timestamp':time, 'nanotimestamp':nanotime, 'qty': qty})\nnum = appender.append(data)\nprint(num)\nprint(s.run(\"select * from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Batch updating distributed table data in DolphinDB with Python\nDESCRIPTION: This snippet illustrates how to perform a batch update on a distributed DolphinDB table via Python session, executing scripts to create database, table, and perform updates, showcasing distributed environment data modifications.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_102\n\nLANGUAGE: Python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n# Create and update distributed table\ns.run(\"\"\"...\"\"\")\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB functions with parameters via Python API\nDESCRIPTION: Demonstrates calling built-in or user-defined DolphinDB functions directly from Python using 'run'. Shows how to execute functions with predefined variables or inline arguments, including partial application for deferred parameter setting.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ns.run(\"x = [1,3,5];y = [2,4,6]\")\n\n# Adding vectors using 'add' after setting variables\na=s.run(\"add(x,y)\")\nrepr(a)\n\n# Partial application example with 'add{v1,v2}':\nimport numpy as np\nresult=s.run(\"add{x,}\", y)\n```\n\n----------------------------------------\n\nTITLE: Enabling High Availability for DBConnectionPool in Python\nDESCRIPTION: Shows how to configure a `DBConnectionPool` for high availability by setting the `highAvailability` parameter to `True`. This allows the pool to potentially connect to any node in the cluster, improving fault tolerance, though load balancing behavior might change. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 创建连接池；开启高可用，使用集群所有节点作为高可用节点\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, \"admin\", \"123456\", highAvailability=True)\n```\n\n----------------------------------------\n\nTITLE: Calculating Trade Cost Using As-of Join in DolphinDB Python API\nDESCRIPTION: This snippet illustrates how to calculate a cost metric for trades using the merge_asof function combined with aggregation. It computes a weighted average absolute price difference between trade prices and the midpoint of bid/offer prices over joined symbol-time records. The input tables are the 'trades' and 'quotes' datasets joined on 'Symbol' and 'Time'. The output is grouped by 'Symbol', showing the cost values for AAPL and FB. Dependencies include the earlier loaded data tables and DolphinDB session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_136\n\nLANGUAGE: python\nCODE:\n```\nprint(trades.merge_asof(quotes, on=[\"Symbol\",\"Time\"]).select(\"sum(Trade_Volume*abs(Trade_Price-(Bid_Price+Offer_Price)/2))/sum(Trade_Volume*Trade_Price)*10000 as cost\").groupby(\"Symbol\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Dropping a DolphinDB Database and Table using Python\nDESCRIPTION: Checks if the database 'dfs://valuedb' exists and drops it if it does. Then, it creates the database, loads data from `example.csv` into a partitioned table named 'trade' using `s.loadTextEx()`, and subsequently drops the 'trade' table using `s.dropTable()`. Requires an active DolphinDB session `s`, the `dolphindb` library, `dolphindb.settings` (aliased as `keys`), and the `example.csv` file accessible via `WORK_DIR`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys # Assuming this import happens earlier\n\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\",\"NVDA\"], dbPath=\"dfs://valuedb\")\ns.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\ns.dropTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n```\n\n----------------------------------------\n\nTITLE: Upsert Records with MultithreadedTableWriter in DolphinDB Python API\nDESCRIPTION: This snippet demonstrates performing upsert operations (update or insert) on a DolphinDB table via MultithreadedTableWriter. It configures the MTW with the mode set to 'UPSERT' and supplies relevant modeOptions such as 'ignoreNull=false' and key columns. Multiple client threads each insert/update data with a shared MTW instance. Required dependencies: dolphindb Python API, threading, time, random, numpy, pandas. Inputs: Random data generated per thread. Outputs: Status/log messages, final MTW status printout. Limitations: Key column for the upsert must be provided and registered; thread safety and upsert semantics rely on correct column type and mode parameter settings.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_91\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport dolphindb.settings as keys\n\nimport threading\n\nHOST = \"192.168.1.193\"\nPORT = 8848\n\ns = ddb.session()\ns.connect(HOST, PORT, \"admin\", \"123456\")\nscript_DFS_HASH = \"\"\"\n    testtable=keyedTable(`id,1000:0,`text`id,[STRING,LONG])\n    share testtable as ttable\n    \"\"\"\ns.run(script_DFS_HASH)\n\ndef insert_mtw(writer, id):\n    try:\n        print(\"thread\",id,\"start.\")\n        for i in range(1000):\n            text=str(time.time())\n            id=random.randint(1, 10)\n            print(text,id)\n            res=writer.insert(text, id)\n        print(\"thread\",id,\"exit.\")\n    except Exception as e:\n        print(e)\n\nprint(\"test start.\")\nwriter = ddb.MultithreadedTableWriter(HOST, PORT,\"admin\",\"123456\",\"\",\"ttable\",False,False,[], 1, 0.1, 1,\"id\",mode=\"UPSERT\",\n                                      modeOption=[\"ignoreNull=false\",\"keyColNames=`id\"])\nthreads=[]\nfor i in range(2):\n    threads.append(threading.Thread(target=insert_mtw, args=(writer,i,)))\nfor t in threads:\n    t.setDaemon(True)\n    t.start()\nfor t in threads:\n    t.join()\nwriter.waitForThreadCompletion()\nstatus=writer.getStatus()\nprint(\"test exit\",status)\n```\n\n----------------------------------------\n\nTITLE: Using runTaskAsync with DBConnectionPool in Python\nDESCRIPTION: Demonstrates how to use the runTaskAsync method to execute multiple scripts asynchronously and retrieve results using the Future objects returned. Includes timing measurements to show the parallel execution.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.2_AsyncMethodsAndOthers.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport time\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10)\n\nt1 = time.time()\ntask1 = pool.runTaskAsync(\"sleep(1000); 1+0;\")\ntask2 = pool.runTaskAsync(\"sleep(2000); 1+1;\")\ntask3 = pool.runTaskAsync(\"sleep(4000); 1+2;\")\ntask4 = pool.runTaskAsync(\"sleep(1000); 1+3;\")\nt2 = time.time()\nprint(\"Task1 Result: \", task1.result())\nt3 = time.time()\nprint(\"Task2 Result: \", task2.result())\nt4 = time.time()\nprint(\"Task4 Result: \", task4.result())\nt5 = time.time()\nprint(\"Task3 Result: \", task3.result())\nt6 = time.time()\n\nprint(\"Add Tasks: \", t2-t1)\nprint(\"Get Task1: \", t3-t1)\nprint(\"Get Task2: \", t4-t1)\nprint(\"Get Task4: \", t5-t1)\nprint(\"Get Task3: \", t6-t1)\npool.shutDown()\n```\n\n----------------------------------------\n\nTITLE: Enabling Asynchronous Communication in DolphinDB Session with Python\nDESCRIPTION: Illustrates activating asynchronous communication mode by setting enableASYNC to True. In this mode, only the session.run() method communicates with the server and does not return any result, optimizing scenarios such as asynchronous data writing by avoiding waiting for responses. Supported from DolphinDB version 1.10.17 onwards.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 开启异步通讯\ns = ddb.session(enableASYNC=True)\n```\n\n----------------------------------------\n\nTITLE: Enabling streaming data in DolphinDB Python API\nDESCRIPTION: Enables the streaming functionality on the client side via the DolphinDB session object's enableStreaming method. Depending on the DolphinDB server version, specifying a port may be required or redundant. In 1.30.x or before 2.00.9 versions, the port is mandatory, while in versions 2.00.9 and later, it is optional and ignored if specified. The snippet demonstrates enabling streaming with and without the port parameter. No additional dependencies beyond a DolphinDB session instance are required.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.4_Subscription/2.4_Subscription.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\n# 1.30.x，2.00.9 之前的版本，开启订阅，指定端口8000\ns.enableStreaming(8000)   \n# 2.00.9 及之后的版本，开启订阅，无需指定端口\ns.enableStreaming() \n```\n\n----------------------------------------\n\nTITLE: Appending Data via DolphinDB Server Function View Asynchronously Using Python API\nDESCRIPTION: This Python snippet connects asynchronously to DolphinDB and repeatedly invokes a previously defined server-side function view (appendStreamingData) to append data. The data is a pandas DataFrame with various columns including time as DATE type handled by the server function. This approach prevents asynchronous timing issues by pushing type conversion logic to the DolphinDB server side and supports improved throughput in asynchronous mode.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.1_SessionAsyncMode.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport random\nimport datetime\n\ns = ddb.session(enableASYNC=True)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nn = 100\n\n# 生成一个 DataFrame\ntime_list = [np.datetime64(datetime.date(2020, random.randint(1, 12), random.randint(1, 20))) for _ in range(n)]\nsym_list = np.random.choice(['IBN', 'GTYU', 'FHU', 'DGT', 'FHU', 'YUG', 'EE', 'ZD', 'FYU'], n)\nprice_list = [round(np.random.uniform(1, 100), 1) for _ in range(n)]\nid_list = np.random.choice([1, 2, 3, 4, 5], n)\n\ntb = pd.DataFrame({\n    'time': time_list,\n    'sym': sym_list,\n    'price': price_list,\n    'id': id_list,\n})\n\nfor _ in range(50000):\n    s.run(\"appendStreamingData\", tb)\n```\n\n----------------------------------------\n\nTITLE: Appending DataFrame with Automatic Time Conversion Using tableAppender - DolphinDB Python API (Python)\nDESCRIPTION: Utilizes the DolphinDB Python API's tableAppender object to append a pandas DataFrame to a DolphinDB memory or distributed table with automatic conversion of datetime64 columns to their corresponding DolphinDB time types. Shows object instantiation, connection, DataFrame creation, and appender usage. Requires dolphindb, pandas, numpy, and an active session. This approach streamlines time column handling and is optimal for repeated time-indexed DataFrame ingestion.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ns.run(\"share table(1000:0, `sym`timestamp`qty, [SYMBOL, TIMESTAMP, INT]) as t\")\nappender = ddb.tableAppender(tableName=\"t\", ddbSession=s)\nsym = ['A1', 'A2', 'A3', 'A4', 'A5']\ntimestamp = np.array(['2012-06-13 13:30:10.008', 'NaT','2012-06-13 13:30:10.008', '2012-06-13 15:30:10.008', 'NaT'], dtype=\"datetime64\")\nqty = np.arange(1, 6)\ndata = pd.DataFrame({'sym': sym, 'timestamp': timestamp, 'qty': qty})\nnum = appender.append(data)\nprint(num)\nt = s.run(\"t\")\nprint(t)\n```\n\n----------------------------------------\n\nTITLE: Filtering with Chained `where()` Conditions and Sorting (Python)\nDESCRIPTION: Loads data from `example.csv`. Selects specific columns, applies multiple filters (`TICKER='AMZN'`, `bid!=NULL`, `ask!=NULL`, `vol>10000000`) using chained `where()` calls, sorts the results by 'vol' descending using `sort('vol desc')`, executes the query, assigns the result to a server variable 't1' using `executeAs(\"t1\")`, and finally prints the result as a Pandas DataFrame and the total number of rows. Requires `s`, `dolphindb`, `WORK_DIR`, and `example.csv`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\n\n# use chaining WHERE conditions and save result to DolphinDB server variable \"t1\" through function \"executeAs\"\nt1=trade.select(['date','bid','ask','prc','vol']).where('TICKER=`AMZN').where('bid!=NULL').where('ask!=NULL').where('vol>10000000').sort('vol desc').executeAs(\"t1\")\nprint(t1.toDF())\n# output\n# ... (omitted for brevity)\nprint(t1.rows)\n```\n\n----------------------------------------\n\nTITLE: Creating a DolphinDB Database using Session Object (Python)\nDESCRIPTION: Illustrates creating a DolphinDB distributed database (DFS) using the `database` method of a `ddb.session` object. It specifies the database name (`dbName`), partition type (`partitionType` using `dolphindb.settings`), partition values (`partitions`), and the database path (`dbPath`). Requires an active DolphinDB session (`s`) and the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\", \"NVDA\"], dbPath=\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Accessing Database Without dbName - DolphinDB Python\nDESCRIPTION: Shows how to create a DolphinDB database without explicitly specifying the dbName parameter, resulting in a randomly generated handle name. Requires an active DolphinDB session and appropriate import of the settings (keys) module. Returns a Database object whose name can be retrieved and used with session.run for server-side operations.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndbPath = \"dfs://dbName\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.VALUE, partitions=[1, 2, 3], dbPath=dbPath)\n\ndbName = db._getDbName()\nprint(dbName)\nprint(s.run(dbName))\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Accessing Database With dbName - DolphinDB Python\nDESCRIPTION: Illustrates creating a DolphinDB database with a user-defined dbName ('testDB'), giving the handle a predictable name in the session. Dependencies include an active DolphinDB session and use of the Python API and settings module. Outputs the specified dbName and validates it on the server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndbPath = \"dfs://dbName\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(dbName=\"testDB\", partitionType=keys.VALUE, partitions=[1, 2, 3], dbPath=dbPath)\n\ndbName = db._getDbName()\nprint(dbName)\nprint(s.run(dbName))\n\n```\n\n----------------------------------------\n\nTITLE: Creating COMPO Partitioned Database in DolphinDB via Python\nDESCRIPTION: Creates a DolphinDB DFS database ('dfs://db_compo_test') with COMPO (composite) partitioning. It first defines two sub-database schemas: `db1` (VALUE partitioning on date) and `db2` (RANGE partitioning on int), ensuring their `dbPath` is empty. It then checks/drops the main database path, creates the COMPO database using the sub-schemas, prepares data, uploads it, creates the partitioned table 'pt' using both 'date' and 'val' as partition columns, appends data, and loads the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndb1 = s.database('db1', partitionType=keys.VALUE,partitions=np.array([\"2012-01-01\", \"2012-01-06\"], dtype=\"datetime64[D]\"), dbPath='')\ndb2 = s.database('db2', partitionType=keys.RANGE,partitions=[1, 6, 11], dbPath='')\ndbPath=\"dfs://db_compo_test\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(dbName='mydb', partitionType=keys.COMPO, partitions=[db1, db2], dbPath=dbPath)\ndf = pd.DataFrame({'date':np.array(['2012-01-01', '2012-01-01', '2012-01-06', '2012-01-06'], dtype='datetime64'), 'val': [1, 6, 1, 6]})\nt = s.table(data=df)\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns=['date', 'val']).append(t)\nre = s.loadTable(tableName='pt', dbPath=dbPath).toDF()\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Distributed DolphinDB Table Asynchronously Using Python\nDESCRIPTION: This snippet demonstrates creating a partitioned database and table on the DolphinDB server, then asynchronously appending a pandas DataFrame of data into that distributed table via the Python API. It assumes a session with async enabled. The snippet shows server-side table setup using inline script executed through session.run(), then uses session.run() again with a parameterized append! function call to add data. The append operation has no return value due to async mode.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.1_SessionAsyncMode.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\nimport numpy as np\nimport pandas as pd\n\ns = ddb.session(enableASYNC=True) # 打开异步模式\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\ndbPath = \"dfs://testDB\"\ntbName = \"tb1\"\n\n\nscript = \"\"\"\n    dbPath=\"dfs://testDB\"\n    tbName=`tb1\n    if(existsDatabase(dbPath))\n        dropDatabase(dbPath)\n    db=database(dbPath, VALUE, [\"AAPL\", \"AMZN\", \"A\"])\n    testDictSchema=table(5:0, `id`ticker`price, [INT,SYMBOL,DOUBLE])\n    tb1=db.createPartitionedTable(testDictSchema, tbName, `ticker)\n\"\"\"\ns.run(script)   #此处脚本可以在服务器端运行       \n\ntb = pd.DataFrame({\n    'id': np.array([1, 2, 2, 3], dtype=\"int32\"),\n    'ticker': ['AAPL', 'AMZN', 'AMZN', 'A'],\n    'price': [22, 3.5, 21, 26],\n})\n\ns.run(f\"append!{{loadTable('{dbPath}', `{tbName})}}\", tb)\n```\n\n----------------------------------------\n\nTITLE: Using executeAs to Save Query Result as a Server-side Table in DolphinDB Python API\nDESCRIPTION: This snippet shows how to use executeAs to save the result of a complex query to a new server-side table named 'AMZN'. It loads trade data from a distributed filesystem, selects and filters specific columns and rows, sorts by volume, and assigns the result to a new named table. The snippet emphasizes the importance of maintaining a client-side variable reference to prevent premature release of the server table. The session run call accesses and prints the server table content afterward.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_139\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n\nt = trade.select(['date','bid','ask','prc','vol']).where('TICKER=`AMZN').where('bid!=NULL').where('ask!=NULL').where('vol>10000000').sort('vol desc').executeAs(\"AMZN\")\n\nprint(s.run('AMZN'))\n```\n\n----------------------------------------\n\nTITLE: Using BatchTableWriter for Data Insertion Example (Python)\nDESCRIPTION: A comprehensive example demonstrating the workflow: connecting to DolphinDB, creating and sharing a table, initializing BatchTableWriter, adding the table, inserting multiple rows asynchronously, checking status/unwritten data, and verifying results in DolphinDB. Illustrates asynchronous behavior.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"\n    t = table(1000:0,`id`date`ticker`price, [INT,DATE,SYMBOL,DOUBLE]);\n    share t as tglobal;\n\"\"\"\ns.run(script)\n\nwriter = ddb.BatchTableWriter(\"localhost\", 8848)\nwriter.addTable(tableName=\"tglobal\")\nwriter.insert(\"\",\"tglobal\", 1, np.datetime64(\"2019-01-01\"),'AAPL', 5.6)\nwriter.insert(\"\",\"tglobal\", 2, np.datetime64(\"2019-01-01\"),'GOOG', 8.3)\nwriter.insert(\"\",\"tglobal\", 3, np.datetime64(\"2019-01-02\"),'GOOG', 4.2)\nwriter.insert(\"\",\"tglobal\", 4, np.datetime64(\"2019-01-03\"),'AMZN', 1.4)\nwriter.insert(\"\",\"tglobal\", 5, np.datetime64(\"2019-01-05\"),'AAPL', 6.9)\n\nprint(writer.getUnwrittenData(dbPath=\"\", tableName=\"tglobal\"))\nprint(writer.getStatus(tableName=\"tglobal\"))\nprint(writer.getAllStatus())\n\nprint(\"rows:\", s.run(\"tglobal.rows()\"))\nprint(s.run(\"select * from tglobal\"))\n```\n\n----------------------------------------\n\nTITLE: Creating VALUE Partitioned Databases by Date and Month - DolphinDB Python\nDESCRIPTION: Provides two approaches for creating databases partitioned by VALUE using date and month arrays. Relies on pandas for range generation and NumPy for type conversion. Demonstrates removal and recreation of databases to avoid conflicts, and creation using session.database with appropriately structured partition arguments.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndbPath=\"dfs://db_value_date\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndates=np.array(pd.date_range(start='20120101', end='20120110'), dtype=\"datetime64[D]\")\ndb = s.database(dbName='mydb', partitionType=keys.VALUE, partitions=dates,dbPath=dbPath)\n\n```\n\nLANGUAGE: Python\nCODE:\n```\ndbPath=\"dfs://db_value_month\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\nmonths=np.array(pd.date_range(start='2012-01', end='2012-10', freq=\"M\"), dtype=\"datetime64[M]\")\ndb = s.database(partitionType=keys.VALUE, partitions=months,dbPath=dbPath)\n\n```\n\n----------------------------------------\n\nTITLE: Connecting with Apache Arrow Protocol and Retrieving Arrow Table - DolphinDB Python API (Python)\nDESCRIPTION: Establishes a Python DolphinDB session with the Arrow protocol, connects to the local server, and retrieves a table, resulting in a pyarrow.Table object. Requires the dolphindb package, a Linux x86_64 environment, DolphinDB's formatArrow plugin, and pyarrow 9.0.0 or higher. Falls back to DDB protocol if Arrow support is missing.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nre = s.run(\"table(1..3 as a)\")\nprint(re)\n-----------------------------\npyarrow.Table\na: int32\n----\na: [[1,2,3]]\n```\n\n----------------------------------------\n\nTITLE: Batch Reading Large DolphinDB Tables in Python\nDESCRIPTION: Demonstrates how to read large tables from DolphinDB in batches using the fetchSize parameter. This approach helps manage memory usage when working with large datasets.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nscript1='''\nselect * from testblock\n'''\nblock= s.run(script1, fetchSize = 8192)\ntotal = 0\nwhile block.hasNext():\n    tem = block.read()\n    total+=len(tem)\n\nprint(\"total=\", total)\n```\n\n----------------------------------------\n\nTITLE: Creating a Distributed Database and Partitioned Table (Python)\nDESCRIPTION: This Python code demonstrates the creation of a distributed database and a partitioned table using the DolphinDB API. It begins by connecting to the DolphinDB server. Then, a script drops an existing database if it exists, creates a table with specific schema, and then creates a partitioned table using the database object and table definition. Dependencies include the `dolphindb`, `pandas`, and `numpy` libraries. This prepares the environment for testing data appending.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.3_PartitionedTableAppender.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript = \"\"\"\n    dbPath = \"dfs://valuedb\"\n    if(existsDatabase(dbPath)){\n        dropDatabase(dbPath)\n    }\n    t = table(100:0, `id`date`vol, [SYMBOL, DATE, LONG])\n    db = database(dbPath, VALUE, `APPL`IBM`AMZN)\n    pt = db.createPartitionedTable(t, `pt, `id)\n\"\"\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Setting Up DolphinDB Database for GROUP BY Examples in Python\nDESCRIPTION: Prepares the DolphinDB environment for subsequent GROUP BY examples. It first checks for and drops the 'dfs://valuedb' database if it exists. Then, it creates a new VALUE-partitioned database with specified partitions and loads data into the 'trade' table within this database from a CSV file using `loadTextEx`. Requires the `dolphindb` library and `settings` (aliased as `keys`).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_119\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\",\"NVDA\"], dbPath=\"dfs://valuedb\")\ns.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\n```\n\n----------------------------------------\n\nTITLE: Creating TSDB Engine Database - DolphinDB Python\nDESCRIPTION: Demonstrates the creation of a database using the TSDB engine by specifying engine=\"TSDB\" during session.database construction. It is necessary for time-series-optimized features. A set of date partitions is provided for the VALUE partition type; prior session setup and appropriate imports are prerequisites.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndates = np.array(pd.date_range(start='20120101', end='20120110'), dtype=\"datetime64[D]\")\ndbPath = \"dfs://tsdb\"\nif s.existsDatabase(dbPath): \n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.VALUE, partitions=dates, dbPath=dbPath, engine=\"TSDB\")\n\n```\n\n----------------------------------------\n\nTITLE: Inserting Data to DolphinDB Stream Table via Python (Optimized)\nDESCRIPTION: Presents a more efficient method for inserting data from a pandas DataFrame into the DolphinDB `Trade` stream table. It selects the required columns in Python and directly calls `tableInsert` via `s.run`, avoiding intermediate `s.upload` and the subsequent select query on the server side. Requires Dolphindb Python API, pandas, numpy, and a running DolphinDB server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_106\n\nLANGUAGE: Python\nCODE:\n```\ncsv_df=csv_df['Symbol', 'Datetime', 'Price', 'Volume']\ns.run(\"tableInsert{Trade}\", csv_df)\n```\n\n----------------------------------------\n\nTITLE: Connecting to DolphinDB server with specific protocols\nDESCRIPTION: Sample code illustrating establishing a DolphinDB session with distinct communication protocols, including PROTOCOL_DDB, PROTOCOL_PICKLE, and PROTOCOL_ARROW, specifying server host, port, username, and password. It demonstrates configuring the protocol for data serialization during session creation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Table to BatchTableWriter in Python\nDESCRIPTION: Adds a target table to the BatchTableWriter instance for writing. Requires tableName, optional dbPath for disk tables, and a partitioned flag (True for partitioned tables, False for unpartitioned disk tables). Memory tables must be shared before adding.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nwriter.addTable(dbPath=None, tableName=None, partitioned=True)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Unwritten Data from BatchTableWriter in Python\nDESCRIPTION: Fetches data that was queued but failed to be written to the DolphinDB server, usually after an error. Requires tableName and optional dbPath. Returns the unwritten data as a pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndata:pandas.DataFrame = writer.getUnwrittenData(dbPath=None, tableName=None)\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Database and Table using `run` Method in Python\nDESCRIPTION: Defines a DolphinDB script as a multiline Python string to create a VALUE partitioned database and table (`dfs://valuedb`, `pt`), executes the script using `s.run()`, then loads the created table back into Python as a pandas DataFrame. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndbPath=\"dfs://valuedb\"\ndstr = \"\"\"\ndbPath=\"dfs://valuedb\"\nif (existsDatabase(dbPath)){\n    dropDatabase(dbPath)\n}\nmydb=database(dbPath, VALUE, ['AMZN','NFLX', 'NVDA'])\nt=table(take(['AMZN','NFLX', 'NVDA'], 10) as sym, 1..10 as id)\nmydb.createPartitionedTable(t,`pt,`sym).append!(t)\n\n\"\"\"\nt1=s.run(dstr)\nt1=s.loadTable(tableName=\"pt\",dbPath=dbPath)\nt1.toDF()\n```\n\n----------------------------------------\n\nTITLE: Deleting rows from a distributed table in DolphinDB via Python scripting\nDESCRIPTION: This snippet shows how to formulate a script to delete data rows from a distributed table in DolphinDB, utilizing the delete-where-execute pattern within a Python script to manipulate distributed data structures.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_104\n\nLANGUAGE: Python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n# Create and delete partitions in distributed table\n```\n\n----------------------------------------\n\nTITLE: Combining Threading and Asyncio for Non-Blocking DolphinDB Tasks\nDESCRIPTION: Presents a class-based approach using DBConnectionPool with both asyncio and threading to run custom DolphinDB scripts concurrently without blocking the main thread. A separate thread runs the asyncio event loop, while the main thread submits tasks using `asyncio.run_coroutine_threadsafe`. Requires 'dolphindb', 'asyncio', and 'threading' libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_95\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport datetime\nimport time\nimport asyncio\nimport threading\nimport sys\nimport numpy\nimport pandas\n\n# 在该例子中主线程负责创建协程对象传入自定义脚本并调用自定义的对象去运行，并新起子线程运行事件循环防止阻塞主线程。\nclass Dolphindb(object):\n\n    pool = ddb.DBConnectionPool (\"localhost\", 8848, 20)\n\n    @classmethod\n    async def test_run1(cls,script):\n        print(\"test_run1\")\n        return await cls.pool.run(script)\n\n    @classmethod\n    async def runTest(cls,script):\n        start = time.time()\n        task = loop.create_task(cls.test_run1(script))\n        result = await asyncio.gather(task)\n        print(time.time()-start)\n        print(result)\n        print (time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n        return result\n\n#定义一个跑事件循环的线程函数\ndef start_thread_loop(loop):\n    asyncio.set_event_loop(loop)\n    loop.run_forever()\n\n\nif __name__==\"__main__\":\n    start = time.time()\n    print(\"In main thread\",threading.current_thread())\n    loop = asyncio.get_event_loop()\n    # 在子线程中运行事件循环, 让它 run_forever\n    t = threading.Thread(target= start_thread_loop, args=(loop,))\n    t.start()\n    task1 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(1000);1+1\"),loop)\n    task2 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(3000);1+2\"),loop)\n    task3 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(5000);1+3\"),loop)\n    task4 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(1000);1+4\"),loop)\n\n    print('主线程不会阻塞')\n    end = time.time()\n    print(end - start)\n```\n\n----------------------------------------\n\nTITLE: Appending to Distributed Table via tableInsert in Python\nDESCRIPTION: Shows how to append data from a Pandas DataFrame (created by a hypothetical `createDemoDataFrame()` function) to a DolphinDB distributed table using the `tableInsert` command executed via `s.run`. This method leverages DolphinDB's automatic time type conversion for distributed tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ntb = createDemoDataFrame()\ns.run(\"tableInsert{{loadTable('{db}', `{tb})}}\".format(db=dbPath,tb=tableName), tb)\n```\n\n----------------------------------------\n\nTITLE: Creating VALUE Partitioned Database (Month) in DolphinDB via Python\nDESCRIPTION: Creates a DolphinDB DFS database ('dfs://db_value_month') with VALUE partitioning based on months. It checks/drops the existing database, defines monthly partitions using NumPy arrays with 'datetime64[M]' type, creates the database, prepares data in a Pandas DataFrame, uploads it, creates the partitioned table based on the 'date' column, appends data, and loads the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndbPath=\"dfs://db_value_month\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\nmonths=np.array(pd.date_range(start='2012-01', end='2012-10', freq=\"M\"), dtype=\"datetime64[M]\")\ndb = s.database(dbName='mydb', partitionType=keys.VALUE, partitions=months,dbPath=dbPath)\ndf = pd.DataFrame({'date': np.array(['2012-01-01', '2012-02-01', '2012-05-01', '2012-06-01'], dtype=\"datetime64\"), 'val':[1,2,3,4]})\nt = s.table(data=df)\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns='date').append(t)\nre=s.loadTable(tableName='pt', dbPath=dbPath).toDF()\n```\n\n----------------------------------------\n\nTITLE: Handling Errors with DolphinDB MultithreadedTableWriter in Python\nDESCRIPTION: Illustrates error handling during data insertion with `MultithreadedTableWriter`. It first inserts correct data, then intentionally inserts data with incorrect types and column counts. It shows how `insert()` might return an error immediately (column count mismatch) or how errors might be detected later by background threads (type mismatch), causing the writer to terminate and potentially raise an exception on subsequent inserts. Uses `try...except`, checks `res.hasError()`, calls `waitForThreadCompletion()`, and inspects `writer.getStatus()` for detailed error information. Requires `numpy` and `time` modules. Note that a new writer must be created if the previous one terminated due to errors.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport random\nimport time\n\nwriter = ddb.MultithreadedTableWriter(\n    \"localhost\", 8848, \"admin\", \"123456\", dbPath=\"dfs://valuedb3\", tableName=\"pdatetest\",\n    batchSize=10000, throttle=1, threadCount=5, partitionCol=\"id\", compressMethods=[\"LZ4\",\"LZ4\",\"DELTA\"]\n)\n\ntry:\n    # 插入100行正确数据 （类型和列数都正确），MTW正常运行\n    for i in range(100):\n        res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000)) # Corrected from example to match table schema implied later\n    # 插入10行类型错误数据，此时 MTW 并不会进行类型判断，这些数据能够进入 MTW 待转换队列\n    # 直到转换线程对这些数据进行转换时，检测到类型不匹配，就会立刻终止 MTW 所有后台线程\n    for i in range(10):\n        # Assuming the second column 'symbol' expects a string, but receives an integer '222'\n        res = writer.insert(random.randint(1,10000), 222, random.randint(1,10000))\n        if res.hasError():\n            # 此处不会执行到\n            print(\"Insert wrong format data:\\n\", res)\n    # 插入1行数据(列数不匹配)，MTW 立刻发现待插入数据列数与待插入表的列数不匹配，立刻返回错误信息，\n    # 本条数据并不会进入待转换队列\n    res = writer.insert(random.randint(1,10000),\"AAAAAAAB\") # Only 2 columns instead of 3\n    if res.hasError():\n        # 数据错误，插入列数不匹配数据\n        print(\"Column counts don't match:\\n\", res)\n    # sleep 1秒，等待 MTW 转换线程处理数据直至检测到第2次插入的10行数据类型不匹配\n    # 此时 MTW 立刻终止所有线程，并修改状态为错误状态\n    time.sleep(1)\n\n    # 再插入1行正确数据，MTW 会因为工作线程终止而抛出异常，且不会写入该行数据\n    res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\n    print(\"MTW has exited\") # This line likely won't be reached if exception occurs\nexcept Exception as ex:\n    # MTW 抛出异常\n    print(\"MTW exit with exception %s\" % ex)\n# 等待 MTW 插入完成\nwriter.waitForThreadCompletion()\nwriteStatus = writer.getStatus()\nif writeStatus.hasError():\n    print(\"Error in writing:\")\nprint(writeStatus)\n# Assuming 's' is a connected dolphindb session\n# print(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table and Enabling Compression - DolphinDB Python\nDESCRIPTION: Demonstrates how to create a partitioned table within an OLAP-engine database, setting column-level compression (timestamp column with delta compression). Requires schema definition, table creation via run and s.table, and use of createPartitionedTable with the compressMethods argument. Outputs the newly created table schema.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndbPath = \"dfs://createPartitionedTable\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.VALUE, partitions=[\"IBM\", \"MS\"], dbPath=dbPath)\ns.run(\"schema_t = table(100:0, `timestamp`symbol`value, [TIMESTAMP, SYMBOL, DOUBLE])\")\nschema_t = s.table(data=\"schema_t\")\npt = db.createPartitionedTable(schema_t, \"pt\", partitionColumns=\"symbol\", compressMethods={'timestamp': \"delta\"})\nschema = s.run(f'schema(loadTable(\"{dbPath}\", \"pt\"))')\nprint(schema[\"colDefs\"])\n\n```\n\n----------------------------------------\n\nTITLE: Loading a table from DolphinDB database in Python\nDESCRIPTION: This snippet exemplifies how to load a table from a DolphinDB database into a Python environment, used after deletion of partitions to verify the current records count and contents. It requires previously loaded data.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_100\n\nLANGUAGE: Python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\", dbPath=\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Script Execution with DolphinDB Connection Pool and asyncio (Python)\nDESCRIPTION: Defines a Python class `Dolphindb` that encapsulates a `DBConnectionPool`. It uses `asyncio` and `threading` to run DolphinDB scripts asynchronously. An event loop is started in a separate thread, and `asyncio.run_coroutine_threadsafe` submits tasks from the main thread to this loop, allowing non-blocking execution of DolphinDB scripts via `pool.run` within async methods. Dependencies include `dolphindb`, `asyncio`, `threading`, `time`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport datetime\nimport time\nimport asyncio\nimport threading\nimport sys\nimport numpy\nimport pandas\n\nclass Dolphindb(object):\n\n    pool = ddb.DBConnectionPool (\"localhost\", 8848, 20)\n\n    @classmethod\n    async def test_run1(cls,script):\n        print(\"test_run1\")\n        return await cls.pool.run(script)\n\n    @classmethod\n    async def runTest(cls,script):\n        start = time.time()\n        task = loop.create_task(cls.test_run1(script))\n        result = await asyncio.gather(task)\n        print(time.time()-start)\n        print(result)\n        print (time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n        return result\n\n#Define an event loop\n\ndef start_thread_loop(loop):\n    asyncio.set_event_loop(loop)\n    loop.run_forever()\n\n\nif __name__==\"__main__\":\n    start = time.time()\n    print(\"In main thread\",threading.current_thread())\n    loop = asyncio.get_event_loop()   \n    # create an event loop and run_forever in subthread\n    \n    t = threading.Thread(target= start_thread_loop, args=(loop,))\n    t.start()\n    task1 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(1000);1+1\"),loop)\n    task2 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(3000);1+2\"),loop)\n    task3 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(5000);1+3\"),loop)\n    task4 = asyncio.run_coroutine_threadsafe(Dolphindb.runTest(\"sleep(1000);1+4\"),loop)\n\n    print('the main thread is not blocked')\n    end = time.time()\n    print(end - start)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous DolphinDB Script Execution using runTaskAsync\nDESCRIPTION: Shows how to use the `runTaskAsync` method of `DBConnectionPool` to submit DolphinDB scripts for asynchronous execution. This method returns a `concurrent.futures.Future` object immediately. The `result()` method of the Future object is then called to retrieve the result, blocking until that specific task completes. Requires the 'dolphindb' library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_96\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport time\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10)\n\nt1 = time.time()\ntask1 = pool.runTaskAsync(\"sleep(1000); 1+0\");\ntask2 = pool.runTaskAsync(\"sleep(2000); 1+1\");\ntask3 = pool.runTaskAsync(\"sleep(4000); 1+2\");\ntask4 = pool.runTaskAsync(\"sleep(1000); 1+3\");\nt2 = time.time()\nprint(task1.result())\nt3 = time.time()\nprint(task2.result())\nt4 = time.time()\nprint(task4.result())\nt5 = time.time()\nprint(task3.result())\nt6 = time.time()\nprint(t2-t1)\nprint(t3-t1)\nprint(t4-t1)\nprint(t5-t1)\nprint(t6-t1)\npool.shutDown()\n```\n\n----------------------------------------\n\nTITLE: Creating and Updating a Partitioned DolphinDB Table (Python)\nDESCRIPTION: Illustrates executing a multi-line DolphinDB script via `session.run` to create a database (`dfs://valuedb`) and a partitioned table (`pt`). It then loads this table into a Python Table object using `s.loadTable` and updates specific records using the table object's `update`, `where`, and `execute` methods. Finally, it displays the updated table content as a Pandas DataFrame using `toDF()`. Requires an active DolphinDB session (`s`) connected to a server and the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ndbPath=\"dfs://valuedb\"\ndstr = \"\"\"\ndbPath=\"dfs://valuedb\"\nif (existsDatabase(dbPath)){\n    dropDatabase(dbPath)\n}\nmydb=database(dbPath, VALUE, ['AMZN','NFLX', 'NVDA'])\nt=table(take(['AMZN','NFLX', 'NVDA'], 10) as sym, 1..10 as id, rand(10,10) as price)\nmydb.createPartitionedTable(t,`pt,`sym).append!(t)\n\n\"\"\"\nt1=s.run(dstr)\nt1=s.loadTable(tableName=\"pt\",dbPath=dbPath)\nt1.update([\"price\"],[\"11\"]).where(\"sym=`AMZN\").execute()\n\nt1.toDF()\nprint(t1.toDF())\n\n# output\n    sym  id  price\n0  AMZN   1     11\n1  AMZN   4     11\n2  AMZN   7     11\n3  AMZN  10     11\n4  NFLX   2      3\n5  NFLX   5      5\n6  NFLX   8      5\n7  NVDA   3      1\n8  NVDA   6      1\n9  NVDA   9      5\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Text File with loadTextEx\nDESCRIPTION: The `loadTextEx` function imports data from a text file into a partitioned database table. It creates the table if it doesn't exist and appends data to it. The example demonstrates creating a partitioned table named `trade`, loading data from `example.csv`, and then retrieving the number of rows, columns, and the schema of the loaded data. The function requires the database path, table name, partition columns, the file path, and the delimiter.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nloadTextEx(dbPath, tableName,  partitionColumns=None, remoteFilePath=None, delimiter=\",\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Data with PROTOCOL_ARROW\nDESCRIPTION: Demonstrates retrieving a table from DolphinDB using Arrow protocol. The result is returned as a pyarrow.Table object rather than a pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.3_PROTOCOL_ARROW.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> s.run(\"table(1..3 as a)\")\npyarrow.Table\na: int32\n----\na: [[1,2,3]]\n```\n\n----------------------------------------\n\nTITLE: Deleting Records from a DolphinDB Table Object (Python)\nDESCRIPTION: Demonstrates deleting records from a DolphinDB table object loaded via `s.loadText`. It uses the `delete` method chained with `where` to specify the condition for deletion and `execute` to perform the operation. The number of remaining rows is printed to verify the deletion. Requires an active DolphinDB session (`s`), the `dolphindb` library, and assumes `WORK_DIR` is defined.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\ntrade.delete().where('date<2013.01.01').execute()\nprint(trade.rows)\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Distributed Table and Handling Status in Python\nDESCRIPTION: This example demonstrates inserting 100 rows of data to a distributed table using MultithreadedTableWriter, waiting for completion, checking the status, and verifying the row count in the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_87\n\nLANGUAGE: Python\nCODE:\n```\ntry:\n    # 插入100行正确数据 \n    for i in range(100):\n        res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\nexcept Exception as ex:\n    # MTW 抛出异常\n    print(\"MTW exit with exception %s\" % ex)\n# 等待 MTW 插入完成\nwriter.waitForThreadCompletion()\nwriteStatus=writer.getStatus()\nif writeStatus.succeed():\n    print(\"Write successfully!\")\nprint(\"writeStatus: \\n\", writeStatus)\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Uploading pandas DataFrame and Checking Type via DolphinDB Python API - Python\nDESCRIPTION: Describes uploading a pandas DataFrame to DolphinDB, confirming upload by checking its type, and retrieving it as an in-memory table using the Python API. Requires pandas, numpy, and a valid session object (s). The DataFrame's index is not preserved and output is a plain table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame({'id': np.int32([1, 4, 3, 2, 3]),\n    'date': np.array(['2019-02-03','2019-02-04','2019-02-05','2019-02-06','2019-02-07'], dtype='datetime64[D]'),\n    'value': np.double([7.8, 4.6, 5.1, 9.6, 0.1]),},\n    index=['one', 'two', 'three', 'four', 'five'])\n\ns.upload({'a':a})\ns.run(\"typestr\",a)\n# output\n'IN-MEMORY TABLE'\n\ns.run('a')\n# output\n   id date        value\n0  1  2019-02-03  7.8\n1  4  2019-02-04  4.6\n2  3  2019-02-05  5.1\n3  2  2019-02-06  9.6\n4  3  2019-02-07  0.1\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Appending to Stream Table (Python)\nDESCRIPTION: Illustrates appending data asynchronously to a DolphinDB stream table. A session is created with `enableASYNC=True`. A stream table is defined using `s.run`. Random data is generated into a Pandas DataFrame, and then appended asynchronously to the stream table using `append!` within `s.run`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport random\nimport datetime\n\ns = ddb.session(enableASYNC=True)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nn = 100\n\nscript = \"\"\"trades = streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,DOUBLE,INT])\"\"\"\ns.run(script) # 此处的脚本可以在服务端直接运行\n\n# 随机生成一个 dataframe\nsym_list = ['IBN', 'GTYU', 'FHU', 'DGT', 'FHU', 'YUG', 'EE', 'ZD', 'FYU']\nprice_list = []\ntime_list = []\nfor i in range(n):\n    price_list.append(round(np.random.uniform(1, 100), 1))\n    time_list.append(np.datetime64(datetime.date(2020, random.randint(1, 12), random.randint(1, 20))))\n\ntb = pd.DataFrame({'time': time_list,\n                   'sym': np.random.choice(sym_list, n),\n                   'price': price_list,\n                   'id': np.random.choice([1, 2, 3, 4, 5], n)})\n\ns.run(\"append!{trades}\", tb)\n```\n\n----------------------------------------\n\nTITLE: Creating VALUE Partitioned Database (Date) in DolphinDB via Python\nDESCRIPTION: Creates a DolphinDB DFS database ('dfs://db_value_date') with VALUE partitioning based on daily dates. It first checks if the database exists and drops it if necessary. Then, it defines date partitions, creates the database using `s.database()`, creates a Pandas DataFrame, uploads it as a temporary table, creates a partitioned table within the database using `db.createPartitionedTable()`, appends the data, and finally loads the table to verify.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndbPath=\"dfs://db_value_date\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndates=np.array(pd.date_range(start='20120101', end='20120110'), dtype=\"datetime64[D]\")\ndb = s.database(dbName='mydb', partitionType=keys.VALUE, partitions=dates,dbPath=dbPath)\ndf = pd.DataFrame({'datetime':np.array(['2012-01-01T00:00:00', '2012-01-02T00:00:00'], dtype='datetime64'), 'sym':['AA', 'BB'], 'val':[1,2]})\nt = s.table(data=df)\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns='datetime').append(t)\nre=s.loadTable(tableName='pt', dbPath=dbPath).toDF()\n```\n\n----------------------------------------\n\nTITLE: Using DolphinDB MultithreadedTableWriter in Python Threads\nDESCRIPTION: Demonstrates using a single `MultithreadedTableWriter` instance concurrently from multiple Python threads. It initializes a DolphinDB session, sets up a partitioned table, creates the MTW instance, defines a function (`insert_MTW`) to insert data into the writer, and then spawns 10 Python threads using `threading.Thread`, each executing the insertion function. It waits for all Python threads to finish using `thread.join()`, then waits for the MTW background threads to complete using `writer.waitForThreadCompletion()`, and finally checks the overall write status. Requires `numpy`, `pandas`, `dolphindb`, `time`, `threading`, and `random` modules.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport time\nimport threading\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"\n    dbName = 'dfs://valuedb3';\n    if(existsDatabase(dbName)){\n        dropDatabase(dbName);\n    }\n    # Example table schema - adjusted to match insert function\n    datetest=table(1000:0,`id`symbol`value,[INT,SYMBOL,INT]); \n    db = database(directory=dbName, partitionType=HASH, partitionScheme=[INT, 10]);\n    pt=db.createPartitionedTable(datetest,'pdatetest','id');\n\"\"\"\ns.run(script)\n\nwriter = ddb.MultithreadedTableWriter(\n    \"localhost\", 8848, \"admin\", \"123456\", dbPath=\"dfs://valuedb3\", tableName=\"pdatetest\",\n    batchSize=10000, throttle=1, threadCount=5, partitionCol=\"id\", compressMethods=[\"LZ4\",\"LZ4\",\"DELTA\"]\n)\n\ndef insert_MTW(writer):\n    try:\n        # 插入100行正确数据 \n        for i in range(100):\n            # Insert data matching the table schema (INT, SYMBOL, INT)\n            res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\n    except Exception as ex:\n        # MTW 抛出异常\n        print(\"MTW exit with exception %s\" % ex)\n\n# 创建 10 个线程，在线程中将数据写入MTW\nthreads = []\nfor i in range(10):\n    threads.append(threading.Thread(target=insert_MTW, args=(writer,))) # Pass writer as argument\n\nfor thread in threads:\n    thread.start()\n\n# 完成其他任务，此处用 sleep 模拟\n# time.sleep(10) # Optional: Simulate other work\n\n# 现在需要结束任务\n# 1 - 等待线程退出\nfor thread in threads:\n    thread.join()\n# 2 - 等待 MTW 线程结束\nwriter.waitForThreadCompletion()\n# 3 - 检查写入结果\nwriteStatus = writer.getStatus()\nprint(\"writeStatus:\\n\", writeStatus)\nprint(s.run(\"select count(*) from dfs://valuedb3.pdatetest\")) # Query the correct table name\n\ns.close() # Close the session\n```\n\n----------------------------------------\n\nTITLE: Concurrent Asynchronous Task Execution with DolphinDB runTaskAsync (Python)\nDESCRIPTION: Demonstrates using the `runTaskAsync` method of a `DBConnectionPool` to submit multiple DolphinDB scripts for concurrent, asynchronous execution. The method returns `Future` objects, and `result()` is called on each future to retrieve the task's outcome, blocking until the specific task completes. Timestamps illustrate the non-blocking nature of task submission and the blocking nature of `result()`. Requires `dolphindb` and `time` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport time\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10)\n\nt1 = time.time()\ntask1 = pool.runTaskAsync(\"sleep(1000); 1+0\");\ntask2 = pool.runTaskAsync(\"sleep(2000); 1+1\");\ntask3 = pool.runTaskAsync(\"sleep(4000); 1+2\");\ntask4 = pool.runTaskAsync(\"sleep(1000); 1+3\");\nt2 = time.time()\nprint(task1.result())\nt3 = time.time()\nprint(task2.result())\nt4 = time.time()\nprint(task4.result())\nt5 = time.time()\nprint(task3.result())\nt6 = time.time()\nprint(t2-t1)\nprint(t3-t1)\nprint(t4-t1)\nprint(t5-t1)\nprint(t6-t1)\npool.shutDown()\n```\n\n----------------------------------------\n\nTITLE: Creating COMPO Partitioned Database and Table - DolphinDB Python\nDESCRIPTION: Shows a two-layer partitioning approach by composing a VALUE (date) and RANGE (int) partition, followed by creation of a top-level COMPO-partitioned database. The COMPO child partitions should have dbPath set to empty or not specified. Prerequisites include prior initialization and proper sequence in constructing child databases.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndb1 = s.database(partitionType=keys.VALUE, partitions=np.array([\"2012-01-01\", \"2012-01-06\"], dtype=\"datetime64[D]\"))\ndb2 = s.database(partitionType=keys.RANGE, partitions=[1, 6, 11])\ndbPath=\"dfs://db_compo_test\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.COMPO, partitions=[db1, db2], dbPath=dbPath)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing tableUpsert Class in Python\nDESCRIPTION: Defines the constructor signature for the `ddb.tableUpsert` class in the DolphinDB Python API. It requires parameters like `dbPath` (optional for memory tables), `tableName`, the active `ddbSession`, and optional `ignoreNull`, `keyColNames` (crucial for DFS tables), and `sortColumns`. This object is used to perform upsert operations on DolphinDB tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.2_TableUpserter.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntableUpsert(dbPath=None, tableName=None, ddbSession=None, ignoreNull=False, keyColNames=[], sortColumns=[])\n```\n\n----------------------------------------\n\nTITLE: Writing Data with DolphinDB MultithreadedTableWriter in Python\nDESCRIPTION: Demonstrates inserting multiple rows into the configured `MultithreadedTableWriter`. It loops 100 times, calling `writer.insert()` with randomly generated data for each row. Includes basic error checking within the loop using `res.hasError()`. After the loop, it waits for all background writing threads to complete using `writer.waitForThreadCompletion()` and then retrieves and prints the final write status using `writer.getStatus()`. Assumes `writer` is an initialized `MultithreadedTableWriter` object and `s` is a connected `dolphindb.session`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\ntry:\n    # 插入100行正确数据 \n    for i in range(100):\n        res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\n        if res.hasError():\n            print(\"MTW insert error: \", res.errorInfo)\nexcept Exception as ex:\n    # MTW 抛出异常\n    print(\"MTW exit with exception: \", ex)\n# 等待 MTW 插入完成\nwriter.waitForThreadCompletion()\nwriteStatus = writer.getStatus()\nif writeStatus.succeed():\n    print(\"Write successfully!\")\nprint(\"writeStatus: \\n\", writeStatus)\n# Assuming 's' is a connected dolphindb session\n# print(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Uploading and executing DolphinDB 'typestr' with pandas DataFrame\nDESCRIPTION: This snippet demonstrates how to upload a pandas DataFrame to the DolphinDB server and then execute a function (typestr) against it. The DataFrame's data types are converted to corresponding DolphinDB types during the upload. Shows how the dataframe will lose its index column after uploaded.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame({'id': np.int32([1, 4, 3, 2, 3]),\n    'date': np.array(['2019-02-03','2019-02-04','2019-02-05','2019-02-06','2019-02-07'], dtype='datetime64[D]'),\n    'value': np.double([7.8, 4.6, 5.1, 9.6, 0.1]),},\n    index=['one', 'two', 'three', 'four', 'five'])\n\ns.upload({'a':a})\ns.run(\"typestr\",a)\n# output\n'IN-MEMORY TABLE'\n\ns.run('a')\n# output\n   id date        value\n0  1  2019-02-03  7.8\n1  4  2019-02-04  4.6\n2  3  2019-02-05  5.1\n3  2  2019-02-06  9.6\n4  3  2019-02-07  0.1\n```\n\n----------------------------------------\n\nTITLE: Uploading pandas DataFrame and Computing Column Average with DolphinDB Python API - Python\nDESCRIPTION: Details how to upload a pandas DataFrame containing integer columns to DolphinDB and then calculate the average of a specified column remotely. Requires pandas, numpy, and a session (s). The DataFrame must have uniform column data types.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'id': np.int32([1, 2, 3, 6, 8]), 'x': np.int32([5, 4, 3, 2, 1])})\ns.upload({'t1': df})\nprint(s.run(\"t1.x.avg()\"))\n# output\n3.0\n```\n\n----------------------------------------\n\nTITLE: Accessing Existing DolphinDB DFS Table in Python\nDESCRIPTION: Uses `s.table()` to obtain a Python object representing an existing DolphinDB DFS table named `trade` located within the database path `dfs://valuedb`. This allows interaction with the table data. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.table(dbPath=\"dfs://valuedb\", data=\"trade\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB Session in Asynchronous Mode Using Python\nDESCRIPTION: This snippet illustrates how to create a DolphinDB session with asynchronous mode enabled in Python by setting the 'enableASYNC' parameter to True. The session connects to a DolphinDB server, allowing subsequent commands to be submitted asynchronously. No return values are received from asynchronous calls, which reduces client-side waiting time.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.1_SessionAsyncMode.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ns = ddb.session(enableASYNC=True)\n```\n\n----------------------------------------\n\nTITLE: Using addTask, isFinished, and getData with DBConnectionPool in Python\nDESCRIPTION: Demonstrates how to use the addTask method to submit a task with a specific taskId, check its status with isFinished, and retrieve results with getData after completion.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.2_AsyncMethodsAndOthers.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport time\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8)\ntaskid = 12\npool.addTask(\"sleep(1500);1+2\", taskId=taskid)\nwhile True:\n    if pool.isFinished(taskId=taskid):\n        break\n    time.sleep(0.01)\n\nres = pool.getData(taskId=taskid)\nprint(res)\n\n# output:\n3\n```\n\n----------------------------------------\n\nTITLE: Appending to Distributed Table via append! in Python\nDESCRIPTION: Demonstrates appending data from a Pandas DataFrame (created by `createDemoDataFrame()`) to a DolphinDB distributed table using the `append!` function within `s.run`. This method is generally not recommended for data saving as it returns the table structure, increasing network traffic.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ntb = createDemoDataFrame()\ns.run(\"append!{{loadTable('{db}', `{tb})}}\".format(db=dbPath,tb=tableName),tb)\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Appending to Distributed Table (Python)\nDESCRIPTION: Demonstrates asynchronous data appending to a DolphinDB distributed table. It initializes a session with `enableASYNC=True`, creates a partitioned table using `s.run`, prepares a Pandas DataFrame, and appends the data asynchronously using `append!` within `s.run`. Note that `s.run` has no return value in asynchronous mode.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport dolphindb.settings as keys\nimport pandas as pd\n\ns = ddb.session(enableASYNC=True) # 打开异步模式\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\ndbPath = \"dfs://testDB\"\ntableName = \"tb1\"\n\nscript = \"\"\"\ndbPath=\"dfs://testDB\"\n\ntableName=`tb1\n\nif(existsDatabase(dbPath))\n    dropDatabase(dbPath)\ndb=database(dbPath, VALUE, [\"AAPL\", \"AMZN\", \"A\"])\n\ntestDictSchema=table(5:0, `id`ticker`price, [INT,STRING,DOUBLE])\n\ntb1=db.createPartitionedTable(testDictSchema, tableName, `ticker)\n\"\"\"\ns.run(script) #此处脚本可以在服务器端运行\n\ntb = pd.DataFrame({'id': [1, 2, 2, 3],\n                   'ticker': ['AAPL', 'AMZN', 'AMZN', 'A'],\n                   'price': [22, 3.5, 21, 26]})\n\ns.run(\"append!{{loadTable('{db}', `{tb})}}\".format(db=dbPath, tb=tableName), tb)\n```\n\n----------------------------------------\n\nTITLE: Appending a DataFrame to Memory Table Using tableInsert - No Time Columns (DolphinDB Python API)\nDESCRIPTION: Illustrates how to append a pandas DataFrame directly to a DolphinDB memory table without time columns using tableInsert. DataFrame columns must align in name and type with destination table. Requires dolphindb, pandas, and a connected session. Outputs the number of rows inserted and the updated table structure.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nimport pandas as pd\nscript = \"\"\"t = table(1000:0,`id`ticker`price, [INT,SYMBOL,DOUBLE])\nshare t as tglobal\"\"\"\ns.run(script)\n\n# 生成要追加的 DataFrame\ntb=pd.DataFrame({'id': [1, 2, 2, 3],\n                 'ticker': ['AAPL', 'AMZN', 'AMZN', 'A'],\n                 'price': [22, 3.5, 21, 26]})\ns.run(\"tableInsert{tglobal}\",tb)\n\n#output\n4\n\ns.run(\"tglobal\")\n#output\n   id\tticker\tprice\n0\t1\tAAPL\t22.0\n1\t2\tAMZN\t3.5\n2\t2\tAMZN\t21.0\n3\t3\tA\t26.0\n```\n\n----------------------------------------\n\nTITLE: Initializing PartitionedTableAppender in DolphinDB (Python)\nDESCRIPTION: This code snippet initializes a `PartitionedTableAppender` object in DolphinDB using Python. It requires the `dolphindb` library for connecting to the database and the `DBConnectionPool` object for handling concurrent writes. The appender is configured to write data to a partitioned table, and the `dbConnectionPool` object is passed as a parameter.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.3_PartitionedTableAppender.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nPartitionedTableAppender(dbPath=None, tableName=None, partitionColName=None, dbConnectionPool=None)\n```\n\n----------------------------------------\n\nTITLE: Checking Table Existence with existsTable\nDESCRIPTION: The `existsTable` function checks for the presence of a table within a specified database. The function takes `dbUrl` and `tableName` as input and returns a boolean value indicating whether the table exists.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> s.existsTable(dbUrl=\"dfs://valuedb\", tableName=\"trade\")\nTrue\n```\n\n----------------------------------------\n\nTITLE: Enabling Streaming Data Subscription with DolphinDB Python API\nDESCRIPTION: This snippet shows how to enable streaming data subscription inside a DolphinDB Python client session with the enableStreaming method, specifying the port parameter. It explains version-dependent behavior of the port parameter and notes that from server version 2.00.9 onwards, the port parameter is ignored due to server push improvements. Dependencies include a DolphinDB session instance. This prepares the client to receive published streaming data from the server without needing to open a listening port.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_142\n\nLANGUAGE: python\nCODE:\n```\ns.enableStreaming(port=0)\n```\n\n----------------------------------------\n\nTITLE: Enabling PROTOCOL_ARROW in DolphinDB Python API\nDESCRIPTION: Shows how to establish connections to DolphinDB using Apache Arrow protocol via both single session and connection pool methods.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.3_PROTOCOL_ARROW.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, \"admin\", \"123456\", 10, protocol=keys.PROTOCOL_ARROW)\n```\n\n----------------------------------------\n\nTITLE: Using the upsert Method in Python\nDESCRIPTION: Shows the signature for the `upsert` method of the `tableUpsert` object. This method takes a single argument, `table`, which is typically a Pandas DataFrame containing the data to be upserted into the target DolphinDB table configured during the `tableUpsert` object initialization.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.2_TableUpserter.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nupsert(table)\n```\n\n----------------------------------------\n\nTITLE: Uploading DataFrame with Explicit Type Conversion - Python\nDESCRIPTION: This snippet demonstrates uploading a pandas DataFrame to DolphinDB with explicit type conversion. It adds the `__DolphinDB_Type__` attribute to the DataFrame, specifying the desired data types for each column.  The `keys.DT_INT`, `keys.DT_SYMBOL`, and `keys.DT_BLOB` constants are used. The output shows the schema after the upload, confirming the successful type conversions.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.4_ForceTypeCasting.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\ndf.__DolphinDB_Type__ = {\n    'cint': keys.DT_INT,\n    'csymbol': keys.DT_SYMBOL,\n    'cblob': keys.DT_BLOB,\n}\n\ns.upload({\"df_true\": df})\nprint(s.run(\"schema(df_true)\")['colDefs'])\n```\n\n----------------------------------------\n\nTITLE: Batch Appending Python Lists to a Memory Table with tableInsert - DolphinDB Python API (Python)\nDESCRIPTION: Demonstrates batch insertion by passing multiple Python lists/ndarrays to the tableInsert function for a global DolphinDB memory table. Connects to server, prepares data in list/numpy array form, and calls s.run with tableInsert, minimizing server round-trips. Requires dolphindb and numpy. Input types must match table definition to avoid type errors.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nids = [1,2,3]\ndates = np.array(['2019-03-03','2019-03-04','2019-03-05'], dtype=\"datetime64[D]\")\ntickers=['AAPL','GOOG','AAPL']\nprices = [302.5, 295.6, 297.5]\nargs = [ids, dates, tickers, prices]\ns.run(\"tableInsert{tglobal}\", args)\n#output\n3\n\ns.run(\"tglobal\")\n#output\n   id       date ticker  price\n0   1 2019-03-03   AAPL  302.5\n1   2 2019-03-04   GOOG  295.6\n2   3 2019-03-05   AAPL  297.5\n```\n\n----------------------------------------\n\nTITLE: Streaming Subscription: Batch Mode (msgAsTable=False)\nDESCRIPTION: This snippet illustrates a batch subscription mode with `msgAsTable=False`. It connects to the DolphinDB server, creates a stream table, enables streaming, defines a handler, and subscribes to the stream. The `batchSize` and `throttle` parameters are set to 2 and 0.1 respectively. When a batch of data is received exceeding `batchSize` or the waiting time exceeds `throttle`, the handler is triggered, which receives a list of data entries. The code then inserts data into the stream table, waits for 3 seconds, and unsubscribes.  Each element of the list corresponds to a single data row and PROTOCOL_DDB is used for deserialization.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.3_SubscriptionOptions/3.3_SubscriptionOptions.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport time\n\ns = ddb.session()\ns.connect(\"192.168.1.113\", 8848, \"admin\", \"123456\")\n\ns.run(\"\"\"\nshare streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,DOUBLE,INT]) as trades\n\"\"\")\n\ns.enableStreaming()\n\ndef handler(lsts):\n    print(lsts)\n\ns.subscribe(\"192.168.1.113\", 8848, handler, \"trades\", \"MultiMode1\", offset=-1, batchSize=2, throttle=0.1, msgAsTable=False)\n\ns.run(\"insert into trades values(take(now(), 6), take(\\`000905\\`600001\\`300201\\`000908\\`600002, 6), rand(1000,6)/10.0, 1..6)\")\n\ntime.sleep(3)\n\ns.unsubscribe(\"192.168.1.113\", 8848, \"trades\", \"MultiMode1\")\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB OHLC Stream Table with Python\nDESCRIPTION: Connects Python to the DolphinDB server, enables streaming on a local port, defines a simple handler function to print received data, and subscribes to the `OHLC` stream table to receive and display the real-time calculated OHLC bars. Requires the Dolphindb Python API and a running DolphinDB server with the `OHLC` table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_111\n\nLANGUAGE: Python\nCODE:\n```\nfrom threading import Event\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\ns=ddb.session()\n# set local port 20001 for subscribed streaming data\ns.enableStreaming(20001)\ndef handler(lst):\n    print(lst)\n# subscribe to the stream table OHLC (local port 8848)\ns.subscribe(\"192.168.1.103\", 8921, handler, \"OHLC\")\nEvent().wait()\n\n# output\n[numpy.datetime64('2018-09-03T09:31:00'), '000001', 10.13, 10.15, 10.1, 10.14, 586160]\n[numpy.datetime64('2018-09-03T09:32:00'), '000001', 10.13, 10.16, 10.1, 10.15, 1217060]\n[numpy.datetime64('2018-09-03T09:33:00'), '000001', 10.13, 10.16, 10.1, 10.13, 1715460]\n[numpy.datetime64('2018-09-03T09:34:00'), '000001', 10.13, 10.16, 10.1, 10.14, 2268260]\n[numpy.datetime64('2018-09-03T09:35:00'), '000001', 10.13, 10.21, 10.1, 10.2, 3783660]\n...\n```\n\n----------------------------------------\n\nTITLE: Performing Least Squares Regression (OLS) with DolphinDB Python API\nDESCRIPTION: This code demonstrates how to perform ordinary least squares (OLS) regression on financial market data loaded from a compressed CSV into a distributed database. It shows how to construct calculated columns, filter data, and call the ols method with independent and dependent variables. The regression result includes coefficient estimates, ANOVA statistics, and regression metrics printed as dataframes. Attention is drawn to the use of the double backslash operator within SQL expressions to correctly parse division in DolphinDB's select syntax.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_140\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\nif s.existsDatabase(\"dfs://US\"):\n\ts.dropDatabase(\"dfs://US\")\ns.database(dbName='USdb', partitionType=keys.VALUE, partitions=[\"GFGC\",\"EWST\", \"EGAS\"], dbPath=\"dfs://US\")\nUS=s.loadTextEx(dbPath=\"dfs://US\", partitionColumns=[\"TICKER\"], tableName='US', remoteFilePath=WORK_DIR + \"/US.csv\")\n\nresult = s.loadTable(tableName=\"US\",dbPath=\"dfs://US\")\\\n         .select(\"select VOL\\\\SHROUT as turnover, abs(RET) as absRet, (ASK-BID)/(BID+ASK)*2 as spread, log(SHROUT*(BID+ASK)/2) as logMV\")\\\n         .where(\"VOL>0\").ols(\"turnover\", [\"absRet\",\"logMV\", \"spread\"], True)\n\nprint(result)\n\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\nz=trade.select(['bid','ask','prc']).ols('PRC', ['BID', 'ASK'])\n\nprint(z[\"ANOVA\"])\nprint(z[\"RegressionStat\"])\nprint(z[\"Coefficient\"])\nprint(z[\"Coefficient\"].beta[1])\n```\n\n----------------------------------------\n\nTITLE: Inserting Data with BatchTableWriter in Python\nDESCRIPTION: Inserts a single row of data into a specified table previously added via addTable. Requires tableName, optional dbPath for disk tables, and variable arguments (*args) representing the row data. Data types and count must match the table schema.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nwriter.insert(dbPath=None, tableName=None, *args)\n```\n\n----------------------------------------\n\nTITLE: Uploading DataFrame with Decimal Precision - Python\nDESCRIPTION: This snippet shows how to upload a pandas DataFrame with DECIMAL32 and DECIMAL64 columns, specifying their precision. The `__DolphinDB_Type__` attribute is used to define the data types and the precision for each decimal column. The output shows the schema of the uploaded table and the content of the table, demonstrating correct handling of precision.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.4_ForceTypeCasting.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom decimal import Decimal\ndf = pd.DataFrame({\n    'decimal32': [Decimal(\"NaN\"), Decimal(\"1.22\")],\n    'decimal64': [Decimal(\"1.33355\"), Decimal(\"NaN\")],\n})\ndf.__DolphinDB_Type__ = {\n    'decimal32': [keys.DT_DECIMAL32, 2],\n    'decimal64': [keys.DT_DECIMAL64, 5],\n}\n\ns.upload({'df': df})\nprint(s.run(\"schema(df)\")['colDefs'])\nprint('-' * 30)\nprint(s.run(\"df\"))\n```\n\n----------------------------------------\n\nTITLE: Custom Ascending/Descending Order in Sorts - DolphinDB Python\nDESCRIPTION: Shows how to specify sorting direction via the 'ascending' parameter for both 'sort' and 'csort'. You can pass a boolean or a list of booleans to specify per-column sort direction. This enables compound sorting of multiple columns with individualized order for each. Requires DolphinDB Python API.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_127\n\nLANGUAGE: Python\nCODE:\n```\nsort(by, ascending=True)\ncsort(by, ascending=True)\n```\n\n----------------------------------------\n\nTITLE: Constructing Heterogeneous Stream Table in DolphinDB\nDESCRIPTION: Defines two in-memory tables with different schemas and shares them as named shared tables. Then combines these shared tables into a heterogeneous stream table named 'outTables'. Uses replay to emit streamed data from the input tables, keyed by message identifiers. Key dependencies include DolphinDB runtime and its streaming capabilities. Parameters such as schemas and table names are specified. Expected output is a stream table aggregating data from both input tables. Constraints include correct matching of column names and types for streaming.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.3_SubscriptionOptions/3.3_SubscriptionOptions.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry{dropStreamTable(`outTables)}catch(ex){}\n// 构造输出流表\nshare streamTable(100:0, `timestampv`sym`blob`price1,[TIMESTAMP,SYMBOL,BLOB,DOUBLE]) as outTables\n\nn = 6;\ntable1 = table(100:0, `datetimev`timestampv`sym`price1`price2, [DATETIME, TIMESTAMP, SYMBOL, DOUBLE, DOUBLE])\ntable2 = table(100:0, `datetimev`timestampv`sym`price1, [DATETIME, TIMESTAMP, SYMBOL, DOUBLE])\ntableInsert(table1, 2012.01.01T01:21:23 + 1..n, 2018.12.01T01:21:23.000 + 1..n, take(`a`b`c,n), rand(100,n)+rand(1.0, n), rand(100,n)+rand(1.0, n))\ntableInsert(table2, 2012.01.01T01:21:23 + 1..n, 2018.12.01T01:21:23.000 + 1..n, take(`a`b`c,n), rand(100,n)+rand(1.0, n))\nshare table1 as pt1\nshare table2 as pt2\n\nd = dict(['msg1', 'msg2'], [pt1, pt2])\nreplay(inputTables=d, outputTables=`outTables, dateColumn=`timestampv, timeColumn=`timestampv)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Stream with Python Using StreamDeserializer\nDESCRIPTION: Connects to a DolphinDB server from Python using the dolphindb API, enables streaming, and defines a stream deserializer that maps message identifiers ('msg1', 'msg2') to corresponding shared table names ('pt1', 'pt2'). Subscribes to the heterogeneous stream table 'outTables' with a batch size of 4, using a custom callback handler to process incoming batches. The handler prints batches of deserialized data combining records from different tables in order of their arrival. Dependencies include dolphindb Python client, a live DolphinDB server with shared tables established, and network connectivity. Inputs are the connection parameters and stream settings; outputs are printed batches of combined streaming data. The batch processing enables handling multiple records from heterogeneous sources in grouped callbacks.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.3_SubscriptionOptions/3.3_SubscriptionOptions.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\n\ndef streamDeserializer_handler(lsts):\n    print(lsts)\n\ns = ddb.session()\ns.connect(\"192.168.1.113\", 8848, \"admin\", \"123456\")\ns.enableStreaming()\n\nsd = ddb.streamDeserializer({\n    'msg1': \"pt1\",\n    'msg2': \"pt2\",\n}, session=s)\ns.subscribe(host=\"192.168.1.113\", port=8848, handler=streamDeserializer_handler, tableName=\"outTables\", actionName=\"action\", offset=0, resub=False, batchSize=4,\n            msgAsTable=False, streamDeserializer=sd, userName=\"admin\", password=\"123456\")\n\nfrom threading import Event\nEvent().wait()\n```\n\n----------------------------------------\n\nTITLE: Subscribing DolphinDB Engine to Trade Stream Table\nDESCRIPTION: Establishes a subscription from the previously created DolphinDB time-series aggregator engine (`tsAggrKline`) to the `Trade` stream table. Incoming data in `Trade` will be fed directly to the engine for real-time OHLC calculation. Requires a running DolphinDB server and the `Trade` stream table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_110\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"Trade\", actionName=\"act_tsaggr\", offset=0, handler=append!{tsAggrKline}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Example of PROTOCOL_PICKLE Deserialization for DolphinDB Table to pandas DataFrame - Python\nDESCRIPTION: This snippet shows how a DolphinDB Table, containing columns with potential NULLs and different data types, is deserialized into a pandas DataFrame using the PROTOCOL_PICKLE protocol. It demonstrates the visible table output, followed by checking the dtype of individual columns to verify correct mapping to NumPy dtypes (float64 and datetime64[ns] respectively). The snippet assumes a running session object and uses the 'run' method to execute DolphinDB queries that return Table data. This example highlights the handling of NULL values and datetime-type columns during deserialization. Inputs are query strings producing Table results; outputs are pandas DataFrames with corresponding Python object types.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.2_PROTOCOL_PICKLE.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> re = s.run(\"table([1, NULL] as a, [2012.01.02, 2012.01.05] as b)\")\n>>> re\n     a          b\n0  1.0 2012-01-02\n1  NaN 2012-01-05\n>>> re['a'].dtype\nfloat64\n>>> re['b'].dtype\ndatetime64[ns]\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Reconnect on Connection Loss in DolphinDB Python API\nDESCRIPTION: Enables automatic reconnection when a connection is lost by setting reconnect=True in the connect method. Should be used when high availability is not enabled. Improves fault tolerance in the client application.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.2_Connect.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\n# Connect with auto-reconnect enabled\ns.connect(host=\"localhost\", port=8848, reconnect=True)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Thread Completion in MultithreadedTableWriter\nDESCRIPTION: The waitForThreadCompletion method pauses the calling thread until all background worker threads in the MultithreadedTableWriter have completed their tasks.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_83\n\nLANGUAGE: Python\nCODE:\n```\nwaitForThreadCompletion()\n```\n\n----------------------------------------\n\nTITLE: Inserting Data with DolphinDB MultithreadedTableWriter in Python\nDESCRIPTION: This snippet demonstrates how to use DolphinDB MultithreadedTableWriter in Python to perform concurrent data insertion into a shared keyed table using multiple threads. It creates and runs two daemon threads each invoking the `insert_mtw` function, which inserts rows with timestamp text and random IDs into the table. The writer is configured in UPSERT mode with options to ignore nulls and specify 'id' as the key column. After threads complete, it waits for the writer to finish, then retrieves and prints the write status. Dependencies include dolphindb Python API, threading, random, time, numpy, pandas, and proper DolphinDB server connection parameters. Inputs are the table name and connection info, and output is the multi-threaded insertion status. Limitations include ensuring thread safety via the MultithreadedTableWriter and proper server sharing of the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport dolphindb.settings as keys\nimport threading\n\nHOST = \"192.168.1.193\"\nPORT = 8848\ns = ddb.session()\ns.connect(HOST, PORT, \"admin\", \"123456\")\nscript_DFS_HASH = \"\"\"\n    testtable=keyedTable(`id,1000:0,`text`id,[STRING,LONG])\n    share testtable as ttable\n    \"\"\"\ns.run(script_DFS_HASH)\n\ndef insert_mtw(writer, id):\n    try:\n        print(\"thread\",id,\"start.\")\n        for i in range(1000):\n            text=str(time.time())\n            id=random.randint(1, 10)\n            print(text,id)\n            res=writer.insert(text, id)\n        print(\"thread\",id,\"exit.\")\n    except Exception as e:\n        print(e)\n\nprint(\"test start.\")\nwriter = ddb.MultithreadedTableWriter(HOST, PORT,\"admin\",\"123456\",\"\",\"ttable\",False,False,[], 1, 0.1, 1,\"id\",mode=\"UPSERT\",\n                                      modeOption=[\"ignoreNull=false\",\"keyColNames=`id\"])\nthreads=[]\nfor i in range(2):\n    threads.append(threading.Thread(target=insert_mtw, args=(writer,i,)))\nfor t in threads:\n    t.setDaemon(True)\n    t.start()\nfor t in threads:\n    t.join()\nwriter.waitForThreadCompletion()\nstatus=writer.getStatus()\nprint(\"test exit\",status)\n```\n\n----------------------------------------\n\nTITLE: Creating HASH Partitioned Database (Int) in DolphinDB via Python\nDESCRIPTION: Creates a DolphinDB DFS database ('dfs://db_hash_int') with HASH partitioning. It checks/drops the existing database, defines the HASH partition scheme using the data type (INT) and the number of partitions (2), creates the database, prepares data, uploads it, creates the partitioned table 'pt' based on the 'id' column, appends data, and loads the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndbPath=\"dfs://db_hash_int\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(dbName='mydb', partitionType=keys.HASH, partitions=[keys.DT_INT, 2], dbPath=dbPath)\ndf = pd.DataFrame({'id':[1,2,3,4,5], 'val':[10, 20, 30, 40, 50]})\nt = s.table(data=df)\npt = db.createPartitionedTable(table=t, tableName='pt', partitionColumns='id')\npt.append(t)\nre = s.loadTable(tableName='pt', dbPath=dbPath).toDF()\n```\n\n----------------------------------------\n\nTITLE: Selecting a Range of Records using LIMIT in Python\nDESCRIPTION: Loads a table from a DolphinDB database path. It selects all columns (`*`) and uses `limit([2, 5])` to retrieve 5 records starting from the row with index 2 (the third row). The result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_115\n\nLANGUAGE: python\nCODE:\n```\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").limit([2, 5])\nprint(t1.toDF())\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Time-Series Aggregator (Overlapping)\nDESCRIPTION: Configures a DolphinDB `createTimeSeriesAggregator` for calculating OHLC bars on the `Trade` stream table with overlapping windows (5-minute `windowSize=300`, 1-minute `step=60`). It specifies the metrics, input table (`Trade`), output table (`OHLC`), time column (`Datetime`), and key column (`Symbol`). Requires a running DolphinDB server and the `Trade` and `OHLC` tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_109\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntsAggrKline = createTimeSeriesAggregator(name=\"aggr_kline\", windowSize=300, step=60, metrics=<[first(Price),max(Price),min(Price),last(Price),sum(volume)]>, dummyTable=Trade, outputTable=OHLC, timeColumn=`Datetime, keyColumn=`Symbol)\n```\n\n----------------------------------------\n\nTITLE: Downloading Vector Data Using DolphinDB Python API - Python\nDESCRIPTION: Demonstrates how to download DolphinDB Vector data using the Python API and verify the data type and numpy dtype of the returned object. It shows that a BOOL Vector without null values returns a numpy ndarray with dtype bool, whereas one with null values returns a numpy ndarray with dtype object. Dependencies include having a connected DolphinDB session object 's'. The input is a DolphinDB Vector expression passed to the run method, and the output is a corresponding Python numpy ndarray demonstrating dtype changes based on content.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> re = s.run(\"[true, false]\")\n>>> re\n[ True False]\n>>> type(re)\n<class 'numpy.ndarray'>\n>>> re.dtype\nbool\n\n>>> re = s.run(\"[true, None]\")\n>>> re\n[True None]\n>>> re.dtype\nobject\n```\n\n----------------------------------------\n\nTITLE: Configuring Keep-Alive Time for DolphinDB Session TCP Connections in Python\nDESCRIPTION: Demonstrates how to adjust the TCP keepAliveTime parameter for the session, which sets the interval between TCP keep-alive probes in seconds. The example sets the interval to 120 seconds. Applicable across Linux, Windows, and MacOS platforms to maintain active connections and prevent premature timeouts.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# 设置保活时间为120秒  \ns = ddb.session(keepAliveTime=120)\n```\n\n----------------------------------------\n\nTITLE: Configuring High Availability for DolphinDB Connection in Python\nDESCRIPTION: Shows how to enable high availability (HA) mode with multiple nodes by setting highAvailability=True and providing a list of node addresses. Explains load balancing behavior, node load calculation, and differences between single-threaded and multi-threaded session creation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.2_Connect.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\n# List of all available nodes\nsites = [\"192.168.1.2:24120\", \"192.168.1.3:24120\", \"192.168.1.4:24120\"]\n# Connect with high availability enabled\ns.connect(host=\"192.168.1.2\", port=24120, userid=\"admin\", password=\"123456\", highAvailability=True, highAvailabilitySites=sites)\n```\n\n----------------------------------------\n\nTITLE: Checking Status of MultithreadedTableWriter in Python\nDESCRIPTION: The getStatus method returns an object containing detailed information about the writer's current state, including error information, row counts, and thread status. It includes methods hasError() and succeed() for checking operation results.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_82\n\nLANGUAGE: Python\nCODE:\n```\ngetStatus()\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB DFS Table into Memory\nDESCRIPTION: Uses the Python API's `session.loadTable()` method to load a DolphinDB DFS partitioned table named 't1' located in the database 'dfs://testdb' into the server's memory. The function returns a Python object (`pt1`) referencing the in-memory table. Assumes 's' is a connected DolphinDB session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npt1=s.loadTable(tableName='t1',dbPath=\"dfs://testdb\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Dict Data Using DolphinDB Python API in Python\nDESCRIPTION: Shows how DolphinDB Dict data is mapped to Python dict objects. Keys in the Dict are converted as Scalars, and values are recursively converted according to their data types, including nested lists or Vectors represented as numpy arrays. This example involves a DolphinDB session object `s` running a script returning a Dict containing a scalar and a Vector. The expected output is a Python dict with a float numpy array and an integer value.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> re = s.run('''{\"a\": 123, \"b\": [1.1, 2.2]}''')\n>>> re\n{'b': array([1.1, 2.2]), 'a': 123}\n>>> type(re)\n<class 'dict'>\n```\n\n----------------------------------------\n\nTITLE: Selecting Top N Records per Group using CONTEXT BY in Python\nDESCRIPTION: Loads the 'trade' table. It uses `contextby('ticker')` to process data within each ticker group independently. The `top(3)` method then selects the first 3 records from each group. The result, containing the top 3 rows for each ticker, is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_122\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\").contextby('ticker').top(3).toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Using Startup Script during DolphinDB Session Connection\nDESCRIPTION: Allows executing pre-connection scripts such as cache clearing, plugin loading, or schema setup via the startup parameter. Useful for preparing the environment before using the session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.2_Connect.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\n# Connect with startup script\ns.connect(host=\"localhost\", port=8848, startup=\"clearAllCache();\")\n```\n\n----------------------------------------\n\nTITLE: Handling Write Errors and Recovery in DolphinDB MultithreadedTableWriter (Python)\nDESCRIPTION: This snippet checks for errors after attempting to write data using MultithreadedTableWriter (MTW). If a write error is detected, it retrieves the unwritten (failed) rows, reprocesses them as necessary, creates a new MTW instance (since the original's worker threads have exited), and attempts to reinsert the fixed data. It demonstrates proper use of getUnwrittenData, insertUnwrittenData, and error/status reporting. Dependencies: dolphindb Python API, valid MTW parameters, and exception handling blocks. Inputs: None required directly, as context is assumed. Outputs: Status and recovery information prints. Constraints: Must reconstruct MTW after write completion; unwritten data requires manual correction before reinsertion.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_89\n\nLANGUAGE: Python\nCODE:\n```\nif writeStatus.hasError():\n    print(\"Error in writing:\")\n    unwrittendata = writer.getUnwrittenData()\n    print(\"Unwrittendata: %d\" % len(unwrittendata))\n    # 重新获取新的 MTW 对象\n    newwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\n    try:\n        # 修正失败数据后将失败数据重新写入 MTW\n        for row in unwrittendata:\n            row[1]=\"aaaaa\"\n        res = newwriter.insertUnwrittenData(unwrittendata)\n        if res.succeed():\n            # 使用 waitForThreadCompletion() 方法等待数据写入完成，并终止所有 MTW 的工作线程\n            newwriter.waitForThreadCompletion()\n            writeStatus=newwriter.getStatus()\n            print(\"Write again:\\n\", writeStatus)\n        else:\n            # 数据写入失败\n            print(\"Failed to write data again: \\n\",res) \n    except Exception as ex:\n        # MTW 抛出异常\n        print(\"MTW exit with exception %s\" % ex)\n    finally:\n        # 确保 newwriter工作线程结束运行\n        newwriter.waitForThreadCompletion()\nelse:\n    print(\"Write successfully:\\n\", writeStatus)\n\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Table for MultithreadedTableWriter Example in Python\nDESCRIPTION: This example shows how to create a DolphinDB distributed database and table before using MultithreadedTableWriter. It creates a hash-partitioned database with 10 partitions.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_85\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport time\nimport random\nimport threading\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"\n    dbName = 'dfs://valuedb3';\n    if(exists(dbName)){\n        dropDatabase(dbName);\n    }\n    datetest=table(1000:0,`date`symbol`id,[DATE,SYMBOL,LONG]);\n    db = database(directory=dbName, partitionType=HASH, partitionScheme=[INT, 10]);\n    pt=db.createPartitionedTable(datetest,'pdatetest','id');\n\"\"\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Calculating Grouped Aggregates (`sum`) using `groupby()` (Python)\nDESCRIPTION: Loads the 'trade' table from 'dfs://valuedb'. Selects the 'vol' and 'prc' columns, groups the results by 'ticker' using `groupby(['ticker'])`, calculates the sum of 'vol' and 'prc' for each group using `sum()`, and displays the aggregated results as a Pandas DataFrame. Requires `s`, `dolphindb`, and the 'trade' table created in the setup step.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\nprint(trade.select(['vol','prc']).groupby(['ticker']).sum().toDF())\n```\n\n----------------------------------------\n\nTITLE: Activating asynchronous communication mode in DolphinDB Python API\nDESCRIPTION: This example demonstrates enabling asynchronous communication with the server by setting 'enableASYNC' to True, which is suitable for high-performance scenarios like batch data writing. In async mode, 'session.run' executes commands without returning results.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ns=ddb.session(enableASYNC=True)\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated SQL for SELECT Statement in Python\nDESCRIPTION: Demonstrates how to view the underlying DolphinDB SQL query generated by the Python API calls. After defining the selection criteria using `select()`, the `showSQL()` method is called to print the corresponding SQL string.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_109\n\nLANGUAGE: python\nCODE:\n```\nprint(trade.select(['ticker','date','bid','ask','prc','vol']).showSQL())\n```\n\n----------------------------------------\n\nTITLE: Appending to a Stream Table Asynchronously\nDESCRIPTION: Shows how to append data to a stream table in asynchronous mode, which provides better performance for high-frequency data writes by eliminating return value detection.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport random\nimport datetime\n\ns = ddb.session(enableASYNC=True)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nn = 100\n\nscript = \"\"\"trades = streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,DOUBLE,INT])\"\"\"\ns.run(script) # The script above can be executed on the server\n\n# Randomly generate a DataFrame\nsym_list = ['IBN', 'GTYU', 'FHU', 'DGT', 'FHU', 'YUG', 'EE', 'ZD', 'FYU']\nprice_list = []\ntime_list = []\nfor i in range(n):\n    price_list.append(round(np.random.uniform(1, 100), 1))\n    time_list.append(np.datetime64(datetime.date(2020, random.randint(1, 12), random.randint(1, 20))))\n\ntb = pd.DataFrame({'time': time_list,\n                   'sym': np.random.choice(sym_list, n),\n                   'price': price_list,\n                   'id': np.random.choice([1, 2, 3, 4, 5], n)})\n\ns.run(\"append!{trades}\", tb)\n```\n\n----------------------------------------\n\nTITLE: Downloading Table Data as Pandas DataFrame Using DolphinDB Python API in Python\nDESCRIPTION: Describes the standard procedure to download DolphinDB Table data into pandas DataFrame via the Python API. Each Table column is converted as a Vector. The snippet emphasizes that time-related columns are converted to pandas-supported datetime64[ns] type for consistency. The API currently supports downloading only Array Vector columns and not Any Vector columns. This section provides conceptual context and mentions limitations related to time type handling in pandas.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_8\n\n\n\n----------------------------------------\n\nTITLE: Uploading DataFrame without Type Conversion - Python\nDESCRIPTION: This snippet demonstrates uploading a pandas DataFrame to DolphinDB without explicit type conversion.  It uses the `upload` method of the DolphinDB session object. The output shows the default type mapping by DolphinDB, where `cint` remains LONG and strings are mapped to STRING.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.4_ForceTypeCasting.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\ndf = pd.DataFrame({\n    'cint': [1, 2, 3],\n    'csymbol': [\"aaa\", \"bbb\", \"aaa\"],\n    'cblob': [\"a1\", \"a2\", \"a3\"],\n})\n\ns.upload({\"df_wrong\": df})\nprint(s.run(\"schema(df_wrong)\")['colDefs'])\n```\n\n----------------------------------------\n\nTITLE: Using tableUpsert with Keyed Memory Table in Python\nDESCRIPTION: Demonstrates how to use the `tableUpsert` object to append data from a Pandas DataFrame to a shared keyed in-memory table in DolphinDB. The script connects to DolphinDB, creates a shared keyed table using `s.run`, initializes `tableUpsert`, prepares a Pandas DataFrame with sample data (including datetime conversion), and performs the upsert operation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport dolphindb.settings as keys\n\nimport threading\n\nHOST = \"192.168.1.193\"\nPORT = 8848\n\ns = ddb.session()\ns.connect(HOST, PORT, \"admin\", \"123456\")\nscript_DFS_HASH = \"\"\"\n    testtable=keyedTable(`id,1000:0,`date`text`id,[DATETIME,STRING,LONG])\n    share testtable as ttable\n    \"\"\"\ns.run(script_DFS_HASH)\n\nupsert=ddb.tableUpsert(\"\",\"ttable\",s)\ndates=[]\ntexts=[]\nids=[]\nprint(np.datetime64('now'))\nfor i in range(1000):\n    dates.append(np.datetime64('2012-06-13 13:30:10.008'))\n    texts.append(str(time.time()))\n    ids.append(i%20)\ndf = pd.DataFrame({'date': dates,'text': texts,'id': np.array(ids,np.int64)})\nupsert.upsert(df)\nprint(s.run(\"ttable\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing MultithreadedTableWriter with Compression for Distributed Table in Python\nDESCRIPTION: Creating a MultithreadedTableWriter instance for a distributed table with 5 threads, a batch size of 10000, and different compression methods for each column (LZ4 for the first two columns and DELTA for the third).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_86\n\nLANGUAGE: Python\nCODE:\n```\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\n```\n\n----------------------------------------\n\nTITLE: Appending Single Row with insert into Statement (with Date Conversion) - DolphinDB Python API\nDESCRIPTION: Demonstrates inserting a single row into a DolphinDB memory table from Python, manually converting numpy datetime64 to int64 and then to a DATE type using the date function on the server. Requires numpy and an established session. Ensures inserted values agree with the destination DolphinDB data types.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nscript = \"insert into tglobal values(%s, date(%s), %s, %s)\" % (1, np.datetime64(\"2019-01-01\").astype(np.int64), '`AAPL', 5.6)\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Creating RANGE Partitioned Database (Int) in DolphinDB via Python\nDESCRIPTION: Creates a DolphinDB DFS database ('dfs://db_range_int') with RANGE partitioning based on integer values ([1, 11, 21]). It checks/drops the existing database, defines the integer range partitions, creates the database, prepares data, uploads it with an alias 't', creates the partitioned table 'pt' based on the 'id' column, appends data, and loads the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndbPath=\"dfs://db_range_int\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(dbName='mydb', partitionType=keys.RANGE, partitions=[1, 11, 21], dbPath=dbPath)\ndf = pd.DataFrame({'id': np.arange(1, 21), 'val': np.repeat(1, 20)})\nt = s.table(data=df, tableAliasName='t')\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns='id').append(t)\nre = s.loadTable(tableName='pt', dbPath=dbPath).toDF()\n```\n\n----------------------------------------\n\nTITLE: Creating a Database and Loading Data\nDESCRIPTION: This code snippet demonstrates the creation of a value-partitioned database named 'mydb' within the 'dfs://valuedb' directory. It uses the `s.database` function with specified partitions and then loads data into a table named 'trade' using the `loadTextEx` function, from the specified file path.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> s.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\", \"NVDA\"], dbPath=\"dfs://valuedb\")\n>>> trade = s.loadTextEx(dbPath=\"mydb\", tableName='trade',partitionColumns=[\"TICKER\"], remoteFilePath=WORK_DIR + \"/example.csv\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB Session and Importing Libraries for Streaming API Example\nDESCRIPTION: This snippet initializes a DolphinDB Python client session and imports the DolphinDB package along with numpy, preparing for subsequent streaming data subscription and processing examples.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_143\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\n```\n\n----------------------------------------\n\nTITLE: Filtering Data Using String Conditions in WHERE Clause (Python)\nDESCRIPTION: Loads data from a CSV file. It selects columns specified as a single comma-separated string in `select()` and applies multiple filtering conditions, also provided as a single comma-separated string, within the `where()` method. The result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_118\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\nprint(trade.select(\"ticker, date, vol\").where(\"bid!=NULL, ask!=NULL, vol>50000000\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Parallel Importing Data to DolphinDB In-Memory Table using `ploadText` in Python\nDESCRIPTION: Uses `s.ploadText()` to perform a parallel load of the `example.csv` file into a DolphinDB in-memory partitioned table. This function is generally faster than `loadText` for large files due to parallel processing. It prints the number of rows loaded into the table. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.ploadText(WORK_DIR+\"/example.csv\")\nprint(trade.rows)\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated SQL for Chained WHERE Query in Python\nDESCRIPTION: Demonstrates viewing the underlying SQL query generated by chaining `select`, `where`, and `sort` methods in the Python API. The `showSQL()` method is called on the query object before execution to display the complete SQL statement.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_117\n\nLANGUAGE: python\nCODE:\n```\nprint(trade.select(['date','bid','ask','prc','vol']).where('TICKER=`AMZN').where('bid!=NULL').where('ask!=NULL').where('vol>10000000').sort('vol desc').showSQL())\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB 'typestr' function with NumPy datetime64\nDESCRIPTION: This snippet demonstrates how to pass NumPy datetime64 objects as arguments to the DolphinDB 'typestr' function, which returns the data type as a string.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\ns.run(\"typestr\",np.datetime64('2019-01-01'))\n# output\n'DATE'\n\ns.run(\"typestr\",np.datetime64('2019-01'))\n# output\n'MONTH'\n\ns.run(\"typestr\",np.datetime64('2019-01-01T20:01:01'))\n# output\n'DATETIME'\n\ns.run(\"typestr\",np.datetime64('2019-01-01T20:01:01.122'))\n# output\n'TIMESTAMP'\n\ns.run(\"typestr\",np.datetime64('2019-01-01T20:01:01.1223461'))\n# output\n'NANOTIMESTAMP'\n```\n\n----------------------------------------\n\nTITLE: Generating and Appending Data to Partitioned Table (Python)\nDESCRIPTION: This code generates synthetic data using the `numpy` and `pandas` libraries and then appends it to a partitioned DolphinDB table using the `PartitionedTableAppender`. The code generates random dates and other necessary data to create a pandas DataFrame, and uses the `append` method of the appender to write the generated data to the table. Dependencies include `dolphindb`, `pandas`, `numpy` and `random` modules. The output includes the number of rows written.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.3_PartitionedTableAppender.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nn = 100\n\ndates = []\nfor i in range(n):\n    dates.append(np.datetime64(\n        \"201{:d}-0{:1d}-{:2d}\".format(random.randint(0, 9), random.randint(1, 9), random.randint(10, 28))))\n\ndata = pd.DataFrame({\n    \"id\": np.random.choice(['AMZN', 'IBM', 'APPL'], n), \n    \"time\": dates,\n    \"vol\": np.random.randint(100, size=n)\n})\nre = appender.append(data)\n```\n\n----------------------------------------\n\nTITLE: Downloading Array Vector and Nested Any Vector Data via DolphinDB Python API - Python\nDESCRIPTION: Illustrates the behavior of downloading DolphinDB Array Vector and nested Any Vector data into Python using the DolphinDB API. Array Vectors are returned as object dtype numpy ndarrays where each element is converted individually, while Any Vectors are converted to Python lists potentially containing nested lists if nested. The example includes multi-element Array Vector with some nulls and nested tuples involving mixed types. Inputs are DolphinDB expressions generating Array and Any Vectors, outputs show Python lists or numpy arrays with appropriate dtypes. Dependencies include a DolphinDB session 's'.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> s.run(\"arrayVector(2 3 4, [1, 2, 3, NULL])\")\n[array([1, 2], dtype=int32) array([3], dtype=int32) array([nan])]\n\n>>> re = s.run('''(1, 2, [12, \"aaa\"])\n''')\n>>> re\n[1, 2, [12, 'aaa']]\n>>> type(re)\n<class 'list'>\n>>> type(re[2])\n<class 'list'>\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB 'add' function with NumPy arrays\nDESCRIPTION: This snippet demonstrates how to execute the DolphinDB built-in function 'add' with NumPy arrays as input. It shows how to define NumPy arrays, pass them to the 'run' method, and retrieve the result. The result will be a NumPy array of float64.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nx=np.array([1.5,2.5,7])\ny=np.array([8.5,7.5,3])\nresult=s.run(\"add\", x, y)\nrepr(result)\n# output\n'array([10., 10., 10.])'\n\nresult.dtype\n# output\ndtype('float64')\n```\n\n----------------------------------------\n\nTITLE: Defining the tableUpsert Interface\nDESCRIPTION: Defines the interface for the `tableUpsert` object in the DolphinDB Python API. This object is used to append or update data in indexed memory tables, keyed memory tables, and distributed tables. Key parameters include database path, table name, session object, null handling flag, key column names (for DFS tables), and sort columns.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_66\n\nLANGUAGE: plaintext\nCODE:\n```\ntableUpsert(dbPath=None, tableName=None, ddbSession=None, ignoreNull=False, keyColNames=[], sortColumns=[])\n```\n\n----------------------------------------\n\nTITLE: Retrieving a Single Column as a Vector using EXEC in Python\nDESCRIPTION: Loads a DolphinDB table from a specified database path using `s.loadTextEx`. It then uses the `exec()` method with a single column name ('ticker') to retrieve that column as a DolphinDB vector. The `toDF()` method converts this vector into a NumPy array for printing in Python.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_111\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\nprint(trade.exec('ticker').toDF())\n```\n\n----------------------------------------\n\nTITLE: Enabling SSL Encryption for DolphinDB Session in Python\nDESCRIPTION: Shows how to enable SSL encrypted communication by setting the enableSSL parameter to True during session initialization. Ensures that the API communicates over a secure SSL channel, provided that the DolphinDB server has HTTPS enabled. Requires DolphinDB version 1.10.17 or later and server-side configuration of enableHTTPS.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# 开启加密通讯\ns = ddb.session(enableSSL=True)\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB TSDB Table with Array Vector Column in Python\nDESCRIPTION: Connects to DolphinDB, sets up a TSDB database, creates a pandas DataFrame where one column ('val') contains NumPy arrays (representing array vectors), converts it to a DolphinDB table, creates a partitioned table `pt` with `sortColumns`, appends data, and retrieves it. Requires `dolphindb`, `numpy`, and `pandas` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\nimport numpy as np\nimport pandas as pd\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ndates = np.array(pd.date_range(start='20120101', end='20120110'), dtype=\"datetime64[D]\")\nvalues = np.array([np.array([11,12,13,14],dtype=np.int64),\n    np.array([15,16,17,18],dtype=np.int64),\n    np.array([19,10,11,12],dtype=np.int64),\n    np.array([13,14,15],dtype=np.int64),\n    np.array([11,14,17,12,15],dtype=np.int64),\n],dtype=object)\n\ndbPath = \"dfs://tsdb\"\nif s.existsDatabase(dbPath): s.dropDatabase(dbPath)\ndb = s.database(dbName='mydb_tsdb', partitionType=keys.VALUE, partitions=dates, dbPath=dbPath, engine=\"TSDB\")\n\ndf = pd.DataFrame({'datetime': np.array(\n    ['2012-01-01T00:00:00', '2012-01-02T00:00:00', '2012-01-04T00:00:00', '2012-01-05T00:00:00', '2012-01-08T00:00:00'],\n    dtype='datetime64'),\n    'sym': ['AA', 'BB', 'BB', 'AA', 'BB'], 'val': values})\nt = s.table(data=df)\n\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns='datetime', sortColumns=[\"sym\", \"datetime\"]).append(t)\nre = s.loadTable(tableName='pt', dbPath=dbPath).toDF()\nprint(re)\n```\n\n----------------------------------------\n\nTITLE: Querying and Printing Table Data (Python)\nDESCRIPTION: This code snippet demonstrates how to query and print data from the partitioned table after appending. It first uses the DolphinDB `loadTable` function to retrieve the data from the specified distributed database and table. Then, the `select * from pt` query is run and the retrieved data is printed. It requires an existing connection to the DolphinDB server and the ability to execute SQL queries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.3_PartitionedTableAppender.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(re)\nprint(s.run(\"pt = loadTable('dfs://valuedb', 'pt'); select * from pt;\"))\n```\n\n----------------------------------------\n\nTITLE: Retrieving Multiple Columns as a DataFrame using EXEC in Python\nDESCRIPTION: Loads a DolphinDB table from a database path using `s.loadTextEx`. The `exec()` method is used with a list of column names to retrieve multiple columns. Unlike the single-column case, this returns a DolphinDB table, which is then converted to a Pandas DataFrame using `toDF()`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_112\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\nprint(trade.exec(['ticker','date','bid','ask','prc','vol']).toDF())\n```\n\n----------------------------------------\n\nTITLE: Selecting Top N Records from a DolphinDB Table in Python\nDESCRIPTION: Loads data from a CSV file into a table object using `s.loadText`. The `top(5)` method is then called on this object to select the first 5 records. The result is converted to a Pandas DataFrame for display.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_113\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\ntrade.top(5).toDF()\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB DFS Database and Partitioned Table\nDESCRIPTION: DolphinDB script to create a DFS database named 'testdb' with RANGE partitioning on integers [1, 5, 11]. It then creates an in-memory table 't1' and uses it as a template to create a partitioned table also named 't1' within the 'testdb' database, partitioned by the 'id' column. Finally, it appends the data from the in-memory table to the partitioned table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_27\n\nLANGUAGE: dolphindb script\nCODE:\n```\ndb = database(\"dfs://testdb\",RANGE, [1, 5 ,11])\nt1=table(1..10 as id, 1..10 as v)\ndb.createPartitionedTable(t1,`t1,`id).append!(t1)\n```\n\n----------------------------------------\n\nTITLE: Appending to a DFS Table using tableAppender\nDESCRIPTION: Demonstrates using the tableAppender method to append pandas DataFrame data to a DFS table, particularly useful for batch operations with various temporal data types.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\nimport numpy as np\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript='''\ndbPath = \"dfs://tableAppender\"\nif(existsDatabase(dbPath))\n    dropDatabase(dbPath)\nt = table(1000:0, `sym`date`month`time`minute`second`datetime`timestamp`nanotimestamp`qty, [SYMBOL, DATE,MONTH,TIME,MINUTE,SECOND,DATETIME,TIMESTAMP,NANOTIMESTAMP, INT])\ndb=database(dbPath,RANGE,100000 200000 300000 400000 600001)\npt = db.createPartitionedTable(t, `pt, `qty)\n'''\ns.run(script)\nappender = ddb.tableAppender(\"dfs://tableAppender\",\"pt\", s)\nsym = list(map(str, np.arange(100000, 600000)))\ndate = np.array(np.tile(['2012-01-01', 'NaT', '1965-07-25', 'NaT', '2020-12-23', '1970-01-01', 'NaT', 'NaT', 'NaT', '2009-08-05'],50000), dtype=\"datetime64[D]\")\nmonth = np.array(np.tile(['1965-08', 'NaT','2012-02', '2012-03', 'NaT'],100000), dtype=\"datetime64\")\ntime = np.array(np.tile(['2012-01-01T00:00:00.000', '2015-08-26T05:12:48.426', 'NaT', 'NaT', '2015-06-09T23:59:59.999'],100000), dtype=\"datetime64\")\nsecond = np.array(np.tile(['2012-01-01T00:00:00', '2015-08-26T05:12:48', 'NaT', 'NaT', '2015-06-09T23:59:59'],100000), dtype=\"datetime64\")\nnanotime = np.array(np.tile(['2012-01-01T00:00:00.000000000', '2015-08-26T05:12:48.008007006', 'NaT', 'NaT', '2015-06-09T23:59:59.999008007'],100000), dtype=\"datetime64\")\nqty = np.arange(100000, 600000)\ndata = pd.DataFrame({'sym': sym, 'date': date, 'month':month, 'time':time, 'minute':time, 'second':second, 'datetime':second, 'timestamp':time, 'nanotimestamp':nanotime, 'qty': qty})\nnum = appender.append(data)\nprint(num)\nprint(s.run(\"select * from pt\"))\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Contextby Example 1\nDESCRIPTION: This snippet demonstrates the usage of the `contextby` function in DolphinDB to group data by the 'ticker' column and then apply the `top(3)` function to each group, returning the top 3 rows within each ticker group. The result is converted to a Pandas DataFrame for display. The data is loaded from a DolphinDB table named 'trade' located at 'dfs://valuedb'.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\").contextby('ticker').top(3).toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Querying Table Information\nDESCRIPTION: These snippets demonstrate how to interact with the `trade` table after the data has been loaded, showing how to use the `toDF`, `rows`, `cols`, and `schema` methods to display data, the number of rows, the number of columns and the table schema.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> trade.toDF()\n      TICKER       date       VOL      PRC      BID      ASK\n0       AMZN 1997-05-15   6029815   23.500   23.500   23.625\n1       AMZN 1997-05-16   1232226   20.750   20.500   21.000\n2       AMZN 1997-05-19    512070   20.500   20.500   20.625\n3       AMZN 1997-05-20    456357   19.625   19.625   19.750\n4       AMZN 1997-05-21   1577414   17.125   17.125   17.250\n...      ...        ...       ...      ...      ...      ...\n13131   NVDA 2016-12-23  16193331  109.780  109.770  109.790\n13132   NVDA 2016-12-27  29857132  117.320  117.310  117.320\n13133   NVDA 2016-12-28  57384116  109.250  109.250  109.290\n13134   NVDA 2016-12-29  54384676  111.430  111.260  111.420\n13135   NVDA 2016-12-30  30323259  106.740  106.730  106.750\n\n[13136 rows x 6 columns]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> trade.rows\n13136\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> trade.cols\n6\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> trade.schema\n     name typeString  typeInt comment\n0  TICKER     SYMBOL       17        \n1    date       DATE        6        \n2     VOL        INT        4        \n3     PRC     DOUBLE       16        \n4     BID     DOUBLE       16        \n5     ASK     DOUBLE       16\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data into DolphinDB Memory Table using `loadText` in Python\nDESCRIPTION: Specifies a working directory, uses `s.loadText()` to load data from `example.csv` into a DolphinDB in-memory table object in Python, and then converts this object to a pandas DataFrame using `toDF()`. This method assumes the data fits entirely within the available memory. Requires `dolphindb` and `pandas` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nWORK_DIR = \"C:/DolphinDB/Data\"\n\n# 返回一个 Python 中的 DolphinDB 表对象\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\n\n# 将返回的 DolphinDB 表对象转化为 pandas DataFrame。表的数据传输发生在此步骤。\ndf = trade.toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Contextby with Csort Example\nDESCRIPTION: This snippet illustrates how to use `contextby` along with `csort` to sort data within each group. The 'trade' table is loaded, grouped by 'ticker', and then sorted within each ticker group by 'date' in descending order. The final result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_78\n\nLANGUAGE: python\nCODE:\n```\ndf = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\").contextby('ticker').csort('date desc').toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Sorting within Groups using csort - DolphinDB Python\nDESCRIPTION: Demonstrates group-wise sorting with 'csort' after grouping by 'ticker' using 'contextby', ordering rows in each group by 'date' in descending order. Requires DolphinDB Python API. The result is a pandas DataFrame where, within each ticker group, records are sorted by date descending. Key parameter: 'csort' column list and order.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_126\n\nLANGUAGE: Python\nCODE:\n```\ndf = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\").contextby('ticker').csort('date desc').toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Custom Asynchronous Task Runner with Thread and Event Loop in Python\nDESCRIPTION: Creates a custom class that manages script execution through the DBConnectionPool. A separate thread runs the event loop continuously to prevent blocking the main thread while tasks execute asynchronously.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.2_AsyncMethodsAndOthers.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport time\nimport asyncio\nimport threading\n\n# 在该例子中主线程负责创建协程对象传入自定义脚本并调用自定义的对象去运行，并新起子线程运行事件循环防止阻塞主线程。\nclass DolphinDBHelper(object):\n    pool = ddb.DBConnectionPool(\"localhost\", 8848, 10)\n    @classmethod\n    async def test_run(cls,script):\n        print(f\"run script: [{script}]\")\n        return await cls.pool.run(script)\n\n    @classmethod\n    async def runTest(cls,script):\n        start = time.time()\n        task = loop.create_task(cls.test_run(script))\n        result = await asyncio.gather(task)\n        print(f\"\"\"[{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}] time: {time.time()-start} result: {result}\"\"\")\n        return result\n\n#定义一个跑事件循环的线程函数\ndef start_thread_loop(loop):\n    asyncio.set_event_loop(loop)\n    loop.run_forever()\n\nif __name__==\"__main__\":\n    start = time.time()\n    print(\"In main thread\",threading.current_thread())\n    loop = asyncio.get_event_loop()\n    # 在子线程中运行事件循环, 让它 run_forever\n    t = threading.Thread(target= start_thread_loop, args=(loop,))\n    t.start()\n    task1 = asyncio.run_coroutine_threadsafe(DolphinDBHelper.runTest(\"sleep(1000);1+1\"),loop)\n    task2 = asyncio.run_coroutine_threadsafe(DolphinDBHelper.runTest(\"sleep(3000);1+2\"),loop)\n    task3 = asyncio.run_coroutine_threadsafe(DolphinDBHelper.runTest(\"sleep(5000);1+3\"),loop)\n    task4 = asyncio.run_coroutine_threadsafe(DolphinDBHelper.runTest(\"sleep(1000);1+4\"),loop)\n\n    end = time.time()\n    print(\"main thread time: \", end - start)\n```\n\n----------------------------------------\n\nTITLE: Async Append using Server-Side Function View (Python)\nDESCRIPTION: Demonstrates asynchronous data appending using a server-side function view (`appendStreamingData`) defined previously. The Python script connects asynchronously, uploads the Pandas DataFrame `tb` to the server using `s.upload`, and then calls the server-side function `appendStreamingData` via `s.run` to perform the append operation, including type conversion handled by the function view.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport random\nimport datetime\n\ns = ddb.session(enableASYNC=True)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nn = 100\n\n# 随机生成一个 dataframe\nsym_list = ['IBN', 'GTYU', 'FHU', 'DGT', 'FHU', 'YUG', 'EE', 'ZD', 'FYU']\nprice_list = []\ntime_list = []\nfor i in range(n):\n    price_list.append(round(np.random.uniform(1, 100), 1))\n    time_list.append(np.datetime64(datetime.date(2020, random.randint(1, 12), random.randint(1, 20))))\n\ntb = pd.DataFrame({'time': time_list,\n                   'sym': np.random.choice(sym_list, n),\n                   'price': price_list,\n                   'id': np.random.choice([1, 2, 3, 4, 5], n)})\n\ns.upload({'tb': tb})\ns.run(\"appendStreamingData(tb)\")\n```\n\n----------------------------------------\n\nTITLE: Undefining Variables with undef and undefAll\nDESCRIPTION: The `undef` method releases a specified object in the session. The `undefAll` method releases all objects in the session. The `undef` method supports different object types such as variables, shared variables, and function definitions.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> s.undef(\"t1\", \"VAR\")\n>>> s.undefAll()\n```\n\n----------------------------------------\n\nTITLE: Multi-Column Group Sort using csort - DolphinDB Python\nDESCRIPTION: Illustrates sorting grouped data by multiple columns ('TICKER', 'VOL') within groups, with customizable sort order. The first snippet sorts both columns ascending; the second sorts 'TICKER' ascending and 'VOL' descending. Returns the first five records. Requires DolphinDB table and API.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_128\n\nLANGUAGE: Python\nCODE:\n```\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").contextby('ticker').csort([\"TICKER\", \"VOL\"], True).limit(5)\n```\n\nLANGUAGE: Python\nCODE:\n```\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").contextby('ticker').csort([\"TICKER\", \"VOL\"], [True, False]).limit(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Database Handle with session.database - DolphinDB Python\nDESCRIPTION: Demonstrates the syntax and parameters for creating a DolphinDB database handle using the session.database method. Required dependencies include the DolphinDB Python API and its settings module. Key parameters such as dbName, partitionType, partitions, dbPath, engine, atomic, and chunkGranularity provide flexibility in configuring persistent or in-memory databases with various partitioning and storage engines. Outputs a Database object which serves as a handle for further operations.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsession.database(dbName=None, prititionType=None, parititions=None, dbPath=None, engine=None, atomic=None, chunkGranularity=None)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Database with database Method\nDESCRIPTION: The `database` method is used to create a database. It requires parameters such as `dbName`, `partitionType`, `partitions`, and `dbPath`. This example demonstrates creating a value-partitioned database. The code first checks if a database exists and drops it if needed, then proceeds to create the new database.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndatabase(dbName=None, partitionType=None, partitions=None, dbPath=None, engine=None, atomic=None, chunkGranularity=None)\n```\n\n----------------------------------------\n\nTITLE: Example of PROTOCOL_PICKLE Deserialization for DolphinDB Matrix to Python np.ndarray - Python\nDESCRIPTION: This snippet illustrates the output of a DolphinDB query returning a Matrix data form when deserialized using the PROTOCOL_PICKLE protocol. The Matrix data form corresponds to a list containing three elements: the first is a NumPy ndarray with the matrix data (here with datetime64[ns] dtype), and the next two elements are the row and column names respectively, which are None if not set. The example shows a Matrix representing dates transformed into a Python representation suitable for further analysis. Key parameters are the DolphinDB run method used on an active session, returning a Python list. Expected output is a list with the main array and optionally the row/column labels.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.2_PROTOCOL_PICKLE.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> s.run(\"date([2012.01.02, 2012.02.03])$1:2\")\n[array([['2012-01-02T00:00:00.000000000', '2012-02-03T00:00:00.000000000']],\n      dtype='datetime64[ns]'), None, None]\n```\n\n----------------------------------------\n\nTITLE: Creating a partitioned database in DolphinDB using Python\nDESCRIPTION: This snippet demonstrates how to create a partitioned database in DolphinDB via Python API, specifying database name, partition type, and partitions. It sets up prerequisites and illustrates database creation with partition configuration.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_97\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb.settings as keys\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\", \"NVDA\"], dbPath=\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Converting datetime64 to NANOTIME in DolphinDB\nDESCRIPTION: This snippet shows how to convert a datetime64 object to a NANOTIME object in DolphinDB by uploading the datetime64 object to DolphinDB server, and then calling the nanotime function.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nts = np.datetime64('2019-01-01T20:01:01.1223461')\ns.upload({'ts':ts})\ns.run('a=nanotime(ts)')\n\ns.run('typestr(a)')\n# output\n'NANOTIME'\n\ns.run('a')\n# output\nnp.datetime64('1970-01-01T20:01:01.122346100')\n```\n\n----------------------------------------\n\nTITLE: Filtering Grouped Data using HAVING in Python\nDESCRIPTION: Loads the 'trade' table. It groups the data by 'vol' using `groupby(['vol'])`, calculates the count of 'ask' within each group using `select('count(ask)')`, and then filters these groups using `having('count(ask)>1')` to keep only those groups where the count is greater than 1. The result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_121\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\nprint(trade.select('count(ask)').groupby(['vol']).having('count(ask)>1').toDF())\n```\n\n----------------------------------------\n\nTITLE: Using Pickle Protocol and pickleTableToList Parameter - DolphinDB Python API (Python)\nDESCRIPTION: Shows how to set up the session to use the Pickle protocol, connect to DolphinDB, and demonstrates the impact of the pickleTableToList parameter in session.run. By toggling this flag, users can control table and matrix conversion behaviors (e.g., to ndarrays or lists). Requires the dolphindb and numpy packages. Output types depend on protocol settings and whether pickleTableToList is enabled.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n# pickleTableToList = False (default)\nre1 = s.run(\"m=matrix(1 2, 3 4, 5 6);m.rename!(1 2, `a`b`x);m\")\nre2 = s.run(\"table(1..3 as a)\")\nprint(re1)\nprint(re2)\n-----------------------------\n[array([[1, 3, 5],\n       [2, 4, 6]], dtype=int32), \n array([1, 2], dtype=int32), \n array(['a', 'b', 'x'], dtype=object)]\n   a\n0  1\n1  2\n2  3\n\n# pickleTableToList = True\nre1 = s.run(\"m=matrix(1 2, 3 4, 5 6);m.rename!(1 2, `a`b`x);m\", pickleTableToList=True)\nre2 = s.run(\"table(1..3 as a)\", pickleTableToList=True)\nprint(re1)\nprint(re2)\n-----------------------------\n[array([[1, 3, 5],\n       [2, 4, 6]], dtype=int32), \n array([1, 2], dtype=int32), \n array(['a', 'b', 'x'], dtype=object)]\n[array([1, 2, 3], dtype=int32)]\n```\n\n----------------------------------------\n\nTITLE: Setting up DolphinDB Database and Table for GroupBy Examples (Python)\nDESCRIPTION: Checks for and drops the 'dfs://valuedb' database if it exists. Creates a new VALUE-partitioned database 'mydb' at 'dfs://valuedb' with specified partitions. Loads data from `example.csv` into the partitioned table 'trade' within this database using `loadTextEx`. This prepares the environment for subsequent `groupby` operations. Requires `s`, `dolphindb`, `dolphindb.settings` (as `keys`), `WORK_DIR`, and `example.csv`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\",\"NVDA\"], dbPath=\"dfs://valuedb\")\ns.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\n```\n\n----------------------------------------\n\nTITLE: Deleting an existing DolphinDB database in Python\nDESCRIPTION: This snippet shows how to delete an existing DolphinDB database using Python API by checking its existence and removing it, ensuring safe cleanup before creating new databases. It requires session object and prior connection.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_98\n\nLANGUAGE: Python\nCODE:\n```\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Creating LIST Partitioned Database by Symbol - DolphinDB Python\nDESCRIPTION: Shows how to partition a database using predefined lists (by symbol strings) with the LIST partition type. The partitions use nested Python lists representing groups of symbol values. Assumes prior setup of DolphinDB session and settings.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndbPath=\"dfs://db_list_sym\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.LIST, partitions=[[\"IBM\", \"ORCL\", \"MSFT\"], [\"GOOG\", \"FB\"]],dbPath=dbPath)\n\n```\n\n----------------------------------------\n\nTITLE: Handling Exception When Loading a Deleted DolphinDB Table\nDESCRIPTION: Shows the Python code `s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")` which attempts to load the 'trade' table previously dropped. The comment indicates that this operation will result in an exception because the table file '/valuedb/trade.tbl' no longer exists. Requires an active DolphinDB session `s`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ns.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n\n# Exception:\n# getFileBlocksMeta on path '/valuedb/trade.tbl' failed, reason: path does not exist\n```\n\n----------------------------------------\n\nTITLE: Outer Join using merge (how='outer') - DolphinDB Python\nDESCRIPTION: Demonstrates an outer join between two in-memory tables, keeping all records from both tables and matching where keys align. Shows behavior when rows exist only in one table. The output includes columns from both tables, and missing values are represented as NaN. Requires DolphinDB in-memory tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_134\n\nLANGUAGE: Python\nCODE:\n```\nt1 = s.table(data={'TICKER': ['AMZN', 'AMZN', 'NFLX'], 'date': ['2015.12.29', '2015.12.30', '2015.12.31'], 'open': [674, 685, 942]})\nt2 = s.table(data={'TICKER': ['AMZN', 'NFLX', 'NFLX'], 'date': ['2015.12.29', '2015.12.30', '2015.12.31'], 'close': [690, 936, 951]})\nprint(t1.merge(t2, how=\"outer\", on=[\"TICKER\",\"date\"]).toDF())\n```\n\n----------------------------------------\n\nTITLE: Dropping a Partition with dropPartition\nDESCRIPTION: The `dropPartition` function deletes data from a specific partition within a database. It requires `dbPath`, `partitionPaths`, and `tableName`. If the table name is not specified, all partitions in the database matching the condition will be deleted.  The function's behavior depends on the database's partition granularity.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> s.dropPartition(dbPath=\"dfs://valuedb\", partitionPaths=\"AMZN\", tableName=\"trade\")\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table with createPartitionedTable (Advanced Option) - DolphinDB Python\nDESCRIPTION: Creates a partitioned table in a TSDB-engine database, specifying both partitionColumns and advanced options such as sortColumns and sortKeyMappingFunction. Demonstrates the use of keepDuplicates for duplicate control and mapping functions for efficient indexing. Requires preloaded schema, valid partitions (dates), and an active session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndbPath = \"dfs://createPartitionedTable\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndates = np.array(pd.date_range(start='20220101', end='20220105'), dtype=\"datetime64[D]\")\ndb = s.database(partitionType=keys.VALUE, partitions=dates, dbPath=dbPath, engine=\"TSDB\")\ns.run(\"schema_t = table(100:0, `SecurityID`TradeDate`TotalVolumeTrade`TotalValueTrade, [SYMBOL, DATE, INT, DOUBLE])\")\nschema_t = s.table(data=\"schema_t\")\npt = db.createPartitionedTable(schema_t, \"pt\", partitionColumns=\"TradeDate\", sortColumns=[\"SecurityID\", \"TradeDate\"], keepDuplicates=\"ALL\", sortKeyMappingFunction=[\"hashBucket{,5}\"])\nschema = s.run(f'schema(loadTable(\"{dbPath}\", \"pt\"))')\nprint(schema[\"colDefs\"])\n\n```\n\n----------------------------------------\n\nTITLE: Enabling SSL encryption for DolphinDB connection\nDESCRIPTION: This snippet shows how to initiate a secure SSL/TLS encrypted connection to DolphinDB server by setting 'enableSSL' to True during session creation. The server must also be configured with HTTPS enabled.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ns=ddb.session(enableSSL=True)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Contextby with Select and Cumsum Example\nDESCRIPTION: This snippet uses `contextby` to group data by 'TICKER' and 'month(date)' and calculates the cumulative sum of 'VOL' for each group. It loads the 'trade' table, selects the required columns, applies the cumulative sum function `cumsum`, groups by ticker and month, and converts the result to a Pandas DataFrame for output.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\").select(\"TICKER, month(date) as month, cumsum(VOL)\").contextby(\"TICKER,month(date)\").toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Waiting for MultithreadedTableWriter Threads to Complete in Python\nDESCRIPTION: Blocks execution until all pending write operations managed by the writer's threads are finished. It's crucial to call this method to ensure all data buffered has been sent and processed before the program exits or the writer object is destroyed. Calling insert methods after this will result in an error.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\nwaitForThreadCompletion()\n```\n\n----------------------------------------\n\nTITLE: Cancelling a streaming data subscription in DolphinDB Python API\nDESCRIPTION: Cancels an active subscription on the client side by specifying the host, port, table name, and optionally the action name using the unsubscribe method. This stops receiving streaming data for the specified subscription. Requires knowledge of the subscription details to properly identify it for cancellation. The snippet shows a typical call to unsubscribe an existing subscription.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.4_Subscription/2.4_Subscription.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ns.unsubscribe(\"192.168.1.113\", 8848, \"trades\", \"action\")\n```\n\n----------------------------------------\n\nTITLE: Creating LIST Partitioned Database (Symbol) in DolphinDB via Python\nDESCRIPTION: Creates a DolphinDB DFS database ('dfs://db_list_sym') with LIST partitioning based on SYMBOL (string) values. It checks/drops the existing database, defines partitions as lists of strings ([['IBM', 'ORCL', 'MSFT'], ['GOOG', 'FB']]), creates the database, prepares data, uploads it, creates the partitioned table 'pt' based on the 'sym' column, appends data, and loads the table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndbPath=\"dfs://db_list_sym\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(dbName='mydb', partitionType=keys.LIST, partitions=[['IBM', 'ORCL', 'MSFT'], ['GOOG', 'FB']],dbPath=dbPath)\ndf = pd.DataFrame({'sym':['IBM', 'ORCL', 'MSFT', 'GOOG', 'FB'], 'val':[1,2,3,4,5]})\nt = s.table(data=df)\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns='sym').append(t)\nre = s.loadTable(tableName='pt', dbPath=dbPath).toDF()\n```\n\n----------------------------------------\n\nTITLE: Creating RANGE Partitioned Database by Integer - DolphinDB Python\nDESCRIPTION: Demonstrates creating a database partitioned by a defined range of integer values using the RANGE partition type. Assumes previous session initialization, and uses a fixed list for partition boundaries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndbPath=\"dfs://db_range_int\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.RANGE, partitions=[1, 11, 21], dbPath=dbPath)\n\n```\n\n----------------------------------------\n\nTITLE: Clear Memory after run in DolphinDB\nDESCRIPTION: This snippet demonstrates clearing variables from server memory after executing the 'run' method. By setting `clearMemory=True`, the variable created within the `run` method will be automatically released.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\ns.run(\"t=1\", clearMemory = True)\ns.run(\"t\")\n```\n\n----------------------------------------\n\nTITLE: Upserting Data into a Partitioned DFS Table using Python\nDESCRIPTION: Python example showing how to upsert data into a partitioned DolphinDB File System (DFS) table (`pt` in `dfs://valuedb`). It first connects and runs a script to set up the DFS table. It then initializes `tableUpsert` specifying the `dbPath`, `tableName`, `ddbSession`, and crucially, `keyColNames=['id']` because DFS tables lack inherent key columns. Data is generated in a loop, packaged into Pandas DataFrames, and upserted iteratively. Dependencies include `dolphindb`, `numpy`, `pandas`, and `datetime`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.2_TableUpserter.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\nimport numpy as np\nimport pandas as pd\nimport datetime\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript_DFS_VALUE = \"\"\"\n    if(existsDatabase(\"dfs://valuedb\")){\n        dropDatabase(\"dfs://valuedb\")\n    }\n    db = database(\"dfs://valuedb\", VALUE, 0..9)\n    t = table(1000:0, `date`text`id`flag, [DATETIME, STRING, LONG, INT])\n    p_table = db.createPartitionedTable(t, `pt, `flag)\n\"\"\"\ns.run(script_DFS_VALUE)\nupserter = ddb.tableUpsert(dbPath=\"dfs://valuedb\", tableName=\"pt\", ddbSession=s, keyColNames=[\"id\"])\n\nfor i in range(10):\n    dates = [np.datetime64(datetime.datetime.now()) for _ in range(100)]\n    texts = [f\"test_{i}_{_}\" for _ in range(100)]\n    ids = np.array([ _ % 10 for _ in range(100)], dtype=\"int32\")\n    flags = [ _ % 10 for _ in range(100)]\n    df = pd.DataFrame({\n        'date': dates,\n        'text': texts,\n        'id': ids,\n        'flag': flags,\n    })\n    upserter.upsert(df)\n\np_table = s.run(\"select * from p_table\")\nprint(p_table)\n```\n\n----------------------------------------\n\nTITLE: Error Handling with MultithreadedTableWriter in Python\nDESCRIPTION: This example demonstrates error handling with MultithreadedTableWriter, including handling type mismatches and column count errors. It shows how different types of errors are processed by the MTW queue and worker threads.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_88\n\nLANGUAGE: Python\nCODE:\n```\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\ntry:\n    # 插入100行正确数据 （类型和列数都正确），MTW正常运行\n    for i in range(100):\n        res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\", random.randint(1,10000))\n    # 插入10行类型错误数据，此时 MTW 队列并不会进行类型判断，这些数据能够进入 MTW 队列\n    # 直到工作线程对这些数据进行转换时，检测到类型不匹配，就会立刻终止 MTW 所有工作线程\n    for i in range(10):\n        res = writer.insert(np.datetime64('2022-03-23'),222, random.randint(1,10000))\n        if res.hasError():\n            # 此处不会执行到\n            print(\"Insert wrong format data:\\n\", res)\n    # 插入1行数据(列数不匹配)，MTW 立刻发现待插入数据列数与待插入表的列数不匹配，立刻返回错误信息\n    res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\")\n    if res.hasError():\n        # 数据错误，插入列数不匹配数据\n        print(\"Column counts don't match:\\n\", res)\n    # sleep 1秒，等待 MTW 工作线程处理数据直至检测到第2次插入的10行数据类型不匹配\n    # 此时 MTW 立刻终止所有工作线程，并修改状态为错误状态\n    time.sleep(1)\n\n    # 再插入1行正确数据，MTW 会因为工作线程终止而抛出异常，且不会写入该行数据\n    res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\", random.randint(1,10000))\n    print(\"MTW has exited\")\nexcept Exception as ex:\n    # MTW 抛出异常\n    print(\"MTW exit with exception %s\" % ex)\n```\n\n----------------------------------------\n\nTITLE: Creating HASH Partitioned Database by Integer - DolphinDB Python\nDESCRIPTION: Creates a HASH-partitioned DolphinDB database using an integer type and a specified number of hash buckets. Employs settings constants (e.g., keys.DT_INT) as required by the API. Useful for even distribution of data by key values.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndbPath=\"dfs://db_hash_int\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.HASH, partitions=[keys.DT_INT, 3], dbPath=dbPath)\n\n```\n\n----------------------------------------\n\nTITLE: Upserting Data into a Keyed Memory Table using Python\nDESCRIPTION: Complete Python example demonstrating how to connect to DolphinDB, create a shared keyed table `keyed_t` using a script, instantiate a `tableUpsert` object targeting `keyed_t`, prepare sample data in a Pandas DataFrame, and use the `upsert` method to insert/update rows based on the 'id' key. Finally, it retrieves and prints the resulting table. Dependencies include `dolphindb`, `numpy`, and `pandas`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.2_TableUpserter.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\nimport numpy as np\nimport pandas as pd\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n\nscript_KEYEDTABLE = \"\"\"\n    testtable=keyedTable(`id,1000:0,`date`text`id,[DATETIME,STRING,LONG])\n    share testtable as keyed_t\n    \"\"\"\ns.run(script_KEYEDTABLE)\nupserter = ddb.tableUpsert(tableName=\"keyed_t\", ddbSession=s)\ndates=[]\ntexts=[]\nids=[]\nfor i in range(1000):\n    dates.append(np.datetime64('2012-06-13 13:30:10.008'))\n    texts.append(f\"test_i_{i}\")\n    ids.append(i%10)\ndf = pd.DataFrame({\n    'date': dates,\n    'text': texts,\n    'id': ids,\n})\nupserter.upsert(df)\nkeyed_t = s.run(\"keyed_t\")\nprint(keyed_t)\n```\n\n----------------------------------------\n\nTITLE: Downloading Set Data Using DolphinDB Python API in Python\nDESCRIPTION: Illustrates how to download DolphinDB Set data into Python as a native set object. The DolphinDB Set elements are converted individually as Scalars to their Python counterparts. This snippet requires a DolphinDB session object `s` and demonstrates retrieving a set of integers, validating its type and contents. Supported types for conversion currently include CHAR, SHORT, INT, LONG, FLOAT, DOUBLE, STRING, and SYMBOL.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> re = s.run(\"set(1..5)\")\n>>> re\n{1, 2, 3, 4, 5}\n>>> type(re)\n<class 'set'>\n```\n\n----------------------------------------\n\nTITLE: Enabling Job Cancellation in DolphinDB (Python)\nDESCRIPTION: The `enableJobCancellation()` method, a static method available in the session object, is used to enable the forced cancellation of jobs in DolphinDB. When enabled, it allows the user to terminate running jobs submitted by the API when the API process is terminated through means such as Ctrl+C. This feature is disabled by default and is currently only available on Linux systems. There are no specific parameters to be passed to this method. The output is the enabling of the function.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.5_OtherFunctions/3.5_OtherFunctions.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Importing Necessary Modules and Dropping a Database\nDESCRIPTION: This snippet imports the `dolphindb.settings` module and checks if a database named 'dfs://valuedb' exists. If it does, the database is dropped using the `s.dropDatabase` function. This ensures a clean environment before creating and loading data into the database.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import dolphindb.settings as keys\n>>> if s.existsDatabase(\"dfs://valuedb\"):\n...     s.dropDatabase(\"dfs://valuedb\")\n...\n```\n\n----------------------------------------\n\nTITLE: Left Join using DolphinDB Python API\nDESCRIPTION: This snippet illustrates performing a left join using the `merge` method with the `how=\"left\"` parameter. It joins a DolphinDB table (`trade`) loaded from DFS with an in-memory table (`t1`). The join columns (`TICKER`, `date`) are specified using the `on` parameter. The result is filtered using `where` clauses and then converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_85\n\nLANGUAGE: Python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = s.table(data={'TICKER': ['AMZN', 'AMZN', 'AMZN'], 'date': ['2015.12.31', '2015.12.30', '2015.12.29'], 'open': [695, 685, 674]}, tableAliasName=\"t1\")\ns.run(\"\"\"t1 = select TICKER,date(date) as date,open from t1\"\"\")\nprint(trade.merge(t1,how=\"left\", on=[\"TICKER\",\"date\"]).where('TICKER=`AMZN').where('2015.12.23<=date<=2015.12.31').toDF())\n```\n\n----------------------------------------\n\nTITLE: Appending DataFrame with Time Columns and Type Conversion - DolphinDB Python API (Python)\nDESCRIPTION: Shows uploading a DataFrame with datetime64 columns and appending it to a DolphinDB memory table, using select statements to explicitly convert time columns to DATE on the server before insertion. Requires pandas, numpy, and dolphindb. Ensures compatibility between Python's datetime64 and DolphinDB's date type to prevent type mismatch issues.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript = \"\"\"t = table(1000:0,`id`date`ticker`price, [INT,DATE,SYMBOL,DOUBLE])\nshare t as tglobal\"\"\"\ns.run(script)\n\nimport pandas as pd\nimport numpy as np\ndef createDemoDict():\n\t\treturn {'id': [1, 2, 2, 3],\n            'date': np.array(['2019-02-04', '2019-02-05', '2019-02-09', '2019-02-13'], dtype='datetime64[D]'),\n            'ticker': ['AAPL', 'AMZN', 'AMZN', 'A'],\n            'price': [22.0, 3.5, 21.0, 26.0]}\n\ntb=pd.DataFrame(createDemoDict())\ns.upload({'tb':tb})\ns.run(\"tableInsert(tglobal,(select id, date(date) as date, ticker, price from tb))\")\nprint(s.run(\"tglobal\"))\n\n#output\n   id\t      date ticker\tprice\n0\t1\t2019-02-04\tAAPL\t22.0\n1\t2\t2019-02-05\tAMZN\t3.5\n2\t2\t2019-02-09\tAMZN\t21.0\n3\t3\t2019-02-13\tA\t26.0\n```\n\n----------------------------------------\n\nTITLE: Deleting rows in a DolphinDB table using Python\nDESCRIPTION: This code demonstrates deleting specific rows from a DolphinDB table based on conditions, using the delete method combined with execute to remove matched records, applied on both local and distributed tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_103\n\nLANGUAGE: Python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+/example.csv)\ntrade.delete().where('date<2013.01.01').execute()\n```\n\n----------------------------------------\n\nTITLE: Removing columns from a DolphinDB table in Python\nDESCRIPTION: This example demonstrates how to remove specific columns from a DolphinDB table in Python, employing the drop method and displaying the top records of the modified table as DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_105\n\nLANGUAGE: Python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+/example.csv)\nt1=trade.drop(['ask', 'bid'])\nprint(t1.top(5).toDF())\n```\n\n----------------------------------------\n\nTITLE: Calculating Sum within Groups using CONTEXT BY in Python (Comparison)\nDESCRIPTION: Loads the 'trade' table and groups data by 'TICKER' and 'month(date)' using `contextby()`. It calculates the total `sum(VOL)` for each group in the `select()` clause. Unlike `groupby`, `contextby` applies the aggregate function (`sum` here) and returns a result for each row within the group, where each row in a group gets the same total sum value for that group. The result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_124\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\").select(\"TICKER, month(date) as month, sum(VOL)\").contextby(\"TICKER,month(date)\").toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Setting TCP Timeout with setTimeout\nDESCRIPTION: The `setTimeout` is a session class method to set the TCP_USER_TIMEOUT option for the TCP connection. This sets the amount of time a TCP connection can live without any end-to-end communication, in seconds. It only works on Linux. The default time is 30 seconds.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> ddb.session.setTimeout(3600)\n```\n\n----------------------------------------\n\nTITLE: Dropping a Database with dropDatabase\nDESCRIPTION: The `dropDatabase` function is used to remove a database and all of its physical files. It takes the `dbPath` as a parameter, which is the path to the database.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> s.dropDatabase(dbPath=\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Sharing a New Memory Table in DolphinDB - Python\nDESCRIPTION: Shows how to connect to DolphinDB from Python and create a new empty memory table using a multi-line script. Defines four columns with specific data types (INT, DATE, SYMBOL, DOUBLE), and makes the table session-global with share. Requires dolphindb, a server endpoint, and appropriate user credentials. The resulting table will be available for subsequent insert and query operations within the session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"t = table(1000:0,`id`date`ticker`price, [INT,DATE,SYMBOL,DOUBLE])\nshare t as tglobal\"\"\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Create and Upload table using DataFrame with 'table' method\nDESCRIPTION: This snippet demonstrates how to create a DolphinDB table from a pandas DataFrame and upload it to the DolphinDB server using the `table` method.  It also creates a sample DataFrame using the function `createDemoDataFrame`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndef createDemoDataFrame():\n    data = {'cid': np.array([1, 2, 3], dtype=np.int32),\n            'cbool': np.array([True, False, np.nan], dtype=np.bool),\n            'cchar': np.array([1, 2, 3], dtype=np.int8),\n            'cshort': np.array([1, 2, 3], dtype=np.int16),\n            'cint': np.array([1, 2, 3], dtype=np.int32),\n            'clong': np.array([0, 1, 2], dtype=np.int64),\n            'cdate': np.array(['2019-02-04', '2019-02-05', ''], dtype='datetime64[D]'),\n            'cmonth': np.array(['2019-01', '2019-02', ''], dtype='datetime64[M]'),\n            'ctime': np.array(['2019-01-01 15:00:00.706', '2019-01-01 15:30:00.706', ''], dtype='datetime64[ms]'),\n            'cminute': np.array(['2019-01-01 15:25', '2019-01-01 15:30', ''], dtype='datetime64[m]'),\n            'csecond': np.array(['2019-01-01 15:00:30', '2019-01-01 15:30:33', ''], dtype='datetime64[s]'),\n            'cdatetime': np.array(['2019-01-01 15:00:30', '2019-01-02 15:30:33', ''], dtype='datetime64[s]'),\n            'ctimestamp': np.array(['2019-01-01 15:00:00.706', '2019-01-01 15:30:00.706', ''], dtype='datetime64[ms]'),\n            'cnanotime': np.array(['2019-01-01 15:00:00.80706', '2019-01-01 15:30:00.80706', ''], dtype='datetime64[ns]'),\n            'cnanotimestamp': np.array(['2019-01-01 15:00:00.80706', '2019-01-01 15:30:00.80706', ''], dtype='datetime64[ns]'),\n            'cfloat': np.array([2.1, 2.658956, np.NaN], dtype=np.float32),\n            'cdouble': np.array([0., 47.456213, np.NaN], dtype=np.float64),\n            'csymbol': np.array(['A', 'B', '']),\n            'cstring': np.array(['abc', 'def', ''])}\n    return pd.DataFrame(data)\n```\n\nLANGUAGE: python\nCODE:\n```\ndt = s.table(data=createDemoDataFrame(), tableAliasName=\"testDataFrame\")\nprint(s.loadTable(\"testDataFrame\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Heterogeneous Stream Table with Python\nDESCRIPTION: Demonstrates how to subscribe to the heterogeneous stream table `outTables` from Python. It defines a handler function to process incoming messages based on their type ('msg1', 'msg2', 'msg3'), creates a `streamDeserializer` to map message keys to table schemas, and uses the Dolphindb Python API to establish the subscription. Requires the Dolphindb Python API and a running DolphinDB server with the `outTables` table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_103\n\nLANGUAGE: Python\nCODE:\n```\nfrom threading import Event\n\ndef streamDeserializer_handler(lst): # the last element of the list returned by the deserializer is a key specified in sym2table\n    if lst[-1]==\"msg1\":\n        print(\"Msg1: \", lst)\n    elif lst[-1]=='msg2':\n        print(\"Msg2: \", lst)\n    else:\n        print(\"Msg3: \", lst)\n\ns = ddb.session(\"192.168.1.103\", 8921, \"admin\", \"123456\")\ns.enableStreaming(10020)\n\n# Construct the deserializer\nsd = ddb.streamDeserializer({\n    'msg1': [\"dfs://test_StreamDeserializer_pair\", \"pt1\"],\t# specify a list of the DFS database path and table name\n    'msg2': \"pt2\",\t\t # Specify the stream table name\n    'msg3': \"pt3\",\t\t # Specify the in-memory table name\n}, session=s)\t\t\t # If session is not specified, get the current session during subscription\ns.subscribe(host=\"192.168.1.103\", port=8921, handler=streamDeserializer_handler, tableName=\"outTables\", actionName=\"action\", offset=0, resub=False,\n            msgAsTable=False, streamDeserializer=sd, userName=\"admin\", password=\"123456\")\nEvent().wait()\n```\n\n----------------------------------------\n\nTITLE: Creating a DolphinDB Distributed Table using Python\nDESCRIPTION: Illustrates creating a DolphinDB distributed table using the Python API. It connects to the DolphinDB server and executes a DolphinDB script via `s.run`. The script checks for an existing database, drops it if necessary, creates a new VALUE-partitioned database, defines a table schema, inserts initial data, and then creates a partitioned table within the database.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n# 生成分布式表\ndbPath=\"dfs://testPython\"\ntableName='t1'\nscript = \"\"\"\ndbPath='{db}'\nif(existsDatabase(dbPath))\n\tdropDatabase(dbPath)\ndb = database(dbPath, VALUE, 0..100)\nt1 = table(10000:0,`id`cbool`cchar`cshort`cint`clong`cdate`cmonth`ctime`cminute`csecond`cdatetime`ctimestamp`cnanotime`cnanotimestamp`cfloat`cdouble`csymbol`cstring,[INT,BOOL,CHAR,SHORT,INT,LONG,DATE,MONTH,TIME,MINUTE,SECOND,DATETIME,TIMESTAMP,NANOTIME,NANOTIMESTAMP,FLOAT,DOUBLE,SYMBOL,STRING])\ninsert into t1 values (0,true,'a',122h,21,22l,2012.06.12,2012.06M,13:10:10.008,13:30m,13:30:10,2012.06.13 13:30:10,2012.06.13 13:30:10.008,13:30:10.008007006,2012.06.13 13:30:10.008007006,2.1f,2.1,'','')\nt = db.createPartitionedTable(t1, `{tb}, `id)\nt.append!(t1)\"\"\".format(db=dbPath,tb=tableName)\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Dropping DolphinDB Database Partitions using Session Object (Python)\nDESCRIPTION: Demonstrates creating a database, loading data, and then deleting specific partitions from it using the `dropPartition` method of a `ddb.session` object. It highlights the requirement to add extra quotes around string-based partition values (e.g., `\"'AMZN'\"`) when specifying `partitionPaths` in the Python API call. Requires an active DolphinDB session (`s`), the `dolphindb` library, and assumes `WORK_DIR` is defined.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\",\"NVDA\"], dbPath=\"dfs://valuedb\")\ntrade=s.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\nprint(trade.rows)\n# output\n13136\n\ns.dropPartition(\"dfs://valuedb\", partitionPaths=[\"'AMZN'\",\"'NFLX'\"]) \n# or s.dropPartition(\"dfs://valuedb\", partitionPaths=[\"`AMZN`NFLX`\"]), tableName=\"trade\")\ntrade = s.loadTable(tableName=\"trade\", dbPath=\"dfs://valuedb\")\nprint(trade.rows)\n# output\n4516\n\nprint(trade.select(\"distinct TICKER\").toDF())\n# output\n  distinct_TICKER\n0            NVDA\n```\n\n----------------------------------------\n\nTITLE: Defining DolphinDB Function View for Async Appends (DolphinDB Script)\nDESCRIPTION: DolphinDB script (presented as text) defining a function `appendStreamingData` that inserts data into a shared stream table (`tglobal`), performing necessary type conversion (e.g., `date(data.time)`). This function is then exposed as a function view, allowing clients to call it remotely, which is useful for handling type conversions in asynchronous append scenarios.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_75\n\nLANGUAGE: txt\nCODE:\n```\nlogin(\"admin\",\"123456\")\ntrades = streamTable(10000:0,`time`sym`price`id, [DATE,SYMBOL,DOUBLE,INT])\nshare trades as tglobal\ndef appendStreamingData(mutable data){\ntableInsert(tglobal, data.replaceColumn!(`time, date(data.time)))\n}\naddFunctionView(appendStreamingData)\n```\n\n----------------------------------------\n\nTITLE: Downloading Matrix Data Using DolphinDB Python API in Python\nDESCRIPTION: Demonstrates how to download DolphinDB Matrix type data into Python using the API. The Matrix corresponds to a 2D numpy ndarray along with row and column names. The snippet shows code that creates a matrix, renames rows and columns, and returns the matrix with its metadata. The expected output includes the ndarray, row names, and column names as numpy arrays. Dependencies include numpy and a DolphinDB session object `s`. This example highlights that row/column names are preserved during download, and None is used if absent.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> s.run(\"\"\"\n...     mtx = 1..12$4:3;\n...     mtx.rename!(1 2 3 4, `c1`c2`c3);\n...     mtx\n... \"\"\")\n[array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]], dtype=int32), array([1, 2, 3, 4], dtype=int32), array(['c1', 'c2', 'c3'], dtype=object)]\n```\n\n----------------------------------------\n\nTITLE: Dropping a Table with dropTable\nDESCRIPTION: The `dropTable` function deletes a table from a database, and it takes the `dbPath` and `tableName` as parameters.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> s.dropTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n```\n\n----------------------------------------\n\nTITLE: Creating and sharing a DolphinDB in-memory table via Python API\nDESCRIPTION: Python script configuring a DolphinDB session to create a new in-memory table with specified size, columns, and data types, then sharing it across sessions for multi-server access. This code exemplifies table creation with explicit schema definition and sharing when necessary.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"t = table(1000:0,`id`date`ticker`price, [INT,DATE,SYMBOL,DOUBLE])\nshare t as tglobal\"\"\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Calculating Trading Cost with Window Join in DolphinDB Python API\nDESCRIPTION: This snippet calculates trading cost by performing a window merge between 'trades' and 'quotes' tables in DolphinDB. It uses aggregated functions (wavg) within the merge and calculates cost based on trade volume, price, and average bid/offer prices, grouped by symbol.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_92\n\nLANGUAGE: python\nCODE:\n```\ntb = trades.merge_window(quotes,-1000000000, 0, aggFunctions=\"[wavg(Offer_Price, Offer_Size) as Offer_Price, wavg(Bid_Price, Bid_Size) as Bid_Price]\",\n                         on=[\"Symbol\",\"Time\"], prevailing=True)\\\n                         .select(\"sum(Trade_Volume*abs(Trade_Price-(Bid_Price+Offer_Price)/2))/sum(Trade_Volume*Trade_Price)*10000 as cost\")\\\n                         .groupby(\"Symbol\")\nprint(tb.toDF())\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Time-Series Aggregator (Non-overlapping)\nDESCRIPTION: Configures a DolphinDB `createTimeSeriesAggregator` for calculating OHLC bars on the `Trade` stream table with non-overlapping 5-minute windows (`windowSize=300`, `step=300`). It specifies the metrics (first, max, min, last Price, sum Volume), input table (`Trade`), output table (`OHLC`), time column (`Datetime`), and key column (`Symbol`). Requires a running DolphinDB server and the `Trade` and `OHLC` tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_108\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntsAggrKline = createTimeSeriesAggregator(name=\"aggr_kline\", windowSize=300, step=300, metrics=<[first(Price),max(Price),min(Price),last(Price),sum(volume)]>, dummyTable=Trade, outputTable=OHLC, timeColumn=`Datetime, keyColumn=`Symbol)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Rewriting Unwritten Data After Error (Python)\nDESCRIPTION: Illustrates the recovery process when a `MultithreadedTableWriter` encounters a critical error and exits. It retrieves the list of unwritten data using `writer.getUnwrittenData()`, creates a *new* `MultithreadedTableWriter` instance, and attempts to insert the unwritten data using `insertUnwrittenData()`. Shows checking the status of the rewrite and the final count in the table. Requires a `MultithreadedTableWriter` that exited with an error.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nif writeStatus.hasError():\n    print(\"Error in writing:\")\n    unwriterdata = writer.getUnwrittenData()\n    print(\"Unwriterdata: %d\" % len(unwriterdata))\n    # Creater a new MTW object\n    newwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\n    try:\n        for row in unwriterdata:\n            row[1]=\"aaaaa\"\n        res = newwriter.insertUnwrittenData(unwriterdata)\n        if res.succeed():\n            newwriter.waitForThreadCompletion()\n            writeStatus=newwriter.getStatus()\n            print(\"Write again:\\n\", writeStatus)\n        else:\n            print(\"Failed to write data again: \\n\",res) \n    except Exception as ex:\n        print(\"MTW exit with exception %s\" % ex)\n    finally:\n        newwriter.waitForThreadCompletion()\nelse:\n    print(\"Write successfully:\\n\", writeStatus)\n\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Connecting with Native DDB Protocol in DolphinDB Python API - Python\nDESCRIPTION: Creates a DolphinDB session using native DDB protocol, establishes a connection to a local server, and executes a query to retrieve a table as a pandas.DataFrame. Requires dolphindb, proper server credentials, and the network endpoint. The run method executes DolphinDB script code and converts results appropriately for use in Python.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nre = s.run(\"table(1..10 as a)\")   # pandas.DataFrame\n```\n\n----------------------------------------\n\nTITLE: Calling DolphinDB Built-in and User-Defined Functions via run in Python\nDESCRIPTION: Demonstrates calling DolphinDB functions directly with the run method, passing parameters as needed. Supports partial application and different ways of assigning values to variables before function execution. Parameters can be scalars, lists, dictionaries, numpy objects, pandas DataFrames, etc.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\na=s.run(\"add(x,y)\")\nrepr(a)\n```\n\nLANGUAGE: Python\nCODE:\n```\ns.run(\"x = [1,3,5]\")\nresult=s.run(\"add{x,}\", y)\nrepr(result)\n# output\n'array([2,5,8])'\n\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nx=np.array([1.5,2.5,7])\ny=np.array([8.5,7.5,3])\nresult=s.run(\"add\", x, y)\nrepr(result)\n```\n\n----------------------------------------\n\nTITLE: Dropping a DolphinDB Database using Session Object (Python)\nDESCRIPTION: Shows how to delete a DolphinDB database using the `dropDatabase` method of a `ddb.session` object. It first checks if the database exists using `existsDatabase` before attempting deletion. Requires an active DolphinDB session (`s`) and the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Create and Upload table using dictionary with 'table' method\nDESCRIPTION: This snippet demonstrates how to create a DolphinDB table from a Python dictionary and upload it to the DolphinDB server using the `table` method. It also shows how to load the table back from the server and display it as a pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef createDemoDict():\n    return {'id': [1, 2, 2, 3],\n            'date': np.array(['2021-05-06', '2021-05-07', '2021-05-06', '2021-05-07'], dtype='datetime64[D]'),\n            'ticker': ['AAPL', 'AAPL', 'AMZN', 'AMZN'],\n            'price': [129.74, 130.21, 3306.37, 3291.61]}\n\n```\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n# save the table to DolphinDB server as table \"testDict\"\ndt = s.table(data=createDemoDict(), tableAliasName=\"testDict\")\n\n# load table \"testDict\" on DolphinDB server\nprint(s.loadTable(\"testDict\").toDF())\n\n# output\n   id       date ticker    price\n0   1 2021-05-06   AAPL   129.74\n1   2 2021-05-07   AAPL   130.21\n2   2 2021-05-06   AMZN  3306.37\n3   3 2021-05-07   AMZN  3291.61\n```\n\n----------------------------------------\n\nTITLE: Performing OLS Regression and Accessing Results on DolphinDB Table Data in Python (Trade Dataset)\nDESCRIPTION: This snippet loads a DolphinDB table named 'trade', selects specific columns, performs an OLS regression with 'PRC' as the dependent variable and 'BID', 'ASK' as independent variables. It then demonstrates how to access different parts of the resulting dictionary, such as ANOVA, RegressionStat, and Coefficient tables, and retrieve individual beta values.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_95\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\nz=trade.select(['bid','ask','prc']).ols('PRC', ['BID', 'ASK'])\n\nprint(z[\"ANOVA\"])\n\nprint(z[\"RegressionStat\"])\n\n\nprint(z[\"Coefficient\"])\n\n\nprint(z[\"Coefficient\"].beta[1])\n```\n\n----------------------------------------\n\nTITLE: Initializing BatchTableWriter in Python\nDESCRIPTION: Instantiates the BatchTableWriter class to connect to a DolphinDB server. Requires host and port, with optional user credentials and a lock flag for thread safety in concurrent scenarios (defaults to True).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nBatchTableWriter(host, port, userid=None, password=None, acquireLock=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a Large Test Table in DolphinDB\nDESCRIPTION: Creates a large test table in DolphinDB for demonstrating batch data reading. The script generates 100,000 rows of sample data with various data types.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript='''\n    rows=100000;\n    testblock=table(take(1,rows) as id,take(`A,rows) as symbol,take(2020.08.01..2020.10.01,rows) as date, rand(50,rows) as size,rand(50.5,rows) as price);\n'''\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated SQL for Chained `where()` and `sort()` (Python)\nDESCRIPTION: Demonstrates calling `showSQL()` on a query object built with `select`, multiple chained `where` conditions, and `sort`. It prints the consolidated DolphinDB SQL statement generated from the chained Python method calls. Requires a `trade` table object.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nprint(trade.select(['date','bid','ask','prc','vol']).where('TICKER=`AMZN').where('bid!=NULL').where('ask!=NULL').where('vol>10000000').sort('vol desc').showSQL())\n```\n\n----------------------------------------\n\nTITLE: Inserting Multiple Rows Data with MultithreadedTableWriter in Python\nDESCRIPTION: The insertUnwrittenData method allows inserting multiple rows of data at once. It's particularly useful for resubmitting data that failed to write previously and can be used with data returned by getUnwrittenData.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_81\n\nLANGUAGE: Python\nCODE:\n```\ninsertUnwrittenData(unwrittenData)\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Partitioned Table (DolphinDB Script)\nDESCRIPTION: Defines a HASH partitioned database 'dfs://valuedb3' and creates a partitioned table 'pdatetest' within it. It drops the database if it already exists. This script sets up the target table schema and partitioning for data insertion using the MultithreadedTableWriter. Requires a running DolphinDB server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_41\n\nLANGUAGE: dolphindb\nCODE:\n```\ndbName = 'dfs://valuedb3';\nif(exists(dbName)){\n    dropDatabase(dbName);\n}\ndatetest=table(1000:0,`date`symbol`id,[DATE,SYMBOL,LONG]);\ndb = database(directory=dbName, partitionType=HASH, partitionScheme=[INT, 10]);\npt=db.createPartitionedTable(datetest,'pdatetest','id');\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned DFS Table in DolphinDB Script\nDESCRIPTION: DolphinDB script snippet used in Example 2. It first checks if the database `dfs://valuedb` exists and drops it if it does. Then, it creates the database using VALUE partitioning based on integer values from 0 to 9. Finally, it defines a table schema and creates a partitioned table `pt` within this database, using the 'flag' column for partitioning.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.2_TableUpserter.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nscript_DFS_VALUE = \"\"\"\n    if(existsDatabase(\"dfs://valuedb\")){\n        dropDatabase(\"dfs://valuedb\")\n    }\n    db = database(\"dfs://valuedb\", VALUE, 0..9)\n    t = table(1000:0, `date`text`id`flag, [DATETIME, STRING, LONG, INT])\n    p_table = db.createPartitionedTable(t, `pt, `flag)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data into DolphinDB DFS Partitioned Table using `loadTextEx` in Python\nDESCRIPTION: Checks and drops an existing database if present, creates a new VALUE partitioned database `mydb` at `dfs://valuedb`, then uses `s.loadTextEx()` to load data from `example.csv` directly into a partitioned table `trade` within that database. The `partitionColumns` parameter specifies the partitioning key (`TICKER`). Finally, it retrieves the table data, row/column counts, and schema information. Requires `dolphindb` and `pandas` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\", \"NVDA\"], dbPath=\"dfs://valuedb\")\n\ntrade = s.loadTextEx(dbPath=\"mydb\", tableName='trade',partitionColumns=[\"TICKER\"], remoteFilePath=WORK_DIR + \"/example.csv\")\nprint(trade.toDF())\n\n#返回表中的行数：\nprint(trade.rows)\n\n#返回表中的列数：\nprint(trade.cols)\n\n#展示表的结构：\nprint(trade.schema)\n```\n\n----------------------------------------\n\nTITLE: Deleting a DolphinDB Database and Table in Python\nDESCRIPTION: Checks if a DolphinDB database ('dfs://valuedb') exists and drops it. Then, it creates a new partitioned database, loads data into a table named 'trade' from a CSV file, and finally drops the 'trade' table. This sequence prepares the environment or cleans up resources.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_106\n\nLANGUAGE: python\nCODE:\n```\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\",\"NVDA\"], dbPath=\"dfs://valuedb\")\ns.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\ns.dropTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Subscription: Single Mode\nDESCRIPTION: This snippet demonstrates a single-mode stream subscription. It establishes a connection to the DolphinDB server, creates a stream table, enables streaming, defines a handler function to process incoming messages, subscribes to the stream, inserts data into the stream table, waits for 3 seconds, and then unsubscribes. The handler receives individual data entries.  No batchSize is specified and msgAsTable is False.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.3_SubscriptionOptions/3.3_SubscriptionOptions.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport time\n\ns = ddb.session()\ns.connect(\"192.168.1.113\", 8848, \"admin\", \"123456\")\n\ns.run(\"\"\"\nshare streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,DOUBLE,INT]) as trades\n\"\"\")\n\ns.enableStreaming()\n\ndef handler(lst):\n    print(lst)\n\ns.subscribe(\"192.168.1.113\", 8848, handler, \"trades\", \"SingleMode\", offset=-1)\n\ns.run(\"insert into trades values(take(now(), 6), take(\\`000905\\`600001\\`300201\\`000908\\`600002, 6), rand(1000,6)/10.0, 1..6)\")\n\ntime.sleep(3)\n\ns.unsubscribe(\"192.168.1.113\", 8848, \"trades\", \"SingleMode\")\n```\n\n----------------------------------------\n\nTITLE: Inner Join with Different Column Names using DolphinDB Python API\nDESCRIPTION: This snippet demonstrates how to perform an inner join between a DolphinDB table loaded from DFS (`trade`) and an in-memory table (`t1`) when the join columns have different names. It uses the `left_on` and `right_on` parameters of the `merge` method. A DolphinDB script is run via `s.run` to convert the string date in `t1` to a proper date type before the join. The result is converted to a Pandas DataFrame for printing.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_84\n\nLANGUAGE: Python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = s.table(data={'TICKER1': ['AMZN', 'AMZN', 'AMZN'], 'date1': ['2015.12.31', '2015.12.30', '2015.12.29'], 'open': [695, 685, 674]}, tableAliasName=\"t1\")\ns.run(\"\"\"t1 = select TICKER1,date(date1) as date1,open from t1\"\"\")\nprint(trade.merge(t1,left_on=[\"TICKER\",\"date\"], right_on=[\"TICKER1\",\"date1\"]).toDF())\n```\n\n----------------------------------------\n\nTITLE: Setting KeepAlive Parameter for Long-Running Connections\nDESCRIPTION: Configures TCP keep-alive time to ensure timely detection of dead connections. The default is 60 seconds, but it can be customized during connection. Useful in unstable network environments to release half-open connections.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.2_Connect.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\n# Set keepAliveTime to 120 seconds\ns.connect(keepAliveTime=120)\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB TSDB Database and Partitioned Table in Python\nDESCRIPTION: Connects to DolphinDB, creates a TSDB database (`dfs://tsdb`) partitioned by date, creates a pandas DataFrame, converts it to a DolphinDB table, creates a partitioned table `pt` with specified `sortColumns`, appends data, and loads the table back to a DataFrame. Requires `dolphindb`, `numpy`, and `pandas` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\nimport numpy as np\nimport pandas as pd\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ndates = np.array(pd.date_range(start='20120101', end='20120110'), dtype=\"datetime64[D]\")\n\ndbPath = \"dfs://tsdb\"\nif s.existsDatabase(dbPath): s.dropDatabase(dbPath)\ndb = s.database(dbName='mydb_tsdb', partitionType=keys.VALUE, partitions=dates, dbPath=dbPath, engine=\"TSDB\")\n\ndf = pd.DataFrame({'datetime': np.array(\n    ['2012-01-01T00:00:00', '2012-01-02T00:00:00', '2012-01-04T00:00:00', '2012-01-05T00:00:00', '2012-01-08T00:00:00'],\n    dtype='datetime64'),\n    'sym': ['AA', 'BB', 'BB', 'AA', 'BB'], 'val': [1, 2, 3, 4, 5]})\nt = s.table(data=df)\n\ndb.createPartitionedTable(table=t, tableName='pt', partitionColumns='datetime', sortColumns=[\"sym\", \"datetime\"]).append(t)\nre = s.loadTable(tableName='pt', dbPath=dbPath).toDF()\nprint(re)\n```\n\n----------------------------------------\n\nTITLE: Error Output: Accessing a Deleted DolphinDB Table\nDESCRIPTION: Shows the Python `Exception` message that occurs when attempting to load a DolphinDB table ('/valuedb/trade.tbl') that has been previously dropped using `s.loadTable`. The error indicates the specified path does not exist.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_107\n\nLANGUAGE: text\nCODE:\n```\ns.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n\nException:\ngetFileBlocksMeta on path '/valuedb/trade.tbl' failed, reason: path does not exist\n```\n\n----------------------------------------\n\nTITLE: Using `limit()` Clause for Record Selection (Python)\nDESCRIPTION: Demonstrates two uses of the `limit()` clause. The first example uses `limit(-N)` with `contextby` to get the last N records per group ('ticker'). The second example uses `limit([offset, count])` to retrieve a specific range of records (5 records starting from index 2). Both require the 'trade' table loaded into 'dfs://valuedb'. Requires `s`, `dolphindb`, and a pre-existing 'trade' table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\n# Limit last N per group\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").contextby('ticker').limit(-2)\n# Assuming print(t1.toDF()) or similar follows based on output\n```\n\nLANGUAGE: python\nCODE:\n```\n# Limit with offset\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").limit([2, 5])\nprint(t1.toDF())\n```\n\n----------------------------------------\n\nTITLE: Bulk Insert Multiple Rows via insert into After Uploading Arrays - DolphinDB Python API (Python)\nDESCRIPTION: Illustrates bulk insertion into a DolphinDB memory table by uploading numpy arrays to the session and executing an insert into statement with those arrays as parameters. Suitable for structured batch inserts when time columns match destination table types. Requires numpy, pandas, and dolphindb. Efficient for small-to-medium batch sizes but less performant for large inserts than specialized bulk APIs.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nimport numpy as np\nimport random\nimport pandas as pd\nrowNum = 5\nids = np.arange(1, rowNum+1, 1, dtype=np.int32)\ndates = np.array(pd.date_range('4/1/2019', periods=rowNum), dtype='datetime64[D]')\ntickers = np.repeat(\"AA\", rowNum)\nprices = np.arange(1, 0.6*(rowNum+1), 0.6, dtype=np.float64)\ns.upload({'ids':ids, \"dates\":dates, \"tickers\":tickers, \"prices\":prices})\nscript = \"insert into tglobal values(ids,dates,tickers,prices);\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Stream Table - DolphinDB Python\nDESCRIPTION: This snippet demonstrates subscribing to a DolphinDB stream table using the `subscribe` function. It sets up a handler function to process incoming data. Dependencies include the `dolphindb` library and `numpy`.  The host and port specify the DolphinDB server.  The handler function receives data and the `Event().wait()` keeps the script running to receive subscription messages.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_98\n\nLANGUAGE: python\nCODE:\n```\nfrom threading import Event \n\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\ns.enableStreaming(10020)\n\n\ndef handler(lst):\n    print(lst)\n\ns.subscribe(\"192.168.1.92\",8848,handler,\"trades\",\"action\",0,False,np.array(['000905']),)\n\n\nEvent().wait()\n```\n\n----------------------------------------\n\nTITLE: Performing Window Join with Aggregation in DolphinDB Python API\nDESCRIPTION: This snippet uses the merge_window method to perform a window join, an extension of asof join, specifying time window bounds and aggregation functions on the right table columns. It calculates average bid and offer prices within the specified window and merges these with trades data joined on 'Symbol' and 'Time'. The output includes trade details with corresponding aggregated quote prices for a time-filtered subset. This technique allows time-bounded aggregations in joins and requires properly ordered input tables.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_137\n\nLANGUAGE: python\nCODE:\n```\nprint(trades.merge_window(quotes, -5000000000, 0, aggFunctions=[\"avg(Bid_Price)\",\"avg(Offer_Price)\"], on=[\"Symbol\",\"Time\"]).where(\"Time>=07:59:59\").top(10).toDF())\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Python API Test Settings\nDESCRIPTION: Configuration variables for connecting to a DolphinDB server and specifying relevant directories for testing. Includes server host, port, working directory, data directory, and subscription port.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/test/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nHOST=\"192.168.1.14\"\nPORT=20932\nWORK_DIR='/hdd/hdd5/hzy/python_api_test_server/WORK_DIR'\nDATA_DIR='/hdd/hdd1/jenkins/jenkins/workspace/python_api_test/data'\nSUBPORT=20936\n```\n\n----------------------------------------\n\nTITLE: Inserting Unwritten Data Batch with MultithreadedTableWriter in Python\nDESCRIPTION: Inserts a collection of records, typically obtained from `getUnwrittenData`, back into the writer's queue for processing. This allows for retrying failed or pending data writes in batches, unlike the single-record `insert` method.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\ninsertUnwrittenData(unwrittenData)\n```\n\n----------------------------------------\n\nTITLE: Using PartitionedTableAppender for Concurrent Writes in Python\nDESCRIPTION: Demonstrates how to create a DBConnectionPool and use PartitionedTableAppender to write data concurrently to a distributed table. The example creates a sample database with a partitioned table, then appends data in parallel using a connection pool.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\nimport numpy as np\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\nscript = '''\ndbPath = \"dfs://valuedb\"\n        if(existsDatabase(dbPath))\n            dropDatabase(dbPath)\n        t = table(100:100,`id`time`vol,[SYMBOL,DATE, INT])\n        db=database(dbPath,VALUE, `APPL`IBM`AMZN)\n        pt = db.createPartitionedTable(t, `pt, `id)\n'''\ns.run(script)\n\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, 20, \"admin\", \"123456\")\nappender = ddb.PartitionedTableAppender(\"dfs://valuedb\", \"pt\", \"id\", pool)\nn = 100\n\ndate = []\nfor i in range(n):\n    date.append(np.datetime64(\n        \"201{:d}-0{:1d}-{:2d}\".format(random.randint(0, 9), random.randint(1, 9), random.randint(10, 28))))\n\ndata = pd.DataFrame({\"id\": np.random.choice(['AMZN', 'IBM', 'APPL'], n), \"time\": date,\n                     \"vol\": np.random.randint(100, size=n)})\nre = appender.append(data)\n\nprint(re)\nprint(s.run(\"pt = loadTable('dfs://valuedb', 'pt'); select * from pt;\"))\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Merge with Identical Column Names Example\nDESCRIPTION: This code snippet demonstrates how to perform an inner join between two DolphinDB tables using the `merge` function when the join column names are identical. It loads the 'trade' table and creates a new table 't1'.  The tables are then joined based on 'TICKER' and 'date' columns.  A `s.run` is used to perform the date casting operation inside dolphindb.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_83\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = s.table(data={'TICKER': ['AMZN', 'AMZN', 'AMZN'], 'date': np.array(['2015-12-31', '2015-12-30', '2015-12-29'], dtype='datetime64[D]'), 'open': [695, 685, 674]}, tableAliasName=\"t1\")\ns.run(\"\"\"t1 = select TICKER,date(date) as date,open from t1\"\"\"\")\nprint(trade.merge(t1,on=[\"TICKER\",\"date\"]).toDF())\n```\n\n----------------------------------------\n\nTITLE: Handling MTW Data Insertion Errors (Python)\nDESCRIPTION: Shows how `MultithreadedTableWriter` handles insertion attempts with incorrect data types or mismatched column counts. It inserts correct data first, then attempts insertions with invalid data, catching errors returned by `insert` and the exception raised when the writer threads terminate due to critical errors. It waits for completion and prints the final detailed error status. Requires an initialized `MultithreadedTableWriter`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\ntry:\n        # insert 100 rows of records with correct data types and column count\n        for i in range(100):\n        res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\", random.randint(1,10000))\n        \n    # insert 10 rows of records with incorrect data types\n    for i in range(10):\n        res = writer.insert(np.datetime64('2022-03-23'),222, random.randint(1,10000))\n        if res.hasError():\n            print(\"Insert wrong format data:\\n\", res)\n    # Insert a row of record with incorrect column count\n    res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\")\n    if res.hasError():\n        print(\"Column counts don't match:\\n\", res)\n    \n    # Wait 1 sec for the working threads to process the data until it detects the incorrect data types for the second insert. All working threads terminate and the status turns to error.\n    time.sleep(1)\n\n    # Insert another row of data.\n    res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\", random.randint(1,10000))\n    print(\"MTW has exited\")\nexcept Exception as ex:\n    print(\"MTW exit with exception %s\" % ex)\nwriter.waitForThreadCompletion()\nwriteStatus=writer.getStatus()\nif writeStatus.hasError():\n    print(\"Error in writing:\")\nprint(writeStatus)\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Performing OLS Regression on DolphinDB Table Data in Python (US Dataset)\nDESCRIPTION: This snippet demonstrates setting up a database and loading data from a CSV file into a DolphinDB distributed table, then performing an Ordinary Least Squares (OLS) regression using the ols method on the table object. It calculates transformed columns (turnover, absRet, spread, logMV) before performing regression with 'turnover' as the dependent variable.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\nif s.existsDatabase(\"dfs://US\"):\n\ts.dropDatabase(\"dfs://US\")\ns.database(dbName='USdb', partitionType=keys.VALUE, partitions=[\"GFGC\",\"EWST\", \"EGAS\"], dbPath=\"dfs://US\")\nUS=s.loadTextEx(dbPath=\"dfs://US\", partitionColumns=[\"TICKER\"], tableName='US', remoteFilePath=WORK_DIR + \"/US.csv\")\n\nresult = s.loadTable(tableName=\"US\",dbPath=\"dfs://US\")\\\n         .select(\"select VOL\\\\SHROUT as turnover, abs(RET) as absRet, (ASK-BID)/(BID+ASK)*2 as spread, log(SHROUT*(BID+ASK)/2) as logMV\")\\\n         .where(\"VOL>0\").ols(\"turnover\", [\"absRet\",\"logMV\", \"spread\"], True)\n\nprint(result)\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB 'add' with list of datetime64\nDESCRIPTION: This snippet shows how to use a list of NumPy datetime64 objects as input to the DolphinDB 'add' function.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\na=[np.datetime64('2019-01-01T20:00:00.000000001'), np.datetime64('2019-01-01T20:00:00.000000001')]\ns.run(\"add{1,}\",a)\n# output\narray(['2019-01-01T20:00:00.000000002', '2019-01-01T20:00:00.000000002'], dtype='datetime64[ns]')\n```\n\n----------------------------------------\n\nTITLE: Updating Records in a DolphinDB Table Object (Python)\nDESCRIPTION: Shows how to update records in a DolphinDB table object loaded from a CSV file (`s.loadText`). It uses method chaining: `update` specifies the columns and new values, `where` filters the rows to be updated based on conditions, and `execute` applies the changes. The result is verified by selecting the updated rows and converting to a Pandas DataFrame using `toDF()`. Requires an active DolphinDB session (`s`), the `dolphindb` library, and assumes `WORK_DIR` is defined.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\ntrade = trade.update([\"VOL\"],[\"999999\"]).where(\"TICKER=`AMZN\").where([\"date=2015.12.16\"]).execute()\nt1=trade.where(\"ticker=`AMZN\").where(\"VOL=999999\")\nprint(t1.toDF())\n\n# output\n  TICKER       date     VOL        PRC        BID        ASK\n0   AMZN 2015-12-16  999999  675.77002  675.76001  675.83002\n```\n\n----------------------------------------\n\nTITLE: Checking MultithreadedTableWriter Status in Python\nDESCRIPTION: Provides a snapshot of the writer's current state, including whether threads are exiting, any error information, counts of rows sent, unsent, and failed, and the status of individual worker threads. Useful for monitoring the asynchronous write process.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\ngetStatus()\n```\n\n----------------------------------------\n\nTITLE: Defining DolphinDB Server-Function View for Time Type Conversion\nDESCRIPTION: This DolphinDB server-side snippet defines a shared streaming table and a function view named appendStreamingData that asynchronously inserts data into the streaming table with time column converted to DATE type. It logs in to DolphinDB, creates or shares a streaming table 'trades', defines a function that replaces the 'time' column type within the data, and registers that function as a callable view. This setup avoids race conditions with data type conversions when used with asynchronous data submissions from clients.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.1_SessionAsyncMode.md#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(\"admin\",\"123456\")\nshare streamTable(10000:0,`time`sym`price`id, [DATE,SYMBOL,DOUBLE,INT]) as trades\ndef appendStreamingData(mutable data){\n    tableInsert(trades, data.replaceColumn!(`time, date(data.time)))\n}\naddFunctionView(appendStreamingData)\n```\n\n----------------------------------------\n\nTITLE: Creating Dimension Table with createTable - DolphinDB Python\nDESCRIPTION: Illustrates using the createTable method on a Database object to create an empty dimension table using an existing Table object as schema. The example assigns the TSDB engine, runs DDL on the DolphinDB server, retrieves the schema, and prints column definitions. Prerequisites include a valid session, DDL privileges, and a predefined schema_t table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.4_ObjectOrientedOperationsOnDdbOBjects/3.4.1_Database.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndbPath = \"dfs://createTable\"\nif s.existsDatabase(dbPath):\n    s.dropDatabase(dbPath)\ndb = s.database(partitionType=keys.VALUE, partitions=[1, 2, 3], dbPath=dbPath, engine=\"TSDB\")\ns.run(\"schema_t = table(100:0, `ctime`csymbol`price`qty, [TIMESTAMP, SYMBOL, DOUBLE, INT])\")\nschema_t = s.table(data=\"schema_t\")\npt = db.createTable(schema_t, \"pt\", [\"csymbol\"])\nschema = s.run(f'schema(loadTable(\"{dbPath}\", \"pt\"))')\nprint(schema[\"colDefs\"])\n\n```\n\n----------------------------------------\n\nTITLE: Defining Python Function for Multithreaded MTW Use (Python)\nDESCRIPTION: Defines a simple Python function `insert_MTW` that takes a `MultithreadedTableWriter` object and performs multiple row insertions within a loop. This function is designed to be called by multiple Python threads simultaneously, demonstrating the thread-safe nature of calling `writer.insert()` from different application threads. Requires an initialized `MultithreadedTableWriter`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# Create a MTW object\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\n\ndef insert_MTW(writer):\n    try:\n        # insert 100 rows of records\n        for i in range(100):\n            res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\n    except Exception as ex:\n        print(\"MTW exit with exception %s\" % ex)\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns with String and Filtering with WHERE in Python\nDESCRIPTION: Selects columns from the 'trade' table object using a comma-separated string argument in `select()`. It then applies multiple filtering conditions by chaining `where()` methods and converts the final filtered result to a Pandas DataFrame for output.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_110\n\nLANGUAGE: python\nCODE:\n```\nprint(trade.select(\"ticker,date,bid,ask,prc,vol\").where(\"date=2012.09.06\").where(\"vol<10000000\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Using PROTOCOL_PICKLE for data serialization and example calls\nDESCRIPTION: Examples demonstrating data transfer with PROTOCOL_PICKLE, including standard and with 'pickleTableToList' parameter set to True, affecting how matrices and tables are serialized across Python and DolphinDB. In this case, data can be transferred as numpy arrays or list structures, with specific handling for tables and matrices.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_PICKLE)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\n# Default transfer, 'pickleTableToList' = False\nre1 = s.run(\"m=matrix(1 2, 3 4, 5 6);m.rename!(1 2, \\`a\\`b\\`x);m\")\nre2 = s.run(\"table(1..3 as a)\")\nprint(re1)\nprint(re2)\n\n# Transfer with 'pickleTableToList' = True\nre1 = s.run(\"m=matrix(1 2, 3 4, 5 6);m.rename!(1 2, \\`a\\`b\\`x);m\", pickleTableToList=True)\nre2 = s.run(\"table(1..3 as a)\", pickleTableToList=True)\nprint(re1)\nprint(re2)\n```\n\n----------------------------------------\n\nTITLE: Selecting Last N Records per Group using LIMIT and CONTEXT BY in Python\nDESCRIPTION: Loads a table from a DolphinDB database path. It selects all columns (`*`), groups the data by 'ticker' using `contextby()`, and then applies `limit(-2)` to retrieve the last 2 records from each 'ticker' group. The final result is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_114\n\nLANGUAGE: python\nCODE:\n```\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").contextby('ticker').limit(-2)\n```\n\n----------------------------------------\n\nTITLE: Deleting Columns from a DolphinDB Table in Python\nDESCRIPTION: Loads data from `example.csv` into a DolphinDB table object `trade` using `s.loadText()`. It then removes the 'ask' and 'bid' columns using the `drop()` method and prints the first 5 rows of the resulting table as a Pandas DataFrame. Requires an active DolphinDB session `s`, the `dolphindb` library, and the `example.csv` file accessible via `WORK_DIR`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\nt1=trade.drop(['ask', 'bid'])\nprint(t1.top(5).toDF())\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Loading Data for Asof Join in DolphinDB\nDESCRIPTION: This snippet prepares the DolphinDB environment for asof join examples. It defines a working directory, drops any existing database named 'tickDB', creates a new VALUE partitioned database named 'tickDB', and loads trade data from 'trades.csv' into a partitioned table 'trades' using `loadTextEx`. Finally, it prints the top 5 rows of the loaded 'trades' table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_87\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb.settings as keys\n\nWORK_DIR = \"C:/DolphinDB/Data\"\nif s.existsDatabase(WORK_DIR+\"/tickDB\"):\n    s.dropDatabase(WORK_DIR+\"/tickDB\")\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AAPL\",\"FB\"], dbPath=WORK_DIR+\"/tickDB\")\ntrades = s.loadTextEx(\"mydb\",  tableName='trades',partitionColumns=[\"Symbol\"], remoteFilePath=WORK_DIR + \"/trades.csv\")\nquotes = s.loadTextEx(\"mydb\",  tableName='quotes',partitionColumns=[\"Symbol\"], remoteFilePath=WORK_DIR + \"/quotes.csv\")\n\nprint(trades.top(5).toDF())\n```\n\n----------------------------------------\n\nTITLE: Uploading Pandas DataFrame with NumPy Array Vectors to DolphinDB\nDESCRIPTION: Connects to a DolphinDB server, creates a Pandas DataFrame containing NumPy arrays (representing DolphinDB array vectors), uploads it to the server using `session.table()`, assigns an alias 'testArrayVector', loads the table back from the server, and prints it as a DataFrame. Requires `numpy`, `pandas`, and `dolphindb` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\ndf = pd.DataFrame({\n                'value': [np.array([1,2,3,4,5,6,7,8,9],dtype=np.int64),np.array([11,12,13,14],dtype=np.int64),np.array([22,13,11,12,13,14],dtype=np.int64)]\n        })\ntmp = s.table(data=df, tableAliasName=\"testArrayVector\")\n\nprint(s.loadTable(\"testArrayVector\").toDF())\n\n# output\n#                          value\n# 0  [1, 2, 3, 4, 5, 6, 7, 8, 9]\n# 1             [11, 12, 13, 14]\n# 2     [22, 13, 11, 12, 13, 14]\n```\n\n----------------------------------------\n\nTITLE: Uploading pandas DataFrame with Multiple Data Types using DolphinDB Python API - Python\nDESCRIPTION: Illustrates how to upload a pandas DataFrame with multiple column types (int, float, string, datetime, bool) to DolphinDB using the table method and retrieve it as a DataFrame. Requires pandas, numpy, a session (s), and the createDemoDataFrame function. Ensures comprehensive data type mapping for compatibility.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndef createDemoDataFrame():\n    data = {'cid': np.array([1, 2, 3], dtype=np.int32),\n            'cbool': np.array([True, False, np.nan], dtype=np.bool),\n            'cchar': np.array([1, 2, 3], dtype=np.int8),\n            'cshort': np.array([1, 2, 3], dtype=np.int16),\n            'cint': np.array([1, 2, 3], dtype=np.int32),\n            'clong': np.array([0, 1, 2], dtype=np.int64),\n            'cdate': np.array(['2019-02-04', '2019-02-05', ''], dtype='datetime64[D]'),\n            'cmonth': np.array(['2019-01', '2019-02', ''], dtype='datetime64[M]'),\n            'ctime': np.array(['2019-01-01 15:00:00.706', '2019-01-01 15:30:00.706', ''], dtype='datetime64[ms]'),\n            'cminute': np.array(['2019-01-01 15:25', '2019-01-01 15:30', ''], dtype='datetime64[m]'),\n            'csecond': np.array(['2019-01-01 15:00:30', '2019-01-01 15:30:33', ''], dtype='datetime64[s]'),\n            'cdatetime': np.array(['2019-01-01 15:00:30', '2019-01-02 15:30:33', ''], dtype='datetime64[s]'),\n            'ctimestamp': np.array(['2019-01-01 15:00:00.706', '2019-01-01 15:30:00.706', ''], dtype='datetime64[ms]'),\n            'cnanotime': np.array(['2019-01-01 15:00:00.80706', '2019-01-01 15:30:00.80706', ''], dtype='datetime64[ns]'),\n            'cnanotimestamp': np.array(['2019-01-01 15:00:00.80706', '2019-01-01 15:30:00.80706', ''], dtype='datetime64[ns]'),\n            'cfloat': np.array([2.1, 2.658956, np.NaN], dtype=np.float32),\n            'cdouble': np.array([0., 47.456213, np.NaN], dtype=np.float64),\n            'csymbol': np.array(['A', 'B', '']),\n            'cstring': np.array(['abc', 'def', ''])}\n    return pd.DataFrame(data)\n\n# save the table to DolphinDB server as table \"testDataFrame\"\ndt = s.table(data=createDemoDataFrame(), tableAliasName=\"testDataFrame\")\n\n# load table \"testDataFrame\" on DolphinDB server \nprint(s.loadTable(\"testDataFrame\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated SQL for a DolphinDB Select Query (Python)\nDESCRIPTION: After defining a `select` operation on the `trade` table object, this snippet calls `showSQL()` to print the corresponding DolphinDB SQL statement that would be executed on the server. Requires a `trade` table object loaded previously (e.g., via `s.loadText`).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nprint(trade.select(['ticker','date','bid','ask','prc','vol']).showSQL())\n```\n\n----------------------------------------\n\nTITLE: Example Using MultithreadedTableWriter and Insert in Python\nDESCRIPTION: Demonstrates connecting to a DolphinDB server, creating a shared in-memory table, initializing `MultithreadedTableWriter` with specific configurations (e.g., 5 threads, batch size 10), inserting records using the `insert` method in a loop, waiting for all threads to finish processing, and finally printing the writer's status.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport time\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"t=table(1000:0, `date`ticker`price, [DATE,SYMBOL,LONG])\nshare t as tglobal\"\"\"\ns.run(script)\n\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"\",\"tglobal\",False,False,[],10,1,5,\"date\")\nfor i in range(10):\n  res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\", random.randint(1,10000))\nwriter.waitForThreadCompletion()\nprint(writer.getStatus())\n```\n\n----------------------------------------\n\nTITLE: Enabling Chunk Granularity Configuration in DolphinDB Python Session\nDESCRIPTION: Illustrates enabling the enableChunkGranularityConfig parameter during session creation to allow configuring the chunkGranularity parameter when creating databases via session.database(). This feature requires the DolphinDB server to have enableChunkGranularityConfig set to true in its configuration file. When enabled, chunkGranularity settings such as 'DATABASE' take effect; otherwise, they are ignored.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n\n# 设置参数enableChunkGranularityConfig为True，即允许配置session.database()中的chunkGranularity参数\ns = ddb.session(\"localhost\", 8848, \"admin\", \"123456\", enableChunkGranularityConfig=True)\n\n# 以下部分仅为展示参数chunkGranularity已生效\nif s.existsDatabase(\"dfs://testdb\"):\n    s.dropDatabase(\"dfs://testdb\")\ndb = s.database(\"db\", partitionType=keys.VALUE, partitions=[1, 2, 3], dbPath=\"dfs://testdb\", chunkGranularity=\"DATABASE\")\nprint(s.run(\"schema(db)\")[\"chunkGranularity\"])\n\n# 输出结果为 DATABASE\n```\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n\n# 设置参数enableChunkGranularityConfig为False，即不允许配置session.database()中的chunkGranularity参数\ns = ddb.session(\"localhost\", 8848, \"admin\", \"123456\", enableChunkGranularityConfig=False)\n\n# 以下部分仅为展示参数chunkGranularity已失效\nif s.existsDatabase(\"dfs://testdb\"):\n    s.dropDatabase(\"dfs://testdb\")\ndb = s.database(\"db\", partitionType=keys.VALUE, partitions=[1, 2, 3], dbPath=\"dfs://testdb\", chunkGranularity=\"TABLE\")\nprint(s.run(\"schema(db)\")[\"chunkGranularity\"])\n\n# 输出结果为 TABLE\n```\n\n----------------------------------------\n\nTITLE: Performing Window Join with DolphinDB Python API\nDESCRIPTION: This snippet demonstrates the `merge_window` method (window join). It joins the 'trades' table with the 'quotes' table based on 'Symbol' and 'Time'. For each trade, it finds quotes within a time window from 5 seconds before (leftBound=-5000000000 nanoseconds) to the exact trade time (rightBound=0). It calculates the average bid and offer prices within this window using `aggFunctions`. The results are filtered by time and the top 10 rows are printed.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_91\n\nLANGUAGE: Python\nCODE:\n```\nprint(trades.merge_window(quotes, -5000000000, 0, aggFunctions=[\"avg(Bid_Price)\",\"avg(Offer_Price)\"], on=[\"Symbol\",\"Time\"]).where(\"Time>=07:59:59\").top(10).toDF())\n```\n\n----------------------------------------\n\nTITLE: Inserting Data to DolphinDB Stream Table via Python (Upload & Run)\nDESCRIPTION: Shows how to read data from a CSV into a pandas DataFrame in Python, connect to DolphinDB, upload the DataFrame, and execute a DolphinDB script (`s.run`) to transform the data (specifically casting `Datetime` to DolphinDB's DATETIME) and insert it into the `Trade` stream table. Requires Dolphindb Python API, pandas, numpy, and a running DolphinDB server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_105\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\ncsv_file = \"trades.csv\"\ncsv_data = pd.read_csv(csv_file,parse_dates=['Datetime'], dtype={'Symbol':str})\ncsv_df = pd.DataFrame(csv_data)\ns = ddb.session();\ns.connect(\"192.168.1.103\", 8921,\"admin\",\"123456\")\n\n# Upload DataFrame to DolphinDB and convert the data type of column 'Datetime'\n\ns.upload({\"tmpData\":csv_df})\ns.run(\"data = select Symbol, datetime(Datetime) as Datetime, Price, Volume from tmpData;tableInsert(Trade,data)\")\n```\n\n----------------------------------------\n\nTITLE: Uploading np.datetime64 and Using as DolphinDB Nanotime - Python\nDESCRIPTION: Shows how to upload a np.datetime64 from Python to DolphinDB and convert it into a DolphinDB NANOTIME type. Requires numpy and a session (s). The original date is discarded; the uploaded value is used and its type is verified. When downloaded, the result is displayed as a numpy.datetime64 with an epoch date.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nts = np.datetime64('2019-01-01T20:01:01.1223461')\ns.upload({'ts':ts})\ns.run('a=nanotime(ts)')\n\ns.run('typestr(a)')\n# output\n'NANOTIME'\n\ns.run('a')\n# output\nnumpy.datetime64('1970-01-01T20:01:01.122346100')\n```\n\n----------------------------------------\n\nTITLE: Saving DolphinDB Query Results as a New Table using executeAs in Python\nDESCRIPTION: This snippet loads a DolphinDB table, filters and sorts the data using select, where, and sort clauses, and saves the result as a new table named 'AMZN' in DolphinDB using the executeAs method. It then retrieves and prints the content of the newly created table using run.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_93\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\n\nt = trade.select(['date','bid','ask','prc','vol']).where('TICKER=`AMZN').where('bid!=NULL').where('ask!=NULL').where('vol>10000000').sort('vol desc').executeAs(\"AMZN\")\n\nprint(s.run('AMZN'))\n```\n\n----------------------------------------\n\nTITLE: Getting Top N Records from a DolphinDB Table using `top()` (Python)\nDESCRIPTION: Loads data from `example.csv` into `trade`. It retrieves the first 5 rows using `trade.top(5)` and displays them as a Pandas DataFrame. Requires an active DolphinDB session `s`, the `dolphindb` library, `WORK_DIR` variable, and `example.csv`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\ntrade.top(5).toDF()\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Display for DolphinDB Python Session\nDESCRIPTION: Shows usage of the show_output parameter to control whether print statements within DolphinDB scripts executed via session.run() are displayed on the client side. When set to True, print outputs are shown, otherwise, they are suppressed. This controls script output verbosity during session operations.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# 启用 show_output\ns = ddb.session(show_output=True)\ns.connect(\"localhost\", 8848)\ns.run(\"print(1);2\")\n\n# 输出结果\n1\n2\n\n# 不启用 show_output\ns = ddb.session(show_output=False)\ns.connect(\"localhost\", 8848)\ns.run(\"print(1);2\")\n\n# 输出结果\n2\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB MultithreadedTableWriter in Python\nDESCRIPTION: Creates an instance of the `MultithreadedTableWriter` class from the `dolphindb` Python library. It connects to a specified DolphinDB server instance ('localhost:8848') with credentials ('admin'/'123456'), targets a specific database path ('dfs://valuedb3') and table ('pdatetest'), and configures writer parameters including batch size, throttle rate, thread count, partition column, and column compression methods. Requires the `dolphindb` library installed.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\nwriter = ddb.MultithreadedTableWriter(\n    \"localhost\", 8848, \"admin\", \"123456\", dbPath=\"dfs://valuedb3\", tableName=\"pdatetest\",\n    batchSize=10000, throttle=1, threadCount=5, partitionCol=\"id\", compressMethods=[\"LZ4\",\"LZ4\",\"DELTA\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data Row by Row via MTW (Python)\nDESCRIPTION: Demonstrates inserting 100 individual rows into the configured partitioned table using the `writer.insert()` method within a loop. Includes error handling with a `try...except` block and waits for all background threads to complete using `writer.waitForThreadCompletion()` before checking and printing the final write status. Requires an initialized `MultithreadedTableWriter`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    # insert 100 rows of data \n    for i in range(100):\n        res = writer.insert(random.randint(1,10000),\"AAAAAAAB\", random.randint(1,10000))\nexcept Exception as ex:\n    print(\"MTW exit with exception %s\" % ex)\nwriter.waitForThreadCompletion()\nwriteStatus=writer.getStatus()\nif writeStatus.succeed():\n    print(\"Write successfully!\")\nprint(\"writeStatus: \\n\", writeStatus)\nprint(s.run(\"select count(*) from pt\"))\n```\n\n----------------------------------------\n\nTITLE: Creating a DFS Table in DolphinDB with Python API\nDESCRIPTION: Creates a DFS table in DolphinDB with various data types by connecting to a DolphinDB server and executing a script that sets up a partitioned table structure.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ndbPath=\"dfs://testPython\"\ntableName='t1'\nscript = \"\"\"\ndbPath='{db}'\nif(existsDatabase(dbPath))\n\tdropDatabase(dbPath)\ndb = database(dbPath, VALUE, 0..100)\nt1 = table(10000:0,`id`cbool`cchar`cshort`cint`clong`cdate`cmonth`ctime`cminute`csecond`cdatetime`ctimestamp`cnanotime`cnanotimestamp`cfloat`cdouble`csymbol`cstring,[INT,BOOL,CHAR,SHORT,INT,LONG,DATE,MONTH,TIME,MINUTE,SECOND,DATETIME,TIMESTAMP,NANOTIME,NANOTIMESTAMP,FLOAT,DOUBLE,SYMBOL,STRING])\ninsert into t1 values (0,true,'a',122h,21,22l,2012.06.12,2012.06M,13:10:10.008,13:30m,13:30:10,2012.06.13 13:30:10,2012.06.13 13:30:10.008,13:30:10.008007006,2012.06.13 13:30:10.008007006,2.1f,2.1,'','')\nt = db.createPartitionedTable(t1, `{tb}, `id)\nt.append!(t1)\"\"\".format(db=dbPath,tb=tableName)\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Outer Join between In-Memory Tables using DolphinDB Python API\nDESCRIPTION: This snippet demonstrates an outer join using the `merge` method with `how=\"outer\"`. It joins two in-memory DolphinDB tables (`t1`, `t2`), created from Python dictionaries, based on common columns specified in the `on` parameter. The resulting joined table, including unmatched rows from both tables, is converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_86\n\nLANGUAGE: Python\nCODE:\n```\nt1 = s.table(data={'TICKER': ['AMZN', 'AMZN', 'NFLX'], 'date': ['2015.12.29', '2015.12.30', '2015.12.31'], 'open': [674, 685, 942]})\nt2 = s.table(data={'TICKER': ['AMZN', 'NFLX', 'NFLX'], 'date': ['2015.12.29', '2015.12.30', '2015.12.31'], 'close': [690, 936, 951]})\nprint(t1.merge(t2, how=\"outer\", on=[\"TICKER\",\"date\"]).toDF())\n```\n\n----------------------------------------\n\nTITLE: Pivoting Table Values using pivotby (Table Output) - DolphinDB Python\nDESCRIPTION: Retrieves the 'trade' table, selects the 'VOL' column, and pivots data on ['TICKER', 'date'], creating a new DataFrame with 'TICKER' as rows and dates as columns. Useful for converting long-format time series to wide format. Requires DolphinDB Python API and DataFrame support.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_129\n\nLANGUAGE: Python\nCODE:\n```\ndf = s.loadTable(tableName=\"trade\", dbPath=\"dfs://valuedb\")\nprint(df.select(\"VOL\").pivotby(\"TICKER\", \"date\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Using insert method with error handling in MultithreadedTableWriter\nDESCRIPTION: Demonstrates how to insert multiple rows of data using MTW and handle errors. This example creates a shared table, inserts 10 rows with error checking, and then waits for thread completion.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"\n    t = table(1000:0, `date`ticker`price, [DATE,SYMBOL,LONG]);\n    share t as tglobal;\n\"\"\"\ns.run(script)\n\nwriter = ddb.MultithreadedTableWriter(\n    \"localhost\", 8848, \"admin\", \"123456\", dbPath=\"\", tableName=\"tglobal\", \n    batchSize=10, throttle=1, threadCount=5, partitionCol=\"date\"\n)\n\nfor i in range(10):\n    if i == 3:\n        res = writer.insert(np.datetime64(f'2022-03-2{i%6}'), random.randint(1,10000))\n    else:\n        res = writer.insert(np.datetime64(f'2022-03-2{i%6}'), \"AAAA\", random.randint(1,10000))\n    if res.hasError():\n        print(\"insert error: \", res.errorInfo)\n\nwriter.waitForThreadCompletion()\nprint(s.run(\"\"\"select count(*) from tglobal\"\"\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing DBConnectionPool and PartitionedTableAppender (Python)\nDESCRIPTION: This snippet shows how to set up a `DBConnectionPool` and a `PartitionedTableAppender` in Python for concurrent data appending. It establishes a connection pool with specified parameters like the host, port, number of connections, user, and password.  Then, the `PartitionedTableAppender` is instantiated, taking the database path, table name, partition column name, and the established connection pool as parameters. This configuration enables parallel data writing to a partitioned table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.3_PartitionedTableAppender.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npool = ddb.DBConnectionPool(\"localhost\", 8848, 3, \"admin\", \"123456\")\nappender = ddb.PartitionedTableAppender(dbPath=\"dfs://valuedb\", tableName=\"pt\", partitionColName=\"id\", dbConnectionPool=pool)\n```\n\n----------------------------------------\n\nTITLE: Pivoting with exec for Matrix Output using pivotby - DolphinDB Python\nDESCRIPTION: Uses 'exec' to aggregate 'VOL' and 'pivotby' to structure data as a matrix (NumPy array) with axes for ticker and date. Returns a tuple with the value matrix, row labels, and column labels, suitable for numerical computations. Requires DolphinDB Python API and NumPy.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_130\n\nLANGUAGE: Python\nCODE:\n```\ndf = s.loadTable(tableName=\"trade\", dbPath=\"dfs://valuedb\")\nprint(df.exec(\"VOL\").pivotby(\"TICKER\", \"date\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Running Functions with np.int using DolphinDB Python API - Python\nDESCRIPTION: Demonstrates using a NumPy integer (np.int) as an argument to a DolphinDB script executed via the Python API. Requires numpy and a DolphinDB Python API session (s). Accepts a NumPy int, passes it to a remote add function, and returns the result as a Python object.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\ns.run(\"add{1,}\",np.int(4))\n# output\n5\n```\n\n----------------------------------------\n\nTITLE: Getting current subscription topics from the DolphinDB session in Python\nDESCRIPTION: Retrieves all current active subscription topics from the session using getSubscriptionTopics, returning a list of strings formatted as \"host/port/tableName/actionName\". Useful for monitoring or managing existing subscriptions in the client session. Requires an active DolphinDB session instance. No input parameters are needed, and the output is a list of topic strings.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.4_Subscription/2.4_Subscription.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns.getSubscriptionTopics()\n```\n\n----------------------------------------\n\nTITLE: Using Function Views for Asynchronous Temporal Data Conversion\nDESCRIPTION: Demonstrates using server-side function views to safely handle temporal data type conversion when appending data asynchronously, avoiding race conditions between data upload and conversion.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport pandas as pd\nimport random\nimport datetime\n\ns = ddb.session(enableASYNC=True)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nn = 100\n\n# Randomly generate a DataFrame\nsym_list = ['IBN', 'GTYU', 'FHU', 'DGT', 'FHU', 'YUG', 'EE', 'ZD', 'FYU']\nprice_list = []\ntime_list = []\nfor i in range(n):\n    price_list.append(round(np.random.uniform(1, 100), 1))\n    time_list.append(np.datetime64(datetime.date(2020, random.randint(1, 12), random.randint(1, 20))))\n\ntb = pd.DataFrame({'time': time_list,\n                   'sym': np.random.choice(sym_list, n),\n                   'price': price_list,\n                   'id': np.random.choice([1, 2, 3, 4, 5], n)})\n\ns.upload({'tb': tb})\ns.run(\"appendStreamingData(tb)\")\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Pivotby with Select Example\nDESCRIPTION: This snippet showcases the use of `pivotby` to reshape data. The 'trade' table is loaded, and the 'VOL' column is pivoted based on 'TICKER' and 'date'. The resulting pivoted table is converted to a Pandas DataFrame for display.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_81\n\nLANGUAGE: python\nCODE:\n```\ndf = s.loadTable(tableName=\"trade\", dbPath=\"dfs://valuedb\")\nprint(df.select(\"VOL\").pivotby(\"TICKER\", \"date\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Importing Data to DolphinDB In-Memory Partitioned Table using `loadTextEx` in Python\nDESCRIPTION: Creates an in-memory VALUE partitioned database `mydb` by setting the `dbPath` argument to an empty string (`\"\"`) in the `s.database()` call. It then uses `s.loadTextEx()` to load data from `example.csv` into the partitioned table `trade`, specifying `TICKER` as the partition column. Finally, it converts the resulting in-memory table to a pandas DataFrame. Requires `dolphindb` and `pandas` libraries.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=[\"AMZN\",\"NFLX\",\"NVDA\"], dbPath=\"\")\n\ntrade=s.loadTextEx(dbPath=\"mydb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\ntrade.toDF()\n```\n\n----------------------------------------\n\nTITLE: Using skipAll Method with Batch Reading in Python\nDESCRIPTION: Shows how to use the skipAll method to discard remaining data when batch reading is not completed. This is important to prevent socket buffer issues that can cause deserialization failures.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nscript='''\n    rows=100000;\n    testblock=table(take(1,rows) as id,take(`A,rows) as symbol,take(2020.08.01..2020.10.01,rows) as date, rand(50,rows) as size,rand(50.5,rows) as price);\n'''\ns.run(script)\n\nscript1='''\nselect * from testblock\n'''\nblock= s.run(script1, fetchSize = 8192)\nre = block.read()\nblock.skipAll()\ns.run(\"1 + 1\") // 若没有调用 skipAll，执行此代码会抛出异常。\n```\n\n----------------------------------------\n\nTITLE: Performing Asof Join with DolphinDB Python API\nDESCRIPTION: This snippet demonstrates the `merge_asof` method for performing an as-of join between the 'trades' and 'quotes' tables. The join is performed based on the 'Symbol' and 'Time' columns, aligning each trade with the most recent quote at or before the trade time. Specific columns are selected from the result, and the top 5 rows are printed as a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_89\n\nLANGUAGE: Python\nCODE:\n```\nprint(trades.merge_asof(quotes,on=[\"Symbol\",\"Time\"]).select([\"Symbol\",\"Time\",\"Trade_Volume\",\"Trade_Price\",\"Bid_Price\", \"Bid_Size\",\"Offer_Price\", \"Offer_Size\"]).top(5).toDF())\n```\n\n----------------------------------------\n\nTITLE: Enabling Compression in DolphinDB Python Session for Large Data Transfers\nDESCRIPTION: Presents how to enable data compression to reduce network bandwidth usage during large data transfers by setting compress=True. It requires attention to API version compatibility: for API versions >=1.30.21.1, the protocol parameter should be set to PROTOCOL_DDB; for earlier versions, enablePickle must be set to False. Compression increases computational load on both client and server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\n# api version >= 1.30.21.1，开启压缩\ns = ddb.session(compress=True, protocol=keys.PROTOCOL_DDB)\n\n# api version <= 1.30.19.4，开启压缩\ns = ddb.session(compress=True, enablePickle=False)\n```\n\n----------------------------------------\n\nTITLE: Inserting Single Record with MultithreadedTableWriter in Python\nDESCRIPTION: Inserts a single record into the buffer queue managed by the writer. The data will be asynchronously sent to the server based on the configured `batchSize` and `throttle`. Returns an `ErrorCodeInfo` object to indicate success or failure for this specific insertion.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\ninsert(*args)\n```\n\n----------------------------------------\n\nTITLE: Using DolphinDB `exec` Clause via Python API\nDESCRIPTION: Demonstrates using the `exec` clause through the Python API. The first example retrieves a single column ('ticker') as a DolphinDB vector (converted to a NumPy array/list by `toDF()`). The second example retrieves multiple columns, resulting in a DolphinDB table (converted to a Pandas DataFrame by `toDF()`). Both examples require loading data into the 'trade' table first. Requires `s`, `dolphindb`, `WORK_DIR`, and `example.csv`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_65\n\nLANGUAGE: sql\nCODE:\n```\n# Single column exec (SQL executed via Python)\ntrade = s.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\nprint(trade.exec('ticker').toDF())\n```\n\nLANGUAGE: sql\nCODE:\n```\n# Multiple column exec (SQL executed via Python)\ntrade = s.loadTextEx(dbPath=\"dfs://valuedb\", partitionColumns=[\"TICKER\"], tableName='trade', remoteFilePath=WORK_DIR + \"/example.csv\")\nprint(trade.exec(['ticker','date','bid','ask','prc','vol']).toDF())\n```\n\n----------------------------------------\n\nTITLE: Calculating Trading Cost using Asof Join in DolphinDB\nDESCRIPTION: This snippet calculates the trading cost per symbol using the result of an asof join. It performs `merge_asof` between 'trades' and 'quotes', then uses the `select` method with an aggregate expression to compute the cost (volume-weighted absolute difference between trade price and mid-quote price, scaled by total value). The result is grouped by 'Symbol' and printed as a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_90\n\nLANGUAGE: Python\nCODE:\n```\nprint(trades.merge_asof(quotes, on=[\"Symbol\",\"Time\"]).select(\"sum(Trade_Volume*abs(Trade_Price-(Bid_Price+Offer_Price)/2))/sum(Trade_Volume*Trade_Price)*10000 as cost\").groupby(\"Symbol\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Filtering Grouped Results using `groupby()` and `having()` (Python)\nDESCRIPTION: Loads the 'trade' table from 'dfs://valuedb'. Calculates the count of 'ask' for each 'vol' group using `select('count(ask)').groupby(['vol'])`. It then filters these groups, keeping only those where the count of 'ask' is greater than 1, using `having('count(ask)>1')`, and displays the result as a Pandas DataFrame. Requires `s`, `dolphindb`, and the 'trade' table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\nprint(trade.select('count(ask)').groupby(['vol']).having('count(ask)>1').toDF())\n```\n\n----------------------------------------\n\nTITLE: Inserting Single Row Data with MultithreadedTableWriter in Python\nDESCRIPTION: The insert method allows adding a single row of data to the writer queue. It returns an ErrorCodeInfo object with error details if the insertion fails. The method includes error checking functionality via hasError() and succeed() methods.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_78\n\nLANGUAGE: Python\nCODE:\n```\ninsert(*args)\n```\n\n----------------------------------------\n\nTITLE: Calculating Trade Cost Using Window Join with Prevalent Option in DolphinDB Python API\nDESCRIPTION: This example calculates trade cost metrics by performing a window join with weighted averages (wavg) of offer and bid prices, enabling the prevailing option to include nearest previous values when the exact window boundary time is missing in the right table. The cost is computed similarly to the asof join example but uses window join's enhanced capabilities for more accurate aggregation. The output is grouped by 'Symbol', demonstrating grouped cost statistics.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_138\n\nLANGUAGE: python\nCODE:\n```\ntb = trades.merge_window(quotes,-1000000000, 0, aggFunctions=\"[wavg(Offer_Price, Offer_Size) as Offer_Price, wavg(Bid_Price, Bid_Size) as Bid_Price]\",\n                         on=[\"Symbol\",\"Time\"], prevailing=True)\\\n                         .select(\"sum(Trade_Volume*abs(Trade_Price-(Bid_Price+Offer_Price)/2))/sum(Trade_Volume*Trade_Price)*10000 as cost\")\\\n                         .groupby(\"Symbol\")\nprint(tb.toDF())\n```\n\n----------------------------------------\n\nTITLE: Uploading Dictionary as Table using DolphinDB Python API - Python\nDESCRIPTION: Shows how to create a dictionary of lists and arrays, upload it using the table method to define a named DolphinDB table, and retrieve it as a pandas DataFrame (toDF) using loadTable. Requires numpy, a createDemoDict function, and an active session (s). Suitable for batch uploads.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef createDemoDict():\n    return {'id': [1, 2, 2, 3],\n            'date': np.array(['2021.05.06', '2021.05.07', '2021.05.06', '2021.05.07'], dtype='datetime64[D]'),\n            'ticker': ['AAPL', 'AAPL', 'AMZN', 'AMZN'],\n            'price': [129.74, 130.21, 3306.37, 3291.61]}\n\n# save the table to DolphinDB server as table \"testDict\"\ndt = s.table(data=createDemoDict(), tableAliasName=\"testDict\")\n\n# load table \"testDict\" on DolphinDB server \nprint(s.loadTable(\"testDict\").toDF())\n\n# output\n        date ticker    price\n0 2021-05-06   AAPL   129.74\n1 2021-05-07   AAPL   130.21\n2 2021-05-06   AMZN  3306.37\n3 2021-05-07   AMZN  3291.61\n```\n\n----------------------------------------\n\nTITLE: Transferring data using PROTOCOL_DDB example in Python\nDESCRIPTION: Sample code showing data transfer using the DolphinDB native protocol PROTOCOL_DDB, connecting to a server, running a simple query to retrieve a table, and storing the result as a pandas DataFrame. This illustrates use of the native protocol for data transfer, ensuring compatibility with DolphinDB API variants.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nre = s.run(\"table(1..10 as a)\")   # pandas.DataFrame\n```\n\n----------------------------------------\n\nTITLE: Deserializing DolphinDB Table Data as List of numpy ndarrays Using Python API with pickleTableToList=True\nDESCRIPTION: Explains the alternative deserialization mode triggered by setting the parameter pickleTableToList=True in the API. In this mode, DolphinDB Table data is returned as a Python list, where each element is a numpy ndarray representing a column in the Table. Time types are converted to datetime64[ns], and array vector columns must have consistent lengths for all elements, resulting in 2D ndarrays. The code snippets demonstrate creating Tables and retrieving the list form, checking the data types of each column array. This feature applies only to the PROTOCOL_DDB communication and disables automatic concatenation into pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> re = s.run(\"table([1, NULL] as a, [2012.01.02, NULL] as b)\", pickleTableToList=True)\n>>> re\n[array([ 1., nan]), array(['2012-01-02T00:00:00.000000000',                           'NaT'],\n      dtype='datetime64[ns]')]\n>>> type(re)\n<class 'list'>\n>>> re[0].dtype\nfloat64\n>>> re[1].dtype\ndatetime64[ns]\n\n>>> s.run(\"table(arrayVector(1 2 3, [1, 2, 3]) as a)\", pickleTableToList=True)\n[array([[1],\n       [2],\n       [3]], dtype=int32)]\n```\n\n----------------------------------------\n\nTITLE: Clearing Server-Side Cache with clearAllCache\nDESCRIPTION: The `clearAllCache` method clears the server-side cache. When the dfs parameter is set to True, it clears the cache on all nodes. Otherwise, it clears the cache only on the connected node.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.3_OtherParams.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> s.clearAllCache()\n>>> s.clearAllCache(dfs=True)\n```\n\n----------------------------------------\n\nTITLE: Creating & Populating DolphinDB Heterogeneous Stream Table\nDESCRIPTION: Sets up a DolphinDB environment with source tables (partitioned, stream, in-memory) and a target heterogeneous stream table. It inserts data into the source tables and then uses `replay` to simulate streaming this data to the target table `outTables`. Dependencies include a running DolphinDB server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_102\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry{dropStreamTable(`outTables)}catch(ex){}\n// Create a heterogeneous table\nshare streamTable(100:0, `timestampv`sym`blob`price1,[TIMESTAMP,SYMBOL,BLOB,DOUBLE]) as outTables\nn = 10;\ndbName = 'dfs://test_StreamDeserializer_pair'\nif(existsDatabase(dbName)){\n    dropDB(dbName)}\n// Create database and tables\ndb = database(dbName,RANGE,2012.01.01 2013.01.01 2014.01.01 2015.01.01 2016.01.01 2017.01.01 2018.01.01 2019.01.01)\ntable1 = table(100:0, `datetimev`timestampv`sym`price1`price2, [DATETIME, TIMESTAMP, SYMBOL, DOUBLE, DOUBLE])\ntable2 = table(100:0, `datetimev`timestampv`sym`price1, [DATETIME, TIMESTAMP, SYMBOL, DOUBLE])\ntable3 = table(100:0, `datetimev`timestampv`sym`price1, [DATETIME, TIMESTAMP, SYMBOL, DOUBLE])\ntableInsert(table1, 2012.01.01T01:21:23 + 1..n, 2018.12.01T01:21:23.000 + 1..n, take(`a`b`c,n), rand(100,n)+rand(1.0, n), rand(100,n)+rand(1.0, n))\ntableInsert(table2, 2012.01.01T01:21:23 + 1..n, 2018.12.01T01:21:23.000 + 1..n, take(`a`b`c,n), rand(100,n)+rand(1.0, n))\ntableInsert(table3, 2012.01.01T01:21:23 + 1..n, 2018.12.01T01:21:23.000 + 1..n, take(`a`b`c,n), rand(100,n)+rand(1.0, n))\n// Create three types of tables (partitioned table, stream table and in-memory table) \npt1 = db.createPartitionedTable(table1,'pt1',`datetimev).append!(table1)\nshare streamTable(100:0, `datetimev`timestampv`sym`price1, [DATETIME, TIMESTAMP, SYMBOL, DOUBLE]).append!(table2) as pt2\nshare table3 as pt3\n// Create the heterogeneous stream table\nd = dict(['msg1', 'msg2', 'msg3'], [table1, table2, table3])\nreplay(inputTables=d, outputTables=`outTables, dateColumn=`timestampv, timeColumn=`timestampv)\n```\n\n----------------------------------------\n\nTITLE: Example of Integer Division in DolphinDB SQL Select Expression with Python Escape Characters\nDESCRIPTION: This snippet highlights the syntax for performing integer division in DolphinDB within Python's select string, emphasizing the need to escape the backslash character due to Python string parsing rules. The code performs select and ols regression on a distributed table by defining aliases for computed columns using escaped division operator VOL\\\\SHROUT and other transformations.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_141\n\nLANGUAGE: python\nCODE:\n```\nresult = s.loadTable(tableName=\"US\",dbPath=\"dfs://US\").select(\"select VOL\\\\SHROUT as turnover, abs(RET) as absRet, (ASK-BID)/(BID+ASK)*2 as spread, log(SHROUT*(BID+ASK)/2) as logMV\").where(\"VOL>0\").ols(\"turnover\", [\"absRet\",\"logMV\", \"spread\"], True)\n```\n\n----------------------------------------\n\nTITLE: Appending Data Asynchronously to a DFS Table\nDESCRIPTION: Demonstrates appending data asynchronously to a DFS table, which increases data throughput by not waiting for execution results from the server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\nimport dolphindb.settings as keys\nimport pandas as pd\n\ns = ddb.session(enableASYNC=True) # enable asynchronous mode\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\ndbPath = \"dfs://testDB\"\ntableName = \"tb1\"\n\nscript = \"\"\"\ndbPath=\"dfs://testDB\"\n\ntableName=`tb1\n\nif(existsDatabase(dbPath))\n    dropDatabase(dbPath)\ndb=database(dbPath, VALUE, [\"AAPL\", \"AMZN\", \"A\"])\n\ntestDictSchema=table(5:0, `id`ticker`price, [INT,STRING,DOUBLE])\n\ntb1=db.createPartitionedTable(testDictSchema, tableName, `ticker)\n\"\"\"\ns.run(script) # The script above can be executed on the server\n\ntb = pd.DataFrame({'id': [1, 2, 2, 3],\n                   'ticker': ['AAPL', 'AMZN', 'AMZN', 'A'],\n                   'price': [22, 3.5, 21, 26]})\n\ns.run(\"append!{{loadTable('{db}', `{tb})}}\".format(db=dbPath, tb=tableName), tb)\n```\n\n----------------------------------------\n\nTITLE: Enabling DolphinDB Streaming Subscription in Python\nDESCRIPTION: This snippet shows how to enable streaming subscription in the DolphinDB Python API using the `enableStreaming` method. It includes examples for both older API versions requiring a specific port number (e.g., 8000) and newer versions (1.30.21/2.00.9 or later) where the port parameter is optional or ignored (defaulting to 0).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_97\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport numpy as np\ns = ddb.session()\n# before 1.30.21/2.00.9, port number must be specified\ns.enableStreaming(8000)   \n# 1.30.21/2.00.9 or later, port number is not required\ns.enableStreaming()\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns (String Input) and Filtering with `where` (Python)\nDESCRIPTION: Selects 'ticker', 'date', 'bid', 'ask', 'prc', 'vol' columns from the `trade` table using a string argument in `select()`. It filters the results using two chained `where()` calls (for date and volume) and displays the output as a Pandas DataFrame. Requires a `trade` table object.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nprint(trade.select(\"ticker,date,bid,ask,prc,vol\").where(\"date=2012.09.06\").where(\"vol<10000000\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns from a DolphinDB Table using a List (Python)\nDESCRIPTION: Loads data from `example.csv` into `trade`. It then selects the 'ticker', 'date', 'bid', 'ask', 'prc', and 'vol' columns using `trade.select(['ticker','date','bid','ask','prc','vol'])` and displays the result as a Pandas DataFrame. Requires an active DolphinDB session `s`, the `dolphindb` library, `WORK_DIR` variable, and `example.csv`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\nprint(trade.select(['ticker','date','bid','ask','prc','vol']).toDF())\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB MultithreadedTableWriter (Python)\nDESCRIPTION: Instantiates the `MultithreadedTableWriter` object to connect to a DolphinDB server and write data to a specified partitioned table. Configures connection details, database/table names, batch size (10000), thread count (5), partitioning column (\"id\"), and compression methods. Requires a running DolphinDB server and the target table to exist.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"dfs://valuedb3\",\"pdatetest\",False,False,[],10000,1,5,\"id\",[\"LZ4\",\"LZ4\",\"DELTA\"])\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Contextby with Having Example\nDESCRIPTION: This snippet showcases the use of `contextby` in conjunction with `having` to filter groups based on a condition. It groups the 'trade' table by 'ticker', calculates the sum of 'VOL' for each group, and filters out groups where the sum is not greater than 40000000000. The result is then converted to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_77\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\").contextby('ticker').having(\"sum(VOL)>40000000000\").toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Converting np.datetime64 to DolphinDB Types - Python\nDESCRIPTION: Illustrates how various np.datetime64 objects are mapped to corresponding DolphinDB temporal types using the DolphinDB Python API. Requires numpy and a valid session object (s). Outputs the DolphinDB type as a string for each input.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\ns.run(\"typestr\",np.datetime64('2019-01-01'))\n# output\n'DATE'\n\ns.run(\"typestr\",np.datetime64('2019-01'))\n# output\n'MONTH'\n\ns.run(\"typestr\",np.datetime64('2019-01-01T20:01:01'))\n# output\n'DATETIME'\n\ns.run(\"typestr\",np.datetime64('2019-01-01T20:01:01.122'))\n# output\n'TIMESTAMP'\n\ns.run(\"typestr\",np.datetime64('2019-01-01T20:01:01.1223461'))\n# output\n'NANOTIMESTAMP'\n```\n\n----------------------------------------\n\nTITLE: Bulk Insert With Explicit Date Conversion for Time Columns - DolphinDB Python API (Python)\nDESCRIPTION: Demonstrates modifying the SQL insert statement to call date() on date columns when numpy arrays are of unsupported types, ensuring correct time type mapping on the server. Provides a fix for cases where pandas produces datetime64 arrays, enabling compatibility with DolphinDB's DATE columns. Requires numpy, pandas, dolphindb.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nscript = \"insert into tglobal values(ids,date(dates),tickers,prices);\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB OLS Regression with Escape Character for Ratio Operator in Python\nDESCRIPTION: This snippet highlights the syntax for using the integer ratio operator ('\\') within a select statement for an OLS regression call in the DolphinDB Python API. Due to Python's use of '\\' as an escape character, it must be escaped as '\\\\' when passed as a string to the DolphinDB engine.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_96\n\nLANGUAGE: python\nCODE:\n```\nresult = s.loadTable(tableName=\"US\",dbPath=\"dfs://US\").select(\"select VOL\\\\SHROUT as turnover, abs(RET) as absRet, (ASK-BID)/(BID+ASK)*2 as spread, log(SHROUT*(BID+ASK)/2) as logMV\").where(\"VOL>0\").ols(\"turnover\", [\"absRet\",\"logMV\", \"spread\"], True)\n```\n\n----------------------------------------\n\nTITLE: Using PROTOCOL_ARROW to transfer Table data in Python with DolphinDB\nDESCRIPTION: This code demonstrates setting up a DolphinDB session utilizing the PROTOCOL_ARROW, compatible on Linux x86_64 with PyArrow 9.0.0 or later, for efficient Table data transfer. It executes a query returning a Table object, which is shown as a pyarrow.Table, illustrating interoperation with Apache Arrow format.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\ns = ddb.session(protocol=keys.PROTOCOL_ARROW)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nre = s.run(\"table(1..3 as a)\")\nprint(re)\n-----------------------------\npyarrow.Table\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Csort with Multiple Ascending Parameters Example\nDESCRIPTION: Demonstrates using the `csort` function with a list of boolean values for the `ascending` parameter to control sorting order for multiple columns. It loads a table, groups by 'ticker', and sorts by 'TICKER' (ascending) and 'VOL' (descending), then limits the result to the first 5 rows overall.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_80\n\nLANGUAGE: python\nCODE:\n```\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").contextby('ticker').csort([\"TICKER\", \"VOL\"], [True, False]).limit(5)\n```\n\n----------------------------------------\n\nTITLE: Constructing Stream Deserializer - DolphinDB Python\nDESCRIPTION: This snippet provides an example of constructing a `streamDeserializer` object for handling heterogeneous stream tables in DolphinDB.  It requires a dictionary `sym2table`, where keys are table names, and values are schema definitions.  The `session` parameter is optional and defaults to the current session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_101\n\nLANGUAGE: python\nCODE:\n```\n1sd = streamDeserializer(sym2table, session=None)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Contextby with Select and Sum Example\nDESCRIPTION: This snippet demonstrates the use of `contextby` to group data by 'TICKER' and 'month(date)' and calculates the sum of 'VOL' for each group. It loads the 'trade' table, selects the specified columns, applies the sum function, groups by ticker and month, and converts the result to a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_76\n\nLANGUAGE: python\nCODE:\n```\ndf= s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\").select(\"TICKER, month(date) as month, sum(VOL)\").contextby(\"TICKER,month(date)\").toDF()\nprint(df)\n```\n\n----------------------------------------\n\nTITLE: Filtering with Multiple Conditions as a String in `where()` (Python)\nDESCRIPTION: Loads data from `example.csv`. Selects columns using a string argument in `select()`. It applies multiple filtering conditions (`bid!=NULL`, `ask!=NULL`, `vol>50000000`) passed as a single comma-separated string to the `where()` method and displays the result as a Pandas DataFrame. Requires `s`, `dolphindb`, `WORK_DIR`, and `example.csv`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ntrade=s.loadText(WORK_DIR+\"/example.csv\")\nprint(trade.select(\"ticker, date, vol\").where(\"bid!=NULL, ask!=NULL, vol>50000000\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Value Partitioned DFS Database in Python\nDESCRIPTION: Imports DolphinDB settings constants (`keys`), then uses `s.database()` to create a database named `mydb` located at the DFS path `dfs://valuedb`. The database is configured with VALUE partitioning based on a list of stock tickers (`AMZN`, `NFLX`, `NVDA`). Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb.settings as keys\n\ns.database(dbName='mydb', partitionType=keys.VALUE, partitions=['AMZN','NFLX', 'NVDA'], dbPath='dfs://valuedb')\n# 等效于 s.run(\"db=database('dfs://valuedb', VALUE, ['AMZN','NFLX','NVDA'])\")\n```\n\n----------------------------------------\n\nTITLE: Initializing session and DBConnectionPool with PROTOCOL_DDB - Python\nDESCRIPTION: This snippet demonstrates how to enable the PROTOCOL_DDB protocol when creating a DolphinDB session and a DBConnectionPool in Python. By setting the `protocol` parameter to `keys.PROTOCOL_DDB`, the communication between the Python client and the DolphinDB server will use the specified protocol. It requires the `dolphindb` library to be installed.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\n\ns = ddb.session(protocol=keys.PROTOCOL_DDB)\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\npool = ddb.DBConnectionPool(\"localhost\", 8848, \"admin\", \"123456\", 10, protocol=keys.PROTOCOL_DDB)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Csort with Ascending Parameter Example\nDESCRIPTION: Demonstrates using the `csort` function with the `ascending` parameter to control sorting order. It loads a table, groups by 'ticker', and sorts by 'TICKER' (ascending) and 'VOL' (ascending), then limits the result to the first 5 rows overall.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_79\n\nLANGUAGE: python\nCODE:\n```\ntb = s.loadTable(dbPath=\"dfs://valuedb\", tableName=\"trade\")\nt1 = tb.select(\"*\").contextby('ticker').csort([\"TICKER\", \"VOL\"], True).limit(5)\n```\n\n----------------------------------------\n\nTITLE: Enabling Python Parser Feature for DBConnectionPool in Python\nDESCRIPTION: Demonstrates how to enable the Python parser feature (intended for DolphinDB 3.00) by setting the `python` parameter to `True` during `DBConnectionPool` initialization. This feature, when available, will allow using Python syntax within scripts executed via the pool. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 启用 python parser 特性\npool = ddb.DBConnectionPool(\"localhost\", 8848, 10, python=True)\n```\n\n----------------------------------------\n\nTITLE: Example of MultithreadedTableWriter Usage with Memory Table in Python\nDESCRIPTION: This example demonstrates creating a MultithreadedTableWriter with 5 threads, inserting 10 rows of data into a memory table, waiting for completion, and checking the status of the operation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_79\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport time\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"t=table(1000:0, `date`ticker`price, [DATE,SYMBOL,LONG])\nshare t as tglobal\"\"\"\ns.run(script)\n\nwriter = ddb.MultithreadedTableWriter(\"localhost\", 8848, \"admin\", \"123456\",\"\",\"tglobal\",False,False,[],10,1,5,\"date\")\nfor i in range(10):\n  res = writer.insert(np.datetime64('2022-03-23'),\"AAAAAAAB\", random.randint(1,10000))\nwriter.waitForThreadCompletion()\nprint(writer.getStatus())\n```\n\n----------------------------------------\n\nTITLE: Adding a List of np.datetime64 Objects with DolphinDB Python API - Python\nDESCRIPTION: Demonstrates adding a list of np.datetime64 objects element-wise via DolphinDB's add function through the Python API. Requires numpy and a valid DolphinDB session. The operation and output preserve the list's datetime type at nanosecond resolution.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\na=[np.datetime64('2019-01-01T20:00:00.000000001'), np.datetime64('2019-01-01T20:00:00.000000001')]\ns.run(\"add{1,}\",a)\n# output\narray(['2019-01-01T20:00:00.000000002', '2019-01-01T20:00:00.000000002'], dtype='datetime64[ns]')\n```\n\n----------------------------------------\n\nTITLE: Creating a distributed table for MTW in DolphinDB using Python API\nDESCRIPTION: Sets up a DolphinDB distributed database and creates a partitioned table for use with MultithreadedTableWriter. This example creates a hash-partitioned table with DATE, SYMBOL, and INT columns.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport time\nimport random\n\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"\n    dbName = 'dfs://valuedb3';\n    if(exists(dbName)){\n        dropDatabase(dbName);\n    }\n    datetest=table(1000:0,`date`symbol`id,[DATE,SYMBOL,INT]);\n    db = database(directory=dbName, partitionType=HASH, partitionScheme=[INT, 10]);\n    pt=db.createPartitionedTable(datetest,'pdatetest','id');\n\"\"\"\ns.run(script)\n```\n\n----------------------------------------\n\nTITLE: Uploading Pair Data Using DolphinDB Python API - Python\nDESCRIPTION: Shows how to upload and execute a DolphinDB Pair type expression using the Python API where the Pair is represented as a list of two elements in Python. The snippet executes the expression \"100:0\" resulting in a Python list. Dependencies include an active DolphinDB session connection 's'. The input is a DolphinDB Pair expression string, and the output is a Python list representation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> s.run(\"100:0\")\n[100, 0]\n```\n\n----------------------------------------\n\nTITLE: Enabling Automatic Reconnection for DBConnectionPool in Python\nDESCRIPTION: Explains how to enable the automatic reconnection feature for a `DBConnectionPool` when high availability is *not* enabled, by setting the `reconnect` parameter to `True`. This allows the API to attempt reconnection upon detecting a connection issue. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\n\n# 创建连接池；开启重连\npool = ddb.DBConnectionPool(\"localhost\", 8848, 8, reconnect=True)\n```\n\n----------------------------------------\n\nTITLE: Adding pandas Series Data via DolphinDB Python API - Python\nDESCRIPTION: Demonstrates passing a pandas Series object, with custom index, to DolphinDB for element-wise addition. Requires pandas, numpy, and a DolphinDB Python API session (s). The index will be lost after upload; the result is a NumPy array.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\na = pd.Series([1,2,3,1,5],index=np.arange(1,6,1))\ns.run(\"add{1,}\",a)\n# output\narray([2, 3, 4, 2, 6])\n```\n\n----------------------------------------\n\nTITLE: Automatically Releasing Variables after Query Execution in DolphinDB Python API - Python\nDESCRIPTION: Explains how to use the clearMemory parameter in the DolphinDB Python API to automatically release variables after executing a script. Illustrates that after enabling auto-clear, subsequent access attempts throw exceptions since the variable no longer exists. Requires established session connection.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\") \ns.run(\"t = 1\", clearMemory = True) \ns.run(\"t\")   \n```\n\n----------------------------------------\n\nTITLE: Inspecting Loaded Quote Data in DolphinDB\nDESCRIPTION: This snippet continues the setup for asof join examples by inspecting the loaded 'quotes' table. It filters the 'quotes' table to show entries from 09:29:59 onwards using the `where` clause and prints the top 5 rows of the filtered result as a Pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_88\n\nLANGUAGE: Python\nCODE:\n```\nprint(quotes.where(\"second(Time)>=09:29:59\").top(5).toDF())\n```\n\n----------------------------------------\n\nTITLE: Uploading Python List to DolphinDB and Checking Type - Python\nDESCRIPTION: Shows how to upload a standard Python list via the DolphinDB Python API upload method and retrieve it and its DolphinDB type. Mixed-type lists are interpreted as ANY VECTOR; using np.array is recommended for consistency. Requires a DolphinDB session (s).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\na = [1,2,3.0]\ns.upload({'a':a})\na_new = s.run(\"a\")\nprint(a_new)\n# output\n[1. 2. 3.]\n\na_type = s.run(\"typestr(a)\")\nprint(a_type)\n# output\nANY VECTOR\n```\n\n----------------------------------------\n\nTITLE: Get Subscription Topics - DolphinDB Python\nDESCRIPTION: This code snippet demonstrates how to retrieve the subscription topics using the `getSubscriptionTopics` function.  It is used to list the active subscriptions, represented by a string in the format \"host/port/tableName/actionName\". The function does not take any arguments and returns a list of strings representing the topics.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_99\n\nLANGUAGE: python\nCODE:\n```\ns.getSubscriptionTopics()\n# output\n['192.168.1.103/8921/trades/action']\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Pivotby with Exec Example\nDESCRIPTION: This example uses the `pivotby` function to pivot the `VOL` column based on `TICKER` and `date`. The difference from the previous example is that `.exec` is used instead of `.select` which causes the data to be returned as a DolphinDB matrix instead of a table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndf = s.loadTable(tableName=\"trade\", dbPath=\"dfs://valuedb\")\nprint(df.exec(\"VOL\").pivotby(\"TICKER\", \"date\").toDF())\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows from a DolphinDB Partitioned Table using Python\nDESCRIPTION: Connects to DolphinDB, creates a sample partitioned table 'pt', appends data, loads it, deletes rows where 'sym' is 'AMZN' using the `delete()` and `where()` methods, and finally converts the remaining data to a Pandas DataFrame for display. Requires an active DolphinDB session `s` and the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\ndbPath=\"dfs://valuedb\"\ndstr = \"\"\"\ndbPath=\"dfs://valuedb\"\nif (existsDatabase(dbPath)){\n    dropDatabase(dbPath)\n}\nmydb=database(dbPath, VALUE, ['AMZN','NFLX', 'NVDA'])\nt=table(take(['AMZN','NFLX', 'NVDA'], 10) as sym, 1..10 as id, rand(10,10) as price)\nmydb.createPartitionedTable(t,`pt,`sym).append!(t)\n\n\"\"\"\nt1=s.run(dstr)\nt1=s.loadTable(tableName=\"pt\",dbPath=dbPath)\nt1.delete().where(\"sym=`AMZN\").execute()\n\nt1.toDF()\nprint(t1.toDF())\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB OHLC Stream Table\nDESCRIPTION: Defines and shares a DolphinDB stream table named `OHLC` to store the calculated Open, High, Low, Close, and Volume data from the time-series aggregation engine. Requires a running DolphinDB server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_107\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(100:0, `datetime`symbol`open`high`low`close`volume,[DATETIME, SYMBOL, DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG]) as OHLC\n\n```\n\n----------------------------------------\n\nTITLE: Handling Integer Vector with Nulls in DolphinDB Python API - Python\nDESCRIPTION: Shows how DolphinDB Integer Vectors containing null values are converted to numpy ndarray with dtype float64, where nulls are represented as np.nan. The example executes a DolphinDB array containing integers and a null value, demonstrating the Python type and dtype of the result. Prerequisites include the DolphinDB session 's'. The input is a vector expression with nulls, and the output is a numpy ndarray with floating-point representation using NaN for nulls.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/3_AdvancedOperations/3.1_DataTypeCasting/3.1.1_PROTOCOL_DDB.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> re = s.run(\"[1, 2, 3, NULL]\")\n>>> re\n[ 1.  2.  3. nan]\n>>> re.dtype\nfloat64\n```\n\n----------------------------------------\n\nTITLE: Constructing a MultithreadedTableWriter in Python\nDESCRIPTION: Creates a MultithreadedTableWriter object with specified connection and configuration parameters. This example sets up 3 writing threads for a partitioned table with LZ4 and DELTA compression and Upsert mode.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.2_MultithreadedTableWriter.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nwriter = ddb.MultithreadedTableWriter(\n    \"localhost\", 8848, \"admin\", \"123456\", \"dfs://testMTW\", \"pt\", batchSize=2, throttle=0.01,  \n    threadCount=3, partitionCol=\"date\", compressMethods=[\"LZ4\", \"DELTA\", \"LZ4\", \"DELTA\"],\n    mode=\"Upsert\", modeOption=[\"true\", \"`index\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Unsubscribing from Stream Table - DolphinDB Python\nDESCRIPTION: This snippet shows how to unsubscribe from a stream table using the `unsubscribe` function in the DolphinDB Python API. It takes the host, port, table name, and action name as parameters to stop receiving data from a specific subscription.  This allows for the termination of a previous subscription.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_100\n\nLANGUAGE: python\nCODE:\n```\ns.unsubscribe(\"192.168.1.103\", 8921,\"trades\",\"action\")\n```\n\n----------------------------------------\n\nTITLE: Defining the DBConnectionPool Constructor Syntax in Python\nDESCRIPTION: Provides the complete signature for initializing a `DBConnectionPool` object, showing all available parameters and their default values. This constructor is used to establish a pool of connections to a DolphinDB server for concurrent task execution.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.2_DBConnectionPool/2.2.1_Constructor.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDBConnectionPool(host, port, threadNum=10, userid=None, password=None, \n                 loadBalance=False, highAvailability=False, compress=False,  \n                 reConnect=False, python=False, protocol=PROTOCOL_DEFAULT,\n                 show_output=True)\n```\n\n----------------------------------------\n\nTITLE: Uploading pandas DataFrame to DolphinDB\nDESCRIPTION: This snippet demonstrates uploading a pandas DataFrame to a DolphinDB server using the `upload` method.  The uploaded DataFrame becomes a variable in the DolphinDB session. The uploaded DataFrame requires that all values in one column have the same data type.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'id': np.int32([1, 2, 3, 6, 8]), 'x': np.int32([5, 4, 3, 2, 1])})\ns.upload({'t1': df})\nprint(s.run(\"t1.x.avg()\"))\n# output\n3.0\n```\n\n----------------------------------------\n\nTITLE: Uploading NumPy Array as DOUBLE Vector via DolphinDB Python API - Python\nDESCRIPTION: Demonstrates uploading a NumPy array, with specified double data type, to DolphinDB. Shows retrieval of the array and its confirmation as a FAST DOUBLE VECTOR in DolphinDB. Requires numpy and an active session (s).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\narr = np.array([1,2,3.0],dtype=np.double)\ns.upload({'arr':arr})\narr_new = s.run(\"arr\")\nprint(arr_new)\n# output\n[1. 2. 3.]\n\narr_type = s.run(\"typestr(arr)\")\nprint(arr_type)\n# output\nFAST DOUBLE VECTOR\n```\n\n----------------------------------------\n\nTITLE: Loading Data from DolphinDB Tables with loadTable\nDESCRIPTION: Shows how to load data from a DolphinDB distributed table into Python using the loadTable function. The example demonstrates retrieving the table schema and converting the table to a pandas DataFrame.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ntrade = s.loadTable(tableName=\"trade\",dbPath=\"dfs://valuedb\")\n\nprint(trade.schema)\n#output\n     name typeString  typeInt comment\n0  TICKER     SYMBOL       17\n1    date       DATE        6\n2     VOL        INT        4\n3     PRC     DOUBLE       16\n4     BID     DOUBLE       16\n5     ASK     DOUBLE       16\n\nprint(trade.toDF())\n```\n\n----------------------------------------\n\nTITLE: Debugging Python Environment with pip (Console)\nDESCRIPTION: Executes the `pip debug --verbose` command to display detailed information about the Python environment, including compatible distribution tags. This command is useful for troubleshooting installation issues by identifying required wheel types for manual installation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_1\n\nLANGUAGE: Console\nCODE:\n```\npip debug --verbose\n```\n\n----------------------------------------\n\nTITLE: Releasing DolphinDB Table Object via Python Variable Scope\nDESCRIPTION: Illustrates that assigning `None` to the Python local variable (`t0`) that references a DolphinDB server-side table object can trigger the release of the corresponding object on the server due to reference counting. Assumes 't0' was previously assigned the result of `s.table()` or `s.loadTable()`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nt0=None\n```\n\n----------------------------------------\n\nTITLE: Retrieving Unwritten Data with MultithreadedTableWriter in Python\nDESCRIPTION: The getUnwrittenData method returns a nested list containing data that hasn't been written to the server, including both failed and pending data. After calling this method, the writer releases these data resources.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_80\n\nLANGUAGE: Python\nCODE:\n```\ngetUnwrittenData()\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB Scripts Using the run Method in Python\nDESCRIPTION: Executes DolphinDB scripts remotely via the session's run method. Supports returning objects, defining functions, and executing multi-line scripts with triple quotes. Allows interaction with DolphinDB runtime and data manipulation.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ns = ddb.session()\ns.connect(\"localhost\", 8848)\na=s.run(\"`IBM`GOOG`YHOO\")\nrepr(a)\n```\n\nLANGUAGE: Python\nCODE:\n```\ns.run(\"def getTypeStr(input){ \\nreturn typestr(input)\\n}\")\n```\n\nLANGUAGE: Python\nCODE:\n```\nscript=\"\"\"\ndef getTypeStr(input){\n    return typestr(input)\n}\n\"\"\"\ns.run(script)\ns.run(\"getTypeStr\", 1)\n```\n\n----------------------------------------\n\nTITLE: Creating a MultithreadedTableWriter Object in Python\nDESCRIPTION: Constructor for the MultithreadedTableWriter class that enables multi-threaded writing to DolphinDB tables. It supports various parameters including connection details, batch size, thread count, and compression methods.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_77\n\nLANGUAGE: Python\nCODE:\n```\nMultithreadedTableWriter(host, port, userId, password, dbPath, tableName, useSSL=False, enableHighAvailability=False, highAvailabilitySites=[], batchSize=1, throttle=1, threadCount=1, partitionCol=\"\", compressMethods=[], mode=\"\", modeOption=[])\n```\n\n----------------------------------------\n\nTITLE: Getting Unwritten Data from MultithreadedTableWriter in Python\nDESCRIPTION: Retrieves data that has not yet been successfully written to the DolphinDB server, including data pending transmission and data that previously failed. Note that data obtained via this method is released by the writer object.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\ngetUnwrittenData()\n```\n\n----------------------------------------\n\nTITLE: Connecting to DolphinDB Server with Basic Parameters in Python\nDESCRIPTION: Establishes a connection to the DolphinDB server using host, port, and optional user credentials. Supports high-availability setup by specifying multiple nodes. Includes examples of basic connection, login, and session ID retrieval.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n```\n\nLANGUAGE: Python\nCODE:\n```\ns.connect(\"localhost\", 8848)\n```\n\nLANGUAGE: Python\nCODE:\n```\ns.login(\"admin\",\"123456\")\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(host=\"192.168.1.2\", port=24120, userid=\"admin\", password=\"123456\", highAvailability=True, highAvailabilitySites=sites)\n```\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(\"localhost\", 8848)\nprint(s.getSessionId())\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Trade Stream Table\nDESCRIPTION: Defines and shares a DolphinDB stream table named `Trade` with columns for Symbol, Datetime, Price, and Volume, used to receive real-time trade data before processing. Requires a running DolphinDB server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_104\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(100:0, `Symbol`Datetime`Price`Volume,[SYMBOL,DATETIME,DOUBLE,INT]) as Trade\n\n```\n\n----------------------------------------\n\nTITLE: Dropping specific partitions from a DolphinDB database in Python\nDESCRIPTION: This code demonstrates how to delete specific partitions from a DolphinDB database using the Python API, considering partition name string formatting. It involves loading data, printing record counts, and dropping specified partitions.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_99\n\nLANGUAGE: Python\nCODE:\n```\ns.dropPartition(\"dfs://valuedb\", partitionPaths=[\"'AMZN'\",\"'NFLX'\"], tableName=\"trade\")\n```\n\n----------------------------------------\n\nTITLE: Checking and Dropping DolphinDB DFS Database in Python\nDESCRIPTION: Uses `s.existsDatabase()` to check if the database `dfs://valuedb` exists and `s.dropDatabase()` to remove it if it does. This is a common preparatory step before creating or recreating a database. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nif s.existsDatabase(\"dfs://valuedb\"):\n    s.dropDatabase(\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Getting Status of All Tables in BatchTableWriter (Python)\nDESCRIPTION: Retrieves status information for all tables currently managed by the BatchTableWriter (excluding removed tables). Returns a pandas DataFrame containing columns like DatabaseName, TableName, WriteQueueDepth, SendedRows, Removing, and Finished.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nres:pandas.DataFrame = writer.getAllStatus()\n```\n\n----------------------------------------\n\nTITLE: Importing TSV Data into DolphinDB Memory Table using `loadText` in Python\nDESCRIPTION: Uses `s.loadText()` to import data from a tab-separated file (`t1.tsv`) into a DolphinDB memory table, specifying the tab character `\\t` as the delimiter argument. Requires the `dolphindb` library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nt1=s.loadText(WORK_DIR+\"/t1.tsv\", '\\t')\n```\n\n----------------------------------------\n\nTITLE: Enabling SSL, Asynchronous, and Compressed Communication in DolphinDB Python API\nDESCRIPTION: Configures a session to support SSL, asynchronous data transmission, or compression for large data transfers. Each feature modifies session behavior and is activated via specific parameters during session initialization.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ns=ddb.session(enableSSL=True)\n```\n\nLANGUAGE: Python\nCODE:\n```\ns=ddb.session(enableASYNC=True)\n```\n\nLANGUAGE: Python\nCODE:\n```\ns=ddb.session(compress=True, enablePickle=False)\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB 'add' function with Python lists\nDESCRIPTION: This snippet demonstrates how to execute the DolphinDB built-in function 'add' with Python lists as input. It passes two lists to the 'run' method and retrieves the NumPy array result.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns.run(\"add\",[1,2,3,4],[1,2,1,1])\n# output\narray([2, 4, 4, 5])\n```\n\n----------------------------------------\n\nTITLE: BatchTableWriter Constructor Example in Python\nDESCRIPTION: Example demonstrating how to create an instance of BatchTableWriter, connecting to 'localhost' on port 8848 with user 'admin', password '123456', and enabling the internal lock.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nwriter = ddb.BatchTableWriter(\"localhost\", 8848, \"admin\", \"123456\", acquireLock=True)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Server-Side Table Name for Loaded Table\nDESCRIPTION: Demonstrates how to retrieve the actual temporary name assigned by the DolphinDB server to an in-memory table loaded via `loadTable`. This is done by calling the `.tableName()` method on the Python table object (`pt1`).\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nprint(pt1.tableName())\n'TMP_TBL_4c5647af'\n```\n\n----------------------------------------\n\nTITLE: Releasing DolphinDB Table Object using run\nDESCRIPTION: Shows how to release a DolphinDB server-side table object by setting its corresponding variable ('t1') to NULL using the `session.run()` method to execute a DolphinDB script. Assumes 's' is a connected DolphinDB session.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ns.run(\"t1=NULL\")\n```\n\n----------------------------------------\n\nTITLE: Appending to a DFS Table using append!\nDESCRIPTION: Shows how to append data to a DFS table using the append! function. This method is not recommended as it returns a schema, increasing data transfer volume.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntb = createDemoDataFrame()\ns.run(\"append!{{loadTable('{db}', `{tb})}}\".format(db=dbPath,tb=tableName),tb)\n```\n\n----------------------------------------\n\nTITLE: Creating a Keyed Memory Table in DolphinDB Script\nDESCRIPTION: DolphinDB script snippet used in Example 1 to create a keyed table named `testtable` with 'id' as the key column and specific column types (DATETIME, STRING, LONG). It then shares this table globally accessible as `keyed_t`.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.3_AutoFitTableAppender/2.3.2_TableUpserter.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nscript_KEYEDTABLE = \"\"\"\n    testtable=keyedTable(`id,1000:0,`date`text`id,[DATETIME,STRING,LONG])\n    share testtable as keyed_t\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB 'add' function with pandas Series\nDESCRIPTION: This snippet shows how to execute the DolphinDB built-in function 'add' with a pandas Series object as input.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\na = pd.Series([1,2,3,1,5],index=np.arange(1,6,1))\ns.run(\"add{1,}\",a)\n# output\narray([2, 3, 4, 2, 6])\n```\n\n----------------------------------------\n\nTITLE: Appending to a DFS Table using tableInsert\nDESCRIPTION: Demonstrates appending data to a DFS table using the tableInsert function, which automatically handles temporal data type conversions when appending to a DFS table.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntb = createDemoDataFrame()\ns.run(\"tableInsert{{loadTable('{db}', `{tb})}}\".format(db=dbPath,tb=tableName), tb)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for DolphinDB Database Operations\nDESCRIPTION: Imports necessary Python libraries (`numpy`, `pandas`, `dolphindb.settings`) required for subsequent examples involving DolphinDB database and table creation using the Python API.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb.settings as keys\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB 'add' function with NumPy scalar\nDESCRIPTION: This snippet demonstrates how to execute the DolphinDB built-in function 'add' with a NumPy scalar as input. The function 'add{1,}' adds 1 to the input parameter.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\ns.run(\"add{1,}\",np.int(4))\n# output\n5\n```\n\n----------------------------------------\n\nTITLE: Setting Up Asynchronous Mode for DolphinDB Python API\nDESCRIPTION: Shows how to enable asynchronous mode in the DolphinDB Python API for high throughput data processing. This mode prevents waiting for execution results from the server.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ns=ddb.session(enableASYNC=True)\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinDB Python API via pip (Console)\nDESCRIPTION: Provides the standard command to install the DolphinDB Python API package using the Python package installer, pip. This is the recommended method for installing the library.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README.md#_snippet_0\n\nLANGUAGE: Console\nCODE:\n```\n$ pip install dolphindb\n```\n\n----------------------------------------\n\nTITLE: NumPy Array Data Type\nDESCRIPTION: Shows the output data type when processing with NumPy arrays. The result will be int64.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndtype('int64')\n```\n\n----------------------------------------\n\nTITLE: Checking Table Status in BatchTableWriter (Python)\nDESCRIPTION: Retrieves the current status of a specific table managed by the writer. Requires tableName and optional dbPath. Returns a list containing [write queue depth (int), is being removed (bool), background thread exited due to error (bool)].\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nres:list = writer.getStatus(dbPath=None, tableName=None)\n```\n\n----------------------------------------\n\nTITLE: Toggling Python Parser Feature in DolphinDB Python Session\nDESCRIPTION: Demonstrates enabling or disabling the experimental Python parser feature by setting the python parameter during session initialization. This feature is currently supported only in DolphinDB version 3.00 (not yet released). It affects how DolphinDB parses Python-like syntax sent from the client.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.1_Session/2.1.1_Constructor.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# 启用 python parser 特性\ns = ddb.session(python=True)\n\n# 不启用 python parser 特性\ns = ddb.session(python=False)\n```\n\n----------------------------------------\n\nTITLE: Removing a Table from BatchTableWriter in Python\nDESCRIPTION: Removes a table previously added by addTable, stopping the background writing thread and releasing associated resources for that table. Requires tableName and optional dbPath.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN_NEW/2_BasicOperations/2.5_AsyncWrites/2.5.3_BatchTableWriter.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nwriter.removeTable(dbPath=None, tableName=None)\n```\n\n----------------------------------------\n\nTITLE: BatchTableWriter API Usage and Method Reference (Python, Deprecated)\nDESCRIPTION: This snippet provides a full usage example of the BatchTableWriter API, including adding a table, inserting multiple rows, retrieving unwritten data, status, and all status information. Parameter descriptions and method signatures for addTable, insert, removeTable, getUnwrittenData, and getStatus are also included. Dependencies: dolphindb Python API, numpy, pandas. Inputs: Example data rows for the 'tglobal' table. Outputs: Print statements of batch write status, unwritten data, and query results. Note: BatchTableWriter is deprecated in favor of MultithreadedTableWriter and may not be maintained.\nSOURCE: https://github.com/dolphindb/api_python3/blob/master/README_CN.md#_snippet_92\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport dolphindb as ddb\nimport time\ns = ddb.session()\ns.connect(\"localhost\", 8848, \"admin\", \"123456\")\n\nscript = \"\"\"t = table(1000:0,`id`date`ticker`price, [INT,DATE,SYMBOL,DOUBLE])\nshare t as tglobal\"\"\"\ns.run(script)\n\nwriter = ddb.BatchTableWriter(\"localhost\", 8848)\nwriter.addTable(tableName=\"tglobal\")\nwriter.insert(\"\",\"tglobal\", 1, np.datetime64(\"2019-01-01\"),'AAPL', 5.6)\nwriter.insert(\"\",\"tglobal\", 2, np.datetime64(\"2019-01-01\"),'GOOG', 8.3)\nwriter.insert(\"\",\"tglobal\", 3, np.datetime64(\"2019-01-02\"),'GOOG', 4.2)\nwriter.insert(\"\",\"tglobal\", 4, np.datetime64(\"2019-01-03\"),'AMZN', 1.4)\nwriter.insert(\"\",\"tglobal\", 5, np.datetime64(\"2019-01-05\"),'AAPL', 6.9)\n\nprint(writer.getUnwrittenData(dbPath=\"\", tableName=\"tglobal\"))\nprint(writer.getStatus(tableName=\"tglobal\"))\nprint(writer.getAllStatus())\n\nprint(\"rows:\", s.run(\"tglobal.rows()\"))\nprint(s.run(\"select * from tglobal\"))\n```"
  }
]
[
  {
    "owner": "german-nlp-group",
    "repo": "german-transformer-training",
    "content": "TITLE: Initializing TPU System in TensorFlow\nDESCRIPTION: Python code for initializing and connecting to a TPU system. Creates a TPU resolver and connects to the cluster.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/Training.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\n```\n\n----------------------------------------\n\nTITLE: Building Pretraining Dataset for German ELECTRA\nDESCRIPTION: Command to create TensorFlow dataset for ELECTRA training with specific parameters for German language processing, including handling of accents.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/model_cards/electra-base-german-uncased.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython build_pretraining_dataset.py --corpus-dir <corpus_dir> --vocab-file <dir>/vocab.txt --output-dir ./tf_data --max-seq-length 512 --num-processes 8 --do-lower-case --no-strip-accents\n```\n\n----------------------------------------\n\nTITLE: Connecting to TPU from Python with TensorFlow\nDESCRIPTION: Simple Python code to create a TPU cluster resolver that automatically connects to the TPU in Cloud Shell environment.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/Training.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver()\n```\n\n----------------------------------------\n\nTITLE: Completing TPU Initialization with TensorFlow\nDESCRIPTION: Code to finalize TPU initialization after creating the resolver. Includes a commented line for listing available TPU devices.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/Training.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntf.tpu.experimental.initialize_tpu_system(resolver)\n#print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n```\n\n----------------------------------------\n\nTITLE: Extracting Content from German Wikipedia Dump using WikiExtractor\nDESCRIPTION: This command uses the WikiExtractor tool to process the German Wikipedia dump. It extracts content from the compressed XML file, splitting it into 100MB chunks, and utilizes 8 parallel processes for efficiency.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/data-prep.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython WikiExtractor.py -b 100M -o output --processes 8 dewiki-20200620-pages-articles.xml.bz2\n```\n\n----------------------------------------\n\nTITLE: Starting TPU with ctpu Command in Google Cloud Shell\nDESCRIPTION: Command to create and start a v3-8 TPU in the europe-west4-a zone with TensorFlow 1.15. Uses preemptible pricing for cost savings.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/Training.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nctpu up --zone=europe-west4-a  --tf-version=1.15 -preemptible --name=tpu-testv13 --tpu-size=v3-8\n```\n\n----------------------------------------\n\nTITLE: Capturing TPU Performance Profile\nDESCRIPTION: Command to analyze TPU usage and performance by capturing a profile at monitoring level 2.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/Training.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncapture_tpu_profile --tpu=$TPU_NAME  --monitoring_level=2\n```\n\n----------------------------------------\n\nTITLE: Listing Common Data Cleanup Targets in Wikipedia Dumps\nDESCRIPTION: This code snippet lists common HTML and wiki markup elements that need to be removed or cleaned up in the Wikipedia data dump. These elements include various HTML tags and wiki-specific markup that are not part of the actual content.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/data-prep.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<BR>\n<br> and </br>\n<onlyinclude> and <onlyinclude>\n<nowiki>\n</ref>\n<ref\n<-\n</poem>\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers Library from Source\nDESCRIPTION: Command to install the Transformers library directly from GitHub source to get support for strip_accents=False parameter.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/model_cards/electra-base-german-uncased.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/huggingface/transformers.git -U\n```\n\n----------------------------------------\n\nTITLE: Deleting TPU and VM Resources with ctpu\nDESCRIPTION: Command to completely delete a TPU and its associated VM to stop all billing for the resources.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/Training.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nctpu delete --zone='europe-west4-a' --name=tpu-testv11\n```\n\n----------------------------------------\n\nTITLE: Dataset Table Structure in Markdown\nDESCRIPTION: Markdown table showing German language datasets with their sizes, quality ratings, and processing factors for transformer training.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nDataset                                                                  | Raw Size    /Characters                                                               | Quality/Filtered?                                            | URL| Notes/Status | Dupe Factor | Total = 178 GB\n---------------------------------------------------------------------------- | ---------------------------------------------------------------- | -------------------------------------------------------- | ----------------------------------------------- | ---------------------------------- | --------------- | ---------------------------------\nGerman Wikipedia Dump + Comments                                        |  5.4 GB / 5.3b   |      ++       |       |   | 10 | 54 GB = 30 % \nOscar Corpus (Common Crawl 2018-47)                                       |  145 GB / 21b Words  |            |  <a href ='https://oscar-corpus.com/'>Downlaod</a>     |  | ----- | ------\n```\n\n----------------------------------------\n\nTITLE: Model Comparison Table in Markdown\nDESCRIPTION: Markdown table comparing different German transformer models with their training steps, results, and associated resources.\nSOURCE: https://github.com/german-nlp-group/german-transformer-training/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nName                                                                  | Steps                                                                   | Result URL                                                            | Training Time | Code | Metrics \n----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------- | ---------------------------------- | ----------------------------------\nDeepset German Bert Base                                     |   810k (1024 SL) + 30k (512 SL)         |  <a href ='https://deepset.ai/german-bert'>Deepset</a>               |      9 Days TPU v2-8    |  \nDdmdz German Bert Base                                     |   1500k (512 SL)         |  <a href ='https://github.com/dbmdz/berts#german-bert'>dbmdz</a>               |         |  <a href ='https://github.com/dbmdz/berts#german-bert'>dbmdz</a> | <a href ='https://github.com/stefan-it/fine-tuned-berts-seq#german'>stefan-it</a>\n```"
  }
]
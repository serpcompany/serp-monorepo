[
  {
    "owner": "roboflow",
    "repo": "supervision",
    "content": "TITLE: Using NCNN Model with Supervision in Python\nDESCRIPTION: Demonstrates how to use an NCNN model (YOLOv8s) with Supervision for object detection and conversion of results to Supervision's Detections format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom ncnn.model_zoo import get_model\nimport supervision as sv\n\nimage = cv2.imread(\"<SOURCE_IMAGE_PATH>\")\nmodel = get_model(\n    \"yolov8s\",\n    target_size=640,\n    prob_threshold=0.5,\n    nms_threshold=0.45,\n    num_threads=4,\n    use_gpu=True,\n)\nresult = model(image)\ndetections = sv.Detections.from_ncnn(result)\n```\n\n----------------------------------------\n\nTITLE: Loading Detection Datasets from Various Formats\nDESCRIPTION: Examples showing how to load detection datasets from different formats including YOLO, Pascal VOC, and COCO formats using Supervision's utilities.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = sv.DetectionDataset.from_yolo(\n    images_directory_path=...,\n    annotations_directory_path=...,\n    data_yaml_path=...\n)\n\ndataset = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=...,\n    annotations_directory_path=...\n)\n\ndataset = sv.DetectionDataset.from_coco(\n    images_directory_path=...,\n    annotations_path=...\n)\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Custom Labels using Transformers in Python\nDESCRIPTION: This code snippet illustrates how to use Supervision library to annotate images with custom labels using the Transformers method. It includes loading a DETR model, processing the image, creating custom labels, and annotating the image with bounding boxes and labels.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Using Supervision with Ultralytics YOLO Model for Object Detection\nDESCRIPTION: Example code showing how to use Supervision with Ultralytics YOLO model. The code loads an image, runs inference with YOLOv8, and converts the results to Supervision's Detections format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(...)\nmodel = YOLO(\"yolov8s.pt\")\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\nlen(detections)\n# 5\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection with Inference Package in Python\nDESCRIPTION: This snippet shows how to perform object detection using the Inference package. It loads a YOLOv8 model, reads an image, and runs inference on it.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\n```\n\n----------------------------------------\n\nTITLE: Using InferenceSlicer with Ultralytics\nDESCRIPTION: Advanced small object detection using Supervision's InferenceSlicer with Ultralytics. The callback function processes image slices with a YOLOv8 model and the results are combined to detect objects in large, high-resolution images.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    result = model(image_slice)[0]\n    return sv.Detections.from_ultralytics(result)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Initializing Detections Class in Python\nDESCRIPTION: This code snippet shows the initialization of the Detections class with various parameters. It includes xyxy coordinates, class_id, confidence, and optional tracker_id.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/core.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDetections(\n    xyxy: np.ndarray,\n    class_id: Optional[np.ndarray] = None,\n    confidence: Optional[np.ndarray] = None,\n    tracker_id: Optional[np.ndarray] = None\n) -> None\n```\n\n----------------------------------------\n\nTITLE: Annotating Video with Object Traces using Ultralytics in Python\nDESCRIPTION: This code snippet demonstrates how to use Supervision with Ultralytics to annotate a video with object traces. It uses YOLO for object detection, ByteTrack for tracking, and various Supervision annotators to visualize bounding boxes, labels, and traces.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Small Object Segmentation with Custom Inference Model in Python\nDESCRIPTION: This snippet demonstrates how to use the InferenceSlicer for small object segmentation using a custom inference model. It loads an image, defines a callback function for inference, and applies mask and label annotations to the detected objects.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-seg-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    results = model.infer(image_slice)[0]\n    return sv.Detections.from_inference(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Saving Custom Inference Detections to JSON with Supervision\nDESCRIPTION: Demonstrates saving object detection results from a custom inference model to JSON using Supervision's JSONSink. The code processes video frames, performs inference, and saves detections with frame indices.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nwith sv.JSONSink(<TARGET_CSV_PATH>) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model.infer(image)[0]\n        detections = sv.Detections.from_inference(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Segmentation Masks using Ultralytics in Python\nDESCRIPTION: This code snippet shows how to use Supervision library to annotate images with segmentation masks using the Ultralytics method. It includes loading a YOLO segmentation model, creating mask and label annotators, and applying them to the image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n-seg.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER_OF_MASS)\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Converting Detections to YOLO Format in Python\nDESCRIPTION: This method converts the Detections object to YOLO format, returning xywh coordinates, class_id, and confidence as separate arrays.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/core.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef to_yolo(self) -> Tuple[np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]:\n```\n\n----------------------------------------\n\nTITLE: Keypoint Detection and Annotation using Ultralytics in Python\nDESCRIPTION: This code snippet shows how to perform keypoint detection using YOLOv8 and annotate the results with Supervision's EdgeAnnotator and VertexAnnotator.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8m-pose.pt\")\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    key_points = sv.KeyPoints.from_ultralytics(results)\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    return vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Custom Labels using Inference in Python\nDESCRIPTION: This code snippet demonstrates how to use Supervision library to annotate images with custom labels using the Inference method. It includes creating custom labels, annotating bounding boxes, and adding labels to the image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Mixed Conditions in Supervision\nDESCRIPTION: Shows how to combine multiple filtering conditions to create complex filters. This example filters detections by both confidence threshold (>0.7) and presence in a polygon zone.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[(detections.confidence > 0.7) & mask]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Detections with BoxAnnotator\nDESCRIPTION: Example demonstrating how to visualize object detections using Supervision's BoxAnnotator. This annotator draws bounding boxes around detected objects on the input image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\n\nimage = cv2.imread(...)\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n  scene=image.copy(),\n  detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Implementing Keypoint Tracking with Inference in Python\nDESCRIPTION: This snippet shows how to use Supervision with Inference to perform keypoint tracking on video frames. It uses a Roboflow model, initializes annotators, and applies ByteTrack for object tracking.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(\n    model_id=\"yolov8m-pose-640\", api_key=<ROBOFLOW API KEY>)\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ntracker = sv.ByteTrack()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model.infer(frame)[0]\n    key_points = sv.KeyPoints.from_inference(results)\n    detections = key_points.as_detections()\n    detections = tracker.update_with_detections(detections)\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    annotated_frame = vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n    annotated_frame = box_annotator.annotate(\n        annotated_frame, detections=detections)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing ByteTrack Tracker Class in Python\nDESCRIPTION: This code snippet defines the ByteTrack class, which implements the ByteTrack algorithm for object tracking. It includes methods for initialization, updating tracks, and managing track states.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/trackers.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ByteTrack:\n    def __init__(\n        self,\n        fps: float,\n        track_thresh: float = 0.25,\n        track_buffer: int = 30,\n        match_thresh: float = 0.8,\n        min_box_area: float = 10,\n        frame_rate: Optional[float] = None,\n    ):\n        self.tracker = BYTETracker(\n            track_thresh=track_thresh,\n            track_buffer=track_buffer,\n            match_thresh=match_thresh,\n            min_box_area=min_box_area,\n            frame_rate=fps if frame_rate is None else frame_rate,\n        )\n        self.fps = fps\n        self.tracks: Dict[int, Track] = {}\n\n    def update(self, detections: Detections) -> Detections:\n        if len(detections) == 0:\n            self.tracks.clear()\n            return detections\n\n        yolo_detections = detections2yolo_detections(detections)\n        byte_detections = yolo2byte_detections(yolo_detections)\n        track_results = self.tracker.update(\n            byte_detections,\n            detections.xyxy,\n            detections.confidence,\n            detections.class_id,\n        )\n\n        self.update_tracks(track_results)\n        return self.update_detections(detections)\n\n    def update_tracks(self, results: List[STrack]) -> None:\n        self.tracks.clear()\n        for result in results:\n            track_id = result.track_id\n            if track_id not in self.tracks:\n                self.tracks[track_id] = Track(id=track_id)\n            self.tracks[track_id].update(result)\n\n    def update_detections(self, detections: Detections) -> Detections:\n        tracker_ids = np.full(len(detections), -1)\n        for i, result in enumerate(self.tracker.tracked_stracks):\n            tracker_ids[i] = result.track_id\n\n        return detections.with_tracker_id(tracker_ids)\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Segmentation Masks using Transformers in Python\nDESCRIPTION: This code snippet illustrates how to use Supervision library to annotate images with segmentation masks using the Transformers method. It includes loading a DETR segmentation model, processing the image, creating mask and label annotators, and applying them to the image with custom labels.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForSegmentation\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_segmentation(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER_OF_MASS)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Detection Dataset with Supervision in Python\nDESCRIPTION: This snippet demonstrates how to create a grid of annotated images from a detection dataset using Supervision's BoxAnnotator and LabelAnnotator. It iterates through a subset of the dataset, draws bounding boxes and class labels, and combines the annotated images into a grid.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds = sv.DetectionDataset(...)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_images = []\nfor i in range(16):\n    _, image, annotations = ds[i]\n\n    labels = [ds.classes[class_id] for class_id in annotations.class_id]\n\n    annotated_image = image.copy()\n    annotated_image = box_annotator.annotate(annotated_image, annotations)\n    annotated_image = label_annotator.annotate(annotated_image, annotations, labels)\n    annotated_images.append(annotated_image)\n\ngrid = sv.create_tiles(\n    annotated_images,\n    grid_size=(4, 4),\n    single_tile_size=(400, 400),\n    tile_padding_color=sv.Color.WHITE,\n    tile_margin_color=sv.Color.WHITE\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Keypoint Tracking with Ultralytics in Python\nDESCRIPTION: This snippet demonstrates how to use Supervision with Ultralytics to perform keypoint tracking on video frames. It initializes YOLO model, annotators, and ByteTrack for object tracking.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8m-pose.pt\")\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ntracker = sv.ByteTrack()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    key_points = sv.KeyPoints.from_ultralytics(results)\n    detections = key_points.as_detections()\n    detections = tracker.update_with_detections(detections)\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    annotated_frame = vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n    annotated_frame = box_annotator.annotate(\n        annotated_frame, detections=detections)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Bounding Box from Polygon in Python\nDESCRIPTION: Function to extract the bounding box (in xyxy format) from a polygon. Helpful for obtaining a rectangular region of interest from a polygon shape.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.polygon_to_xyxy\n```\n\n----------------------------------------\n\nTITLE: Instance Segmentation with InferenceSlicer in Python\nDESCRIPTION: Demonstrates how to use InferenceSlicer with an instance segmentation model. The code shows defining a callback function to process image slices with a YOLOv8 segmentation model and then using the slicer to process a full image, followed by visualization with mask and label annotators.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-seg-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    results = model.infer(image_slice)[0]\n    return sv.Detections.from_inference(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Set of Classes in Supervision\nDESCRIPTION: Shows how to filter object detections to include only those belonging to a specified set of class IDs. This example filters for objects with class IDs 0, 2, and 3.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\n\nselected_classes = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class_id, selected_classes)]\n```\n\n----------------------------------------\n\nTITLE: Baseline Object Detection with Transformers\nDESCRIPTION: Implementation of object detection using Hugging Face's Transformers library with DETR model. This demonstrates how to load a DETR model, process an image, and visualize results with Supervision's annotation tools.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForSegmentation\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image_slice.size\ntarget_size = torch.tensor([[width, height]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    model.config.id2label[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Instance Segmentation with Transformers in Python\nDESCRIPTION: Demonstrates using the from_transformers method to parse segmentation results from a Transformers-based model. The code shows loading a DETR panoptic segmentation model, running inference, and visualizing the results with MaskAnnotator and LabelAnnotator.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForSegmentation\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_segmentation(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(results, id2label=model.config.id2label)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Segmentation Masks using Inference in Python\nDESCRIPTION: This code snippet demonstrates how to use Supervision library to annotate images with segmentation masks using the Inference method. It includes loading a segmentation model, creating mask and label annotators, and applying them to the image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-seg-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER_OF_MASS)\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Saving DETR Transformer Detections to JSON with Supervision\nDESCRIPTION: Demonstrates saving object detection results from Facebook's DETR model to JSON using Supervision's JSONSink. Includes frame preprocessing, model inference, and post-processing steps specific to the Transformers pipeline.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nwith sv.JSONSink(<TARGET_CSV_PATH>) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        frame = sv.cv2_to_pillow(frame)\n        inputs = processor(images=frame, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        width, height = frame.size\n        target_size = torch.tensor([[height, width]])\n        results = processor.post_process_object_detection(\n            outputs=outputs, target_sizes=target_size)[0]\n        detections = sv.Detections.from_transformers(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n```\n\n----------------------------------------\n\nTITLE: Advanced Video Annotation with Tracking IDs - Ultralytics\nDESCRIPTION: Implements comprehensive video annotation including bounding boxes, tracker IDs, and class labels using Ultralytics and ByteTrack.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    return label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Keypoint Tracking with Smoothing (Inference) in Python\nDESCRIPTION: This snippet enhances the previous Inference implementation by adding DetectionsSmoother for stabilizing bounding boxes across frames. It shows how to integrate smoothing into the keypoint tracking process using a Roboflow model.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(\n    model_id=\"yolov8m-pose-640\", api_key=<ROBOFLOW API KEY>)\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ntracker = sv.ByteTrack()\nsmoother = sv.DetectionsSmoother()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model.infer(frame)[0]\n    key_points = sv.KeyPoints.from_inference(results)\n    detections = key_points.as_detections()\n    detections = tracker.update_with_detections(detections)\n    detections = smoother.update_with_detections(detections)\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    annotated_frame = vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n    annotated_frame = box_annotator.annotate(\n        annotated_frame, detections=detections)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Small Object Segmentation with Ultralytics YOLO Model in Python\nDESCRIPTION: This snippet shows how to use the InferenceSlicer for small object segmentation using the Ultralytics YOLO model. It loads an image, defines a callback function for YOLO inference, and applies mask and label annotations to the detected objects.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x-seg.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    result = model(image_slice)[0]\n    return sv.Detections.from_ultralytics(result)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Implementing Keypoint Tracking with Smoothing (Ultralytics) in Python\nDESCRIPTION: This snippet enhances the previous Ultralytics implementation by adding DetectionsSmoother for stabilizing bounding boxes across frames. It demonstrates how to integrate smoothing into the keypoint tracking process.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8m-pose.pt\")\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ntracker = sv.ByteTrack()\nsmoother = sv.DetectionsSmoother()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    key_points = sv.KeyPoints.from_ultralytics(results)\n    detections = key_points.as_detections()\n    detections = tracker.update_with_detections(detections)\n    detections = smoother.update_with_detections(detections)\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    annotated_frame = vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n    annotated_frame = box_annotator.annotate(\n        annotated_frame, detections=detections)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Saving YOLOv8 Detections to JSON with Supervision\nDESCRIPTION: Shows how to save object detection results from Ultralytics YOLOv8 model to JSON using Supervision's JSONSink. The code processes video frames and saves detections with corresponding frame indices.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nwith sv.JSONSink(<TARGET_CSV_PATH>) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n```\n\n----------------------------------------\n\nTITLE: Using SAM (Segment Anything Model) with Supervision for Object Detection\nDESCRIPTION: Illustrates how to use the Segment Anything Model (SAM) with Supervision's Detections class for object detection and segmentation tasks.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom segment_anything import (\n    sam_model_registry,\n    SamAutomaticMaskGenerator\n)\nsam_model_reg = sam_model_registry[MODEL_TYPE]\nsam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_generator = SamAutomaticMaskGenerator(sam)\nsam_result = mask_generator.generate(IMAGE)\ndetections = sv.Detections.from_sam(sam_result=sam_result)\n```\n\n----------------------------------------\n\nTITLE: Using InferenceSlicer with Inference API\nDESCRIPTION: Advanced small object detection using Supervision's InferenceSlicer with Roboflow's Inference API. This approach divides high-resolution images into smaller slices, performs detection on each slice, and then aggregates the results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    results = model.infer(image_slice)[0]\n    return sv.Detections.from_inference(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Loading Oriented Bounding Boxes Dataset in YOLO Format\nDESCRIPTION: Shows how to load a dataset with oriented bounding boxes (OBB) in YOLO format using the DetectionDataset class, and how to visualize the oriented boxes using the OrientedBoxAnnotator.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ntrain_ds = sv.DetectionDataset.from_yolo(\n    images_directory_path=\"/content/dataset/train/images\",\n    annotations_directory_path=\"/content/dataset/train/labels\",\n    data_yaml_path=\"/content/dataset/data.yaml\",\n    is_obb=True,\n)\n\n_, image, detections in train_ds[0]\n\nobb_annotator = OrientedBoxAnnotator()\nannotated_image = obb_annotator.annotate(scene=image.copy(), detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Bounding Box Dimensions in Supervision\nDESCRIPTION: Shows how to filter object detections based on their bounding box width and height. This example keeps only detections where both width and height exceed 200 pixels.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ndetections = sv.Detections(...)\nw = detections.xyxy[:, 2] - detections.xyxy[:, 0]\nh = detections.xyxy[:, 3] - detections.xyxy[:, 1]\ndetections = detections[(w > 200) & (h > 200)]\n```\n\n----------------------------------------\n\nTITLE: Inference and Class Remapping for YOLO Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to perform inference on a YOLO dataset, remap classes, and filter predictions to match the dataset classes. It uses the Supervision library for dataset handling and detection processing.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ntest_set = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/test/images\",\n    annotations_directory_path=f\"{dataset.location}/test/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n\nimage_paths = []\npredictions_list = []\ntargets_list = []\n\nfor image_path, image, label in test_set:\n    result = model.infer(image)[0]\n    predictions = sv.Detections.from_inference(result)\n\n    remap_classes(\n        detections=predictions,\n        class_ids_from_to={16: 0},\n        class_names_from_to={\"dog\": \"Corgi\"}\n    )\n    predictions = predictions[\n        np.isin(predictions[\"class_name\"], test_set.classes)\n    ]\n\n    image_paths.append(image_path)\n    predictions_list.append(predictions)\n    targets_list.append(label)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection with Inference Framework\nDESCRIPTION: Demonstrates how to perform object detection using the Inference framework and convert results to Supervision's Detection format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nfor frame in frames_generator:\n\n    results = model.infer(image)[0]\n    detections = sv.Detections.from_inference(results)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Average Precision (mAP) for Object Detection in Python\nDESCRIPTION: This code calculates the Mean Average Precision (mAP) metric for object detection using the Supervision library. It computes mAP across different IoU thresholds and provides a breakdown of results by object size.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.metrics import MeanAveragePrecision, MetricTarget\n\nmap_metric = MeanAveragePrecision(metric_target=MetricTarget.MASKS)\nmap_result = map_metric.update(predictions_list, targets_list).compute()\n\nprint(map_result)\n\nmap_result.plot()\n```\n\n----------------------------------------\n\nTITLE: Using OrientedBoxAnnotator for OBB Visualization with Supervision\nDESCRIPTION: Demonstrates how to use sv.OrientedBoxAnnotator to annotate images and videos with Oriented Bounding Boxes (OBB). The code loads an image, runs inference with a YOLOv8 OBB model, and displays the annotated image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nmodel = YOLO(\"yolov8n-obb.pt\")\n\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\noriented_box_annotator = sv.OrientedBoxAnnotator()\nannotated_frame = oriented_box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Using VertexAnnotator to visualize keypoints in Supervision\nDESCRIPTION: This snippet demonstrates how to use the VertexAnnotator class to draw keypoints on an image. It initializes the annotator with a green color and 10-pixel radius, then applies it to an image with provided keypoints.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/keypoint/annotators.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nvertex_annotator = sv.VertexAnnotator(\n    color=sv.Color.GREEN,\n    radius=10\n)\nannotated_frame = vertex_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n```\n\n----------------------------------------\n\nTITLE: Applying TraceAnnotator with YOLO and Video Processing in Python\nDESCRIPTION: This snippet demonstrates how to use the TraceAnnotator from Supervision with a YOLO model for object detection and tracking in video frames. It processes video frames, applies detections, and annotates the frames with traces.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfor frame in frames_generator:\n    result = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update_with_detections(detections)\n    annotated_frame = trace_annotator.annotate(\n        scene=frame.copy(),\n        detections=detections)\n    sink.write_frame(frame=annotated_frame)\n```\n\n----------------------------------------\n\nTITLE: Pose Estimation with KeyPoints and Ultralytics in Python\nDESCRIPTION: Shows how to use the KeyPoints class with YOLOv8 pose estimation models. The code demonstrates loading a YOLOv8 pose model, running inference on an image, converting the results to the KeyPoints format, and visualizing them with EdgeAnnotator.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nmodel = YOLO('yolov8l-pose')\n\nresult = model(image, verbose=False)[0]\nkeypoints = sv.KeyPoints.from_ultralytics(result)\n\nedge_annotators = sv.EdgeAnnotator(color=sv.Color.GREEN, thickness=5)\nannotated_image = edge_annotators.annotate(image.copy(), keypoints)\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Confidence Threshold in Supervision\nDESCRIPTION: Demonstrates filtering object detections based on their confidence scores. This example retains only detections with confidence values greater than 0.5.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence > 0.5]\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Area in Supervision\nDESCRIPTION: Shows how to filter object detections based on their absolute area in pixels. This example removes small detections by keeping only those with an area greater than 1000 pixels.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area > 1000]\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Custom Labels using Ultralytics in Python\nDESCRIPTION: This code snippet shows how to use Supervision library to annotate images with custom labels using the Ultralytics method. It includes loading a YOLO model, creating custom labels, and annotating the image with bounding boxes and labels.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Keypoint Detection and Annotation using Inference in Python\nDESCRIPTION: This code snippet demonstrates keypoint detection using a Roboflow model and annotates the results with Supervision's EdgeAnnotator and VertexAnnotator.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(\n    model_id=\"yolov8m-pose-640\", api_key=<ROBOFLOW API KEY>)\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model.infer(frame)[0]\n    key_points = sv.KeyPoints.from_inference(results)\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    return vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Keypoints to Detections and Annotating using Inference in Python\nDESCRIPTION: This code snippet demonstrates how to convert keypoints to detections using a Roboflow model and annotate them with Supervision's BoxAnnotator, EdgeAnnotator, and VertexAnnotator.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(\n    model_id=\"yolov8m-pose-640\", api_key=<ROBOFLOW API KEY>)\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model.infer(frame)[0]\n    key_points = sv.KeyPoints.from_inference(results)\n    detections = key_points.as_detections()\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    annotated_frame = vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n    return box_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Object Tracking with Ultralytics and ByteTrack\nDESCRIPTION: Extends object detection with ByteTrack tracking functionality, assigning unique IDs to detected objects across video frames using Ultralytics.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating EasyOCR with Supervision in Python\nDESCRIPTION: Shows how to integrate EasyOCR results into the Supervision framework using the new from_easyocr method. This allows for easy incorporation of optical character recognition results into Supervision's detection and annotation pipeline.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nimport easyocr\nimport cv2\n\nimage = cv2.imread(\"<SOURCE_IMAGE_PATH>\")\n\nreader = easyocr.Reader([\"en\"])\nresult = reader.readtext(\"<SOURCE_IMAGE_PATH>\", paragraph=True)\ndetections = sv.Detections.from_easyocr(result)\n\nbox_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nannotated_image = image.copy()\nannotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\nannotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n\nsv.plot_image(annotated_image)\n```\n\n----------------------------------------\n\nTITLE: Applying Albumentations Augmentation to Detection Dataset with Supervision in Python\nDESCRIPTION: This snippet shows how to apply Albumentations augmentation to a detection dataset using Supervision. It demonstrates augmenting an image and its annotations, and updating the Supervision annotations object with the augmented data.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom dataclasses import replace\n\nds = sv.DetectionDataset(...)\n\n_, original_image, original_annotations = ds[0]\n\noutput = augmentation(\n    image=original_image,\n    bboxes=original_annotations.xyxy,\n    category=original_annotations.class_id\n)\n\naugmented_image = output['image']\naugmented_annotations = replace(\n    original_annotations,\n    xyxy=np.array(output['bboxes']),\n    class_id=np.array(output['category'])\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Detections to CSV with Custom Fields\nDESCRIPTION: Shows how to save detection results to CSV file with additional custom fields like frame index using Supervision's CSVSink.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith sv.CSVSink(<TARGET_CSV_PATH>) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n```\n\n----------------------------------------\n\nTITLE: Using TraceAnnotator for Object Tracking Visualization\nDESCRIPTION: This snippet shows how to use TraceAnnotator in combination with a tracker to visualize object movements in a video. It requires a video source, YOLO model, and ByteTrack tracker.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8x.pt')\n\ntrace_annotator = sv.TraceAnnotator()\n\nvideo_info = sv.VideoInfo.from_video_path(video_path='...')\nframes_generator = sv.get_video_frames_generator(source_path='...')\ntracker = sv.ByteTrack()\n\nwith sv.VideoSink(target_path='...', video_info=video_info) as sink:\n```\n\n----------------------------------------\n\nTITLE: Object Detection with Roboflow Inference\nDESCRIPTION: Implements object detection using YOLOv8 model via Roboflow Inference package, processing video frames and annotating detected objects.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(model_id=\"yolov8n-640\", api_key=<ROBOFLOW API KEY>)\nbox_annotator = sv.BoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(results)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Using BackgroundOverlayAnnotator for Image Annotation in Python\nDESCRIPTION: This snippet demonstrates how to use the BackgroundOverlayAnnotator from Supervision to add a background overlay to an image with detections. It applies the annotator to a copy of the input image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nbackground_overlay_annotator = sv.BackgroundOverlayAnnotator()\nannotated_frame = background_overlay_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Detections Attributes in Python\nDESCRIPTION: This snippet demonstrates how to access various attributes of the Detections class, including xyxy, class_id, confidence, and tracker_id.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/core.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> detections.xyxy\narray([[100, 100, 200, 200],\n       [300, 300, 400, 400]])\n\n>>> detections.class_id\narray([0, 1])\n\n>>> detections.confidence\narray([0.9, 0.8])\n\n>>> detections.tracker_id\narray([0, 1])\n```\n\n----------------------------------------\n\nTITLE: Using IconAnnotator for Object Detection Visualization in Python\nDESCRIPTION: Demonstrates how to use the new IconAnnotator to draw specific icons for each detected class (e.g., dog and cat) on an image using the Supervision library and a YOLOv8 model.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom inference import get_model\n\nimage = <SOURCE_IMAGE_PATH>\nicon_dog = <DOG_PNG_PATH>\nicon_cat = <CAT_PNG_PATH>\n\nmodel = get_model(model_id=\"yolov8n-640\")\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nicon_paths = []\nfor class_name in detections.data[\"class_name\"]:\n    if class_name == \"dog\":\n        icon_paths.append(icon_dog)\n    elif class_name == \"cat\":\n        icon_paths.append(icon_cat)\n    else:\n        icon_paths.append(\"\")\n\nicon_annotator = sv.IconAnnotator()\nannotated_frame = icon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    icon_path=icon_paths\n)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection with Ultralytics\nDESCRIPTION: Shows how to use YOLO model from Ultralytics to perform object detection and convert results to Supervision's Detection format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nfor frame in frames_generator:\n\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n```\n\n----------------------------------------\n\nTITLE: Annotating Video with Object Traces using Inference in Python\nDESCRIPTION: This code snippet shows how to use Supervision with Inference to annotate a video with object traces. It uses a Roboflow model for object detection, ByteTrack for tracking, and various Supervision annotators to visualize bounding boxes, labels, and traces.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(model_id=\"yolov8n-640\", api_key=<ROBOFLOW API KEY>)\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Roboflow Supervision\nDESCRIPTION: This snippet lists the required Python packages and their versions for the Roboflow Supervision project. It includes packages for computer vision tasks, progress tracking, HTTP requests, and machine learning frameworks.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nsupervision>=0.20.0\ntqdm==4.66.3\nrequests\nultralytics==8.0.237\nsuper-gradients==3.5.0\ninference==0.9.17\n```\n\n----------------------------------------\n\nTITLE: Using Detections with Detectron2 Segmentation Models\nDESCRIPTION: Shows how to extract segmentation data from Detectron2 models and display it using Supervision's MaskAnnotator. This supports instance segmentation models from the Detectron2 platform.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nimport cv2\n\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\npredictor = DefaultPredictor(cfg)\n\nresult = predictor(image)\ndetections = sv.Detections.from_detectron2(result)\n\nmask_annotator = sv.MaskAnnotator()\nannotated_frame = mask_annotator.annotate(scene=image.copy(), detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Polygon Zone in Supervision\nDESCRIPTION: Demonstrates how to filter object detections based on their presence within a defined polygon zone. This example keeps only detections that trigger the specified zone.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n```\n\n----------------------------------------\n\nTITLE: Using JSONSink for Saving Detection Results in JSON Format with Supervision\nDESCRIPTION: Demonstrates how to use sv.JSONSink to save inference results from a YOLO model to a JSON file. The code processes video frames, generates detections, and appends them to a JSON file with optional custom data.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(<SOURCE_MODEL_PATH>)\njson_sink = sv.JSONSink(<RESULT_JSON_FILE_PATH>)\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nwith json_sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        json_sink.append(detections, custom_data={<CUSTOM_LABEL>:<CUSTOM_DATA>})\n```\n\n----------------------------------------\n\nTITLE: Creating Detections from YOLO Format in Python\nDESCRIPTION: This code shows how to create a Detections object from YOLO format using the from_yolo() class method. It takes xywh coordinates and optional class_id and confidence.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/core.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef from_yolo(\n    cls,\n    xyxy: np.ndarray,\n    class_id: Optional[np.ndarray] = None,\n    confidence: Optional[np.ndarray] = None\n) -> Detections:\n```\n\n----------------------------------------\n\nTITLE: Converting Mask to Polygons in Python\nDESCRIPTION: Function to convert a binary mask to a set of polygons. Useful for representing complex shapes as vectorized contours.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.mask_to_polygons\n```\n\n----------------------------------------\n\nTITLE: Increasing Input Resolution with Inference API\nDESCRIPTION: Improved small object detection by increasing the input resolution from 640 to 1280 using Roboflow's Inference API. This approach enhances detection of small objects at the cost of increased processing time and memory usage.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Converting Polygon to Mask in Python\nDESCRIPTION: Function to convert a polygon representation to a binary mask. Useful for transitioning between different object representation formats.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.polygon_to_mask\n```\n\n----------------------------------------\n\nTITLE: Implementing VideoSink Class in Python\nDESCRIPTION: The VideoSink class is probably used for writing video output. It may handle tasks such as encoding frames, managing codecs, and writing to output files or streams.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/video.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.video.VideoSink\n```\n\n----------------------------------------\n\nTITLE: Annotating Images with Detections using Supervision in Python\nDESCRIPTION: This snippet demonstrates how to annotate an image with object detection results using Supervision's BoxAnnotator and LabelAnnotator classes.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Supervision Detection Datasets\nDESCRIPTION: Python code demonstrating two methods to iterate through a Supervision DetectionDataset: using a for loop directly on the dataset instance or accessing elements by index.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds = sv.DetectionDataset(...)\n\n# Option 1\nfor image_path, image, annotations in ds:\n    ... # Process each image and its annotations\n\n# Option 2\nfor idx in range(len(ds)):\n    image_path, image, annotations = ds[idx]\n    ... # Process the image and annotations at index `idx`\n```\n\n----------------------------------------\n\nTITLE: Baseline Object Detection with Inference API\nDESCRIPTION: Basic implementation of object detection using Roboflow's Inference API with a YOLOv8x model. This demonstrates how to load a model, process an image, and visualize the detection results using Supervision.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection with Ultralytics Package in Python\nDESCRIPTION: This code demonstrates how to use the Ultralytics package for object detection. It loads a YOLOv8 model, reads an image, and performs inference.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model(image)[0]\n```\n\n----------------------------------------\n\nTITLE: Converting XYXY to XYWH Format in Python\nDESCRIPTION: Function to convert bounding boxes from XYXY (top-left, bottom-right) format to XYWH (center, width, height) format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.xyxy_to_xywh\n```\n\n----------------------------------------\n\nTITLE: Applying Background Overlay to Object Detections using BackgroundColorAnnotator\nDESCRIPTION: Shows how to use the new BackgroundColorAnnotator to draw an overlay on the background of detected objects in an image using the Supervision library and a YOLOv8 model.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom inference import get_model\n\nimage = <SOURCE_IMAGE_PATH>\n\nmodel = get_model(model_id=\"yolov8n-640\")\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbackground_overlay_annotator = sv.BackgroundOverlayAnnotator()\nannotated_frame = background_overlay_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Scaling Images in Python using Supervision\nDESCRIPTION: Function to scale an image by a given factor. It takes an image and a scale factor as input and returns the scaled image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/image.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.image.scale_image\n```\n\n----------------------------------------\n\nTITLE: Implementing MaskAnnotator for Object Segmentation Visualization\nDESCRIPTION: This code illustrates how to use MaskAnnotator to visualize object segmentation masks. It takes an image and detections with mask information as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nmask_annotator = sv.MaskAnnotator()\nannotated_frame = mask_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Running Model Predictions with Inference\nDESCRIPTION: Iterate through a dataset, run model predictions, and store results using the Inference library and Supervision.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ntest_set = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/test/images\",\n    annotations_directory_path=f\"{dataset.location}/test/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n\nimage_paths = []\npredictions_list = []\ntargets_list = []\n\nfor image_path, image, label in test_set:\n    result = model.infer(image)[0]\n    predictions = sv.Detections.from_inference(result)\n\n    image_paths.append(image_path)\n    predictions_list.append(predictions)\n    targets_list.append(label)\n```\n\n----------------------------------------\n\nTITLE: Converting Keypoints to Detections and Annotating using Ultralytics in Python\nDESCRIPTION: This code snippet shows how to convert keypoints to detections and annotate them using Supervision's BoxAnnotator, along with EdgeAnnotator and VertexAnnotator for keypoints.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8m-pose.pt\")\nedge_annotator = sv.EdgeAnnotator()\nvertex_annotator = sv.VertexAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    key_points = sv.KeyPoints.from_ultralytics(results)\n    detections = key_points.as_detections()\n\n    annotated_frame = edge_annotator.annotate(\n        frame.copy(), key_points=key_points)\n    annotated_frame = vertex_annotator.annotate(\n        annotated_frame, key_points=key_points)\n    return box_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"skiing.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Object Detection Predictions with Supervision in Python\nDESCRIPTION: This code visualizes object detection predictions alongside ground truth annotations. It uses Supervision's PolygonAnnotator to draw bounding boxes and sv.plot_images_grid to display multiple images in a grid layout.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nN = 9\nGRID_SIZE = (3, 3)\n\ntarget_annotator = sv.PolygonAnnotator(color=sv.Color.from_hex(\"#8315f9\"), thickness=8)\nprediction_annotator = sv.PolygonAnnotator(color=sv.Color.from_hex(\"#00cfc6\"), thickness=6)\n\n\nannotated_images = []\nfor image_path, predictions, targets in zip(\n  image_paths[:N], predictions_list[:N], targets_list[:N]\n):\n    annotated_image = cv2.imread(image_path)\n    annotated_image = target_annotator.annotate(scene=annotated_image, detections=targets)\n    annotated_image = prediction_annotator.annotate(scene=annotated_image, detections=prediction)\n    annotated_images.append(annotated_image)\n\nsv.plot_images_grid(images=annotated_images, grid_size=GRID_SIZE)\n```\n\n----------------------------------------\n\nTITLE: Object Detection with Ultralytics YOLOv8\nDESCRIPTION: Implements basic object detection using YOLOv8 model via Ultralytics package, processing video frames and annotating detected objects.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nbox_annotator = sv.BoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Using CropAnnotator to Display Detection Crops with Supervision\nDESCRIPTION: Demonstrates how to use sv.CropAnnotator to annotate an image with scaled-up crops of detections. The code loads an image, runs inference with a YOLO model, and displays the annotated image with detection crops.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nmodel = get_model(model_id=\"yolov8n-640\")\n\nresult = model.infer(image)[0]\ndetections = sv.Detections.from_inference(result)\n\ncrop_annotator = sv.CropAnnotator()\nannotated_frame = crop_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary Python packages for the speed estimation example.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Tracking KeyPoints with ByteTrack\nDESCRIPTION: Demonstrates how to track keypoints detected by YOLOv8 pose model using ByteTrack tracker and visualize traces\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8m-pose.pt\")\ntracker = sv.ByteTrack()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -> np.ndarray:\n    results = model(frame)[0]\n    key_points = sv.KeyPoints.from_ultralytics(results)\n\n    detections = key_points.as_detections()\n    detections = tracker.update_with_detections(detections)\n\n    annotated_image = trace_annotator.annotate(frame.copy(), detections)\n    return annotated_image\n\nsv.process_video(\n    source_path=\"input_video.mp4\",\n    target_path=\"output_video.mp4\",\n    callback=callback\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing OverlapFilter Class for Object Detection in Python\nDESCRIPTION: The OverlapFilter class provides methods for filtering overlapping detections in object detection tasks. It likely includes parameters for setting overlap thresholds and methods for applying the filter to detection results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/double_detection_filter.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.overlap_filter.OverlapFilter\n```\n\n----------------------------------------\n\nTITLE: Using BoundingBoxAnnotator in Python with Supervision\nDESCRIPTION: Example of using the BoundingBoxAnnotator class to draw bounding boxes on an image with detections.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> image = ...\n>>> detections = sv.Detections(...)\n\n>>> bounding_box_annotator = sv.BoundingBoxAnnotator()\n>>> annotated_frame = bounding_box_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n```\n\n----------------------------------------\n\nTITLE: Saving Detection Dataset in COCO Format with Supervision in Python\nDESCRIPTION: This snippet shows how to save a detection dataset in COCO format using Supervision's DetectionDataset.as_coco method. It requires specifying the image directory path and annotations path.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds = sv.DetectionDataset(...)\n\nds.as_coco(\n    images_directory_path='<IMAGE_DIRECTORY_PATH>',\n    annotations_path='<ANNOTATIONS_PATH>'\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing HeatMapAnnotator with YOLO for Video Processing in Python\nDESCRIPTION: This code snippet shows how to use the HeatMapAnnotator from Supervision with a YOLO model for object detection in video frames. It processes video frames, applies detections, and creates a heat map annotation on the frames.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8x.pt')\n\nheat_map_annotator = sv.HeatMapAnnotator()\n\nvideo_info = sv.VideoInfo.from_video_path(video_path='...')\nframes_generator = sv.get_video_frames_generator(source_path='...')\n\nwith sv.VideoSink(target_path='...', video_info=video_info) as sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        annotated_frame = heat_map_annotator.annotate(\n            scene=frame.copy(),\n            detections=detections)\n        sink.write_frame(frame=annotated_frame)\n```\n\n----------------------------------------\n\nTITLE: Computing IoU for Oriented Bounding Boxes in Python with Supervision\nDESCRIPTION: Demonstrates the use of the new oriented_box_iou_batch function to compute Intersection over Union (IoU) for oriented or rotated bounding boxes (OBB).\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nboxes_true = np.array([[[1, 0], [0, 1], [3, 4], [4, 3]]])\nboxes_detection = np.array([[[1, 1], [2, 0], [4, 2], [3, 3]]])\nious = sv.oriented_box_iou_batch(boxes_true, boxes_detection)\nprint(\"IoU between true and detected boxes:\", ious)\n```\n\n----------------------------------------\n\nTITLE: Implementing ComparisonAnnotator for Detection Comparison in Python\nDESCRIPTION: This code snippet shows how to use the ComparisonAnnotator from Supervision to compare two sets of detections on an image. It applies the annotator to a copy of the input image with two different detection sets.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections_1 = sv.Detections(...)\ndetections_2 = sv.Detections(...)\n\ncomparison_annotator = sv.ComparisonAnnotator()\nannotated_frame = comparison_annotator.annotate(\n    scene=image.copy(),\n    detections_1=detections_1,\n    detections_2=detections_2\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Model and Video Files\nDESCRIPTION: Command to execute the setup script that downloads the required model weights (traffic_analysis.pt) and video file (traffic_analysis.mov). These files are essential for running the traffic analysis examples.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./setup.sh\n```\n\n----------------------------------------\n\nTITLE: Calculating Batch IoU for Oriented Bounding Boxes in Python\nDESCRIPTION: Function to calculate Intersection over Union (IoU) for batches of oriented bounding boxes. Useful for rotated object detection tasks.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.oriented_box_iou_batch\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Specific Class in Supervision\nDESCRIPTION: Demonstrates how to filter object detections to include only those of a specific class ID. This example shows filtering for objects with class_id of 0.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class_id == 0]\n```\n\n----------------------------------------\n\nTITLE: Using LineZoneAnnotatorMulticlass in Python with Supervision\nDESCRIPTION: Demonstrates the use of LineZoneAnnotatorMulticlass for visualizing per-class counts when objects cross a line. This is useful for applications like traffic monitoring or crowd analysis.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nimport cv2\n\nimage = cv2.imread(\"<SOURCE_IMAGE_PATH>\")\n\nline_zone = sv.LineZone(\n    start=sv.Point(0, 100),\n    end=sv.Point(50, 200)\n)\nline_zone_annotator = sv.LineZoneAnnotatorMulticlass()\n\nframe = line_zone_annotator.annotate(\n    frame=frame, line_zones=[line_zone]\n)\n\nsv.plot_image(frame)\n```\n\n----------------------------------------\n\nTITLE: Using PercentageBarAnnotator for Confidence Visualization\nDESCRIPTION: This snippet demonstrates how to use PercentageBarAnnotator to visualize confidence levels of detected objects. It requires an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npercentage_bar_annotator = sv.PercentageBarAnnotator()\nannotated_frame = percentage_bar_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Roboflow in COCO Format\nDESCRIPTION: Example showing how to load a dataset from Roboflow and convert it to Supervision's DetectionDataset format from COCO annotations. Provides ways to iterate through the dataset.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom roboflow import Roboflow\n\nproject = Roboflow().workspace(<WORKSPACE_ID>).project(<PROJECT_ID>)\ndataset = project.version(<PROJECT_VERSION>).download(\"coco\")\n\nds = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\n\npath, image, annotation = ds[0]\n    # loads image on demand\n\nfor path, image, annotation in ds:\n    # loads image on demand\n```\n\n----------------------------------------\n\nTITLE: Using DetectionDataset with Lazy Loading in Supervision\nDESCRIPTION: Demonstrates how to use the new DetectionDataset class to load a COCO dataset from Roboflow with lazy loading functionality, which only loads images into memory when needed.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport roboflow\nfrom roboflow import Roboflow\nimport supervision as sv\n\nroboflow.login()\nrf = Roboflow()\n\nproject = rf.workspace(<WORKSPACE_ID>).project(<PROJECT_ID>)\ndataset = project.version(<PROJECT_VERSION>).download(\"coco\")\n\nds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\n\npath, image, annotation = ds_train[0]\n    # loads image on demand\n\nfor path, image, annotation in ds_train:\n    # loads image on demand\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple Detection Datasets\nDESCRIPTION: Example demonstrating how to merge multiple detection datasets using Supervision. The code shows merging datasets with different classes and preserving all class information.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nds_1 = sv.DetectionDataset(...)\nlen(ds_1)\n# 100\nds_1.classes\n# ['dog', 'person']\n\nds_2 = sv.DetectionDataset(...)\nlen(ds_2)\n# 200\nds_2.classes\n# ['cat']\n\nds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\nlen(ds_merged)\n# 300\nds_merged.classes\n# ['cat', 'dog', 'person']\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision from source using virtualenv\nDESCRIPTION: Development installation steps using git clone and virtualenv setup for contributing to the Supervision package.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# clone repository and navigate to root directory\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# installation\npip install -e \".\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Confusion Matrix for Object Detection in Python\nDESCRIPTION: Demonstrates how to use the ConfusionMatrix class to benchmark and evaluate object detection models.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n>>> from ultralytics import YOLO\n\n>>> dataset = sv.DetectionDataset.from_yolo(...)\n\n>>> model = YOLO(...)\n>>> def callback(image: np.ndarray) -> sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_yolov8(result)\n\n>>> confusion_matrix = sv.ConfusionMatrix.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n>>> confusion_matrix.matrix\narray([\n    [0., 0., 0., 0.],\n    [0., 1., 0., 1.],\n    [0., 1., 1., 0.],\n    [1., 1., 0., 0.]\n])\n```\n\n----------------------------------------\n\nTITLE: MeanAveragePrecision Class Reference\nDESCRIPTION: Class definition for calculating Mean Average Precision (MAP) metrics for object detection evaluation\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/mean_average_precision.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsupervision.metrics.mean_average_precision.MeanAveragePrecision\n```\n\n----------------------------------------\n\nTITLE: Splitting Detection Datasets into Train, Test, and Validation Sets\nDESCRIPTION: Example showing how to split a dataset into training, testing, and validation sets using Supervision's DetectionDataset split method. The code demonstrates creating splits with specific ratios.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataset, test_dataset = dataset.split(split_ratio=0.7)\ntest_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)\n\nlen(train_dataset), len(test_dataset), len(valid_dataset)\n# (700, 150, 150)\n```\n\n----------------------------------------\n\nTITLE: Using Supervision with Roboflow Inference API\nDESCRIPTION: Example showing how to use Supervision with the Roboflow Inference API. Requires a Roboflow API key to authenticate and access models hosted on Roboflow.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nimage = cv2.imread(...)\nmodel = get_model(model_id=\"yolov8s-640\", api_key=<ROBOFLOW API KEY>)\nresult = model.infer(image)[0]\ndetections = sv.Detections.from_inference(result)\n\nlen(detections)\n# 5\n```\n\n----------------------------------------\n\nTITLE: Downloading Pascal VOC Format Dataset from Roboflow\nDESCRIPTION: Python code to log into Roboflow and download a dataset in Pascal VOC format. Requires workspace ID, project ID, and project version to be specified.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.workspace('<WORKSPACE_ID>').project('<PROJECT_ID>')\ndataset = project.version('<PROJECT_VERSION>').download(\"voc\")\n```\n\n----------------------------------------\n\nTITLE: Using RichLabelAnnotator with Custom Font Support\nDESCRIPTION: Demonstrates how to use the RichLabelAnnotator with a custom font path to properly display labels in various languages. This ensures proper rendering of symbols from different language scripts.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nimport\n\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\nmodel = get_model(model_id=\"yolov8n-640\")\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nrich_label_annotator = sv.RichLabelAnnotator(font_path=<TTF_FONT_PATH>)\nannotated_image = rich_label_annotator.annotate(scene=image.copy(), detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Configuring LineZoneAnnotator in Python with Supervision\nDESCRIPTION: Shows how to use the enhanced LineZoneAnnotator with new features like line-aligned labels, disabled text background, and off-center label drawing to minimize overlaps for multiple LineZone labels.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nimport cv2\n\nimage = cv2.imread(\"<SOURCE_IMAGE_PATH>\")\n\nline_zone = sv.LineZone(\n    start=sv.Point(0, 100),\n    end=sv.Point(50, 200)\n)\nline_zone_annotator = sv.LineZoneAnnotator(\n    text_orient_to_line=True,\n    display_text_box=False,\n    text_centered=False\n)\n\nannotated_frame = line_zone_annotator.annotate(\n    frame=image.copy(), line_counter=line_zone\n)\n\nsv.plot_image(frame)\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision via uv\nDESCRIPTION: UV package manager installation commands for the Supervision package, includes both direct install and project-based install options.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install supervision\n\n# For uv projects:\nuv add supervision\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Average Precision with Supervision\nDESCRIPTION: Shows how to use the MeanAveragePrecision class to benchmark object detection models using a dataset.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n>>> from ultralytics import YOLO\n\n>>> dataset = sv.DetectionDataset.from_yolo(...)\n\n>>> model = YOLO(...)\n>>> def callback(image: np.ndarray) -> sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_yolov8(result)\n\n>>> mean_average_precision = sv.MeanAveragePrecision.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n>>> mean_average_precision.map50_95\n0.433\n```\n\n----------------------------------------\n\nTITLE: Saving Detection Dataset in YOLO Format with Supervision in Python\nDESCRIPTION: This snippet demonstrates how to save a detection dataset in YOLO format using Supervision's DetectionDataset.as_yolo method. It requires specifying the image directory path, annotations directory path, and data YAML path.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds = sv.DetectionDataset(...)\n\nds.as_yolo(\n    images_directory_path='<IMAGE_DIRECTORY_PATH>',\n    annotations_directory_path='<ANNOTATIONS_DIRECTORY_PATH>',\n    data_yaml_path='<DATA_YAML_PATH>'\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Pascal VOC Format Detection Dataset with Supervision\nDESCRIPTION: Python code to load train, validation, and test datasets from Pascal VOC format using Supervision's DetectionDataset class. Shows how to access class names and dataset lengths.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds_train = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels'\n)\nds_valid = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels'\n)\nds_test = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n```\n\n----------------------------------------\n\nTITLE: Implementing RichLabelAnnotator for Customized Text Labels\nDESCRIPTION: This snippet illustrates the use of RichLabelAnnotator for adding customized text labels to detected objects. It requires an image, detections, custom labels, and a font path as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nrich_label_annotator = sv.RichLabelAnnotator(\n    font_path=\"<TTF_FONT_PATH>\",\n    text_position=sv.Position.CENTER\n)\nannotated_frame = rich_label_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n```\n\n----------------------------------------\n\nTITLE: Using Metadata in Detections\nDESCRIPTION: Demonstrates how to use the new metadata feature to store custom per-image data in Detections objects\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8m\")\n\nresult = model(\"image.png\")[0]\ndetections = sv.Detections.from_ultralytics(result)\n\n# Items in `data` must match length of detections\nobject_ids = [num for num in range(len(detections))]\ndetections.data[\"object_number\"] = object_ids\n\n# Items in `metadata` can be of any length.\ndetections.metadata[\"camera_model\"] = \"Luxonis OAK-D\"\n```\n\n----------------------------------------\n\nTITLE: Using CSVSink for Saving Detection Results in CSV Format with Supervision\nDESCRIPTION: Demonstrates how to use sv.CSVSink to save inference results from a YOLO model to a CSV file. The code processes video frames, generates detections, and appends them to a CSV file with optional custom data.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(<SOURCE_MODEL_PATH>)\ncsv_sink = sv.CSVSink(<RESULT_CSV_FILE_PATH>)\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nwith csv_sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        csv_sink.append(detections, custom_data={<CUSTOM_LABEL>:<CUSTOM_DATA>})\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision via poetry\nDESCRIPTION: Poetry package manager installation command for the Supervision package.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npoetry add supervision\n```\n\n----------------------------------------\n\nTITLE: Loading Ultralytics Results into Supervision in Python\nDESCRIPTION: This snippet demonstrates how to load object detection results from the Ultralytics package into Supervision using the from_ultralytics method.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n```\n\n----------------------------------------\n\nTITLE: Using PolygonAnnotator in Python with Supervision\nDESCRIPTION: Demonstrates how to use the new PolygonAnnotator to annotate images and videos with segmentation mask outlines. It initializes the annotator and applies it to an image with detections.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> image = ...\n>>> detections = sv.Detections(...)\n\n>>> polygon_annotator = sv.PolygonAnnotator()\n>>> annotated_frame = polygon_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n```\n\n----------------------------------------\n\nTITLE: Merging COCO Format Datasets with Supervision\nDESCRIPTION: Python code showing how to merge train, validation, and test datasets loaded from COCO format into a single dataset using Supervision's merge method.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/train',\n    annotations_path=f'{dataset.location}/train/_annotations.coco.json',\n)\nds_valid = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/valid',\n    annotations_path=f'{dataset.location}/valid/_annotations.coco.json',\n)\nds_test = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/test',\n    annotations_path=f'{dataset.location}/test/_annotations.coco.json',\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n\nds = sv.DetectionDataset.merge([ds_train, ds_valid, ds_test])\n\nds.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds)\n# 1000\n```\n\n----------------------------------------\n\nTITLE: Parsing LMM Results with PaliGemma in Python\nDESCRIPTION: Demonstrates how to use the new from_lmm method to parse Large Multimodal Model (LMM) text results into Detections objects, specifically with PaliGemma output. The code shows creating detections from a text description containing bounding box coordinates and class.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\npaligemma_result = \"<loc0256><loc0256><loc0768><loc0768> cat\"\ndetections = sv.Detections.from_lmm(\n    sv.LMM.PALIGEMMA,\n    paligemma_result,\n    resolution_wh=(1000, 1000),\n    classes=[\"cat\", \"dog\"],\n)\ndetections.xyxy\n# array([[250., 250., 750., 750.]])\n\ndetections.class_id\n# array([0])\n```\n\n----------------------------------------\n\nTITLE: Baseline Object Detection with Ultralytics\nDESCRIPTION: Implementation of object detection using the Ultralytics YOLOv8 implementation. This shows how to load a YOLO model, run inference on an image, and annotate the results using Supervision's visualization tools.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Loading YOLO Format Detection Dataset with Supervision\nDESCRIPTION: Python code to load train, validation, and test datasets from YOLO format using Supervision's DetectionDataset class. Shows how to access class names and dataset lengths.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds_train = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_valid = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_test = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n```\n\n----------------------------------------\n\nTITLE: Calculate Optimal Line Thickness Function Reference\nDESCRIPTION: Function interface for calculating optimal line thickness\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.calculate_optimal_line_thickness\n```\n\n----------------------------------------\n\nTITLE: Loading Transformers Results into Supervision in Python\nDESCRIPTION: This code shows how to load object detection results from the Transformers package into Supervision using the from_transformers method.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n```\n\n----------------------------------------\n\nTITLE: Using InferenceSlicer with YOLO in Python\nDESCRIPTION: Demonstrates how to use the InferenceSlicer class with a YOLO model to perform inference on image slices.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n>>> import cv2\n>>> import supervision as sv\n>>> from ultralytics import YOLO\n\n>>> image = cv2.imread(SOURCE_IMAGE_PATH)\n>>> model = YOLO(...)\n\n>>> def callback(image_slice: np.ndarray) -> sv.Detections:\n...     result = model(image_slice)[0]\n...     return sv.Detections.from_ultralytics(result)\n\n>>> slicer = sv.InferenceSlicer(callback = callback)\n\n>>> detections = slicer(image)\n```\n\n----------------------------------------\n\nTITLE: Loading COCO Format Detection Dataset with Supervision\nDESCRIPTION: Python code to load train, validation, and test datasets from COCO format using Supervision's DetectionDataset class. Shows how to access class names and dataset lengths.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/train',\n    annotations_path=f'{dataset.location}/train/_annotations.coco.json',\n)\nds_valid = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/valid',\n    annotations_path=f'{dataset.location}/valid/_annotations.coco.json',\n)\nds_test = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/test',\n    annotations_path=f'{dataset.location}/test/_annotations.coco.json',\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Confidence Threshold in Python\nDESCRIPTION: This method filters the Detections object based on a confidence threshold, returning a new Detections object with only the detections above the threshold.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/core.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef filter(self, confidence_threshold: float = 0.5) -> Detections:\n```\n\n----------------------------------------\n\nTITLE: Overlaying Images in Python using Supervision\nDESCRIPTION: Function to overlay one image on top of another. It takes a background image, overlay image, and coordinates as input and returns the combined image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/image.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.image.overlay_image\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Detections Objects in Python\nDESCRIPTION: This class method combines multiple Detections objects into a single Detections object, concatenating their attributes.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/core.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef merge(cls, detections: List[Detections]) -> Detections:\n```\n\n----------------------------------------\n\nTITLE: Metrics Documentation Structure in Markdown\nDESCRIPTION: Basic markdown structure defining documentation for common metric values, including MetricTarget and AveragingMethod enumerations.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/common_values.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ncomments: true\nstatus: new\n---\n\n# Common Values\n\nThis page contains supplementary values, types and enums that metrics use.\n\n<div class=\"md-typeset\">\n    <h2><a href=\"#supervision.metrics.core.MetricTarget\">MetricTarget</a></h2>\n</div>\n\n:::supervision.metrics.core.MetricTarget\n\n<div class=\"md-typeset\">\n    <h2><a href=\"#supervision.metrics.core.AveragingMethod\">AveragingMethod</a></h2>\n</div>\n\n:::supervision.metrics.core.AveragingMethod\n```\n\n----------------------------------------\n\nTITLE: Using F1 Score Metric in Python with Supervision\nDESCRIPTION: Demonstrates how to use the new F1 score metric for detection and segmentation evaluations. It shows initialization of the metric, updating with predictions and targets, and computing the result.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom supervision.metrics import F1Score\n\npredictions = sv.Detections(...)\ntargets = sv.Detections(...)\n\nf1_metric = F1Score()\nf1_result = f1_metric.update(predictions, targets).compute()\n\nprint(f1_result)\nprint(f1_result.f1_50)\nprint(f1_result.small_objects.f1_50)\n```\n\n----------------------------------------\n\nTITLE: Remapping Classes for Object Detection in Python\nDESCRIPTION: This function remaps class IDs and names in detection results to match the dataset classes. It takes a Detections object, dictionaries for ID and name mapping, and modifies the detections in-place.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef remap_classes(\n    detections: sv.Detections,\n    class_ids_from_to: dict[int, int],\n    class_names_from_to: dict[str, str]\n) -> None:\n    new_class_ids = [\n        class_ids_from_to.get(class_id, class_id) for class_id in detections.class_id]\n    detections.class_id = np.array(new_class_ids)\n\n    new_class_names = [\n        class_names_from_to.get(name, name) for name in detections[\"class_name\"]]\n    predictions[\"class_name\"] = np.array(new_class_names)\n```\n\n----------------------------------------\n\nTITLE: Converting Detection Dataset Between Formats\nDESCRIPTION: Example demonstrating how to convert a detection dataset from one format to another, specifically from YOLO format to Pascal VOC format using Supervision's utilities.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsv.DetectionDataset.from_yolo(\n    images_directory_path=...,\n    annotations_directory_path=...,\n    data_yaml_path=...\n).as_pascal_voc(\n    images_directory_path=...,\n    annotations_directory_path=...\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Detections by Relative Area in Supervision\nDESCRIPTION: Demonstrates how to filter object detections based on the percentage of the image they occupy. This example removes large detections that cover more than 80% of the image area.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/filter_detections.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\nheight, width, channels = image.shape\nimage_area = height * width\n\ndetections = sv.Detections(...)\ndetections = detections[(detections.area / image_area) < 0.8]\n```\n\n----------------------------------------\n\nTITLE: Filtering Polygons by Area in Python\nDESCRIPTION: Function to filter polygons based on their area. Useful for removing small or large polygons that may represent noise or unwanted detections.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.filter_polygons_by_area\n```\n\n----------------------------------------\n\nTITLE: Applying Non-Max Suppression to Bounding Boxes in Python\nDESCRIPTION: The box_non_max_suppression function applies non-maximum suppression to filter overlapping bounding boxes in object detection results. It likely takes detection boxes, scores, and an overlap threshold as input and returns filtered detections.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/double_detection_filter.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.overlap_filter.box_non_max_suppression\n```\n\n----------------------------------------\n\nTITLE: Defining PrecisionResult Class in Python for Supervision Library\nDESCRIPTION: This snippet defines the PrecisionResult class in the supervision.metrics.precision module. It is likely used to represent the results of precision calculations in a structured format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/precision.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.metrics.precision.PrecisionResult\n```\n\n----------------------------------------\n\nTITLE: Including License Content in Markdown\nDESCRIPTION: This snippet uses a markdown-specific syntax to include the contents of the LICENSE.md file. It employs the --8<-- syntax, which is often used in MkDocs for content insertion.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/license.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n--8<-- \"LICENSE.md\"\n```\n\n----------------------------------------\n\nTITLE: Converting XYWH to XYXY Format in Python\nDESCRIPTION: Function to convert bounding boxes from XYWH (center, width, height) format to XYXY (top-left, bottom-right) format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.xywh_to_xyxy\n```\n\n----------------------------------------\n\nTITLE: Scaling Bounding Boxes in Python\nDESCRIPTION: Function to scale bounding boxes by a specified factor. Useful for resizing detection results or data augmentation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.scale_boxes\n```\n\n----------------------------------------\n\nTITLE: Managing Image Output in Python using Supervision's ImageSink\nDESCRIPTION: Class for managing image output in various formats. It provides methods for saving images to files or streams, supporting different output types.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/image.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.image.ImageSink\n```\n\n----------------------------------------\n\nTITLE: Performing Non-Max Suppression on Instance Segmentation Masks in Python\nDESCRIPTION: The mask_non_max_suppression function applies non-maximum suppression to filter overlapping instance segmentation masks. It likely takes mask predictions, scores, and an overlap threshold as input and returns filtered mask results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/double_detection_filter.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.overlap_filter.mask_non_max_suppression\n```\n\n----------------------------------------\n\nTITLE: Seeking to Specific Video Frame using Supervision's Video Utility\nDESCRIPTION: Shows how to use the new iterative seek method in Supervision's video utility to reliably seek to a specific frame in a video, especially useful when traditional seek methods fail.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nfor frame in sv.get_video_frames_generator(\n    source_path=<SOURCE_VIDEO_PATH>,\n    start=60,\n    iterative_seek=True\n):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Merging Overlapping Bounding Boxes in Python\nDESCRIPTION: The box_non_max_merge function merges overlapping bounding boxes in object detection results. It likely takes detection boxes, scores, and a merge threshold as input and returns merged bounding box results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/double_detection_filter.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.overlap_filter.box_non_max_merge\n```\n\n----------------------------------------\n\nTITLE: Converting XCYCWH to XYXY Format in Python\nDESCRIPTION: Function to convert bounding boxes from XCYCWH (center x, center y, width, height) format to XYXY (top-left, bottom-right) format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.xcycwh_to_xyxy\n```\n\n----------------------------------------\n\nTITLE: Translating Masks in Python\nDESCRIPTION: Function to move binary masks by a specified offset. Useful for adjusting segmentation results or data augmentation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.move_masks\n```\n\n----------------------------------------\n\nTITLE: Saving Detection Dataset in Pascal VOC Format with Supervision in Python\nDESCRIPTION: This snippet shows how to save a detection dataset in Pascal VOC format using Supervision's DetectionDataset.as_pascal_voc method. It requires specifying the image directory path and annotations directory path.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds = sv.DetectionDataset(...)\n\nds.as_pascal_voc(\n    images_directory_path='<IMAGE_DIRECTORY_PATH>',\n    annotations_directory_path='<ANNOTATIONS_DIRECTORY_PATH>'\n)\n```\n\n----------------------------------------\n\nTITLE: Clipping Bounding Boxes in Python\nDESCRIPTION: Function to clip bounding boxes to fit within specified image dimensions. Ensures that boxes do not extend beyond the image boundaries.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.clip_boxes\n```\n\n----------------------------------------\n\nTITLE: Using PercentageBarAnnotator for Confidence Visualization with Supervision\nDESCRIPTION: Demonstrates how to use sv.PercentageBarAnnotator to annotate images and videos with percentage bars representing confidence values or other custom properties of detections.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> image = ...\n>>> detections = sv.Detections(...)\n\n>>> percentage_bar_annotator = sv.PercentageBarAnnotator()\n>>> annotated_frame = percentage_bar_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n```\n\n----------------------------------------\n\nTITLE: Calculating Batch IoU for Bounding Boxes in Python\nDESCRIPTION: Function to calculate Intersection over Union (IoU) for batches of bounding boxes. Useful for comparing multiple detections efficiently.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.box_iou_batch\n```\n\n----------------------------------------\n\nTITLE: Extracting Bounding Box from Mask in Python\nDESCRIPTION: Function to extract the bounding box (in xyxy format) from a binary mask. Helpful for obtaining a rectangular region of interest from a segmentation mask.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.mask_to_xyxy\n```\n\n----------------------------------------\n\nTITLE: Generating Video Frames in Python\nDESCRIPTION: The get_video_frames_generator function probably creates a generator that yields individual frames from a video source. This can be useful for processing video frames one at a time without loading the entire video into memory.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/video.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.video.get_video_frames_generator\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving ClassificationDataset in Folder Structure Format\nDESCRIPTION: Illustrates how to load a ClassificationDataset from a folder structure and save it back to a folder structure using the new from_folder_structure and as_folder_structure methods.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> cs = sv.ClassificationDataset.from_folder_structure(\n...     root_directory_path='...'\n... )\n\n>>> cs.as_folder_structure(\n...     root_directory_path='...'\n... )\n```\n\n----------------------------------------\n\nTITLE: Merging Pascal VOC Format Datasets with Supervision\nDESCRIPTION: Python code showing how to merge train, validation, and test datasets loaded from Pascal VOC format into a single dataset using Supervision's merge method.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds_train = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels'\n)\nds_valid = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels'\n)\nds_test = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n\nds = sv.DetectionDataset.merge([ds_train, ds_valid, ds_test])\n\nds.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds)\n# 1000\n```\n\n----------------------------------------\n\nTITLE: Computing Recall Metric\nDESCRIPTION: Shows how to use the new Recall metric for comparing model predictions against ground truth\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom supervision.metrics import Recall\n\npredictions = sv.Detections(...)\ntargets = sv.Detections(...)\n\nrecall_metric = Recall()\nrecall_result = recall_metric.update(predictions, targets).compute()\n\nrecall_result.plot()\n```\n\n----------------------------------------\n\nTITLE: Computing Batch IoU for Masks in Python\nDESCRIPTION: Function to compute Intersection over Union (IoU) for batches of masks. Essential for comparing segmentation results in bulk.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.mask_iou_batch\n```\n\n----------------------------------------\n\nTITLE: Using ColorAnnotator for Color-coded Object Detection\nDESCRIPTION: This snippet illustrates the use of ColorAnnotator to apply color-coded annotations to detected objects. It requires an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncolor_annotator = sv.ColorAnnotator()\nannotated_frame = color_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Resizing Images in Python using Supervision\nDESCRIPTION: Function to resize an image to a specified size. It takes an image and target dimensions as input and returns the resized image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/image.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.image.resize_image\n```\n\n----------------------------------------\n\nTITLE: Selecting Detections by Index, List, or Slice\nDESCRIPTION: Demonstrates the new ability to select Detections by index, list of indexes, or slice, providing more flexible access to detection data.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> detections = sv.Detections(...)\n>>> len(detections[0])\n1\n>>> len(detections[[0, 1]])\n2\n>>> len(detections[0:2])\n2\n```\n\n----------------------------------------\n\nTITLE: Applying BlurAnnotator for Object Anonymization\nDESCRIPTION: This snippet demonstrates how to use BlurAnnotator to blur detected objects for anonymization purposes. It requires an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nblur_annotator = sv.BlurAnnotator()\nannotated_frame = blur_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Defining VideoInfo Class in Python\nDESCRIPTION: The VideoInfo class likely contains metadata and information about a video file. It may include attributes such as resolution, frame rate, duration, and codec information.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/video.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.video.VideoInfo\n```\n\n----------------------------------------\n\nTITLE: Cropping Images in Python using Supervision\nDESCRIPTION: Function to crop an image based on specified coordinates. It takes an image and bounding box as input and returns the cropped portion of the image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/image.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.image.crop_image\n```\n\n----------------------------------------\n\nTITLE: Letterboxing Images in Python using Supervision\nDESCRIPTION: Function to apply letterboxing to an image, maintaining aspect ratio. It takes an image and target size as input and returns the letterboxed image.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/image.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.image.letterbox_image\n```\n\n----------------------------------------\n\nTITLE: Using LabelAnnotator for Text Labels on Detected Objects\nDESCRIPTION: This code demonstrates how to use LabelAnnotator to add text labels to detected objects. It takes an image, detections, and custom labels as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\nannotated_frame = label_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n```\n\n----------------------------------------\n\nTITLE: Using EdgeAnnotator for Keypoint Visualization in Python\nDESCRIPTION: Shows how to use the EdgeAnnotator to visualize keypoint edges in a skeleton with custom color and thickness. This annotator connects keypoints with lines according to a predefined skeleton structure.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nedge_annotator = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n)\nannotated_frame = edge_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n```\n\n----------------------------------------\n\nTITLE: Using VertexLabelAnnotator to label keypoints in Supervision\nDESCRIPTION: This snippet demonstrates how to use the VertexLabelAnnotator class to add text labels to keypoints on an image. It initializes the annotator with a green background color, black text color, and 5-pixel border radius, then applies it to an image with provided keypoints.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/keypoint/annotators.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nvertex_label_annotator = sv.VertexLabelAnnotator(\n    color=sv.Color.GREEN,\n    text_color=sv.Color.BLACK,\n    border_radius=5\n)\nannotated_frame = vertex_label_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n```\n\n----------------------------------------\n\nTITLE: Applying HaloAnnotator for Halo Effect on Detected Objects\nDESCRIPTION: This code shows how to use HaloAnnotator to create a halo effect around detected objects. It takes an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nhalo_annotator = sv.HaloAnnotator()\nannotated_frame = halo_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using BoxAnnotator in Supervision\nDESCRIPTION: This snippet demonstrates how to initialize a BoxAnnotator and use it to draw bounding boxes around detected objects in an image. It requires an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Loading YOLO Dataset and Splitting DetectionDataset\nDESCRIPTION: Demonstrates loading a YOLO format dataset using Roboflow, converting it to a DetectionDataset, and then splitting it into train and test sets.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_45\n\nLANGUAGE: python\nCODE:\n```\n>>> import roboflow\n>>> from roboflow import Roboflow\n>>> import supervision as sv\n\n>>> roboflow.login()\n\n>>> rf = Roboflow()\n\n>>> project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n>>> dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n>>> ds = sv.DetectionDataset.from_yolo(\n...     images_directory_path=f\"{dataset.location}/train/images\",\n...     annotations_directory_path=f\"{dataset.location}/train/labels\",\n...     data_yaml_path=f\"{dataset.location}/data.yaml\"\n... )\n\n>>> ds.classes\n['dog', 'person']\n\n>>> train_ds, test_ds = ds.split(split_ratio=0.7, random_state=42, shuffle=True)\n\n>>> len(train_ds), len(test_ds)\n(700, 300)\n```\n\n----------------------------------------\n\nTITLE: Implementing PixelateAnnotator for Object Pixelation\nDESCRIPTION: This code illustrates how to use PixelateAnnotator to pixelate detected objects. It takes an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npixelate_annotator = sv.PixelateAnnotator()\nannotated_frame = pixelate_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Roboflow Supervision\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for the Roboflow Supervision project. It includes specific versions for inference and supervision, as well as constraints for ultralytics.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/tracking/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ninference==0.9.17\nsupervision==0.19.0\ntqdm\nultralytics<=8.3.40\n```\n\n----------------------------------------\n\nTITLE: Listing Files with Specific Extensions in Python\nDESCRIPTION: This function retrieves a list of files with specified extensions from a given directory. It supports searching in subdirectories and can handle multiple file extensions.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/file.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef list_files_with_extensions(directory: str, extensions: Union[str, List[str]], recursive: bool = True) -> List[str]:\n    \"\"\"Lists all files in a directory with specified extensions.\n\n    Args:\n        directory (str): The directory to search for files.\n        extensions (Union[str, List[str]]): File extension(s) to filter by. Can be a single string or a list of strings.\n        recursive (bool, optional): Whether to search recursively in subdirectories. Defaults to True.\n\n    Returns:\n        List[str]: A list of file paths matching the specified extensions.\n    \"\"\"\n    if isinstance(extensions, str):\n        extensions = [extensions]\n\n    extensions = [ext.lower() for ext in extensions]\n\n    file_list = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if any(file.lower().endswith(ext) for ext in extensions):\n                file_list.append(os.path.join(root, file))\n        if not recursive:\n            break\n\n    return file_list\n```\n\n----------------------------------------\n\nTITLE: Increasing Input Resolution with Ultralytics\nDESCRIPTION: Improved small object detection by setting a higher image size (1280) during inference with Ultralytics. This configuration parameter allows processing at higher resolution without changing the base model.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model(image, imgsz=1280)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n```\n\n----------------------------------------\n\nTITLE: Implementing CircleAnnotator for Circular Object Annotations\nDESCRIPTION: This code demonstrates how to use CircleAnnotator to draw circular annotations around detected objects. It takes an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncircle_annotator = sv.CircleAnnotator()\nannotated_frame = circle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Using IconAnnotator for Custom Icon Annotations\nDESCRIPTION: This code shows how to use IconAnnotator to add custom icon annotations to detected objects. It takes an image, detections, and icon paths as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nicon_paths = [\n    \"<ICON_PATH>\"\n    for _ in detections\n]\n\nicon_annotator = sv.IconAnnotator()\nannotated_frame = icon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    icon_path=icon_paths\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing EllipseAnnotator for Elliptical Object Annotations\nDESCRIPTION: This snippet illustrates how to use EllipseAnnotator to draw elliptical annotations around detected objects. It requires an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nellipse_annotator = sv.EllipseAnnotator()\nannotated_frame = ellipse_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision via pip\nDESCRIPTION: Standard pip installation command for the Supervision package. Requires Python 3.8 or higher.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install supervision\n```\n\n----------------------------------------\n\nTITLE: Saving Detections to JSON using Supervision\nDESCRIPTION: The JSONSink class from supervision.detection.tools.json_sink module is used to save detection results in JSON format. It provides options for customizing the output file and data structure.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/tools/save_detections.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.tools.json_sink.JSONSink\n```\n\n----------------------------------------\n\nTITLE: Configuring Albumentations for Dataset Augmentation in Python\nDESCRIPTION: This snippet demonstrates how to configure Albumentations for dataset augmentation. It creates a composition of image transformations including perspective change, horizontal flip, and random brightness/contrast adjustment, with bounding box parameters set for Pascal VOC format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport albumentations as A\n\naugmentation = A.Compose(\n    transforms=[\n        A.Perspective(p=0.1),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5)\n    ],\n    bbox_params=A.BboxParams(\n        format='pascal_voc',\n        label_fields=['category']\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Applying DotAnnotator for Minimal Object Detection Markers\nDESCRIPTION: This snippet shows how to use DotAnnotator to place small dot markers on detected objects. It requires an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ndot_annotator = sv.DotAnnotator()\nannotated_frame = dot_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Applying PolygonAnnotator for Polygon-shaped Object Annotations\nDESCRIPTION: This snippet shows how to use PolygonAnnotator to draw polygon-shaped annotations around detected objects. It requires an image and detections with polygon information as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npolygon_annotator = sv.PolygonAnnotator()\nannotated_frame = polygon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Using HaloAnnotator in Python with Supervision\nDESCRIPTION: Illustrates the usage of the new HaloAnnotator to add a halo effect to detections in images and videos. It initializes the annotator and applies it to an image with detections.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> image = ...\n>>> detections = sv.Detections(...)\n\n>>> halo_annotator = sv.HaloAnnotator()\n>>> annotated_frame = halo_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Speed Estimation Example\nDESCRIPTION: Command to run the speed estimation example using the Ultralytics YOLOv8 model, specifying input and output video paths and detection thresholds.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ultralytics_example.py \\\n    --source_video_path data/vehicles.mp4 \\\n    --target_video_path data/vehicles-result.mp4 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5\n```\n\n----------------------------------------\n\nTITLE: Using TriangleAnnotator for Triangle-shaped Object Markers\nDESCRIPTION: This code demonstrates the use of TriangleAnnotator to place triangle-shaped markers on detected objects. It takes an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ntriangle_annotator = sv.TriangleAnnotator()\nannotated_frame = triangle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Using EdgeAnnotator to connect keypoints in Supervision\nDESCRIPTION: This snippet shows how to use the EdgeAnnotator class to draw lines between keypoints on an image. It initializes the annotator with a green color and 5-pixel thickness, then applies it to an image with provided keypoints.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/keypoint/annotators.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nedge_annotator = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n)\nannotated_frame = edge_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n```\n\n----------------------------------------\n\nTITLE: Saving Multiple Images Using ImageSink\nDESCRIPTION: Shows how to use the new ImageSink class to conveniently save multiple images into a directory, in this case saving every tenth video frame.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> with sv.ImageSink(target_dir_path='target/directory/path') as sink:\n...     for image in sv.get_video_frames_generator(source_path='source_video.mp4', stride=10):\n...         sink.save_image(image=image)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection with Transformers\nDESCRIPTION: Illustrates object detection using Hugging Face's DETR model and converting results to Supervision's Detection format.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/save_detections.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nframes_generator = sv.get_video_frames_generator(<SOURCE_VIDEO_PATH>)\n\nfor frame in frames_generator:\n\n    frame = sv.cv2_to_pillow(frame)\n    inputs = processor(images=frame, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    width, height = frame.size\n    target_size = torch.tensor([[height, width]])\n    results = processor.post_process_object_detection(\n        outputs=outputs, target_sizes=target_size)[0]\n    detections = sv.Detections.from_transformers(results)\n```\n\n----------------------------------------\n\nTITLE: Running Roboflow Inference Example\nDESCRIPTION: Command to run the traffic analysis using the Roboflow Inference API. It requires an API key and specifies video source, confidence threshold, IoU threshold, and output video path.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython inference_example.py \\\n    --roboflow_api_key <ROBOFLOW API KEY> \\\n    --source_video_path data/traffic_analysis.mov \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5 \\\n    --target_video_path data/traffic_analysis_result.mov\n```\n\n----------------------------------------\n\nTITLE: Converting RLE to Mask - Python Supervision Framework\nDESCRIPTION: Function reference for converting Run Length Encoding (RLE) format to binary mask representation. Used in computer vision tasks for efficient mask storage and manipulation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/datasets/utils.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsupervision.dataset.utils.rle_to_mask\n```\n\n----------------------------------------\n\nTITLE: Merging YOLO Format Datasets with Supervision\nDESCRIPTION: Python code showing how to merge train, validation, and test datasets loaded from YOLO format into a single dataset using Supervision's merge method.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds_train = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_valid = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_test = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n\nds = sv.DetectionDataset.merge([ds_train, ds_valid, ds_test])\n\nds.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds)\n# 1000\n```\n\n----------------------------------------\n\nTITLE: Applying BoxCornerAnnotator for Corner Bounding Boxes\nDESCRIPTION: This code shows how to use the BoxCornerAnnotator to draw corner-style bounding boxes around detected objects. It takes an image and detections as input.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncorner_annotator = sv.BoxCornerAnnotator()\nannotated_frame = corner_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Inference Results into Supervision in Python\nDESCRIPTION: This code shows how to load object detection results from the Inference package into Supervision using the from_inference method.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection on Video Stream (Bash)\nDESCRIPTION: Script to perform object detection on a video stream using the Roboflow Inference model, specifying zone configuration, RTSP URL, and model parameters.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython inference_stream_example.py \\\n    --zone_configuration_path \"data/checkout/config.json\" \\\n    --rtsp_url \"rtsp://localhost:8554/live0.stream\" \\\n    --model_id \"yolov8x-640\" \\\n    --classes 0 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.7\n```\n\n----------------------------------------\n\nTITLE: Converting Mask to RLE - Python Supervision Framework\nDESCRIPTION: Function reference for converting binary mask representation to Run Length Encoding (RLE) format. Enables efficient storage and transmission of mask data in computer vision applications.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/datasets/utils.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsupervision.dataset.utils.mask_to_rle\n```\n\n----------------------------------------\n\nTITLE: Splitting Datasets with Supervision\nDESCRIPTION: Python code demonstrating how to split a detection dataset into training, validation, and test sets using Supervision's split method with random shuffling.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\nds = sv.DetectionDataset(...)\n\nlen(ds)\n# 1000\n\nds_train, ds = ds.split(split_ratio=0.8, shuffle=True)\nds_valid, ds_test = ds.split(split_ratio=0.5, shuffle=True)\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n```\n\n----------------------------------------\n\nTITLE: Padding Bounding Boxes in Python\nDESCRIPTION: Function to add padding to bounding boxes. Useful for expanding the region of interest around detected objects.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.pad_boxes\n```\n\n----------------------------------------\n\nTITLE: MeanAveragePrecisionResult Data Structure\nDESCRIPTION: Data structure for storing and representing Mean Average Precision calculation results\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/mean_average_precision.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsupervision.metrics.mean_average_precision.MeanAveragePrecisionResult\n```\n\n----------------------------------------\n\nTITLE: Downloading YOLO Format Dataset from Roboflow\nDESCRIPTION: Python code to log into Roboflow and download a dataset in YOLO format. Requires workspace ID, project ID, and project version to be specified.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.workspace('<WORKSPACE_ID>').project('<PROJECT_ID>')\ndataset = project.version('<PROJECT_VERSION>').download(\"yolov8\")\n```\n\n----------------------------------------\n\nTITLE: Defining Precision Class in Python for Supervision Library\nDESCRIPTION: This snippet defines the Precision class in the supervision.metrics.precision module. It is likely used to calculate precision metrics for machine learning or computer vision tasks.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/precision.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.metrics.precision.Precision\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset from Roboflow\nDESCRIPTION: Use the Roboflow library to download a dataset for benchmarking. This example uses the Corgi v2 dataset.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom roboflow import Roboflow\n\nrf = Roboflow(api_key=\"<YOUR_API_KEY>\")\nproject = rf.workspace(\"fbamse1-gm2os\").project(\"corgi-v2\")\ndataset = project.version(4).download(\"yolov11\")\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection with Transformers Package in Python\nDESCRIPTION: This snippet illustrates object detection using the Transformers package. It loads a DETR model, processes an image, and performs inference.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_and_annotate.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\n```\n\n----------------------------------------\n\nTITLE: Defining RecallResult Class in Python for Supervision Framework\nDESCRIPTION: This snippet defines the RecallResult class in the supervision.metrics.recall module. It is likely a data structure used to store and represent the results of recall calculations.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/recall.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.metrics.recall.RecallResult\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision Package with Assets Support in Python\nDESCRIPTION: This snippet shows how to install the Supervision package with assets support using pip. The '-q' flag is used for a quiet installation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/notebooks/download-supervision-assets.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -q \"supervision[assets]\"\n```\n\n----------------------------------------\n\nTITLE: Draw Rectangle Function Reference\nDESCRIPTION: Function interface for drawing rectangles on images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_rectangle\n```\n\n----------------------------------------\n\nTITLE: Merging Multiple DetectionDatasets\nDESCRIPTION: Shows how to merge multiple DetectionDataset objects using the new merge method, combining their data and class lists.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> ds_1 = sv.DetectionDataset(...)\n>>> len(ds_1)\n100\n>>> ds_1.classes\n['dog', 'person']\n\n>>> ds_2 = sv.DetectionDataset(...)\n>>> len(ds_2)\n200\n>>> ds_2.classes\n['cat']\n\n>>> ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n>>> len(ds_merged)\n300\n>>> ds_merged.classes\n['cat', 'dog', 'person']\n```\n\n----------------------------------------\n\nTITLE: Detecting Holes in Polygons in Python\nDESCRIPTION: Function to check if a polygon contains holes. Useful for analyzing the topology of complex shapes in detection results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.contains_holes\n```\n\n----------------------------------------\n\nTITLE: Loading Deployed Model with Inference\nDESCRIPTION: Load a deployed model from Roboflow using the Inference library.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom inference import get_model\n\nmodel_id = \"<PROJECT_NAME>/<MODEL_VERSION>\"\nmodel = get_model(model_id=model_id)\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision via mamba\nDESCRIPTION: Mamba package manager installation command for the Supervision package from conda-forge channel.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmamba install -c conda-forge supervision\n```\n\n----------------------------------------\n\nTITLE: Draw Line Function Reference\nDESCRIPTION: Function interface for drawing lines on images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_line\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection on Video File (Bash)\nDESCRIPTION: Script to perform object detection on a video file using the Roboflow Inference model, specifying zone configuration, model parameters, and class filters.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython inference_file_example.py \\\n    --zone_configuration_path \"data/checkout/config.json\" \\\n    --source_video_path \"data/checkout/video.mp4\" \\\n    --model_id \"yolov8x-640\" \\\n    --classes 0 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.7\n```\n\n----------------------------------------\n\nTITLE: Smart Label Positioning\nDESCRIPTION: Shows how to use smart label positioning to avoid label overlaps in annotations\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(\"image.jpg\")\n\nlabel_annotator = sv.LabelAnnotator(smart_position=True)\n\nmodel = YOLO(\"yolo11m.pt\")\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nannotated_frame = label_annotator.annotate(first_frame.copy(), detections)\nsv.plot_image(annotated_frame)\n```\n\n----------------------------------------\n\nTITLE: Detecting Multiple Segments in Polygons in Python\nDESCRIPTION: Function to check if a polygon contains multiple segments. Useful for identifying disconnected or complex shapes in detection results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.contains_multiple_segments\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Model with Inference\nDESCRIPTION: Load a pre-trained model using the Inference library. This example uses the YOLOv11s-seg-640 model.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov11s-seg-640\")\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision Project Dependencies with UV\nDESCRIPTION: Command to install all required project dependencies using the UV package manager, including development, documentation, and metrics extras.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuv pip install -r pyproject.toml --extra dev --extra docs --extra metrics\n```\n\n----------------------------------------\n\nTITLE: Importing get_polygon_center Function from Supervision Geometry Utils\nDESCRIPTION: This snippet shows how to import the 'get_polygon_center' function from the Supervision library's geometry utilities module. This function is likely used to calculate the center point of a given polygon.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/geometry.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.geometry.utils.get_polygon_center\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection on Video File with Ultralytics YOLOv8 (Bash)\nDESCRIPTION: Script to perform object detection on a video file using the Ultralytics YOLOv8 model, specifying zone configuration, model weights, and detection parameters.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython ultralytics_file_example.py \\\n    --zone_configuration_path \"data/checkout/config.json\" \\\n    --source_video_path \"data/checkout/video.mp4\" \\\n    --weights \"yolov8x.pt\" \\\n    --device \"cpu\" \\\n    --classes 0 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.7\n```\n\n----------------------------------------\n\nTITLE: Processing Video in Python\nDESCRIPTION: The process_video function likely provides a high-level interface for applying operations or transformations to a video. It may handle tasks such as reading frames, applying processing functions, and writing output.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/video.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.video.process_video\n```\n\n----------------------------------------\n\nTITLE: Running Model Predictions with Ultralytics\nDESCRIPTION: Iterate through a dataset, run model predictions, and store results using the Ultralytics library and Supervision.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\n\ntest_set = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/test/images\",\n    annotations_directory_path=f\"{dataset.location}/test/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n\nimage_paths = []\npredictions_list = []\ntargets_list = []\n\nfor image_path, image, label in test_set:\n    result = model(image)[0]\n    predictions = sv.Detections.from_ultralytics(result)\n\n    image_paths.append(image_path)\n    predictions_list.append(predictions)\n    targets_list.append(label)\n```\n\n----------------------------------------\n\nTITLE: Importing DetectionDataset class from Supervision\nDESCRIPTION: This snippet shows how to import the DetectionDataset class from the Supervision library's dataset.core module. The DetectionDataset is used for handling object detection datasets.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/datasets/core.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.dataset.core import DetectionDataset\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for isolation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/tracking/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Downloading Video Assets for Keypoint Detection in Python\nDESCRIPTION: This code snippet demonstrates how to download video assets for keypoint detection using the Supervision library's assets module.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.assets import download_assets, VideoAssets\n\ndownload_assets(VideoAssets.SKIING)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection on Video Stream with Ultralytics YOLOv8 (Bash)\nDESCRIPTION: Script to perform object detection on a video stream using the Ultralytics YOLOv8 model, specifying zone configuration, RTSP URL, and model parameters.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython ultralytics_stream_example.py \\\n    --zone_configuration_path \"data/checkout/config.json\" \\\n    --rtsp_url \"rtsp://localhost:8554/live0.stream\" \\\n    --weights \"yolov8x.pt\" \\\n    --device \"cpu\" \\\n    --classes 0 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.7\n```\n\n----------------------------------------\n\nTITLE: Translating Bounding Boxes in Python\nDESCRIPTION: Function to move bounding boxes by a specified offset. Useful for adjusting detection results or data augmentation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/utils.md#2025-04-21_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nsupervision.detection.utils.move_boxes\n```\n\n----------------------------------------\n\nTITLE: Loading Model with Ultralytics\nDESCRIPTION: Load a YOLO model using the Ultralytics library. This example uses the YOLOv11s-seg model.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11s-seg.pt\")\n```\n\n----------------------------------------\n\nTITLE: Importing Position Class from Supervision Geometry Core\nDESCRIPTION: This snippet demonstrates the import of the 'Position' class from the Supervision library's geometry core module. The Position class is likely used to represent and manipulate 2D or 3D positions in geometric calculations.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/geometry.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.geometry.core.Position\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install all the necessary Python packages listed in requirements.txt file. This ensures all dependencies needed for the traffic analysis are properly installed.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: ColorPalette Class Reference\nDESCRIPTION: Class interface for managing color palettes\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.color.ColorPalette\n```\n\n----------------------------------------\n\nTITLE: Running Inference Example with Roboflow\nDESCRIPTION: Command to run the cloud-based inference script using Roboflow's API, specifying the API key, zone configuration, video source, and detection thresholds.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython inference_example.py \\\n    --roboflow_api_key <ROBOFLOW API KEY> \\\n    --zone_configuration_path data/multi-zone-config.json \\\n    --source_video_path data/market-square.mp4 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5\n```\n\n----------------------------------------\n\nTITLE: Draw Filled Polygon Function Reference\nDESCRIPTION: Function interface for drawing filled polygons on images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_filled_polygon\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving DetectionDataset in COCO Format\nDESCRIPTION: Demonstrates how to load a DetectionDataset from COCO format and save it back to COCO format using the new from_coco and as_coco methods.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> ds = sv.DetectionDataset.from_coco(\n...     images_directory_path='...',\n...     annotations_path='...'\n... )\n\n>>> ds.as_coco(\n...     images_directory_path='...',\n...     annotations_path='...'\n... )\n```\n\n----------------------------------------\n\nTITLE: Draw Filled Rectangle Function Reference\nDESCRIPTION: Function interface for drawing filled rectangles on images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_filled_rectangle\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for isolating the project dependencies. This is an optional step but recommended for dependency management.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Example with YOLOv8\nDESCRIPTION: Command to run the tracking example using a local YOLOv8 model for object detection.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/tracking/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ultralytics_example.py \\\n    --source_weights_path yolov8s.pt \\\n    --source_video_path input.mp4 \\\n    --target_video_path tracking_result.mp4\n```\n\n----------------------------------------\n\nTITLE: Running Inference Example with Roboflow\nDESCRIPTION: Command to run the inference example script using Roboflow's API for object detection.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/tracking/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython inference_example.py \\\n    --roboflow_api_key <ROBOFLOW API KEY> \\\n    --source_video_path input.mp4 \\\n    --target_video_path tracking_result.mp4\n```\n\n----------------------------------------\n\nTITLE: Draw Image Function Reference\nDESCRIPTION: Function interface for drawing images on other images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_image\n```\n\n----------------------------------------\n\nTITLE: Creating ColorPalette from Matplotlib Palette in Supervision\nDESCRIPTION: Demonstrates how to create a sv.ColorPalette instance from a Matplotlib color palette using the from_matplotlib method. This allows users to leverage Matplotlib's wide range of color schemes for Supervision annotations.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n>>> import supervision as sv\n\n>>> sv.ColorPalette.from_matplotlib('viridis', 5)\nColorPalette(colors=[Color(r=68, g=1, b=84), Color(r=59, g=82, b=139), ...])\n```\n\n----------------------------------------\n\nTITLE: Color Class Reference\nDESCRIPTION: Class interface for color management\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.color.Color\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Navigating to Example Directory\nDESCRIPTION: Commands to clone the Supervision repository and navigate to the example directory for the count people in zone project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision/examples/count_people_in_zone\n```\n\n----------------------------------------\n\nTITLE: Cloning and Navigating to Repository\nDESCRIPTION: Commands to clone the supervision repository and navigate to the traffic analysis example directory. This is the first step to set up the project locally.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision/examples/traffic_analysis\n```\n\n----------------------------------------\n\nTITLE: Importing MeanAverageRecall Class in Python\nDESCRIPTION: This snippet shows how to import the MeanAverageRecall class from the supervision library. The class is used for calculating Mean Average Recall metrics in object detection tasks.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/mean_average_recall.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.metrics.mean_average_recall import MeanAverageRecall\n```\n\n----------------------------------------\n\nTITLE: Saving Detections to CSV using Supervision\nDESCRIPTION: The CSVSink class from supervision.detection.tools.csv_sink module is used to save detection results in CSV format. It allows for flexible configuration of output file and data formatting.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/tools/save_detections.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.tools.csv_sink.CSVSink\n```\n\n----------------------------------------\n\nTITLE: Importing ConfusionMatrix Class - Python\nDESCRIPTION: Class reference for the ConfusionMatrix metric used for evaluating object detection performance. This is part of the legacy metrics that will be deprecated in future versions.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/metrics.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsupervision.metrics.detection.ConfusionMatrix\n```\n\n----------------------------------------\n\nTITLE: Downloading Video Assets in Supervision\nDESCRIPTION: Shows how to download built-in video assets provided by the Supervision library\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.assets import download_assets, VideoAssets\n\npath_to_video = download_assets(VideoAssets.SKIING)\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Setting Up Environment (Bash)\nDESCRIPTION: Commands to clone the repository, navigate to the example directory, set up a Python virtual environment, and install required dependencies.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision/examples/time_in_zone\n\npython3 -m venv venv\nsource venv/bin/activate\n\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Draw Text Function Reference\nDESCRIPTION: Function interface for drawing text on images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_text\n```\n\n----------------------------------------\n\nTITLE: Importing MeanAverageRecallResult Class in Python\nDESCRIPTION: This snippet demonstrates the import of the MeanAverageRecallResult class from the supervision library. This class likely represents the result of Mean Average Recall calculations.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/mean_average_recall.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.metrics.mean_average_recall import MeanAverageRecallResult\n```\n\n----------------------------------------\n\nTITLE: Saving Detection Datasets in Various Formats\nDESCRIPTION: Examples showing how to save detection datasets in different formats including YOLO, Pascal VOC, and COCO formats using Supervision's export methods.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndataset.as_yolo(\n    images_directory_path=...,\n    annotations_directory_path=...,\n    data_yaml_path=...\n)\n\ndataset.as_pascal_voc(\n    images_directory_path=...,\n    annotations_directory_path=...\n)\n\ndataset.as_coco(\n    images_directory_path=...,\n    annotations_path=...\n)\n```\n\n----------------------------------------\n\nTITLE: Using Metrics with Oriented Bounding Boxes\nDESCRIPTION: Demonstrates how to use metrics with Oriented Bounding Boxes (OBB) support\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom supervision.metrics import F1_Score\n\npredictions = sv.Detections(...)\ntargets = sv.Detections(...)\n\nf1_metric = MeanAverageRecall(metric_target=sv.MetricTarget.ORIENTED_BOUNDING_BOXES)\nf1_result = f1_metric.update(predictions, targets).compute()\n```\n\n----------------------------------------\n\nTITLE: Computing Mean Average Recall\nDESCRIPTION: Demonstrates the usage of the new Mean Average Recall (mAR) metric for evaluating object detection performance\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport supervision as sv\nfrom supervision.metrics import MeanAverageRecall\n\npredictions = sv.Detections(...)\ntargets = sv.Detections(...)\n\nmap_metric = MeanAverageRecall()\nmap_result = map_metric.update(predictions, targets).compute()\n\nmap_result.plot()\n```\n\n----------------------------------------\n\nTITLE: Streaming Video Files (Bash)\nDESCRIPTION: Script to stream video files from a directory, mocking a live video stream for local testing using Docker.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/stream_from_file.py \\\n    --video_directory \"data/checkout\" \\\n    --number_of_streams 1\n```\n\n----------------------------------------\n\nTITLE: Importing ClassificationDataset class from Supervision\nDESCRIPTION: This snippet demonstrates how to import the ClassificationDataset class from the Supervision library's dataset.core module. The ClassificationDataset is used for managing image classification datasets.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/datasets/core.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.dataset.core import ClassificationDataset\n```\n\n----------------------------------------\n\nTITLE: Downloading COCO Format Dataset from Roboflow\nDESCRIPTION: Python code to log into Roboflow and download a dataset in COCO format. Requires workspace ID, project ID, and project version to be specified.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.workspace('<WORKSPACE_ID>').project('<PROJECT_ID>')\ndataset = project.version('<PROJECT_VERSION>').download(\"coco\")\n```\n\n----------------------------------------\n\nTITLE: Video Assets Management Enumeration\nDESCRIPTION: The VideoAssets class provides an enumeration of available video assets that can be downloaded and used with Supervision. It defines the catalog of accessible video content.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/assets.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.assets.list.VideoAssets\n```\n\n----------------------------------------\n\nTITLE: Creating FPSMonitor Class in Python\nDESCRIPTION: The FPSMonitor class is likely used to track and calculate frames per second during video processing or playback. It may provide methods for updating and retrieving FPS statistics.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/video.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.utils.video.FPSMonitor\n```\n\n----------------------------------------\n\nTITLE: Running Roboflow Inference Speed Estimation Example\nDESCRIPTION: Command to run the speed estimation example using Roboflow Inference, requiring an API key and specifying input/output paths and thresholds.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython inference_example.py \\\n    --roboflow_api_key <ROBOFLOW API KEY> \\\n    --source_video_path data/vehicles.mp4 \\\n    --target_video_path data/vehicles-result.mp4 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5\n```\n\n----------------------------------------\n\nTITLE: Running YOLO-NAS Speed Estimation Example\nDESCRIPTION: Command to run the speed estimation example using the YOLO-NAS model, specifying input and output video paths and detection thresholds.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython yolo_nas_example.py \\\n    --source_video_path data/vehicles.mp4 \\\n    --target_video_path data/vehicles-result.mp4 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5\n```\n\n----------------------------------------\n\nTITLE: Downloading Video Assets with Supervision\nDESCRIPTION: Downloads example video assets using Supervision's assets module for demonstration purposes.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/track_objects.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.assets import download_assets, VideoAssets\n\ndownload_assets(VideoAssets.PEOPLE_WALKING)\n```\n\n----------------------------------------\n\nTITLE: Downloading Video Assets with Supervision\nDESCRIPTION: The download_assets utility provided by Supervision allows downloading pre-configured video files for demo and testing purposes. This utility fetches video assets from a remote source and makes them available locally.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/assets.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.assets.downloader.download_assets\n```\n\n----------------------------------------\n\nTITLE: Importing KeyPoints class from Supervision library\nDESCRIPTION: This code snippet shows how to import the KeyPoints class from the Supervision library's keypoint detection module. The KeyPoints class is likely used for representing and manipulating keypoint data in computer vision applications.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/keypoint/core.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.keypoint.core import KeyPoints\n```\n\n----------------------------------------\n\nTITLE: Installing Repository and Navigation\nDESCRIPTION: Commands to clone the supervision repository and navigate to the example directory for heatmap and tracking implementation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/heatmap_and_track/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision/examples/heatmap_and_track\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Example\nDESCRIPTION: Command to execute the Ultralytics-based object detection and counting script with specific parameters for zone configuration, video source, and detection thresholds.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ultralytics_example.py \\\n    --zone_configuration_path data/multi-zone-config.json \\\n    --source_video_path data/market-square.mp4 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision Package with pip\nDESCRIPTION: Command to install the Supervision Python package using pip. Requires Python 3.8 or higher.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install supervision\n```\n\n----------------------------------------\n\nTITLE: Running Ultralytics Example\nDESCRIPTION: Command to run the traffic analysis using the Ultralytics implementation of YOLOv8. It specifies model path, video source, confidence threshold, IoU threshold, and output video path.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython ultralytics_example.py \\\n    --source_weights_path data/traffic_analysis.pt \\\n    --source_video_path data/traffic_analysis.mov \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5 \\\n    --target_video_path data/traffic_analysis_result.mov\n```\n\n----------------------------------------\n\nTITLE: Importing Classifications Class in Python\nDESCRIPTION: This snippet shows how to import the Classifications class from the supervision.classification.core module. It's the primary way to access the Classifications functionality in the Roboflow Supervision project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/classification/core.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.classification.core import Classifications\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Navigating to Example Directory\nDESCRIPTION: Commands to clone the Supervision repository and navigate to the tracking example directory.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/tracking/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision/examples/tracking\n```\n\n----------------------------------------\n\nTITLE: Downloading Model and Video Files\nDESCRIPTION: Command to run a setup script that downloads the required model weights and video files for the example.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./setup.sh\n```\n\n----------------------------------------\n\nTITLE: Cloning Supervision Repository for Development\nDESCRIPTION: Command options for cloning the Supervision repository. Provides two methods: a recommended shallow clone of the develop branch and a full repository clone for those who need complete history.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/YOUR_USERNAME/supervision.git\ncd supervision\n```\n\n----------------------------------------\n\nTITLE: Running the Analysis Script\nDESCRIPTION: Example command to run the heatmap and tracking analysis script with various configuration parameters including source weights, video paths, and detection thresholds.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/heatmap_and_track/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython script.py \\\n    --source_weights_path weight.pt \\\n    --source_video_path  input_video.mp4 \\\n    --confidence_threshold 0.3 \\\n    --iou_threshold 0.5 \\\n    --target_video_path  output_video.mp4\n```\n\n----------------------------------------\n\nTITLE: Defining Recall Class in Python for Supervision Framework\nDESCRIPTION: This snippet defines the Recall class in the supervision.metrics.recall module. It is likely used for calculating recall metrics in object detection or classification tasks.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/recall.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.metrics.recall.Recall\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision via rye\nDESCRIPTION: Rye package manager installation command for the Supervision package.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrye add supervision\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install all required packages from the requirements.txt file.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/tracking/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Roboflow Supervision for Development in Python\nDESCRIPTION: This code snippet demonstrates how to install the Roboflow Supervision package for development purposes using pip. It installs the package in editable mode with additional dependencies for development.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/contributing.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Video Asset from Supervision in Python\nDESCRIPTION: This code demonstrates how to download a specific video asset (subway video) from Supervision using the download_assets function. It imports necessary modules and saves the file path of the downloaded video for further use.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/notebooks/download-supervision-assets.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.assets import download_assets, VideoAssets\n\n# Download the a video of the subway.\npath_to_video = download_assets(VideoAssets.SUBWAY)\n```\n\n----------------------------------------\n\nTITLE: Example Projects Listing in Markdown\nDESCRIPTION: Markdown list of example projects with links to their respective directories and author attributions.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- [tracking](./tracking) by [@SkalskiP](https://github.com/SkalskiP)\n- [count people in zone](./count_people_in_zone) by [@SkalskiP](https://github.com/SkalskiP)\n- [traffic analysis](./traffic_analysis) by [@SkalskiP](https://github.com/SkalskiP)\n- [speed estimation](./speed_estimation) by [@SkalskiP](https://github.com/SkalskiP)\n- [time in zone](./time_in_zone) by [@SkalskiP](https://github.com/SkalskiP)\n- [heatmap and track](./heatmap_and_track/) by [@HinePo](https://github.com/HinePo)\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision from source using uv\nDESCRIPTION: Development installation steps using git clone and uv environment setup for contributing to the Supervision package.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# clone repository and navigate to root directory\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision\n\n# setup python environment and activate it\nuv venv\nsource .venv/bin/activate\n\n# installation\nuv pip install -r pyproject.toml -e . --all-extras\n```\n\n----------------------------------------\n\nTITLE: Drawing Custom Zones (Bash)\nDESCRIPTION: Script to design custom zones on a source image or video file and save results as a JSON file.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/draw_zones.py \\\n    --source_path \"data/checkout/video.mp4\" \\\n    --zone_configuration_path \"data/checkout/config.json\"\n```\n\n----------------------------------------\n\nTITLE: Importing InferenceSlicer Class from supervision.detection.tools Module in Python\nDESCRIPTION: This code snippet shows how to import the InferenceSlicer class from the supervision.detection.tools module. It uses the ':::' syntax, which is likely a custom documentation format for the project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/tools/inference_slicer.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:::supervision.detection.tools.inference_slicer.InferenceSlicer\n```\n\n----------------------------------------\n\nTITLE: Downloading Video Assets in Python with Supervision\nDESCRIPTION: Shows how to use the new assets module to download video files for use in demos. It imports the necessary functions and downloads a specific video asset.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n>>> from supervision.assets import download_assets, VideoAssets\n>>> download_assets(VideoAssets.VEHICLES)\n\"vehicles.mp4\"\n```\n\n----------------------------------------\n\nTITLE: F1ScoreResult Class References\nDESCRIPTION: Class references for F1ScoreResult that holds the computed F1 score along with related metrics like precision and recall.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/f1_score.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsupervision.metrics.f1_score.F1ScoreResult\n```\n\n----------------------------------------\n\nTITLE: Creating a Feature Branch in Git\nDESCRIPTION: Command to create and checkout a new branch for development. The branch is based on the develop branch of the upstream repository and follows the project's naming conventions.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b <scope>/<your_branch_name> upstream/develop\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for Computer Vision Project\nDESCRIPTION: This requirements list specifies the dependencies needed for a computer vision project using Roboflow Supervision. It includes gdown for downloading large files, inference for model inference, supervision for annotation and visualization, tqdm for progress tracking, and ultralytics for YOLO model support.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/traffic_analysis/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ngdown\ninference==0.9.17\nsupervision>=0.20.0\ntqdm\nultralytics<=8.3.40\n```\n\n----------------------------------------\n\nTITLE: Importing MeanAveragePrecision Class - Python\nDESCRIPTION: Class reference for the MeanAveragePrecision (mAP) metric used for evaluating object detection performance. This is part of the legacy metrics that will be deprecated in future versions.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/metrics.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsupervision.metrics.detection.MeanAveragePrecision\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Computer Vision Project\nDESCRIPTION: A list of required Python packages for a computer vision project. The dependencies include OpenCV for image processing, Supervision for computer vision workflows, Ultralytics for YOLO models, Inference for model deployment, and PyTube for handling YouTube videos.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nopencv-python\nsupervision>=0.20.0\nultralytics<=8.3.40\ninference==0.9.17\npytube\n```\n\n----------------------------------------\n\nTITLE: F1Score Class References\nDESCRIPTION: Class references for F1Score implementation that calculates precision and recall metrics to derive the F1 score for evaluation purposes.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/metrics/f1_score.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsupervision.metrics.f1_score.F1Score\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Supervision Project\nDESCRIPTION: Command to execute the pytest test suite to verify the project setup and test coverage.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuv run pytest\n```\n\n----------------------------------------\n\nTITLE: Downloading YouTube Video (Bash)\nDESCRIPTION: Script to download a video from YouTube, specifying the URL, output path, and file name.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/time_in_zone/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/download_from_youtube.py \\\n    --url \"https://www.youtube.com/watch?v=-8zyEwAa50Q\" \\\n    --output_path \"data/checkout\" \\\n    --file_name \"video.mp4\"\n```\n\n----------------------------------------\n\nTITLE: Creating Centered Social Media Links with HTML\nDESCRIPTION: This HTML snippet creates a centered div containing social media and resource links for Roboflow. It uses inline styling and images to represent each link.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_11\n\nLANGUAGE: html\nCODE:\n```\n<div align=\"center\">\n\n<div align=\"center\">\n      <a href=\"https://youtube.com/roboflow\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://www.linkedin.com/company/roboflow-ai/\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://docs.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://discuss.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584\"\n            width=\"3%\"\n          />\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://blog.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605\"\n            width=\"3%\"\n          />\n      </a>\n      </a>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Model Benchmarking\nDESCRIPTION: Install the necessary Python libraries including Roboflow, Supervision, and a specific version of Inference for model benchmarking.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install roboflow supervision\npip install git+https://github.com/roboflow/inference.git@linas/allow-latest-rc-supervision\n```\n\n----------------------------------------\n\nTITLE: Calculate Optimal Text Scale Function Reference\nDESCRIPTION: Function interface for calculating optimal text scaling\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.calculate_optimal_text_scale\n```\n\n----------------------------------------\n\nTITLE: Formatting Roboflow Supervision Code in Python\nDESCRIPTION: This command formats the Roboflow Supervision codebase using the black formatter. It ensures consistent code style across the project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/contributing.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nblack .\n```\n\n----------------------------------------\n\nTITLE: Using BoxAnnotator Instead of BoundingBoxAnnotator\nDESCRIPTION: BoundingBoxAnnotator has been renamed to BoxAnnotator. The old implementation will be removed in supervision-0.26.0.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/deprecated.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nBoxAnnotator()\n```\n\n----------------------------------------\n\nTITLE: Updating InferenceSlicer Parameters\nDESCRIPTION: In InferenceSlicer.__init__, 'overlap_filter_strategy' is deprecated and will be removed in supervision-0.27.0. Use 'overlap_strategy' instead. Similarly, 'overlap_ratio_wh' is deprecated and should be replaced with 'overlap_wh'.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/deprecated.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nInferenceSlicer(overlap_strategy=..., overlap_wh=...)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary Python packages for the project using pip and the requirements file.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Draw Polygon Function Reference\nDESCRIPTION: Function interface for drawing polygons on images\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/draw.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsupervision.draw.utils.draw_polygon\n```\n\n----------------------------------------\n\nTITLE: Running MyPy Type Checker on Roboflow Supervision Python Code\nDESCRIPTION: This command runs the MyPy type checker on the Roboflow Supervision codebase. It performs static type checking to catch potential type-related errors.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/contributing.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmypy .\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Roboflow Supervision in Python\nDESCRIPTION: This command runs the test suite for the Roboflow Supervision project using pytest. It includes all tests and generates a coverage report.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/contributing.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npytest tests -v --cov=supervision --cov-report=term-missing\n```\n\n----------------------------------------\n\nTITLE: Installing Supervision via conda\nDESCRIPTION: Conda package manager installation command for the Supervision package from conda-forge channel.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/index.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge supervision\n```\n\n----------------------------------------\n\nTITLE: Looping Over DetectionDataset Images\nDESCRIPTION: The DetectionDataset.images property is being removed. Users should loop over the dataset using a for loop to avoid loading all images into memory at once.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/deprecated.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor path, image, annotation in dataset:\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment\nDESCRIPTION: Optional commands to create and activate a Python virtual environment for the project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Displaying Contributor Images in Markdown\nDESCRIPTION: This snippet uses HTML within Markdown to center-align and display an image of contributors to the Roboflow Supervision project. It links to the GitHub contributors page.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/README.md#2025-04-21_snippet_10\n\nLANGUAGE: markdown\nCODE:\n```\n<p align=\"center\">\n    <a href=\"https://github.com/roboflow/supervision/graphs/contributors\">\n      <img src=\"https://contrib.rocks/image?repo=roboflow/supervision\" />\n    </a>\n</p>\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository and Navigating to Example Directory\nDESCRIPTION: Commands to clone the supervision repository and navigate to the speed estimation example directory.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --depth 1 -b develop https://github.com/roboflow/supervision.git\ncd supervision/examples/speed_estimation\n```\n\n----------------------------------------\n\nTITLE: Using InferenceSlicer with Transformers\nDESCRIPTION: Advanced small object detection using Supervision's InferenceSlicer with HuggingFace Transformers. The callback function converts image slices to RGB for DETR model compatibility, processes them, and combines results to handle high-resolution images.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/detect_small_objects.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport torch\nimport numpy as np\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = cv2.imread(<SOURCE_IMAGE_PATH>)\n\ndef callback(image_slice: np.ndarray) -> sv.Detections:\n    image_slice = cv2.cvtColor(image_slice, cv2.COLOR_BGR2RGB)\n    image_slice = Image.fromarray(image_slice)\n    inputs = processor(images=image_slice, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    width, height = image_slice.size\n    target_size = torch.tensor([[width, height]])\n    results = processor.post_process_object_detection(\n        outputs=outputs, target_sizes=target_size)[0]\n    return sv.Detections.from_transformers(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    model.config.id2label[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n```\n\n----------------------------------------\n\nTITLE: Constructing DetectionDataset with List of Paths\nDESCRIPTION: The 'images' parameter in DetectionDataset constructor should now be a List[str] of image paths instead of a Dict[str, np.ndarray]. This change will be enforced in supervision-0.26.0.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/deprecated.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDetectionDataset(images=List[str])\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for isolated dependency management.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/heatmap_and_track/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Git Release Commands for Version Tagging\nDESCRIPTION: Series of git commands used to tag and push a new version release of the supervision project. These commands verify changes, create version tags, and push them to the remote repository.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/release_process.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit log\ngit tag x.y.z\ngit log\ngit push origin --tags\n```\n\n----------------------------------------\n\nTITLE: Linting Roboflow Supervision Code in Python\nDESCRIPTION: This command runs the linter (ruff) on the Roboflow Supervision codebase. It checks the code for style and potential errors according to the project's coding standards.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/contributing.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nruff check .\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment for Development\nDESCRIPTION: Commands to create and activate a Python virtual environment for isolated development of the Supervision project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Computing F1 Score for Object Detection with Supervision in Python\nDESCRIPTION: This snippet demonstrates how to calculate the F1 score for object detection using the Supervision library. It computes F1 scores across different IoU thresholds and provides a breakdown of results by object size.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/benchmark_a_model.md#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom supervision.metrics import F1Score, MetricTarget\n\nf1_metric = F1Score(metric_target=MetricTarget.MASKS)\nf1_result = f1_metric.update(predictions_list, targets_list).compute()\n\nprint(f1_result)\n\nf1_result.plot()\n```\n\n----------------------------------------\n\nTITLE: Checking Import Order in Roboflow Supervision Python Code\nDESCRIPTION: This command checks the import order in the Roboflow Supervision codebase using isort. It ensures that imports are organized consistently throughout the project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/contributing.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nisort --check-only .\n```\n\n----------------------------------------\n\nTITLE: Configuring YAML Frontmatter\nDESCRIPTION: Basic YAML frontmatter configuration specifying that comments are enabled and the status is marked as new.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/detection/annotators.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ncomments: true\nstatus: new\n---\n```\n\n----------------------------------------\n\nTITLE: Setting Up Git Remote for Supervision Repository\nDESCRIPTION: Commands to add the original Supervision repository as an upstream remote and fetch its contents. This allows contributors to keep their fork in sync with the main project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream https://github.com/roboflow/supervision.git\ngit fetch upstream\n```\n\n----------------------------------------\n\nTITLE: Installing Roboflow Python Package\nDESCRIPTION: Command to install the Roboflow Python package for downloading datasets from Roboflow Universe.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/how_to/process_datasets.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install roboflow\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation Structure\nDESCRIPTION: Basic markdown structure for documenting plotting utilities in Supervision's notebook module. Includes section headers and reference links for two main plotting functions.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/utils/notebook.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ncomments: true\n---\n\n# Notebooks Utils\n\n<div class=\"md-typeset\">\n    <h2><a href=\"#supervision.utils.notebook.plot_image\">plot_image</a></h2>\n</div>\n\n:::supervision.utils.notebook.plot_image\n\n<div class=\"md-typeset\">\n    <h2><a href=\"#supervision.utils.notebook.plot_images_grid\">plot_images_grid</a></h2>\n</div>\n\n:::supervision.utils.notebook.plot_images_grid\n```\n\n----------------------------------------\n\nTITLE: Committing and Pushing Changes to GitHub\nDESCRIPTION: Git commands to stage, commit, and push changes to your fork. Uses conventional commit messages to describe changes according to project standards.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/CONTRIBUTING.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit add -A\ngit commit -m \"feat: add line counter functionality\"\ngit push -u origin <your_branch_name>\n```\n\n----------------------------------------\n\nTITLE: Object Detection with Transformers v5 and Supervision in Python\nDESCRIPTION: Demonstrates how to use Transformers v5 functions with Supervision's Detections class, specifically using the DETR model for object detection and post-processing the results.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/changelog.md#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(<SOURCE_IMAGE_PATH>)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Command to install all required dependencies from the requirements.txt file.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/heatmap_and_track/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for the project.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3.10 -m venv venv\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Video\nDESCRIPTION: Command to download the sample video file for speed estimation analysis.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/speed_estimation/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3.10 video_downloader.py\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements - Python Package List\nDESCRIPTION: Specifies required Python packages and their version constraints. Includes gdown for downloading files, inference v0.9.17, supervision v0.20.0 or higher, tqdm for progress bars, and ultralytics v8.3.40 or lower.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/count_people_in_zone/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngdown\ninference==0.9.17\nsupervision>=0.20.0\ntqdm\nultralytics<=8.3.40\n```\n\n----------------------------------------\n\nTITLE: Specifying Package Dependencies with Version Constraints\nDESCRIPTION: This snippet defines exact requirements for two Python packages used in computer vision tasks. It specifies Supervision version 0.19.0 (with a specific assets subpackage) and Ultralytics version 8.3.40 or lower.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/examples/heatmap_and_track/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsupervision[assets]==0.19.0\nultralytics<=8.3.40\n```\n\n----------------------------------------\n\nTITLE: Configuring Markdown Template for Supervision Cookbook\nDESCRIPTION: This snippet sets up the metadata for a Markdown-based documentation page. It specifies the template to use, enables comments, and hides navigation and table of contents.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/cookbooks.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntemplate: cookbooks.html\ncomments: true\nhide:\n  - navigation\n  - toc\n---\n```\n\n----------------------------------------\n\nTITLE: Including Code of Conduct File in Markdown\nDESCRIPTION: Uses markdown file inclusion syntax to embed the contents of CODE_OF_CONDUCT.md into the current document. This is commonly used in documentation generators like MkDocs to maintain code of conduct content in a separate file while including it in documentation.\nSOURCE: https://github.com/roboflow/supervision/blob/develop/docs/code_of_conduct.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n--8<-- \"CODE_OF_CONDUCT.md\"\n```"
  }
]
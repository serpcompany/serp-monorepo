[
  {
    "owner": "llamaedge",
    "repo": "llamaedge",
    "content": "TITLE: Running LlamaEdge API Server with Docker\nDESCRIPTION: Command to start an OpenAI-compatible LLM API server using Docker with Qwen-2-0.5B and all-miniLM models.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -p 8080:8080 --name api-server secondstate/qwen-2-0.5b-allminilm-2:latest\n```\n\n----------------------------------------\n\nTITLE: Running Production-Ready LlamaEdge API Server\nDESCRIPTION: Docker command to run the LlamaEdge API server using Llama-3-8B and Nomic-1.5 models.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -p 8080:8080 --name api-server secondstate/llama-3-8b-nomic-1.5:latest\n```\n\n----------------------------------------\n\nTITLE: Running LlamaEdge on Nvidia CUDA 12 Machine\nDESCRIPTION: Docker command to start the LlamaEdge server on an Nvidia CUDA 12 machine, utilizing GPU capabilities.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -p 8080:8080 --device nvidia.com/gpu=all --name api-server secondstate/qwen-2-0.5b-allminilm-2:cuda12\n```\n\n----------------------------------------\n\nTITLE: Running LlamaEdge Container with WebGPU\nDESCRIPTION: Docker command to run the pre-built LlamaEdge speech-to-text API server container with WebGPU support. Sets up necessary runtime, platform, and environment variables for GPU access.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n  --runtime=io.containerd.wasmedge.v1 \\\n  --platform=wasi/wasm \\\n  --device=\"docker.com/gpu=webgpu\" \\\n  --env WASMEDGE_WASINN_PRELOAD=default:Burn:GPU:/tiny_en.mpk:/tiny_en.cfg:/tokenizer.json:en \\\n  -p 8080:8080 \\\n  secondstate/burn-whisper-server:latest\n```\n\n----------------------------------------\n\nTITLE: Downloading and Running a Llama-2 Chat Model\nDESCRIPTION: Commands to download the Llama-2-7B-Chat model in GGUF format and run it using the llama-chat WebAssembly application through WasmEdge with appropriate directory and neural network configurations.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-chat/README.md#2025-04-21_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n# download model\ncurl -LO https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n\n# run the `llama-chat` wasm app with the model\nwasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat.Q5_K_M.gguf llama-chat.wasm --prompt-template llama-2-chat\n```\n\n----------------------------------------\n\nTITLE: Running LlamaEdge Chat with LLM\nDESCRIPTION: Command to start a chat session with the downloaded LLM using the LlamaEdge CLI application.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwasmedge --dir .:. --nn-preload default:GGML:AUTO:Llama-3.2-1B-Instruct-Q5_K_M.gguf llama-chat.wasm -p llama-3-chat\n```\n\n----------------------------------------\n\nTITLE: Executing LlamaEdge WASM with WasmEdge\nDESCRIPTION: Runs the LlamaEdge WASM binary using WasmEdge, preloading the Llama 2 model and specifying a prompt for text completion.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-simple/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nwasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat.Q5_K_M.gguf llama-simple.wasm \\\n  --prompt 'Robert Oppenheimer most important achievement is ' --ctx-size 4096\n```\n\n----------------------------------------\n\nTITLE: Running LlamaEdge with Custom Context Window Sizes\nDESCRIPTION: Docker command to start LlamaEdge with specified context window sizes for chat LLM and embedding model.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -p 8080:8080 --name api-server secondstate/qwen-2-0.5b-allminilm-2:latest ctx-size 1024 256\n```\n\n----------------------------------------\n\nTITLE: Running LlamaEdge with Maximum Context Sizes\nDESCRIPTION: Docker command to run LlamaEdge with maximum context sizes for both models, requiring at least 32GB of RAM.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -p 8080:8080 --name api-server secondstate/llama-3-8b-nomic-1.5:latest ctx-size 8192 8192\n```\n\n----------------------------------------\n\nTITLE: Downloading Production-Ready LLM Models\nDESCRIPTION: Commands to download GGUF format model files for Llama-3-8B and Nomic-1.5 from Huggingface.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf\ncurl -LO https://huggingface.co/second-state/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM Model Files\nDESCRIPTION: Commands to download GGUF format model files for Qwen-2-0.5B and all-MiniLM-L6-v2 from Huggingface.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://huggingface.co/second-state/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf\ncurl -LO https://huggingface.co/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-ggml-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Downloading LLM Model for LlamaEdge\nDESCRIPTION: Command to download the Meta Llama 3.2 1B model file for use with LlamaEdge.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf\n```\n\n----------------------------------------\n\nTITLE: Downloading LlamaEdge CLI Chat Application\nDESCRIPTION: Command to download the LlamaEdge CLI chat application, a cross-platform Wasm app for LLM interaction.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-chat.wasm\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama 2 Model\nDESCRIPTION: Downloads the Llama 2 7B Chat model in GGUF format from Hugging Face repository.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-simple/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n```\n\n----------------------------------------\n\nTITLE: Installing WasmEdge with Plugins\nDESCRIPTION: Installs the latest version of WasmEdge with plugins using a curl command and installation script.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-simple/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s\n```\n\n----------------------------------------\n\nTITLE: Installing WasmEdge for LlamaEdge\nDESCRIPTION: Command to install WasmEdge, a prerequisite for running LlamaEdge applications.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash\n```\n\n----------------------------------------\n\nTITLE: Downloading LlamaEdge WASM Binary\nDESCRIPTION: Downloads the pre-compiled WASM file for LlamaEdge simple text completion from the GitHub releases page.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-simple/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-simple.wasm\n```\n\n----------------------------------------\n\nTITLE: Downloading the LlamaEdge WASM Chat Application\nDESCRIPTION: Command to download the prebuilt WebAssembly application for LLM chat functionality from the LlamaEdge GitHub repository.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-chat/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm\n```\n\n----------------------------------------\n\nTITLE: Making OpenAI-style API Request to LlamaEdge\nDESCRIPTION: cURL command to send an OpenAI-compatible chat completion request to the LlamaEdge server.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080/v1/chat/completions \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"messages\":[{\"role\":\"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\":\"user\", \"content\": \"Where is Paris?\"}]}'\n```\n\n----------------------------------------\n\nTITLE: Making Embedding Request to LlamaEdge\nDESCRIPTION: cURL command to send an embedding request to the LlamaEdge server, converting text paragraphs into vectors.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST http://localhost:8080/v1/embeddings \\\n    -H 'accept:application/json' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\":\"all-MiniLM-L6-v2-ggml-model-f16.gguf\", \"input\":[\"Paris is the capital of France.\",\"Paris occupies a central position in the rich agricultural region of 890 square miles (2,300 square km).\",\"The population of Paris is 2,145,906\"]}'\n```\n\n----------------------------------------\n\nTITLE: Installing WasmEdge with plugins on macOS (Apple Silicon)\nDESCRIPTION: Commands to install WasmEdge version 0.13.4 with wasi-nn-ggml plugin on macOS with Apple Silicon and activate the installation in the shell environment.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-chat/README.md#2025-04-21_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# install WasmEdge-0.13.4 with wasi-nn-ggml plugin\ncurl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s\n\n# Assuming you use zsh (the default shell on macOS), run the following command to activate the environment\nsource $HOME/.zshenv\n```\n\n----------------------------------------\n\nTITLE: Installing WasmEdge with plugins on Ubuntu 20.04+\nDESCRIPTION: Commands to install required dependencies and WasmEdge version 0.13.4 with wasi-nn-ggml plugin on Ubuntu 20.04 or later, including activating the installation in the shell environment.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-chat/README.md#2025-04-21_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n# install libopenblas-dev\napt update && apt install -y libopenblas-dev\n\n# install WasmEdge-0.13.4 with wasi-nn-ggml plugin\ncurl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s\n\n# Assuming you use bash (the default shell on Ubuntu), run the following command to activate the environment\nsource $HOME/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Installing WasmEdge with plugins on General Linux\nDESCRIPTION: Commands to install WasmEdge version 0.13.4 with wasi-nn-ggml plugin on general Linux distributions and activate the installation in the bash shell environment.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-chat/README.md#2025-04-21_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n# install WasmEdge-0.13.4 with wasi-nn-ggml plugin\ncurl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s\n\n# Assuming you use bash (the default shell on Ubuntu), run the following command to activate the environment\nsource $HOME/.bashrc\n```\n\n----------------------------------------\n\nTITLE: LlamaEdge CLI Options\nDESCRIPTION: Displays the command-line options available for the LlamaEdge simple text completion WASM application, including prompt, model alias, context size, and other parameters.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-simple/README.md#2025-04-21_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n~/llama-utils/simple$ wasmedge llama-simple.wasm -h\nUsage: llama-simple.wasm [OPTIONS] --prompt <PROMPT>\n\nOptions:\n  -p, --prompt <PROMPT>\n          Sets the prompt string, including system message if required.\n  -m, --model-alias <ALIAS>\n          Sets the model alias [default: default]\n  -c, --ctx-size <CTX_SIZE>\n          Sets the prompt context size [default: 4096]\n  -n, --n-predict <N_PRDICT>\n          Number of tokens to predict [default: 1024]\n  -g, --n-gpu-layers <N_GPU_LAYERS>\n          Number of layers to run on the GPU [default: 100]\n      --no-mmap\n          Disable memory mapping for file access of chat models\n  -b, --batch-size <BATCH_SIZE>\n          Batch size for prompt processing [default: 4096]\n  -r, --reverse-prompt <REVERSE_PROMPT>\n          Halt generation at PROMPT, return control.\n      --log-enable\n          Enable trace logs\n  -h, --help\n          Print help\n  -V, --version\n        Print version\n```\n\n----------------------------------------\n\nTITLE: Building Production-Ready LlamaEdge Docker Image\nDESCRIPTION: Docker command to build a multi-platform image for LlamaEdge using Llama-3-8B and Nomic-1.5 models.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx build . --platform linux/arm64,linux/amd64 \\\n  --tag secondstate/llama-3-8b-nomic-1.5:latest -f Dockerfile \\\n  --build-arg CHAT_MODEL_FILE=Meta-Llama-3-8B-Instruct-Q5_K_M.gguf \\\n  --build-arg EMBEDDING_MODEL_FILE=nomic-embed-text-v1.5-f16.gguf \\\n  --build-arg PROMPT_TEMPLATE=llama-3-chat\n```\n\n----------------------------------------\n\nTITLE: Building Multi-Platform Docker Image for LlamaEdge\nDESCRIPTION: Docker command to build a multi-platform image for LlamaEdge, specifying model files and prompt template.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker buildx build . --platform linux/arm64,linux/amd64 \\\n  --tag secondstate/qwen-2-0.5b-allminilm-2:latest -f Dockerfile \\\n  --build-arg CHAT_MODEL_FILE=Qwen2-0.5B-Instruct-Q5_K_M.gguf \\\n  --build-arg EMBEDDING_MODEL_FILE=all-MiniLM-L6-v2-ggml-model-f16.gguf \\\n  --build-arg PROMPT_TEMPLATE=chatml\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for LlamaEdge Server\nDESCRIPTION: Dockerfile to create a minimal container image with the Wasm binary and model files.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_7\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM scratch\n\n# Copy the prepared files from the current directory to the image\nCOPY tiny_en.cfg /tiny_en.cfg\nCOPY tiny_en.mpk /tiny_en.mpk\nCOPY tokenizer.json /tokenizer.json\nCOPY whisper-api-server.wasm /app.wasm\n\n# Set the entrypoint\nENTRYPOINT [ \"/app.wasm\" ]\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Files\nDESCRIPTION: Commands to download and extract the Whisper AI model files needed for speech recognition.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://huggingface.co/second-state/whisper-burn/resolve/main/tiny_en.tar.gz\ntar -xvzf tiny_en.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Building and Publishing Docker Image\nDESCRIPTION: Commands to build and push the Docker image to Docker Hub with the correct platform specification.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . --platform wasi/wasm -t secondstate/burn-whisper-server:latest\ndocker push secondstate/burn-whisper-server:latest\n```\n\n----------------------------------------\n\nTITLE: Publishing LlamaEdge Docker Image\nDESCRIPTION: Commands to log in to Docker Hub and push the built LlamaEdge image.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ndocker login\ndocker push secondstate/qwen-2-0.5b-allminilm-2:latest\n```\n\n----------------------------------------\n\nTITLE: Building the llama-chat WASM Application from Source\nDESCRIPTION: Command to build the llama-chat WebAssembly application from source code using Cargo, targeting the wasm32-wasip1 platform with release optimizations.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-chat/README.md#2025-04-21_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ncargo build --target wasm32-wasip1 --release\n```\n\n----------------------------------------\n\nTITLE: Building LlamaEdge WASM from Source\nDESCRIPTION: Compiles the LlamaEdge application to WebAssembly using Cargo, targeting wasm32-wasip1 architecture in release mode.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-simple/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncargo build --target wasm32-wasip1 --release\n```\n\n----------------------------------------\n\nTITLE: Building Wasm Binary\nDESCRIPTION: Commands to build the Rust project into a WebAssembly binary and prepare it for containerization.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncargo build --release --target wasm32-wasip1\ncp target/wasm32-wasip1/release/whisper-api-server.wasm  .\n```\n\n----------------------------------------\n\nTITLE: Making API Request for Transcription\nDESCRIPTION: cURL command to send an audio file to the API server for transcription using multipart form data.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/audio/transcriptions \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F file=\"@audio16k.wav\"\n```\n\n----------------------------------------\n\nTITLE: Downloading YouTube Audio\nDESCRIPTION: Command using yt-dlp to download and convert YouTube audio to the required WAV format specification.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyt-dlp -f bestaudio --extract-audio --audio-format wav --postprocessor-args \"-ss 25 -t 10 -ar 16000 -ac 1\" -o \"output.wav\" \"https://www.youtube.com/watch?v=UF8uR6Z6KLc\"\n```\n\n----------------------------------------\n\nTITLE: JSON API Response Structure with Token Usage\nDESCRIPTION: Shows the structure of an API response containing message content, tool calls, completion information, and token usage statistics. The response includes weather information for Auckland along with prompt and completion token counts.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/llama-api-server/ToolUse.md#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"content\": \"Today in Auckland, the current weather is fine but there's a chance of showers. Make sure to check the forecast for any potential changes throughout the day!\",\n    \"tool_calls\": [],\n    \"role\": \"assistant\",\n    \"finish_reason\": \"stop\",\n    \"logprobs\": null,\n    \"usage\": {\n        \"prompt_tokens\": 60,\n        \"completion_tokens\": 36,\n        \"total_tokens\": 96\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: API Response Format\nDESCRIPTION: Example JSON response from the speech-to-text API showing the transcribed text.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \" Hello, I am the whisper machine learning model. If you see this as text then I am working properly.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping and Removing LlamaEdge Docker Container\nDESCRIPTION: Command to stop and remove the running LlamaEdge Docker container.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker stop api-server\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Audio File\nDESCRIPTION: Command to download a sample WAV file for testing the speech-to-text API server.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/docker/webgpu.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LO https://huggingface.co/second-state/whisper-burn/resolve/main/audio16k.wav\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit Messages with Issue References in LlamaEdge\nDESCRIPTION: Example of proper commit message formatting that includes issue references for automatic closing when merged. The format includes a title line, optional detailed description, and the 'Fixes #123' syntax separated by a blank line.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAdd foobar\n\nSome longer description goes here, if you\nwant to describe your change in detail.\n\nFixes #123\n```\n\n----------------------------------------\n\nTITLE: Defining LlamaEdge API Endpoints Path\nDESCRIPTION: Reference to the endpoints path within the LlamaEdge API Server project structure, linking to the OpenAI API documentation for compatibility reference.\nSOURCE: https://github.com/llamaedge/llamaedge/blob/main/crates/endpoints/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nendpoints\n```"
  }
]
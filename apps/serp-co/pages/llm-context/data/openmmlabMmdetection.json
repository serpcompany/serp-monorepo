[
  {
    "owner": "open-mmlab",
    "repo": "mmdetection",
    "content": "TITLE: Dataloader and Dataset Config in MMDetection 3.x\nDESCRIPTION: This code shows the updated dataloader and dataset configuration structure in MMDetection 3.x. It defines separate configurations for `train_dataloader`, `val_dataloader`, and `test_dataloader`, each containing parameters such as `batch_size`, `num_workers`, `sampler`, `batch_sampler`, and the dataset configuration. This structure allows independent configuration of data loading for training, validation and testing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataloader = dict(\n    batch_size=2,\n    num_workers=2,\n    persistent_workers=True,  # Avoid recreating subprocesses after each iteration\n    sampler=dict(type='DefaultSampler', shuffle=True),  # Default sampler, supports both distributed and non-distributed training\n    batch_sampler=dict(type='AspectRatioBatchSampler'),  # Default batch_sampler, used to ensure that images in the batch have similar aspect ratios, so as to better utilize graphics memory\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_train2017.json',\n        data_prefix=dict(img='train2017/'),\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n        pipeline=train_pipeline))\n# In version 3.x, validation and test dataloaders can be configured independently\nval_dataloader = dict(\n    batch_size=1,\n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_val2017.json',\n        data_prefix=dict(img='val2017/'),\n        test_mode=True,\n        pipeline=test_pipeline))\ntest_dataloader = val_dataloader  # The configuration of the testing dataloader is the same as that of the validation dataloader, which is omitted here\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Data Processing Pipeline for Faster R-CNN (Training)\nDESCRIPTION: This code defines the data processing pipeline for training a Faster R-CNN model. It includes steps for loading images and annotations, resizing, random flipping for augmentation, and packing the data into a suitable format for the model. The `backend_args` parameter is assumed to be defined elsewhere. Each step takes a dictionary as input and produces a dictionary as output.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/transforms.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrain_pipeline = [  # Training data processing pipeline\n    dict(type='LoadImageFromFile', backend_args=backend_args),  # First pipeline to load images from file path\n    dict(\n        type='LoadAnnotations',  # Second pipeline to load annotations for current image\n        with_bbox=True),  # Whether to use bounding box, True for detection\n    dict(\n        type='Resize',  # Pipeline that resize the images and their annotations\n        scale=(1333, 800),  # The largest scale of image\n        keep_ratio=True  # Whether to keep the ratio between height and width\n        ),\n    dict(\n        type='RandomFlip',  # Augmentation pipeline that flip the images and their annotations\n        prob=0.5),  # The probability to flip\n    dict(type='PackDetInputs')  # Pipeline that formats the annotation data and decides which keys in the data should be packed into data_samples\n]\n```\n\n----------------------------------------\n\nTITLE: Configure Dataset and DataLoaders in MMDetection (Python)\nDESCRIPTION: This code snippet configures the dataset type, data root, data processing pipelines, and dataloaders for training, validation, and testing in MMDetection. It defines the data loading process, including image loading, annotation loading, resizing, and data packing. It also defines batch sizes, number of workers, and samplers for the dataloaders.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset_type = 'CocoDataset'  # Dataset type, this will be used to define the dataset\ndata_root = 'data/coco/'  # Root path of data\nbackend_args = None # Arguments to instantiate the corresponding file backend\n\ntrain_pipeline = [  # Training data processing pipeline\n    dict(type='LoadImageFromFile', backend_args=backend_args),  # First pipeline to load images from file path\n    dict(\n        type='LoadAnnotations',  # Second pipeline to load annotations for current image\n        with_bbox=True,  # Whether to use bounding box, True for detection\n        with_mask=True,  # Whether to use instance mask, True for instance segmentation\n        poly2mask=True),  # Whether to convert the polygon mask to instance mask, set False for acceleration and to save memory\n    dict(\n        type='Resize',  # Pipeline that resizes the images and their annotations\n        scale=(1333, 800),  # The largest scale of the images\n        keep_ratio=True  # Whether to keep the ratio between height and width\n        ),\n    dict(\n        type='RandomFlip',  # Augmentation pipeline that flips the images and their annotations\n        prob=0.5),  # The probability to flip\n    dict(type='PackDetInputs')  # Pipeline that formats the annotation data and decides which keys in the data should be packed into data_samples\n]\ntest_pipeline = [  # Testing data processing pipeline\n    dict(type='LoadImageFromFile', backend_args=backend_args),  # First pipeline to load images from file path\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # Pipeline that resizes the images\n    dict(\n        type='PackDetInputs',  # Pipeline that formats the annotation data and decides which keys in the data should be packed into data_samples\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(   # Train dataloader config\n    batch_size=2,  # Batch size of a single GPU\n    num_workers=2,  # Worker to pre-fetch data for each single GPU\n    persistent_workers=True,  # If ``True``, the dataloader will not shut down the worker processes after an epoch end, which can accelerate training speed.\n    sampler=dict(  # training data sampler\n        type='DefaultSampler',  # DefaultSampler which supports both distributed and non-distributed training. Refer to https://mmengine.readthedocs.io/en/latest/api/generated/mmengine.dataset.DefaultSampler.html#mmengine.dataset.DefaultSampler\n        shuffle=True),  # randomly shuffle the training data in each epoch\n    batch_sampler=dict(type='AspectRatioBatchSampler'),  # Batch sampler for grouping images with similar aspect ratio into a same batch. It can reduce GPU memory cost.\n    dataset=dict(  # Train dataset config\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_train2017.json',  # Path of annotation file\n        data_prefix=dict(img='train2017/'),  # Prefix of image path\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),  # Config of filtering images and annotations\n        pipeline=train_pipeline,\n        backend_args=backend_args))\nval_dataloader = dict(  # Validation dataloader config\n    batch_size=1,  # Batch size of a single GPU. If batch-size > 1, the extra padding area may influence the performance.\n    num_workers=2,  # Worker to pre-fetch data for each single GPU\n    persistent_workers=True,  # If ``True``, the dataloader will not shut down the worker processes after an epoch end, which can accelerate training speed.\n    drop_last=False,  # Whether to drop the last incomplete batch, if the dataset size is not divisible by the batch size\n    sampler=dict(\n        type='DefaultSampler',\n        shuffle=False),  # not shuffle during validation and testing\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_val2017.json',\n        data_prefix=dict(img='val2017/'),\n        test_mode=True,  # Turn on the test mode of the dataset to avoid filtering annotations or images\n        pipeline=test_pipeline,\n        backend_args=backend_args))\ntest_dataloader = val_dataloader  # Testing dataloader config\n```\n\n----------------------------------------\n\nTITLE: Train MMDetection Model (Shell)\nDESCRIPTION: This shell command initiates the training process for an MMDetection model using a specified configuration file. The configuration file defines the model architecture, dataset, and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py configs/balloon/mask-rcnn_r50-caffe_fpn_ms-poly-1x_balloon.py\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Dataset in MMDetection\nDESCRIPTION: This snippet demonstrates how to create a custom dataset in MMDetection by inheriting from `BaseDetDataset`. It shows how to define the `METAINFO`, including class names and palette, and how to implement the `load_data_list` method to load annotation data from a file. This function parses the annotation file, extracts image paths, bounding boxes, and labels, and returns a list of data information dictionaries.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_dataset.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mmengine\n\nfrom mmdet.base_det_dataset import BaseDetDataset\nfrom mmdet.registry import DATASETS\n\n\n@DATASETS.register_module()\nclass MyDataset(BaseDetDataset):\n\n    METAINFO = {\n       'classes': ('person', 'bicycle', 'car', 'motorcycle'),\n        'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230)]\n    }\n\n    def load_data_list(self, ann_file):\n        ann_list = mmengine.list_from_file(ann_file)\n\n        data_infos = []\n        for i, ann_line in enumerate(ann_list):\n            if ann_line != '#':\n                continue\n\n            img_shape = ann_list[i + 2].split(' ')\n            width = int(img_shape[0])\n            height = int(img_shape[1])\n            bbox_number = int(ann_list[i + 3])\n\n            instances = []\n            for anns in ann_list[i + 4:i + 4 + bbox_number]:\n                instance = {}\n                instance['bbox'] = [float(ann) for ann in anns.split(' ')[:4]]\n                instance['bbox_label']=int(anns[4])\n \t\t\t\tinstances.append(instance)\n\n            data_infos.append(\n                dict(\n                    img_path=ann_list[i + 1],\n                    img_id=i,\n                    width=width,\n                    height=height,\n                    instances=instances\n                ))\n\n        return data_infos\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Pre-trained Model (Python)\nDESCRIPTION: This snippet shows how to initialize the `DetInferencer` with a pre-trained model using its name. The model weights are automatically downloaded from the OpenMMLab model zoo. Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco')\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Hook in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to implement a new hook in MMDetection by inheriting from `mmengine.hooks.Hook` and registering it with `mmdet.registry.HOOKS`. The hook defines methods for different stages of the training process, such as `before_run`, `after_run`, `before_train`, `after_train`, `before_train_epoch`, `after_train_epoch`, `before_train_iter`, and `after_train_iter`. The `DATA_BATCH` type hint needs to be imported.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.hooks import Hook\nfrom mmdet.registry import HOOKS\nfrom typing import Optional\n\n# Assuming DATA_BATCH is defined elsewhere or imported\n# For example:\n# from typing import Any\n# DATA_BATCH = Any # Replace Any with the actual type if known\n\n@HOOKS.register_module()\nclass MyHook(Hook):\n\n    def __init__(self, a, b):\n        pass # Initialize hook parameters if needed\n\n    def before_run(self, runner) -> None:\n        pass # Define actions before the run starts\n\n    def after_run(self, runner) -> None:\n        pass # Define actions after the run finishes\n\n    def before_train(self, runner) -> None:\n        pass # Define actions before each training epoch\n\n    def after_train(self, runner) -> None:\n        pass # Define actions after each training epoch\n\n    def before_train_epoch(self, runner) -> None:\n        pass # Define actions before each training epoch\n\n    def after_train_epoch(self, runner) -> None:\n        pass # Define actions after each training epoch\n\n    def before_train_iter(self,\n                          runner,\n                          batch_idx: int):\n        pass # Define actions before each training iteration\n\n    def after_train_iter(self,\n                         runner,\n                         batch_idx: int,\n                         outputs: Optional[dict] = None) -> None:\n        pass # Define actions after each training iteration\n```\n\n----------------------------------------\n\nTITLE: Defining a MobileNet Backbone in MMDetection\nDESCRIPTION: Defines a new MobileNet backbone module within the MMDetection framework. The module inherits from `nn.Module` and is registered using the `MODELS.register_module()` decorator.  It includes an `__init__` method for initialization and a `forward` method for feature extraction, which should return a tuple.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\n\nfrom mmdet.registry import MODELS\n\n\n@MODELS.register_module()\nclass MobileNet(nn.Module):\n\n    def __init__(self, arg1, arg2):\n        pass\n\n    def forward(self, x):  # should return a tuple\n        pass\n```\n\n----------------------------------------\n\nTITLE: Modifying the Head for Finetuning in MMDetection\nDESCRIPTION: This code snippet shows how to modify the head of a Mask RCNN model for finetuning on a new dataset with a different number of classes.  Specifically, it adjusts the `num_classes` parameter in the `roi_head`'s `bbox_head` and `mask_head`. This ensures the model is compatible with the new dataset's class labels while reusing the weights of the pre-trained model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/finetune.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=8,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=8,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n```\n\n----------------------------------------\n\nTITLE: Download Datasets using MIM (Bash)\nDESCRIPTION: Downloads and preprocesses specified datasets (voc2007, voc2012, coco2017) using MIM. Requires OpenXLab CLI tools and login. Datasets are downloaded specifically for mmdet (MMDetection).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\n# install OpenXLab CLI tools\npip install -U openxlab\n# log in OpenXLab, registry\nopenxlab login\n\n# download voc2007 and preprocess by MIM\nmim download mmdet --dataset voc2007\n\n# download voc2012 and preprocess by MIM\nmim download mmdet --dataset voc2012\n\n# download coco2017 and preprocess by MIM\nmim download mmdet --dataset coco2017\n```\n\n----------------------------------------\n\nTITLE: Using MobileNetV3 from MMPretrain with RetinaNet\nDESCRIPTION: This code snippet demonstrates how to configure RetinaNet to use MobileNetV3 as its backbone network, leveraging the MMPretrain library. It requires installing mmpretrain and adjusting the model configuration to import and use the MobileNetV3 architecture, specifying the desired architecture, output indices, and initialization configurations. The pre-trained weights are loaded using a checkpoint, with the prefix removed to ensure correct loading.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/how_to.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# please install mmpretrain\n# import mmpretrain.models to trigger register_module in mmpretrain\ncustom_imports = dict(imports=['mmpretrain.models'], allow_failed_imports=False)\npretrained = 'https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/convert/mobilenet_v3_small-8427ecf0.pth'\nmodel = dict(\n    backbone=dict(\n        _delete_=True, # Delete the backbone field in _base_\n        type='mmpretrain.MobileNetV3', # Using MobileNetV3 from mmpretrain\n        arch='small',\n        out_indices=(3, 8, 11), # Modify out_indices\n        init_cfg=dict(\n            type='Pretrained',\n            checkpoint=pretrained,\n            prefix='backbone.')), # The pre-trained weights of backbone network in mmpretrain have prefix='backbone.'. The prefix in the keys will be removed so that these weights can be normally loaded.\n    # Modify in_channels\n    neck=dict(in_channels=[24, 48, 96], start_level=0))\n\n```\n\n----------------------------------------\n\nTITLE: Migrating RandomChoiceResize Configuration in MMDetection\nDESCRIPTION: Shows the change in the RandomChoiceResize transform configuration from v2.x to v3.x. The parameter name `img_scale` is changed to `scales`, and the transform type changed from 'Resize' to 'RandomChoiceResize'. The parameter `multiscale_mode` has been removed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='Resize',\n    img_scale=[\n        (1333, 640), (1333, 672),\n        (1333, 704), (1333, 736),\n        (1333, 768), (1333, 800)],\n    multiscale_mode='value',\n    keep_ratio=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='RandomChoiceResize',\n    scales=[\n        (1333, 640), (1333, 672),\n        (1333, 704), (1333, 736),\n        (1333, 768), (1333, 800)],\n    keep_ratio=True)\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Testing with 8 GPUs in MMDetection\nDESCRIPTION: This snippet demonstrates how to test the Mask R-CNN model with 8 GPUs using the `tools/dist_test.sh` script. It specifies the configuration file, checkpoint file, and the number of GPUs to use. The `--out` flag is used to save the test results to `results.pkl`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n    configs/mask-rcnn_r50_fpn_1x_coco.py \\\n    checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n    8 \\\n    --out results.pkl\n```\n\n----------------------------------------\n\nTITLE: Switching Normalization Layers in Configuration\nDESCRIPTION: This code snippet demonstrates how to switch normalization layers (e.g., from SyncBN to BN) in an MMDetection configuration file. It defines a `norm_cfg` variable and then substitutes this variable into the `norm_cfg` fields of the `backbone` and `neck` configurations. This ensures consistent normalization settings across different modules of the model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\nnorm_cfg = dict(type='BN', requires_grad=True)\nmodel = dict(\n    backbone=dict(norm_cfg=norm_cfg),\n    neck=dict(norm_cfg=norm_cfg),\n    ...)\n```\n\n----------------------------------------\n\nTITLE: Configure Optimizer Wrapper in MMDetection (Python)\nDESCRIPTION: This code snippet configures the optimizer wrapper, which includes the optimizer type, learning rate, momentum, weight decay, and gradient clipping options. It uses the SGD optimizer with specified hyperparameters. The optimizer wrapper facilitates functionalities like mixed-precision training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(  # Optimizer wrapper config\n    type='OptimWrapper',  # Optimizer wrapper type, switch to AmpOptimWrapper to enable mixed precision training.\n    optimizer=dict(  # Optimizer config. Support all kinds of optimizers in PyTorch. Refer to https://pytorch.org/docs/stable/optim.html#algorithms\n        type='SGD',  # Stochastic gradient descent optimizer\n        lr=0.02,  # The base learning rate\n        momentum=0.9,  # Stochastic gradient descent with momentum\n        weight_decay=0.0001),  # Weight decay of SGD\n    clip_grad=None,  # Gradient clip option. Set None to disable gradient clip. Find usage in https://mmengine.readthedocs.io/en/latest/tutorials/optimizer.html\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using DetInferencer for Inference\nDESCRIPTION: This snippet initializes the `DetInferencer` with a pre-trained model ('rtmdet_tiny_8xb32-300e_coco') and performs inference on a demo image ('demo/demo.jpg'). The output is saved to the './output' directory. Requires the mmdet package.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.apis import DetInferencer\n\n# Initialize the DetInferencer\ninferencer = DetInferencer('rtmdet_tiny_8xb32-300e_coco')\n\n# Perform inference\ninferencer('demo/demo.jpg', out_dir='./output')\n```\n\n----------------------------------------\n\nTITLE: Importing the Custom Loss in `__init__.py` (Python)\nDESCRIPTION: Imports the custom loss function and module into the `__init__.py` file of the losses directory. This makes the loss function available for use in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom .my_loss import MyLoss, my_loss\n```\n\n----------------------------------------\n\nTITLE: Convert Cityscapes and Pascal VOC Datasets to COCO Format\nDESCRIPTION: This snippet showcases how to use the dataset conversion tools in `tools/data_converters/` to convert Cityscapes and Pascal VOC datasets to the COCO dataset format. It takes the path to the respective datasets as arguments, along with other optional parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/cityscapes.py ${CITYSCAPES_PATH} [-h] [--img-dir ${IMG_DIR}] [--gt-dir ${GT_DIR}] [-o ${OUT_DIR}] [--nproc ${NPROC}]\npython tools/dataset_converters/pascal_voc.py ${DEVKIT_PATH} [-h] [-o ${OUT_DIR}]\n```\n\n----------------------------------------\n\nTITLE: Configuring MMDetection to Use Custom Dataset\nDESCRIPTION: This code snippet shows how to modify the MMDetection configuration file to use the custom dataset defined in the previous step. It sets the `type` parameter to the name of the custom dataset class (`MyDataset`) and specifies the path to the annotation file (`ann_file`). It also includes a placeholder for the `train_pipeline` which defines the data augmentation and preprocessing steps.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_dataset.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset_A_train = dict(\n    type='MyDataset',\n    ann_file = 'image_list.txt',\n    pipeline=train_pipeline\n)\n```\n\n----------------------------------------\n\nTITLE: Runtime Configuration - Python\nDESCRIPTION: This section configures runtime settings, including the default scope, environment configurations (cudnn benchmark, multiprocessing settings, distributed settings), visualization backends, visualizer, log processor, log level, loading pretrained models, and resuming training. These settings control the execution environment and logging behavior.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndefault_scope = 'mmdet'  # 默认的注册器域名，默认从此注册器域中寻找模块。请参考 https://mmengine.readthedocs.io/zh_CN/latest/advanced_tutorials/registry.html\n\nenv_cfg = dict(\n    cudnn_benchmark=False,  # 是否启用 cudnn benchmark\n    mp_cfg=dict(  # 多进程设置\n        mp_start_method='fork',  # 使用 fork 来启动多进程。'fork' 通常比 'spawn' 更快，但可能存在隐患。请参考 https://github.com/pytorch/pytorch/issues/1355\n        opencv_num_threads=0),  # 关闭 opencv 的多线程以避免系统超负荷\n    dist_cfg=dict(backend='nccl'),  # 分布式相关设置\n)\n\nvis_backends = [dict(type='LocalVisBackend')]  # 可视化后端，请参考 https://mmengine.readthedocs.io/zh_CN/latest/advanced_tutorials/visualization.html\nvisualizer = dict(\n    type='DetLocalVisualizer', vis_backends=vis_backends, name='visualizer')\nlog_processor = dict(\n    type='LogProcessor',  # 日志处理器用于处理运行时日志\n    window_size=50,  # 日志数值的平滑窗口\n    by_epoch=True)  # 是否使用 epoch 格式的日志。需要与训练循环的类型保存一致。\n\nlog_level = 'INFO'  # 日志等级\nload_from = None  # 从给定路径加载模型检查点作为预训练模型。这不会恢复训练。\nresume = False  # 是否从 `load_from` 中定义的检查点恢复。 如果 `load_from` 为 None，它将恢复 `work_dir` 中的最新检查点。\n```\n\n----------------------------------------\n\nTITLE: Define Data Pipeline for Labeled Data (Python)\nDESCRIPTION: This pipeline defines the data augmentation steps for labeled data in a semi-supervised object detection setup.  It includes loading images and annotations, resizing, flipping, RandAugment for color adjustments, filtering annotations based on bounding box size, and packing the inputs for the detection model.  The `backend_args` and `scale` variables need to be defined beforehand.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment labeled data,\n# which will be sent to student model for supervised training.\nsup_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='RandomResize', scale=scale, keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='RandAugment', aug_space=color_space, aug_num=1),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),\n    dict(type='MultiBranch', sup=dict(type='PackDetInputs'))\n]\n```\n\n----------------------------------------\n\nTITLE: Image Demo Usage with MMDetection\nDESCRIPTION: This script performs object detection inference on a single image using a specified configuration file and optionally a weights file. It supports specifying the device and confidence threshold for predictions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py \\\n    ${IMAGE_FILE} \\\n    ${CONFIG_FILE} \\\n    [--weights ${WEIGHTS}] \\\n    [--device ${GPU_ID}] \\\n    [--pred-score-thr ${SCORE_THR}]\n```\n\n----------------------------------------\n\nTITLE: Integrating the Custom Loss into the Model Configuration\nDESCRIPTION: This snippet demonstrates how to integrate the custom loss function into the model configuration by modifying the `loss_bbox` field. It specifies the type of loss as `MyLoss` and sets the loss weight to 1.0.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nloss_bbox=dict(type='MyLoss', loss_weight=1.0))\n```\n\n----------------------------------------\n\nTITLE: Converting MMDetection model to ONNX using MMDeploy in Python\nDESCRIPTION: This code snippet demonstrates how to convert a MMDetection model (Faster R-CNN in this case) to an ONNX model using the MMDeploy library. It utilizes the `torch2onnx` function to perform the conversion and `export2SDK` to extract pipeline information for MMDeploy SDK inference. The script requires the mmdeploy and mmdet packages to be installed, along with the specified model and deploy configurations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/deploy.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy.apis import torch2onnx\nfrom mmdeploy.backend.sdk.export_info import export2SDK\n\nimg = 'demo/demo.jpg'\nwork_dir = 'mmdeploy_models/mmdet/onnx'\nsave_file = 'end2end.onnx'\ndeploy_cfg = '../mmdeploy/configs/mmdet/detection/detection_onnxruntime_dynamic.py'\nmodel_cfg = 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\nmodel_checkpoint = 'faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\ndevice = 'cpu'\n\n# 1. convert model to onnx\ntorch2onnx(img, work_dir, save_file, deploy_cfg, model_cfg,\n           model_checkpoint, device)\n\n# 2. extract pipeline info for inference by MMDeploy SDK\nexport2SDK(deploy_cfg, model_cfg, work_dir, pth=model_checkpoint,\n           device=device)\n```\n\n----------------------------------------\n\nTITLE: Disable GPUs with Environment Variable\nDESCRIPTION: This shell command disables the use of GPUs by setting the `CUDA_VISIBLE_DEVICES` environment variable to `-1`. This forces the training process to run on the CPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=-1\n```\n\n----------------------------------------\n\nTITLE: Define Strong Augmentation Pipeline for Unlabeled Data (Python)\nDESCRIPTION: This pipeline defines strong data augmentation steps for unlabeled data, designed for the student model in a teacher-student semi-supervised learning framework. It includes resizing, flipping, RandAugment for color and geometric transformations, random erasing, filtering annotations, and packing inputs with metadata. The `scale`, `color_space`, and `geometric` variables should be defined beforehand.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment unlabeled data strongly,\n# which will be sent to student model for unsupervised training.\nstrong_pipeline = [\n    dict(type='RandomResize', scale=scale, keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(\n        type='RandomOrder',\n        transforms=[\n            dict(type='RandAugment', aug_space=color_space, aug_num=1),\n            dict(type='RandAugment', aug_space=geometric, aug_num=1),\n        ]),\n    dict(type='RandomErasing', n_patches=(1, 5), ratio=(0, 0.2)),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor', 'flip', 'flip_direction',\n                   'homography_matrix')),\n]\n```\n\n----------------------------------------\n\nTITLE: Defining a Standard RoI Head in MMDetection\nDESCRIPTION: This code defines the StandardRoIHead class in MMDetection, which is a basic RoI head consisting of a bounding box head and a mask head. It includes methods for initialization, forward propagation, loss calculation, and prediction during training and testing. It relies on configurations for the bbox_roi_extractor, bbox_head, mask_roi_extractor and mask_head.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom mmdet.registry import MODELS, TASK_UTILS\nfrom mmdet.structures import DetDataSample\nfrom mmdet.structures.bbox import bbox2roi\nfrom mmdet.utils import ConfigType, InstanceList\nfrom ..task_modules.samplers import SamplingResult\nfrom ..utils import empty_instances, unpack_gt_instances\nfrom .base_roi_head import BaseRoIHead\n\n\n@MODELS.register_module()\nclass StandardRoIHead(BaseRoIHead):\n    \"\"\"Simplest base roi head including one bbox head and one mask head.\"\"\"\n\n    def init_assigner_sampler(self) -> None:\n\n    def init_bbox_head(self, bbox_roi_extractor: ConfigType,\n                       bbox_head: ConfigType) -> None:\n\n    def init_mask_head(self, mask_roi_extractor: ConfigType,\n                       mask_head: ConfigType) -> None:\n\n    def forward(self, x: Tuple[Tensor],\n                rpn_results_list: InstanceList) -> tuple:\n\n    def loss(self, x: Tuple[Tensor], rpn_results_list: InstanceList,\n             batch_data_samples: List[DetDataSample]) -> dict:\n\n    def _bbox_forward(self, x: Tuple[Tensor], rois: Tensor) -> dict:\n\n    def bbox_loss(self, x: Tuple[Tensor],\n                  sampling_results: List[SamplingResult]) -> dict:\n\n    def mask_loss(self, x: Tuple[Tensor],\n                  sampling_results: List[SamplingResult], bbox_feats: Tensor,\n                  batch_gt_instances: InstanceList) -> dict:\n\n    def _mask_forward(\n                      x: Tuple[Tensor],\n                      rois: Tensor = None,\n                      pos_inds: Optional[Tensor] = None,\n                      bbox_feats: Optional[Tensor] = None) -> dict:\n\n    def predict_bbox(self,\n                     x: Tuple[Tensor],\n                     batch_img_metas: List[dict],\n                     rpn_results_list: InstanceList,\n                     rcnn_test_cfg: ConfigType,\n                     rescale: bool = False) -> InstanceList:\n\n    def predict_mask(self,\n                     x: Tuple[Tensor],\n                     batch_img_metas: List[dict],\n                     results_list: InstanceList,\n                     rescale: bool = False) -> InstanceList:\n```\n\n----------------------------------------\n\nTITLE: Define Faster R-CNN Training Pipeline Configuration Python\nDESCRIPTION: Defines the training data pipeline configuration for Faster R-CNN in MMDetection. The pipeline includes loading images, loading annotations, resizing, random flipping, normalization, padding, and collecting the necessary keys for training. `img_norm_cfg` specifies the mean, standard deviation, and `to_rgb` flag used for image normalization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/transforms.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\n```\n\n----------------------------------------\n\nTITLE: Multiple GPU Training Shell Command\nDESCRIPTION: This shell command demonstrates how to launch training on multiple GPUs using MMDetection's `tools/dist_train.sh` script. `${CONFIG_FILE}` is the path to the configuration file, and `${GPU_NUM}` specifies the number of GPUs to use. Optional arguments can be added to customize the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbash ./tools/dist_train.sh \\\n    ${CONFIG_FILE} \\\n    ${GPU_NUM} \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with a Model Name\nDESCRIPTION: This snippet initializes the `DetInferencer` with a specified model name ('rtmdet_tiny_8xb32-300e_coco'). The weights will be automatically downloaded from the OpenMMLab model zoo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco')\n```\n\n----------------------------------------\n\nTITLE: Training Faster R-CNN with Pre-trained FCOS RPN (Bash)\nDESCRIPTION: This bash command is used for training Faster R-CNN, utilizing a pre-trained FCOS model as the Region Proposal Network (RPN). The command executes a distributed training script, specifying the configuration file, the number of GPUs, and the working directory for saving training outputs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/single_stage_as_rpn.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash tools/dist_train.sh \\\n    configs/faster_rcnn/faster-rcnn_r50-caffe_fpn_fcos-rpn_1x_coco.py \\\n    8 \\\n    --work-dir ./work_dirs/faster-rcnn_r50-caffe_fpn_fcos-rpn_1x_coco\n```\n\n----------------------------------------\n\nTITLE: Downloading Datasets (COCO, VOC, LVIS)\nDESCRIPTION: These commands downloads datasets such as COCO, VOC and LVIS using the download_dataset.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name coco2017\npython tools/misc/download_dataset.py --dataset-name voc2007\npython tools/misc/download_dataset.py --dataset-name lvis\n```\n\n----------------------------------------\n\nTITLE: DoubleHeadRoIHead Implementation (Python)\nDESCRIPTION: This code implements the `DoubleHeadRoIHead`, which inherits from `StandardRoIHead`. It modifies the `_bbox_forward` method to incorporate features from both classification and regression branches, utilizing a `reg_roi_scale_factor` for scaling RoIs used for regression. This head is registered with `MODELS` and intended for placement in `mmdet/models/roi_heads/double_roi_head.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\n\nfrom torch import Tensor\n\nfrom mmdet.registry import MODELS\nfrom .standard_roi_head import StandardRoIHead\n\n\n@MODELS.register_module()\nclass DoubleHeadRoIHead(StandardRoIHead):\n    \"\"\"RoI head for `Double Head RCNN <https://arxiv.org/abs/1904.06493>`_.\n\n    Args:\n        reg_roi_scale_factor (float): The scale factor to extend the rois\n            used to extract the regression features.\n    \"\"\"\n\n    def __init__(self, reg_roi_scale_factor: float, **kwargs):\n        super().__init__(**kwargs)\n        self.reg_roi_scale_factor = reg_roi_scale_factor\n\n    def _bbox_forward(self, x: Tuple[Tensor], rois: Tensor) -> dict:\n        \"\"\"Box head forward function used in both training and testing.\n\n        Args:\n            x (tuple[Tensor]): List of multi-level img features.\n            rois (Tensor): RoIs with the shape (n, 5) where the first\n                column indicates batch id of each RoI.\n\n        Returns:\n             dict[str, Tensor]: Usually returns a dictionary with keys:\n\n                - `cls_score` (Tensor): Classification scores.\n                - `bbox_pred` (Tensor): Box energies / deltas.\n                - `bbox_feats` (Tensor): Extract bbox RoI features.\n        \"\"\"\n        bbox_cls_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        bbox_reg_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs],\n            rois,\n            roi_scale_factor=self.reg_roi_scale_factor)\n        if self.with_shared_head:\n            bbox_cls_feats = self.shared_head(bbox_cls_feats)\n            bbox_reg_feats = self.shared_head(bbox_reg_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score,\n            bbox_pred=bbox_pred,\n            bbox_feats=bbox_cls_feats)\n        return bbox_results\n```\n\n----------------------------------------\n\nTITLE: COCO Panoptic Dataset Configuration in MMDetection\nDESCRIPTION: This snippet shows how to configure MMDetection to use the COCO Panoptic dataset format. It sets the `type` to `CocoPanopticDataset` and defines the paths to the image and segmentation annotation data using `data_prefix`. Ensure the annotation JSON has the required keys (`images`, `annotations`, `categories`) and that the `seg` data prefix points to the panoptic annotation images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_dataset.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset_type = 'CocoPanopticDataset'\ndata_root='path/to/your/'\n\ntrain_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_prefix=dict(\n            img='train/image_data/', seg='train/panoptic/image_annotation_data/')\n    )\n)\nval_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_prefix=dict(\n            img='val/image_data/', seg='val/panoptic/image_annotation_data/')\n    )\n)\ntest_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_prefix=dict(\n            img='test/image_data/', seg='test/panoptic/image_annotation_data/')\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Image Demo Example with MMDetection\nDESCRIPTION: This is an example of how to use the image demo script with specific paths to the image, configuration file, and weights file. It sets the device to CPU for inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    --weights checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --device cpu\n```\n\n----------------------------------------\n\nTITLE: Install Multimodal Dependencies - Shell\nDESCRIPTION: This shell script navigates to the MMDETROOT directory and installs the required dependencies for multimodal functionalities using pip. It also installs emoji, ddd-dataset, and the lvis-api library from GitHub, ensuring the environment is properly set up for running MM Grounding DINO.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd $MMDETROOT\n\npip install -r requirements/multimodal.txt\npip install emoji ddd-dataset\npip install git+https://github.com/lvis-dataset/lvis-api.git\"\n```\n\n----------------------------------------\n\nTITLE: Customize Optimization Settings with OptimWrapper in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to customize optimization settings in MMDetection using `optim_wrapper`. It shows how to configure the optimizer type (AdamW), learning rate, weight decay, and gradient clipping. The `paramwise_cfg` allows for setting different learning rates for different parts of the model, such as the backbone.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    # optimizer\n    optimizer=dict(\n        type='AdamW',\n        lr=0.0001,\n        weight_decay=0.05,\n        eps=1e-8,\n        betas=(0.9, 0.999)),\n\n    # Parameter-level learning rate and weight decay settings\n    paramwise_cfg=dict(\n        custom_keys={\n            'backbone': dict(lr_mult=0.1, decay_mult=1.0),\n        },\n        norm_decay_mult=0.0),\n\n    # gradient clipping\n    clip_grad=dict(max_norm=0.01, norm_type=2))\n\n```\n\n----------------------------------------\n\nTITLE: Basic Inference with DetInferencer in Python\nDESCRIPTION: This code snippet demonstrates the basic usage of the `DetInferencer` class for performing object detection inference on an image. It initializes the inferencer with a pre-trained model and then runs inference on a demo image, displaying the results. Requires the `mmdet` library to be installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.apis import DetInferencer\n\n# Initialize the DetInferencer\ninferencer = DetInferencer('rtmdet_tiny_8xb32-300e_coco')\n\n# Perform inference\ninferencer('demo/demo.jpg', show=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Loss Function\nDESCRIPTION: This snippet defines a custom loss function, `my_loss`, and a corresponding module, `MyLoss`, for bounding box regression in MMDetection. The `my_loss` function calculates the absolute difference between the prediction and target, while `MyLoss` encapsulates the loss function and allows for customization of reduction and weight.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nfrom mmdet.registry import MODELS\nfrom .utils import weighted_loss\n\n@weighted_loss\ndef my_loss(pred, target):\n    assert pred.size() == target.size() and target.numel() > 0\n    loss = torch.abs(pred - target)\n    return loss\n\n@MODELS.register_module()\nclass MyLoss(nn.Module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super(MyLoss, self).__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction =\n            reduction_override if reduction_override else self.reduction\n        loss_bbox = self.loss_weight * my_loss(\n            pred, target, weight, reduction=reduction, avg_factor=avg_factor)\n        return loss_bbox\n```\n\n----------------------------------------\n\nTITLE: Inference with a Directory of Images (Python)\nDESCRIPTION: This code snippet shows how to perform inference on all images within a specified directory. Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninferencer('path/to/your_imgs/')\n```\n\n----------------------------------------\n\nTITLE: Configure Training and Testing Loops in MMDetection (Python)\nDESCRIPTION: This code snippet configures the training, validation, and testing loops used by MMEngine's runner. It defines the maximum training epochs, validation intervals, and the loop types for each process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(\n    type='EpochBasedTrainLoop',  # The training loop type. Refer to https://github.com/open-mmlab/mmengine/blob/main/mmengine/runner/loops.py\n    max_epochs=12,  # Maximum training epochs\n    val_interval=1)  # Validation intervals. Run validation every epoch.\nval_cfg = dict(type='ValLoop')  # The validation loop type\ntest_cfg = dict(type='TestLoop')  # The testing loop type\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Transform in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to create a custom transform in MMDetection.  It defines a new class `MyTransform` that inherits from `BaseTransform` and registers it using `@TRANSFORMS.register_module()`. The transform randomly adds a 'dummy' key to the results dictionary based on a given probability. Requires mmcv and mmdet dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_transforms.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom mmcv.transforms import BaseTransform\nfrom mmdet.registry import TRANSFORMS\n\n\n@TRANSFORMS.register_module()\nclass MyTransform(BaseTransform):\n    \"\"\"Add your transform\n\n    Args:\n        p (float): Probability of shifts. Default 0.5.\n    \"\"\"\n\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def transform(self, results):\n        if random.random() > self.prob:\n            results['dummy'] = True\n        return results\n```\n\n----------------------------------------\n\nTITLE: MMDetection Model Configuration (RTMDet)\nDESCRIPTION: This Python dictionary defines the model configuration for RTMDet in MMDetection.  It specifies the backbone, neck, bbox_head, data preprocessor, and training/testing configurations. The config defines the network architecture and hyperparameters for object detection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    type='RTMDet',  # The name of detector\n    data_preprocessor=dict(  # The config of data preprocessor, usually includes image normalization and padding\n        type='DetDataPreprocessor',  # The type of the data preprocessor. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.data_preprocessors.DetDataPreprocessor\n        mean=[103.53, 116.28, 123.675],  # Mean values used to pre-training the pre-trained backbone models, ordered in R, G, B\n        std=[57.375, 57.12, 58.395],  # Standard variance used to pre-training the pre-trained backbone models, ordered in R, G, B\n        bgr_to_rgb=False,  # whether to convert image from BGR to RGB\n        batch_augments=None),  # Batch-level augmentations\n    backbone=dict(  # The config of backbone\n        type='CSPNeXt',  # The type of backbone network. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.backbones.CSPNeXt\n        arch='P5',  # Architecture of CSPNeXt, from {P5, P6}. Defaults to P5\n        expand_ratio=0.5,  # Ratio to adjust the number of channels of the hidden layer. Defaults to 0.5\n        deepen_factor=1,  # Depth multiplier, multiply number of blocks in CSP layer by this amount. Defaults to 1.0\n        widen_factor=1,  # Width multiplier, multiply number of channels in each layer by this amount. Defaults to 1.0\n        channel_attention=True,  # Whether to add channel attention in each stage. Defaults to True\n        norm_cfg=dict(type='SyncBN'),  # Dictionary to construct and config norm layer. Defaults to dict(type=’BN’, requires_grad=True)\n        act_cfg=dict(type='SiLU', inplace=True)),  # Config dict for activation layer. Defaults to dict(type=’SiLU’)\n    neck=dict(\n        type='CSPNeXtPAFPN',  # The type of neck is CSPNeXtPAFPN. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.necks.CSPNeXtPAFPN\n        in_channels=[256, 512, 1024],  # Number of input channels per scale\n        out_channels=256,  # Number of output channels (used at each scale)\n        num_csp_blocks=3,  # Number of bottlenecks in CSPLayer. Defaults to 3\n        expand_ratio=0.5,  # Ratio to adjust the number of channels of the hidden layer. Default: 0.5\n        norm_cfg=dict(type='SyncBN'),  # Config dict for normalization layer. Default: dict(type=’BN’)\n        act_cfg=dict(type='SiLU', inplace=True)),  # Config dict for activation layer. Default: dict(type=’Swish’)\n    bbox_head=dict(\n        type='RTMDetSepBNHead',  # The type of bbox_head is RTMDetSepBNHead. RTMDetHead with separated BN layers and shared conv layers. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.dense_heads.RTMDetSepBNHead\n        num_classes=80,  # Number of categories excluding the background category\n        in_channels=256,  # Number of channels in the input feature map\n        stacked_convs=2,  # Whether to share conv layers between stages. Defaults to True\n        feat_channels=256,  # Feature channels of convolutional layers in the head\n        anchor_generator=dict(  # The config of anchor generator\n            type='MlvlPointGenerator',  # The methods use MlvlPointGenerator. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/prior_generators/point_generator.py#L92\n            offset=0,  # The offset of points, the value is normalized with corresponding stride. Defaults to 0.5\n            strides=[8, 16, 32]),  # Strides of anchors in multiple feature levels in order (w, h)\n        bbox_coder=dict(type='DistancePointBBoxCoder'),  # Distance Point BBox coder.This coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left,right) and decode it back to the original. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/coders/distance_point_bbox_coder.py#L9\n        loss_cls=dict(  # Config of loss function for the classification branch\n            type='QualityFocalLoss',  # Type of loss for classification branch. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.losses.QualityFocalLoss\n            use_sigmoid=True,  # Whether sigmoid operation is conducted in QFL. Defaults to True\n            beta=2.0,  # The beta parameter for calculating the modulating factor. Defaults to 2.0\n            loss_weight=1.0),  #  Loss weight of current loss\n        loss_bbox=dict(  # Config of loss function for the regression branch\n            type='GIoULoss',  # Type of loss. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.losses.GIoULoss\n            loss_weight=2.0),  # Loss weight of the regression branch\n        with_objectness=False,  # Whether to add an objectness branch. Defaults to True\n        exp_on_reg=True,  # Whether to use .exp() in regression\n        share_conv=True,  # Whether to share conv layers between stages. Defaults to True\n        pred_kernel_size=1,  # Kernel size of prediction layer. Defaults to 1\n        norm_cfg=dict(type='SyncBN'),  # Config dict for normalization layer. Defaults to dict(type='BN', momentum=0.03, eps=0.001)\n        act_cfg=dict(type='SiLU', inplace=True)),  # Config dict for activation layer. Defaults to dict(type='SiLU')\n    train_cfg=dict(  # Config of training hyperparameters for ATSS\n        assigner=dict(  # Config of assigner\n            type='DynamicSoftLabelAssigner',   # Type of assigner. DynamicSoftLabelAssigner computes matching between predictions and ground truth with dynamic soft label assignment. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/assigners/dynamic_soft_label_assigner.py#L40\n            topk=13),  # Select top-k predictions to calculate dynamic k best matches for each gt. Defaults to 13\n        allowed_border=-1,  # The border allowed after padding for valid anchors\n        pos_weight=-1,  # The weight of positive samples during training\n        debug=False),  # Whether to set the debug mode\n    test_cfg=dict(  # Config for testing hyperparameters for ATSS\n        nms_pre=30000,  # The number of boxes before NMS\n        min_bbox_size=0,  # The allowed minimal box size\n        score_thr=0.001,  # Threshold to filter out boxes\n        nms=dict(  # Config of NMS in the second stage\n            type='nms',  # Type of NMS\n            iou_threshold=0.65),  # NMS threshold\n        max_per_img=300),  # Max number of detections of each image\n)\n```\n\n----------------------------------------\n\nTITLE: Inference with Image Path using DetInferencer\nDESCRIPTION: This snippet demonstrates how to perform object detection inference using the DetInferencer with an image path as input. The path to the image file is passed directly to the inferencer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninferencer('demo/demo.jpg')\n```\n\n----------------------------------------\n\nTITLE: Customize Optimizer Wrapper Constructor in MMDetection (Python)\nDESCRIPTION: This code snippet shows how to define a custom optimizer wrapper constructor, `MyOptimizerWrapperConstructor`, for fine-grained parameter tuning, such as setting weight decay for BatchNorm layers. It inherits from `DefaultOptiWrapperConstructor` and overrides the `__call__` method to return an `OptimWrapper` with custom settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.optim import DefaultOptiWrapperConstructor\n\nfrom mmdet.registry import OPTIM_WRAPPER_CONSTRUCTORS\nfrom .my_optimizer import MyOptimizer\n\n\n@OPTIM_WRAPPER_CONSTRUCTORS.register_module()\nclass MyOptimizerWrapperConstructor(DefaultOptimWrapperConstructor):\n\n    def __init__(self,\n                 optim_wrapper_cfg: dict,\n                 paramwise_cfg: Optional[dict] = None):\n\n    def __call__(self, model: nn.Module) -> OptimWrapper:\n\n        return optim_wrapper\n\n```\n\n----------------------------------------\n\nTITLE: Running Test Script with TTA\nDESCRIPTION: These commands show how to run the `tools/test.py` script with the `--tta` flag for different testing scenarios: single-GPU, CPU (by disabling GPUs), and multi-GPU (using `dist_test.sh`).  Placeholders `${CONFIG_FILE}`, `${CHECKPOINT_FILE}`, `${RESULT_FILE}`, and `${GPU_NUM}` need to be replaced with actual values.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n# Single-gpu testing\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--tta]\n\n# CPU: disable GPUs and run single-gpu testing script\nexport CUDA_VISIBLE_DEVICES=-1\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--tta]\n\n# Multi-gpu testing\nbash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    ${GPU_NUM} \\\n    [--tta]\n```\n\n----------------------------------------\n\nTITLE: Single GPU Model Testing with No Ground Truth\nDESCRIPTION: This shell command executes the testing script `tools/test.py` for a given model configuration and checkpoint file without ground truth data, useful for evaluating model performance on unlabeled datasets.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n# 单 GPU 测试\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Modifying CheckpointHook in MMDetection (Python)\nDESCRIPTION: This snippet shows how to modify the `CheckpointHook` by setting parameters like `interval`, `max_keep_ckpts`, and `save_optimizer`.  This allows control over how frequently checkpoints are saved, how many are kept, and whether the optimizer state is included.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        interval=1,\n        max_keep_ckpts=3,\n        save_optimizer=True))\n\n```\n\n----------------------------------------\n\nTITLE: Running GLIP Inference with Multiple Targets\nDESCRIPTION: This command runs the inference demo for the GLIP model using a demo image, model weights, and multiple text prompts to detect several object categories. The '--texts' argument specifies the objects to be detected, separated by '. '.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg glip_tiny_a_mmdet-b3654169.pth --texts 'bench. car'\n```\n\n----------------------------------------\n\nTITLE: Running Large Image Inference\nDESCRIPTION: This script performs object detection inference on large images by slicing them into smaller patches. It allows specifying various parameters such as the GPU device, score threshold, patch size, overlap ratio, merge IOU threshold, merge NMS type, batch size, debug mode, and whether to save patches.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npython demo/large_image_demo.py \\\n\t${IMG_PATH} \\\n\t${CONFIG_FILE} \\\n\t${CHECKPOINT_FILE} \\\n\t--device ${GPU_ID}  \\\n\t--show \\\n\t--tta  \\\n\t--score-thr ${SCORE_THR} \\\n\t--patch-size ${PATCH_SIZE} \\\n\t--patch-overlap-ratio ${PATCH_OVERLAP_RATIO} \\\n\t--merge-iou-thr ${MERGE_IOU_THR} \\\n\t--merge-nms-type ${MERGE_NMS_TYPE} \\\n\t--batch-size ${BATCH_SIZE} \\\n\t--debug \\\n\t--save-patch\n```\n\n----------------------------------------\n\nTITLE: Phrase Grounding with NLTK - Shell\nDESCRIPTION: This shell command demonstrates phrase grounding using MM Grounding DINO with automatic noun phrase extraction via NLTK. It runs the `image_demo.py` script with a configuration, weights file, and a natural language description provided via the `--texts` parameter. The script uses NLTK to extract noun phrases and detect corresponding objects in the image.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/apples.jpg \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'There are many apples here.'\n```\n\n----------------------------------------\n\nTITLE: Configuring Visualizer Backends in MMDetection\nDESCRIPTION: This code snippet demonstrates how to configure the Visualizer and its backends in MMDetection. It shows how to set up the Visualizer type and specify the visualization backends to use, such as the LocalVisBackend, which saves visualization results to a local folder.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/visualization.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nvis_backends = [dict(type='LocalVisBackend')]\nvisualizer = dict(\n    type='DetLocalVisualizer',\n    vis_backends=vis_backends,\n    name='visualizer')\n```\n\n----------------------------------------\n\nTITLE: Implementing a custom CheckInvalidLossHook\nDESCRIPTION: This snippet shows how to implement a custom hook, `CheckInvalidLossHook`, that checks for NaN or infinite loss values during training. It inherits from the `Hook` class in MMEngine and overrides the `after_train_iter` method. It uses the `@HOOKS.register_module()` decorator to register the hook.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_hooks.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nfrom mmengine.hooks import Hook\nfrom mmengine.runner import Runner\n\nfrom mmdet.registry import HOOKS\n\n\n@HOOKS.register_module()\nclass CheckInvalidLossHook(Hook):\n    \"\"\"Check invalid loss hook.\n\n    This hook will regularly check whether the loss is valid\n    during training.\n\n    Args:\n        interval (int): Checking interval (every k iterations).\n            Default: 50.\n    \"\"\"\n\n    def __init__(self, interval: int = 50) -> None:\n        self.interval = interval\n\n    def after_train_iter(self,\n                         runner: Runner,\n                         batch_idx: int,\n                         data_batch: Optional[dict] = None,\n                         outputs: Optional[dict] = None) -> None:\n        \"\"\"Regularly check whether the loss is valid every n iterations.\n\n        Args:\n            runner (:obj:`Runner`): The runner of the training process.\n            batch_idx (int): The index of the current batch in the train loop.\n            data_batch (dict, Optional): Data from dataloader.\n                Defaults to None.\n            outputs (dict, Optional): Outputs from model. Defaults to None.\n        \"\"\"\n        if self.every_n_train_iters(runner, self.interval):\n            assert torch.isfinite(outputs['loss']),\n                runner.logger.info('loss become infinite or NaN!')\n```\n\n----------------------------------------\n\nTITLE: Iter-Based Training Configuration in MMDetection\nDESCRIPTION: This code demonstrates how to configure MMDetection for iter-based training instead of the default epoch-based training. It involves modifying the `train_cfg`, `param_scheduler`, `train_dataloader`, `default_hooks`, and `log_processor`. This snippet shows the changes needed to switch from an epoch-based to an iter-based training loop, including setting the loop type, maximum iterations, validation interval, learning rate scheduler, sampler, checkpoint saving interval, and log format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Iter-based training config\ntrain_cfg = dict(\n    _delete_=True,  # Ignore the base config setting (optional)\n    type='IterBasedTrainLoop',  # Use iter-based training loop\n    max_iters=90000,  # Maximum iterations\n    val_interval=10000)  # Validation interval\n\n\n# Change the scheduler to iter-based\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=90000,\n        by_epoch=False,\n        milestones=[60000, 80000],\n        gamma=0.1)\n]\n\n# Switch to InfiniteSampler to avoid dataloader restart\ntrain_dataloader = dict(sampler=dict(type='InfiniteSampler'))\n\n# Change the checkpoint saving interval to iter-based\ndefault_hooks = dict(checkpoint=dict(by_epoch=False, interval=10000))\n\n# Change the log format to iter-based\nlog_processor = dict(by_epoch=False)\n```\n\n----------------------------------------\n\nTITLE: Overriding Backbone Configuration with HRNet\nDESCRIPTION: This code snippet demonstrates how to replace the default backbone (ResNet) in a Mask R-CNN configuration with HRNet, using `_delete_=True` to remove the existing backbone configuration.  It shows how to define the HRNet architecture and its specific parameters within the `model` dictionary.  The `_delete_=True` ensures that all previous backbone settings are removed before applying the new HRNet configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n_base_ = '../mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256))),\n        init_cfg=dict(type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w32')),\n    neck=dict(...))\n```\n\n----------------------------------------\n\nTITLE: Modifying Training Schedule in MMDetection\nDESCRIPTION: This code snippet demonstrates modifying the training schedule for finetuning. It adjusts the optimizer's learning rate, defines a learning rate scheduler with linear warm-up and multi-step decay, sets the maximum number of training epochs, and configures the logging interval.  A smaller learning rate and fewer epochs are often used for finetuning compared to training from scratch.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/finetune.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# optimizer\n# lr is set for a batch size of 8\noptim_wrapper = dict(optimizer=dict(lr=0.01))\n\n# learning rate\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=8,\n        by_epoch=True,\n        milestones=[7],\n        gamma=0.1)\n]\n\n# max_epochs\ntrain_cfg = dict(max_epochs=8)\n\n# log config\ndefault_hooks = dict(logger=dict(interval=100)),\n```\n\n----------------------------------------\n\nTITLE: Define a Custom Optimizer in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to define a custom optimizer, `MyOptimizer`, within MMDetection. It registers the optimizer with the `OPTIMIZERS` registry and subclasses `torch.optim.Optimizer`.  The code shows the basic structure of a custom optimizer with initialization parameters a, b, and c. The optimizer needs to be located in `mmdet/engine/optimizers/my_optimizer.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.registry import OPTIMIZERS\nfrom torch.optim import Optimizer\n\n\n@OPTIMIZERS.register_module()\nclass MyOptimizer(Optimizer):\n\n    def __init__(self, a, b, c)\n\n```\n\n----------------------------------------\n\nTITLE: Testing with Single GPU, CPU, Multi-GPU in MMDetection\nDESCRIPTION: This snippet demonstrates how to test a model using single GPU, CPU, and multiple GPUs. It shows the basic commands for running the `test.py` script with different configurations, including disabling GPUs for CPU testing and using `dist_test.sh` for multi-GPU testing. The snippet also lists the optional arguments such as result file, show, and show-dir.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Single-gpu testing\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--show]\n\n# CPU: disable GPUs and run single-gpu testing script\nexport CUDA_VISIBLE_DEVICES=-1\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--show]\n\n# Multi-gpu testing\nbash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    ${GPU_NUM} \\\n    [--out ${RESULT_FILE}]\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Hooks in MMDetection\nDESCRIPTION: This code snippet shows how to define a list of custom hooks in MMDetection. Custom hooks allow users to insert their own operations during the training, validation, and testing loops. Users can develop their own hooks and insert them into this list. This example showcases an empty list, indicating no custom hooks are currently defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = []\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection from Source\nDESCRIPTION: Clones the MMDetection repository and installs it in editable mode.  This allows for direct modification of the code without reinstallation. The '-v' flag provides verbose output during installation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/open-mmlab/mmdetection.git\ncd mmdetection\npip install -v -e .\n```\n\n----------------------------------------\n\nTITLE: Create and activate conda environment\nDESCRIPTION: This snippet creates a new conda environment named 'openmmlab' with Python 3.8 and then activates it. This isolates the project dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nconda create --name openmmlab python=3.8 -y\nconda activate openmmlab\n```\n\n----------------------------------------\n\nTITLE: Testing MMDetection Model\nDESCRIPTION: This shell command executes the `tools/test.py` script to evaluate a trained MMDetection model. It requires the configuration file (`configs/cityscapes/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes.py`) and the model checkpoint file (`work_dirs/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes/epoch_10.pth`) as arguments. The script will then run the testing process based on the provided configuration and evaluate the model's performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/test.py configs/cityscapes/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes.py work_dirs/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes/epoch_10.pth\n```\n\n----------------------------------------\n\nTITLE: Import Custom Optimizer via custom_imports in MMDetection (Python)\nDESCRIPTION: This code snippet uses `custom_imports` to import the module containing the custom optimizer. `mmdet.engine.optimizers.my_optimizer` is imported at the beginning of the program, which automatically registers the `MyOptimizer` class. Only the package containing the class should be imported.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(imports=['mmdet.engine.optimizers.my_optimizer'], allow_failed_imports=False)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Model and Checkpoint\nDESCRIPTION: This code initializes the `DetInferencer` with both a model name ('rtmdet_tiny_8xb32-300e_coco') and a specific checkpoint file. This allows for using a specific pre-trained weight for inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Setup a checkpoint file to load\ncheckpoint = './checkpoints/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n\n# Initialize the DetInferencer\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', weights=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Customize Training Schedule with CosineAnnealingLR in MMDetection (Python)\nDESCRIPTION: This code snippet configures a `CosineAnnealingLR` learning rate scheduler within the `param_scheduler` list. It specifies the maximum number of epochs (T_max=8), the minimum learning rate (eta_min=lr * 1e-5), and the beginning and end epochs for the scheduler.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='CosineAnnealingLR',\n        T_max=8,\n        eta_min=lr * 1e-5,\n        begin=0,\n        end=8,\n        by_epoch=True)]\n\n\n```\n\n----------------------------------------\n\nTITLE: Setting Sampler for CrossEntropyLoss in RPN Head\nDESCRIPTION: This configuration snippet demonstrates how to set the sampler for the CrossEntropyLoss within the RPN head in MMDetection. It defines the type of sampler, the number of samples, the fraction of positive samples, and other related parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg=dict(\n    rpn=dict(\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False))\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating OpenImages Evaluator Configuration in MMDetection\nDESCRIPTION: Shows how to migrate the OpenImages evaluator configuration from MMDetection v2.x to v3.x. Like other evaluators, OpenImages now uses a dedicated `OpenImagesMetric` in v3.x.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type='OpenImagesDataset',\n        ann_file=data_root + 'annotations/validation-annotations-bbox.csv',\n        img_prefix=data_root + 'OpenImages/validation/',\n        label_file=data_root + 'annotations/class-descriptions-boxable.csv',\n        hierarchy_file=data_root +\n        'annotations/bbox_labels_600_hierarchy.json',\n        meta_file=data_root + 'annotations/validation-image-metas.pkl',\n        image_level_ann_file=data_root +\n        'annotations/validation-annotations-human-imagelabels-boxable.csv'))\nevaluation = dict(interval=1, metric='mAP')\n```\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='OpenImagesMetric',\n    iou_thrs=0.5,\n    ioa_thrs=0.5,\n    use_group_of=True,\n    get_supercategory=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a new neck (AugFPN) in MMDetection\nDESCRIPTION: This Python code defines a new neck module called `AugFPN` for the MMDetection framework. It registers the module with `@MODELS.register_module()` for easy use in configs. The `forward` function is a placeholder.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.nn as nn\nfrom mmdet.registry import MODELS\n\n\n@MODELS.register_module()\nclass AugFPN(nn.Module):\n\n    def __init__(self,\n                in_channels,\n                out_channels,\n                num_outs,\n                start_level=0,\n                end_level=-1,\n                add_extra_convs=False):\n        pass\n\n    def forward(self, inputs):\n        # implementation is ignored\n        pass\n```\n\n----------------------------------------\n\nTITLE: Customize Optimizer Supported by PyTorch in MMDetection (Python)\nDESCRIPTION: This code snippet shows how to use a different optimizer, specifically Adam, that is supported by PyTorch, by modifying the `optimizer` field within `optim_wrapper` in the MMDetection configuration file. The snippet also demonstrates how to modify learning rate and weight decay using the same method.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='Adam', lr=0.0003, weight_decay=0.0001))\n\n```\n\n----------------------------------------\n\nTITLE: Migrating Data Preprocessor Config (MMDetection)\nDESCRIPTION: This snippet shows how to configure the `DataPreprocessor` module in MMDetection 3.x.  This module handles data preprocessing steps like normalization and padding. The `mean`, `std`, `bgr_to_rgb`, `pad_mask`, and `pad_size_divisor` parameters define the normalization and padding behavior.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    data_preprocessor=dict(\n        type='DetDataPreprocessor',\n        # 图像归一化参数\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True,\n        # 图像 padding 参数\n        pad_mask=True,  # 在实例分割中，需要将 mask 也进行 padding\n        pad_size_divisor=32)  # 图像 padding 到 32 的倍数\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Config and Weights (Python)\nDESCRIPTION: This snippet shows how to initialize the `DetInferencer` with a custom configuration file and a weight file. This allows using custom models defined outside of the pre-trained model zoo.  Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='path/to/rtmdet_config.py', weights='path/to/rtmdet.pth')\n```\n\n----------------------------------------\n\nTITLE: Filter Low Score Prediction Results (analyze_results.py)\nDESCRIPTION: This example filters low score prediction results by specifying the `show-score-thr` parameter to 0.3. It requires a configuration file (`configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py`) and a prediction file (`result.pkl`). The results are saved in `results` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       result.pkl \\\n       results \\\n       --show-score-thr 0.3\n```\n\n----------------------------------------\n\nTITLE: Configure SoftTeacher model in Python\nDESCRIPTION: This code snippet configures the SoftTeacher model for semi-supervised object detection. It inherits from a base Faster R-CNN configuration and modifies the data preprocessor, backbone network, and semi-training/testing configurations.  Specifically, it configures the ResNet backbone with Caffe style, sets up the MultiBranchDataPreprocessor, and defines parameters for freezing the teacher model, adjusting supervision weights, and setting pseudo-label thresholds.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py', '../_base_/default_runtime.py',\n    '../_base_/datasets/semi_coco_detection.py'\n]\n\ndetector = _base_.model\ndetector.data_preprocessor = dict(\n    type='DetDataPreprocessor',\n    mean=[103.530, 116.280, 123.675],\n    std=[1.0, 1.0, 1.0],\n    bgr_to_rgb=False,\n    pad_size_divisor=32)\ndetector.backbone = dict(\n    type='ResNet',\n    depth=50,\n    num_stages=4,\n    out_indices=(0, 1, 2, 3),\n    frozen_stages=1,\n    norm_cfg=dict(type='BN', requires_grad=False),\n    norm_eval=True,\n    style='caffe',\n    init_cfg=dict(\n        type='Pretrained',\n        checkpoint='open-mmlab://detectron2/resnet50_caffe'))\n\nmodel = dict(\n    _delete_=True,\n    type='SoftTeacher',\n    detector=detector,\n    data_preprocessor=dict(\n        type='MultiBranchDataPreprocessor',\n        data_preprocessor=detector.data_preprocessor),\n    semi_train_cfg=dict(\n        freeze_teacher=True,\n        sup_weight=1.0,\n        unsup_weight=4.0,\n        pseudo_label_initial_score_thr=0.5,\n        rpn_pseudo_thr=0.9,\n        cls_pseudo_thr=0.9,\n        reg_pseudo_thr=0.02,\n        jitter_times=10,\n        jitter_scale=0.06,\n        min_pseudo_bbox_wh=(1e-2, 1e-2)),\n    semi_test_cfg=dict(predict_on='teacher'))\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Model in MMDetection\nDESCRIPTION: This code snippet shows how to specify the path to a pre-trained model for finetuning. The `load_from` variable should contain the URL or local path to the pre-trained weights.  Downloading the weights before training can save time during the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/finetune.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'  # noqa\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluators for MMDetection (Python)\nDESCRIPTION: This code snippet configures the evaluators used to compute the metrics of the trained model on the validation and testing datasets in MMDetection. The config includes the evaluator type, annotation file path, and metrics to be evaluated. The snippet also provides an example for setting up the evaluator for a test dataset without annotation files, specifically saving the detection results in COCO JSON format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(  # Validation evaluator config\n    type='CocoMetric',  # The coco metric used to evaluate AR, AP, and mAP for detection and instance segmentation\n    ann_file=data_root + 'annotations/instances_val2017.json',  # Annotation file path\n    metric=['bbox', 'segm'],  # Metrics to be evaluated, `bbox` for detection and `segm` for instance segmentation\n    format_only=False,\n    backend_args=backend_args)\ntest_evaluator = val_evaluator  # Testing evaluator config\n```\n\n----------------------------------------\n\nTITLE: Defining Default Hooks in MMDetection\nDESCRIPTION: This code snippet demonstrates how to define the default hooks within an MMDetection configuration file. Default hooks are essential components executed during the training, validation, and testing loops.  Each hook config specifies the 'type' of hook and optionally includes other parameters for customization. These hooks manage tasks like timing, logging, parameter scheduling, checkpointing, distributed sampling, and visualization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    timer=dict(type='IterTimerHook'),  # Update the time spent during iteration into message hub\n    logger=dict(type='LoggerHook', interval=50),  # Collect logs from different components of Runner and write them to terminal, JSON file, tensorboard and wandb .etc\n    param_scheduler=dict(type='ParamSchedulerHook'), # update some hyper-parameters of optimizer\n    checkpoint=dict(type='CheckpointHook', interval=1), # Save checkpoints periodically\n    sampler_seed=dict(type='DistSamplerSeedHook'),  # Ensure distributed Sampler shuffle is active\n    visualization=dict(type='DetVisualizationHook'))  # Detection Visualization Hook. Used to visualize validation and testing process prediction results\n```\n\n----------------------------------------\n\nTITLE: Migrating Resize Configuration in MMDetection\nDESCRIPTION: Shows the change in the Resize transform configuration from v2.x to v3.x. The parameter name `img_scale` is changed to `scale`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndict(type='Resize',\n     img_scale=(1333, 800),\n     keep_ratio=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(type='Resize',\n     scale=(1333, 800),\n     keep_ratio=True)\n```\n\n----------------------------------------\n\nTITLE: Large Image Inference Example (No TTA)\nDESCRIPTION: This example demonstrates how to run large image inference without test-time augmentation (TTA). It downloads a Faster R-CNN checkpoint and then runs the inference script on a sample image using the downloaded checkpoint and a corresponding configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n# inferecnce without tta\nwget -P checkpoint https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r101_fpn_2x_coco/faster_rcnn_r101_fpn_2x_coco_bbox_mAP-0.398_20200504_210455-1d2dac9c.pth\n\npython demo/large_image_demo.py \\\n    demo/large_image.jpg \\\n    configs/faster_rcnn/faster-rcnn_r101_fpn_2x_coco.py \\\n    checkpoint/faster_rcnn_r101_fpn_2x_coco_bbox_mAP-0.398_20200504_210455-1d2dac9c.pth\n```\n\n----------------------------------------\n\nTITLE: Running Video Inference\nDESCRIPTION: This script performs object detection inference on a video file using a specified configuration and checkpoint file. It allows specifying the GPU device, score threshold, output file, display option, and wait time.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_demo.py \\\n    ${VIDEO_FILE} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--score-thr ${SCORE_THR}] \\\n    [--out ${OUT_FILE}] \\\n    [--show] \\\n    [--wait-time ${WAIT_TIME}]\n```\n\n----------------------------------------\n\nTITLE: Model Evaluation Interval Configuration\nDESCRIPTION: This Python code snippet shows how to configure the evaluation interval during training in MMDetection. The `val_interval` parameter within the `train_cfg` dictionary specifies the number of epochs between evaluations on the validation set. In this example, the model is evaluated every 12 epochs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# evaluate the model every 12 epochs.\ntrain_cfg = dict(val_interval=12)\n```\n\n----------------------------------------\n\nTITLE: Inference on COCO Test-Dev (Shell)\nDESCRIPTION: This script shows how to perform inference on the COCO test-dev set using `tools/test.py`. It demonstrates updating the `test_dataloader` and `test_evaluator` settings using command-line arguments to specify the correct annotation file and data prefix. The configuration file and checkpoint file must be specified.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test_results_submission.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# 用一个 gpu 测试\nCUDA_VISIBLE_DEVICES=0 python tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=${WORK_DIR}/results\n# 用四个 gpu 测试\nCUDA_VISIBLE_DEVICES=0,1,3,4 bash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    8 \\  # eights gpus\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=${WORK_DIR}/results\n# 用 slurm 测试\nGPUS=8 tools/slurm_test.sh \\\n    ${Partition} \\\n    ${JOB_NAME} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=${WORK_DIR}/results\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Proposals in CascadeRoIHead (Python)\nDESCRIPTION: This code snippet demonstrates how to handle empty proposals in the `CascadeRoIHead` in MMDetection, addressing both cases where the entire batch has no proposals and where individual images have no proposals. This ensures that the detector can gracefully handle such scenarios without crashing. This involves creating zero-filled arrays to return results of correct shapes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/conventions.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# simple_test method\n...\n# There is no proposal in the whole batch\nif rois.shape[0] == 0:\n    bbox_results = [[\n        np.zeros((0, 5), dtype=np.float32)\n        for _ in range(self.bbox_head[-1].num_classes)\n    ]] * num_imgs\n    if self.with_mask:\n        mask_classes = self.mask_head[-1].num_classes\n        segm_results = [[[] for _ in range(mask_classes)]\n                        for _ in range(num_imgs)]\n        results = list(zip(bbox_results, segm_results))\n    else:\n        results = bbox_results\n    return results\n...\n\n# There is no proposal in the single image\nfor i in range(self.num_stages):\n    ...\n    if i < self.num_stages - 1:\n          for j in range(num_imgs):\n                # Handle empty proposal\n                if rois[j].shape[0] > 0:\n                    bbox_label = cls_score[j][:, :-1].argmax(dim=1)\n                    refine_roi = self.bbox_head[i].regress_by_class(\n                         rois[j], bbox_label, bbox_pred[j], img_metas[j])\n                    refine_roi_list.append(refine_roi)\n```\n\n----------------------------------------\n\nTITLE: Training Grounding DINO Model (mmdetection)\nDESCRIPTION: This command trains the Grounding DINO model using distributed training with 8 GPUs. It specifies the configuration file, the number of GPUs, and a working directory to save the training results. The `dist_train.sh` script handles the distributed training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/grounding_dino/grounding_dino_swin-t_finetune_8xb2_20e_cat.py 8 --work-dir cat_work_dir\n```\n\n----------------------------------------\n\nTITLE: CityScapes Evaluator Configuration (3.x)\nDESCRIPTION: This code snippet demonstrates the CityScapes evaluator configuration in MMDetection 3.x. It uses a list of evaluators: `CocoMetric` and `CityScapesMetric`. It specifies the annotation file, the evaluation metrics (`bbox` and `segm`), and `seg_prefix` and `outfile_prefix` for `CityScapesMetric`. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = [\n    dict(\n        type='CocoMetric',\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        metric=['bbox', 'segm']),\n    dict(\n        type='CityScapesMetric',\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        seg_prefix=data_root + '/gtFine/val',\n        outfile_prefix='./work_dirs/cityscapes_metric/instance')\n]\n```\n\n----------------------------------------\n\nTITLE: Using a Pre-Implemented Hook in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to use an existing hook, `NumClassCheckHook`, by directly adding it to the `custom_hooks` list in the configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [dict(type='NumClassCheckHook')]\n```\n\n----------------------------------------\n\nTITLE: Example of comparing TorchServe and PyTorch results\nDESCRIPTION: This is an example execution of the test_torchserver.py script. It takes image, config and checkpoint files and a model name as input to compare TorchServe's output to that of a PyTorch model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npython tools/deployment/test_torchserver.py \\\ndemo/demo.jpg \\\nconfigs/yolo/yolov3_d53_8xb8-320-273e_coco.py \\\ncheckpoint/yolov3_d53_320_273e_coco-421362b6.pth \\\nyolov3 \\\n--work-dir ./work-dir\n```\n\n----------------------------------------\n\nTITLE: Video Inference Example\nDESCRIPTION: This is an example of how to run the video inference demo script with specific video, configuration, and checkpoint files. It demonstrates how to specify the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_demo.py demo/demo.mp4 \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --out result.mp4\n```\n\n----------------------------------------\n\nTITLE: Calculating FLOPs and parameters of a model\nDESCRIPTION: This command calculates the FLOPs (Floating Point Operations) and the number of parameters of a given model using the get_flops.py script. It requires a configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/get_flops.py ${CONFIG_FILE} [--shape ${INPUT_SHAPE}]\n```\n\n----------------------------------------\n\nTITLE: Use Momentum Schedule in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to use a momentum schedule, along with a learning rate scheduler, to accelerate model convergence in MMDetection. It uses `CosineAnnealingLR` for the learning rate and `CosineAnnealingMomentum` for the momentum, with different configurations for the first 8 epochs and the subsequent 12 epochs. These schedulers are part of the `param_scheduler` list.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    # learning rate scheduler\n    # During the first 8 epochs, learning rate increases from 0 to lr * 10\n    # during the next 12 epochs, learning rate decreases from lr * 10 to lr * 1e-4\n    dict(\n        type='CosineAnnealingLR',\n        T_max=8,\n        eta_min=lr * 10,\n        begin=0,\n        end=8,\n        by_epoch=True,\n        convert_to_iter_based=True),\n    dict(\n        type='CosineAnnealingLR',\n        T_max=12,\n        eta_min=lr * 1e-4,\n        begin=8,\n        end=20,\n        by_epoch=True,\n        convert_to_iter_based=True),\n    # momentum scheduler\n    # During the first 8 epochs, momentum increases from 0 to 0.85 / 0.95\n    # during the next 12 epochs, momentum increases from 0.85 / 0.95 to 1\n    dict(\n        type='CosineAnnealingMomentum',\n        T_max=8,\n        eta_min=0.85 / 0.95,\n        begin=0,\n        end=8,\n        by_epoch=True,\n        convert_to_iter_based=True),\n    dict(\n        type='CosineAnnealingMomentum',\n        T_max=12,\n        eta_min=1,\n        begin=8,\n        end=20,\n        by_epoch=True,\n        convert_to_iter_based=True)\n]\n\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer on GPU (Python)\nDESCRIPTION: This snippet demonstrates how to initialize the `DetInferencer` on a specific GPU device.  Requires the `mmdet` library and a CUDA-enabled GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cuda:1')\n```\n\n----------------------------------------\n\nTITLE: Dataset and Data Loader Configuration - Python\nDESCRIPTION: This snippet configures the dataset and data loaders for training and validation in MMDetection using the CocoDataset. It defines the dataset type, root path, training and testing pipelines (including image loading, resizing, flipping, and annotation packing), batch sizes, number of workers, samplers, and dataset-specific configurations like annotation files and image prefixes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndataset_type = 'CocoDataset'  # 数据集类型，这将被用来定义数据集。\ndata_root = 'data/coco/'  # 数据的根路径.\n\ntrain_pipeline = [  # 训练数据处理流程\n    dict(type='LoadImageFromFile'),  # 第 1 个流程，从文件路径里加载图像。\n    dict(\n        type='LoadAnnotations',  # 第 2 个流程，对于当前图像，加载它的注释信息。\n        with_bbox=True,  # 是否使用标注框(bounding box)， 目标检测需要设置为 True。\n        with_mask=True,  # 是否使用 instance mask，实例分割需要设置为 True。\n        poly2mask=False),  # 是否将 polygon mask 转化为 instance mask, 设置为 False 以加速和节省内存。\n    dict(\n        type='Resize',  # 变化图像和其标注大小的流程。\n        scale=(1333, 800),  # 图像的最大尺寸\n        keep_ratio=True  # 是否保持图像的长宽比。\n        ),\n    dict(\n        type='RandomFlip',  # 翻转图像和其标注的数据增广流程。\n        prob=0.5),  # 翻转图像的概率。\n    dict(type='PackDetInputs')  # 将数据转换为检测器输入格式的流程\n]\ntest_pipeline = [  # 测试数据处理流程\n    dict(type='LoadImageFromFile'),  # 第 1 个流程，从文件路径里加载图像。\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # 变化图像大小的流程。\n    dict(\n        type='PackDetInputs',  # 将数据转换为检测器输入格式的流程\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(  # 训练 dataloader 配置\n    batch_size=2,  # 单个 GPU 的 batch size\n    num_workers=2,  # 单个 GPU 分配的数据加载线程数\n    persistent_workers=True,  # 如果设置为 True，dataloader 在迭代完一轮之后不会关闭数据读取的子进程，可以加速训练\n    sampler=dict(  # 训练数据的采样器\n        type='DefaultSampler',  # 默认的采样器，同时支持分布式和非分布式训练。请参考 https://mmengine.readthedocs.io/zh_CN/latest/api/generated/mmengine.dataset.DefaultSampler.html#mmengine.dataset.DefaultSampler\n        shuffle=True),  # 随机打乱每个轮次训练数据的顺序\n    batch_sampler=dict(type='AspectRatioBatchSampler'),  # 批数据采样器，用于确保每一批次内的数据拥有相似的长宽比，可用于节省显存\n    dataset=dict(  # 训练数据集的配置\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_train2017.json',  # 标注文件路径\n        data_prefix=dict(img='train2017/'),  # 图片路径前缀\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),  # 图片和标注的过滤配置\n        pipeline=train_pipeline))  # 这是由之前创建的 train_pipeline 定义的数据处理流程。\nval_dataloader = dict(  # 验证 dataloader 配置\n    batch_size=1,  # 单个 GPU 的 Batch size。如果 batch-szie > 1，组成 batch 时的额外填充会影响模型推理精度\n    num_workers=2,  # 单个 GPU 分配的数据加载线程数\n    persistent_workers=True,  # 如果设置为 True，dataloader 在迭代完一轮之后不会关闭数据读取的子进程，可以加速训练\n    drop_last=False,  # 是否丢弃最后未能组成一个批次的数据\n    sampler=dict(\n        type='DefaultSampler',\n        shuffle=False),  # 验证和测试时不打乱数据顺序\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_val2017.json',\n        data_prefix=dict(img='val2017/'),\n        test_mode=True,  # 开启测试模式，避免数据集过滤图片和标注\n        pipeline=test_pipeline))\ntest_dataloader = val_dataloader  # 测试 dataloader 配置\n```\n\n----------------------------------------\n\nTITLE: Customizing Optimizer with AdamW in MMDetection\nDESCRIPTION: This code snippet shows how to customize the optimizer using AdamW in MMDetection. It includes settings for learning rate, weight decay, and gradient clipping, as well as parameter-wise configurations for the backbone.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    # 优化器\n    optimizer=dict(\n        type='AdamW',\n        lr=0.0001,\n        weight_decay=0.05,\n        eps=1e-8,\n        betas=(0.9, 0.999)),\n\n    # 参数层面的学习率和正则化设置\n    paramwise_cfg=dict(\n        custom_keys={\n            'backbone': dict(lr_mult=0.1, decay_mult=1.0),\n        },\n        norm_decay_mult=0.0),\n\n    # 梯度裁剪\n    clip_grad=dict(max_norm=0.01, norm_type=2))\n\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with a Specific Device (CPU)\nDESCRIPTION: This snippet initializes the `DetInferencer` and explicitly sets the device to 'cpu'. This forces the inference to be performed on the CPU, even if a GPU is available.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cpu')\n```\n\n----------------------------------------\n\nTITLE: Tweaking FocalLoss Hyperparameters - Python\nDESCRIPTION: This snippet modifies the `gamma` and `alpha` hyperparameters of the `FocalLoss` in the configuration.  It demonstrates how to change these values (gamma to 1.5 and alpha to 0.5) through the config, allowing for fine-tuning of the loss function's behavior during training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=1.5,\n    alpha=0.5,\n    loss_weight=1.0)\n```\n\n----------------------------------------\n\nTITLE: RTMDet Testing with Image Saving in MMDetection\nDESCRIPTION: This snippet demonstrates how to test the RTMDet model and save the painted images for future visualization.  It specifies the configuration file and checkpoint file for RTMDet and uses the `--show-dir` flag to save the detection results to the `faster_rcnn_r50_fpn_1x_results` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --show-dir faster_rcnn_r50_fpn_1x_results\n```\n\n----------------------------------------\n\nTITLE: Inferencing with MMDeploy Model Converter API using Python\nDESCRIPTION: This Python snippet utilizes the MMDeploy Model Converter API to perform inference on an image using a converted RTMDet model. It specifies the model configuration, deployment configuration, backend files (e.g., TensorRT engine), input image, and device (e.g., CUDA). The result is stored in the `result` variable.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nfrom mmdeploy.apis import inference_model\n\nresult = inference_model(\n  model_cfg='${PATH_TO_MMDET}/configs/rtmdet/rtmdet_s_8xb32-300e_coco.py',\n  deploy_cfg='${PATH_TO_MMDEPLOY}/configs/mmdet/detection/detection_tensorrt_static-640x640.py',\n  backend_files=['work_dirs/rtmdet/end2end.engine'],\n  img='demo/resources/det.jpg',\n  device='cuda:0')\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset and Evaluation Metrics\nDESCRIPTION: This configuration modifies the dataset related settings such as data root, metadata, and annotation files, along with evaluation metrics for training a Mask R-CNN model on the balloon dataset.  It specifies paths to training and validation data and sets classes and color palettes. The evaluation metrics are set to use the coco annotation format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndata_root = 'data/balloon/'\nmetainfo = {\n    'classes': ('balloon', ),\n    'palette': [\n        (220, 20, 60),\n    ]\n}\ntrain_dataloader = dict(\n    batch_size=1,\n    dataset=dict(\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='train/annotation_coco.json',\n        data_prefix=dict(img='train/')))\nval_dataloader = dict(\n    dataset=dict(\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='val/annotation_coco.json',\n        data_prefix=dict(img='val/')))\ntest_dataloader = val_dataloader\n\n# 修改评价指标相关配置\nval_evaluator = dict(ann_file=data_root + 'val/annotation_coco.json')\ntest_evaluator = val_evaluator\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer on CPU (Python)\nDESCRIPTION: This snippet demonstrates how to initialize the `DetInferencer` on the CPU.  Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cpu')\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Configuration (Python)\nDESCRIPTION: This Python code defines a configuration for training a Mask R-CNN model on the balloon dataset using MMDetection. It modifies the base Mask R-CNN configuration by setting the number of classes to 1 (balloon), updating the dataset paths and metadata, and specifying evaluation metrics. It loads pre-trained weights for faster convergence.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# The new config inherits a base config to highlight the necessary modification\n_base_ = '../mask_rcnn/mask-rcnn_r50-caffe_fpn_ms-poly-1x_coco.py'\n\n# We also need to change the num_classes in head to match the dataset's annotation\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(num_classes=1), mask_head=dict(num_classes=1)))\n\n# Modify dataset related settings\ndata_root = 'data/balloon/'\nmetainfo = {\n    'classes': ('balloon', ),\n    'palette': [\n        (220, 20, 60),\n    ]\n}\ntrain_dataloader = dict(\n    batch_size=1,\n    dataset=dict(\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='train/annotation_coco.json',\n        data_prefix=dict(img='train/')))\nval_dataloader = dict(\n    dataset=dict(\n        data_root=data_root,\n        metainfo=metainfo,\n        ann_file='val/annotation_coco.json',\n        data_prefix=dict(img='val/')))\ntest_dataloader = val_dataloader\n\n# Modify metric related settings\nval_evaluator = dict(ann_file=data_root + 'val/annotation_coco.json')\ntest_evaluator = val_evaluator\n\n# We can use the pre-trained Mask RCNN model to obtain higher performance\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpoint Hook for Best Model Saving in MMDetection\nDESCRIPTION: This snippet demonstrates how to configure the `CheckpointHook` in MMDetection to save the best model during training.  The `save_best='auto'` setting automatically selects the best model based on the first key in the validation results. Alternatively, you can specify a key manually, like `save_best='coco/bbox_mAP'`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_16\n\nLANGUAGE: text\nCODE:\n```\ndefault_hooks = dict(checkpoint=dict(type='CheckpointHook', interval=1, save_best='auto')\n```\n\n----------------------------------------\n\nTITLE: Tweaking FocalLoss Reduction Method - Python\nDESCRIPTION: This code changes the reduction method of `FocalLoss` from `mean` to `sum`.  By modifying the `reduction` parameter, the accumulated loss will be summed instead of averaged across all samples, influencing how the gradients are calculated and applied during optimization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=2.0,\n    alpha=0.25,\n    loss_weight=1.0,\n    reduction='sum')\n```\n\n----------------------------------------\n\nTITLE: Listing Available Models in MMDetection (Python)\nDESCRIPTION: This code demonstrates how to list the available pre-trained models in MMDetection using the `DetInferencer.list_models` method. It requires the `mmdet` library to be installed. The method returns a list of model names.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# models is a list of model names, and them will print automatically\nmodels = DetInferencer.list_models('mmdet')\n```\n\n----------------------------------------\n\nTITLE: Training Configuration for RPN and RCNN in MMDetection\nDESCRIPTION: This snippet configures the training hyperparameters for the Region Proposal Network (RPN) and Region-based Convolutional Neural Network (RCNN). It defines parameters for assigners (MaxIoUAssigner), samplers (RandomSampler), proposal generation, and NMS during the training phase, controlling how positive/negative samples are selected and proposals are generated. Key parameters include IoU thresholds, sample numbers, and NMS thresholds.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(  # Config of training hyperparameters for rpn and rcnn\n    rpn=dict(  # Training config of rpn\n        assigner=dict(  # Config of assigner\n            type='MaxIoUAssigner',  # Type of assigner, MaxIoUAssigner is used for many common detectors. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/assigners/max_iou_assigner.py#L14 for more details.\n            pos_iou_thr=0.7,  # IoU >= threshold 0.7 will be taken as positive samples\n            neg_iou_thr=0.3,  # IoU < threshold 0.3 will be taken as negative samples\n            min_pos_iou=0.3,  # The minimal IoU threshold to take boxes as positive samples\n            match_low_quality=True,  # Whether to match the boxes under low quality (see API doc for more details).\n            ignore_iof_thr=-1),  # IoF threshold for ignoring bboxes\n        sampler=dict(  # Config of positive/negative sampler\n            type='RandomSampler',  # Type of sampler, PseudoSampler and other samplers are also supported. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/samplers/random_sampler.py#L14 for implementation details.\n            num=256,  # Number of samples\n            pos_fraction=0.5,  # The ratio of positive samples in the total samples.\n            neg_pos_ub=-1,  # The upper bound of negative samples based on the number of positive samples.\n            add_gt_as_proposals=False),  # Whether add GT as proposals after sampling.\n        allowed_border=-1,  # The border allowed after padding for valid anchors.\n        pos_weight=-1,  # The weight of positive samples during training.\n        debug=False),  # Whether to set the debug mode\n    rpn_proposal=dict(  # The config to generate proposals during training\n        nms_across_levels=False,  # Whether to do NMS for boxes across levels. Only work in `GARPNHead`, naive rpn does not support do nms cross levels.\n        nms_pre=2000,  # The number of boxes before NMS\n        nms_post=1000,  # The number of boxes to be kept by NMS. Only work in `GARPNHead`.\n        max_per_img=1000,  # The number of boxes to be kept after NMS.\n        nms=dict( # Config of NMS\n            type='nms',  # Type of NMS\n            iou_threshold=0.7 # NMS threshold\n            ),\n        min_bbox_size=0),  # The allowed minimal box size\n    rcnn=dict(  # The config for the roi heads.\n        assigner=dict(  # Config of assigner for second stage, this is different for that in rpn\n            type='MaxIoUAssigner',  # Type of assigner, MaxIoUAssigner is used for all roi_heads for now. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/assigners/max_iou_assigner.py#L14 for more details.\n            pos_iou_thr=0.5,  # IoU >= threshold 0.5 will be taken as positive samples\n            neg_iou_thr=0.5,  # IoU < threshold 0.5 will be taken as negative samples\n            min_pos_iou=0.5,  # The minimal IoU threshold to take boxes as positive samples\n            match_low_quality=False,  # Whether to match the boxes under low quality (see API doc for more details).\n            ignore_iof_thr=-1),  # IoF threshold for ignoring bboxes\n        sampler=dict(\n            type='RandomSampler',  # Type of sampler, PseudoSampler and other samplers are also supported. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/samplers/random_sampler.py#L14 for implementation details.\n            num=512,  # Number of samples\n            pos_fraction=0.25,  # The ratio of positive samples in the total samples.\n            neg_pos_ub=-1,  # The upper bound of negative samples based on the number of positive samples.\n            add_gt_as_proposals=True\n        ),  # Whether add GT as proposals after sampling.\n        mask_size=28,  # Size of mask\n        pos_weight=-1,  # The weight of positive samples during training.\n        debug=False))  # Whether to set the debug mode\n```\n\n----------------------------------------\n\nTITLE: Image Inference Demo Script\nDESCRIPTION: This script performs object detection inference on a single image using a specified configuration file and optional weights file. It supports specifying the device (CPU or GPU) and a confidence threshold for displaying predictions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py \\\n    ${IMAGE_FILE} \\\n    ${CONFIG_FILE} \\\n    [--weights ${WEIGHTS}] \\\n    [--device ${GPU_ID}] \\\n    [--pred-score-thr ${SCORE_THR}]\n```\n\n----------------------------------------\n\nTITLE: Backend Model Inference using ONNX Runtime in Python\nDESCRIPTION: This code snippet demonstrates how to perform inference on a converted ONNX model using the ONNX Runtime backend in MMDeploy. It builds a task processor and a backend model, processes an input image, performs inference, and visualizes the results. It requires the mmdeploy and mmdet packages to be installed, along with the specified model and deploy configurations, and a converted ONNX model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/deploy.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy.apis.utils import build_task_processor\nfrom mmdeploy.utils import get_input_shape, load_config\nimport torch\n\ndeploy_cfg = '../mmdeploy/configs/mmdet/detection/detection_onnxruntime_dynamic.py'\nmodel_cfg = 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\ndevice = 'cpu'\nbackend_model = ['mmdeploy_models/mmdet/onnx/end2end.onnx']\nimage = 'demo/demo.jpg'\n\n# read deploy_cfg and model_cfg\ndeploy_cfg, model_cfg = load_config(deploy_cfg, model_cfg)\n\n# build task and backend model\ntask_processor = build_task_processor(model_cfg, deploy_cfg, device)\nmodel = task_processor.build_backend_model(backend_model)\n\n# process input image\ninput_shape = get_input_shape(deploy_cfg)\nmodel_inputs, _ = task_processor.create_input(image, input_shape)\n\n# do model inference\nwith torch.no_grad():\n    result = model.test_step(model_inputs)\n\n# visualize results\ntask_processor.visualize(\n    image=image,\n    model=model,\n    result=result[0],\n    window_name='visualize',\n    output_file='output_detection.png')\n```\n\n----------------------------------------\n\nTITLE: Loading Weights Trained on MMEngine (Python)\nDESCRIPTION: This code shows how to load a weight file trained on MMEngine by passing the weight path to `DetInferencer`. It assumes the config is dumped in the weight file.  Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# It will raise an error if the config file cannot be found in the weight. Currently, within the MMDetection model repository, only the weights of ddq-detr-4scale_r50 can be loaded in this manner.\ninferencer = DetInferencer(weights='https://download.openmmlab.com/mmdetection/v3.0/ddq/ddq-detr-4scale_r50_8xb2-12e_coco/ddq-detr-4scale_r50_8xb2-12e_coco_20230809_170711-42528127.pth')\n```\n\n----------------------------------------\n\nTITLE: Migrating Train Pipeline v2.x to v3.x in MMDetection\nDESCRIPTION: This code demonstrates the change in the train pipeline configuration from MMDetection v2.x to v3.x. The key changes include moving normalization and padding configurations to the `data_preprocessor` and merging `Collect` and `DefaultFormatBundle` into `PackDetInputs`. It removes `Normalize` and `Pad` transforms.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\n```\n\nLANGUAGE: python\nCODE:\n```\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs')\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Validation Image Region Proposals (Python)\nDESCRIPTION: This snippet configures the `val_dataloader` and `test_evaluator` to use `DumpProposals` for extracting region proposals from the validation set.  It specifies the output directory and proposals filename.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fast_rcnn/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# For validation set\nval_dataloader = dict(\n  _delete_=True,\n  type='DumpProposals',\n  output_dir='data/coco/proposals/',\n  proposals_file='rpn_r50_fpn_1x_val2017.pkl')\ntest_evaluator = val_dataloader\n```\n\n----------------------------------------\n\nTITLE: Start RTMDet Backend Inference Service\nDESCRIPTION: This snippet starts the RTMDet backend inference service using `label-studio-ml`. It specifies the configuration file, checkpoint file, and device (CPU or GPU) to use for inference, as well as the port on which the service will run.  The path `projects/LabelStudio/backend_template` likely refers to a template or directory containing necessary files for the backend setup.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\n\nlabel-studio-ml start projects/LabelStudio/backend_template --with \\\nconfig_file=configs/rtmdet/rtmdet_m_8xb32-300e_coco.py \\\ncheckpoint_file=./work_dirs/rtmdet_m_8xb32-300e_coco_20220719_112220-229f527c.pth \\\ndevice=cpu \\\n--port 8003\n# device=cpu 为使用 CPU 推理，如果使用 GPU 推理，将 cpu 替换为 cuda:0\n```\n\n----------------------------------------\n\nTITLE: Defining a New Bounding Box Head (DoubleConvFCBBoxHead) in MMDetection\nDESCRIPTION: This snippet defines a new bounding box head, DoubleConvFCBBoxHead, for object detection in MMDetection. It inherits from the base BBoxHead class and implements the forward method to compute classification scores and bounding box predictions from RoI features.  The `init_cfg` parameter configures initialization of the head's layers.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\n\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule\nfrom mmengine.model import BaseModule, ModuleList\nfrom torch import Tensor\n\nfrom mmdet.models.backbones.resnet import Bottleneck\nfrom mmdet.registry import MODELS\nfrom mmdet.utils import ConfigType, MultiConfig, OptConfigType, OptMultiConfig\nfrom .bbox_head import BBoxHead\n\n@MODELS.register_module()\nclass DoubleConvFCBBoxHead(BBoxHead):\n    r\"\"\"Bbox head used in Double-Head R-CNN\n\n    .. code-block:: none\n\n                                          /-> cls\n                      /-> shared convs ->\n                                          \\-> reg\n        roi features\n                                          /-> cls\n                      \\-> shared fc    ->\n                                          \\-> reg\n    \"\"\"  # noqa: W605\n\n    def __init__(self,\n                 num_convs: int = 0,\n                 num_fcs: int = 0,\n                 conv_out_channels: int = 1024,\n                 fc_out_channels: int = 1024,\n                 conv_cfg: OptConfigType = None,\n                 norm_cfg: ConfigType = dict(type='BN'),\n                 init_cfg: MultiConfig = dict(\n                     type='Normal',\n                     override=[\n                         dict(type='Normal', name='fc_cls', std=0.01),\n                         dict(type='Normal', name='fc_reg', std=0.001),\n                         dict(\n                             type='Xavier',\n                             name='fc_branch',\n                             distribution='uniform')\n                     ]),\n                 **kwargs) -> None:\n        kwargs.setdefault('with_avg_pool', True)\n        super().__init__(init_cfg=init_cfg, **kwargs)\n\n    def forward(self, x_cls: Tensor, x_reg: Tensor) -> Tuple[Tensor]:\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with a NumPy Array\nDESCRIPTION: This snippet reads an image into a NumPy array using `mmcv.imread` and then performs inference using the `DetInferencer`. This demonstrates how to use a NumPy array as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport mmcv\narray = mmcv.imread('demo/demo.jpg')\ninferencer(array)\n```\n\n----------------------------------------\n\nTITLE: Getting Output Channels of a New Backbone\nDESCRIPTION: This code snippet illustrates how to determine the output channels of a new backbone network by building the backbone and feeding it a pseudo image. The example utilizes a ResNet backbone with depth 18.  It involves creating an instance of the backbone, setting it to evaluation mode, generating a random input tensor, and passing it through the network's forward pass.  The shape of each output level is then printed to reveal the channel information.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/how_to.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.models import ResNet\nimport torch\nself = ResNet(depth=18)\nself.eval()\ninputs = torch.rand(1, 3, 32, 32)\nlevel_outputs = self.forward(inputs)\nfor level_out in level_outputs:\n    print(tuple(level_out.shape))\n\n```\n\nLANGUAGE: python\nCODE:\n```\n(1, 64, 8, 8)\n(1, 128, 4, 4)\n(1, 256, 2, 2)\n(1, 512, 1, 1)\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating GLIP Model Performance (Single GPU)\nDESCRIPTION: This command runs the evaluation script for the GLIP model using a single GPU. It specifies the configuration file and the model weights file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/glip/glip_atss_swin-t_fpn_dyhead_pretrain_obj365.py glip_tiny_a_mmdet-b3654169.pth\n```\n\n----------------------------------------\n\nTITLE: Train model on multiple nodes in MMDetection\nDESCRIPTION: These snippets demonstrate how to train a model on multiple machines connected via ethernet using the dist_train.sh script. The NNODES, NODE_RANK, PORT, MASTER_ADDR, CONFIG, and GPUS variables are used to configure the distributed training environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_7\n\nLANGUAGE: shell script\nCODE:\n```\nNNODES=2 NODE_RANK=0 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR bash tools/dist_train.sh $CONFIG $GPUS\n```\n\nLANGUAGE: shell script\nCODE:\n```\nNNODES=2 NODE_RANK=1 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR bash tools/dist_train.sh $CONFIG $GPUS\n```\n\n----------------------------------------\n\nTITLE: Optimizer Configuration - Python\nDESCRIPTION: This configures the optimizer used for training, specifying the optimizer wrapper type, optimizer type (SGD), learning rate, momentum, and weight decay. It also allows configuration of gradient clipping. The optim_wrapper handles optimization with features like gradient clipping and mixed precision training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(  # 优化器封装的配置\n    type='OptimWrapper',  # 优化器封装的类型。可以切换至 AmpOptimWrapper 来启用混合精度训练\n    optimizer=dict(  # 优化器配置。支持 PyTorch 的各种优化器。请参考 https://pytorch.org/docs/stable/optim.html#algorithms\n        type='SGD',  # 随机梯度下降优化器\n        lr=0.02,  # 基础学习率\n        momentum=0.9,  # 带动量的随机梯度下降\n        weight_decay=0.0001),  # 权重衰减\n    clip_grad=None,  # 梯度裁剪的配置，设置为 None 关闭梯度裁剪。使用方法请见 https://mmengine.readthedocs.io/en/latest/tutorials/optimizer.html\n    )\n```\n\n----------------------------------------\n\nTITLE: Define Weak Augmentation Pipeline for Unlabeled Data (Python)\nDESCRIPTION: This pipeline defines weak data augmentation steps for unlabeled data, intended for the teacher model in a teacher-student semi-supervised learning framework. It includes resizing, flipping, and packing the inputs with relevant metadata.  The `scale` variable needs to be defined beforehand.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment unlabeled data weakly,\n# which will be sent to teacher model for predicting pseudo instances.\nweak_pipeline = [\n    dict(type='RandomResize', scale=scale, keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor', 'flip', 'flip_direction',\n                   'homography_matrix')),\n]\n```\n\n----------------------------------------\n\nTITLE: Modify config for batch inference in MMDetection\nDESCRIPTION: This snippet shows how to modify config files to perform batch inference by setting `test_dataloader.batch_size`\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ndata = dict(train_dataloader=dict(...), val_dataloader=dict(...), test_dataloader=dict(batch_size=2, ...))\n```\n\n----------------------------------------\n\nTITLE: Open Vocabulary Object Detection Inference - Shell\nDESCRIPTION: This shell command demonstrates open vocabulary object detection using MM Grounding DINO. It runs the `image_demo.py` script with the configuration and weights file. The `--texts` parameter allows specifying arbitrary class names ('zebra. giraffe' in this example) for detection. The `-c` flag enables visualization of confidence scores.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/animals.png \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'zebra. giraffe' -c\n```\n\n----------------------------------------\n\nTITLE: Modifying Train/Test Pipelines in Configuration\nDESCRIPTION: This snippet demonstrates how to modify the `train_pipeline` and `test_pipeline` variables in MMDetection configuration files, specifically for using a multi-scale strategy. It shows the definition of new pipelines with multi-scale resizing and then passes these pipelines into the `train_dataloader`, `val_dataloader`, and `test_dataloader` configurations. The `dataset` key ensures the new pipelines are correctly applied.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='RandomResize', scale=[(1333, 640), (1333, 800)],\n        keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs')\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(dataset=dict(pipeline=train_pipeline))\nval_dataloader = dict(dataset=dict(pipeline=test_pipeline))\ntest_dataloader = dict(dataset=dict(pipeline=test_pipeline))\n```\n\n----------------------------------------\n\nTITLE: Import Custom Optimizer in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to import the custom optimizer `MyOptimizer` defined in `mmdet/engine/optimizers/my_optimizer.py` into the main namespace of MMDetection.  It can be achieved either by modifying `mmdet/engine/optimizers/__init__.py` or by using the `custom_imports` configuration option.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom .my_optimizer import MyOptimizer\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Data Processing Pipeline for Faster R-CNN (Testing)\nDESCRIPTION: This code defines the data processing pipeline for testing a Faster R-CNN model. It involves loading images, resizing, and packing the necessary metadata. The `backend_args` parameter is assumed to be defined elsewhere. The metadata keys include image ID, path, original shape, image shape, and scale factor, which are essential for post-processing and evaluation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/transforms.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntest_pipeline = [  # Testing data processing pipeline\n    dict(type='LoadImageFromFile', backend_args=backend_args),  # First pipeline to load images from file path\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # Pipeline that resize the images\n    dict(\n        type='PackDetInputs',  # Pipeline that formats the annotation data and decides which keys in the data should be packed into data_samples\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\n```\n\n----------------------------------------\n\nTITLE: Downloading COCO2014 Dataset\nDESCRIPTION: This command downloads the COCO2014 dataset and unpacks the archive. The dataset is downloaded to the `data/coco` directory under the current path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name coco2014 --unzip\n```\n\n----------------------------------------\n\nTITLE: Configuring DetVisualizationHook with Multiple Backends (Python)\nDESCRIPTION: This code snippet configures the DetVisualizationHook to visualize detection results using DetLocalVisualizer. The DetLocalVisualizer is set up with multiple backends, including LocalVisBackend and TensorboardVisBackend, enabling visualization in different environments. The draw parameter enables the drawing of predictions during visualization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    visualization=dict(type='DetVisualizationHook', draw=True))\n\nvis_backends = [dict(type='LocalVisBackend'),\n                dict(type='TensorboardVisBackend')]\nvisualizer = dict(\n    type='DetLocalVisualizer', vis_backends=vis_backends, name='visualizer')\n\n```\n\n----------------------------------------\n\nTITLE: Configure semi-supervised dataloader in Python\nDESCRIPTION: This code snippet configures a semi-supervised dataloader using ConcatDataset to combine labeled and unlabeled datasets. It defines the dataset types, data roots, annotation files, data prefixes, filter configurations, and pipelines for both labeled and unlabeled data.  A GroupMultiSourceSampler is used to sample data from both datasets with a specified source ratio, ensuring similar aspect ratios within each batch.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlabeled_dataset = dict(\n    type=dataset_type,\n    data_root=data_root,\n    ann_file='annotations/instances_train2017.json',\n    data_prefix=dict(img='train2017/'),\n    filter_cfg=dict(filter_empty_gt=True, min_size=32),\n    pipeline=sup_pipeline)\n\nunlabeled_dataset = dict(\n    type=dataset_type,\n    data_root=data_root,\n    ann_file='annotations/instances_unlabeled2017.json',\n    data_prefix=dict(img='unlabeled2017/'),\n    filter_cfg=dict(filter_empty_gt=False),\n    pipeline=unsup_pipeline)\n\ntrain_dataloader = dict(\n    batch_size=batch_size,\n    num_workers=num_workers,\n    persistent_workers=True,\n    sampler=dict(\n        type='GroupMultiSourceSampler',\n        batch_size=batch_size,\n        source_ratio=[1, 4]),\n    dataset=dict(\n        type='ConcatDataset', datasets=[labeled_dataset, unlabeled_dataset]))\n```\n\n----------------------------------------\n\nTITLE: Perform inference with MMDeploy SDK (Python)\nDESCRIPTION: This code snippet demonstrates how to perform inference on a deployed model using the MMDeploy SDK's Python API. It loads an image, creates a detector instance, performs inference, and visualizes the results by drawing bounding boxes on the image.  Dependencies: mmdeploy_python, cv2. The script requires the path to the MMDeploy SDK model and the input image. It outputs an image with bounding boxes drawn around detected objects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/deploy.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy_python import Detector\nimport cv2\n\nimg = cv2.imread('demo/demo.jpg')\n\n# create a detector\ndetector = Detector(model_path='mmdeploy_models/mmdet/onnx',\n                    device_name='cpu', device_id=0)\n# perform inference\nbboxes, labels, masks = detector(img)\n\n# visualize inference result\nindices = [i for i in range(len(bboxes))]\nfor index, bbox, label_id in zip(indices, bboxes, labels):\n    [left, top, right, bottom], score = bbox[0:4].astype(int), bbox[4]\n    if score < 0.3:\n        continue\n\n    cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0))\n\ncv2.imwrite('output_detection.png', img)\n```\n\n----------------------------------------\n\nTITLE: Testing Configuration for RPN and RCNN in MMDetection\nDESCRIPTION: This configuration defines the testing hyperparameters for the Region Proposal Network (RPN) and Region-based Convolutional Neural Network (RCNN). It sets parameters for proposal generation, non-maximum suppression (NMS), score thresholds, and the maximum number of detections per image. The configuration dictates how proposals are generated during testing and how detections are filtered to produce the final output.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntest_cfg = dict(  # Config for testing hyperparameters for rpn and rcnn\n    rpn=dict(  # The config to generate proposals during testing\n        nms_across_levels=False,  # Whether to do NMS for boxes across levels. Only work in `GARPNHead`, naive rpn does not support do nms cross levels.\n        nms_pre=1000,  # The number of boxes before NMS\n        nms_post=1000,  # The number of boxes to be kept by NMS. Only work in `GARPNHead`.\n        max_per_img=1000,  # The number of boxes to be kept after NMS.\n        nms=dict( # Config of NMS\n            type='nms',  #Type of NMS\n            iou_threshold=0.7 # NMS threshold\n            ),\n        min_bbox_size=0),  # The allowed minimal box size\n    rcnn=dict(  # The config for the roi heads.\n        score_thr=0.05,  # Threshold to filter out boxes\n        nms=dict(  # Config of NMS in the second stage\n            type='nms',  # Type of NMS\n            iou_thr=0.5),  # NMS threshold\n        max_per_img=100,  # Max number of detections of each image\n        mask_thr_binary=0.5))  # Threshold of mask prediction\n```\n\n----------------------------------------\n\nTITLE: Configuring the Double Head R-CNN Model\nDESCRIPTION: This snippet shows a configuration example for the Double Head R-CNN model in MMDetection. It specifies the `DoubleHeadRoIHead` as the RoI head type, sets the `reg_roi_scale_factor`, and configures the `DoubleConvFCBBoxHead` with its specific parameters, such as the number of convolutional and fully connected layers, input/output channels, and loss functions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n_base_ = '../faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        type='DoubleHeadRoIHead',\n        reg_roi_scale_factor=1.3,\n        bbox_head=dict(\n            _delete_=True,\n            type='DoubleConvFCBBoxHead',\n            num_convs=4,\n            num_fcs=2,\n            in_channels=256,\n            conv_out_channels=1024,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=2.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=2.0))))\n```\n\n----------------------------------------\n\nTITLE: Inference with StrongSORT\nDESCRIPTION: This Python command performs inference on a video using the StrongSORT algorithm.  It utilizes `mot_demo.py`, specifying the input video, configuration file, detector checkpoint, ReID model checkpoint, and output video path.  It uses a single GPU to process the video.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/strongsort/README.md#_snippet_5\n\nLANGUAGE: shell script\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/strongsort/strongsort_yolox_x_8xb4-80e_crowdhuman-mot17halftrain_test-mot17halfval.py --detector ${CHECKPOINT_FILE} --reid ${CHECKPOINT_PATH} --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: StandardRoIHead Definition (Python)\nDESCRIPTION: This code block defines the `StandardRoIHead`, which includes bbox and mask heads, in MMDetection. It handles the forward pass and loss computation for both bbox and mask predictions. It integrates with the `BaseRoIHead` and utilizes modules like samplers and RoI extractors for feature extraction and sampling. Intended for `mmdet/models/roi_heads/standard_roi_head.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom mmdet.registry import MODELS, TASK_UTILS\nfrom mmdet.structures import DetDataSample\nfrom mmdet.structures.bbox import bbox2roi\nfrom mmdet.utils import ConfigType, InstanceList\nfrom ..task_modules.samplers import SamplingResult\nfrom ..utils import empty_instances, unpack_gt_instances\nfrom .base_roi_head import BaseRoIHead\n\n\n@MODELS.register_module()\nclass StandardRoIHead(BaseRoIHead):\n    \"\"\"Simplest base roi head including one bbox head and one mask head.\"\"\"\n\n    def init_assigner_sampler(self) -> None:\n\n    def init_bbox_head(self, bbox_roi_extractor: ConfigType,\n                       bbox_head: ConfigType) -> None:\n\n    def init_mask_head(self, mask_roi_extractor: ConfigType,\n                       mask_head: ConfigType) -> None:\n\n    def forward(self, x: Tuple[Tensor],\n                rpn_results_list: InstanceList) -> tuple:\n\n    def loss(self, x: Tuple[Tensor], rpn_results_list: InstanceList,\n             batch_data_samples: List[DetDataSample]) -> dict:\n\n    def _bbox_forward(self, x: Tuple[Tensor], rois: Tensor) -> dict:\n\n    def bbox_loss(self, x: Tuple[Tensor],\n                  sampling_results: List[SamplingResult]) -> dict:\n\n    def mask_loss(self, x: Tuple[Tensor],\n                  sampling_results: List[SamplingResult], bbox_feats: Tensor,\n                  batch_gt_instances: InstanceList) -> dict:\n\n    def _mask_forward(\n                      x: Tuple[Tensor],\n                      rois: Tensor = None,\n                      pos_inds: Optional[Tensor] = None,\n                      bbox_feats: Optional[Tensor] = None) -> dict:\n\n    def predict_bbox(self,\n                     x: Tuple[Tensor],\n                     batch_img_metas: List[dict],\n                     rpn_results_list: InstanceList,\n                     rcnn_test_cfg: ConfigType,\n                     rescale: bool = False) -> InstanceList:\n\n    def predict_mask(self,\n                     x: Tuple[Tensor],\n                     batch_img_metas: List[dict],\n                     results_list: InstanceList,\n                     rescale: bool = False) -> InstanceList:\n```\n\n----------------------------------------\n\nTITLE: Downloading MM Grounding DINO-T Model Weights\nDESCRIPTION: This shell command downloads the pre-trained weights for the MM Grounding DINO-T model. It uses `wget` to download the weights from the provided URL and saves them to the current directory. This allows users to quickly get started with the model without training from scratch.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget load_from = 'https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth' # noqa\n```\n\n----------------------------------------\n\nTITLE: Unlabeled Strong Data Pipeline Configuration (Python)\nDESCRIPTION: This Python code defines the data augmentation pipeline for strongly augmented unlabeled data, which is used for the student model's unsupervised training. It includes random resizing, random flipping, RandAugment for both color and geometric transformations, RandomErasing, filtering annotations, and packing the data. The strong augmentation helps the student model learn from the pseudo-labels generated by the teacher model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment unlabeled data strongly,\n# which will be sent to student model for unsupervised training.\nstrong_pipeline = [\n    dict(type='RandomResize', scale=scale, keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(\n        type='RandomOrder',\n        transforms=[\n            dict(type='RandAugment', aug_space=color_space, aug_num=1),\n            dict(type='RandAugment', aug_space=geometric, aug_num=1),\n        ]),\n    dict(type='RandomErasing', n_patches=(1, 5), ratio=(0, 0.2)),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor', 'flip', 'flip_direction',\n                   'homography_matrix')),\n]\n```\n\n----------------------------------------\n\nTITLE: ATSSHead Class Definition - Python\nDESCRIPTION: This Python code shows the definition of the `ATSSHead` class, which inherits from `AnchorHead`. The snippet highlights that `ATSSHead` overwrites the `get_targets` method inherited from `AnchorHead`, allowing for customization of `label_weights` and `bbox_weights` calculation during target assignment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass ATSSHead(AnchorHead):\n\n    ...\n\n    def get_targets(self,\n                    anchor_list,\n                    valid_flag_list,\n                    gt_bboxes_list,\n                    img_metas,\n                    gt_bboxes_ignore_list=None,\n                    gt_labels_list=None,\n                    label_channels=1,\n                    unmap_outputs=True):\n```\n\n----------------------------------------\n\nTITLE: Reusing Variables from Base Config - Python\nDESCRIPTION: This snippet demonstrates how to reuse variables defined in a base configuration file within MMDetection using the `{{_base_.xxx}}` syntax. This allows for inheriting and extending configurations without duplicating code. `_base_` specifies the base configuration file path. The `a` variable will contain a copy of the `model` variable defined in the `./mask-rcnn_r50_fpn_1x_coco.py` file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\n\na = {{_base_.model}} # Variable `a` is equal to the `model` defined in `_base_`\n```\n\n----------------------------------------\n\nTITLE: Inference with NumPy Array (Python)\nDESCRIPTION: This code snippet shows how to perform inference using a NumPy array representing an image. The image must be in BGR format. Requires the `mmdet` and `mmcv` libraries.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport mmcv\narray = mmcv.imread('demo/demo.jpg')\ninferencer(array)\n```\n\n----------------------------------------\n\nTITLE: Integrating Custom Transform into MMDetection Config (Python)\nDESCRIPTION: This code snippet shows how to integrate a custom transform into an MMDetection configuration file. It uses `custom_imports` to import the custom transform module.  The `train_pipeline` list is then modified to include the custom transform `MyTransform` with a specified probability. This requires specifying the correct path to the custom pipeline file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_transforms.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(imports=['path.to.my_pipeline'], allow_failed_imports=False)\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='MyTransform', prob=0.2),\n    dict(type='PackDetInputs')\n]\n```\n\n----------------------------------------\n\nTITLE: Inference with mot_demo.py\nDESCRIPTION: This Python command demonstrates how to perform inference using Mask2Former on a video using the `mot_demo.py` script. It requires specifying the input video, configuration file, checkpoint path, and output video file. This script performs video instance segmentation and saves the result as a video file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask2former_vis/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/mask2former_vis/mask2former_r50_8xb2-8e_youtubevis2021.py  --checkpoint {CHECKPOINT_PATH} --out vis.mp4\n```\n\n----------------------------------------\n\nTITLE: Customize Training Schedule with PolyLR in MMDetection (Python)\nDESCRIPTION: This code snippet configures a `PolyLR` learning rate scheduler within the `param_scheduler` list in MMDetection. It sets the power to 0.9, the minimum learning rate to 1e-4, and specifies that the scheduler runs from the beginning (epoch 0) to epoch 8.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='PolyLR',\n        power=0.9,\n        eta_min=1e-4,\n        begin=0,\n        end=8,\n        by_epoch=True)]\n\n```\n\n----------------------------------------\n\nTITLE: Run image demo (source install)\nDESCRIPTION: This command runs the image demo script using a pre-trained RTMDet model to detect objects in a sample image. It specifies the configuration file, weights, and device (CPU in this case).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg rtmdet_tiny_8xb32-300e_coco.py --weights rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --device cpu\n```\n\n----------------------------------------\n\nTITLE: Training EfficientDet model\nDESCRIPTION: This command trains the EfficientDet model using the MMDetection training script. It requires specifying the configuration file for the EfficientDet model to be trained.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython tools/train.py projects/EfficientDet/configs/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py\n```\n\n----------------------------------------\n\nTITLE: Importing PAFPN Neck in MMDetection\nDESCRIPTION: Imports the newly defined PAFPN neck into the MMDetection framework. This can be done by adding an import statement to the `mmdet/models/necks/__init__.py` file or by using the `custom_imports` dictionary in the configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom .pafpn import PAFPN\n```\n\n----------------------------------------\n\nTITLE: Plotting Loss/mAP Curves from Training Logs (analyze_logs.py)\nDESCRIPTION: This tool plots loss and mAP curves based on a training log file. It requires the `seaborn` library. Users can specify the keys to plot, legend, title, and output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve [--keys ${KEYS}] [--eval-interval ${EVALUATION_INTERVAL}] [--title ${TITLE}] [--legend ${LEGEND}] [--backend ${BACKEND}] [--style ${STYLE}] [--out ${OUT_FILE}]\n```\n\n----------------------------------------\n\nTITLE: Run MMDetection Docker Container (Shell)\nDESCRIPTION: This snippet runs the MMDetection Docker container with GPU support, shared memory, interactive terminal, and volume mounting for data. `--gpus all` enables access to all GPUs. `--shm-size=8g` sets the shared memory size to 8GB. `-it` enables interactive mode. `-v {DATA_DIR}:/mmdetection/data` mounts the host's `DATA_DIR` to `/mmdetection/data` inside the container.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --gpus all --shm-size=8g -it -v {DATA_DIR}:/mmdetection/data mmdetection\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Command (Shell)\nDESCRIPTION: This shell script executes distributed training across multiple GPUs using MMDetection's `tools/dist_train.sh` script. The `${CONFIG_FILE}` placeholder is the path to the configuration file, and `${GPU_NUM}` specifies the number of GPUs to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbash ./tools/dist_train.sh \\\n    ${CONFIG_FILE} \\\n    ${GPU_NUM} \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Benchmarking model FPS\nDESCRIPTION: This command calculates the FPS (Frames Per Second) of a given model, including model forward and post-processing. It requires a configuration file and can optionally take a checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node=1 --master_port=${PORT} tools/analysis_tools/benchmark.py \\\n    ${CONFIG} \\\n    [--checkpoint ${CHECKPOINT}] \\\n    [--repeat-num ${REPEAT_NUM}] \\\n    [--max-iter ${MAX_ITER}] \\\n    [--log-interval ${LOG_INTERVAL}] \\\n    --launcher pytorch\n```\n\n----------------------------------------\n\nTITLE: Train model with Slurm in MMDetection\nDESCRIPTION: This snippet shows how to train a model on a Slurm-managed cluster using the slurm_train.sh script. The PARTITION, JOB_NAME, CONFIG_FILE, WORK_DIR, and GPUS variables specify the Slurm partition, job name, configuration file, working directory, and number of GPUs to use, respectively.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_8\n\nLANGUAGE: shell script\nCODE:\n```\nbash ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR} ${GPUS}\n```\n\n----------------------------------------\n\nTITLE: Webcam Demo Usage with MMDetection\nDESCRIPTION: This script runs a live object detection demo from a webcam using a specified configuration and checkpoint file. It allows configuring the device, camera ID, and score threshold.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython demo/webcam_demo.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--camera-id ${CAMERA-ID}] \\\n    [--score-thr ${SCORE_THR}]\n```\n\n----------------------------------------\n\nTITLE: Evaluating GLIP Model Performance (Multiple GPUs)\nDESCRIPTION: This command runs the distributed evaluation script for the GLIP model using multiple GPUs. It specifies the configuration file, the model weights file, and the number of GPUs to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh configs/glip/glip_atss_swin-t_fpn_dyhead_pretrain_obj365.py glip_tiny_a_mmdet-b3654169.pth 8\n```\n\n----------------------------------------\n\nTITLE: Importing a Custom Hook via `custom_imports` in MMDetection (Python)\nDESCRIPTION: This code shows how to import a custom hook using the `custom_imports` configuration option. This is an alternative to modifying the `__init__.py` file. Setting `allow_failed_imports` to `False` ensures that the program will raise an error if the import fails.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(imports=['mmdet.engine.hooks.my_hook'], allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: Test MMDetection Model (Shell)\nDESCRIPTION: This shell command runs the testing process of a trained MMDetection model using the specified configuration and checkpoint file.  It evaluates the model's performance on the test dataset and reports metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/balloon/mask-rcnn_r50-caffe_fpn_ms-poly-1x_balloon.py work_dirs/mask-rcnn_r50-caffe_fpn_ms-poly-1x_balloon/epoch_12.pth\n```\n\n----------------------------------------\n\nTITLE: Multi-Machine Training Commands\nDESCRIPTION: These shell commands demonstrate how to launch training across multiple machines connected with Ethernet. `NNODES` specifies the total number of nodes, `NODE_RANK` is the rank of the current node (0 for the first, 1 for the second, etc.), `PORT` is the master port, and `MASTER_ADDR` is the address of the master node.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nNNODES=2 NODE_RANK=0 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR sh tools/dist_train.sh $CONFIG $GPUS\n```\n\nLANGUAGE: shell\nCODE:\n```\nNNODES=2 NODE_RANK=1 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR sh tools/dist_train.sh $CONFIG $GPUS\n```\n\n----------------------------------------\n\nTITLE: Using MemoryProfilerHook in MMDetection\nDESCRIPTION: This snippet demonstrates how to add the MemoryProfilerHook to the config file to monitor memory usage during training. It requires the `memory_profiler` and `psutil` libraries to be installed. The `interval` parameter specifies how often the memory usage is logged.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_hooks.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MemoryProfilerHook', interval=50)\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Runtime Settings in MMDetection\nDESCRIPTION: This snippet presents runtime configurations for MMDetection, covering aspects such as default scope, environment settings, visualization backends, log processing, and model loading/resuming.  `env_cfg` manages CUDA benchmark and multiprocessing settings, while `vis_backends` configures visualization outputs.  `log_processor` handles runtime logs and `load_from`/`resume` control model loading and training continuation. The `default_scope` defines where to find modules in the registry.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndefault_scope = 'mmdet'  # The default registry scope to find modules. Refer to https://mmengine.readthedocs.io/en/latest/advanced_tutorials/registry.html\n\nenv_cfg = dict(\n    cudnn_benchmark=False,  # Whether to enable cudnn benchmark\n    mp_cfg=dict(  # Multi-processing config\n        mp_start_method='fork',  # Use fork to start multi-processing threads. 'fork' usually faster than 'spawn' but maybe unsafe. See discussion in https://github.com/pytorch/pytorch/issues/1355\n        opencv_num_threads=0),  # Disable opencv multi-threads to avoid system being overloaded\n    dist_cfg=dict(backend='nccl'),  # Distribution configs\n)\n\nvis_backends = [dict(type='LocalVisBackend')]  # Visualization backends. Refer to https://mmengine.readthedocs.io/en/latest/advanced_tutorials/visualization.html\nvisualizer = dict(\n    type='DetLocalVisualizer', vis_backends=vis_backends, name='visualizer')\nlog_processor = dict(\n    type='LogProcessor',  # Log processor to process runtime logs\n    window_size=50,  # Smooth interval of log values\n    by_epoch=True)  # Whether to format logs with epoch type. Should be consistent with the train loop's type.\n\nlog_level = 'INFO'  # The level of logging.\nload_from = None  # Load model checkpoint as a pre-trained model from a given path. This will not resume training.\nresume = False  # Whether to resume from the checkpoint defined in `load_from`. If `load_from` is None, it will resume the latest checkpoint in the `work_dir`.\n```\n\n----------------------------------------\n\nTITLE: Configuring CocoPanopticDataset in mmdetection\nDESCRIPTION: This snippet demonstrates how to configure the `CocoPanopticDataset` in mmdetection for training, validation, and testing. It defines the `dataset_type`, `data_root`, and `data_prefix` for specifying the image and segmentation annotation paths.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndataset_type = 'CocoPanopticDataset'\ndata_root='path/to/your/'\n\ntrain_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_prefix=dict(\n            img='train/image_data/', seg='train/panoptic/image_annotation_data/')\n    )\n)\nval_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_prefix=dict(\n            img='val/image_data/', seg='val/panoptic/image_annotation_data/')\n    )\n)\ntest_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        data_prefix=dict(\n            img='test/image_data/', seg='test/panoptic/image_annotation_data/')\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Fast R-CNN Data Pipeline Configuration (Python)\nDESCRIPTION: This snippet shows the configuration for the training and testing data pipelines in Fast R-CNN. It includes loading images and proposals, data augmentation transforms, and packing the data for input into the model. The `ProposalBroadcaster` is used to transform both ground truth bounding boxes and region proposals.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fast_rcnn/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_pipeline = [\n    dict(\n        type='LoadImageFromFile',\n        backend_args={{_base_.backend_args}}),\n    dict(type='LoadProposals', num_max_proposals=2000),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='ProposalBroadcaster',\n        transforms=[\n            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n            dict(type='RandomFlip', prob=0.5),\n        ]),\n    dict(type='PackDetInputs')\n]\ntest_pipeline = [\n    dict(\n        type='LoadImageFromFile',\n        backend_args={{_base_.backend_args}}),\n    dict(type='LoadProposals', num_max_proposals=None),\n    dict(\n        type='ProposalBroadcaster',\n        transforms=[\n            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n        ]),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(\n    dataset=dict(\n        proposal_file='proposals/rpn_r50_fpn_1x_train2017.pkl',\n        pipeline=train_pipeline))\nval_dataloader = dict(\n    dataset=dict(\n        proposal_file='proposals/rpn_r50_fpn_1x_val2017.pkl',\n        pipeline=test_pipeline))\ntest_dataloader = val_dataloader\n```\n\n----------------------------------------\n\nTITLE: Testing DeepSORT on MOT17-halfval (Shell)\nDESCRIPTION: This command tests DeepSORT on the motXX-half-val dataset using a separate trained detector and ReID model. It utilizes the `tools/dist_test_tracking.sh` script for distributed testing across 8 GPUs. It requires the config file, number of GPUs, detector checkpoint path, and ReID checkpoint path as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepsort/README.md#_snippet_1\n\nLANGUAGE: Shell Script\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/deepsort/deepsort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8 --detector ${DETECTOR_CHECKPOINT_PATH} --reid ${REID_CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Inheriting Base Configurations in MMDetection\nDESCRIPTION: This code snippet demonstrates how to inherit base configurations for finetuning a Mask RCNN model in MMDetection. It inherits configurations for the model structure, dataset, runtime settings, and training schedules. These configurations are used to reduce redundancy and simplify the configuration process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/finetune.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/mask-rcnn_r50_fpn.py',\n    '../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py',\n    '../_base_/schedules/schedule_1x.py'\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Faster R-CNN with FCOS as RPN in MMDetection (Python)\nDESCRIPTION: This configuration file replaces the default RPNHead in Faster R-CNN with FCOSHead. It includes modifications to the neck, RPN head, and region of interest (RoI) head configurations. The learning rate scheduler is also adjusted to prevent loss from going to NaN during early training stages.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/single_stage_as_rpn.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    # copied from configs/fcos/fcos_r50-caffe_fpn_gn-head_1x_coco.py\n    neck=dict(\n        start_level=1,\n        add_extra_convs='on_output',  # use P5\n        relu_before_extra_convs=True),\n    rpn_head=dict(\n        _delete_=True,  # ignore the unused old settings\n        type='FCOSHead',\n        num_classes=1,  # num_classes = 1 for rpn, if num_classes > 1, it will be set to 1 in TwoStageDetector automatically\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n    roi_head=dict(  # update featmap_strides due to the strides in neck\n        bbox_roi_extractor=dict(featmap_strides=[8, 16, 32, 64, 128])))\n\n# learning rate\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0,\n        end=1000),  # Slowly increase lr, otherwise loss becomes NAN\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=12,\n        by_epoch=True,\n        milestones=[8, 11],\n        gamma=0.1)\n]\n```\n\n----------------------------------------\n\nTITLE: Large Image Inference Example (With TTA)\nDESCRIPTION: This example demonstrates how to run large image inference with test-time augmentation (TTA). It downloads a RetinaNet checkpoint and then runs the inference script on a sample image using the downloaded checkpoint, a corresponding configuration file, and the `--tta` flag.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\n# inference with tta\nwget -P checkpoint https://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth\n\npython demo/large_image_demo.py \\\n    demo/large_image.jpg \\\n    configs/retinanet/retinanet_r50_fpn_1x_coco.py \\\n    checkpoint/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth --tta\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot ODinW13 Evaluation - Single GPU - Shell\nDESCRIPTION: This shell command evaluates the MM Grounding DINO model on the ODinW13 dataset using a single GPU.  It uses `tools/test.py` with the relevant configuration and pretrained weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/odinw/grounding_dino_swin-t_pretrain_odinw13.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Use Gradient Clipping in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to use gradient clipping to stabilize the training process in MMDetection. It sets the `clip_grad` parameter in the `optim_wrapper` to clip gradients with a maximum norm of 35 and a norm type of 2. The `_delete_=True` is used to override existing settings in inherited base configurations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    _delete_=True, clip_grad=dict(max_norm=35, norm_type=2))\n\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Hook in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to configure a custom hook in the MMDetection configuration file.  The `type` key specifies the name of the hook, and other keys are used to pass parameters to the hook's constructor.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MyHook', a=a_value, b=b_value)\n]\n```\n\n----------------------------------------\n\nTITLE: Joint Trained Detector Evaluation (Shell)\nDESCRIPTION: This shell script executes the evaluation and testing of a joint trained detector using the dist_test_tracking.sh script. It requires specifying the configuration file and the path to the checkpoint file containing the trained weights.  This script is used for evaluating the performance of the tracker on a specified dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/bytetrack/bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval.py 8 --checkpoint ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Installing Cityscapes scripts in MMDetection (Python)\nDESCRIPTION: This snippet installs the cityscapesscripts Python package, a dependency for converting Cityscapes dataset annotations for use in MMDetection. It also converts the Cityscapes dataset to COCO format, specifying the data directory, number of processes, and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install cityscapesscripts\n\npython tools/dataset_converters/cityscapes.py \\\n    ./data/cityscapes \\\n    --nproc 8 \\\n    --out-dir ./data/cityscapes/annotations\n```\n\n----------------------------------------\n\nTITLE: Analyze Detection Results (analyze_results.py)\nDESCRIPTION: This tool calculates the single image mAP and shows or saves top-k images with the highest and lowest scores. It requires a configuration file and a prediction file in pickle format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n      ${CONFIG} \\\n      ${PREDICTION_PATH} \\\n      ${SHOW_DIR} \\\n      [--show] \\\n      [--wait-time ${WAIT_TIME}] \\\n      [--topk ${TOPK}] \\\n      [--show-score-thr ${SHOW_SCORE_THR}] \\\n      [--cfg-options ${CFG_OPTIONS}]\n```\n\n----------------------------------------\n\nTITLE: Download Datasets Using MIM from OpenDataLab (Bash)\nDESCRIPTION: These commands use the `mim` tool to download and preprocess datasets (voc2007, voc2012, coco2017) directly from OpenDataLab, preparing them for use with MMDetection. The `mmdet` argument specifies the MMDetection package.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\n# download voc2007 and preprocess by MIM\nmim download mmdet --dataset voc2007\n\n# download voc2012 and preprocess by MIM\nmim download mmdet --dataset voc2012\n\n# download coco2017 and preprocess by MIM\nmim download mmdet --dataset coco2017\n```\n\n----------------------------------------\n\nTITLE: Defining Mask R-CNN Model Configuration in MMDetection (Python)\nDESCRIPTION: This Python code snippet defines the model configuration for a Mask R-CNN object detection model within the MMDetection framework. It includes configurations for data preprocessing, the backbone (ResNet), the neck (FPN), the RPN head, RoI head, and training/testing configurations. Key parameters include image normalization, feature extraction, loss functions, and sampling strategies. The configuration dictates how the model processes input data, extracts features, proposes regions, and ultimately predicts object bounding boxes and segmentation masks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    type='MaskRCNN',  # 检测器名\n    data_preprocessor=dict(  # 数据预处理器的配置，通常包括图像归一化和 padding\n        type='DetDataPreprocessor',  # 数据预处理器的类型，参考 https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.data_preprocessors.DetDataPreprocessor\n        mean=[123.675, 116.28, 103.53],  # 用于预训练骨干网络的图像归一化通道均值，按 R、G、B 排序\n        std=[58.395, 57.12, 57.375],  # 用于预训练骨干网络的图像归一化通道标准差，按 R、G、B 排序\n        bgr_to_rgb=True,  # 是否将图片通道从 BGR 转为 RGB\n        pad_mask=True,  # 是否填充实例分割掩码\n        pad_size_divisor=32),  # padding 后的图像的大小应该可以被 ``pad_size_divisor`` 整除\n    backbone=dict(  # 主干网络的配置文件\n        type='ResNet',  # 主干网络的类别，可用选项请参考 https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.backbones.ResNet\n        depth=50,  # 主干网络的深度，对于 ResNet 和 ResNext 通常设置为 50 或 101\n        num_stages=4,  # 主干网络状态(stages)的数目，这些状态产生的特征图作为后续的 head 的输入\n        out_indices=(0, 1, 2, 3),  # 每个状态产生的特征图输出的索引\n        frozen_stages=1,  # 第一个状态的权重被冻结\n        norm_cfg=dict(  # 归一化层(norm layer)的配置项\n            type='BN',  # 归一化层的类别，通常是 BN 或 GN\n            requires_grad=True),  # 是否训练归一化里的 gamma 和 beta\n        norm_eval=True,  # 是否冻结 BN 里的统计项\n        style='pytorch',  # 主干网络的风格，'pytorch' 意思是步长为2的层为 3x3 卷积， 'caffe' 意思是步长为2的层为 1x1 卷积\n        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),  # 加载通过 ImageNet 预训练的模型\n    neck=dict(\n        type='FPN',  # 检测器的 neck 是 FPN，我们同样支持 'NASFPN', 'PAFPN' 等，更多细节可以参考 https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.necks.FPN\n        in_channels=[256, 512, 1024, 2048],  # 输入通道数，这与主干网络的输出通道一致\n        out_channels=256,  # 金字塔特征图每一层的输出通道\n        num_outs=5),  # 输出的范围(scales)\n    rpn_head=dict(\n        type='RPNHead',  # rpn_head 的类型是 'RPNHead', 我们也支持 'GARPNHead' 等，更多细节可以参考 https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.dense_heads.RPNHead\n        in_channels=256,  # 每个输入特征图的输入通道，这与 neck 的输出通道一致\n        feat_channels=256,  # head 卷积层的特征通道\n        anchor_generator=dict(  # 锚点(Anchor)生成器的配置\n            type='AnchorGenerator',  # 大多数方法使用 AnchorGenerator 作为锚点生成器, SSD 检测器使用 `SSDAnchorGenerator`。更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/prior_generators/anchor_generator.py#L18\n            scales=[8],  # 锚点的基本比例，特征图某一位置的锚点面积为 scale * base_sizes\n            ratios=[0.5, 1.0, 2.0],  # 高度和宽度之间的比率\n            strides=[4, 8, 16, 32, 64]),  # 锚生成器的步幅。这与 FPN 特征步幅一致。 如果未设置 base_sizes，则当前步幅值将被视为 base_sizes\n        bbox_coder=dict(  # 在训练和测试期间对框进行编码和解码\n            type='DeltaXYWHBBoxCoder',  # 框编码器的类别，'DeltaXYWHBBoxCoder' 是最常用的，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/coders/delta_xywh_bbox_coder.py#L13\n            target_means=[0.0, 0.0, 0.0, 0.0],  # 用于编码和解码框的目标均值\n            target_stds=[1.0, 1.0, 1.0, 1.0]),  # 用于编码和解码框的标准差\n        loss_cls=dict(  # 分类分支的损失函数配置\n            type='CrossEntropyLoss',  # 分类分支的损失类型，我们也支持 FocalLoss 等，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/losses/cross_entropy_loss.py#L201\n            use_sigmoid=True,  # RPN 通常进行二分类，所以通常使用 sigmoid 函数\n            los_weight=1.0),  # 分类分支的损失权重\n        loss_bbox=dict(  # 回归分支的损失函数配置\n            type='L1Loss',  # 损失类型，我们还支持许多 IoU Losses 和 Smooth L1-loss 等，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/losses/smooth_l1_loss.py#L56\n            loss_weight=1.0)),  # 回归分支的损失权重\n    roi_head=dict(  # RoIHead 封装了两步(two-stage)/级联(cascade)检测器的第二步\n        type='StandardRoIHead',  # RoI head 的类型，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/roi_heads/standard_roi_head.py#L17\n        bbox_roi_extractor=dict(  # 用于 bbox 回归的 RoI 特征提取器\n            type='SingleRoIExtractor',  # RoI 特征提取器的类型，大多数方法使用 SingleRoIExtractor，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py#L13\n            roi_layer=dict(  # RoI 层的配置\n                type='RoIAlign',  # RoI 层的类别, 也支持 DeformRoIPoolingPack 和 ModulatedDeformRoIPoolingPack，更多细节请参考 https://mmcv.readthedocs.io/en/latest/api.html#mmcv.ops.RoIAlign\n                output_size=7,  # 特征图的输出大小\n                sampling_ratio=0),  # 提取 RoI 特征时的采样率。0 表示自适应比率\n            out_channels=256,  # 提取特征的输出通道\n            featmap_strides=[4, 8, 16, 32]),  # 多尺度特征图的步幅，应该与主干的架构保持一致\n        bbox_head=dict(  # RoIHead 中 box head 的配置\n            type='Shared2FCBBoxHead',  # bbox head 的类别，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py#L220\n            in_channels=256,  # bbox head 的输入通道。 这与 roi_extractor 中的 out_channels 一致\n            fc_out_channels=1024,  # FC 层的输出特征通道\n            roi_feat_size=7,  # 候选区域(Region of Interest)特征的大小\n            num_classes=80,  # 分类的类别数量\n            bbox_coder=dict(  # 第二阶段使用的框编码器\n                type='DeltaXYWHBBoxCoder',  # 框编码器的类别，大多数情况使用 'DeltaXYWHBBoxCoder'\n                target_means=[0.0, 0.0, 0.0, 0.0],  # 用于编码和解码框的均值\n                target_stds=[0.1, 0.1, 0.2, 0.2]),  # 编码和解码的标准差。因为框更准确，所以值更小，常规设置时 [0.1, 0.1, 0.2, 0.2]。\n            reg_class_agnostic=False,  # 回归是否与类别无关\n            loss_cls=dict(  # 分类分支的损失函数配\n                type='CrossEntropyLoss',  # 分类分支的损失类型，我们也支持 FocalLoss 等\n                use_sigmoid=False,  # 是否使用 sigmoid\n                loss_weight=1.0),  # 分类分支的损失权重\n            loss_bbox=dict(  # 回归分支的损失函数配置\n                type='L1Loss',  # 损失类型，我们还支持许多 IoU Losses 和 Smooth L1-loss 等\n                loss_weight=1.0)),  # 回归分支的损失权重\n        mask_roi_extractor=dict(  # 用于 mask 生成的 RoI 特征提取器\n            type='SingleRoIExtractor',  # RoI 特征提取器的类型，大多数方法使用 SingleRoIExtractor\n            roi_layer=dict(  # 提取实例分割特征的 RoI 层配置\n                type='RoIAlign',  # RoI 层的类型，也支持 DeformRoIPoolingPack 和 ModulatedDeformRoIPoolingPack\n                output_size=14,  # 特征图的输出大小\n                sampling_ratio=0),  # 提取 RoI 特征时的采样率\n            out_channels=256,  # 提取特征的输出通道\n            featmap_strides=[4, 8, 16, 32]),  # 多尺度特征图的步幅\n        mask_head=dict(  # mask 预测 head 模型\n            type='FCNMaskHead',  # mask head 的类型，更多细节请参考 https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.roi_heads.FCNMaskHead\n            num_convs=4,  # mask head 中的卷积层数\n            in_channels=256,  # 输入通道，应与 mask roi extractor 的输出通道一致\n            conv_out_channels=256,  # 卷积层的输出通道\n            num_classes=80,  # 要分割的类别数\n            loss_mask=dict(  # mask 分支的损失函数配置\n                type='CrossEntropyLoss',  # 用于分割的损失类型\n                use_mask=True,  # 是否只在正确的类中训练 mask\n                loss_weight=1.0))),  # mask 分支的损失权重\n    train_cfg = dict(  # rpn 和 rcnn 训练超参数的配置\n        rpn=dict(  # rpn 的训练配置\n            assigner=dict(  # 分配器(assigner)的配置\n                type='MaxIoUAssigner',  # 分配器的类型，MaxIoUAssigner 用于许多常见的检测器，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/assigners/max_iou_assigner.py#L14\n                pos_iou_thr=0.7,  # IoU >= 0.7(阈值) 被视为正样本\n                neg_iou_thr=0.3,  # IoU < 0.3(阈值) 被视为负样本\n                min_pos_iou=0.3,  # 将框作为正样本的最小 IoU 阈值\n                match_low_quality=True,  # 是否匹配低质量的框(更多细节见 API 文档)\n                ignore_iof_thr=-1),  # 忽略 bbox 的 IoF 阈值\n            sampler=dict(  # 正/负采样器(sampler)的配置\n                type='RandomSampler',  # 采样器类型，还支持 PseudoSampler 和其他采样器，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/samplers/random_sampler.py#L14\n                num=256,  # 样本数量。\n                pos_fraction=0.5,  # 正样本占总样本的比例\n                neg_pos_ub=-1,  # 基于正样本数量的负样本上限\n                add_gt_as_proposals=False),  # 采样后是否添加 GT 作为 proposal\n            allowed_border=-1,  # 填充有效锚点后允许的边框\n            pos_weight=-1,  # 训练期间正样本的权重\n            debug=False),  # 是否设置调试(debug)模式\n        rpn_proposal=dict(  # 在训练期间生成 proposals 的配置\n            nms_across_levels=False,  # 是否对跨层的 box 做 NMS。仅适用于 `GARPNHead` ，naive rpn 不支持 nms cross levels\n            nms_pre=2000,  # NMS 前的 box 数\n            nms_post=1000,  # NMS 要保留的 box 的数量，只在 GARPNHHead 中起作用\n            max_per_img=1000,  # NMS 后要保留的 box 数量\n            nms=dict( # NMS 的配置\n                type='nms',  # NMS 的类别\n                iou_threshold=0.7 # NMS 的阈值\n                ),\n            min_bbox_size=0),  # 允许的最小 box 尺寸\n        rcnn=dict(  # roi head 的配置。\n            assigner=dict(  # 第二阶段分配器的配置，这与 rpn 中的不同\n                type='MaxIoUAssigner',  # 分配器的类型，MaxIoUAssigner 目前用于所有 roi_heads。更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/assigners/max_iou_assigner.py#L14\n                pos_iou_thr=0.5,  # IoU >= 0.5(阈值)被认为是正样本\n                neg_iou_thr=0.5,  # IoU < 0.5(阈值)被认为是负样本\n                min_pos_iou=0.5,  # 将 box 作为正样本的最小 IoU 阈值\n                match_low_quality=False,  # 是否匹配低质量下的 box(有关更多详细信息，请参阅 API 文档)\n                ignore_iof_thr=-1),  # 忽略 bbox 的 IoF 阈值\n            sampler=dict(\n                type='RandomSampler',  # 采样器的类型，还支持 PseudoSampler 和其他采样器，更多细节请参考 https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/samplers/random_sampler.py#L14\n                num=512,  # 样本数量\n                pos_fraction=0.25,  # 正样本占总样本的比例\n                neg_pos_ub=-1,  # 基于正样本数量的负样本上限\n                add_gt_as_proposals=True\n            ),  # 采样后是否添加 GT 作为 proposal\n            mask_size=28,  # mask 的大小\n            pos_weight=-1,  # 训练期间正样本的权重\n            debug=False)),  # 是否设置调试模式\n    test_cfg = dict(  # 用于测试 rpn 和 rcnn 超参数的配置\n        rpn=dict(  # 测试阶段生成 proposals 的配置\n            nms_across_levels=False,  # 是否对跨层的 box 做 NMS。仅适用于 `GARPNHead`，naive rpn 不支持做 NMS cross levels\n            nms_pre=1000,  # NMS 前的 box 数\n            nms_post=1000  # NMS 要保留的 box 的数量，只在 `GARPNHHead` 中起作用\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Setting RandomSampler in RPN Head - Python\nDESCRIPTION: This code snippet demonstrates how to configure the `RandomSampler` in the RPN head's `train_cfg` to address class imbalance during training. It specifies the sampler type, number of samples, positive fraction, negative-positive upper bound, and whether to add ground truth as proposals.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg=dict(\n    rpn=dict(\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False))\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Faster R-CNN with SoftTeacher for Semi-Supervised Learning (Python)\nDESCRIPTION: This snippet configures Faster R-CNN with SoftTeacher for semi-supervised learning in MMDetection. It defines the model structure, data preprocessor, backbone (ResNet50 with caffe style), and semi-supervised training configurations, including pseudo-labeling thresholds and weight parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py', '../_base_/default_runtime.py',\n    '../_base_/datasets/semi_coco_detection.py'\n]\n\ndetector = _base_.model\ndetector.data_preprocessor = dict(\n    type='DetDataPreprocessor',\n    mean=[103.530, 116.280, 123.675],\n    std=[1.0, 1.0, 1.0],\n    bgr_to_rgb=False,\n    pad_size_divisor=32)\ndetector.backbone = dict(\n    type='ResNet',\n    depth=50,\n    num_stages=4,\n    out_indices=(0, 1, 2, 3),\n    frozen_stages=1,\n    norm_cfg=dict(type='BN', requires_grad=False),\n    norm_eval=True,\n    style='caffe',\n    init_cfg=dict(\n        type='Pretrained',\n        checkpoint='open-mmlab://detectron2/resnet50_caffe'))\n\nmodel = dict(\n    _delete_=True,\n    type='SoftTeacher',\n    detector=detector,\n    data_preprocessor=dict(\n        type='MultiBranchDataPreprocessor',\n        data_preprocessor=detector.data_preprocessor),\n    semi_train_cfg=dict(\n        freeze_teacher=True,\n        sup_weight=1.0,\n        unsup_weight=4.0,\n        pseudo_label_initial_score_thr=0.5,\n        rpn_pseudo_thr=0.9,\n        cls_pseudo_thr=0.9,\n        reg_pseudo_thr=0.02,\n        jitter_times=10,\n        jitter_scale=0.06,\n        min_pseudo_bbox_wh=(1e-2, 1e-2)),\n    semi_test_cfg=dict(predict_on='teacher'))\n```\n\n----------------------------------------\n\nTITLE: Initializing a Model with init_cfg Directly in Code (Python)\nDESCRIPTION: This code snippet shows how to initialize a model by directly using `init_cfg` within the `__init__` method of the `FooModel` class.  The `init_cfg` argument is passed to the `BaseModule` constructor, enabling weight initialization according to the specified configuration. This approach offers direct control over the initialization process within the model's code.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule\n# or directly inherit mmdet models\n\nclass FooModel(BaseModule):\n\tdef __init__(self,\n                  arg1,\n                  arg2,\n                  init_cfg=XXX):\n\t\tsuper(FooModel, self).__init__(init_cfg)\n  \t  ...\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration with Visualization (3.x)\nDESCRIPTION: This snippet configures logging with visualization backends like TensorBoard and WandB in MMDetection 3.x. It defines visual backends and a visualizer.  The visualizer orchestrates logging to local files, Tensorboard, and Wandb.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nvis_backends = [\n    dict(type='LocalVisBackend'),\n    dict(type='TensorboardVisBackend'),\n    dict(type='WandbVisBackend',\n         init_kwargs={\n            'project': 'mmdetection',\n            'group': 'maskrcnn-r50-fpn-1x-coco'\n         })\n]\nvisualizer = dict(\n    type='DetLocalVisualizer',\n    vis_backends=vis_backends,\n    name='visualizer')\n```\n\n----------------------------------------\n\nTITLE: Running Video Inference with GPU Acceleration\nDESCRIPTION: This script performs object detection inference on a video file using GPU acceleration. It uses the nvdecode flag for GPU-based decoding and allows specifying device, score threshold, output file, display option, and wait time.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_gpuaccel_demo.py \\\n    ${VIDEO_FILE} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--score-thr ${SCORE_THR}] \\\n    [--nvdecode] \\\n    [--out ${OUT_FILE}] \\\n    [--show] \\\n    [--wait-time ${WAIT_TIME}]\n```\n\n----------------------------------------\n\nTITLE: Running GLIP Inference\nDESCRIPTION: This command runs the inference demo for the GLIP model using a demo image, model weights, and text prompts. The '--texts' argument specifies the objects to be detected.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg glip_tiny_a_mmdet-b3654169.pth --texts bench\n```\n\n----------------------------------------\n\nTITLE: SDK Model Inference using MMDeploy Python API\nDESCRIPTION: This code snippet demonstrates how to perform model inference using the MMDeploy SDK's Python API.  It loads an image, creates a detector instance using a specified model path and device, performs inference, and then visualizes the detected bounding boxes. It requires the `mmdeploy_python` package and OpenCV (`cv2`) to be installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/deploy.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy_python import Detector\nimport cv2\n\nimg = cv2.imread('demo/demo.jpg')\n\n# create a detector\ndetector = Detector(model_path='mmdeploy_models/mmdet/onnx',\n                    device_name='cpu', device_id=0)\n# perform inference\nbboxes, labels, masks = detector(img)\n\n# visualize inference result\nindices = [i for i in range(len(bboxes))]\nfor index, bbox, label_id in zip(indices, bboxes, labels):\n    [left, top, right, bottom], score = bbox[0:4].astype(int), bbox[4]\n    if score < 0.3:\n        continue\n\n    cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0))\n\ncv2.imwrite('output_detection.png', img)\n```\n\n----------------------------------------\n\nTITLE: Webcam Demo Example with MMDetection\nDESCRIPTION: This is an example usage of the webcam demo script, providing specific paths to the configuration and checkpoint files. It demonstrates a minimal setup without optional arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython demo/webcam_demo.py \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth\n```\n\n----------------------------------------\n\nTITLE: Train model on multiple GPUs with different ports\nDESCRIPTION: This snippet details how to launch multiple training jobs on a single machine with multiple GPUs by specifying different ports for each job to avoid communication conflicts. The CUDA_VISIBLE_DEVICES environment variable is used to select the GPUs for each job, and the PORT variable is used to set the communication port.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_5\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh ${CONFIG_FILE} 4\nCUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh ${CONFIG_FILE} 4\n```\n\n----------------------------------------\n\nTITLE: Image Inference Example Usage\nDESCRIPTION: This command shows an example of how to run the image inference demo script with a specific image, configuration file, weights file, and device.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    --weights checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --device cpu\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration (3.x Visualisation)\nDESCRIPTION: This code snippet demonstrates how to configure logging to TensorBoard and WandB in MMDetection 3.x. It defines a list of visual backends, including `LocalVisBackend`, `TensorboardVisBackend`, and `WandbVisBackend`, configuring the project and group for WandB logging.  The `visualizer` is then configured to use these backends. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nvis_backends = [\n    dict(type='LocalVisBackend'),\n    dict(type='TensorboardVisBackend'),\n    dict(type='WandbVisBackend',\n         init_kwargs={\n            'project': 'mmdetection',\n            'group': 'maskrcnn-r50-fpn-1x-coco'\n         })\n]\nvisualizer = dict(\n    type='DetLocalVisualizer', vis_backends=vis_backends, name='visualizer')\n```\n\n----------------------------------------\n\nTITLE: Run MOT Demo for Inference (MMDetection, Python/Shell)\nDESCRIPTION: This command executes the `mot_demo.py` script to perform multi-object tracking (MOT) inference on a video using a specified configuration file and checkpoint. The script takes a video as input, uses the specified model and configuration, and saves the output as a new video file.  The `${CHECKPOINT_FILE}` variable needs to be replaced with the actual path to the model checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/bytetrack/bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval.py --checkpoint ${CHECKPOINT_FILE} --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Multi-GPU Testing on Cityscapes\nDESCRIPTION: This shell script tests Mask R-CNN on the Cityscapes dataset using 8 GPUs. It generates txt and png files, which can be uploaded to the official evaluation server. The configuration files need to be updated with the correct test_evaluator and test_dataloader definitions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n       configs/cityscapes/mask-rcnn_r50_fpn_1x_cityscapes.py \\\n       checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe51d5a.pth \\\n       8\n```\n\n----------------------------------------\n\nTITLE: Prepare COCO Semantic Annotations from Panoptic\nDESCRIPTION: This script prepares semantic segmentation annotations for COCO from the panoptic annotations. It converts panoptic annotations into a format suitable for semantic segmentation tasks. The script expects the panoptic annotations to be located in the specified `data/coco` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/prepare_coco_semantic_annos_from_panoptic_annos.py data/coco\n```\n\n----------------------------------------\n\nTITLE: Config File Naming Convention - Shell\nDESCRIPTION: Example of the config file naming convention for mmdetection, where `{method}` represents the method name, `{module}` represents basic modules, `{train_cfg}` represents the training config, `{train_data}` represents the training data, and `{test_data}` represents the testing data.  This shows the general structure used for naming config files to maintain consistency and readability.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_config.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n{method}_{module}_{train_cfg}_{train_data}_{test_data}\n```\n\n----------------------------------------\n\nTITLE: Using EfficientNet-B1 from TIMM via MMPretrain\nDESCRIPTION: This code snippet shows how to integrate the EfficientNet-B1 backbone from the TIMM library into RetinaNet, using MMPretrain as an intermediary.  It involves installing mmpretrain, importing the necessary modules, and configuring the model to use the `TIMMBackbone` wrapper. The configuration specifies the model name, sets `features_only` to True, enables pre-trained weights, and defines the output indices. The neck's in_channels are also modified to match the EfficientNet-B1 output.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/how_to.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/open-mmlab/mmdetection/blob/main/configs/timm_example/retinanet_timm-efficientnet-b1_fpn_1x_coco.py\n\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\n# please install mmpretrain\n# import mmpretrain.models to trigger register_module in mmpretrain\ncustom_imports = dict(imports=['mmpretrain.models'], allow_failed_imports=False)\nmodel = dict(\n    backbone=dict(\n        _delete_=True, # Delete the backbone field in _base_\n        type='mmpretrain.TIMMBackbone', # Using timm from mmpretrain\n        model_name='efficientnet_b1',\n        features_only=True,\n        pretrained=True,\n        out_indices=(1, 2, 3, 4)), # Modify out_indices\n    neck=dict(in_channels=[24, 40, 112, 320])) # Modify in_channels\n\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n\n```\n\n----------------------------------------\n\nTITLE: Configure SemiBaseDetector model in Python\nDESCRIPTION: This code snippet configures the SemiBaseDetector model for semi-supervised object detection, using RetinaNet as the detector. It inherits from a base RetinaNet configuration and sets up the SemiBaseDetector with a MultiBranchDataPreprocessor and specific semi-training/testing configurations. Key parameters include the suppression weight, unsupervision weight, classification pseudo-threshold and minimum pseudo bounding box width and height.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py', '../_base_/default_runtime.py',\n    '../_base_/datasets/semi_coco_detection.py'\n]\n\ndetector = _base_.model\n\nmodel = dict(\n    _delete_=True,\n    type='SemiBaseDetector',\n    detector=detector,\n    data_preprocessor=dict(\n        type='MultiBranchDataPreprocessor',\n        data_preprocessor=detector.data_preprocessor),\n    semi_train_cfg=dict(\n        freeze_teacher=True,\n        sup_weight=1.0,\n        unsup_weight=1.0,\n        cls_pseudo_thr=0.9,\n        min_pseudo_bbox_wh=(1e-2, 1e-2)),\n    semi_test_cfg=dict(predict_on='teacher'))\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Evaluation on Test Dataset (Shell)\nDESCRIPTION: This command evaluates the pre-trained Grounding DINO model on the 'cat' dataset in a zero-shot setting. It employs the test.py tool for evaluation. Requires mmdetection environment and pre-trained weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_finetune_8xb4_20e_cat.py grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Mask R-CNN with DCN, GC Block (r=4), and X-101-FPN\nDESCRIPTION: This config file configures a Cascade Mask R-CNN model that includes an X-101-FPN backbone.  It employs Deformable Convolutions (DCN) in the c3-c5 stages and a Global Context (GC) block with a reduction ratio of 4 inserted after the 1x1 convolution layers of the backbone's c3-c5 stages.  SyncBN is utilized, and the training is performed for 1x epochs on COCO.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[config](./cascade-mask-rcnn_x101-32x4d-syncbn-dconv-c3-c5-r4-gcb-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Inference with List of Images using DetInferencer\nDESCRIPTION: This snippet performs object detection inference using the DetInferencer with a list of image paths or numpy arrays as input. Each element in the list is processed individually.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninferencer(['img_1.jpg', 'img_2.jpg])\n# 列表内混合类型也是允许的\ninferencer(['img_1.jpg', array])\n```\n\n----------------------------------------\n\nTITLE: Installing ddd-dataset (mmdetection)\nDESCRIPTION: This command installs the `ddd-dataset` package, which is likely a dependency or tool used for working with description detection datasets in the `mmdetection` project.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install ddd-dataset\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Grounding DINO Model (Shell)\nDESCRIPTION: This script fine-tunes the Grounding DINO model on the 'cat' dataset. It uses the dist_train.sh script for distributed training, assuming 8 GPUs are available. Requires the dataset and configurations to be prepared beforehand.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/mm_grounding_dino/grounding_dino_swin-t_finetune_8xb4_20e_cat.py 8 --work-dir cat_work_dir\n```\n\n----------------------------------------\n\nTITLE: Initializing a Sub-module When Layer is None (Python)\nDESCRIPTION: This code snippet demonstrates how to initialize only a specific sub-module (`reg`) when the `layer` key in `init_cfg` is not defined.  The `override` key is used to target the `reg` sub-module, and since no `layer` is specified, only `reg` will be initialized. This is useful when you want to use the default initialization for all other layers except for a specific sub-module.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# layers:\n# self.feat = nn.Conv1d(3, 1, 3)\n# self.reg = nn.Conv2d(3, 3, 3)\n# self.cls = nn.Linear(1,2)\n\ninit_cfg = dict(type='Constant', val=1, bias=2, \toverride=dict(name='reg'))\n\n# self.feat and self.cls will be initialized by Pytorch\n# The module called 'reg' will be initialized with dict(type='Constant', val=1, bias=2)\n```\n\n----------------------------------------\n\nTITLE: Importing MobileNet Backbone in MMDetection\nDESCRIPTION: Imports the newly defined MobileNet backbone into the MMDetection framework. This can be done by adding an import statement to the `mmdet/models/backbones/__init__.py` file or by using the `custom_imports` dictionary in the configuration file. The `custom_imports` approach avoids modifying the original code.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom .mobilenet import MobileNet\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Image Visualization (Shell)\nDESCRIPTION: This command visualizes a single image from the 'cat' dataset using the pre-trained Grounding DINO model. It leverages the image_demo.py script for demonstration. Requires mmdetection environment and pre-trained weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\npython demo/image_demo.py data/cat/images/IMG_20211205_120756.jpg configs/mm_grounding_dino/grounding_dino_swin-t_finetune_8xb4_20e_cat.py --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth --texts cat.\n```\n\n----------------------------------------\n\nTITLE: Enabling torch.compile and AMP for RTMDet in MMDetection (Multi-GPU)\nDESCRIPTION: This shell script demonstrates enabling both `torch.compile` and Automatic Mixed Precision (AMP) for RTMDet using multiple GPUs on a single node. The `--amp` flag is added alongside `--cfg-options compile=True` for combined optimization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/rtmdet/rtmdet_s_8xb32-300e_coco.py 8 --cfg-options compile=True --amp\n```\n\n----------------------------------------\n\nTITLE: Define DoubleConvFCBBoxHead in MMDetection (Python)\nDESCRIPTION: Defines a custom bounding box head, `DoubleConvFCBBoxHead`, for use in Double-Head R-CNN. It includes convolutional and fully connected layers for classification and regression, leveraging `mmcv.cnn.ConvModule` and `mmengine.model.BaseModule`. This head is registered with `MODELS` and intended for `mmdet/models/roi_heads/bbox_heads/double_bbox_head.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\n\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule\nfrom mmengine.model import BaseModule, ModuleList\nfrom torch import Tensor\n\nfrom mmdet.models.backbones.resnet import Bottleneck\nfrom mmdet.registry import MODELS\nfrom mmdet.utils import ConfigType, MultiConfig, OptConfigType, OptMultiConfig\nfrom .bbox_head import BBoxHead\n\n\n@MODELS.register_module()\nclass DoubleConvFCBBoxHead(BBoxHead):\n    r\"\"Bbox head used in Double-Head R-CNN\n\n    .. code-block:: none\n\n                                          /-> cls\n                      /-> shared convs ->\n                                          \\-> reg\n        roi features\n                                          /-> cls\n                      \\-> shared fc    ->\n                                          \\-> reg\n    \"\"  # noqa: W605\n\n    def __init__(self,\n                 num_convs: int = 0,\n                 num_fcs: int = 0,\n                 conv_out_channels: int = 1024,\n                 fc_out_channels: int = 1024,\n                 conv_cfg: OptConfigType = None,\n                 norm_cfg: ConfigType = dict(type='BN'),\n                 init_cfg: MultiConfig = dict(\n                     type='Normal',\n                     override=[\n                         dict(type='Normal', name='fc_cls', std=0.01),\n                         dict(type='Normal', name='fc_reg', std=0.001),\n                         dict(\n                             type='Xavier',\n                             name='fc_branch',\n                             distribution='uniform')\n                     ]),\n                 **kwargs) -> None:\n        kwargs.setdefault('with_avg_pool', True)\n        super().__init__(init_cfg=init_cfg, **kwargs)\n\n    def forward(self, x_cls: Tensor, x_reg: Tensor) -> Tuple[Tensor]:\n\n```\n\n----------------------------------------\n\nTITLE: Downloading a Pre-trained Model Checkpoint\nDESCRIPTION: This code snippet uses the `mim` tool to download the configuration and checkpoint file for the 'rtmdet_tiny_8xb32-300e_coco' model from MMDetection, saving them to the './checkpoints' directory. The `mkdir` command creates the directory if it doesn't exist.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!mkdir ./checkpoints\n!mim download mmdet --config rtmdet_tiny_8xb32-300e_coco --dest ./checkpoints\n```\n\n----------------------------------------\n\nTITLE: Phrase Grounding with Manual Tokens - Shell\nDESCRIPTION: This shell command demonstrates phrase grounding with manually specified noun phrases. It runs the `image_demo.py` script with the model configuration, weights file, and a descriptive text. The `--texts` parameter provides the sentence, and `--tokens-positive` specifies the character positions of the noun phrases to be detected. The `--pred-score-thr` sets the prediction score threshold.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/fruit.jpg \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'The picture contains watermelon, flower, and a white bottle.' \\\n        --tokens-positive \"[[[21,31]], [[45,59]]]\"  --pred-score-thr 0.12\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Faster RCNN model using a RegNetX-1.6GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n[config](./faster-rcnn_regnetx-1.6GF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Configuring PAFPN Neck in MMDetection\nDESCRIPTION: Configures the PAFPN neck within the MMDetection model configuration. The `type` parameter specifies the name of the neck module, and other parameters like `in_channels`, `out_channels`, and `num_outs` are passed to the neck's constructor.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nneck=dict(\n    type='PAFPN',\n    in_channels=[256, 512, 1024, 2048],\n    out_channels=256,\n    num_outs=5)\n```\n\n----------------------------------------\n\nTITLE: Single GPU Training Shell Command\nDESCRIPTION: This shell command demonstrates how to train a model on a single GPU using MMDetection. The `${CONFIG_FILE}` variable should be replaced with the path to the configuration file for the desired model. Optional arguments can be added to further customize the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Converting OpenImages v6 to ODVG format\nDESCRIPTION: This snippet demonstrates how to convert the OpenImages v6 dataset to the ODVG (Object Detection with Visual Grounding) format using the provided Python script. The script processes the annotations from OpenImages and generates two new files: `oidv6-train-annotation_od.json` and `openimages_label_map.json` within the specified annotation directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/openimages2odvg.py data/OpenImages/annotations\n```\n\n----------------------------------------\n\nTITLE: Configuring RPN for Proposal Evaluation (Python)\nDESCRIPTION: This configuration file sets up the Region Proposal Network (RPN) using the FCOSHead for evaluating proposal quality.  It defines the model architecture and evaluation metrics for assessing the RPN's performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/single_stage_as_rpn.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/rpn_r50_fpn.py', '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nval_evaluator = dict(metric='proposal_fast')\ntest_evaluator = val_evaluator\n\nmodel = dict(\n    # copied from configs/fcos/fcos_r50-caffe_fpn_gn-head_1x_coco.py\n    neck=dict(\n        start_level=1,\n        add_extra_convs='on_output',  # use P5\n        relu_before_extra_convs=True),\n    rpn_head=dict(\n        _delete_=True,  # ignore the unused old settings\n        type='FCOSHead',\n        num_classes=1,  # num_classes = 1 for rpn, if num_classes > 1, it will be set to 1 in RPN automatically\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n\n```\n\n----------------------------------------\n\nTITLE: Train DINO with FSDP on 8 GPUs (Bash)\nDESCRIPTION: This bash script demonstrates how to train the `dino-5scale_swin-l_fsdp_8xb2-12e_coco.py` configuration using Fully Sharded Data Parallelism (FSDP) across 8 GPUs. The second command adds Automatic Mixed Precision (AMP) for further memory reduction. Assumes you are in the mmdetection root directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_largemodel/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd mmdetection\n./tools/dist_train.sh projects/example_largemodel/dino-5scale_swin-l_fsdp_8xb2-12e_coco.py 8\n./tools/dist_train.sh projects/example_largemodel/dino-5scale_swin-l_fsdp_8xb2-12e_coco.py 8 --amp\n```\n\n----------------------------------------\n\nTITLE: Install PyTorch with Conda (GPU)\nDESCRIPTION: This command installs PyTorch and torchvision with CUDA support using conda. It is specific to systems with GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nconda install pytorch torchvision -c pytorch\n```\n\n----------------------------------------\n\nTITLE: Visualizing Single Image Detections with Grounding DINO (Shell)\nDESCRIPTION: This command visualizes the detection results for a single image using a pre-trained Grounding DINO model. It specifies the image path, configuration file, weights file, and the text prompt for the object to be detected (in this case, 'cat').\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\npython demo/image_demo.py data/cat/images/IMG_20211205_120756.jpg configs/mm_grounding_dino/grounding_dino_swin-t_finetune_8xb4_20e_cat.py --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth --texts cat.\n```\n\n----------------------------------------\n\nTITLE: Overriding categories in ODinW annotations\nDESCRIPTION: This script is used to modify the category information within the ODinW (Object Detection in the Wild) dataset's annotation files, which is necessary for customizing prompts during evaluation of grounding pre-trained models.  It takes the path to the ODinW data directory as input and generates new annotation files without overwriting the original ones.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\npython configs/mm_grounding_dino/odinw/override_category.py data/odinw/\n```\n\n----------------------------------------\n\nTITLE: Dumping Predictions to File (Python)\nDESCRIPTION: This snippet shows how to save inference results (predictions and visualizations) to files in a specified output directory. Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninferencer('demo/demo.jpg', out_dir='outputs/', no_save_pred=False)\n```\n\n----------------------------------------\n\nTITLE: FreeAnchor Config - ResNet-101 Backbone - PyTorch\nDESCRIPTION: This configuration file sets up FreeAnchor with a ResNet-101 backbone in PyTorch. It configures the model, training, and evaluation settings for object detection on the COCO dataset. The configuration is linked to corresponding model weights and log files for reproducibility.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/free_anchor/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[config](./freeanchor_r101_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Test Faster R-CNN and Visualize Results (analyze_results.py)\nDESCRIPTION: This example tests a Faster R-CNN model, visualizes the detection results, and saves the images to the `results/` directory. It requires a configuration file (`configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py`) and a prediction file (`result.pkl`).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       result.pkl \\\n       results \\\n       --show\n```\n\n----------------------------------------\n\nTITLE: Inference demo (MIM install)\nDESCRIPTION: This snippet shows how to perform inference using MMDetection after installing it with MIM. It initializes a detector model and runs inference on an image, printing the results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom mmdet.apis import init_detector, inference_detector\n\nconfig_file = 'rtmdet_tiny_8xb32-300e_coco.py'\ncheckpoint_file = 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\nmodel = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\ninference_detector(model, 'demo/demo.jpg')\n```\n\n----------------------------------------\n\nTITLE: Installing Grounding DINO via PIP\nDESCRIPTION: These commands install the necessary dependencies for Grounding DINO using pip or mim. The first command changes the current directory to the MMDETROOT directory. The pip command installs the multimodal requirements from the requirements.txt file.  The mim command installs the mmdet[multimodal] package.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd $MMDETROOT\n\n# source installation\npip install -r requirements/multimodal.txt\n\n# or mim installation\nmim install mmdet[multimodal]\n```\n\n----------------------------------------\n\nTITLE: Chunked Closed-Set Object Detection - Shell\nDESCRIPTION: This shell command performs chunked closed-set object detection using the MM Grounding DINO model. It runs the `image_demo.py` script with the specified configuration, weights, and dataset (lvis). The `--chunked-size` parameter splits the category names into chunks to avoid exceeding the token limit, and `--palette random` sets a random color palette for visualization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/animals.png \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts '$: lvis'  --chunked-size 70 \\\n        --palette random\n```\n\n----------------------------------------\n\nTITLE: Running MOT Inference Script in MMDetection\nDESCRIPTION: This script `mot_demo.py` performs inference on a video or image sequence using a multiple object tracking model. It requires specifying the input, configuration file, and optionally the checkpoint file, detector file, reid file, score threshold, device, and output path. The script outputs a visualized demo video or displays it on the fly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_inference.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py \\\n    ${INPUTS}\n    ${CONFIG_FILE} \\\n    [--checkpoint ${CHECKPOINT_FILE}] \\\n    [--detector ${DETECTOR_FILE}] \\\n    [--reid ${REID_FILE}] \\\n    [--score-thr ${SCORE_THR}] \\\n    [--device ${DEVICE}] \\\n    [--out ${OUTPUT}] \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Evaluation of Grounding DINO (Shell)\nDESCRIPTION: This command performs zero-shot evaluation of a pre-trained Grounding DINO model on a test dataset. It takes the configuration file and the weights file as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_finetune_8xb4_20e_cat.py grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Testing with Noise Corruptions\nDESCRIPTION: This shell command tests model performance specifically with noise corruptions. It uses the `test_robustness.py` script, specifying the configuration file, checkpoint file, output file (optional), evaluation metrics (optional), and the `--corruptions noise` argument to apply noise-based image corruptions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# noise\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --corruptions noise\n```\n\n----------------------------------------\n\nTITLE: Defining a PAFPN Neck in MMDetection\nDESCRIPTION: Defines a new PAFPN neck module within the MMDetection framework. The module inherits from `nn.Module` and is registered using the `MODELS.register_module()` decorator. It includes an `__init__` method for initialization with input/output channel parameters and a `forward` method for feature fusion.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\n\nfrom mmdet.registry import MODELS\n\n@MODELS.register_module()\nclass PAFPN(nn.Module):\n\n    def __init__(self,\n                in_channels,\n                out_channels,\n                num_outs,\n                start_level=0,\n                end_level=-1,\n                add_extra_convs=False):\n        pass\n\n    def forward(self, inputs):\n        # implementation is ignored\n        pass\n```\n\n----------------------------------------\n\nTITLE: Initializing with a Pretrained Model (Python)\nDESCRIPTION: This code snippet shows how to initialize the model with weights from a pre-trained model using the `Pretrained` type in `init_cfg`. The `checkpoint` key specifies the path to the pre-trained model's checkpoint file. In this case, it uses a ResNet50 model from `torchvision`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninit_cfg = dict(type='Pretrained',\n               checkpoint='torchvision://resnet50')\n```\n\n----------------------------------------\n\nTITLE: Plotting classification and regression loss (Shell)\nDESCRIPTION: This command plots both the classification and regression loss curves from 'log.json' and saves the output as 'losses.pdf'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls loss_bbox --out losses.pdf\n```\n\n----------------------------------------\n\nTITLE: Testing the EfficientDet model\nDESCRIPTION: This command tests the EfficientDet model using the MMDetection testing script. It requires specifying the configuration file for the EfficientDet model and the path to the checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython tools/test.py projects/EfficientDet/configs/efficientdet_effb3_bifpn_8xb16-crop896-300e_coco.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Custom Imports Configuration in MMDetection\nDESCRIPTION: This snippet demonstrates the use of `custom_imports` in MMDetection's configuration system to import and register custom modules, specifically the DoubleHeadRoIHead and DoubleConvFCBBoxHead. This allows the framework to find and load these modules during model initialization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports=dict(\n    imports=['mmdet.models.roi_heads.double_roi_head', 'mmdet.models.roi_heads.bbox_heads.double_bbox_head'])\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - R-50 Backbone, 1x Schedule\nDESCRIPTION: This configuration file is used to train VarifocalNet with a ResNet-50 backbone, using a 1x learning rate schedule, and trained on the COCO dataset. It defines the model architecture, training parameters, and evaluation metrics for object detection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_r50_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Testing TorchServe deployment using curl\nDESCRIPTION: This command tests the TorchServe deployment by sending an image to the server using curl and retrieving the prediction results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/3dogs.jpg\ncurl http://127.0.0.1:8080/predictions/${MODEL_NAME} -T 3dogs.jpg\n```\n\n----------------------------------------\n\nTITLE: Convert Balloon to COCO (Python)\nDESCRIPTION: Python script to convert the balloon dataset's annotation format into the COCO format. It iterates through the balloon dataset annotations, extracts image information (height, width), polygon coordinates, and creates corresponding entries for images and annotations in the COCO format. It depends on `mmcv` and `mmengine` for file I/O and image processing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport os.path as osp\n\nimport mmcv\n\nfrom mmengine.fileio import dump, load\nfrom mmengine.utils import track_iter_progress\n\n\ndef convert_balloon_to_coco(ann_file, out_file, image_prefix):\n    data_infos = load(ann_file)\n\n    annotations = []\n    images = []\n    obj_count = 0\n    for idx, v in enumerate(track_iter_progress(data_infos.values())):\n        filename = v['filename']\n        img_path = osp.join(image_prefix, filename)\n        height, width = mmcv.imread(img_path).shape[:2]\n\n        images.append(\n            dict(id=idx, file_name=filename, height=height, width=width))\n\n        for _, obj in v['regions'].items():\n            assert not obj['region_attributes']\n            obj = obj['shape_attributes']\n            px = obj['all_points_x']\n            py = obj['all_points_y']\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = [p for x in poly for p in x]\n\n            x_min, y_min, x_max, y_max = (min(px), min(py), max(px), max(py))\n\n            data_anno = dict(\n                image_id=idx,\n                id=obj_count,\n                category_id=0,\n                bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n                area=(x_max - x_min) * (y_max - y_min),\n                segmentation=[poly],\n                iscrowd=0)\n            annotations.append(data_anno)\n            obj_count += 1\n\n    coco_format_json = dict(\n        images=images,\n        annotations=annotations,\n        categories=[{\n            'id': 0,\n            'name': 'balloon'\n        }])\n    dump(coco_format_json, out_file)\n\n\nif __name__ == '__main__':\n    convert_balloon_to_coco(ann_file='data/balloon/train/via_region_data.json',\n                            out_file='data/balloon/train/annotation_coco.json',\n                            image_prefix='data/balloon/train')\n    convert_balloon_to_coco(ann_file='data/balloon/val/via_region_data.json',\n                            out_file='data/balloon/val/annotation_coco.json',\n                            image_prefix='data/balloon/val')\n```\n\n----------------------------------------\n\nTITLE: Converting COCO2017 to ODVG format (Python)\nDESCRIPTION: This script converts the COCO2017 annotation file (after removing overlapping images with RefCOCO) to the ODVG format, which is required for training. It uses the coco2odvg.py conversion tool.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/coco/annotations/instances_train2017_norefval.json -d coco\n```\n\n----------------------------------------\n\nTITLE: Test model on single GPU in MMDetection\nDESCRIPTION: This snippet shows how to test a model on a single GPU using tools/test_tracking.py. The CONFIG_FILE variable specifies the path to the configuration file, and optional arguments can be passed to customize the testing process. The CUDA_VISIBLE_DEVICES environment variable is used to specify the GPU to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_12\n\nLANGUAGE: shell script\nCODE:\n```\npython tools/test_tracking.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Multiple Jobs with Port Specification\nDESCRIPTION: These shell commands demonstrate how to launch multiple training jobs simultaneously on a single machine with multiple GPUs, specifying different ports for each job to avoid communication conflicts. `CUDA_VISIBLE_DEVICES` specifies which GPUs to use for each job, and `PORT` sets the communication port.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh ${CONFIG_FILE} 4\nCUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh ${CONFIG_FILE} 4\n```\n\n----------------------------------------\n\nTITLE: Model Conversion using deploy.py (MMDetection)\nDESCRIPTION: This command converts an MMDetection model to ONNX and TensorRT formats using the `deploy.py` script. It takes configuration files for both MMDetection and MMDeploy, a checkpoint file, and an input image.  The converted model and engine file are saved in the specified work directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npython tools/deploy.py \\\n  configs/mmdet/instance-seg/instance-seg_rtmdet-ins_tensorrt_static-640x640.py \\\n  ${PATH_TO_MMDET}/configs/rtmdet/rtmdet-ins_s_8xb32-300e_coco.py \\\n  checkpoint/rtmdet-ins_s_8xb32-300e_coco_20221121_212604-fdc5d7ec.pth \\\n  demo/resources/det.jpg \\\n  --work-dir ./work_dirs/rtmdet-ins \\\n  --device cuda:0 \\\n  --show\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Digital Corruptions\nDESCRIPTION: This command runs a robustness test with digital corruptions. `${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] --corruptions digital\n```\n\n----------------------------------------\n\nTITLE: Testing Grounding DINO on Flickr30k (Shell)\nDESCRIPTION: This shell script executes the testing process for Grounding DINO on the Flickr30k dataset. It changes the directory to mmdetection, then executes the dist_test.sh script with the configuration file, checkpoint file, and the number of GPUs to use (8).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\nbash tools/dist_test.sh configs/grounding_dino/flickr30k/grounding_dino_swin-t-pretrain_zeroshot_flickr30k.py checkpoints/groundingdino_swint_ogc_mmdet-822d7e9d.pth 8\n```\n\n----------------------------------------\n\nTITLE: Download MMDetection Config and Checkpoint\nDESCRIPTION: Downloads a pre-trained RTMDet model configuration and checkpoint file using MIM for demonstration purposes.  The files are saved to the current directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmim download mmdet --config rtmdet_tiny_8xb32-300e_coco --dest .\n```\n\n----------------------------------------\n\nTITLE: Test model on CPU in MMDetection\nDESCRIPTION: This snippet demonstrates how to test a model on a CPU by disabling GPU visibility. The CUDA_VISIBLE_DEVICES environment variable is set to -1, and then the tools/test_tracking.py script is executed with the desired configuration file and optional arguments, including specifying the detector checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_10\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/test_tracking.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Example: Train QDTrack with Slurm\nDESCRIPTION: This snippet provides a concrete example of training the QDTrack model on a Slurm cluster. It sets specific configurations, including the port, number of GPUs per node, and Slurm arguments. It then calls the slurm_train.sh script with the partition, job name, config file, working directory, and number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_9\n\nLANGUAGE: shell script\nCODE:\n```\nPORT=29501 \\\nGPUS_PER_NODE=8 \\\nSRUN_ARGS=\"--quotatype=reserved\" \\\nbash ./tools/slurm_train.sh \\\nmypartition \\\nmottrack\nconfigs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py\n./work_dirs/QDTrack \\\n8\n```\n\n----------------------------------------\n\nTITLE: Plot Classification Loss (analyze_logs.py)\nDESCRIPTION: This example demonstrates how to plot the classification loss from a training log using `analyze_logs.py`. The `log.json` file is analyzed and the 'loss_cls' key is plotted with the corresponding legend.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls --legend loss_cls\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection from Source (Shell)\nDESCRIPTION: This snippet clones the MMDetection repository from GitHub and installs it in editable mode using `pip`. The `git clone` command downloads the repository, `%cd` changes the current directory to the cloned repository, and `pip install -e .` installs MMDetection with symbolic links to the source code, allowing for modifications without reinstalling.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection\n!pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Testing with Severity Level 1\nDESCRIPTION: This shell command evaluates model performance with a corruption severity level of 1. The `test_robustness.py` script is executed using a config file, checkpoint file, and optional arguments for output and eval metrics. The `--severities 1` argument indicates that image corruptions should be applied with a severity level of 1.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# severity 1\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --severities 1\n```\n\n----------------------------------------\n\nTITLE: Test model on multiple GPUs (single node) in MMDetection\nDESCRIPTION: This snippet demonstrates how to test a model on multiple GPUs within a single node using the dist_test_tracking.sh script. The CONFIG_FILE and GPU_NUM variables specify the configuration file and the number of GPUs to use, respectively. Optional arguments can be passed to customize the testing process, including specifying the detector and reid checkpoint files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_14\n\nLANGUAGE: shell script\nCODE:\n```\nbash ./tools/dist_test_tracking.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Inference with a List of Inputs (Python)\nDESCRIPTION: This code snippet demonstrates how to perform inference on a list of image paths and numpy arrays. Requires the `mmdet` and `mmcv` libraries.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninferencer(['img_1.jpg', 'img_2.jpg])\n# You can even mix the types\ninferencer(['img_1.jpg', array])\n```\n\n----------------------------------------\n\nTITLE: Fusing and evaluating three model predictions (Shell)\nDESCRIPTION: This command fuses the predictions from three models using specified weights and evaluates the performance against an annotation file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ./faster-rcnn_r50-caffe_fpn_1x_coco.json \\\n       ./retinanet_r50-caffe_fpn_1x_coco.json \\\n       ./cascade-rcnn_r50-caffe_fpn_1x_coco.json \\\n       --annotation ./annotation.json \\\n       --weights 1 2 3\n```\n\n----------------------------------------\n\nTITLE: Convert CrowdHuman Annotations to COCO format\nDESCRIPTION: This script converts the CrowdHuman dataset annotations to the COCO format. It takes input and output directories as arguments.  The output is saved as a JSON file containing bounding box and annotation data in COCO format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_dataset_prepare.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython ./tools/dataset_converters/crowdhuman2coco.py -i ./data/crowdhuman -o ./data/crowdhuman/annotations\n```\n\n----------------------------------------\n\nTITLE: Preparing Cat Dataset (mmdetection)\nDESCRIPTION: These commands download and extract a sample cat dataset for fine-tuning. `wget` downloads the `cat_dataset.zip` file, and `unzip` extracts its contents into the `data/cat/` directory. This prepares the dataset for use in training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\nwget https://download.openmmlab.com/mmyolo/data/cat_dataset.zip\nunzip cat_dataset.zip -d data/cat/\n```\n\n----------------------------------------\n\nTITLE: Converting COCO to ODVG format\nDESCRIPTION: This snippet demonstrates how to convert the COCO dataset to the ODVG (Object Detection with Visual Grounding) format.  It utilizes the `coco2odvg.py` script, specifying the COCO annotation file and the dataset type (-d coco) to generate `instances_train2017_norefval_od.json` and `coco_label_map.json` files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/coco/annotations/instances_train2017_norefval.json -d coco\n```\n\n----------------------------------------\n\nTITLE: Using MobileNetV3 Backbone from MMPretrain in RetinaNet (Python)\nDESCRIPTION: This configuration demonstrates how to use MobileNetV3-small from MMPretrain as the backbone for RetinaNet in MMDetection. It imports the necessary module from MMPretrain, deletes the default backbone configuration, specifies the MobileNetV3 architecture, modifies output indices, and sets the initialization configuration to load pretrained weights. It also adjusts the in_channels of the neck.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/how_to.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# please install mmpretrain\n# import mmpretrain.models to trigger register_module in mmpretrain\ncustom_imports = dict(imports=['mmpretrain.models'], allow_failed_imports=False)\npretrained = 'https://download.openmmlab.com/mmclassification/v0/mobilenet_v3/convert/mobilenet_v3_small-8427ecf0.pth'\nmodel = dict(\n    backbone=dict(\n        _delete_=True, # 将 _base_ 中关于 backbone 的字段删除\n        type='mmpretrain.MobileNetV3', # 使用 mmpretrain 中的 MobileNetV3\n        arch='small',\n        out_indices=(3, 8, 11), # 修改 out_indices\n        init_cfg=dict(\n            type='Pretrained',\n            checkpoint=pretrained,\n            prefix='backbone.')), # mmpretrain 中骨干网络的预训练权重含义 prefix='backbone.'，为了正常加载权重，需要把这个 prefix 去掉。\n    # 修改 in_channels\n    neck=dict(in_channels=[24, 48, 96], start_level=0))\n\n```\n\n----------------------------------------\n\nTITLE: Training a New Model\nDESCRIPTION: This command trains a new Mask R-CNN model using the provided configuration file. It uses the `tools/train.py` script from MMDetection, specifying the configuration file for the balloon dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py configs/balloon/mask-rcnn_r50-caffe_fpn_ms-poly-1x_balloon.py\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot COCO2017 Validation - Multi GPU - Shell\nDESCRIPTION: This shell script evaluates the MM Grounding DINO model on the COCO2017 validation dataset using multiple GPUs. It executes the `tools/dist_test.sh` script with the model configuration, weights file, and the number of GPUs to utilize (8 in this example). This allows for faster evaluation by distributing the workload across multiple GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth 8\n```\n\n----------------------------------------\n\nTITLE: Testing RPN Proposals with Trained Checkpoint (Bash)\nDESCRIPTION: This bash command tests the quality of proposals generated by the RPN using a trained checkpoint.  It specifies the configuration file, the path to the checkpoint file, and the number of GPUs to use for testing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/single_stage_as_rpn.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbash tools/dist_test.sh \\\n    configs/rpn/fcos-rpn_r50_fpn_1x_coco.py \\\n    ./work_dirs/faster-rcnn_r50_fpn_fcos-rpn_1x_coco/epoch_12.pth \\\n    8\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Inference Latency in MMDetection (Shell)\nDESCRIPTION: This shell command runs the benchmark script in MMDetection to measure inference latency. It takes a configuration file and a checkpoint file as input.  The optional `--log-interval` argument sets the output log interval. The optional `--fuse-conv-bn` argument fuses convolution and batch normalization layers for potentially lower latency.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/model_zoo.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/benchmark.py ${CONFIG} ${CHECKPOINT} [--log-interval $[LOG-INTERVAL]] [--fuse-conv-bn]\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration with Custom Dataset\nDESCRIPTION: This snippet demonstrates how to configure the dataset in the configuration file to use the newly defined custom dataset (`MyDataset`).  The `type` parameter is set to the name of the registered dataset, and the path to the annotation file is specified.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndataset_A_train = dict(\n    type='MyDataset',\n    ann_file = 'image_list.txt',\n    pipeline=train_pipeline\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing TorchServe and PyTorch results\nDESCRIPTION: This command runs a script to compare the results of TorchServe and PyTorch, visualizing the differences. It requires an image file, a configuration file, a checkpoint file, and a model name.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\npython tools/deployment/test_torchserver.py ${IMAGE_FILE} ${CONFIG_FILE} ${CHECKPOINT_FILE} ${MODEL_NAME}\n[--inference-addr ${INFERENCE_ADDR}] [--device ${DEVICE}] [--score-thr ${SCORE_THR}] [--work-dir ${WORK_DIR}]\n```\n\n----------------------------------------\n\nTITLE: Configure Parameter Scheduler in MMDetection (Python)\nDESCRIPTION: This code snippet configures the parameter scheduler, which adjusts optimization hyperparameters such as learning rate. It combines a linear learning rate warm-up scheduler with a multi-step learning rate scheduler. The linear warm-up occurs in the initial iterations, followed by a multi-step decay during training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    # Linear learning rate warm-up scheduler\n    dict(\n        type='LinearLR',  # Use linear policy to warmup learning rate\n        start_factor=0.001, # The ratio of the starting learning rate used for warmup\n        by_epoch=False,  # The warmup learning rate is updated by iteration\n        begin=0,  # Start from the first iteration\n        end=500),  # End the warmup at the 500th iteration\n    # The main LRScheduler\n    dict(\n        type='MultiStepLR',  # Use multi-step learning rate policy during training\n        by_epoch=True,  # The learning rate is updated by epoch\n        begin=0,   # Start from the first epoch\n        end=12,  # End at the 12th epoch\n        milestones=[8, 11],  # Epochs to decay the learning rate\n        gamma=0.1)  # The learning rate decay ratio\n]\n```\n\n----------------------------------------\n\nTITLE: Migrating CityScapes Evaluator Configuration in MMDetection\nDESCRIPTION: Illustrates migrating the CityScapes evaluator configuration. Version 3.x uses a dedicated `CityScapesMetric` and can also use the `CocoMetric` concurrently.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type='CityScapesDataset',\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        img_prefix=data_root + 'leftImg8bit/val/',\n        pipeline=test_pipeline))\nevaluation = dict(metric=['bbox', 'segm'])\n```\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = [\n    dict(\n        type='CocoMetric',\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        metric=['bbox', 'segm']),\n    dict(\n        type='CityScapesMetric',\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        seg_prefix=data_root + '/gtFine/val',\n        outfile_prefix='./work_dirs/cityscapes_metric/instance')\n]\n```\n\n----------------------------------------\n\nTITLE: Training Mask2Former with dist_train.sh\nDESCRIPTION: This shell script command demonstrates how to train Mask2Former on the YouTube-VIS-2021 dataset using the `dist_train.sh` script. It specifies the configuration file and the number of GPUs to use (8 in this case).  The training process utilizes distributed training to leverage multiple GPUs for faster convergence.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask2former_vis/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Training Mask2Former on YouTube-VIS-2021 dataset with following command.\n# The number after config file represents the number of GPUs used. Here we use 8 GPUs.\nbash tools/dist_train.sh configs/mask2former_vis/mask2former_r50_8xb2-8e_youtubevis2021.py 8\n```\n\n----------------------------------------\n\nTITLE: Enabling DetVisualizationHook and Displaying Images (Python)\nDESCRIPTION: This snippet demonstrates how to enable the DetVisualizationHook and configure it to draw annotations and predictions while simultaneously displaying the images. It updates the default visualization configuration to enable drawing and showing the images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/visualization.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nvisualization = _base_.default_hooks.visualization\nvisualization.update(dict(draw=True, show=True))\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Neck Module (AugFPN)\nDESCRIPTION: Defines a custom neck module called `AugFPN` in the `mmdet/models/necks` directory. It registers the module with `MODELS.register_module()` for use in MMDetection configurations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom mmdet.registry import MODELS\n\n@MODELS.register_module()\nclass AugFPN(nn.Module):\n\n    def __init__(self,\n                in_channels,\n                out_channels,\n                num_outs,\n                start_level=0,\n                end_level=-1,\n                add_extra_convs=False):\n        pass\n\n    def forward(self, inputs):\n        # implementation is ignored\n        pass\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluators for Test Dataset in MMDetection (Python)\nDESCRIPTION: This code configures the test dataloader and evaluator for a test dataset with no annotation files. It is configured to output results as a COCO JSON file. This snippet shows how to configure dataloaders and evaluators when the test dataset does not have annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# inference on test dataset and\n# format the output results for submission.\ntest_dataloader = dict(\n    batch_size=1,\n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file=data_root + 'annotations/image_info_test-dev2017.json',\n        data_prefix=dict(img='test2017/'),\n        test_mode=True,\n        pipeline=test_pipeline))\ntest_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'annotations/image_info_test-dev2017.json',\n    metric=['bbox', 'segm'],  # Metrics to be evaluated\n    format_only=True,  # Only format and save the results to coco json file\n    outfile_prefix='./work_dirs/coco_detection/test')  # The prefix of output json files\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Faster RCNN model with a RegNetX-4GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-4GF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-4GF_fpn_mstrain_3x_coco_20210526_095201-65eaf841.pth)\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Faster RCNN model with a RegNetX-800MF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-800MF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-800MF_fpn_mstrain_3x_coco_20210526_095118-a2c70b20.pth)\n```\n\n----------------------------------------\n\nTITLE: Training on Multi-GPU with MMDetection\nDESCRIPTION: This snippet illustrates how to train a model on multiple GPUs using MMDetection. It uses the `tools/dist_train.sh` script, specifying the configuration file and the number of GPUs to use. This script facilitates distributed training across multiple GPUs on a single node.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_4\n\nLANGUAGE: shell 脚本\nCODE:\n```\nbash ./tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Finetuning with Image-Labeled Data\nDESCRIPTION: This script starts the distributed training process to finetune the baseline model with image-labeled data. It leverages the specified configuration file and number of GPUs for training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nbash ./tools/dist_train.sh projects/Detic_new/detic_centernet2_r50_fpn_4x_lvis_in21k-lvis.py 8\n```\n\n----------------------------------------\n\nTITLE: Using Momentum Scheduling for Faster Convergence in MMDetection\nDESCRIPTION: This code snippet demonstrates how to use momentum scheduling in conjunction with learning rate scheduling to accelerate model convergence. It includes settings for `CosineAnnealingLR` and `CosineAnnealingMomentum`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    # 学习率调度器\n    # 在前 8 个 epoch, 学习率从 0 增大到 lr * 10\n    # 在接下来 12 个 epoch, 学习率从 lr * 10 减小到 lr * 1e-4\n    dict(\n        type='CosineAnnealingLR',\n        T_max=8,\n        eta_min=lr * 10,\n        begin=0,\n        end=8,\n        by_epoch=True,\n        convert_to_iter_based=True),\n    dict(\n        type='CosineAnnealingLR',\n        T_max=12,\n        eta_min=lr * 1e-4,\n        begin=8,\n        end=20,\n        by_epoch=True,\n        convert_to_iter_based=True),\n    # 动量调度器\n    # 在前 8 个 epoch, 动量从 0 增大到 0.85 / 0.95\n    # 在接下来 12 个 epoch, 学习率从 0.85 / 0.95 增大到 1\n    dict(\n        type='CosineAnnealingMomentum',\n        T_max=8,\n        eta_min=0.85 / 0.95,\n        begin=0,\n        end=8,\n        by_epoch=True,\n        convert_to_iter_based=True),\n    dict(\n        type='CosineAnnealingMomentum',\n        T_max=12,\n        eta_min=1,\n        begin=8,\n        end=20,\n        by_epoch=True,\n        convert_to_iter_based=True)\n]\n\n```\n\n----------------------------------------\n\nTITLE: Converting MMDetection model to TorchServe format\nDESCRIPTION: This command converts an MMDetection model to the TorchServe format. It takes a configuration file and a checkpoint file as input and generates a model archive.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython tools/deployment/mmdet2torchserve.py ${CONFIG_FILE} ${CHECKPOINT_FILE} \\\n--output-folder ${MODEL_STORE} \\\n--model-name ${MODEL_NAME}\n```\n\n----------------------------------------\n\nTITLE: Testing Grounding DINO Models (mmdetection)\nDESCRIPTION: These commands are used to test the performance of Grounding DINO models with different backbones (Swin-T and Swin-B) on the RefCOCO dataset. The script `dist_test.sh` is used for distributed testing, requiring a configuration file, model weights, and the number of GPUs.  It downloads the specified model weights from the given URL.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\n./tools/dist_test.sh configs/grounding_dino/refcoco/grounding_dino_swin-t_pretrain_zeroshot_refexp.py https://download.openmmlab.com/mmdetection/v3.0/grounding_dino/groundingdino_swint_ogc_mmdet-822d7e9d.pth 8\n./tools/dist_test.sh configs/grounding_dino/refcoco/grounding_dino_swin-b_pretrain_zeroshot_refexp.py https://download.openmmlab.com/mmdetection/v3.0/grounding_dino/groundingdino_swinb_cogcoor_mmdet-55949c9c.pth 8\n```\n\n----------------------------------------\n\nTITLE: Convert COCO 2017 to ODVG Format (Python)\nDESCRIPTION: This script converts the COCO 2017 training data to the ODVG (Object Detection with Visual Grounding) format.  It uses the `coco2odvg.py` script located in the `tools/dataset_converters/` directory, specifying the path to the COCO annotation file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/coco/annotations/instances_train2017.json -d coco\n```\n\n----------------------------------------\n\nTITLE: Inference using MMDeploy Model Converter API\nDESCRIPTION: This code snippet demonstrates how to perform inference using the MMDeploy Model Converter API. It takes the model configuration, deployment configuration, the path to the converted engine file, an input image, and the device to use for inference as inputs. It then returns the inference result.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy.apis import inference_model\n\nresult = inference_model(\n  model_cfg='${PATH_TO_MMDET}/configs/rtmdet/rtmdet-ins_s_8xb32-300e_coco.py',\n  deploy_cfg='${PATH_TO_MMDEPLOY}/configs/mmdet/instance-seg/instance-seg_rtmdet-ins_tensorrt_static-640x640.py',\n  backend_files=['work_dirs/rtmdet-ins/end2end.engine'],\n  img='demo/resources/det.jpg',\n  device='cuda:0')\n```\n\n----------------------------------------\n\nTITLE: Fusing, evaluating, and assessing single model predictions (Shell)\nDESCRIPTION: This command fuses predictions, evaluates the fused results, and also evaluates each individual model's performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ./faster-rcnn_r50-caffe_fpn_1x_coco.json \\\n       ./retinanet_r50-caffe_fpn_1x_coco.json \\\n       ./cascade-rcnn_r50-caffe_fpn_1x_coco.json \\\n       --annotation ./annotation.json \\\n       --weights 1 2 3 \\\n       --eval-single\n```\n\n----------------------------------------\n\nTITLE: Citation of Sparse R-CNN in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Sparse R-CNN paper. It can be used to properly credit the authors in academic publications or research reports.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sparse_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{peize2020sparse,\n  title   =  {{SparseR-CNN}: End-to-End Object Detection with Learnable Proposals},\n  author  =  {Peize Sun and Rufeng Zhang and Yi Jiang and Tao Kong and Chenfeng Xu and Wei Zhan and Masayoshi Tomizuka and Lei Li and Zehuan Yuan and Changhu Wang and Ping Luo},\n  journal =  {arXiv preprint arXiv:2011.12450},\n  year    =  {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing with Severities 0, 2, and 4\nDESCRIPTION: This shell command runs the `test_robustness.py` script to evaluate model performance with image corruptions at severity levels 0, 2, and 4. It uses a configuration file, checkpoint file, optional output file, and optional eval metrics. The `--severities 0 2 4` argument specifies the severity levels to be applied.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n# severities 0,2,4\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --severities 0 2 4\n```\n\n----------------------------------------\n\nTITLE: Dataset Configuration in MMDetection 2.x\nDESCRIPTION: This code demonstrates the dataset configuration format in MMDetection 2.x. It defines parameters such as the number of samples per GPU, number of workers per GPU, and the configuration for train, val, and test datasets including dataset type, annotation file paths, image prefix paths, and the data processing pipeline.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline))\n\n```\n\n----------------------------------------\n\nTITLE: Running Text Image Region Retrieval Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for text image region retrieval using the X-Decoder model. It specifies the image directory, configuration file, pre-trained weights, and the text prompt. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/coco configs/xdecoder-tiny_zeroshot_text-image-retrieval.py --weights ../../xdecoder_focalt_last_novg.pt --text 'pizza on the plate'\n```\n\n----------------------------------------\n\nTITLE: Modifying Training Strategy for Fine-tuning in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to modify the training strategy when fine-tuning a model on a new dataset, typically involving smaller learning rates and fewer training epochs. It configures the optimizer, learning rate scheduler, maximum epochs, and logging interval. It defines settings for the optimizer, learning rate scheduling, the maximum number of epochs, and the logging interval.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/finetune.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 优化器\n# batch size 为 8 时的 lr 配置\noptim_wrapper = dict(optimizer=dict(lr=0.01))\n\n# 学习率\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=8,\n        by_epoch=True,\n        milestones=[7],\n        gamma=0.1)\n]\n\n# 设置 max epoch\ntrain_cfg = dict(max_epochs=8)\n\n# 设置 log config\ndefault_hooks = dict(logger=dict(interval=100)),\n```\n\n----------------------------------------\n\nTITLE: Slurm Training Script\nDESCRIPTION: This snippet demonstrates how to launch training on a Slurm cluster using the `slurm_train.sh` script. It requires specifying the configuration file and the number of GPUs.  The first command trains with default save path, the second defines `my_work_dirs` as save directory. It is necessary to navigate to the `projects/RF100-Benchmark/` directory first for the command to work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nbash scripts/slurm_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8\n# Specify the save path\nbash scripts/slurm_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Generate COCO Panoptic Test Info\nDESCRIPTION: This Python script updates the category information in the testing image info. It addresses the missing `isthing` attribute in the 'image_info_test-dev2017.json' file by incorporating category information from the 'panoptic_val2017.json' file. This step is crucial for correct evaluation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/gen_coco_panoptic_test_info.py data/coco/annotations\n```\n\n----------------------------------------\n\nTITLE: Configure Faster R-CNN with Pre-trained FCOS (Python)\nDESCRIPTION: This code configures a Faster R-CNN model to use a pre-trained FCOS model as its RPN in MMDetection. It involves loading pre-trained weights, adjusting data pre-processing parameters, and modifying the backbone configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/single_stage_as_rpn.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    data_preprocessor=dict(\n        mean=[103.530, 116.280, 123.675],\n        std=[1.0, 1.0, 1.0],\n        bgr_to_rgb=False),\n    backbone=dict(\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='caffe',\n        init_cfg=None),  # the checkpoint in ``load_from`` contains the weights of backbone\n    neck=dict(\n        start_level=1,\n        add_extra_convs='on_output',  # 使用 P5\n        relu_before_extra_convs=True),\n    rpn_head=dict(\n        _delete_=True,  # 忽略未使用的旧设置\n        type='FCOSHead',\n        num_classes=1,  # 对于 rpn, num_classes = 1，如果 num_classes > 1，它将在 TwoStageDetector 中自动设置为1\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n    roi_head=dict(  # update featmap_strides due to the strides in neck\n        bbox_roi_extractor=dict(featmap_strides=[8, 16, 32, 64, 128])))\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/fcos_r50_caffe_fpn_gn-head_1x_coco-821213aa.pth'\n```\n\n----------------------------------------\n\nTITLE: Inheriting Multiple Configuration Files in MMDetection\nDESCRIPTION: This code snippet demonstrates how to inherit from multiple base configuration files in MMDetection. By assigning a list of file paths to the `_base_` variable, the current configuration inherits settings from all listed files, creating a modular configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/mask-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Auto Scaling Shell Command\nDESCRIPTION: This shell command demonstrates how to enable learning rate auto scaling during training using MMDetection. The `${CONFIG_FILE}` variable should be replaced with the path to the configuration file for the desired model. The `--auto-scale-lr` flag enables the auto scaling feature, which automatically adjusts the learning rate based on the number of GPUs and batch size.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    --auto-scale-lr \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Training with iSAID Dataset (Python)\nDESCRIPTION: This snippet demonstrates how to train a model using the iSAID dataset in MMDetection. It uses the `tools/train.py` script with a configuration file specific to the iSAID dataset, located in `projects/iSAID/configs`. The script will begin training the specified model using the dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/iSAID/README_zh-CN.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython tools/train.py projects/iSAID/configs/mask_rcnn_r50_fpn_1x_isaid.py\n```\n\n----------------------------------------\n\nTITLE: Install PyTorch (GPU)\nDESCRIPTION: Installs PyTorch and torchvision with CUDA support using conda. This command is for GPU-enabled platforms.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda install pytorch torchvision -c pytorch\n```\n\n----------------------------------------\n\nTITLE: GPU Accelerated Video Inference Example\nDESCRIPTION: This is an example of how to run the video inference demo script with GPU acceleration using specific video, configuration, and checkpoint files. It uses the `--nvdecode` flag to enable GPU decoding and specifies the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_gpuaccel_demo.py demo/demo.mp4 \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --nvdecode --out result.mp4\n```\n\n----------------------------------------\n\nTITLE: Download Coco Dataset (Shell)\nDESCRIPTION: This script downloads the COCO 2017 dataset, which is a common dataset for object detection tasks, and decompresses it automatically. It's a prerequisite step for setting up the semi-supervised object detection experiments described in the document.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/misc/download_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Testing Detic Models with mmdetection\nDESCRIPTION: This command is used to evaluate a Detic model using a specified configuration file and checkpoint file within the mmdetection framework.  The command invokes the `test.py` script, providing it with the paths to the configuration and checkpoint files as arguments.  Ensure that the `CONFIG_FILE` and `CHECKPOINT_FILE` variables are correctly set to the appropriate file paths.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\npython ./tools/test.py ${CONFIG_FILE} ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Objects365v1 to ODVG Conversion\nDESCRIPTION: This command converts the Objects365v1 dataset's JSON annotation file into the ODVG (Object Detection with Visual Grounding) format, which is required for training the MM-GDINO-T model. It uses the `coco2odvg.py` script from the `tools/dataset_converters` directory, specifying the input JSON file and the dataset name (`o365v1`) as arguments. The script outputs two new JSON files containing the converted annotations and a label map.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/objects365v1/objects365_train.json -d o365v1\n```\n\n----------------------------------------\n\nTITLE: Visualizing Output Dataset (Shell)\nDESCRIPTION: This script visualizes the output dataset, including the results of data augmentation.  It helps to review the final processed data with labels.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_dataset.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py  -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: YOLO Anchor Optimization with Differential Evolution - Shell\nDESCRIPTION: This script optimizes YOLO anchors using the differential evolution algorithm.  It requires specifying the configuration file, input shape (width and height), and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_40\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm differential_evolution --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for TOOD\nDESCRIPTION: This BibTeX entry provides the citation information for the TOOD paper, including the title, authors, conference (ICCV), and year of publication. It can be used to properly credit the authors when referencing TOOD in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/tood/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{feng2021tood,\n    title={TOOD: Task-aligned One-stage Object Detection},\n    author={Feng, Chengjian and Zhong, Yujie and Gao, Yu and Scott, Matthew R and Huang, Weilin},\n    booktitle={ICCV},\n    year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DetTTAModel and TTA Pipeline\nDESCRIPTION: This code configures the `tta_model` with `DetTTAModel` and a nested `tta_cfg` including Non-Maximum Suppression (NMS) parameters. It also defines the `tta_pipeline`, which includes loading the image, applying test-time augmentations like resizing and flipping, and packing the inputs using `PackDetInputs`. The pipeline uses both resizing and flipping transformations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ntta_model = dict(\n    type='DetTTAModel',\n    tta_cfg=dict(nms=dict(\n                   type='nms',\n                   iou_threshold=0.5),\n                   max_per_img=100))\n\ntta_pipeline = [\n    dict(type='LoadImageFromFile',\n        backend_args=None),\n    dict(\n        type='TestTimeAug',\n        transforms=[[            dict(type='Resize', scale=(1333, 800), keep_ratio=True)\n        ], [ # It uses 2 flipping transformations (flipping and not flipping).\n            dict(type='RandomFlip', prob=1.),\n            dict(type='RandomFlip', prob=0.)\n        ], [\n            dict(\n               type='PackDetInputs',\n               meta_keys=('img_id', 'img_path', 'ori_shape',\n                       'img_shape', 'scale_factor', 'flip',\n                       'flip_direction'))\n       ]])]\n```\n\n----------------------------------------\n\nTITLE: Updating Values of List/Tuples - Shell\nDESCRIPTION: This command updates the `mean` value, which is a list, inside `data_preprocessor` by specifying new values without any whitespace within the specified values. Note that no white space is allowed inside the specified value.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_config.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n--cfg-options model.data_preprocessor.mean=[0,0,0]\n```\n\n----------------------------------------\n\nTITLE: Browse Detection Dataset (browse_dataset.py)\nDESCRIPTION: This tool helps to browse a detection dataset visually, showing images and bounding box annotations, or save the images to a directory. It requires a configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_dataset.py ${CONFIG} [-h] [--skip-type ${SKIP_TYPE[SKIP_TYPE...]}] [--output-dir ${OUTPUT_DIR}] [--not-show] [--show-interval ${SHOW_INTERVAL}]\n```\n\n----------------------------------------\n\nTITLE: Update OpenMIM (Bash)\nDESCRIPTION: Updates the OpenMIM package to the latest version using pip.  Ensures access to the newest features and bug fixes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\npip install -U openmim\n```\n\n----------------------------------------\n\nTITLE: Visualize Grounding Dataset Output (browse_grounding_dataset.py)\nDESCRIPTION: This script visualizes the output of the grounding dataset, including data augmentation results. It overlays bounding boxes and labels on the augmented images and saves them to a specified output directory.  It requires the path to the configuration file and an output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_dataset.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py  -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: Referential Expression Comprehension - Shell\nDESCRIPTION: This shell command performs referential expression comprehension using MM Grounding DINO. It runs the `image_demo.py` script with the configuration and weights. The `--texts` parameter provides the referential expression ('red apple.' in this example), and `--tokens-positive -1` indicates that the model should automatically comprehend the expression without manual noun phrase extraction.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/apples.jpg \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'red apple.' \\\n        --tokens-positive -1\n```\n\n----------------------------------------\n\nTITLE: Initializing Mosaic Transformation in MMDetection (Python)\nDESCRIPTION: This code snippet shows the initialization of the `Mosaic` transformation, a data augmentation technique used in MMDetection. It defines the parameters for the transformation, including image scale, center ratio range, bounding box clip border, padding value, and probability. The `img_scale` parameter specifies the target width and height for the mosaic image.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/conventions.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@TRANSFORMS.register_module()\nclass Mosaic(BaseTransform):\n    def __init__(self,\n                img_scale: Tuple[int, int] = (640, 640),\n                center_ratio_range: Tuple[float, float] = (0.5, 1.5),\n                bbox_clip_border: bool = True,\n                pad_val: float = 114.0,\n                prob: float = 1.0) -> None:\n       ...\n\n       # img_scale order should be (width, height)\n       self.img_scale = img_scale\n\n    def transform(self, results: dict) -> dict:\n        ...\n\n        results['img'] = mosaic_img\n        # (height, width)\n        results['img_shape'] = mosaic_img.shape[:2]\n```\n\n----------------------------------------\n\nTITLE: Converting COCO Data to ODVG Format (GoldG)\nDESCRIPTION: This script converts the extracted COCO data into the ODVG format required for training. It takes the `final_mixed_train_only_coco.json` file as input and generates a new file in the ODVG format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/goldg2odvg.py data/coco/mdetr_annotations/final_mixed_train_only_coco.json\n```\n\n----------------------------------------\n\nTITLE: Webcam Inference Example Usage\nDESCRIPTION: This command provides an example of running the webcam inference demo script with a specific configuration file and checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython demo/webcam_demo.py \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth\n```\n\n----------------------------------------\n\nTITLE: Comparing TorchServe and PyTorch results (Shell)\nDESCRIPTION: This script compares the results from TorchServe and PyTorch, and visualizes the differences. It takes an image file, config file, checkpoint file, and model name as arguments, and supports options for inference address, device, score threshold, and work directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\npython tools/deployment/test_torchserver.py ${IMAGE_FILE} ${CONFIG_FILE} ${CHECKPOINT_FILE} ${MODEL_NAME}\n[--inference-addr ${INFERENCE_ADDR}] [--device ${DEVICE}] [--score-thr ${SCORE_THR}] [--work-dir ${WORK_DIR}]\n```\n\n----------------------------------------\n\nTITLE: Training Faster R-CNN for DeepSORT (Shell)\nDESCRIPTION: This command trains the Faster R-CNN detector on the mot17-half-train dataset using the specified configuration file. It utilizes the `tools/dist_train.sh` script for distributed training across 8 GPUs. The script requires the configuration file path as the first argument and the number of GPUs as the second argument.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepsort/README.md#_snippet_0\n\nLANGUAGE: Shell Script\nCODE:\n```\nbash tools/dist_train.sh configs/sort/faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Training Grounding DINO Model (Shell)\nDESCRIPTION: These scripts train the Grounding DINO model on the obj365v1 dataset and other datasets. It utilizes the dist_train.sh script for distributed training, assuming 8 GPUs are available. It expects the necessary configurations and datasets to be prepared beforehand.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n# Training on a single machine with 8 GPUs for obj365v1 dataset\n./tools/dist_train.sh configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py 8\n```\n\nLANGUAGE: shell\nCODE:\n```\n# Training on a single machine with 8 GPUs for datasets like obj365v1, goldg, grit, v3det, and other datasets is similar.\n./tools/dist_train.sh configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det.py 8\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Mask R-CNN with GC Block (r=4) and X-101-FPN\nDESCRIPTION: This config file provides the settings for a Cascade Mask R-CNN model that makes use of an X-101-FPN backbone. It includes a Global Context (GC) block with a reduction ratio of 4 inserted after the 1x1 convolution layers of the backbone's c3-c5 stages. SyncBN is applied. Training is performed for 1x epochs on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[config](./cascade-mask-rcnn_x101-32x4d-syncbn-r4-gcb-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Log Printing Interval Configuration (3.x)\nDESCRIPTION: This snippet configures the log printing interval in MMDetection 3.x using `LoggerHook` within `default_hooks`. The `interval` parameter specifies how often logs are printed (in iterations). The `LogProcessor` can be used to configure moving average window size.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    logger=dict(type='LoggerHook', interval=50))\n# Optional: set moving average window size\nlog_processor = dict(\n    type='LogProcessor', window_size=50)\n```\n\n----------------------------------------\n\nTITLE: Importing a Custom Hook in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to import a custom hook defined in `mmdet/engine/hooks/my_hook.py` by modifying the `mmdet/engine/hooks/__init__.py` file. This makes the hook discoverable by the registry.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom .my_hook import MyHook\n```\n\n----------------------------------------\n\nTITLE: Testing Model in MMDetection (Shell)\nDESCRIPTION: This command tests a model within the MMDetection framework using the specified configuration file and checkpoint. The configuration file defines the model architecture and training parameters, while the checkpoint contains the learned weights.  The script outputs evaluation metrics, such as Average Precision (AP).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py projects/DiffusionDet/configs/diffusiondet_r50_fpn_500-proposals_1-step_crop-ms-480-800-450k_coco.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: MOT Inference Example with Detector and ReID\nDESCRIPTION: This example demonstrates running the `mot_demo.py` script for MOT inference, loading the detector and ReID models separately using `--detector` and `--reid` flags.  It uses a SORT configuration file, a Faster R-CNN detector, and saves the output to mot.mp4.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_inference.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Example 1: do not specify --checkpoint to use --detector\npython demo/mot_demo.py \\\n    demo/demo_mot.mp4 \\\n    configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py \\\n    --detector \\\n    https://download.openmmlab.com/mmtracking/mot/faster_rcnn/faster-rcnn_r50_fpn_4e_mot17-half-64ee2ed4.pth \\\n    --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Evaluate Metric (eval_metric.py)\nDESCRIPTION: This snippet showcases how to evaluate a model's performance based on the evaluation metrics defined in the config file using `tools/analysis_tools/eval_metric.py`. It takes a config file and a pkl results file as arguments, along with other optional parameters for formatting and evaluation options.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/eval_metric.py ${CONFIG} ${PKL_RESULTS} [-h] [--format-only] [--eval ${EVAL[EVAL ...]}]\n                      [--cfg-options ${CFG_OPTIONS [CFG_OPTIONS ...]}]\n                      [--eval-options ${EVAL_OPTIONS [EVAL_OPTIONS ...]}]\n```\n\n----------------------------------------\n\nTITLE: Setting the dataset root\nDESCRIPTION: This shell command sets the dataset root using an environment variable. This allows the framework to locate the dataset without hardcoding paths in the config files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nexport MMDET_DATASETS=$data_root\n```\n\n----------------------------------------\n\nTITLE: Migrating RandomResize Configuration in MMDetection\nDESCRIPTION: Shows the change in the RandomResize transform configuration from v2.x to v3.x. The parameter name `img_scale` is changed to `scale`, and the transform type changed from 'Resize' to 'RandomResize'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='Resize',\n    img_scale=[\n        (1333, 640), (1333, 800)],\n    multiscale_mode='range',\n    keep_ratio=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='RandomResize',\n    scale=[\n        (1333, 640), (1333, 800)],\n    keep_ratio=True)\n```\n\n----------------------------------------\n\nTITLE: Updating Keys Inside a List of Configs - Shell\nDESCRIPTION: This command updates a specific element within a list of configurations. It targets the `type` field of the first element in the `pipeline` list under `test_dataloader.dataset`, changing it from `LoadImageFromFile` to `LoadImageFromWebcam`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_config.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n--cfg-options test_dataloader.dataset.pipeline.0.type=LoadImageFromWebcam\n```\n\n----------------------------------------\n\nTITLE: Inference with Custom Vocabularies\nDESCRIPTION: This script runs the image demo with customized vocabulary (class names provided directly), using the specified image, configuration, and model paths, along with an optional score threshold and palette.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython demo/image_demo.py \\\n  ${IMAGE_PATH} \\\n  ${CONFIG_PATH} \\\n  ${MODEL_PATH} \\\n  --texts 'headphone . webcam . paper . coffe.' \\\n  --pred-score-thr 0.3 \\\n  --palette 'random'\n```\n\n----------------------------------------\n\nTITLE: Single GPU Model Testing with MMDetection\nDESCRIPTION: This shell command executes the testing script `tools/test.py` for a given model configuration and checkpoint file. It allows specifying an output file for the results and displaying the detection results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Using Custom Imports for PAFPN in MMDetection Config\nDESCRIPTION: Illustrates using `custom_imports` in the config file to import the PAFPN neck module. This avoids direct modification of the original MMDetection code.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(\n    imports=['mmdet.models.necks.pafpn'],\n    allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing TrackLocalVisualizer (Python)\nDESCRIPTION: This Python snippet demonstrates how to initialize the `TrackLocalVisualizer` class, which is used for visualizing MOT and VIS tasks. It shows the basic structure for creating an instance of the visualizer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_visualization.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvisualizer = dict(type='TrackLocalVisualizer')\n```\n\n----------------------------------------\n\nTITLE: Closed-Set Object Detection with Chunked Prediction for LVIS\nDESCRIPTION: This shell command performs closed-set object detection on the LVIS dataset using MM Grounding DINO with chunked prediction. Due to the large number of LVIS categories, the `--chunked-size` parameter is used to divide the categories into smaller chunks to avoid exceeding the token limit. The `--palette random` argument enables random color palettes for visualization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/animals.png \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts '$: lvis'  --chunked-size 70 \\\n        --palette random\n```\n\n----------------------------------------\n\nTITLE: Training on CPU with MMDetection\nDESCRIPTION: This snippet demonstrates how to train a model on the CPU using MMDetection. It first disables GPU visibility by setting the CUDA_VISIBLE_DEVICES environment variable to -1, and then executes the training script, specifying the configuration file and any optional arguments. This is useful for development or when GPUs are unavailable.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_0\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/train.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Removing MMDetection from PYTHONPATH\nDESCRIPTION: This shell script line, typically found in training or testing scripts, adds the parent directory of the script to the PYTHONPATH, ensuring that the local MMDetection package is used. Removing this line forces the script to use the MMDetection package installed in the environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nPYTHONPATH=\"$(dirname $0)/..\":$PYTHONPATH\n```\n\n----------------------------------------\n\nTITLE: Example FPS benchmarking\nDESCRIPTION: An example command for benchmarking a Faster R-CNN model's FPS. Requires a config and checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node=1 --master_port=29500 tools/analysis_tools/benchmark.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n       --launcher pytorch\n```\n\n----------------------------------------\n\nTITLE: Configure FCOSHead as RPNHead in Faster R-CNN (Python)\nDESCRIPTION: This code snippet shows how to configure the FCOSHead as the RPNHead in a Faster R-CNN model within the MMDetection framework. It involves replacing the rpn_head settings with the bbox_head settings from FCOS, setting the strides, and adjusting the learning rate schedule. The loss functions for classification, bounding box regression, and centerness are also defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/single_stage_as_rpn.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    # 从 configs/fcos/fcos_r50-caffe_fpn_gn-head_1x_coco.py 复制\n    neck=dict(\n        start_level=1,\n        add_extra_convs='on_output',  # 使用 P5\n        relu_before_extra_convs=True),\n    rpn_head=dict(\n        _delete_=True,  # 忽略未使用的旧设置\n        type='FCOSHead',\n        num_classes=1,  # 对于 rpn, num_classes = 1，如果 num_classes > 1，它将在 TwoStageDetector 中自动设置为1\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n    roi_head=dict(  # featmap_strides 的更新取决于于颈部的步伐\n        bbox_roi_extractor=dict(featmap_strides=[8, 16, 32, 64, 128])))\n# 学习率\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0,\n        end=1000),  # 慢慢增加 lr，否则损失变成 NAN\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=12,\n        by_epoch=True,\n        milestones=[8, 11],\n        gamma=0.1)\n]\n```\n\n----------------------------------------\n\nTITLE: Configure MaskRCNN Model in MMDetection (Python)\nDESCRIPTION: This snippet defines the configuration for a Mask R-CNN model in MMDetection. It includes configurations for the data preprocessor, backbone (ResNet), neck (FPN), RPN head, and RoI head (including box and mask heads). Each component's parameters are specified, such as layer types, input/output channels, loss functions, and anchor generation settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    type='MaskRCNN',  # The name of detector\n    data_preprocessor=dict(  # The config of data preprocessor, usually includes image normalization and padding\n        type='DetDataPreprocessor',  # The type of the data preprocessor, refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.data_preprocessors.DetDataPreprocessor\n        mean=[123.675, 116.28, 103.53],  # Mean values used to pre-training the pre-trained backbone models, ordered in R, G, B\n        std=[58.395, 57.12, 57.375],  # Standard variance used to pre-training the pre-trained backbone models, ordered in R, G, B\n        bgr_to_rgb=True,  # whether to convert image from BGR to RGB\n        pad_mask=True,  # whether to pad instance masks\n        pad_size_divisor=32),  # The size of padded image should be divisible by ``pad_size_divisor``\n    backbone=dict(  # The config of backbone\n        type='ResNet',  # The type of backbone network. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.backbones.ResNet\n        depth=50,  # The depth of backbone, usually it is 50 or 101 for ResNet and ResNext backbones.\n        num_stages=4,  # Number of stages of the backbone.\n        out_indices=(0, 1, 2, 3),  # The index of output feature maps produced in each stage\n        frozen_stages=1,  # The weights in the first stage are frozen\n        norm_cfg=dict(  # The config of normalization layers.\n            type='BN',  # Type of norm layer, usually it is BN or GN\n            requires_grad=True),  # Whether to train the gamma and beta in BN\n        norm_eval=True,  # Whether to freeze the statistics in BN\n        style='pytorch', # The style of backbone, 'pytorch' means that stride 2 layers are in 3x3 Conv, 'caffe' means stride 2 layers are in 1x1 Convs.\n    \tinit_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),  # The ImageNet pretrained backbone to be loaded\n    neck=dict(\n        type='FPN',  # The neck of detector is FPN. We also support 'NASFPN', 'PAFPN', etc. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.necks.FPN for more details.\n        in_channels=[256, 512, 1024, 2048],  # The input channels, this is consistent with the output channels of backbone\n        out_channels=256,  # The output channels of each level of the pyramid feature map\n        num_outs=5),  # The number of output scales\n    rpn_head=dict(\n        type='RPNHead',  # The type of RPN head is 'RPNHead', we also support 'GARPNHead', etc. Refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.dense_heads.RPNHead for more details.\n        in_channels=256,  # The input channels of each input feature map, this is consistent with the output channels of neck\n        feat_channels=256,  # Feature channels of convolutional layers in the head.\n        anchor_generator=dict(  # The config of anchor generator\n            type='AnchorGenerator',  # Most of methods use AnchorGenerator, SSD Detectors uses `SSDAnchorGenerator`. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/prior_generators/anchor_generator.py#L18 for more details\n            scales=[8],  # Basic scale of the anchor, the area of the anchor in one position of a feature map will be scale * base_sizes\n            ratios=[0.5, 1.0, 2.0],  # The ratio between height and width.\n            strides=[4, 8, 16, 32, 64]),  # The strides of the anchor generator. This is consistent with the FPN feature strides. The strides will be taken as base_sizes if base_sizes is not set.\n        bbox_coder=dict(  # Config of box coder to encode and decode the boxes during training and testing\n            type='DeltaXYWHBBoxCoder',  # Type of box coder. 'DeltaXYWHBBoxCoder' is applied for most of the methods. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/task_modules/coders/delta_xywh_bbox_coder.py#L13 for more details.\n            target_means=[0.0, 0.0, 0.0, 0.0],  # The target means used to encode and decode boxes\n            target_stds=[1.0, 1.0, 1.0, 1.0]),  # The standard variance used to encode and decode boxes\n        loss_cls=dict(  # Config of loss function for the classification branch\n            type='CrossEntropyLoss',  # Type of loss for classification branch, we also support FocalLoss etc. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/losses/cross_entropy_loss.py#L201 for more details\n            use_sigmoid=True,  # RPN usually performs two-class classification, so it usually uses the sigmoid function.\n            loss_weight=1.0),  # Loss weight of the classification branch.\n        loss_bbox=dict(  # Config of loss function for the regression branch.\n            type='L1Loss',  # Type of loss, we also support many IoU Losses and smooth L1-loss, etc. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/losses/smooth_l1_loss.py#L56 for implementation.\n            loss_weight=1.0)),  # Loss weight of the regression branch.\n    roi_head=dict(  # RoIHead encapsulates the second stage of two-stage/cascade detectors.\n        type='StandardRoIHead',\n        bbox_roi_extractor=dict(  # RoI feature extractor for bbox regression.\n            type='SingleRoIExtractor',  # Type of the RoI feature extractor, most of methods uses SingleRoIExtractor. Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/roi_heads/roi_extractors/single_level_roi_extractor.py#L13 for details.\n            roi_layer=dict(  # Config of RoI Layer\n                type='RoIAlign',  # Type of RoI Layer, DeformRoIPoolingPack and ModulatedDeformRoIPoolingPack are also supported. Refer to https://mmcv.readthedocs.io/en/latest/api.html#mmcv.ops.RoIAlign for details.\n                output_size=7,  # The output size of feature maps.\n                sampling_ratio=0),  # Sampling ratio when extracting the RoI features. 0 means adaptive ratio.\n            out_channels=256,  # output channels of the extracted feature.\n            featmap_strides=[4, 8, 16, 32]),  # Strides of multi-scale feature maps. It should be consistent with the architecture of the backbone.\n        bbox_head=dict(  # Config of box head in the RoIHead.\n            type='Shared2FCBBoxHead',  # Type of the bbox head, Refer to https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py#L220 for implementation details.\n            in_channels=256,  # Input channels for bbox head. This is consistent with the out_channels in roi_extractor\n            fc_out_channels=1024,  # Output feature channels of FC layers.\n            roi_feat_size=7,  # Size of RoI features\n            num_classes=80,  # Number of classes for classification\n            bbox_coder=dict(  # Box coder used in the second stage.\n                type='DeltaXYWHBBoxCoder',  # Type of box coder. 'DeltaXYWHBBoxCoder' is applied for most of the methods.\n                target_means=[0.0, 0.0, 0.0, 0.0],  # Means used to encode and decode box\n                target_stds=[0.1, 0.1, 0.2, 0.2]),  # Standard variance for encoding and decoding. It is smaller since the boxes are more accurate. [0.1, 0.1, 0.2, 0.2] is a conventional setting.\n            reg_class_agnostic=False,  # Whether the regression is class agnostic.\n            loss_cls=dict(  # Config of loss function for the classification branch\n                type='CrossEntropyLoss',  # Type of loss for classification branch, we also support FocalLoss etc.\n                use_sigmoid=False,  # Whether to use sigmoid.\n                loss_weight=1.0),  # Loss weight of the classification branch.\n            loss_bbox=dict(  # Config of loss function for the regression branch.\n                type='L1Loss',  # Type of loss, we also support many IoU Losses and smooth L1-loss, etc.\n                loss_weight=1.0)),  # Loss weight of the regression branch.\n        mask_roi_extractor=dict(  # RoI feature extractor for mask generation.\n            type='SingleRoIExtractor',  # Type of the RoI feature extractor, most of methods uses SingleRoIExtractor.\n            roi_layer=dict(  # Config of RoI Layer that extracts features for instance segmentation\n                type='RoIAlign',  # Type of RoI Layer, DeformRoIPoolingPack and ModulatedDeformRoIPoolingPack are also supported\n                output_size=14,  # The output size of feature maps.\n                sampling_ratio=0),  # Sampling ratio when extracting the RoI features.\n            out_channels=256,  # Output channels of the extracted feature.\n            featmap_strides=[4, 8, 16, 32]),  # Strides of multi-scale feature maps.\n        mask_head=dict(  # Mask prediction head\n            type='FCNMaskHead',  # Type of mask head, refer to https://mmdetection.readthedocs.io/en/latest/api.html#mmdet.models.roi_heads.FCNMaskHead for implementation details.\n            num_convs=4,  # Number of convolutional layers in mask head.\n            in_channels=256,  # Input channels, should be consistent with the output channels of mask roi extractor.\n            conv_out_channels=256,  # Output channels of the convolutional layer.\n            num_classes=80,  # Number of class to be segmented.\n            loss_mask=dict(  # Config of loss function for the mask branch.\n                type='CrossEntropyLoss',  # Type of loss used for segmentation\n                use_mask=True,  # Whether to only train the mask in the correct class.\n                loss_weight=1.0))),  # Loss weight of mask branch.\n\n```\n\n----------------------------------------\n\nTITLE: Running Open Vocabulary Panoptic Segmentation Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for open vocabulary panoptic segmentation using the X-Decoder model. It specifies the image, configuration file, pre-trained weights, and text prompts for both objects and stuff. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/street.jpg configs/xdecoder-tiny_zeroshot_open-vocab-panoptic_coco.py --weights ../../xdecoder_focalt_last_novg.pt  --text car.person --stuff-text tree.sky\n```\n\n----------------------------------------\n\nTITLE: Split Coco Dataset for Semi-Supervised Learning (Shell)\nDESCRIPTION: This script splits the COCO 2017 training dataset into labeled and unlabeled subsets according to specified ratios (1%, 2%, 5%, 10%). It also performs cross-validation by repeating the splitting process five times. The resulting annotation files are named according to the fold and percentage of labeled data.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/misc/split_coco.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing Evaluation Results with Grounding DINO (Shell)\nDESCRIPTION: This script runs the test.py tool to generate visualization results for the evaluation dataset. It saves the results to the specified directory.  The user should replace the configuration file and path to the model's weights appropriately. Requires mmdetection environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/refcoco/grounding_dino_swin-t_pretrain_zeroshot_refexp \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth --work-dir refcoco_result --show-dir save_path\n```\n\n----------------------------------------\n\nTITLE: Converting RTMDet-Ins to TensorRT using Shell\nDESCRIPTION: This shell script demonstrates how to convert an RTMDet-Ins model to a TensorRT engine using MMDeploy's `tools/deploy.py`. It downloads the RTMDet-ins checkpoint, then runs the conversion script, specifying the RTMDet-Ins deploy config.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\n# go to the mmdeploy folder\ncd ${PATH_TO_MMDEPLOY}\n\n# download RTMDet-s checkpoint\nwget -P checkpoint https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet-ins_s_8xb32-300e_coco/rtmdet-ins_s_8xb32-300e_coco_20221121_212604-fdc5d7ec.pth\n```\n\n----------------------------------------\n\nTITLE: Modifying Normalization Configuration in MMDetection\nDESCRIPTION: This snippet shows how to consistently modify the normalization configuration (e.g., switching from SyncBN to BN) across different parts of the model in MMDetection by defining a `norm_cfg` variable and reusing it in the backbone and neck configurations. This ensures consistent normalization settings throughout the model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\nnorm_cfg = dict(type='BN', requires_grad=True)\nmodel = dict(\n    backbone=dict(norm_cfg=norm_cfg),\n    neck=dict(norm_cfg=norm_cfg),\n    ...)\n```\n\n----------------------------------------\n\nTITLE: Plot Classification and Regression Loss (analyze_logs.py)\nDESCRIPTION: This example plots both classification and regression losses from a training log and saves the figure to a PDF file. The `log.json` file is analyzed, and 'loss_cls' and 'loss_bbox' keys are plotted. The output is saved as 'losses.pdf'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls loss_bbox --out losses.pdf\n```\n\n----------------------------------------\n\nTITLE: Configuring Iteration-Based Training Loop with Dynamic Intervals in MMDetection\nDESCRIPTION: This snippet shows how to configure an iteration-based training loop with dynamic validation intervals in MMDetection using `IterBasedTrainLoop`, specifying the maximum iterations, validation interval, and dynamic intervals for validation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ninterval = 5000\nmax_iters = 368750\ndynamic_intervals = [(max_iters // interval * interval + 1, max_iters)]\ntrain_cfg = dict(\n    type='IterBasedTrainLoop',\n    max_iters=max_iters,\n    val_interval=interval,\n    dynamic_intervals=dynamic_intervals)\n\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Root Path\nDESCRIPTION: Sets the dataset root path using an environment variable, allowing the configuration files to remain unchanged. This avoids hardcoding the dataset path within the configurations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport MMDET_DATASETS=$data_root\n```\n\n----------------------------------------\n\nTITLE: Labeled Dataset Configuration (Python)\nDESCRIPTION: This Python code configures the labeled dataset for semi-supervised object detection. It specifies the dataset type, root directory, annotation file, and data prefix, as well as filtering configurations and the data pipeline defined earlier. The 'filter_empty_gt' option removes images with no ground truth bounding boxes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nlabeled_dataset = dict(\n    type=dataset_type,\n    data_root=data_root,\n    ann_file='annotations/instances_train2017.json',\n    data_prefix=dict(img='train2017/'),\n    filter_cfg=dict(filter_empty_gt=True, min_size=32),\n    pipeline=sup_pipeline)\n```\n\n----------------------------------------\n\nTITLE: Closed-Set Object Detection Inference - Shell\nDESCRIPTION: This shell command runs the `image_demo.py` script for closed-set object detection using the MM Grounding DINO model on the specified image. It uses a configuration file, weights file, and specifies the dataset to use for detection (coco in this example) via the `--texts` parameter. The detected objects and their bounding boxes are then saved to an output image.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/animals.png \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts '$: coco'\n```\n\n----------------------------------------\n\nTITLE: Using Mosaic Augmentation in Training\nDESCRIPTION: This snippet details how to integrate Mosaic augmentation into the training pipeline, specifically with the Faster R-CNN algorithm. It involves modifying the `train_pipeline` and `train_dataset` configurations. The `train_pipeline` includes the `Mosaic` and `RandomAffine` transformations, while the `train_dataset` uses `MultiImageMixDataset` with a pipeline containing `LoadImageFromFile` and `LoadAnnotations`. It's important to use `MultiImageMixDataset` when using `Mosaic`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/how_to.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Open configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py directly and add the following fields\ndata_root = 'data/coco/'\ndataset_type = 'CocoDataset'\nimg_scale=(1333, 800)\n\ntrain_pipeline = [\n    dict(type='Mosaic', img_scale=img_scale, pad_val=114.0),\n    dict(\n        type='RandomAffine',\n        scaling_ratio_range=(0.1, 2),\n        border=(-img_scale[0] // 2, -img_scale[1] // 2)), # The image will be enlarged by 4 times after Mosaic processing,so we use affine transformation to restore the image size.\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs')\n]\n\ntrain_dataset = dict(\n    _delete_ = True, # remove unnecessary Settings\n    type='MultiImageMixDataset',\n    dataset=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(type='LoadAnnotations', with_bbox=True)\n        ],\n        filter_empty_gt=False,\n    ),\n    pipeline=train_pipeline\n    )\n\ndata = dict(\n    train=train_dataset\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Download NLTK Weights - Python\nDESCRIPTION: This Python script downloads the 'punkt' and 'averaged_perceptron_tagger' models from NLTK and saves them to the `~/nltk_data` directory. It imports the nltk library and utilizes the `nltk.download()` function to download the specified resources to the designated location. This ensures that the necessary models are available for noun phrase extraction, even without an internet connection during runtime.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nnltk.download('punkt', download_dir='~/nltk_data')\nnltk.download('averaged_perceptron_tagger', download_dir='~/nltk_data')\n```\n\n----------------------------------------\n\nTITLE: Configuring ResNet Style in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to configure the stride of convolutional layers within the Bottleneck module of a ResNet based on the specified style (`pytorch` or `caffe`). The `style` parameter determines which convolutional layer in the Bottleneck module will have a stride of 2.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nif self.style == 'pytorch':\n        self.conv1_stride = 1\n        self.conv2_stride = stride\n  else:\n        self.conv1_stride = stride\n        self.conv2_stride = 1\n```\n\n----------------------------------------\n\nTITLE: CPU Model Testing with MMDetection\nDESCRIPTION: This shell command disables GPU usage by setting the `CUDA_VISIBLE_DEVICES` environment variable to -1 and then runs the single GPU testing script. This enables testing the model on the CPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=-1\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Generate Confusion Matrix (confusion_matrix.py)\nDESCRIPTION: This describes how to generate a confusion matrix to analyze the detection results using `tools/analysis_tools/confusion_matrix.py`. First, run `tools/test.py` to save the detection results to a `.pkl` file. Then, run the `confusion_matrix.py` script, providing the config file, the path to the `.pkl` results, and the save directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_40\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/confusion_matrix.py ${CONFIG}  ${DETECTION_RESULTS}  ${SAVE_DIR} --show\n```\n\n----------------------------------------\n\nTITLE: Configuring Mask R-CNN with GC Block and X-101-FPN in MMDetection\nDESCRIPTION: This config file specifies a Mask R-CNN model that uses an X-101-FPN backbone. It incorporates a Global Context (GC) block with a reduction ratio of 16 inserted after the 1x1 convolution layers of the backbone's c3-c5 stages. It also uses SyncBN. The model is trained for 1x epochs on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[config](./mask-rcnn_x101-32x4d-syncbn-gcb-r16-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Converting RTMDet to TensorRT using Shell\nDESCRIPTION: This shell script demonstrates how to convert an RTMDet model to a TensorRT engine using MMDeploy's `tools/deploy.py`. It downloads the RTMDet-s checkpoint, then runs the conversion script, specifying the deployment configuration, model configuration, checkpoint path, input image, and working directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\n# go to the mmdeploy folder\ncd ${PATH_TO_MMDEPLOY}\n\n# download RTMDet-s checkpoint\nwget -P checkpoint https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet_s_8xb32-300e_coco/rtmdet_s_8xb32-300e_coco_20220905_161602-387a891e.pth\n\n# run the command to start model conversion\npython tools/deploy.py \\\n  configs/mmdet/detection/detection_tensorrt_static-640x640.py \\\n  ${PATH_TO_MMDET}/configs/rtmdet/rtmdet_s_8xb32-300e_coco.py \\\n  checkpoint/rtmdet_s_8xb32-300e_coco_20220905_161602-387a891e.pth \\\n  demo/resources/det.jpg \\\n  --work-dir ./work_dirs/rtmdet \\\n  --device cuda:0 \\\n  --show\n```\n\n----------------------------------------\n\nTITLE: Slurm Training with Port Specification via Options (Shell)\nDESCRIPTION: These shell commands demonstrate how to launch multiple training tasks using Slurm, specifying the communication port through command-line options. This approach avoids modifying the original configuration files. `CUDA_VISIBLE_DEVICES` assigns GPUs to each task, and `--cfg-options 'dist_params.port=...'` sets the port.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR} --cfg-options 'dist_params.port=29500'\n```\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR} --cfg-options 'dist_params.port=29501'\n```\n\n----------------------------------------\n\nTITLE: Implementing DoubleHeadRoIHead by Inheriting from StandardRoIHead\nDESCRIPTION: This snippet defines the DoubleHeadRoIHead, which inherits from StandardRoIHead.  It overrides the `_bbox_forward` method to implement the specific logic of the Double Head R-CNN. The `reg_roi_scale_factor` parameter controls the scaling of RoIs for regression feature extraction.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple\n\nfrom torch import Tensor\n\nfrom mmdet.registry import MODELS\nfrom .standard_roi_head import StandardRoIHead\n\n\n@MODELS.register_module()\nclass DoubleHeadRoIHead(StandardRoIHead):\n    \"\"\"RoI head for `Double Head RCNN <https://arxiv.org/abs/1904.06493>`_.\n\n    Args:\n        reg_roi_scale_factor (float): The scale factor to extend the rois\n            used to extract the regression features.\n    \"\"\"\n\n    def __init__(self, reg_roi_scale_factor: float, **kwargs):\n        super().__init__(**kwargs)\n        self.reg_roi_scale_factor = reg_roi_scale_factor\n\n    def _bbox_forward(self, x: Tuple[Tensor], rois: Tensor) -> dict:\n        \"\"\"Box head forward function used in both training and testing.\n\n        Args:\n            x (tuple[Tensor]): List of multi-level img features.\n            rois (Tensor): RoIs with the shape (n, 5) where the first\n                column indicates batch id of each RoI.\n\n        Returns:\n             dict[str, Tensor]: Usually returns a dictionary with keys:\n\n                - `cls_score` (Tensor): Classification scores.\n                - `bbox_pred` (Tensor): Box energies / deltas.\n                - `bbox_feats` (Tensor): Extract bbox RoI features.\n        \"\"\"\n        bbox_cls_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        bbox_reg_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs],\n            rois,\n            roi_scale_factor=self.reg_roi_scale_factor)\n        if self.with_shared_head:\n            bbox_cls_feats = self.shared_head(bbox_cls_feats)\n            bbox_reg_feats = self.shared_head(bbox_reg_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score,\n            bbox_pred=bbox_pred,\n            bbox_feats=bbox_cls_feats)\n        return bbox_results\n```\n\n----------------------------------------\n\nTITLE: Testing on Single GPU with MMDetection\nDESCRIPTION: This snippet shows how to test a model on a single GPU using MMDetection. It executes the `tools/test_tracking.py` script with the specified configuration file and optional arguments. The CUDA_VISIBLE_DEVICES environment variable can be used to select a specific GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_12\n\nLANGUAGE: shell 脚本\nCODE:\n```\npython tools/test_tracking.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: COCO Occluded/Separated Mask Recall - Shell\nDESCRIPTION: Calculates the COCO occluded and separated mask recall using a dumped prediction file (.pkl).  Requires the path to the results file and an output file for saving the results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_44\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/coco_occluded_separated_recall.py results.pkl --out occluded_separated_recall.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Mask R-CNN with GC Block (r=16) and X-101-FPN\nDESCRIPTION: This config file details a Cascade Mask R-CNN model based on an X-101-FPN backbone. It incorporates a Global Context (GC) block with a reduction ratio of 16 inserted after the 1x1 convolution layers of the backbone's c3-c5 stages. SyncBN is used. It is trained for 1x epochs on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[config](./cascade-mask-rcnn_x101-32x4d-syncbn-r16-gcb-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Severity 1\nDESCRIPTION: This command runs a robustness test with a specific corruption severity (severity 1).`${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file. `${EVAL_METRICS}` is the metric to evaluate.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --severities 1\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Directory Structure\nDESCRIPTION: This code snippet shows the directory structure required for the COCO dataset to be used with Mask2Former. It outlines the location of annotation files (instances and panoptic) as well as the image directories for training, validation and testing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask2former/README.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n|   |   |   ├── instances_train2017.json\n|   |   |   ├── instances_val2017.json\n│   │   │   ├── panoptic_train2017.json\n│   │   │   ├── panoptic_train2017\n│   │   │   ├── panoptic_val2017.json\n│   │   │   ├── panoptic_val2017\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n```\n\n----------------------------------------\n\nTITLE: Inferencing with SDK Python API\nDESCRIPTION: This Python code demonstrates how to use the MMDeploy SDK's Python API for object detection inference. It loads a model, performs inference on an input image, filters results based on a score threshold, and draws bounding boxes on the image.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom mmdeploy_python import Detector\nimport cv2\n\nimg = cv2.imread('demo/resources/det.jpg')\n\n# create a detector\ndetector = Detector(model_path='work_dirs/rtmdet-sdk', device_name='cuda', device_id=0)\n# run the inference\nbboxes, labels, _ = detector(img)\n# Filter the result according to threshold\nindices = [i for i in range(len(bboxes))]\nfor index, bbox, label_id in zip(indices, bboxes, labels):\n  [left, top, right, bottom], score = bbox[0:4].astype(int),  bbox[4]\n  if score < 0.3:\n      continue\n  # draw bbox\n  cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0))\n\ncv2.imwrite('output_detection.png', img)\n```\n\n----------------------------------------\n\nTITLE: Inference with Directory Path using DetInferencer\nDESCRIPTION: This snippet demonstrates how to perform object detection inference using the DetInferencer with a directory path as input. All images within the specified directory will be processed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ninferencer('path/to/your_imgs/')\n```\n\n----------------------------------------\n\nTITLE: Testing Semantic Segmentation on COCO2017 with X-Decoder\nDESCRIPTION: This command tests the X-Decoder model for semantic segmentation on the COCO2017 dataset. It uses the `dist_test.sh` script for distributed testing and specifies the configuration file, pre-trained weights, and number of GPUs. `model.test_cfg.use_thr_for_mc=False` disables thresholding.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-semseg_coco.py xdecoder_focalt_last_novg.pt 8 --cfg-options model.test_cfg.use_thr_for_mc=False\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with a List of Inputs\nDESCRIPTION: This snippet showcases using a list containing image paths to run inference. This allows the inferencer to process multiple images in a single call.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ninferencer(['tests/data/color.jpg', 'tests/data/gray.jpg'])\n# You can even mix the types\ninferencer(['tests/data/color.jpg', array])\n```\n\n----------------------------------------\n\nTITLE: Citation of ATSS Algorithm in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the ATSS (Adaptive Training Sample Selection) algorithm.  It specifies the title, authors, journal, and year for referencing the ATSS paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/atss/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhang2019bridging,\n  title   =  {Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection},\n  author  =  {Zhang, Shifeng and Chi, Cheng and Yao, Yongqiang and Lei, Zhen and Li, Stan Z.},\n  journal =  {arXiv preprint arXiv:1912.02424},\n  year    =  {2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing with Custom Corruptions\nDESCRIPTION: This shell command demonstrates testing with a customized set of image corruptions: gaussian noise, zoom blur, and snow.  It runs the `test_robustness.py` script with specified config and checkpoint files, with optional output and evaluation. The `--corruptions` argument lists the specific corruptions to apply, separated by spaces.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# gaussian noise, zoom blur and snow\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --corruptions gaussian_noise zoom_blur snow\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with an Image Path\nDESCRIPTION: This code initializes DetInferencer on GPU 0, then performs inference using the image specified by path 'demo/demo.jpg'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cuda:0')\ninferencer('demo/demo.jpg')\n```\n\n----------------------------------------\n\nTITLE: Enabling FP16 mode in TensorRT Configuration using Python\nDESCRIPTION: This Python snippet shows how to enable FP16 mode in the TensorRT backend configuration within an MMDeploy deployment config. Enabling FP16 can improve inference speed, trading off some precision.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# in MMDeploy config\nbackend_config = dict(\n    type='tensorrt',\n    common_config=dict(\n        fp16_mode=True  # enable fp16\n    ))\n```\n\n----------------------------------------\n\nTITLE: Training Command (Multi-GPU) - Bash\nDESCRIPTION: This command trains the dummy ResNet model using multiple GPUs. It utilizes `torch.distributed.launch` to manage distributed training across multiple nodes and processes.  `NUM_GPUS` should be set to the number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_project/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=${NUM_GPUS} --master_port=29506 --master_addr=\"127.0.0.1\" tools/train.py projects/example_project/configs/faster-rcnn_dummy-resnet_fpn_1x_coco.py\n```\n\n----------------------------------------\n\nTITLE: Example: Test DeepSort on single node multi-GPU\nDESCRIPTION: This snippet provides an example of testing the DeepSort model on a single node using 8 GPUs with the dist_test_tracking.sh script, a specific configuration file, and specifying both the detector and reid checkpoint files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_15\n\nLANGUAGE: shell script\nCODE:\n```\nbash ./tools/dist_test_tracking.sh configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8 --detector ${CHECKPOINT_FILE} --reid ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Training Faster R-CNN for SORT (Shell Script)\nDESCRIPTION: This shell script command trains a Faster R-CNN detector on the `mot17-half-train` dataset.  It utilizes the `dist_train.sh` script from MMDetection. The configuration file specifies the model architecture and training parameters. It's recommended to use 8 GPUs for training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sort/README.md#_snippet_1\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_train.sh configs/sort/faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom CheckInvalidLossHook\nDESCRIPTION: This code snippet shows how to implement a custom hook, CheckInvalidLossHook, that checks for NaN loss values during training. It inherits from the Hook class from MMEngine, registers the custom hook using `@HOOKS.register_module()`, and overrides the `after_train_iter` method to check the loss value every `interval` iterations. Requires `torch`, `mmengine.hooks`, `mmengine.runner`, and `mmdet.registry`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_hooks.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nfrom mmengine.hooks import Hook\nfrom mmengine.runner import Runner\n\nfrom mmdet.registry import HOOKS\n\n\n@HOOKS.register_module()\nclass CheckInvalidLossHook(Hook):\n    \"\"\"Check invalid loss hook.\n\n    This hook will regularly check whether the loss is valid\n    during training.\n\n    Args:\n        interval (int): Checking interval (every k iterations).\n            Default: 50.\n    \"\"\"\n\n    def __init__(self, interval: int = 50) -> None:\n        self.interval = interval\n\n    def after_train_iter(self,\n                         runner: Runner,\n                         batch_idx: int,\n                         data_batch: Optional[dict] = None,\n                         outputs: Optional[dict] = None) -> None:\n        \"\"\"Regularly check whether the loss is valid every n iterations.\n\n        Args:\n            runner (:obj:`Runner`): The runner of the training process.\n            batch_idx (int): The index of the current batch in the train loop.\n            data_batch (dict, Optional): Data from dataloader.\n                Defaults to None.\n            outputs (dict, Optional): Outputs from model. Defaults to None.\n        \"\"\"\n        if self.every_n_train_iters(runner, self.interval):\n            assert torch.isfinite(outputs['loss']),\n                runner.logger.info('loss become infinite or NaN!')\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Multi-GPU Testing for Class-wise Metrics\nDESCRIPTION: This shell script tests the Mask R-CNN model on the COCO dataset using 8 GPUs to calculate class-wise bounding box and mask mAP. It generates JSON files containing the results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n       configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py \\\n       checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n       8\n```\n\n----------------------------------------\n\nTITLE: Inferencing with SDK C++ API\nDESCRIPTION: This C++ code snippet demonstrates how to use the MMDeploy SDK's C++ API for object detection inference. It loads a model, performs inference on an input image, and visualizes the detection results by drawing bounding boxes on the image.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n#include <cstdlib>\n#include <opencv2/opencv.hpp>\n#include \"mmdeploy/detector.hpp\"\n\nint main() {\n  const char* device_name = \"cuda\";\n  int device_id = 0;\n  std::string model_path = \"work_dirs/rtmdet-sdk\";\n  std::string image_path = \"demo/resources/det.jpg\";\n\n  // 1. load model\n  mmdeploy::Model model(model_path);\n  // 2. create predictor\n  mmdeploy::Detector detector(model, mmdeploy::Device{device_name, device_id});\n  // 3. read image\n  cv::Mat img = cv::imread(image_path);\n  // 4. inference\n  auto dets = detector.Apply(img);\n  // 5. deal with the result. Here we choose to visualize it\n  for (int i = 0; i < dets.size(); ++i) {\n    const auto& box = dets[i].bbox;\n    fprintf(stdout, \"box %d, left=%.2f, top=%.2f, right=%.2f, bottom=%.2f, label=%d, score=%.4f\\n\",\n            i, box.left, box.top, box.right, box.bottom, dets[i].label_id, dets[i].score);\n    if (bboxes[i].score < 0.3) {\n      continue;\n    }\n    cv::rectangle(img, cv::Point{(int)box.left, (int)box.top},\n                  cv::Point{(int)box.right, (int)box.bottom}, cv::Scalar{0, 255, 0});\n  }\n  cv::imwrite(\"output_detection.png\", img);\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating on Zero-Shot ODinW13 (8 Cards)\nDESCRIPTION: This shell script evaluates the MM Grounding DINO model on the ODinW13 dataset in a zero-shot setting using distributed testing with 8 GPUs. It executes the `tools/dist_test.sh` script with the specified configuration file, model weights, and the number of GPUs (8).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh configs/mm_grounding_dino/odinw/grounding_dino_swin-t_pretrain_odinw13.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth 8\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN S50 Training Log\nDESCRIPTION: This URL contains the JSON log file for training a Mask R-CNN model with ResNeSt-50 as a backbone.  The log provides details of the training progress.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/resnest/mask_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco/mask_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco-20200926_125503.log.json)\n```\n\n----------------------------------------\n\nTITLE: Define Unsupervised Data Pipeline with MultiBranch (Python)\nDESCRIPTION: This pipeline combines weak and strong augmentation pipelines for unsupervised learning using a MultiBranch structure. It loads images and empty annotations and then applies both weak (for the teacher) and strong (for the student) augmentations. This allows the model to learn from different views of the unlabeled data. The `backend_args` variable should be defined beforehand.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment unlabeled data into different views\nunsup_pipeline = [\n    dict(type='LoadImageFromFile', backend_args=backend_args),\n    dict(type='LoadEmptyAnnotations'),\n    dict(\n        type='MultiBranch',\n        unsup_teacher=weak_pipeline,\n        unsup_student=strong_pipeline,\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN Model Download\nDESCRIPTION: This URL provides access to the pre-trained weights for a Faster R-CNN model using a ResNeSt-50 backbone and FPN, trained with synchronized batch normalization. These weights are useful for fine-tuning or directly using the model for inference. The model was trained on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/resnest/faster_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/faster_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco_20200926_125502-20289c16.pth)\n```\n\n----------------------------------------\n\nTITLE: Citation LaTeX for Mask R-CNN\nDESCRIPTION: This LaTeX code provides the citation information for the Mask R-CNN paper, including the authors, title, journal, publisher, year, and month of publication. It is intended for use in academic publications or other contexts where proper citation is required.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{He_2017,\n   title={Mask R-CNN},\n   journal={2017 IEEE International Conference on Computer Vision (ICCV)},\n   publisher={IEEE},\n   author={He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},\n   year={2017},\n   month={Oct}\n}\n```\n\n----------------------------------------\n\nTITLE: Joint Training and Tracking with ByteTrack (Shell)\nDESCRIPTION: This shell script initiates the training process for ByteTrack using the dist_train.sh script. It specifies the configuration file, which defines the training parameters, and the number of GPUs to use. This is for algorithms that don't need reid model, facilitating joint training and tracking.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Training Bytetrack on crowdhuman and mot17-half-train dataset with following command\n# The number after config file represents the number of GPUs used. Here we use 8 GPUs\nbash tools/dist_train.sh configs/bytetrack/bytetrack_yolox_x_8xb4-80e_crowdhuman-mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Example: Test Mask2former with Slurm\nDESCRIPTION: This snippet provides a concrete example of testing the VIS model Mask2former on a Slurm cluster. It sets the number of GPUs and calls the slurm_test_tracking.sh script with the partition, job name, config file and checkpoint.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_17\n\nLANGUAGE: shell script\nCODE:\n```\nGPUS=8\nbash tools/slurm_test_tracking.sh \\\nmypartition \\\nvis \\\nconfigs/mask2former_vis/mask2former_r50_8xb2-8e_youtubevis2021.py \\\n--checkpoint ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Training DINO with FSDP in MMDetection (Bash)\nDESCRIPTION: This bash script demonstrates how to train the DINO object detection model with a Swin-L backbone using FSDP (Fully Sharded Data Parallel) on 8 GPUs with MMDetection. It includes options for mixed precision training (AMP).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_largemodel/README_zh-CN.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd mmdetection\n./tools/dist_train.sh projects/example_largemodel/dino-5scale_swin-l_fsdp_8xb2-12e_coco.py 8\n./tools/dist_train.sh projects/example_largemodel/dino-5scale_swin-l_fsdp_8xb2-12e_coco.py 8 --amp\n```\n\n----------------------------------------\n\nTITLE: Multi-task Training with Port Specification (Shell)\nDESCRIPTION: These shell commands demonstrate how to launch multiple training tasks on a single machine with multiple GPUs, specifying different ports for each task to avoid conflicts.  `CUDA_VISIBLE_DEVICES` assigns GPUs to each task, and `PORT` sets the communication port.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh ${CONFIG_FILE} 4\n```\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh ${CONFIG_FILE} 4\n```\n\n----------------------------------------\n\nTITLE: RTMDet Testing with Visualization in MMDetection\nDESCRIPTION: This snippet demonstrates how to test the RTMDet model with visualization using the `tools/test.py` script. It specifies the configuration file and checkpoint file for RTMDet and uses the `--show` flag to display the detection results. This configuration allows to press any key for the next image.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --show\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Optimizer with custom_imports in MMDetection\nDESCRIPTION: This snippet shows how to use `custom_imports` in the configuration file to import the custom optimizer module.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(imports=['mmdet.engine.optimizers.my_optimizer'], allow_failed_imports=False)\n\n```\n\n----------------------------------------\n\nTITLE: Using Custom Imports in MMDetection Config\nDESCRIPTION: Shows how to use `custom_imports` in the config file to import the MobileNet module. The `imports` list specifies the path to the module, and `allow_failed_imports` controls whether to allow the import to fail silently.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(\n    imports=['mmdet.models.backbones.mobilenet'],\n    allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration (2.x Save Best)\nDESCRIPTION: This code snippet shows how to configure the saving of the best model in MMDetection 2.x. It sets `save_best` to `auto` within the `evaluation` config, which automatically saves the best model based on the evaluation metric. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nevaluation = dict(\n    save_best='auto')\n```\n\n----------------------------------------\n\nTITLE: Modifying Dataset Classes in MMDetection\nDESCRIPTION: This snippet demonstrates how to modify the dataset classes to train on a subset of the original classes. It defines a tuple of class names to be used and sets the `metainfo` parameter within the dataset configuration of the dataloaders. This will cause the dataset to filter out ground truth boxes that do not belong to the specified classes. Remember to also change the `num_classes` in the head accordingly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_dataset.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclasses = ('person', 'bicycle', 'car')\ntrain_dataloader = dict(\n    dataset=dict(\n        metainfo=dict(classes=classes))\n    )\nval_dataloader = dict(\n    dataset=dict(\n        metainfo=dict(classes=classes))\n    )\ntest_dataloader = dict(\n    dataset=dict(\n        metainfo=dict(classes=classes))\n    )\n```\n\n----------------------------------------\n\nTITLE: Using AvoidCUDAOOM as a Decorator in Python\nDESCRIPTION: This demonstrates how to use `AvoidCUDAOOM` as a decorator to handle CUDA out-of-memory errors within a function. The retry_if_cuda_oom decorator retries the decorated function with different strategies (empty cache, FP16 conversion, CPU offloading) to prevent crashes due to GPU memory exhaustion.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.utils import AvoidCUDAOOM\n\n@AvoidCUDAOOM.retry_if_cuda_oom\ndef function(*args, **kwargs):\n    ...\n    return xxx\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection\nDESCRIPTION: Clones the MMDetection repository from GitHub, navigates into the directory, and installs it in editable mode using pip. This allows for easy modification of the MMDetection codebase.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/open-mmlab/mmdetection\ncd mmdetection\npip install -v -e .\n```\n\n----------------------------------------\n\nTITLE: Saving Inference Results using DetInferencer\nDESCRIPTION: This snippet shows how to save the prediction results and visualized images to a specified directory using the DetInferencer.  `out_dir` sets the output directory, and `no_save_pred=False` enables saving the prediction JSON files. `no_save_vis` defaults to `False` to also save visualizations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninferencer('demo/demo.jpg', out_dir='outputs/', no_save_pred=False)\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Model Testing with MMDetection\nDESCRIPTION: This shell script executes the distributed testing script `tools/dist_test.sh` for a given model configuration and checkpoint file across multiple GPUs.  It requires specifying the number of GPUs to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    ${GPU_NUM} \\\n    [--out ${RESULT_FILE}]\n```\n\n----------------------------------------\n\nTITLE: Migrating Test Pipeline v2.x to v3.x in MMDetection\nDESCRIPTION: This code demonstrates how to migrate the test pipeline configuration from MMDetection v2.x to v3.x. Key changes include removing `Normalize`, `Pad`, and `MultiScaleFlipAug` transforms. The TTA is now separated from the normal testing process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\n```\n\nLANGUAGE: python\nCODE:\n```\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\n```\n\n----------------------------------------\n\nTITLE: Training YOLOX-X Detector\nDESCRIPTION: This shell script command trains the YOLOX-X detector on the CrowdHuman and MOT17-half-train datasets.  It uses the `dist_train.sh` script, specifying the configuration file and the number of GPUs (8 in this case).  The configuration file defines the training parameters and dataset paths.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/strongsort/README.md#_snippet_1\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_train.sh configs/det/yolox_x_8xb4-80e_crowdhuman-mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Download Datasets (download_dataset.py)\nDESCRIPTION: This snippet shows how to use `tools/misc/download_dataset.py` to download various datasets like COCO, VOC, and LVIS.  The `--dataset-name` argument specifies which dataset to download.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name coco2017\npython tools/misc/download_dataset.py --dataset-name voc2007\npython tools/misc/download_dataset.py --dataset-name lvis\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training Script\nDESCRIPTION: This snippet demonstrates how to perform distributed multi-GPU training using the `dist_train.sh` script, specifying the configuration file and the number of GPUs. The first command trains with default save path, the second defines `my_work_dirs` as save directory.  It is necessary to navigate to the `projects/RF100-Benchmark/` directory first for the command to work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8\n# Specify the save path\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Testing with Blur Corruptions\nDESCRIPTION: This shell command tests model performance with blur corruptions. It utilizes the `test_robustness.py` script, including the configuration file, checkpoint file, and optional arguments for output and evaluation. The `--corruptions blur` argument indicates that blur-based image corruptions should be applied during the test.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# blur\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --corruptions blur\n```\n\n----------------------------------------\n\nTITLE: Performing Open-Vocabulary Object Detection\nDESCRIPTION: This shell command executes open-vocabulary object detection using the MM Grounding DINO model. It allows the user to input arbitrary class names for detection. The `--texts 'zebra. giraffe'` argument specifies the class names to detect in the image, and the `-c` argument likely enables some custom configuration or flag within the `image_demo.py` script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/animals.png \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'zebra. giraffe' -c\n```\n\n----------------------------------------\n\nTITLE: Testing Faster R-CNN with top-k parameter (Shell)\nDESCRIPTION: This command tests a Faster R-CNN model and saves the top 50 scoring images to the 'results/' directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       result.pkl \\\n       results \\\n       --topk 50\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot ODinW13 Evaluation - Multi GPU - Shell\nDESCRIPTION: This shell script evaluates the MM Grounding DINO model on the ODinW13 dataset using multiple GPUs. It employs `tools/dist_test.sh` alongside the appropriate configuration and the pre-trained model weights, running the evaluation across 8 GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh configs/mm_grounding_dino/odinw/grounding_dino_swin-t_pretrain_odinw13.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth 8\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Hyperparameters in Focal Loss\nDESCRIPTION: This demonstrates how to adjust the hyperparameters `gamma` and `alpha` in the Focal Loss configuration.  The values of gamma is set to 1.5 and alpha to 0.5.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=1.5,\n    alpha=0.5,\n    loss_weight=1.0)\n```\n\n----------------------------------------\n\nTITLE: Installing MMPreTrain with pip\nDESCRIPTION: This command installs the MMPreTrain library using pip, which is required as a dependency for using ConvNeXt backbones in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/convnext/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install mmpretrain\n```\n\n----------------------------------------\n\nTITLE: Optimizer Configuration (3.x)\nDESCRIPTION: This snippet shows the optimizer configuration in MMDetection 3.x, using the `OptimWrapper`. It includes the optimizer type, learning rate, momentum, and weight decay. Gradient clipping is configured within the `optim_wrapper`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(  # Configuration for the optimizer wrapper\n    type='OptimWrapper',  # Type of optimizer wrapper, you can switch to AmpOptimWrapper to enable mixed precision training\n    optimizer=dict(  # Optimizer configuration, supports various PyTorch optimizers, please refer to https://pytorch.org/docs/stable/optim.html#algorithms\n        type='SGD',  # SGD\n        lr=0.02,  # Base learning rate\n        momentum=0.9,  # SGD with momentum\n        weight_decay=0.0001),  # Weight decay\n    clip_grad=None,  # Configuration for gradient clipping, set to None to disable. For usage, please see https://mmengine.readthedocs.io/en/latest/tutorials/optimizer.html\n    )\n```\n\n----------------------------------------\n\nTITLE: Converting Detic Model Format\nDESCRIPTION: This script converts the Detic model weight format to the MMDetection format. This allows the Detic models to be used within the MMDetection framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/model_converters/detic_to_mmdet.py --src /path/to/detic_weight.pth --dst /path/to/mmdet_weight.pth\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training of SparseInst with MMDetection\nDESCRIPTION: This command enables multi-GPU training for the SparseInst model within MMDetection. It uses `torch.distributed.launch` to manage distributed training across multiple GPUs, configuring the number of nodes, rank, GPUs per node, master port, and master address. Requires setting the `NUM_GPUS` environment variable.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/SparseInst/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=${NUM_GPUS} --master_port=29506 --master_addr=\"127.0.0.1\" tools/train.py projects/SparseInst/configs/sparseinst_r50_iam_8xb8-ms-270k_coco.py\n```\n\n----------------------------------------\n\nTITLE: Separate Trained Detector Evaluation (Shell)\nDESCRIPTION: This shell script executes the evaluation and testing of a separate trained detector using the dist_test_tracking.sh script. It requires specifying the configuration file and the path to the detector checkpoint file containing the trained weights.  This script is used for evaluating the performance of the tracker on a specified dataset using a separately trained detector.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/bytetrack/bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval.py 8 --detector ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Loading Weights with DetInferencer (Python)\nDESCRIPTION: This snippet demonstrates how to load a specific weight file for the `DetInferencer`, overriding the default weights for the specified model. Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', weights='path/to/rtmdet.pth')\n```\n\n----------------------------------------\n\nTITLE: ADE20K Directory Structure (Text)\nDESCRIPTION: This text block shows the expected directory structure after downloading and processing the ADE20K dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ndata\n├── ADEChallengeData2016\n│   ├── ade20k_instance_train.json\n│   ├── ade20k_instance_val.json\n│   ├── ade20k_panoptic_train\n│   │   ├── ADE_train_00000001.png\n│   │   ├── ADE_train_00000002.png\n│   │   ├── ...\n│   ├── ade20k_panoptic_train.json\n│   ├── ade20k_panoptic_val\n│   │   ├── ADE_val_00000001.png\n│   │   ├── ADE_val_00000002.png\n│   │   ├── ...\n│   ├── ade20k_panoptic_val.json\n│   ├── annotations\n│   │   ├── training\n│   │   │   ├── ADE_train_00000001.png\n│   │   │   ├── ADE_train_00000002.png\n│   │   │   ├── ...\n│   │   ├── validation\n│   │   │   ├── ADE_val_00000001.png\n│   │   │   ├── ADE_val_00000002.png\n│   │   │   ├── ...\n│   ├── annotations_instance\n│   │   ├── training\n│   │   │   ├── ADE_train_00000001.png\n│   │   │   ├── ADE_train_00000002.png\n│   │   │   ├── ...\n│   │   ├── validation\n│   │   │   ├── ADE_val_00000001.png\n│   │   │   ├── ADE_val_00000002.png\n│   │   │   ├── ...\n│   ├── categoryMapping.txt\n│   ├── images\n│   │   ├── training\n│   │   │   ├── ADE_train_00000001.jpg\n│   │   │   ├── ADE_train_00000002.jpg\n│   │   │   ├── ...\n│   │   ├── validation\n│   │   │   ├── ADE_val_00000001.jpg\n│   │   │   ├── ADE_val_00000002.jpg\n│   │   │   ├── ...\n│   ├── imgCatIds.json\n│   ├── objectInfo150.txt\n│   │── sceneCategories.txt\n```\n\n----------------------------------------\n\nTITLE: Separate Training and Tracking with ByteTrack (Shell)\nDESCRIPTION: This shell script initiates the training process for the detector independently using the dist_train.sh script. It specifies the detector configuration file, which defines the training parameters, and the number of GPUs to use. This allows training the detector separately for algorithms like SORT, DeepSORT, and StrongSORT, before using it for tracking.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Training Bytetrack on crowdhuman and mot17-half-train dataset with following command\n# The number after config file represents the number of GPUs used. Here we use 8 GPUs\nbash tools/dist_train.sh configs/bytetrack/yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: RefCOCO to ODVG Conversion\nDESCRIPTION: This script converts the Referring Expression Comprehension datasets (RefCOCO, RefCOCO+, RefCOCOg) to the ODVG format required by MMDetection. The script `refcoco2odvg.py` is located in the `tools/dataset_converters/` directory and expects the RefExp annotations in `data/coco/mdetr_annotations`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/refcoco2odvg.py data/coco/mdetr_annotations\n```\n\n----------------------------------------\n\nTITLE: Training SparseInst with MMDetection\nDESCRIPTION: This command trains the SparseInst model using the specified configuration file in the MMDetection environment. It leverages the `tools/train.py` script and requires the `sparseinst_r50_iam_8xb8-ms-270k_coco.py` configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/SparseInst/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tools/train.py projects/SparseInst/configs/sparseinst_r50_iam_8xb8-ms-270k_coco.py\n```\n\n----------------------------------------\n\nTITLE: Configuring DetVisualizationHook (Python)\nDESCRIPTION: This code snippet shows the default configuration for the DetVisualizationHook, which controls the visualization of validation and test results. The 'draw' parameter controls whether the hook is enabled, 'interval' controls the frequency of storing or displaying results, and 'show' controls whether to visualize the results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/visualization.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nvisualization=dict( # user visualization of validation and test results\n    type='DetVisualizationHook',\n    draw=False,\n    interval=1,\n    show=False)\n```\n\n----------------------------------------\n\nTITLE: Migrating Train Pipeline Config (MMDetection 2.x)\nDESCRIPTION: This code snippet demonstrates the train pipeline configuration in MMDetection 2.x. It includes data loading, annotation loading, resizing, random flipping, normalization, padding, data formatting, and collecting relevant keys.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\n\n```\n\n----------------------------------------\n\nTITLE: Initializing a Model with init_cfg in Config File (Python)\nDESCRIPTION: This code snippet illustrates how to define the `init_cfg` within a configuration file, which is then used to initialize the model. This approach allows for flexible configuration of the initialization process without modifying the model's code directly. The example shows a dictionary structure where `type` specifies the model class and `init_cfg` holds the initialization parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n\t...\n\tmodel = dict(\n      \ttype='FooModel',\n      \targ1=XXX,\n      \targ2=XXX,\n      \tinit_cfg=XXX),\n          ...\n```\n\n----------------------------------------\n\nTITLE: Evaluating Best Model Performance (mmdetection)\nDESCRIPTION: These lines present the evaluation metrics for the best-performing fine-tuned Grounding DINO model (at epoch 16).  Similar to the previous metrics, they include AP and AR values across different IoU thresholds and area sizes, demonstrating the improved detection performance after fine-tuning.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.905\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 1.000\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.923\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.905\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.927\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.937\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.937\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.937\n```\n\n----------------------------------------\n\nTITLE: Runtime Configuration (3.x)\nDESCRIPTION: This snippet demonstrates the runtime configuration in MMDetection 3.x, using the `env_cfg` dictionary.  It consolidates settings related to cuDNN benchmark, multiprocessing, distributed training, log level, and loading/resuming models. `resume_from` is replaced with `resume = True`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nenv_cfg = dict(\n    cudnn_benchmark=False,\n    mp_cfg=dict(mp_start_method='fork',\n                opencv_num_threads=0),\n    dist_cfg=dict(backend='nccl'))\nlog_level = 'INFO'\nload_from = None\nresume = False\n```\n\n----------------------------------------\n\nTITLE: Modifying Head for New Datasets in MMDetection (Python)\nDESCRIPTION: This snippet showcases how to modify the head of a detection model to match the number of classes in a new dataset when fine-tuning. Specifically, it adjusts the `num_classes` parameter within the `roi_head` configuration. The code defines the `model` dictionary with the structure for modifying the ROI head, bbox head, and mask head, setting the number of classes for each.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/finetune.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=8,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=8,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN Single GPU Testing on Pascal VOC\nDESCRIPTION: This shell command tests the Faster R-CNN model on the Pascal VOC dataset without saving the results. The goal is to evaluate the mean Average Precision (mAP).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n       configs/pascal_voc/faster-rcnn_r50_fpn_1x_voc0712.py \\\n       checkpoints/faster_rcnn_r50_fpn_1x_voc0712_20200624-c9895d40.pth\n```\n\n----------------------------------------\n\nTITLE: Slurm Training with Config Files (Shell)\nDESCRIPTION: These shell commands demonstrate how to launch Slurm training jobs using the modified configuration files (`config1.py`, `config2.py`) that already define the communication ports. `CUDA_VISIBLE_DEVICES` assigns GPUs to each task.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR}\n```\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR}\n```\n\n----------------------------------------\n\nTITLE: Example TorchServe Prediction Response\nDESCRIPTION: This JSON snippet shows an example response from a TorchServe deployment, containing bounding box detections, class labels, class names and confidence scores.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"class_label\": 16,\n    \"class_name\": \"dog\",\n    \"bbox\": [\n      294.63409423828125,\n      203.99111938476562,\n      417.048583984375,\n      281.62744140625\n    ],\n    \"score\": 0.9987992644309998\n  },\n  {\n    \"class_label\": 16,\n    \"class_name\": \"dog\",\n    \"bbox\": [\n      404.26019287109375,\n      126.0080795288086,\n      574.5091552734375,\n      293.6662292480469\n    ],\n    \"score\": 0.9979367256164551\n  },\n  {\n    \"class_label\": 16,\n    \"class_name\": \"dog\",\n    \"bbox\": [\n      197.2144775390625,\n      93.3067855834961,\n      307.8505554199219,\n      276.7560119628906\n    ],\n    \"score\": 0.993338406085968\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Migrating DataLoader Config (MMDetection 3.x)\nDESCRIPTION: This code demonstrates how to configure the data loaders in MMDetection 3.x. It defines separate configurations for training, validation, and testing data loaders, including parameters like `batch_size`, `num_workers`, `persistent_workers`, `sampler`, `batch_sampler`, and dataset-specific settings. The `persistent_workers` parameter helps avoid re-creating subprocesses, and `AspectRatioBatchSampler` improves memory utilization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_dataloader = dict(\n    batch_size=2,\n    num_workers=2,\n    persistent_workers=True,  # 避免每次迭代后 dataloader 重新创建子进程\n    sampler=dict(type='DefaultSampler', shuffle=True),  # 默认的 sampler，同时支持分布式训练和非分布式训练\n    batch_sampler=dict(type='AspectRatioBatchSampler'),  # 默认的 batch_sampler，用于保证 batch 中的图片具有相似的长宽比，从而可以更好地利用显存\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_train2017.json',\n        data_prefix=dict(img='train2017/'),\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n        pipeline=train_pipeline))\n# 在 3.x 版本中可以独立配置验证和测试的 dataloader\nval_dataloader = dict(\n    batch_size=1,\n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/instances_val2017.json',\n        data_prefix=dict(img='val2017/'),\n        test_mode=True,\n        pipeline=test_pipeline))\ntest_dataloader = val_dataloader  # 测试 dataloader 的配置与验证 dataloader 的配置相同，这里省略\n\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Mask RCNN model with a RegNetX-800MF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-800MF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-800MF_fpn_mstrain-poly_3x_coco_20210602_210641-715d51f5.pth)\n```\n\n----------------------------------------\n\nTITLE: Modifying DetVisualizationHook in MMDetection (Python)\nDESCRIPTION: This code shows how to modify the `DetVisualizationHook` to enable drawing visualizations and specify multiple visualization backends, such as `LocalVisBackend` and `TensorboardVisBackend`.  It configures the `DetLocalVisualizer` to use these backends.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    visualization=dict(type='DetVisualizationHook', draw=True))\n\nvis_backends = [dict(type='LocalVisBackend'),\n                dict(type='TensorboardVisBackend')]\nvisualizer = dict(\n    type='DetLocalVisualizer', vis_backends=vis_backends, name='visualizer')\n\n```\n\n----------------------------------------\n\nTITLE: FocalLoss Configuration - Python\nDESCRIPTION: This configuration snippet shows how to set the parameters for the `FocalLoss` within the `loss_cls` dictionary. It defines the loss type as 'FocalLoss' and sets initial values for parameters like `use_sigmoid`, `gamma`, `alpha`, and `loss_weight`.  This enables configuring the `FocalLoss` from the config file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=2.0,\n    alpha=0.25,\n    loss_weight=1.0)\n```\n\n----------------------------------------\n\nTITLE: Installing MMPreTrain via MIM\nDESCRIPTION: This command installs the MMPreTrain library using MIM (MMDetection Installation Manager). MIM simplifies the installation of OpenMMLab projects by handling dependencies and environment setup.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/classification/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmim install mmpretrain\n```\n\n----------------------------------------\n\nTITLE: Browse Dataset - Shell\nDESCRIPTION: Executes the `browse_dataset.py` script to visualize the training dataset. This is used to verify that the dataset configuration is correct. The script takes the configuration file as an argument and optionally allows specifying the show interval and whether to display the images on the fly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_analysis_tools.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/browse_dataset.py ${CONFIG_FILE} [--show-interval ${SHOW_INTERVAL}]\n```\n\n----------------------------------------\n\nTITLE: Starting TorchServe\nDESCRIPTION: This command starts the TorchServe server, loading the specified model archive from the model store. The --ncs flag enables the no-console mode.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ntorchserve --start --ncs \\\n  --model-store ${MODEL_STORE} \\\n  --models  ${MODEL_NAME}.mar\n```\n\n----------------------------------------\n\nTITLE: Run MMDetection Docker Container\nDESCRIPTION: This command runs the MMDetection Docker image, mounting a data directory and enabling GPU access. Adjust the DATA_DIR environment variable to point to your data directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run --gpus all --shm-size=8g -it -v {DATA_DIR}:/mmdetection/data mmdetection\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - X-101-32x4d Backbone, MDConv, MS-train, 2x Schedule\nDESCRIPTION: This configuration file sets up VarifocalNet with a ResNeXt-101-32x4d backbone, using Modulated Deformable Convolution (MDConv) in C3-C5 stages. It's trained with multi-scale training and a 2x learning rate schedule on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_x101-32x4d-mdconv-c3-c5_fpn_ms-2x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: DeepSORT Video Inference (Python)\nDESCRIPTION: This command performs inference on a video using DeepSORT with a single GPU and saves the output as a video. It utilizes the `demo/mot_demo.py` script, requiring the input video path, configuration file path, detector checkpoint path, ReID checkpoint path, and output video path as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepsort/README.md#_snippet_3\n\nLANGUAGE: Shell Script\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/deepsort/deepsort_faster-rcnn_r50_fpn_8xb2-4e_mot17train_test-mot17test --detector ${DETECTOR_CHECKPOINT_PATH} --reid ${REID_CHECKPOINT_PATH} --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Hooks in MMDetection\nDESCRIPTION: This code snippet shows how to configure custom hooks in MMDetection, specifying the hook type and its parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MyHook', a=a_value, b=b_value)\n]\n\n```\n\n----------------------------------------\n\nTITLE: Confusion Matrix Generation - Shell\nDESCRIPTION: Generates a confusion matrix from detection results.  Requires a configuration file, the path to the detection results (a .pkl file), and a save directory. The `--show` flag displays the matrix.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_42\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/confusion_matrix.py ${CONFIG}  ${DETECTION_RESULTS}  ${SAVE_DIR} --show\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with a Specific Device (GPU)\nDESCRIPTION: This snippet initializes the `DetInferencer` and explicitly sets the device to 'cuda:0', which corresponds to the first GPU. This ensures that the inference is performed on the specified GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cuda:0')\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Configuration (3.x)\nDESCRIPTION: This snippet demonstrates the learning rate configuration in MMDetection 3.x using `param_scheduler`. It includes both a linear warmup and a multi-step decay scheduler. The warmup is iteration-based, and the decay is epoch-based.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='LinearLR',  # Use linear learning rate warmup\n        start_factor=0.001, # Coefficient for learning rate warmup\n        by_epoch=False,  # Update the learning rate during warmup at each iteration\n        begin=0,  # Starting from the first iteration\n        end=500),\n    dict(\n        type='MultiStepLR',  # Use multi-step learning rate strategy during training\n        by_epoch=True,  # Update the learning rate at each epoch\n        begin=0,   # Starting from the first epoch\n        end=12,  # Ending at the 12th epoch\n        milestones=[8, 11],  # Learning rate decay at which epochs\n        gamma=0.1)  # Learning rate decay coefficient\n]\n```\n\n----------------------------------------\n\nTITLE: Testing Model with Specified Timesteps (Shell)\nDESCRIPTION: This command tests a model using a specified number of sampling timesteps during inference. The `--cfg-options` argument allows modifying configuration parameters at runtime. Here it overrides `model.bbox_head.sampling_timesteps` to control the number of denoising steps in DiffusionDet.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# for 1 step inference\n# test command\npython tools/test.py projects/DiffusionDet/configs/diffusiondet_r50_fpn_500-proposals_1-step_crop-ms-480-800-450k_coco.py ${CHECKPOINT_PATH}\n\n# for 4 steps inference\n\n# test command\npython tools/test.py projects/DiffusionDet/configs/diffusiondet_r50_fpn_500-proposals_1-step_crop-ms-480-800-450k_coco.py ${CHECKPOINT_PATH} --cfg-options model.bbox_head.sampling_timesteps=4\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Cascade Mask RCNN model with a RegNetX-800MF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_35\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/cascade_mask_rcnn_regnetx-800MF_fpn_mstrain_3x_coco/cascade_mask_rcnn_regnetx-800MF_fpn_mstrain_3x_coco_20210715_211616-dcbd13f4.pth)\n```\n\n----------------------------------------\n\nTITLE: Example: Test SORT on CPU\nDESCRIPTION: This snippet provides a concrete example of testing the SORT model on the CPU using a specific configuration file and checkpoint. The CUDA_VISIBLE_DEVICES variable is set to -1 to disable the GPU, and the testing script is executed with the config file and detector checkpoint for the SORT model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_11\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/test_tracking.py configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py --detector ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Using Custom Imports in Configuration (Python)\nDESCRIPTION: Demonstrates how to use `custom_imports` in a configuration file to import the custom loss function and module. This provides an alternative to importing the loss in the `__init__.py` file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports=dict(\n    imports=['mmdet.models.losses.my_loss'])\n```\n\n----------------------------------------\n\nTITLE: Converting TensorFlow weights to PyTorch\nDESCRIPTION: This command converts EfficientDet weights from TensorFlow to PyTorch format. It requires specifying the backbone name, the path to the TensorFlow weight file, and the desired output path for the converted PyTorch weight file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython projects/EfficientDet/convert_tf_to_pt.py --backbone {BACKBONE_NAME} --tensorflow_weight {TENSORFLOW_WEIGHT_PATH} --out_weight {OUT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Migrating RandomFlip Configuration in MMDetection\nDESCRIPTION: Demonstrates the change in the RandomFlip transform configuration from v2.x to v3.x. The parameter name `flip_ratio` is changed to `prob`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndict(type='RandomFlip', flip_ratio=0.5)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(type='RandomFlip', prob=0.5)\n```\n\n----------------------------------------\n\nTITLE: Inference with Pre-trained Model (Phrase Grounding)\nDESCRIPTION: This shell command performs inference for phrase grounding using a pre-trained Grounding DINO model. It uses the `tools/test.py` script with a specific configuration file for the flickr30k dataset and a model weights file.  The resulting pseudo-labels, which consist of bounding boxes associated with phrases, are saved in a JSON file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_pseudo-labeling_flickr30k.py \\\n    grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Extracting Region Proposals Command (Bash)\nDESCRIPTION: This bash command uses the `dist_test.sh` script to extract region proposals. It specifies the configuration file, checkpoint file, and the number of GPUs to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fast_rcnn/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./tools/dist_test.sh \\\n    configs/rpn_r50_fpn_1x_coco.py \\\n    checkpoints/rpn_r50_fpn_1x_coco_20200218-5525fa2e.pth \\\n    8\n```\n\n----------------------------------------\n\nTITLE: Training on Multiple Nodes with MMDetection\nDESCRIPTION: This snippet shows how to train a model across multiple nodes using MMDetection. It illustrates the commands to run on the first and second machines, including setting environment variables like NNODES, NODE_RANK, PORT, and MASTER_ADDR. It relies on the `tools/dist_train.sh` script for distributed training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_7\n\nLANGUAGE: shell 脚本\nCODE:\n```\nNNODES=2 NODE_RANK=0 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR bash tools/dist_train.sh $CONFIG $GPUS\n```\n\nLANGUAGE: shell 脚本\nCODE:\n```\nNNODES=2 NODE_RANK=1 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR bash tools/dist_train.sh $CONFIG $GPUS\n```\n\n----------------------------------------\n\nTITLE: Installing MMDetection with Multimodal Support using mim\nDESCRIPTION: This command installs or updates MMDetection using mim (OpenMMLab package manager) and includes support for multimodal models like GLIP. This command ensures that the correct version of MMDetection with all required dependencies is installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/glip/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmim install mmdet[multimodal]\n```\n\n----------------------------------------\n\nTITLE: Configuring Faster R-CNN with Pre-trained FCOS RPN (Python)\nDESCRIPTION: This Python configuration sets up Faster R-CNN to use a pre-trained FCOS model as the RPN. It updates the data preprocessor to handle the caffe version of ResNet50 and loads weights from the provided pre-trained model URL. It also configures the neck and rpn_head to use FCOS.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/single_stage_as_rpn.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    data_preprocessor=dict(\n        mean=[103.530, 116.280, 123.675],\n        std=[1.0, 1.0, 1.0],\n        bgr_to_rgb=False),\n    backbone=dict(\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='caffe',\n        init_cfg=None),  # the checkpoint in ``load_from`` contains the weights of backbone\n    neck=dict(\n        start_level=1,\n        add_extra_convs='on_output',  # use P5\n        relu_before_extra_convs=True),\n    rpn_head=dict(\n        _delete_=True,  # ignore the unused old settings\n        type='FCOSHead',\n        num_classes=1,  # num_classes = 1 for rpn, if num_classes > 1, it will be set to 1 in TwoStageDetector automatically\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n    roi_head=dict(  # update featmap_strides due to the strides in neck\n        bbox_roi_extractor=dict(featmap_strides=[8, 16, 32, 64, 128])))\n\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco/fcos_r50_caffe_fpn_gn-head_1x_coco-821213aa.pth'\n```\n\n----------------------------------------\n\nTITLE: Testing with Digital Corruptions\nDESCRIPTION: This shell command evaluates model performance with digital image corruptions. It executes `test_robustness.py`, requiring the configuration and checkpoint files, and allowing optional output and evaluation arguments. The `--corruptions digital` argument ensures that digital corruptions are applied to the images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# digital\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --corruptions digital\n```\n\n----------------------------------------\n\nTITLE: Training Model in MMDetection (Shell)\nDESCRIPTION: This command trains a model using the specified configuration file. The configuration file defines the model architecture, training parameters, and data loading pipeline.  The training process involves iteratively updating the model's weights based on the training data.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py projects/DiffusionDet/configs/diffusiondet_r50_fpn_500-proposals_1-step_crop-ms-480-800-450k_coco.py\n```\n\n----------------------------------------\n\nTITLE: Parameter Scheduler Configuration - Python\nDESCRIPTION: This defines the parameter scheduler to adjust optimizer hyperparameters like learning rate. It includes a linear learning rate warmup and a multi-step learning rate decay strategy, configuring the start/end epochs, milestones, and gamma for learning rate adjustments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='LinearLR',  # 使用线性学习率预热\n        start_factor=0.001, # 学习率预热的系数\n        by_epoch=False,  # 按 iteration 更新预热学习率\n        begin=0,  # 从第一个 iteration 开始\n        end=500),  # 到第 500 个 iteration 结束\n    dict(\n        type='MultiStepLR',  # 在训练过程中使用 multi step 学习率策略\n        by_epoch=True,  # 按 epoch 更新学习率\n        begin=0,   # 从第一个 epoch 开始\n        end=12,  # 到第 12 个 epoch 结束\n        milestones=[8, 11],  # 在哪几个 epoch 进行学习率衰减\n        gamma=0.1)  # 学习率衰减系数\n]\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - R-50 Backbone, MDConv, MS-train, 2x Schedule\nDESCRIPTION: This configuration file is designed for VarifocalNet with a ResNet-50 backbone and utilizing Modulated Deformable Convolution (MDConv) in C3-C5 stages. It is trained with multi-scale training and a 2x learning rate schedule on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_r50-mdconv-c3-c5_fpn_ms-2x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Run GLIP Inference Demo\nDESCRIPTION: Executes the `image_demo.py` script for multimodal object detection using the GLIP model.  It takes an image, checkpoint file, and text prompts as input. The `--texts` argument specifies the text prompts to use for object detection. In this case, it searches for \"bench\".\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg glip_tiny_a_mmdet-b3654169.pth --texts bench\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Faster RCNN model with a RegNetX-400MF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-400MF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-400MF_fpn_mstrain_3x_coco_20210526_095112.log.json)\n```\n\n----------------------------------------\n\nTITLE: Testing Semantic Segmentation on ADE20K with X-Decoder\nDESCRIPTION: This command tests the X-Decoder model for semantic segmentation on the ADE20K dataset. It uses the `dist_test.sh` script for distributed testing and specifies the configuration file, pre-trained weights, and number of GPUs.  `model.test_cfg.use_thr_for_mc=False` disables thresholding.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-semseg_ade20k.py xdecoder_focalt_best_openseg.pt 8 --cfg-options model.test_cfg.use_thr_for_mc=False\n```\n\n----------------------------------------\n\nTITLE: Move ADE20K Annotations and Preprocess (Shell)\nDESCRIPTION: Moves the downloaded ADE20K annotations to the `data/ADEChallengeData2016` directory and runs the `ade20k2coco.py` script to convert the annotations to the COCO format for both panoptic and instance segmentation tasks. Requires the `ade20k2coco.py` script from the `tools/dataset_converters` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmv data/annotations_instance data/ADEChallengeData2016/\nmv data/categoryMapping.txt data/ADEChallengeData2016/\nmv data/imgCatIds.json data/ADEChallengeData2016/\npython tools/dataset_converters/ade20k2coco.py data/ADEChallengeData2016 --task panoptic\npython tools/dataset_converters/ade20k2coco.py data/ADEChallengeData2016 --task instance\n```\n\n----------------------------------------\n\nTITLE: Install MMEngine and MMCV using MIM (Shell)\nDESCRIPTION: This snippet uses `pip` and `mim` to install MMEngine and MMCV, which are dependencies of MMDetection. It first installs `openmim` and then uses it to install `mmengine` and `mmcv`. The `mmcv` version is pinned to be greater than or equal to 2.0.0 and less than 2.1.0.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n!pip3 install openmim\n!mim install mmengine\n!mim install \"mmcv>=2.0.0,<2.1.0\"\n```\n\n----------------------------------------\n\nTITLE: Running GLIP Image Demo\nDESCRIPTION: This set of commands downloads the GLIP model weights and then executes the image demo script to perform object detection and grounding on a sample image.  The command requires specifying the configuration file, the weight file, the image path, and the text prompts for grounding. It showcases how to use a pre-trained GLIP model for inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/glip/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd $MMDETROOT\n\nwget https://download.openmmlab.com/mmdetection/v3.0/glip/glip_tiny_a_mmdet-b3654169.pth\n\npython demo/image_demo.py demo/demo.jpg \\\nconfigs/glip/glip_atss_swin-t_a_fpn_dyhead_pretrain_obj365.py \\\n--weights glip_tiny_a_mmdet-b3654169.pth \\\n--texts 'bench. car'\n```\n\n----------------------------------------\n\nTITLE: Convert MoCo ResNet-50 Model (Bash)\nDESCRIPTION: This example demonstrates how to convert a ResNet-50 model pretrained with MoCo to the MMDetection format. It uses the `selfsup2mmdet.py` script to convert the downloaded MoCo checkpoint to a PyTorch-style checkpoint.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/selfsup_pretrain/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -u tools/model_converters/selfsup2mmdet.py ./moco_v2_800ep_pretrain.pth.tar mocov2_r50_800ep_pretrain.pth --selfsup moco\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - X-101-64x4d Backbone, MDConv, MS-train, 2x Schedule\nDESCRIPTION: This configuration file is for VarifocalNet with a ResNeXt-101-64x4d backbone, using Modulated Deformable Convolution (MDConv) in C3-C5 stages. It is trained with multi-scale training and a 2x learning rate schedule on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_x101-64x4d-mdconv-c3-c5_fpn_ms-2x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Phrase Grounding Results (Shell)\nDESCRIPTION: This command visualizes the results of phrase grounding using a visualization script. It requires the data directory, JSON file, image directory, and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_raw.py data/flickr30k_entities/ flickr_simple_train_vg_v1.json flickr30k_images -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training with MMPreTrain\nDESCRIPTION: This command initiates training using multiple GPUs. The `${CONFIG_FILE}` variable should be replaced with the path to the configuration file. `${GPU_NUM}` should be replaced with the number of GPUs to use. Optional arguments can be added to customize the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/classification/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbash ./tools/dist_train.sh \\\n    ${CONFIG_FILE} \\\n    ${GPU_NUM} \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Runtime Configuration (2.x)\nDESCRIPTION: This code snippet shows the runtime configuration in MMDetection 2.x. It configures cudnn benchmark, the number of OpenCV threads, the multiprocessing start method, distributed parameters, the log level, loading from a checkpoint, and resuming from a checkpoint. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ncudnn_benchmark = False\nopencv_num_threads = 0\nmp_start_method = 'fork'\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\n\n```\n\n----------------------------------------\n\nTITLE: Start MMYOLO Backend Inference Service\nDESCRIPTION: Starts the Label-Studio ML backend using MMYOLO configuration and weights files.  Similar to the RTMDet setup, it allows specifying device (CPU or GPU) and port number.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\n\nlabel-studio-ml start projects/LabelStudio/backend_template --with \\\nconfig_file= path/to/mmyolo_config.py \\\ncheckpoint_file= path/to/mmyolo_weights.pth \\\ndevice=cpu \\\n--port 8003\n# device=cpu is for using CPU inference. If using GPU inference, replace cpu with cuda:0.\n```\n\n----------------------------------------\n\nTITLE: Single GPU Robustness Testing\nDESCRIPTION: This command runs a single-GPU test using `test_robustness.py` on a specified configuration file and checkpoint file. It supports outputting results to a file. `${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}]\n```\n\n----------------------------------------\n\nTITLE: Download RefCOCO Dataset\nDESCRIPTION: Downloads and unzips the RefCOCO dataset to the specified directory. The `--dataset-name` argument specifies which dataset to download (refcoco).  The `--save-dir` option specifies the directory to save the downloaded dataset. The `--unzip` option extracts the downloaded dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name refcoco --save-dir data/coco --unzip\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN with PISA Configuration (COCO Dataset, X101 Backbone)\nDESCRIPTION: This snippet provides the configuration file name for Faster R-CNN with PISA enabled, trained on the COCO dataset using a ResNeXt-101 backbone. It describes the specific configuration for training a Faster R-CNN model, incorporating PISA sampling to improve detection accuracy when using a more powerful backbone.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[config](./faster-rcnn_x101-32x4d_fpn_pisa_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Using EfficientNet-B1 Backbone from TIMM via MMPretrain (Python)\nDESCRIPTION: This configuration shows how to integrate EfficientNet-B1 from the TIMM library, accessed via MMPretrain, as the backbone in a RetinaNet model within MMDetection. The configuration deletes the default backbone, specifies the `TIMMBackbone` type, sets the model name to 'efficientnet_b1', enables feature extraction only, loads pretrained weights, and configures output indices. It also modifies the in_channels of the neck and adjusts the optimizer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/how_to.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# https://github.com/open-mmlab/mmdetection/blob/main/configs/timm_example/retinanet_timm_efficientnet_b1_fpn_1x_coco.py\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\n# please install mmpretrain\n# import mmpretrain.models to trigger register_module in mmpretrain\ncustom_imports = dict(imports=['mmpretrain.models'], allow_failed_imports=False)\nmodel = dict(\n    backbone=dict(\n        _delete_=True, # 将 _base_ 中关于 backbone 的字段删除\n        type='mmpretrain.TIMMBackbone', # 使用 mmpretrain 中 timm 骨干网络\n        model_name='efficientnet_b1',\n        features_only=True,\n        pretrained=True,\n        out_indices=(1, 2, 3, 4)), # 修改 out_indices\n    neck=dict(in_channels=[24, 40, 112, 320])) # 修改 in_channels\n\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n\n```\n\n----------------------------------------\n\nTITLE: Modifying LoggerHook in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to modify the `LoggerHook` by setting the `interval` parameter. This controls how frequently the logs are written.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(logger=dict(type='LoggerHook', interval=50))\n```\n\n----------------------------------------\n\nTITLE: Download and Unzip ADE20K Dataset (Shell)\nDESCRIPTION: Downloads and unzips the ADE20K 2016 dataset to a specified directory. This script utilizes `download_dataset.py` from the `tools/misc` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name ade20k_2016 --save-dir data --unzip\n```\n\n----------------------------------------\n\nTITLE: Video Inference Demo Script (GPU Accelerated)\nDESCRIPTION: This script performs object detection inference on a video file with GPU acceleration using NVIDIA's video decoding. It requires a video file, a configuration file, and a checkpoint file. It supports specifying the device, confidence threshold, output file, and display options.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_gpuaccel_demo.py \\\n     ${VIDEO_FILE} \\\n     ${CONFIG_FILE} \\\n     ${CHECKPOINT_FILE} \\\n     [--device ${GPU_ID}] \\\n     [--score-thr ${SCORE_THR}] \\\n     [--nvdecode] \\\n     [--out ${OUT_FILE}] \\\n     [--show] \\\n     [--wait-time ${WAIT_TIME}]\n```\n\n----------------------------------------\n\nTITLE: Launch MMDetection Gradio Demo\nDESCRIPTION: This command navigates to the `mmdetection` directory and then executes the `launch.py` script located in the `projects/gradio_demo` directory.  This script starts a local Gradio server hosting the MMDetection demo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/gradio_demo/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\npython projects/gradio_demo/launch.py\n```\n\n----------------------------------------\n\nTITLE: FocalLoss Configuration\nDESCRIPTION: This snippet shows the default configuration for Focal Loss in MMDetection's config file. It specifies the type of loss as 'FocalLoss' and sets default values for parameters like `use_sigmoid`, `gamma`, `alpha`, and `loss_weight`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=2.0,\n    alpha=0.25,\n    loss_weight=1.0)\n```\n\n----------------------------------------\n\nTITLE: Showing Test Results Using Command Line (Shell)\nDESCRIPTION: These commands demonstrate how to visualize annotation and prediction results during testing using the `--show` and `--show-dir` parameters. The first command shows the test results, while the second command specifies the directory to store the prediction results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/visualization.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n# Show test results\npython tools/test.py configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet_tiny_8xb32-300e_coco/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --show\n\n# Specify where to store the prediction results\npython tools/test.py configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet_tiny_8xb32-300e_coco/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --show-dir imgs/\n```\n\n----------------------------------------\n\nTITLE: MOT Challenge Test Set Evaluation (Shell)\nDESCRIPTION: This shell script generates result files for submission to the MOT Challenge using dist_test_tracking.sh. It specifies the configuration file and checkpoint, storing the results in `./mot_17_test_res`, which can be modified in the config's `test_evaluator`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/bytetrack/bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17test.py 8 --checkpoint ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Citing VarifocalNet\nDESCRIPTION: LaTeX code for citing the VarifocalNet paper.  This includes the title, authors, journal, year, and other relevant information for the citation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_8\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhang2020varifocalnet,\n  title={VarifocalNet: An IoU-aware Dense Object Detector},\n  author={Zhang, Haoyang and Wang, Ying and Dayoub, Feras and S{\"u}nderhauf, Niko},\n  journal={arXiv preprint arXiv:2008.13367},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Inference with Pre-trained Model (Object Detection)\nDESCRIPTION: This shell command performs inference using a pre-trained Grounding DINO model. It utilizes the `tools/test.py` script from the MMDetection framework, along with a configuration file (`configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_pseudo-labeling_cat.py`) and the model weights (`grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth`). The resulting pseudo-labels are saved in a JSON file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_pseudo-labeling_cat.py \\\n    grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Modify MMDetection Config for MoCo Backbone (Python)\nDESCRIPTION: This snippet shows the necessary modifications to an MMDetection configuration file to use a MoCo-pretrained backbone. It sets the `pretrained` parameter to the path of the converted model, freezes the initial stages, and configures the normalization layers to use SyncBN.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/selfsup_pretrain/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/mask-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    pretrained='./mocov2_r50_800ep_pretrain.pth',\n    backbone=dict(\n        frozen_stages=0,\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False))\n```\n\n----------------------------------------\n\nTITLE: Training ReID Model\nDESCRIPTION: This shell script command trains the ReID model on the MOT17-train80 dataset. It utilizes the `dist_train.sh` script, providing the ReID configuration file and specifying the number of GPUs (8). The configuration file contains the training configurations for the ReID model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/strongsort/README.md#_snippet_2\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_train.sh configs/reid/reid_r50_8xb32-6e_mot17train80_test-mot17val20.py 8\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection\nDESCRIPTION: This snippet clones the MMDetection repository from GitHub, navigates into the cloned directory, and then installs MMDetection in editable mode using `pip install -e .`. This allows for easy modification and testing of MMDetection's source code.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/open-mmlab/mmdetection\ncd mmdetection\npip install -v -e .\n```\n\n----------------------------------------\n\nTITLE: Installing Multimodal Dependencies in MMDetection\nDESCRIPTION: This shell script installs the required dependencies for multimodal tasks in MMDetection, including the LVIS API. It first navigates to the MMDETROOT directory and then uses pip to install the packages listed in requirements/multimodal.txt, along with emoji, ddd-dataset, and the LVIS API from its GitHub repository. It also suggests installing numpy version 1.23 due to incompatibility with LVIS.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd $MMDETROOT\n\npip install -r requirements/multimodal.txt\npip install emoji ddd-dataset\npip install git+https://github.com/lvis-dataset/lvis-api.git\"\n```\n\n----------------------------------------\n\nTITLE: RandomResize Config (MMDetection)\nDESCRIPTION: This code snippet demonstrates the configuration for the RandomResize transform in both MMDetection 2.x and 3.x. The key change is the parameter name: `img_scale` in 2.x is replaced with `scale` in 3.x. Also, multiscale_mode is removed\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='Resize',\n    img_scale=[\n        (1333, 640), (1333, 800)],\n    multiscale_mode='range',\n    keep_ratio=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='RandomResize',\n    scale=[\n        (1333, 640), (1333, 800)],\n    keep_ratio=True)\n```\n\n----------------------------------------\n\nTITLE: Install MMCV using pip\nDESCRIPTION: Installs MMCV using pip instead of MIM. This command requires manually specifying a find-url based on the PyTorch version and its CUDA version.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npip install \"mmcv>=2.0.0\" -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.12.0/index.html\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Custom Config and Weights\nDESCRIPTION: This snippet initializes the DetInferencer with a custom configuration file and a custom path to the weights file. The `model` parameter specifies the path to the configuration file, and the `weights` parameter specifies the location of the custom checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='path/to/rtmdet_config.py', weights='path/to/rtmdet.pth')\n```\n\n----------------------------------------\n\nTITLE: Customize Iteration-Based Training Loop with Dynamic Intervals in MMDetection (Python)\nDESCRIPTION: This code snippet configures an `IterBasedTrainLoop` with dynamic intervals for validation. It sets the initial validation interval to 5000 iterations and then changes it to a larger interval to perform validation only at the end of training. `max_iters` specifies the total number of training iterations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ninterval = 5000\nmax_iters = 368750\ndynamic_intervals = [(max_iters // interval * interval + 1, max_iters)]\ntrain_cfg = dict(\n    type='IterBasedTrainLoop',\n    max_iters=max_iters,\n    val_interval=interval,\n    dynamic_intervals=dynamic_intervals)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring DetVisualizationHook\nDESCRIPTION: This snippet shows how to configure the DetVisualizationHook, which is used to draw validation and testing prediction results. The configuration includes parameters like `draw` to enable/disable drawing, `interval` to control the frequency of storing or displaying results, and `show` to control whether to visualize the results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/visualization.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nvisualization=dict( #用户可视化验证和测试结果\n    type='DetVisualizationHook',\n    draw=False,\n    interval=1,\n    show=False)\n```\n\n----------------------------------------\n\nTITLE: Importing the AugFPN module in MMDetection\nDESCRIPTION: This Python code shows how to import the custom `AugFPN` module into the MMDetection framework. It can be done by directly importing in the `__init__.py` or using `custom_imports` in the config file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom .augfpn import AugFPN\n```\n\n----------------------------------------\n\nTITLE: Testing a Trained Model with iSAID (MMDetection)\nDESCRIPTION: This command executes the testing of a pre-trained model on the iSAID dataset using MMDetection. It invokes the `tools/test.py` script, providing the configuration file and the path to the model's checkpoint file (`CHECKPOINT_PATH`). The script evaluates the model's performance on the test dataset and reports the evaluation metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/iSAID/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython tools/test.py projects/iSAID/configs/mask_rcnn_r50_fpn_1x_isaid.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Cascade R-CNN S50 Config Path\nDESCRIPTION: Configuration file path for a Cascade R-CNN model utilizing a ResNeSt-50 backbone. It configures the model architecture, training schedule, and evaluation settings within the MMDetection framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n[config](./cascade-rcnn_s50_fpn_syncbn-backbone+head_ms-range-1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Modifying number of classes in ReID model\nDESCRIPTION: This code modifies the number of classes in the ReID model's head within the MMDetection configuration.  This setting is crucial for adapting the model to a new dataset with a different number of identities.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/reid/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(reid=dict(head=dict(num_classes=100)))\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-s\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-s model. This file configures the architecture, training parameters, and other settings of the 's' variant of the RTMDet model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_s_8xb32-300e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Balloon Dataset Conversion to COCO Format (Python)\nDESCRIPTION: This Python code converts the balloon dataset annotation format to the COCO format, enabling its use with MMDetection. It reads the original annotations, extracts image dimensions and polygon data, and structures it into the COCO JSON format with images, annotations, and category information.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os.path as osp\n\nimport mmcv\n\nfrom mmengine.fileio import dump, load\nfrom mmengine.utils import track_iter_progress\n\n\ndef convert_balloon_to_coco(ann_file, out_file, image_prefix):\n    data_infos = load(ann_file)\n\n    annotations = []\n    images = []\n    obj_count = 0\n    for idx, v in enumerate(track_iter_progress(data_infos.values())):\n        filename = v['filename']\n        img_path = osp.join(image_prefix, filename)\n        height, width = mmcv.imread(img_path).shape[:2]\n\n        images.append(\n            dict(id=idx, file_name=filename, height=height, width=width))\n\n        for _, obj in v['regions'].items():\n            assert not obj['region_attributes']\n            obj = obj['shape_attributes']\n            px = obj['all_points_x']\n            py = obj['all_points_y']\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = [p for x in poly for p in x]\n\n            x_min, y_min, x_max, y_max = (min(px), min(py), max(px), max(py))\n\n            data_anno = dict(\n                image_id=idx,\n                id=obj_count,\n                category_id=0,\n                bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n                area=(x_max - x_min) * (y_max - y_min),\n                segmentation=[poly],\n                iscrowd=0)\n            annotations.append(data_anno)\n            obj_count += 1\n\n    coco_format_json = dict(\n        images=images,\n        annotations=annotations,\n        categories=[{\n            'id': 0,\n            'name': 'balloon'\n        }])\n    dump(coco_format_json, out_file)\n\n\nif __name__ == '__main__':\n    convert_balloon_to_coco(ann_file='data/balloon/train/via_region_data.json',\n                            out_file='data/balloon/train/annotation_coco.json',\n                            image_prefix='data/balloon/train')\n    convert_balloon_to_coco(ann_file='data/balloon/val/via_region_data.json',\n                            out_file='data/balloon/val/annotation_coco.json',\n                            image_prefix='data/balloon/val')\n```\n\n----------------------------------------\n\nTITLE: Convert Detectron ResNet Model to Pytorch Model (detectron2pytorch.py)\nDESCRIPTION: This snippet shows how to convert a pre-trained RegNet model from Detectron to the MMDetection style using `tools/model_converters/detectron2pytorch.py`. It takes the source, destination file paths, and the depth of the ResNet as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/detectron2pytorch.py ${SRC} ${DST} ${DEPTH} [-h]\n```\n\n----------------------------------------\n\nTITLE: Class-wise Evaluation Testing with 8 GPUs in MMDetection\nDESCRIPTION: This snippet shows how to test Mask R-CNN with 8 GPUs and evaluate the metric class-wise. It utilizes `tools/dist_test.sh` and passes the `--cfg-options` flag to enable class-wise evaluation using `test_evaluator.classwise=True`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n    configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py \\\n    checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n    8 \\\n    --out results.pkl \\\n    --cfg-options test_evaluator.classwise=True\n```\n\n----------------------------------------\n\nTITLE: Test-Time Augmentation with Scaling\nDESCRIPTION: This shell snippet illustrates an example of TTA with multi-scale resizing, as well as flipping.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ntta_model = dict(\n    type='DetTTAModel',\n    tta_cfg=dict(nms=dict(\n                   type='nms',\n                   iou_threshold=0.5),\n                   max_per_img=100))\n\nimg_scales = [(1333, 800), (666, 400), (2000, 1200)]\ntta_pipeline = [\n    dict(type='LoadImageFromFile',\n         backend_args=None),\n    dict(\n        type='TestTimeAug',\n        transforms=[[[\n            dict(type='Resize', scale=s, keep_ratio=True) for s in img_scales\n        ]], [[\n            dict(type='RandomFlip', prob=1.),\n            dict(type='RandomFlip', prob=0.)\n        ]], [[\n            dict(\n               type='PackDetInputs',\n               meta_keys=('img_id', 'img_path', 'ori_shape',\n                       'img_shape', 'scale_factor', 'flip',\n                       'flip_direction'))\n       ]]])]\n```\n\n----------------------------------------\n\nTITLE: Downloading Roboflow 100 Datasets\nDESCRIPTION: This snippet provides the command to download the Roboflow 100 datasets using a bash script. The script leverages the Roboflow API and the previously set API key to download the datasets into the specified directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncd projects/RF100-Benchmark/\nbash scripts/download_datasets.sh\n```\n\n----------------------------------------\n\nTITLE: Initializing a Model with init_cfg in mmcv.Sequential/ModuleList (Python)\nDESCRIPTION: This code snippet demonstrates how to initialize a model component (specifically a `ModuleList`) by specifying `init_cfg` directly within its definition. This is useful when you want to initialize a specific part of the model differently than the rest.  The example uses `mmcv.ModuleList` but the concept extends to `mmcv.Sequential` as well.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmcv.runner import BaseModule, ModuleList\n\nclass FooModel(BaseModule):\n\tdef __init__(self,\n              \targ1,\n              \targ2,\n              \tinit_cfg=None):\n\t\tsuper(FooModel, self).__init__(init_cfg)\n      \t...\n      \tself.conv1 = ModuleList(init_cfg=XXX)\n```\n\n----------------------------------------\n\nTITLE: Hook Configuration - Python\nDESCRIPTION: This section defines configurations for various hooks, including IterTimerHook, LoggerHook, ParamSchedulerHook, CheckpointHook, DistSamplerSeedHook, and DetVisualizationHook. Hooks are inserted into the training, validation, and testing loops to perform operations during runtime.  Default hooks are configured with default priorities, while custom hooks allow for user-defined functionalities.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    timer=dict(type='IterTimerHook'),\n    logger=dict(type='LoggerHook', interval=50),\n    param_scheduler=dict(type='ParamSchedulerHook'),\n    checkpoint=dict(type='CheckpointHook', interval=1),\n    sampler_seed=dict(type='DistSamplerSeedHook'),\n    visualization=dict(type='DetVisualizationHook'))\n```\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = []\n```\n\n----------------------------------------\n\nTITLE: Fusion and Saving of Prediction Results (fuse_results.py)\nDESCRIPTION: This example fuses prediction results from three models and saves the fusion results to a specified directory. The `--save-fusion-results` flag saves the results, and `--out-dir` specifies the output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ./faster-rcnn_r50-caffe_fpn_1x_coco.json \\\n       ./retinanet_r50-caffe_fpn_1x_coco.json \\\n       ./cascade-rcnn_r50-caffe_fpn_1x_coco.json \\\n       --annotation ./annotation.json \\\n       --weights 1 2 3 \\\n       --save-fusion-results \\\n       --out-dir outputs/fusion\n```\n\n----------------------------------------\n\nTITLE: Training YOLACT with ResNet-101 in Shell\nDESCRIPTION: This shell script trains the YOLACT model with a ResNet-101 backbone using a batch size of 8 on a single GPU.  It uses the `dist_train.sh` script provided by mmdetection. The script requires the configuration file path and the number of GPUs to use as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/yolact/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n# Trains using the resnet-101 backbone with a batch size of 8 on a single GPU.\n./tools/dist_train.sh configs/yolact/yolact_r101.py 1\n```\n\n----------------------------------------\n\nTITLE: Install Timm and MMPretrain\nDESCRIPTION: This script installs the dataclasses package for older Python versions, the timm library, and the mmpretrain library, which are necessary for using Timm backbones with MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/timm_example/README.md#_snippet_0\n\nLANGUAGE: Shell Script\nCODE:\n```\npip install 'dataclasses; python_version<\"3.7\"'\npip install timm\npip install mmpretrain\n```\n\n----------------------------------------\n\nTITLE: Converting Detectron2 Checkpoint to MMDetection\nDESCRIPTION: This shell command converts a Detectron2 checkpoint to MMDetection format using the `detectron2_to_mmdet.py` script.  The script requires two arguments: the path to the Detectron2 checkpoint and the desired path for the converted MMDetection checkpoint. This conversion is a prerequisite for using Detectron2's released checkpoints for inference in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/how_to.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/detectron2_to_mmdet.py ${Detectron2 ckpt path} ${MMDetectron ckpt path}\n```\n\n----------------------------------------\n\nTITLE: Convert Images to COCO Format\nDESCRIPTION: This python script converts a set of images into the COCO format, generating a JSON file that describes the image data and annotations. This is useful when ground-truth labels are not available.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/images2coco.py \\\n    ${IMG_PATH} \\\n    ${CLASSES} \\\n    ${OUT} \\\n    [--exclude-extensions]\n```\n\n----------------------------------------\n\nTITLE: Configuration File Modification for Custom Dataset\nDESCRIPTION: This snippet shows how to modify the configuration file for a custom dataset. It involves modifying the `dataloader` and `model` sections to reflect the new dataset's classes and number of classes.  This ensures the model is trained and evaluated correctly on the custom data.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# 新的配置来自基础的配置以更好地说明需要修改的地方\n_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'\n\n# 1. 数据集设定\ndataset_type = 'CocoDataset'\nclasses = ('a', 'b', 'c', 'd', 'e')\ndata_root='path/to/your/'\n\ntrain_dataloader = dict(\n    batch_size=2,\n    num_workers=2,\n    dataset=dict(\n        type=dataset_type,\n        # 将类别名字添加至 `metainfo` 字段中\n        metainfo=dict(classes=classes),\n        data_root=data_root,\n        ann_file='train/annotation_data',\n        data_prefix=dict(img='train/image_data')\n        )\n    )\n\nval_dataloader = dict(\n    batch_size=1,\n    num_workers=2,\n    dataset=dict(\n        type=dataset_type,\n        test_mode=True,\n        # 将类别名字添加至 `metainfo` 字段中\n        metainfo=dict(classes=classes),\n        data_root=data_root,\n        ann_file='val/annotation_data',\n        data_prefix=dict(img='val/image_data')\n    )\n\ntest_dataloader = dict(\n    batch_size=1,\n    num_workers=2,\n    dataset=dict(\n        type=dataset_type,\n        test_mode=True,\n        # 将类别名字添加至 `metainfo` 字段中\n        metainfo=dict(classes=classes),\n        data_root=data_root,\n        ann_file='test/annotation_data',\n        data_prefix=dict(img='test/image_data')\n        )\n    )\n\n# 2. 模型设置\n\n# 将所有的 `num_classes` 默认值修改为 5（原来为80）\nmodel = dict(\n    roi_head=dict(\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                # 将所有的 `num_classes` 默认值修改为 5（原来为 80）\n                num_classes=5),\n            dict(\n                type='Shared2FCBBoxHead',\n                # 将所有的 `num_classes` 默认值修改为 5（原来为 80）\n                num_classes=5),\n            dict(\n                type='Shared2FCBBoxHead',\n                # 将所有的 `num_classes` 默认值修改为 5（原来为 80）\n                num_classes=5)],\n    # 将所有的 `num_classes` 默认值修改为 5（原来为 80）\n    mask_head=dict(num_classes=5)))\n```\n\n----------------------------------------\n\nTITLE: Phrase Grounding with User-Defined Noun Phrases\nDESCRIPTION: This shell command illustrates phrase grounding using the MM Grounding DINO model with user-defined noun phrases, avoiding potential NLTK extraction errors. The `--tokens-positive` argument specifies the character indices corresponding to the noun phrases in the text.  The `--pred-score-thr` sets the prediction score threshold.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/fruit.jpg \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'The picture contains watermelon, flower, and a white bottle.' \\\n        --tokens-positive \"[[[21,31]], [[45,59]]]\"  --pred-score-thr 0.12\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Pre-trained Model (Shell)\nDESCRIPTION: This command runs inference using a pre-trained model and saves the results in a new JSON file. It uses the specified configuration file and weights file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_pseudo-labeling_cat.py \\\n    grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Download and Extract Example Cat Images\nDESCRIPTION: Downloads a zip file containing example cat images from OpenMMLab's server and extracts its contents.  A 'data' directory is created to store the images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\nmkdir data && cd data\n\nwget https://download.openmmlab.com/mmyolo/data/cat_dataset.zip && unzip cat_dataset.zip\n```\n\n----------------------------------------\n\nTITLE: Preparing a model for publishing\nDESCRIPTION: This command helps users prepare their model for publishing by converting model weights to CPU tensors, deleting optimizer states, and computing the hash of the checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/publish_model.py ${INPUT_FILENAME} ${OUTPUT_FILENAME}\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Training in MMDetection (Shell)\nDESCRIPTION: This command launches a distributed training job across multiple GPUs.  It utilizes PyTorch's distributed training utilities to parallelize the training process. The command specifies the number of nodes, the node rank, the number of GPUs per node, and the master port and address for communication between processes. The `${NUM_GPUS}` variable should be set to the number of available GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node=${NUM_GPUS} --master_port=29506 --master_addr=\"127.0.0.1\" tools/train.py projects/DiffusionDet/configs/diffusiondet_r50_fpn_500-proposals_1-step_crop-ms-480-800-450k_coco.py\n```\n\n----------------------------------------\n\nTITLE: Benchmark FPS Example (benchmark.py)\nDESCRIPTION: This is an example usage of the `benchmark.py` script, demonstrating how to use it to measure the FPS of a Faster R-CNN model. Requires a config file and optionally a checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node=1 --master_port=29500 tools/analysis_tools/benchmark.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n       --launcher pytorch\n```\n\n----------------------------------------\n\nTITLE: Enabling torch.compile for RTMDet in MMDetection (Multi-GPU)\nDESCRIPTION: This shell script shows how to enable the `torch.compile` function for RTMDet with multiple GPUs on a single node. The `dist_train.sh` script is used for distributed training, and `--cfg-options compile=True` activates compilation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/rtmdet/rtmdet_s_8xb32-300e_coco.py 8 --cfg-options compile=True\n```\n\n----------------------------------------\n\nTITLE: Config file for Cascade Mask R-CNN with AugFPN and AutoAugment\nDESCRIPTION: This is a config file that inherits from base configs to customize the Cascade Mask R-CNN R50 model for the cityscapes dataset. It configures the model to use AugFPN, modifies the number of classes, and adds AutoAugment policies for training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# The new config inherits the base configs to highlight the necessary modification\n_base_ = [\n    '../_base_/models/cascade-mask-rcnn_r50_fpn.py',\n    '../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    # set None to avoid loading ImageNet pre-trained backbone,\n    # instead here we set `load_from` to load from COCO pre-trained detectors.\n    backbone=dict(init_cfg=None),\n    # replace neck from defaultly `FPN` to our new implemented module `AugFPN`\n    neck=dict(\n        type='AugFPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    # We also need to change the num_classes in head from 80 to 8, to match the\n    # cityscapes dataset's annotation. This modification involves `bbox_head` and `mask_head`.\n    roi_head=dict(\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                # change the number of classes from defaultly COCO to cityscapes\n                num_classes=8,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                # change the number of classes from defaultly COCO to cityscapes\n                num_classes=8,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                # change the number of classes from defaultly COCO to cityscapes\n                num_classes=8,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n        ],\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            # change the number of classes from default COCO to cityscapes\n            num_classes=8,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n\n# over-write `train_pipeline` for new added `AutoAugment` training setting\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='AutoAugment',\n        policies=[\n            [dict(\n                 type='Rotate',\n                 level=5,\n                 img_border_value=(124, 116, 104),\n                 prob=0.5)\n            ],\n            [dict(type='Rotate', level=7, img_border_value=(124, 116, 104)),\n             dict(\n                 type='TranslateX',\n                 level=5,\n                 prob=0.5,\n                 img_border_value=(124, 116, 104))\n            ],\n        ]),\n    dict(\n        type='RandomResize',\n        scale=[(2048, 800), (2048, 1024)],\n        keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs'),\n]\n\n# set batch_size per gpu, and set new training pipeline\ntrain_dataloader = dict(\n    batch_size=1,\n    num_workers=3,\n    # over-write `pipeline` with new training pipeline setting\n    dataset=dict(pipeline=train_pipeline))\n\n# Set optimizer\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001))\n\n# Set customized learning policy\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=10,\n        by_epoch=True,\n        milestones=[8],\n        gamma=0.1)\n]\n\n# train, val, test loop config\ntrain_cfg = dict(max_epochs=10, val_interval=1)\n\n# We can use the COCO pre-trained Cascade Mask R-CNN R50 model for a more stable performance initialization\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco/cascade_mask_rcnn_r50_fpn_1x_coco_20200203-9d4dcb24.pth'\n```\n\n----------------------------------------\n\nTITLE: Configuration File Example\nDESCRIPTION: Presents a complete example of a configuration file for training a Cascade Mask R-CNN R50 model with the custom `AugFPN` neck and data augmentation on the Cityscapes dataset. It includes modifications to the model, dataset, optimizer, and training schedule.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# 继承 base 配置，然后进行针对性修改\n_base_ = [\n    '../_base_/models/cascade-mask-rcnn_r50_fpn.py',\n    '../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    # 设置 `init_cfg` 为 None，表示不加载 ImageNet 预训练权重，\n    # 后续可以设置 `load_from` 参数用来加载 COCO 预训练权重\n    backbone=dict(init_cfg=None),\n    # 使用新增的 `AugFPN` 模块代替默认的 `FPN`\n    neck=dict(\n        type='AugFPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    # 我们也需要将 num_classes 从 80 修改为 8 来匹配 cityscapes 数据集标注\n    # 这个修改包括 `bbox_head` 和 `mask_head`.\n    roi_head=dict(\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                # 将 COCO 类别修改为 cityscapes 类别 \n                num_classes=8,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                # 将 COCO 类别修改为 cityscapes 类别 \n                num_classes=8,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                # 将 COCO 类别修改为 cityscapes 类别 \n                num_classes=8,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n        ],\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            # 将 COCO 类别修改为 cityscapes 类别 \n            num_classes=8,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n\n# 覆写 `train_pipeline`，然后新增 `AutoAugment` 训练配置\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='AutoAugment',\n        policies=[\n            [dict(\n                 type='Rotate',\n                 level=5,\n                 img_border_value=(124, 116, 104),\n                 prob=0.5)\n            ],\n            [dict(type='Rotate', level=7, img_border_value=(124, 116, 104)),\n             dict(\n                 type='TranslateX',\n                 level=5,\n                 prob=0.5,\n                 img_border_value=(124, 116, 104))\n            ],\n        ]),\n    dict(\n        type='RandomResize',\n        scale=[(2048, 800), (2048, 1024)],\n        keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs'),\n]\n\n# 设置每张显卡的批处理大小，同时设置新的训练 pipeline\ndata = dict(\n    samples_per_gpu=1,\n    workers_per_gpu=3,\n    train=dict(dataset=dict(pipeline=train_pipeline)))\n\n# 设置优化器\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001))\n\n# 设置定制的学习率策略\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=10,\n        by_epoch=True,\n        milestones=[8],\n        gamma=0.1)\n]\n\n# 训练，验证，测试配置\ntrain_cfg = dict(type='EpochBasedTrainLoop', max_epochs=10, val_interval=1)\nval_cfg = dict(type='ValLoop')\ntest_cfg = dict(type='TestLoop')\n\n# 我们采用 COCO 预训练过的 Cascade Mask R-CNN R50 模型权重作为初始化权重，可以得到更加稳定的性能\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco/cascade_mask_rcnn_r50_fpn_1x_coco_20200203-9d4dcb24.pth'\n```\n\n----------------------------------------\n\nTITLE: COCO Evaluator Configuration (3.x)\nDESCRIPTION: This code snippet demonstrates the COCO evaluator configuration in MMDetection 3.x. It specifies the evaluator type as `CocoMetric`, the annotation file, the evaluation metrics (`bbox` and `segm`), and `format_only=False` to trigger actual evaluation. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'annotations/instances_val2017.json',\n    metric=['bbox', 'segm'],\n    format_only=False)\n```\n\n----------------------------------------\n\nTITLE: Convert RegNet Model to MMDetection Model (regnet2mmdet.py)\nDESCRIPTION: This snippet shows how to convert a pre-trained RegNet model (trained with pycls) to the MMDetection style using `tools/model_converters/regnet2mmdet.py`. It takes the source and destination file paths as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/regnet2mmdet.py ${SRC} ${DST} [-h]\n```\n\n----------------------------------------\n\nTITLE: Testing SORT with Trained Detector (Shell Script)\nDESCRIPTION: This shell script tests the SORT tracker on the `motXX-half-val` dataset using a separately trained detector model. It uses the `dist_test_tracking.sh` script.  The `--detector` argument specifies the path to the trained detector checkpoint. It's configured to use 8 GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sort/README.md#_snippet_2\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8 --detector ${DETECTOR_CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Download GLIP Model\nDESCRIPTION: Downloads the pre-trained GLIP model checkpoint file from a specified URL to the current directory. This model will be used for the multimodal inference demo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\nwget https://download.openmmlab.com/mmdetection/v3.0/glip/glip_tiny_a_mmdet-b3654169.pth\n```\n\n----------------------------------------\n\nTITLE: Install Label-Studio and label-studio-ml-backend\nDESCRIPTION: Installs specific versions of Label-Studio and label-studio-ml-backend using pip. Specifying the versions ensures compatibility and reproducibility.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Installing Label-Studio may take some time, if the version is not found, please use the official source\npip install label-studio==1.7.2\npip install label-studio-ml==1.0.9\n```\n\n----------------------------------------\n\nTITLE: Running Referring Expression Image Captioning Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for referring expression image captioning using the X-Decoder model. It specifies the image, configuration file, pre-trained weights, and the referring text prompt. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/fruit.jpg configs/xdecoder-tiny_zeroshot_ref-caption.py --weights ../../xdecoder_focalt_last_novg.pt --text 'White tea pot'\n```\n\n----------------------------------------\n\nTITLE: Convert LVIS to OVD format for open-vocabulary fine-tuning\nDESCRIPTION: Converts the LVIS dataset to the OVD (Open-Vocabulary Detection) format for open-vocabulary fine-tuning. The script handles specific LVIS requirements.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/lvis2ovd.py data/coco/\n```\n\n----------------------------------------\n\nTITLE: Inheriting Configuration from a Single Base Config\nDESCRIPTION: This snippet shows how to inherit configurations from a single base configuration file in MMDetection.  By setting `_base_` to a file path, the current configuration inherits all the settings from that file. This promotes code reuse and modularity in configuring different models and training pipelines.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for Roboflow 100 Dataset\nDESCRIPTION: This BibTeX entry provides the citation information for the Roboflow 100 dataset. It includes the authors, title, year, and Eprint details for proper referencing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_9\n\nLANGUAGE: BibTeX\nCODE:\n```\n@misc{2211.13523,\nAuthor = {Floriana Ciaglia and Francesco Saverio Zuppichini and Paul Guerrie and Mark McQuade and Jacob Solawetz},\nTitle = {Roboflow 100: A Rich, Multi-Domain Object Detection Benchmark},\nYear = {2022},\nEprint = {arXiv:2211.13523},\n}\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-x-P6\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-x-P6 model. This configuration file is used to set up and run the RTMDet-x-P6 model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_x_p6_4xb8-300e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Using Cosine Annealing Learning Rate Scheduling in MMDetection\nDESCRIPTION: This code snippet shows how to use `CosineAnnealingLR` as learning rate scheduler with specific settings in the configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='CosineAnnealingLR',\n        T_max=8,\n        eta_min=lr * 1e-5,\n        begin=0,\n        end=8,\n        by_epoch=True)]\n\n\n```\n\n----------------------------------------\n\nTITLE: Convert MOT17 Annotations to COCO format\nDESCRIPTION: This script converts the official MOT17 dataset annotations to the COCO format. It also generates ReID data. The script takes input and output directories as arguments, splits the training set, and converts detections.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_dataset_prepare.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython ./tools/dataset_converters/mot2coco.py -i ./data/MOT17/ -o ./data/MOT17/annotations --split-train --convert-det\npython ./tools/dataset_converters/mot2reid.py -i ./data/MOT17/ -o ./data/MOT17/reid --val-split 0.2 --vis-threshold 0.3\n```\n\n----------------------------------------\n\nTITLE: Enabling Mosaic Data Augmentation in Faster R-CNN (Python)\nDESCRIPTION: This snippet demonstrates how to enable Mosaic data augmentation in MMDetection's Faster R-CNN. It configures the training pipeline to include `Mosaic` and `RandomAffine` transformations.  The `Mosaic` augmentation combines multiple images into one, and `RandomAffine` adjusts the image to counteract the scaling introduced by Mosaic. It utilizes `MultiImageMixDataset` to handle the mixed images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/how_to.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 直接打开 configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py ,增添如下字段\ndata_root = 'data/coco/'\ndataset_type = 'CocoDataset'\nimg_scale=(1333, 800)\n\ntrain_pipeline = [\n    dict(type='Mosaic', img_scale=img_scale, pad_val=114.0),\n    dict(\n        type='RandomAffine',\n        scaling_ratio_range=(0.1, 2),\n        border=(-img_scale[0] // 2, -img_scale[1] // 2)), # 图像经过马赛克处理后会放大4倍，所以我们使用仿射变换来恢复图像的大小。\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs'))\n]\n\ntrain_dataset = dict(\n    _delete_ = True, # 删除不必要的设置\n    type='MultiImageMixDataset',\n    dataset=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(type='LoadAnnotations', with_bbox=True)\n        ],\n        filter_empty_gt=False,\n    ),\n    pipeline=train_pipeline\n    )\n\ndata = dict(\n    train=train_dataset\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Download RTMDet Weights\nDESCRIPTION: This snippet creates 'work_dirs' directory, navigates into it and downloads the RTMDet model weights file using `wget`. The weights are required for running inference with the RTMDet model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\nmkdir work_dirs\ncd work_dirs\nwget https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet_m_8xb32-300e_coco/rtmdet_m_8xb32-300e_coco_20220719_112220-229f527c.pth\n```\n\n----------------------------------------\n\nTITLE: Online COCO Occluded/Separated Metric - Python\nDESCRIPTION: Configuration snippet to enable online evaluation of COCO occluded and separated mask recall during training by replacing the evaluator metric type in the config file. `ann_file` should point to the validation annotation file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\nval_evaluator = dict(\n    type='CocoOccludedSeparatedMetric',  # modify this\n    ann_file=data_root + 'annotations/instances_val2017.json',\n    metric=['bbox', 'segm'],\n    format_only=False)\ntest_evaluator = val_evaluator\n```\n\n----------------------------------------\n\nTITLE: Cityscapes to COCO Conversion\nDESCRIPTION: This script converts the Cityscapes dataset annotations into the COCO format.  It requires the `cityscapesscripts` package and uses multiple processes to speed up the conversion. The output annotations are saved in the specified output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install cityscapesscripts\n\npython tools/dataset_converters/cityscapes.py \\\n    ./data/cityscapes \\\n    --nproc 8 \\\n    --out-dir ./data/cityscapes/annotations\n```\n\n----------------------------------------\n\nTITLE: BBox Head Loss Calculation in MMDetection (Python)\nDESCRIPTION: This code snippet shows the `loss` method in a `BBoxHead` module, which calculates classification loss, classification accuracy, and bounding box regression loss. The method returns a dictionary containing these losses and metrics, which are used during model training and evaluation. Note that only values with keys containing 'loss' will be used for backpropagation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/conventions.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass BBoxHead(nn.Module):\n    ...\n    def loss(self, ...):\n        losses = dict()\n        # classification loss\n        losses['loss_cls'] = self.loss_cls(...)\n        # classification accuracy\n        losses['acc'] = accuracy(...)\n        # bbox regression loss\n        losses['loss_bbox'] = self.loss_bbox(...)\n        return losses\n```\n\n----------------------------------------\n\nTITLE: Download Coco Dataset (Shell)\nDESCRIPTION: This script downloads and extracts the COCO 2017 dataset, which is commonly used for object detection tasks. It's a prerequisite for running the semi-supervised object detection experiments described in the document.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/misc/download_dataset.py\n```\n\n----------------------------------------\n\nTITLE: Download MMDetection config and weights with MIM\nDESCRIPTION: This snippet downloads a pre-trained RTMDet model and its configuration file using MIM. This allows for quick setup and testing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nmim download mmdet --config rtmdet_tiny_8xb32-300e_coco --dest .\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection with MIM\nDESCRIPTION: This command installs MMDetection using MIM. This is a simpler method compared to installing from source if you only intend to use it as a dependency.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nmim install mmdet\n```\n\n----------------------------------------\n\nTITLE: Testing on MOTxx-halfval Dataset\nDESCRIPTION: This shell script command tests the tracking performance on the MOTxx-half-val dataset. It calls `dist_test_tracking.sh` with the StrongSORT configuration file, the number of GPUs, and the paths to the detector and ReID model checkpoints.  It requires the detector and ReID models to be pre-trained.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/strongsort/README.md#_snippet_3\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/strongsort/strongsort_yolox_x_8xb4-80e_crowdhuman-mot17halftrain_test-mot17halfval.py 8 --detector ${CHECKPOINT_PATH} --reid ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Fusion and Evaluation of Predictions (fuse_results.py)\nDESCRIPTION: This example fuses predictions from three models (Faster R-CNN, RetinaNet, and Cascade R-CNN) and evaluates their effectiveness. It requires the paths to the JSON result files from each model and the ground truth annotation file. Weights for each model can be specified.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ./faster-rcnn_r50-caffe_fpn_1x_coco.json \\\n       ./retinanet_r50-caffe_fpn_1x_coco.json \\\n       ./cascade-rcnn_r50-caffe_fpn_1x_coco.json \\\n       --annotation ./annotation.json \\\n       --weights 1 2 3\n```\n\n----------------------------------------\n\nTITLE: Convert RegNet Model to MMDetection Format (Python)\nDESCRIPTION: This script converts pre-trained RegNet models from the pycls format to the ResNet-style checkpoint format used by MMDetection. It takes the path to the pre-trained model and the desired output path as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -u tools/model_converters/regnet2mmdet.py ${PRETRAIN_PATH} ${STORE_PATH}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Specific Sub-module with Override Key (Python)\nDESCRIPTION: This code snippet demonstrates how to initialize a specific sub-module (`reg`) with a different configuration using the `override` key within `init_cfg`. The `override` dictionary specifies the initializer type and value for the `reg` sub-module, overriding the default configuration for other layers (`Conv1d`, `Conv2d`).  It also demonstrates how to set bias values.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# layers:\n# self.feat = nn.Conv1d(3, 1, 3)\n# self.reg = nn.Conv2d(3, 3, 3)\n# self.cls = nn.Linear(1,2)\n\ninit_cfg = dict(type='Constant',\n                  layer=['Conv1d','Conv2d'], val=1, bias=2,\n                  override=dict(type='Constant', name='reg', val=3, bias=4))\n# self.feat and self.cls will be initialized with \tdict(type='Constant', val=1, bias=2)\n# The module called 'reg' will be initialized with dict(type='Constant', val=3, bias=4)\n```\n\n----------------------------------------\n\nTITLE: Example Slurm Training Command (Shell)\nDESCRIPTION: This shell command provides an example of training ViTDet on a Slurm partition. It uses 16 GPUs, specifies a partition named 'dev', a job name, a configuration file, and a working directory on a shared file system.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/ViTDet/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nGPUS=16 ./tools/slurm_train.sh dev vitdet_mask_b projects/ViTDet/configs/vitdet_mask-rcnn_vit-b-mae_lsj-100e.py /nfs/xxxx/vitdet_mask-rcnn_vit-b-mae_lsj-100e\n```\n\n----------------------------------------\n\nTITLE: Testing on MOTxx-test Dataset\nDESCRIPTION: This shell script command tests the tracking algorithm on the MOTxx-test dataset and generates result files suitable for submission to the MOT Challenge.  It uses `dist_test_tracking.sh`, providing the configuration file, number of GPUs, and paths to the detector and ReID checkpoints. The results are stored in `./mot_20_test_res`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/strongsort/README.md#_snippet_4\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/strongsort/strongsort_yolox_x_8xb4-80e_crowdhuman-mot20train_test-mot20test.py 8 --detector ${CHECKPOINT_PATH} --reid ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Migrating COCO Evaluator Configuration in MMDetection\nDESCRIPTION: Shows how to migrate the COCO evaluator configuration from MMDetection v2.x to v3.x. In v3.x, evaluation is handled by dedicated Evaluator configurations (val_evaluator/test_evaluator) instead of being tied to the dataset definition.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type='CocoDataset',\n        ann_file=data_root + 'annotations/instances_val2017.json'))\nevaluation = dict(metric=['bbox', 'segm'])\n```\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'annotations/instances_val2017.json',\n    metric=['bbox', 'segm'],\n    format_only=False)\n```\n\n----------------------------------------\n\nTITLE: Testing the converted TensorFlow model\nDESCRIPTION: This command tests the converted EfficientDet model using the MMDetection testing script. It requires specifying the configuration file for the TensorFlow version of EfficientDet and the path to the converted checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython tools/test.py projects/EfficientDet/configs/tensorflow/efficientdet_effb0_bifpn_8xb16-crop512-300e_coco_tf.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for ByteTrack\nDESCRIPTION: This LaTeX snippet provides the citation information for the ByteTrack paper. It includes the title, authors, journal, and year of publication, which is essential for academic references.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{zhang2021bytetrack,\n  title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box},\n  author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},\n  journal={arXiv preprint arXiv:2110.06864},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Download MMDetection Config and Checkpoint (Tracking)\nDESCRIPTION: Downloads a pre-trained ByteTrack model's configuration and checkpoint using MIM for a tracking demo. The files are saved to the current directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nmim download mmdet --config bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval --dest .\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - R-101 Backbone, MS-train, 2x Schedule\nDESCRIPTION: This configuration file is tailored for VarifocalNet with a ResNet-101 backbone, employing multi-scale training and a 2x learning rate schedule on the COCO dataset. It configures training parameters specifically for this setup.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_r101_fpn_ms-2x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Example: Train QDTrack on single node multi-GPU\nDESCRIPTION: This snippet provides an example of training the QDTrack model on a single node using 8 GPUs with the dist_train.sh script and a specific configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_6\n\nLANGUAGE: shell script\nCODE:\n```\nbash ./tools/dist_train.sh configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Change directory to Detic project\nDESCRIPTION: This command changes the current directory to the Detic project folder within the broader project structure. This is a preliminary step before running any Detic-specific scripts or commands.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/Detic\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Mask R-CNN with DCN, GC Block (r=16), and X-101-FPN\nDESCRIPTION: This config file describes a Cascade Mask R-CNN model utilizing an X-101-FPN backbone. It incorporates Deformable Convolutions (DCN) in the c3-c5 stages and a Global Context (GC) block with a reduction ratio of 16 inserted after the 1x1 convolution layers of the backbone's c3-c5 stages. SyncBN is used, and the model is trained for 1x epochs using the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[config](./cascade-mask-rcnn_x101-32x4d-syncbn-dconv-c3-c5-r16-gcb-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - R-50 Backbone, MS-train, 2x Schedule\nDESCRIPTION: This configuration file is for training VarifocalNet with a ResNet-50 backbone, using multi-scale training (MS-train) and a 2x learning rate schedule on the COCO dataset. It specifies the training regime, including image scaling and learning rate adjustments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_r50_fpn_ms-2x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Training and Testing Configuration - Python\nDESCRIPTION: This snippet sets up the training and testing configurations, defining the types of loops to use (EpochBasedTrainLoop, ValLoop, TestLoop), the maximum number of epochs, and the validation interval. These configurations control the overall training and evaluation process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(\n    type='EpochBasedTrainLoop',  # 训练循环的类型，请参考 https://github.com/open-mmlab/mmengine/blob/main/mmengine/runner/loops.py\n    max_epochs=12,  # 最大训练轮次\n    val_interval=1)  # 验证间隔。每个 epoch 验证一次\nval_cfg = dict(type='ValLoop')  # 验证循环的类型\ntest_cfg = dict(type='TestLoop')  # 测试循环的类型\n```\n\n----------------------------------------\n\nTITLE: Train DINO with DeepSpeed on 8 GPUs (Bash)\nDESCRIPTION: This bash script demonstrates how to train the `dino-5scale_swin-l_deepspeed_8xb2-12e_coco.py` configuration using DeepSpeed across 8 GPUs. Assumes you are in the mmdetection root directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_largemodel/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd mmdetection\n./tools/dist_train.sh projects/example_largemodel/dino-5scale_swin-l_deepspeed_8xb2-12e_coco.py 8\n```\n\n----------------------------------------\n\nTITLE: Use PAFPN Neck in MMDetection Configuration (Python)\nDESCRIPTION: This snippet illustrates how to configure the MMDetection model to use the newly defined PAFPN neck. The configuration specifies the type of the neck as 'PAFPN' and includes parameters such as input channels, output channels, and the number of output feature maps.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nneck=dict(\n    type='PAFPN',\n    in_channels=[256, 512, 1024, 2048],\n    out_channels=256,\n    num_outs=5)\n```\n\n----------------------------------------\n\nTITLE: Fusing detection results from multiple models (Shell)\nDESCRIPTION: This script fuses detection results from multiple models using the Weighted Boxes Fusion (WBF) method. It supports configuration of weights, IoU threshold, confidence threshold, confidence type, and options for evaluating individual models and saving fusion results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ${PRED_RESULTS} \\\n       [--annotation ${ANNOTATION}] \\\n       [--weights ${WEIGHTS}] \\\n       [--fusion-iou-thr ${FUSION_IOU_THR}] \\\n       [--skip-box-thr ${SKIP_BOX_THR}] \\\n       [--conf-type ${CONF_TYPE}] \\\n       [--eval-single ${EVAL_SINGLE}] \\\n       [--save-fusion-results ${SAVE_FUSION_RESULTS}] \\\n       [--out-dir ${OUT_DIR}]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Raw Grounding Results (Shell)\nDESCRIPTION: This command visualizes the raw grounding results using a provided script.  It takes the data directory, JSON file, image directory, label map file, and output directory as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_raw.py data/cat/ cat_train_od_v1.json images --label-map-file cat_label_map.json -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for Empirical Attention Study\nDESCRIPTION: This LaTeX code provides the citation information for the \"An Empirical Study of Spatial Attention Mechanisms in Deep Networks\" paper. It includes the title, authors, journal, and year of publication for proper referencing in academic works.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/empirical_attention/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhu2019empirical,\n  title={An Empirical Study of Spatial Attention Mechanisms in Deep Networks},\n  author={Zhu, Xizhou and Cheng, Dazhi and Zhang, Zheng and Lin, Stephen and Dai, Jifeng},\n  journal={arXiv preprint arXiv:1904.05873},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Citing mmdetection with BibTeX\nDESCRIPTION: This BibTeX entry provides the information needed to cite the mmdetection project in academic publications. It includes the title, authors, journal, year, and arXiv identifier.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhao2024open,\n  title={An Open and Comprehensive Pipeline for Unified Object Grounding and Detection},\n  author={Zhao, Xiangyu and Chen, Yicheng and Xu, Shilin and Li, Xiangtai and Wang, Xinjiang and Li, Yining and Huang, Haian},\n  journal={arXiv preprint arXiv:2401.02361},\n  year={2024}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Modules\nDESCRIPTION: Shows one way to import custom modules by adding the import statement into `mmdet/models/necks/__init__.py`\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom .augfpn import AugFPN\n```\n\n----------------------------------------\n\nTITLE: Training with Object365v1 Dataset (8 Cards)\nDESCRIPTION: This shell script trains the MM Grounding DINO model using the Object365v1 dataset with distributed training on 8 GPUs. It executes the `tools/dist_train.sh` script with the specified configuration file and the number of GPUs (8).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py 8\n```\n\n----------------------------------------\n\nTITLE: Webcam Inference Demo Script\nDESCRIPTION: This script performs real-time object detection inference using a webcam. It requires a configuration file and a checkpoint file, and it supports specifying the device, camera ID, and confidence threshold.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython demo/webcam_demo.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--camera-id ${CAMERA-ID}] \\\n    [--score-thr ${SCORE_THR}]\n```\n\n----------------------------------------\n\nTITLE: Training QDTrack MOT Model on a Single GPU\nDESCRIPTION: This example demonstrates how to train the QDTrack MOT model on a specific GPU (GPU 2 in this case). It uses the `CUDA_VISIBLE_DEVICES` environment variable to select the GPU and then runs the `tools/train.py` script with the corresponding QDTrack configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_3\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=2 python tools/train.py configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py\n```\n\n----------------------------------------\n\nTITLE: Dump Detection Results - Shell\nDESCRIPTION: This command uses the `tools/test.py` script to generate and save detection results into a .pkl file, which can later be used for generating the confusion matrix or evaluating the COCO separated & occluded mask metric.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_43\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/test.py ${CONFIG} ${MODEL_PATH} --out results.pkl\n```\n\n----------------------------------------\n\nTITLE: Phrase Grounding Format (VG) Example\nDESCRIPTION: This is an example of the Phrase Grounding (VG) format. It includes filename, height, width, caption, and regions with bounding boxes, phrases, and token positions. Requires text descriptions of images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n{\"filename\": \"2405116.jpg\",\n \"height\": 375,\n \"width\": 500,\n \"grounding\":\n     {\"caption\": \"Two surfers walking down the shore. sand on the beach.\",\n      \"regions\": [\n            {\"bbox\": [206, 156, 282, 248], \"phrase\": \"Two surfers\", \"tokens_positive\": [[0, 3], [4, 11]]},\n            {\"bbox\": [303, 338, 443, 343], \"phrase\": \"sand\", \"tokens_positive\": [[36, 40]]},\n            {\"bbox\": [[327, 223, 421, 282], [300, 200, 400, 210]], \"phrase\": \"beach\", \"tokens_positive\": [[48, 53]]}\n               ]\n      }\n}\n```\n\n----------------------------------------\n\nTITLE: Group Normalization (GN) BibTeX Citation\nDESCRIPTION: This BibTeX entry provides the citation information for the Group Normalization paper. It includes the title, authors, booktitle, and year of publication, which are essential for properly attributing the work in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{wu2018group,\n  title={Group Normalization},\n  author={Wu, Yuxin and He, Kaiming},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  year={2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Torch Compile in MMDetection with RTMDet\nDESCRIPTION: Demonstrates how to enable `torch.compile` for training and testing with MMDetection, specifically using the RTMDet model. It includes examples for single-GPU and multi-GPU setups.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# 单卡\npython tools/train.py configs/rtmdet/rtmdet_s_8xb32-300e_coco.py  --cfg-options compile=True\n\n# 单机 8 卡\n./tools/dist_train.sh configs/rtmdet/rtmdet_s_8xb32-300e_coco.py 8 --cfg-options compile=True\n\n# 单机 8 卡 + AMP 混合精度训练\n./tools/dist_train.sh configs/rtmdet/rtmdet_s_8xb32-300e_coco.py 8 --cfg-options compile=True --amp\n```\n\n----------------------------------------\n\nTITLE: Define Faster R-CNN Testing Pipeline Configuration Python\nDESCRIPTION: Defines the testing data pipeline configuration for Faster R-CNN in MMDetection. This pipeline uses `MultiScaleFlipAug` for test-time augmentation, including resizing, random flipping, normalization, padding, and converting the image to a tensor before collecting the necessary keys for testing.  `img_norm_cfg` is used for normalization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/transforms.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\n```\n\n----------------------------------------\n\nTITLE: MOT Inference Example with Checkpoint\nDESCRIPTION: This example shows how to run `mot_demo.py` using a single checkpoint file that contains both the detector and tracking models, specified using the `--checkpoint` argument.  It uses a QDTrack configuration file and saves the output to mot.mp4.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_inference.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# Example 2: use --checkpoint\npython demo/mot_demo.py \\\n    demo/demo_mot.mp4 \\\n    configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py \\\n    --checkpoint https://download.openmmlab.com/mmtracking/mot/qdtrack/mot_dataset/qdtrack_faster-rcnn_r50_fpn_4e_mot17_20220315_145635-76f295ef.pth \\\n    --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Modify Config for video_based evaluation and testing\nDESCRIPTION: This snippet describes modification of the validation dataloader configuration to enable video_based evaluation and testing. The sampler type needs to be set to 'DefaultSampler', shuffle should be False and round_up should also be False.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sort/README.md#_snippet_5\n\nLANGUAGE: shell script\nCODE:\n```\nval_dataloader = dict(\n    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False))\n```\n\n----------------------------------------\n\nTITLE: Training Configuration (3.x)\nDESCRIPTION: This code snippet showcases the training configuration in MMDetection 3.x. It defines the train loop type, maximum epochs, validation interval, validation loop type, and test loop type.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(\n    type='EpochBasedTrainLoop',  # 训练循环的类型，请参考 https://github.com/open-mmlab/mmengine/blob/main/mmengine/runner/loops.py\n    max_epochs=12,  # 最大训练轮次\n    val_interval=2)  # 验证间隔。每 2 个 epoch 验证一次\nval_cfg = dict(type='ValLoop')  # 验证循环的类型\ntest_cfg = dict(type='TestLoop')  # 测试循环的类型\n```\n\n----------------------------------------\n\nTITLE: Keep Latest Model Configuration (3.x)\nDESCRIPTION: This snippet configures the maximum number of checkpoints to keep in MMDetection 3.x using `CheckpointHook` within `default_hooks`. The `max_keep_ckpts` parameter specifies the number of latest checkpoints to retain.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        max_keep_ckpts=3))\n```\n\n----------------------------------------\n\nTITLE: Install MMEngine and MMCV in Google Colab\nDESCRIPTION: These commands install MMEngine and MMCV in a Google Colab environment using MIM. The versions may need to be adjusted based on compatibility.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\n!pip3 install openmim\n!mim install mmengine\n!mim install \"mmcv>=2.0.0,<2.1.0\"\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Configuration (2.x)\nDESCRIPTION: This snippet shows the learning rate configuration in MMDetection 2.x, including the policy, warmup settings, step decay, and gamma.  It configures a step-based learning rate decay with a linear warmup.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlr_config = dict(\n    policy='step',  # Use multi-step learning rate strategy during training\n    warmup='linear',  # Use linear learning rate warmup\n    warmup_iters=500,  # End warmup at iteration 500\n    warmup_ratio=0.001,  # Coefficient for learning rate warmup\n    step=[8, 11],  # Learning rate decay at which epochs\n    gamma=0.1)  # Learning rate decay coefficient\n```\n\n----------------------------------------\n\nTITLE: Collecting Environment Information for MMDetection\nDESCRIPTION: This Python script defines a function `collect_env` to gather information about the running environment, including the MMDetection version and Git hash. The script prints the collected environment information. It requires the `mmengine` and `mmdet` packages.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.utils import get_git_hash\nfrom mmengine.utils.dl_utils import collect_env as collect_base_env\n\nimport mmdet\n\n\ndef collect_env():\n    \"\"\"Collect the information of the running environments.\"\"\"\n    env_info = collect_base_env()\n    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n    return env_info\n\n\nif __name__ == '__main__':\n    for name, val in collect_env().items():\n        print(f'{name}: {val}')\n```\n\n----------------------------------------\n\nTITLE: Resuming Training with ExpMomentumEMAHook in MMDetection\nDESCRIPTION: This snippet shows how to properly resume training when using `ExpMomentumEMAHook` in MMDetection. When resuming from a checkpoint, the `resume_from` parameter must be specified both globally and within the `ExpMomentumEMAHook` configuration to ensure correct weight loading. The example demonstrates how to modify the configuration file directly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# 直接打开 configs/yolox/yolox_s_8x8_300e_coco.py 修改所有 resume_from 字段\nresume_from=./work_dir/yolox_s_8x8_300e_coco/epoch_x.pth\ncustom_hooks=[...\n    dict(\n        type='ExpMomentumEMAHook',\n        resume_from=./work_dir/yolox_s_8x8_300e_coco/epoch_x.pth,\n        momentum=0.0001,\n        priority=49)\n    ]\n```\n\n----------------------------------------\n\nTITLE: Balloon Dataset Format (JSON)\nDESCRIPTION: Illustrates the original format of the balloon dataset. Each entry contains image data (base64 encoded), file attributes, filename, regions (polygon coordinates), and size. The 'regions' key holds the polygon coordinates defining object boundaries.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{'base64_img_data': '',\n 'file_attributes': {},\n 'filename': '34020010494_e5cb88e1c4_k.jpg',\n 'fileref': '',\n 'regions': {'0': {'region_attributes': {},\n   'shape_attributes': {'all_points_x': [1020,\n     1000,\n     994,\n     1003,\n     1023,\n     1050,\n     1089,\n     1134,\n     1190,\n     1265,\n     1321,\n     1361,\n     1403,\n     1428,\n     1442,\n     1445,\n     1441,\n     1427,\n     1400,\n     1361,\n     1316,\n     1269,\n     1228,\n     1198,\n     1207,\n     1210,\n     1190,\n     1177,\n     1172,\n     1174,\n     1170,\n     1153,\n     1127,\n     1104,\n     1061,\n     1032,\n     1020],\n    'all_points_y': [963,\n     899,\n     841,\n     787,\n     738,\n     700,\n     663,\n     638,\n     621,\n     619,\n     643,\n     672,\n     720,\n     765,\n     800,\n     860,\n     896,\n     942,\n     990,\n     1035,\n     1079,\n     1112,\n     1129,\n     1134,\n     1144,\n     1153,\n     1166,\n     1166,\n     1150,\n     1136,\n     1129,\n     1122,\n     1112,\n     1084,\n     1037,\n     989,\n     963],\n    'name': 'polygon'}}},\n 'size': 1115004}\n```\n\n----------------------------------------\n\nTITLE: Unfreezing Backbone Network During Training\nDESCRIPTION: This snippet demonstrates how to unfreeze a backbone network during training after it has been initially frozen. It shows how to configure a Faster R-CNN model with a ResNet backbone, initially freezing one stage of the backbone. A custom hook, `UnfreezeBackboneEpochBasedHook`, is added to the config, which unfreezes the specified stage after a certain number of epochs. The hook class needs to be implemented separately.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/how_to.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    # freeze one stage of the backbone network.\n    backbone=dict(frozen_stages=1),\n)\ncustom_hooks = [dict(type=\"UnfreezeBackboneEpochBasedHook\", unfreeze_epoch=1)]\n\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.model import is_model_wrapper\nfrom mmengine.hooks import Hook\nfrom mmdet.registry import HOOKS\n\n\n@HOOKS.register_module()\nclass UnfreezeBackboneEpochBasedHook(Hook):\n    \"\"\"Unfreeze backbone network Hook.\n\n    Args:\n        unfreeze_epoch (int): The epoch unfreezing the backbone network.\n    \"\"\"\n\n    def __init__(self, unfreeze_epoch=1):\n        self.unfreeze_epoch = unfreeze_epoch\n\n    def before_train_epoch(self, runner):\n        # Unfreeze the backbone network.\n        # Only valid for resnet.\n        if runner.epoch == self.unfreeze_epoch:\n            model = runner.model\n            if is_model_wrapper(model):\n                model = model.module\n            backbone = model.backbone\n            if backbone.frozen_stages >= 0:\n                if backbone.deep_stem:\n                    backbone.stem.train()\n                    for param in backbone.stem.parameters():\n                        param.requires_grad = True\n                else:\n                    backbone.norm1.train()\n                    for m in [backbone.conv1, backbone.norm1]:\n                        for param in m.parameters():\n                            param.requires_grad = True\n\n            for i in range(1, backbone.frozen_stages + 1):\n                m = getattr(backbone, f'layer{i}')\n                m.train()\n                for param in m.parameters():\n                    param.requires_grad = True\n\n```\n\n----------------------------------------\n\nTITLE: Upgrade MMDetection Model Version (upgrade_model_version.py)\nDESCRIPTION: This snippet demonstrates how to upgrade older MMDetection checkpoints to the newer version using `tools/model_converters/upgrade_model_version.py`. It takes an input checkpoint file and an output file path as arguments, and an optional argument to specify the number of classes. Note that it may not work with incompatible updates in newer versions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/upgrade_model_version.py ${IN_FILE} ${OUT_FILE} [-h] [--num-classes NUM_CLASSES]\n```\n\n----------------------------------------\n\nTITLE: Install DSDL from source\nDESCRIPTION: This code snippet details the steps to install DSDL from the source code. It involves cloning the DSDL repository, navigating to the cloned directory, and then running the setup script to install the package. It specifies the branch `schema-dsdl`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dsdl/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/opendatalab/dsdl-sdk.git -b schema-dsdl\ncd dsdl-sdk\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: COCO Format Conversion for Testing in MMDetection\nDESCRIPTION: This snippet shows how to convert images to the COCO format using the `images2coco.py` script, useful for testing models without ground truth annotations. It takes image path, class names and output path as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/images2coco.py \\\n    ${IMG_PATH} \\\n    ${CLASSES} \\\n    ${OUT} \\\n    [--exclude-extensions]\n```\n\n----------------------------------------\n\nTITLE: Training QDTrack with Slurm\nDESCRIPTION: This example shows how to train the QDTrack MOT model using the Slurm job scheduler. It sets various environment variables like PORT, GPUS_PER_NODE, and SRUN_ARGS, and then executes the `tools/slurm_train.sh` script with the appropriate parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_9\n\nLANGUAGE: shell脚本\nCODE:\n```\nPORT=29501 \\\nGPUS_PER_NODE=8 \\\nSRUN_ARGS=\"--quotatype=reserved\" \\\nbash ./tools/slurm_train.sh \\\nmypartition \\\nmottrack\nconfigs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py\n./work_dirs/QDTrack \\\n8\n```\n\n----------------------------------------\n\nTITLE: Training Grounding DINO Model (Shell)\nDESCRIPTION: This script trains the Grounding DINO model using distributed training. It specifies the configuration file, the number of GPUs to use, and the working directory for storing training outputs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/mm_grounding_dino/grounding_dino_swin-t_finetune_8xb4_20e_cat.py 8 --work-dir cat_work_dir\n```\n\n----------------------------------------\n\nTITLE: Configuring MobileNet Backbone in MMDetection\nDESCRIPTION: Configures the MobileNet backbone within the MMDetection model configuration. The `type` parameter specifies the name of the backbone module, and other parameters such as `arg1` and `arg2` are passed to the backbone's constructor.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    ...\n    backbone=dict(\n        type='MobileNet',\n        arg1=xxx,\n        arg2=xxx),\n    ...\n```\n\n----------------------------------------\n\nTITLE: Verifying MMCV Installation\nDESCRIPTION: Illustrates how to check if MMCV is correctly installed and its operations are accessible using a Python command. This helps confirm that the library is properly set up and can be imported.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import mmcv; import mmcv.ops'\n```\n\n----------------------------------------\n\nTITLE: Model Conversion Script Execution (Shell)\nDESCRIPTION: This command executes a Python script to convert a DiffusionDet model checkpoint to the MMDetection format.  It requires the paths to both the DiffusionDet checkpoint and the desired MMDetection checkpoint as input. This conversion is necessary to use the DiffusionDet model within the MMDetection framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython projects/DiffusionDet/model_converters/diffusiondet_resnet_to_mmdet.py ${DiffusionDet ckpt path} ${MMDetectron ckpt path}\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN Config Path\nDESCRIPTION: This configuration file specifies the settings for training and evaluating a Faster R-CNN model using a ResNeSt backbone within the MMDetection framework.  It includes details about the backbone architecture, feature pyramid network (FPN), and the RPN and RoI heads.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n[config](./faster-rcnn_s50_fpn_syncbn-backbone+head_ms-range-1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Training QDTrack using dist_train.sh\nDESCRIPTION: This command trains the QDTrack model on the mot17-half-train dataset using the distributed training script. It specifies the configuration file and the number of GPUs to use (8 in this case).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/qdtrack/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_train.sh configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Mask RCNN model using a RegNetX-3.2GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n[config](./mask-rcnn_regnetx-3.2GF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Mask RCNN model using a RegNetX-800MF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n[config](./mask-rcnn_regnetx-800MF_fpn_ms-poly-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Blur Corruptions\nDESCRIPTION: This command runs a robustness test with blur corruptions. `${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] --corruptions blur\n```\n\n----------------------------------------\n\nTITLE: Training ReID on single GPU\nDESCRIPTION: This command trains a ReID model using a specified configuration file on a single GPU. It leverages the `tools/train.py` script provided by MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/reid/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py configs/reid/reid_r50_8xb32-6e_mot17train80_test-mot17val20.py\n```\n\n----------------------------------------\n\nTITLE: Testing with iSAID Dataset (Python)\nDESCRIPTION: This snippet shows how to test a trained model on the iSAID dataset using MMDetection. It uses the `tools/test.py` script with the same configuration file used for training and specifies the path to the trained model checkpoint. This script evaluates the trained model on the dataset's test split.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/iSAID/README_zh-CN.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython tools/test.py projects/iSAID/configs/mask_rcnn_r50_fpn_1x_isaid.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Inference with Existing Dataset Vocabulary\nDESCRIPTION: This script runs the image demo using an existing dataset vocabulary, using the specified image, configuration, and model paths, along with an optional score threshold and palette.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython demo/image_demo.py \\\n  ${IMAGE_PATH} \\\n  ${CONFIG_PATH} \\\n  ${MODEL_PATH} \\\n  --texts lvis \\\n  --pred-score-thr 0.5 \\\n  --palette 'random'\n```\n\n----------------------------------------\n\nTITLE: Install Multi-modal Dependencies\nDESCRIPTION: This command installs the necessary dependencies for multi-modal algorithms in MMDetection, either from source using pip or by installing the mmdet[multimodal] package via MIM.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\n# if source\npip install -r requirements/multimodal.txt\n\n# if wheel\nmim install mmdet[multimodal]\n```\n\n----------------------------------------\n\nTITLE: Modifying Dataset Classes\nDESCRIPTION: This snippet demonstrates how to modify the dataset's classes in the configuration file. By modifying the `metainfo` dictionary within the `train_dataloader`, `val_dataloader`, and `test_dataloader`, the dataset will only consider the specified classes during training and evaluation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclasses = ('person', 'bicycle', 'car')\ntrain_dataloader = dict(\n    dataset=dict(\n        metainfo=dict(classes=classes))\n    )\nval_dataloader = dict(\n    dataset=dict(\n        metainfo=dict(classes=classes))\n    )\ntest_dataloader = dict(\n    dataset=dict(\n        metainfo=dict(classes=classes))\n    )\n```\n\n----------------------------------------\n\nTITLE: Install PyTorch (CPU)\nDESCRIPTION: Installs PyTorch and torchvision for CPU-only environments using conda. The 'cpuonly' flag ensures that CUDA is not required.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nconda install pytorch torchvision cpuonly -c pytorch\n```\n\n----------------------------------------\n\nTITLE: Defining Search Parameters for MOT Tracker (Python)\nDESCRIPTION: This snippet demonstrates how to define the parameters to be searched within the `tracker` configuration of a MOT model. By providing a list of values for parameters like `obj_score_thr` and `match_iou_thr`, the `mot_param_search.py` script can automatically test all possible combinations and identify the optimal parameter settings. The original values are replaced with lists of values to be tested.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_analysis_tools.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel=dict(\n    tracker=dict(\n        type='BaseTracker',\n        obj_score_thr=[0.4, 0.5, 0.6],\n        match_iou_thr=[0.4, 0.5, 0.6, 0.7]\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Convert MMDetection model to ONNX using MMDeploy\nDESCRIPTION: This code snippet demonstrates how to convert a Faster R-CNN model from MMDetection to ONNX format using the `torch2onnx` function from MMDeploy. It also extracts pipeline information for inference using the MMDeploy SDK.  Dependencies: mmdeploy, mmdet. The script requires paths to the image, work directory, save file, deploy config, model config, and model checkpoint. It outputs the converted ONNX model and associated metadata files in the specified work directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/deploy.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy.apis import torch2onnx\nfrom mmdeploy.backend.sdk.export_info import export2SDK\n\nimg = 'demo/demo.jpg'\nwork_dir = 'mmdeploy_models/mmdet/onnx'\nsave_file = 'end2end.onnx'\ndeploy_cfg = '../mmdeploy/configs/mmdet/detection/detection_onnxruntime_dynamic.py'\nmodel_cfg = 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\nmodel_checkpoint = 'faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\ndevice = 'cpu'\n\n# 1. convert model to onnx\ntorch2onnx(img, work_dir, save_file, deploy_cfg, model_cfg,\n           model_checkpoint, device)\n\n# 2. extract pipeline info for inference by MMDeploy SDK\nexport2SDK(deploy_cfg, model_cfg, work_dir, pth=model_checkpoint,\n           device=device)\n```\n\n----------------------------------------\n\nTITLE: Testing Instance Segmentation on COCO2017 with X-Decoder\nDESCRIPTION: This command tests the X-Decoder model for instance segmentation on the COCO2017 dataset. It uses the `dist_test.sh` script for distributed testing and specifies the configuration file, pre-trained weights, and number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-instance_coco.py xdecoder_focalt_last_novg.pt 8\n```\n\n----------------------------------------\n\nTITLE: Enabling torch.compile for RTMDet in MMDetection (Single GPU)\nDESCRIPTION: This shell command demonstrates how to enable the `torch.compile` function for the RTMDet model in MMDetection using a single GPU. The `--cfg-options compile=True` flag is added to the training script to activate compilation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py configs/rtmdet/rtmdet_s_8xb32-300e_coco.py  --cfg-options compile=True\n```\n\n----------------------------------------\n\nTITLE: V3Det Dataset Directory Structure\nDESCRIPTION: This code snippet shows the directory structure of the V3Det dataset, including the location of images, category tree, category names, training set annotations, and validation set annotations. It is essential for accessing and using the dataset effectively.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/v3det/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ndata/\n    V3Det/\n        images/\n            <category_node>/\n                |────<image_name>.png\n                ...\n            ...\n        annotations/\n            |────v3det_2023_v1_category_tree.json       # Category tree\n            |────category_name_13204_v3det_2023_v1.txt  # Category name\n            |────v3det_2023_v1_train.json               # Train set\n            |────v3det_2023_v1_val.json                 # Validation set\n```\n\n----------------------------------------\n\nTITLE: Training ReID on multiple GPUs\nDESCRIPTION: This script launches the training of a ReID model on multiple GPUs using the distributed training script `tools/dist_train.sh`. It requires a configuration file and the number of GPUs to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/reid/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_train.sh configs/reid/reid_r50_8xb32-6e_mot17train80_test-mot17val20.py 8\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection with Multi-Modality Dependencies\nDESCRIPTION: This command installs MMDetection in editable mode with extra dependencies for multi-modality tasks, enabling features such as image captioning and text-to-image retrieval. It should be executed from the root directory of the MMDetection repository.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/gradio_demo/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -e \".[multimodal]\"\n```\n\n----------------------------------------\n\nTITLE: Running Inference for Phrase Grounding (Shell)\nDESCRIPTION: This command runs inference on a phrase grounding dataset using a pre-trained Grounding DINO model and configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_pseudo-labeling_flickr30k.py \\\n    grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: YOLO Anchor Optimization with K-Means - Shell\nDESCRIPTION: This script optimizes YOLO anchors using the k-means clustering algorithm. It requires specifying the configuration file, input shape (width and height), and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_39\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm k-means --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation Entry (BibTeX)\nDESCRIPTION: This is a BibTeX entry for citing the DiffusionDet paper. It includes the title, authors, journal, and year of publication. This entry can be used in LaTeX documents to properly cite the DiffusionDet paper.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_6\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{chen2022diffusiondet,\n      title={DiffusionDet: Diffusion Model for Object Detection},\n      author={Chen, Shoufa and Sun, Peize and Song, Yibing and Luo, Ping},\n      journal={arXiv preprint arXiv:2211.09788},\n      year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Cityscapes Testing with 8 GPUs in MMDetection\nDESCRIPTION: This snippet tests the Mask R-CNN model on the Cityscapes test dataset with 8 GPUs using the `tools/dist_test.sh` script. It generates txt and png files in the `./work_dirs/cityscapes_metric/` directory for submitting to the official evaluation server. Note that some config changes might be needed as explained in the documentation\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n    configs/cityscapes/mask-rcnn_r50_fpn_1x_cityscapes.py \\\n    checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe51d5a.pth \\\n    8\n```\n\n----------------------------------------\n\nTITLE: Testing Command - Bash\nDESCRIPTION: This command tests the trained dummy ResNet model. It executes the `tools/test.py` script with the configuration file and a specified checkpoint path, `CHECKPOINT_PATH`, representing the saved model weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_project/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython tools/test.py projects/example_project/configs/faster-rcnn_dummy-resnet_fpn_1x_coco.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Converting RTMDet with SDK info dump using Shell\nDESCRIPTION: This shell script converts an RTMDet model to TensorRT and also dumps information required by the MMDeploy SDK. It adds the `--dump-info` flag to the model conversion command.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/deploy.py \\\n  configs/mmdet/detection/detection_tensorrt_static-640x640.py \\\n  ${PATH_TO_MMDET}/configs/rtmdet/rtmdet_s_8xb32-300e_coco.py \\\n  checkpoint/rtmdet_s_8xb32-300e_coco_20220905_161602-387a891e.pth \\\n  demo/resources/det.jpg \\\n  --work-dir ./work_dirs/rtmdet-sdk \\\n  --device cuda:0 \\\n  --show \\\n  --dump-info  # dump sdk info\n```\n\n----------------------------------------\n\nTITLE: RetinaNet with PISA Configuration (COCO Dataset)\nDESCRIPTION: This snippet provides the configuration file name for RetinaNet with PISA enabled, trained on the COCO dataset. It outlines the training configuration for a RetinaNet object detector with a ResNet-50-FPN backbone, utilizing the PISA sampling strategy to improve the model's ability to detect objects, especially smaller ones.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[config](./retinanet-r50_fpn_pisa_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Display Result Dictionary Structure - Python\nDESCRIPTION: This code snippet demonstrates how to use the `rich.pretty` library to display the structure of the result dictionary obtained from the MMDetection inferencer. It imports the `pprint` function and uses it to print the `result` object with a maximum length of 4.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom rich.pretty import pprint\n\nresult = inferencer('demo/demo.jpg')\npprint(result, max_length=4)\n```\n\n----------------------------------------\n\nTITLE: Convert COCO 2017 to OVD Format (Python)\nDESCRIPTION: This script converts the COCO 2017 training data to the OVD (Open Vocabulary Detection) format. It executes the `coco2ovd.py` script from the `tools/dataset_converters/` directory, taking the COCO data directory as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2ovd.py data/coco/\n```\n\n----------------------------------------\n\nTITLE: Installing MMPreTrain via pip\nDESCRIPTION: This command installs the MMPreTrain library using pip, the Python package installer. This is an alternative method for installing MMPreTrain if MIM is not available or preferred.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/classification/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install mmpretrain\n```\n\n----------------------------------------\n\nTITLE: Training Command for Custom Faster R-CNN (Bash)\nDESCRIPTION: This bash script demonstrates how to train a custom Faster R-CNN model in MMDetection using multiple GPUs. It specifies the configuration file and the working directory for saving the training outputs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/single_stage_as_rpn.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ntools/dist_train.sh\nconfigs/faster_rcnn/faster-rcnn_r50_fpn_fcos-rpn_1x_coco.py\n--work-dir  /work_dirs/faster-rcnn_r50_fpn_fcos-rpn_1x_coco\n```\n\n----------------------------------------\n\nTITLE: Testing SORT MOT Model on CPU\nDESCRIPTION: This example shows how to test the SORT MOT model on a CPU. It disables GPUs and specifies the detector checkpoint using the `--detector` argument.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_11\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/test_tracking.py configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py --detector ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Testing Command for Proposal Evaluation (Bash)\nDESCRIPTION: This bash script demonstrates how to test the proposal quality of an RPN model in MMDetection using multiple GPUs. It specifies the configuration file and the path to the trained model weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/single_stage_as_rpn.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ntools/dist_test.sh\nconfigs/rpn/fcos-rpn_r50_fpn_1x_coco.py\n--work_dirs /faster-rcnn_r50_fpn_fcos-rpn_1x_coco/epoch_12.pth\n```\n\n----------------------------------------\n\nTITLE: Cascade R-CNN S101 Config Path\nDESCRIPTION: Configuration file specifying the setup for Cascade R-CNN with a ResNeSt-101 backbone.  It includes details for training and evaluation within the MMDetection framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n[config](./cascade-rcnn_s101_fpn_syncbn-backbone+head_ms-range-1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot COCO2017 Validation - Single GPU - Shell\nDESCRIPTION: This shell command evaluates the MM Grounding DINO model on the COCO2017 validation dataset using a single GPU. It runs the `tools/test.py` script with the model configuration and weights file to perform zero-shot evaluation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Directory Structure\nDESCRIPTION: This code snippet shows the required directory structure for the COCO dataset, which is necessary for training DetectoRS. It includes the 'annotations', 'train2017', 'val2017', 'test2017', and 'stuffthingmaps' subdirectories inside the 'coco' directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/detectors/README.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n|   |   ├── stuffthingmaps\n```\n\n----------------------------------------\n\nTITLE: QDTrack Citation\nDESCRIPTION: This LaTeX snippet provides the citation information for the QDTrack paper.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/qdtrack/README.md#_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{pang2021quasi,\n  title={Quasi-dense similarity learning for multiple object tracking},\n  author={Pang, Jiangmiao and Qiu, Linlu and Li, Xia and Chen, Haofeng and Li, Qi and Darrell, Trevor and Yu, Fisher},\n  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages={164--173},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: SSD512 with PISA Configuration (COCO Dataset)\nDESCRIPTION: This snippet provides the configuration file name for SSD512 with PISA enabled, trained on the COCO dataset. It outlines the training configuration for an SSD512 object detector with a VGG16 backbone, utilizing the PISA sampling strategy to improve the model's ability to detect objects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[config](./ssd512_pisa_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Running Open Vocabulary Instance Segmentation Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for open vocabulary instance segmentation using the X-Decoder model. It specifies the image, configuration file, pre-trained weights, and text prompts for instance segmentation. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/owls.jpeg configs/xdecoder-tiny_zeroshot_open-vocab-instance_coco.py --weights ../../xdecoder_focalt_last_novg.pt --texts owl\n```\n\n----------------------------------------\n\nTITLE: Runtime Configuration (2.x)\nDESCRIPTION: This snippet shows the runtime configuration in MMDetection 2.x, including settings for cuDNN benchmark, OpenCV threads, multiprocessing start method, distributed parameters, log level, loading, and resuming.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ncudnn_benchmark = False\nopencv_num_threads = 0\nmp_start_method = 'fork'\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\n\n\n```\n\n----------------------------------------\n\nTITLE: Evaluating on Zero-Shot COCO2017 val (8 Cards)\nDESCRIPTION: This shell script evaluates the MM Grounding DINO model on the COCO2017 validation dataset in a zero-shot setting using distributed testing with 8 GPUs. It executes the `tools/dist_test.sh` script with the specified configuration file, model weights, and the number of GPUs (8).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth 8\n```\n\n----------------------------------------\n\nTITLE: DeepFashion Data Directory Structure (Shell)\nDESCRIPTION: This code snippet shows the expected file and directory structure for the DeepFashion dataset when used with MMDetection.  The structure includes the Anno, Eval, and Img directories, with specific subdirectories for segmentation annotations, evaluation lists, and image data respectively.  This structure is necessary for the MMDetection framework to correctly load and process the dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepfashion/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── DeepFashion\n│   │   ├── In-shop\n|   │   │   ├── Anno\n|   │   │   │   ├── segmentation\n|   │   │   │   |   ├── DeepFashion_segmentation_train.json\n|   │   │   │   |   ├── DeepFashion_segmentation_query.json\n|   │   │   │   |   ├── DeepFashion_segmentation_gallery.json\n|   │   │   │   ├── list_bbox_inshop.txt\n|   │   │   │   ├── list_description_inshop.json\n|   │   │   │   ├── list_item_inshop.txt\n|   │   │   │   └── list_landmarks_inshop.txt\n|   │   │   ├── Eval\n|   │   │   │   └── list_eval_partition.txt\n|   │   │   ├── Img\n|   │   │   │   ├── img\n|   │   │   │   │   ├──XXX.jpg\n|   │   │   │   ├── img_highres\n|   │   │   │   └── ├──XXX.jpg\n```\n\n----------------------------------------\n\nTITLE: Citation in Latex\nDESCRIPTION: This is the Latex code to cite the BoxInst paper. It provides the necessary information, including the title, authors, booktitle, and year of publication, needed for properly referencing the BoxInst algorithm.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/boxinst/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{tian2020boxinst,\n  title     =  {{BoxInst}: High-Performance Instance Segmentation with Box Annotations},\n  author    =  {Tian, Zhi and Shen, Chunhua and Wang, Xinlong and Chen, Hao},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Panoptic Segmentation on ADE20K with X-Decoder\nDESCRIPTION: This command tests the X-Decoder model for panoptic segmentation on the ADE20K dataset. It uses the `dist_test.sh` script for distributed testing and specifies the configuration file, pre-trained weights, and number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-panoptic_ade20k.py xdecoder_focalt_best_openseg.pt 8\n```\n\n----------------------------------------\n\nTITLE: Inheriting Configuration from Multiple Base Configs\nDESCRIPTION: This code demonstrates inheriting configurations from multiple base configuration files in MMDetection. By assigning a list of file paths to `_base_`, the current configuration combines the settings from all listed files. This allows for highly modular and reusable configurations by composing different components such as models, datasets, schedules, and runtime settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/config.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/mask-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n```\n\n----------------------------------------\n\nTITLE: Migrating Train Pipeline Config (MMDetection 3.x)\nDESCRIPTION: This code snippet shows the train pipeline configuration in MMDetection 3.x. It includes data loading, annotation loading, resizing, random flipping, and packing data inputs. The `Normalize` and `Pad` transforms are removed, and `PackDetInputs` replaces `Collect` and `DefaultFormatBundle`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs')\n]\n\n```\n\n----------------------------------------\n\nTITLE: Download ADE20K Dataset in MMDetection (Python)\nDESCRIPTION: This snippet downloads and unzips the ADE20K dataset using a Python script provided in the MMDetection tools directory. The dataset will be saved in the `data` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name ade20k_2016 --save-dir data --unzip\n```\n\n----------------------------------------\n\nTITLE: COCO Panoptic Annotation Format\nDESCRIPTION: This snippet illustrates the required structure and keys of the annotation JSON file for the COCO Panoptic dataset. The `images` list contains image metadata, `annotations` list has segmentation information, and `categories` list includes both foreground and background categories. The `segments_info` within `annotations` holds detailed segment information, including `id`, `category_id`, `iscrowd`, `bbox`, and `area`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_dataset.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n'images': [\n    {\n        'file_name': '000000001268.jpg',\n        'height': 427,\n        'width': 640,\n        'id': 1268\n    },\n    ...\n]\n\n'annotations': [\n    {\n        'filename': '000000001268.jpg',\n        'image_id': 1268,\n        'segments_info': [\n            {\n                'id':8345037,  # One-to-one correspondence with the id in the annotation map.\n                'category_id': 51,\n                'iscrowd': 0,\n                'bbox': (x1, y1, w, h),  # The bbox of the background is the outer rectangle of its mask.\n                'area': 24315\n            },\n            ...\n        ]\n    },\n    ...\n]\n\n'categories': [  # including both foreground categories and background categories\n    {'id': 0, 'name': 'person'},\n    ...\n ]\n```\n\n----------------------------------------\n\nTITLE: Install TrackEval\nDESCRIPTION: Installs the TrackEval library from its GitHub repository. This library is used for evaluating tracking performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/JonathonLuiten/TrackEval.git\n```\n\n----------------------------------------\n\nTITLE: Runtime Configuration (3.x)\nDESCRIPTION: This code snippet illustrates the runtime configuration in MMDetection 3.x. It configures cudnn benchmark, multiprocessing settings (start method and number of OpenCV threads), distributed parameters, the log level, loading from a checkpoint, and resuming from a checkpoint. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nenv_cfg = dict(\n    cudnn_benchmark=False,\n    mp_cfg=dict(mp_start_method='fork',\n                opencv_num_threads=0),\n    dist_cfg=dict(backend='nccl'))\nlog_level = 'INFO'\nload_from = None\nresume = False\n```\n\n----------------------------------------\n\nTITLE: Evaluating Test Dataset (mmdetection)\nDESCRIPTION: This command evaluates the model on the test dataset using the specified configuration file and model weights. The `tools/test.py` script calculates metrics such as Average Precision (AP) and Average Recall (AR) to assess the model's performance on object detection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/grounding_dino/grounding_dino_swin-t_finetune_8xb2_20e_cat.py https://download.openmmlab.com/mmdetection/v3.0/grounding_dino/groundingdino_swint_ogc_mmdet-822d7e9d.pth\n```\n\n----------------------------------------\n\nTITLE: Convert Flickr30k dataset to ODVG format (goldg2odvg.py)\nDESCRIPTION: This script converts the Flickr30k portion of the GoldG dataset to the ODVG format, generating `final_flickr_separateGT_train_vg.json` in the `data/flickr30k_entities` directory.\nDependencies: goldg2odvg.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/goldg2odvg.py data/flickr30k_entities/final_flickr_separateGT_train.json\n```\n\n----------------------------------------\n\nTITLE: YouTube-VIS to COCO Conversion (2019)\nDESCRIPTION: Converts YouTube-VIS 2019 dataset annotations to COCO format. The script uses `youtubevis2coco.py` to convert YouTube-VIS 2019 annotations.  The script takes the input path to the YouTube-VIS 2019 dataset and the output path for the COCO annotations, and specifies the version as 2019.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_dataset_prepare.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# YouTube-VIS 2019\npython ./tools/dataset_converters/youtubevis2coco.py -i ./data/youtube_vis_2019 -o ./data/youtube_vis_2019/annotations --version 2019\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Severities 0, 2, 4\nDESCRIPTION: This command runs a robustness test with specific corruption severities (0, 2, and 4).`${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file. `${EVAL_METRICS}` is the metric to evaluate.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --severities 0 2 4\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Faster RCNN model using a RegNetX-3.2GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n[config](./faster-rcnn_regnetx-3.2GF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Modifying validation dataloader configuration\nDESCRIPTION: This configuration snippet modifies the validation dataloader to enable video-based evaluation and testing by setting shuffle to False and round_up to False.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/qdtrack/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nval_dataloader = dict(\n    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False))\n\n```\n\n----------------------------------------\n\nTITLE: Video Inference Demo Script\nDESCRIPTION: This script performs object detection inference on a video file. It requires a video file, a configuration file, and a checkpoint file. It supports specifying the device, confidence threshold, output file, and display options.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_demo.py \\\n    ${VIDEO_FILE} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--device ${GPU_ID}] \\\n    [--score-thr ${SCORE_THR}] \\\n    [--out ${OUT_FILE}] \\\n    [--show] \\\n    [--wait-time ${WAIT_TIME}]\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Custom Corruptions\nDESCRIPTION: This command runs a robustness test with a custom set of image corruptions (`gaussian_noise`, `zoom_blur`, and `snow`). It also supports evaluating metrics. `${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file. `${EVAL_METRICS}` is the metric to evaluate.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --corruptions gaussian_noise zoom_blur snow\n```\n\n----------------------------------------\n\nTITLE: Optimize YOLO Anchors (optimize_anchors.py) - K-means\nDESCRIPTION: This snippet shows how to optimize YOLO anchors using the K-means clustering algorithm with `tools/analysis_tools/optimize_anchors.py`.  It requires a configuration file, the `--algorithm k-means` option, the input shape, and an output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm k-means --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: GoldG (GQA) to ODVG Conversion\nDESCRIPTION: This command converts the GQA portion of the GoldG dataset's JSON annotation file into the ODVG format, suitable for training the MM-GDINO-T model. It utilizes the `goldg2odvg.py` script from the `tools/dataset_converters` directory, taking the path to the GQA JSON file as input. The script generates a new JSON file containing the converted annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/goldg2odvg.py data/gqa/final_mixed_train_no_coco.json\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN S101 Config Path\nDESCRIPTION: Path to the configuration file for Mask R-CNN with a ResNeSt-101 backbone within MMDetection.  This config defines the model architecture, training schedule, and evaluation metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n[config](./mask-rcnn_s101_fpn_syncbn-backbone+head_ms-1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Enable Visualization in TrackVisualizationHook (Shell Script)\nDESCRIPTION: This shell script snippet shows how to enable the drawing of prediction results by setting `draw=True` in the `TrackVisualizationHook` within the `default_hooks` configuration.  This configuration is used to turn on the visualization feature during object tracking tasks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_visualization.md#_snippet_0\n\nLANGUAGE: shell script\nCODE:\n```\ndefault_hooks = dict(visualization=dict(type='TrackVisualizationHook', draw=True))\n```\n\n----------------------------------------\n\nTITLE: Download MM Grounding DINO-T Weight - Shell\nDESCRIPTION: This shell command downloads the MM Grounding DINO-T model weights from the specified URL using `wget`. This allows users to conveniently download the model weights for demonstration purposes, ensuring they are available in the current path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwget load_from = 'https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth' # noqa\n```\n\n----------------------------------------\n\nTITLE: Install MMEngine using pip\nDESCRIPTION: Installs MMEngine using pip instead of MIM. This command requires to follow MMEngine installation guides.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npip install mmengine\n```\n\n----------------------------------------\n\nTITLE: Output of Training Speed Analysis (analyze_logs.py)\nDESCRIPTION: This is the expected output format from running the `cal_train_time` function in `analyze_logs.py`. It displays the slowest and fastest epochs, the standard deviation of training time across epochs, and the average iteration time.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n-----Analyze train time of work_dirs/some_exp/20190611_192040.log.json-----\nslowest epoch 11, average time is 1.2024\nfastest epoch 1, average time is 1.1909\ntime std over epochs is 0.0028\naverage iter time: 1.1959 s/iter\n```\n\n----------------------------------------\n\nTITLE: Install forked lvis-api with pip\nDESCRIPTION: This shell command installs the forked lvis-api using pip. It directly installs from the specified git repository.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/lvis/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/lvis-dataset/lvis-api.git\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-l-Swin-B\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-l-Swin-B model. This configuration file is used to specify the parameters for the RTMDet-l-Swin-B model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_l_swin_b_4xb32-100e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN S101 Config Path\nDESCRIPTION: Configuration file for training/evaluating Faster R-CNN with a ResNeSt-101 backbone and FPN. The config specifies details about the network architecture, data augmentation strategies, and training hyperparameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n[config](./faster-rcnn_s101_fpn_syncbn-backbone+head_ms-range-1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN S101 Model Download\nDESCRIPTION: Download link for the pre-trained weights of a Faster R-CNN model with a ResNeSt-101 backbone and FPN. These weights can be used for transfer learning or direct inference on object detection tasks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/resnest/faster_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/faster_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco_20201006_021058-421517f1.pth)\n```\n\n----------------------------------------\n\nTITLE: Dynamic R-CNN Citation (LaTeX)\nDESCRIPTION: This LaTeX snippet provides the citation information for the Dynamic R-CNN paper. It includes the authors, title, journal, and year of publication. This citation can be used in academic papers or other documents to properly credit the work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dynamic_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{DynamicRCNN,\n    author = {Hongkai Zhang and Hong Chang and Bingpeng Ma and Naiyan Wang and Xilin Chen},\n    title = {Dynamic {R-CNN}: Towards High Quality Object Detection via Dynamic Training},\n    journal = {arXiv preprint arXiv:2004.06002},\n    year = {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Running Referring Expression Segmentation Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for referring expression segmentation using the X-Decoder model. It specifies the image, configuration file, pre-trained weights, and the referring text prompt. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/fruit.jpg configs/xdecoder-tiny_zeroshot_open-vocab-ref-seg_refcocog.py --weights ../../xdecoder_focalt_last_novg.pt  --text \"The larger watermelon. The front white flower. White tea pot.\"\n```\n\n----------------------------------------\n\nTITLE: Dump Results to Files - Python\nDESCRIPTION: This code snippet shows how to export the predictions and visualizations to files by setting the `out_dir` argument to the desired output directory and setting `no_save_pred` to `False`. This saves the prediction results to the specified directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ninferencer('demo/demo.jpg', out_dir='outputs/', no_save_pred=False)\n```\n\n----------------------------------------\n\nTITLE: Detecting Anomalous Parameters in MMDetection\nDESCRIPTION: This snippet describes how to set `detect_anomalous_params=True` in the optimizer configuration to find unused parameters during training. This feature requires MMCV version 1.4.1 or higher.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_15\n\nLANGUAGE: text\nCODE:\n```\noptimizer_config=dict(detect_anomalous_params=True)\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Training Jobs on Single Node\nDESCRIPTION: This snippet demonstrates how to run multiple training jobs on a single node with multiple GPUs. It shows setting different ports for each job to avoid communication conflicts. The CUDA_VISIBLE_DEVICES environment variable is used to assign specific GPUs to each job.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_5\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 ./tools/dist_train.sh ${CONFIG_FILE} 4\n```\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 ./tools/dist_train.sh ${CONFIG_FILE} 4\n```\n\n----------------------------------------\n\nTITLE: Specify MMEngine Dependency Version Range\nDESCRIPTION: Defines the acceptable version range for the mmengine dependency.  It requires mmengine to be greater than or equal to version 0.7.1 but less than version 1.0.0. This constraint ensures the project uses a compatible version of mmengine, avoiding potential conflicts or errors.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/requirements/mminstall.txt#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\nmmengine>=0.7.1,<1.0.0\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Faster RCNN model with a RegNetX-400MF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-400MF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-400MF_fpn_mstrain_3x_coco_20210526_095112-e1967c37.pth)\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Cascade Mask RCNN model using a RegNetX-800MF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_34\n\nLANGUAGE: text\nCODE:\n```\n[config](./cascade-mask-rcnn_regnetx-800MF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Test Faster R-CNN and Specify Topk (analyze_results.py)\nDESCRIPTION: This example tests a Faster R-CNN model, specifies the `topk` parameter to 50, and saves the images to the `results/` directory. It requires a configuration file (`configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py`) and a prediction file (`result.pkl`).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       result.pkl \\\n       results \\\n       --topk 50\n```\n\n----------------------------------------\n\nTITLE: DDOD ATSS Configuration\nDESCRIPTION: This config file is used for DDOD-ATSS with a ResNet-50 backbone. It specifies the training schedule, the model architecture, and the dataset to be used for training or evaluation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ddod/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n[config](./ddod_r50_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Benchmark FPS (benchmark.py)\nDESCRIPTION: This snippet demonstrates how to benchmark Frames Per Second (FPS) using `tools/analysis_tools/benchmark.py`. It utilizes distributed computing with a single GPU to measure the model's forward propagation and post-processing speed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\npython -m torch.distributed.launch --nproc_per_node=1 --master_port=${PORT} tools/analysis_tools/benchmark.py \\\n    ${CONFIG} \\\n    [--checkpoint ${CHECKPOINT}] \\\n    [--repeat-num ${REPEAT_NUM}] \\\n    [--max-iter ${MAX_ITER}] \\\n    [--log-interval ${LOG_INTERVAL}] \\\n    --launcher pytorch\n```\n\n----------------------------------------\n\nTITLE: Retrying Failed Datasets Training\nDESCRIPTION: This snippet demonstrates how to retry training on a specific list of failed datasets using the `RETRY_PATH` variable. This variable specifies the path to a text file containing a list of dataset names to be retried.  It is necessary to navigate to the `projects/RF100-Benchmark/` directory first for the command to work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nRETRY_PATH=failed_dataset_list.txt bash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Optimizer Wrapper Constructor in MMDetection\nDESCRIPTION: This code snippet demonstrates defining a custom optimizer wrapper constructor by inheriting from `DefaultOptiWrapperConstructor` and registering it using `OPTIM_WRAPPER_CONSTRUCTORS`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.optim import DefaultOptiWrapperConstructor\n\nfrom mmdet.registry import OPTIM_WRAPPER_CONSTRUCTORS\nfrom .my_optimizer import MyOptimizer\n\n\n@OPTIM_WRAPPER_CONSTRUCTORS.register_module()\nclass MyOptimizerWrapperConstructor(DefaultOptimWrapperConstructor):\n\n    def __init__(self,\n                 optim_wrapper_cfg: dict,\n                 paramwise_cfg: Optional[dict] = None):\n\n    def __call__(self, model: nn.Module) -> OptimWrapper:\n\n        return optim_wrapper\n\n```\n\n----------------------------------------\n\nTITLE: Configuring BERT for Grounding DINO in Python\nDESCRIPTION: This Python code snippet demonstrates how to download and save the BERT language model locally for use with Grounding DINO. It uses the `transformers` library to download the pre-trained \"bert-base-uncased\" model and tokenizer, then saves them to a specified local path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import BertConfig, BertModel\nfrom transformers import AutoTokenizer\n\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False, config=config)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nconfig.save_pretrained(\"your path/bert-base-uncased\")\nmodel.save_pretrained(\"your path/bert-base-uncased\")\ntokenizer.save_pretrained(\"your path/bert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: MOT Model Inference Example (Detector)\nDESCRIPTION: This example demonstrates MOT inference using a specified detector model.  It utilizes a pre-trained Faster R-CNN model for object detection within the tracking pipeline. The command specifies the input video, configuration file, detector weights, and output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_interference.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# 示例 1：不指定 --checkpoint 使用 --detector\npython demo/mot_demo.py \\\n    demo/demo_mot.mp4 \\\n    configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py \\\n    --detector \\\n    https://download.openmmlab.com/mmtracking/mot/faster_rcnn/faster-rcnn_r50_fpn_4e_mot17-half-64ee2ed4.pth \\\n    --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Testing and Inference\nDESCRIPTION: This command tests a trained Mask R-CNN model using the provided configuration file and the specified model weights.  It runs the `tools/test.py` script, pointing to the configuration file and the path to the trained model's weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/balloon/mask-rcnn_r50-caffe_fpn_ms-poly-1x_balloon.py work_dirs/mask-rcnn_r50-caffe_fpn_ms-poly-1x_balloon/epoch_12.pth\n```\n\n----------------------------------------\n\nTITLE: Citation for V3Det Dataset\nDESCRIPTION: This LaTeX code snippet provides the citation information for the V3Det dataset. It is used to properly attribute the dataset when used in research publications or projects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/v3det/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{wang2023v3det,\n      title = {V3Det: Vast Vocabulary Visual Detection Dataset},\n      author = {Wang, Jiaqi and Zhang, Pan and Chu, Tao and Cao, Yuhang and Zhou, Yujie and Wu, Tong and Wang, Bin and He, Conghui and Lin, Dahua},\n      booktitle = {The IEEE International Conference on Computer Vision (ICCV)},\n      month = {October},\n      year = {2023}\n}\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch CUDA Availability Check in Python\nDESCRIPTION: This snippet checks if PyTorch is correctly installed and can utilize CUDA operations. It prints True if CUDA is available and False otherwise.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import torch; print(torch.cuda.is_available())'\n```\n\n----------------------------------------\n\nTITLE: Converting Detectron2 Checkpoint to MMDetection (Shell)\nDESCRIPTION: This shell command demonstrates how to convert a Detectron2 pretrained weight to an MMDetection-compatible checkpoint structure using the provided conversion script. This is a prerequisite for using Detectron2 pretrained models within MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/how_to.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/detectron2_to_mmdet.py ${Detectron2 ckpt path} ${MMDetectron ckpt path}。\n```\n\n----------------------------------------\n\nTITLE: Citation for Libra R-CNN (CVPR 2019)\nDESCRIPTION: This LaTeX snippet provides the citation information for the original Libra R-CNN paper presented at CVPR 2019.  It includes the title, authors, conference, and year of publication, allowing proper attribution when using or referencing the Libra R-CNN framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/libra_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{pang2019libra,\n  title={Libra R-CNN: Towards Balanced Learning for Object Detection},\n  author={Pang, Jiangmiao and Chen, Kai and Shi, Jianping and Feng, Huajun and Ouyang, Wanli and Dahua Lin},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Preprocessing ODinW Dataset Categories\nDESCRIPTION: This script preprocesses the annotated JSON files for the ODinW dataset by overriding the categories. It's required for evaluating ODinW35, which uses custom prompts. The script generates new annotation files without overwriting the original ones.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython configs/mm_grounding_dino/odinw/override_category.py data/odinw/\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Cascade Mask RCNN model using a RegNetX-1.6GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_37\n\nLANGUAGE: text\nCODE:\n```\n[config](./cascade-mask-rcnn_regnetx-1.6GF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Install and Login to OpenXLab CLI tools (Bash)\nDESCRIPTION: This set of commands installs and authenticates the OpenXLab CLI tools, required to download datasets from OpenDataLab. The first command installs or updates the openmim package, the second installs the openxlab package, and the third logs the user into the OpenXLab platform.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\npip install -U openmim\n```\n\nLANGUAGE: Bash\nCODE:\n```\npip install -U openxlab\n```\n\nLANGUAGE: Bash\nCODE:\n```\nopenxlab login\n```\n\n----------------------------------------\n\nTITLE: Video-Based Evaluation Configuration (Python)\nDESCRIPTION: This python code snippet shows the configuration to switch to video-based evaluating and testing. When val_dataloader sampler type is set to 'DefaultSampler', the evaluation will be based on video.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/bytetrack/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nval_dataloader = dict(\n    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False))\n\n```\n\n----------------------------------------\n\nTITLE: Tweaking FocalLoss Weight - Python\nDESCRIPTION: This configuration snippet modifies the `loss_weight` of the `FocalLoss`.  Setting the `loss_weight` to 0.5 scales the contribution of the classification loss during multi-task learning, influencing the overall training process and balancing the impact of different loss components.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=2.0,\n    alpha=0.25,\n    loss_weight=0.5)\n```\n\n----------------------------------------\n\nTITLE: Optimizer Configuration (2.x)\nDESCRIPTION: This snippet shows the optimizer configuration in MMDetection 2.x, including the optimizer type, learning rate, momentum, and weight decay. It also includes the gradient clipping configuration, which is set to None to disable it.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\noptimizer = dict(\n    type='SGD',  # Optimizer: Stochastic Gradient Descent\n    lr=0.02,  # Base learning rate\n    momentum=0.9,  # SGD with momentum\n    weight_decay=0.0001)  # Weight decay\noptimizer_config = dict(grad_clip=None)  # Configuration for gradient clipping, set to None to disable\n```\n\n----------------------------------------\n\nTITLE: Testing ViTDet Model (Bash)\nDESCRIPTION: This bash command tests the ViTDet model using a specified configuration file and checkpoint path.  It uses the tools/test.py script from MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/ViTDet/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython tools/test.py projects/ViTDet/configs/vitdet_mask-rcnn_vit-b-mae_lsj-100e.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Configuring Visualizer and Backends in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to configure the Visualizer and its backends in MMDetection using a configuration file. It shows how to set the type of Visualizer and specify the visualization backends, such as LocalVisBackend, for storing visualization results locally.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/visualization.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nvis_backends = [dict(type='LocalVisBackend')]\nvisualizer = dict(\n    type='DetLocalVisualizer',\n    vis_backends=vis_backends,\n    name='visualizer')\n```\n\n----------------------------------------\n\nTITLE: Plotting loss/mAP curves from training logs (Shell)\nDESCRIPTION: This script analyzes training log files to plot loss and mAP curves. It requires the 'seaborn' package to be installed. The script takes arguments to specify keys, evaluation interval, title, legend, backend, style, and output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve [--keys ${KEYS}] [--eval-interval ${EVALUATION_INTERVAL}] [--title ${TITLE}] [--legend ${LEGEND}] [--backend ${BACKEND}] [--style ${STYLE}] [--out ${OUT_FILE}]\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Mask RCNN model with a RegNetX-4GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-4GF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-4GF_fpn_mstrain-poly_3x_coco_20210602_032621-00f0331c.pth)\n```\n\n----------------------------------------\n\nTITLE: Configure TeacherStudentValLoop (Python)\nDESCRIPTION: This snippet configures the TeacherStudentValLoop, which replaces the standard ValLoop to evaluate both the teacher and student models during training. This is beneficial for monitoring the performance of both models in the joint training framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nval_cfg = dict(type='TeacherStudentValLoop')\n```\n\n----------------------------------------\n\nTITLE: Single-GPU Testing for Robustness\nDESCRIPTION: This shell command runs a single-GPU test to evaluate model performance using image corruptions. It takes a configuration file, checkpoint file, and optional arguments for specifying the output file and evaluation metrics. The command invokes the `test_robustness.py` script, which applies specified image corruptions and measures the model's performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# single-gpu testing\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}]\n```\n\n----------------------------------------\n\nTITLE: Perform backend model inference with ONNX Runtime\nDESCRIPTION: This code snippet demonstrates how to perform inference on an ONNX model converted from MMDetection using ONNX Runtime through MMDeploy APIs. It builds a task processor and a backend model, processes the input image, performs inference, and visualizes the results. Dependencies: mmdeploy, mmdet, torch. The script requires paths to the deploy config, model config, ONNX model, and input image. It outputs a visualization of the detection results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/deploy.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdeploy.apis.utils import build_task_processor\nfrom mmdeploy.utils import get_input_shape, load_config\nimport torch\n\ndeploy_cfg = '../mmdeploy/configs/mmdet/detection/detection_onnxruntime_dynamic.py'\nmodel_cfg = 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\ndevice = 'cpu'\nbackend_model = ['mmdeploy_models/mmdet/onnx/end2end.onnx']\nimage = 'demo/demo.jpg'\n\n# read deploy_cfg and model_cfg\ndeploy_cfg, model_cfg = load_config(deploy_cfg, model_cfg)\n\n# build task and backend model\ntask_processor = build_task_processor(model_cfg, deploy_cfg, device)\nmodel = task_processor.build_backend_model(backend_model)\n\n# process input image\ninput_shape = get_input_shape(deploy_cfg)\nmodel_inputs, _ = task_processor.create_input(image, input_shape)\n\n# do model inference\nwith torch.no_grad():\n    result = model.test_step(model_inputs)\n\n# visualize results\ntask_processor.visualize(\n    image=image,\n    model=model,\n    result=result[0],\n    window_name='visualize',\n    output_file='output_detection.png')\n```\n\n----------------------------------------\n\nTITLE: Using AvoidCUDAOOM to handle CUDA OOM errors in MMDetection\nDESCRIPTION: This snippet demonstrates two ways to use `AvoidCUDAOOM` in MMDetection to prevent GPU out-of-memory errors. The first method uses `AvoidCUDAOOM.retry_if_cuda_oom` to retry a function call after attempting to free GPU memory. The second method uses `AvoidCUDAOOM` as a decorator for a function, which achieves the same result. It assumes that `some_function` and `function` are defined elsewhere and may cause CUDA OOM errors.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.utils import AvoidCUDAOOM\n\noutput = AvoidCUDAOOM.retry_if_cuda_oom(some_function)(input1, input2)\n```\n\n----------------------------------------\n\nTITLE: Save Best Model Configuration (2.x)\nDESCRIPTION: This snippet configures saving the best model during evaluation in MMDetection 2.x. The `save_best` parameter determines when to save the model with the best evaluation metric.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nevaluation = dict(\n    save_best='auto')\n```\n\n----------------------------------------\n\nTITLE: Download LVIS class name embeddings\nDESCRIPTION: This command downloads pre-computed CLIP embeddings for LVIS (Large Vocabulary Instance Segmentation) dataset class names. These embeddings are used by the zero-shot classifier during inference to recognize objects based on their textual descriptions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget -P datasets/metadata https://raw.githubusercontent.com/facebookresearch/Detic/main/datasets/metadata/lvis_v1_clip_a%2Bcname.npy\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - R-101 Backbone, 1x Schedule\nDESCRIPTION: This configuration file trains VarifocalNet with a ResNet-101 backbone and a 1x learning rate schedule on the COCO dataset. It sets up the model and training pipeline for object detection tasks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_r101_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Fixing Object365 v2 Category Names (Python)\nDESCRIPTION: This script corrects incorrect category names in the Object365 v2 dataset annotations. It processes the original JSON annotation file and generates a new file with corrected names.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/fix_o365_names.py\n```\n\n----------------------------------------\n\nTITLE: Upgrading MMDetection model version\nDESCRIPTION: This command upgrades a previous MMDetection checkpoint to the new version. It takes an input file and an output file as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/upgrade_model_version.py ${IN_FILE} ${OUT_FILE} [-h] [--num-classes NUM_CLASSES]\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection from source\nDESCRIPTION: This snippet clones the MMDetection repository, navigates into it, and installs it in editable mode. Editable mode allows local code changes to take effect without reinstalling.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone https://github.com/open-mmlab/mmdetection.git\ncd mmdetection\npip install -v -e .\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Model Testing with No Ground Truth\nDESCRIPTION: This shell script executes the distributed testing script `tools/dist_test.sh` for a given model configuration and checkpoint file across multiple GPUs.  It requires specifying the number of GPUs to use, when ground truth isn't available.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n# 单节点多 GPU 测试\nbash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    ${GPU_NUM} \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Performing Closed-Set Object Detection with MM Grounding DINO\nDESCRIPTION: This shell command demonstrates how to perform closed-set object detection using the MM Grounding DINO model. It runs the `image_demo.py` script with a specified image, configuration file, model weights, and a set of predefined categories (coco). The `--texts '$: coco'` argument specifies the categories to detect, and the output will be saved to `outputs/vis/animals.png`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/animals.png \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts '$: coco'\n```\n\n----------------------------------------\n\nTITLE: Citation of Simple Copy-Paste paper in LaTeX\nDESCRIPTION: This snippet provides the LaTeX code for citing the Simple Copy-Paste paper. It includes the title, authors, conference, pages, and year of publication. This citation can be used in academic papers or other documents referencing this work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/simple_copy_paste/README.md#_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\n@inproceedings{ghiasi2021simple,\n  title={Simple copy-paste is a strong data augmentation method for instance segmentation},\n  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={2918--2928},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN S50 Config Path\nDESCRIPTION: This configuration file defines the architecture and training parameters for a Mask R-CNN model with a ResNeSt-50 backbone and FPN. It's part of the MMDetection framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n[config](./mask-rcnn_s50_fpn_syncbn-backbone+head_ms-1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: CPU Model Testing with No Ground Truth\nDESCRIPTION: This shell command disables GPU usage by setting the `CUDA_VISIBLE_DEVICES` environment variable to -1 and then runs the single GPU testing script. This enables testing the model on the CPU with no ground truth.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n# CPU 测试：禁用 GPU 并运行单 GPU 测试脚本\nexport CUDA_VISIBLE_DEVICES=-1\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Training ViTDet with Slurm (Bash)\nDESCRIPTION: This bash script trains the ViTDet model using Slurm, leveraging multiple GPUs.  It requires the GPUS environment variable to be set, along with PARTITION, JOB_NAME, CONFIG_FILE, and WORK_DIR.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/ViTDet/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nGPUS=${GPUS} ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-m\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-m model. The config file likely defines the model's structure, training hyperparameters, and evaluation settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_m_8xb32-300e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Dependency Specification\nDESCRIPTION: This snippet defines the dependencies required for the MMDetection project. It includes version constraints for mmcv and mmengine, specifying the acceptable ranges. Other dependencies like scipy, torch, torchvision, and urllib3 are listed without specific version requirements, but with an upper bound for urllib3.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/requirements/readthedocs.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmmcv>=2.0.0rc4,<2.2.0\nmmengine>=0.7.1,<1.0.0\nscipy\ntorch\ntorchvision\nurllib3<2.0.0\n```\n\n----------------------------------------\n\nTITLE: Overriding Backbone Configuration in MMDetection\nDESCRIPTION: This snippet demonstrates how to override specific parts of a base configuration, such as the backbone of a Mask R-CNN model, using `_delete_=True` to remove old keys before replacing them with new ones. This allows for modifying the model architecture without rewriting the entire configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n_base_ = '../mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256))),\n        init_cfg=dict(type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w32')),\n    neck=dict(...))\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN S101 Training Log\nDESCRIPTION: Link to the training log (JSON file) for the Mask R-CNN model using a ResNeSt-101 backbone. This log contains detailed information on the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_11\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/resnest/mask_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco/mask_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco-20201005_215831.log.json)\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for Robustness Benchmarking\nDESCRIPTION: This LaTeX code provides a citation for the 'Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming' paper. It includes the title, authors, journal, and year of publication, useful for academic referencing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_9\n\nLANGUAGE: latex\nCODE:\n```\n@article{michaelis2019winter,\n  title={Benchmarking Robustness in Object Detection:\n    Autonomous Driving when Winter is Coming},\n  author={Michaelis, Claudio and Mitzkus, Benjamin and\n    Geirhos, Robert and Rusak, Evgenia and\n    Bringmann, Oliver and Ecker, Alexander S. and\n    Bethge, Matthias and Brendel, Wieland},\n  journal={arXiv:1907.07484},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Weather Corruptions\nDESCRIPTION: This command runs a robustness test with weather corruptions. `${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] --corruptions weather\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Mask R-CNN with X-101-FPN in MMDetection\nDESCRIPTION: This config file defines a Cascade Mask R-CNN model employing an X-101-FPN backbone. It utilizes SyncBN.  The model is trained for 1x epochs using the COCO dataset. This configuration does not include a Global Context block or Deformable Convolution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[config](./cascade-mask-rcnn_x101-32x4d-syncbn_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: COCO Annotation Format (JSON)\nDESCRIPTION: Defines the JSON structure for COCO annotations, including images, annotations, and categories. It specifies the required keys and data types for each component, such as image ID, file name, width, height, annotation ID, category ID, segmentation (RLE or polygon), area, bounding box, and iscrowd flag.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"images\": [image],\n    \"annotations\": [annotation],\n    \"categories\": [category]\n}\n\nimage = {\n    \"id\": int,\n    \"width\": int,\n    \"height\": int,\n    \"file_name\": str,\n}\n\nannotation = {\n    \"id\": int,\n    \"image_id\": int,\n    \"category_id\": int,\n    \"segmentation\": RLE or [polygon],\n    \"area\": float,\n    \"bbox\": [x,y,width,height], # (x, y) are the coordinates of the upper left corner of the bbox\n    \"iscrowd\": 0 or 1,\n}\n\ncategories = [{\n    \"id\": int,\n    \"name\": str,\n    \"supercategory\": str,\n}]\n```\n\n----------------------------------------\n\nTITLE: Running Open Vocabulary Semantic Segmentation Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for open vocabulary semantic segmentation using the X-Decoder model.  It specifies the image, configuration file, pre-trained weights, and text prompts for segmentation. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/animals.png configs/xdecoder-tiny_zeroshot_open-vocab-semseg_coco.py --weights ../../xdecoder_focalt_last_novg.pt --texts zebra.giraffe\n```\n\n----------------------------------------\n\nTITLE: Configuring LoggerHook in MMDetection (Python)\nDESCRIPTION: This code snippet configures the LoggerHook to log training information at specified intervals. The interval parameter determines how frequently the training logs are updated. This hook is essential for monitoring the training process and performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(logger=dict(type='LoggerHook', interval=50))\n\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection with Tracking Requirements from Source\nDESCRIPTION: Clones the MMDetection repository and installs it in editable mode, including the requirements for tracking tasks. The '-r requirements/tracking.txt' installs tracking related dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/open-mmlab/mmdetection.git\ncd mmdetection\npip install -v -e . -r requirements/tracking.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading Roboflow Datasets (Shell)\nDESCRIPTION: This command downloads the Roboflow 100 datasets using a shell script. The script leverages the Roboflow API key and Python package to download and organize the datasets into a structured directory. It requires the Roboflow API key to be set and the roboflow package to be installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/RF100-Benchmark/\nbash scripts/download_datasets.sh\n```\n\n----------------------------------------\n\nTITLE: Testing SparseInst with MMDetection\nDESCRIPTION: This command tests the trained SparseInst model using the specified configuration file and checkpoint path in MMDetection. It utilizes the `tools/test.py` script and requires providing the configuration file and the path to the model's checkpoint file via the `${CHECKPOINT_PATH}` environment variable.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/SparseInst/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython tools/test.py projects/SparseInst/configs/sparseinst_r50_iam_8xb8-ms-270k_coco.py ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Installing Multimodal Dependencies for GLIP using pip\nDESCRIPTION: This command installs the necessary dependencies for multimodal tasks, specifically related to GLIP, using pip. It reads the requirements from the `requirements/multimodal.txt` file. This step is crucial for setting up the environment for GLIP.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/glip/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd $MMDETROOT\n\n# source installation\npip install -r requirements/multimodal.txt\n```\n\n----------------------------------------\n\nTITLE: Download RTMDet Weights\nDESCRIPTION: Downloads the pretrained RTMDet model weights from OpenMMLab's server using wget. The weights are stored in a 'work_dirs' directory created within the mmdetection directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\nmkdir work_dirs\ncd work_dirs\nwget https://download.openmmlab.com/mmdetection/v3.0/rtmdet/rtmdet_m_8xb32-300e_coco/rtmdet_m_8xb32-300e_coco_20220719_112220-229f527c.pth\n```\n\n----------------------------------------\n\nTITLE: Single GPU Training (Shell)\nDESCRIPTION: This command starts a single GPU training run for a specified configuration file using the `dist_train.sh` script. It takes the configuration file path and the number of GPUs (1 in this case) as arguments. An optional third argument specifies the directory to save the training outputs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# 当前位于 projects/RF100-Benchmark/\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 1\n# 如果想指定保存路径\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 1 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Mask RCNN model with a RegNetX-3.2GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco/mask_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco_20200521_202221.log.json)\n```\n\n----------------------------------------\n\nTITLE: Installing X-Decoder Dependencies via Pip\nDESCRIPTION: This command installs the necessary dependencies for the multimodal functionalities of MMDetection using pip.  It reads the requirements from the `requirements/multimodal.txt` file. It is a prerequisite for using the X-Decoder.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements/multimodal.txt\n```\n\n----------------------------------------\n\nTITLE: Train model on single GPU in MMDetection\nDESCRIPTION: This snippet shows how to train a model on a single GPU using tools/train.py. The CONFIG_FILE variable specifies the path to the configuration file, and optional arguments can be passed to customize the training process. Use CUDA_VISIBLE_DEVICES to select the GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_2\n\nLANGUAGE: shell script\nCODE:\n```\npython tools/train.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Prepare COCO Semantic annotations (Python)\nDESCRIPTION: This snippet executes a Python script to prepare COCO semantic segmentation annotations from panoptic annotations, located in the 'data/coco' directory. This conversion is needed to generate the dataset for training and testing semantic segmentation models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/prepare_coco_semantic_annos_from_panoptic_annos.py data/coco\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration with Visualization (2.x)\nDESCRIPTION: This snippet configures logging with visualization backends like TensorBoard and WandB in MMDetection 2.x.  It includes hooks for text logging, TensorBoard logging, and WandB logging. It also configures Wandb specific settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook'),\n        dict(type='MMDetWandbHook',\n             init_kwargs={\n                'project': 'mmdetection',\n                'group': 'maskrcnn-r50-fpn-1x-coco'\n             },\n             interval=50,\n             log_checkpoint=True,\n             log_checkpoint_metadata=True,\n             num_eval_images=100)\n    ])\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-l\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-l model. The linked config file probably specifies the model's architecture and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_l_8xb32-300e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: YouTube-VIS to COCO Conversion (2021)\nDESCRIPTION: Converts YouTube-VIS 2021 dataset annotations to COCO format.  The script uses `youtubevis2coco.py` for the conversion of YouTube-VIS 2021 annotations.  The script requires the input path to the YouTube-VIS 2021 dataset, the output path for the COCO annotations, and specifies the dataset version as 2021.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_dataset_prepare.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# YouTube-VIS 2021\npython ./tools/dataset_converters/youtubevis2coco.py -i ./data/youtube_vis_2021 -o ./data/youtube_vis_2021/annotations --version 2021\n```\n\n----------------------------------------\n\nTITLE: Iter-based Training Configuration - Python\nDESCRIPTION: This showcases an example of transitioning from epoch-based to iteration-based training, involving modifications to train_cfg, param_scheduler, train_dataloader, default_hooks, and log_processor.  It changes the training loop type, maximum iterations, validation interval, sampler, checkpoint interval, and logging format to suit iteration-based training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# iter-based 训练配置\ntrain_cfg = dict(\n    _delete_=True,  # 忽略继承的配置文件中的值（可选）\n    type='IterBasedTrainLoop',  # iter-based 训练循环\n    max_iters=90000,  # 最大迭代次数\n    val_interval=10000)  # 每隔多少次进行一次验证\n\n\n# 将参数调度器修改为 iter-based\nparam_scheduler = [\n    dict(\n        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n    dict(\n        type='MultiStepLR',\n        begin=0,\n        end=90000,\n        by_epoch=False,\n        milestones=[60000, 80000],\n        gamma=0.1)\n]\n\n# 切换至 InfiniteSampler 来避免 dataloader 重启\ntrain_dataloader = dict(sampler=dict(type='InfiniteSampler'))\n\n# 将模型检查点保存间隔设置为按 iter 保存\ndefault_hooks = dict(checkpoint=dict(by_epoch=False, interval=10000))\n\n# 将日志格式修改为 iter-based\nlog_processor = dict(by_epoch=False)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Loss Function and Module in MMDetection (Python)\nDESCRIPTION: Defines a custom loss function `my_loss` and a loss module `MyLoss` for use in MMDetection.  The `weighted_loss` decorator allows for per-part weighting of the loss. The `MyLoss` module is registered with the `LOSSES` registry for easy integration into MMDetection's configuration system.  It inherits from `nn.Module` and implements a `forward` method to compute the loss.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nfrom mmdet.registry import LOSSES\nfrom .utils import weighted_loss\n\n\n@weighted_loss\ndef my_loss(pred, target):\n    assert pred.size() == target.size() and target.numel() > 0\n    loss = torch.abs(pred - target)\n    return loss\n\n@LOSSES.register_module()\nclass MyLoss(nn.Module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super(MyLoss, self).__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_bbox = self.loss_weight * my_loss(\n            pred, target, weight, reduction=reduction, avg_factor=avg_factor)\n        return loss_bbox\n```\n\n----------------------------------------\n\nTITLE: Slurm Training with Port Specification in Config (Python)\nDESCRIPTION: These Python snippets illustrate how to set different communication ports within the MMDetection configuration files for Slurm training. Each configuration file (`config1.py`, `config2.py`) should define the `dist_params` dictionary with a unique port number.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndist_params = dict(backend='nccl', port=29500)\n```\n\nLANGUAGE: python\nCODE:\n```\ndist_params = dict(backend='nccl', port=29501)\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for RegNet Design\nDESCRIPTION: This LaTeX snippet provides the citation information for the 'Designing Network Design Spaces' paper by Radosavovic et al. It includes the title, author, year, eprint, archive prefix, and primary class for proper academic referencing of the RegNet architecture.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_40\n\nLANGUAGE: latex\nCODE:\n```\n@article{radosavovic2020designing,\n    title={Designing Network Design Spaces},\n    author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Dollár},\n    year={2020},\n    eprint={2003.13678},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for HTC\nDESCRIPTION: This snippet provides the BibTeX entry for citing the Hybrid Task Cascade (HTC) paper in academic publications. It includes the title, authors, conference, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/htc/README.md#_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n@inproceedings{chen2019hybrid,\n  title={Hybrid task cascade for instance segmentation},\n  author={Chen, Kai and Pang, Jiangmiao and Wang, Jiaqi and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Shi, Jianping and Ouyang, Wanli and Chen Change Loy and Dahua Lin},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing layers with different configurations\nDESCRIPTION: This code illustrates initializing different layers with different configurations using a list of dictionaries. Each dictionary specifies the `type` and parameters for the initialization of a specific layer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninit_cfg = [dict(type='Constant', layer='Conv1d', val=1),\n              dict(type='Constant', layer='Conv2d', val=2),\n              dict(type='Constant', layer='Linear', val=3)]\n# nn.Conv1d 将被初始化为 dict(type='Constant', val=1)\n# nn.Conv2d 将被初始化为 dict(type='Constant', val=2)\n# nn.Linear 将被初始化为 dict(type='Constant', val=3)\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-l-ConvNeXt-B\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-l-ConvNeXt-B model. It defines the model architecture and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_l_convnext_b_4xb32-100e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Compare BBox mAP of Two Runs (analyze_logs.py)\nDESCRIPTION: This example compares the bbox mAP of two different training runs by analyzing their respective log files.  The `log1.json` and `log2.json` files are compared using the 'bbox_mAP' key, and the legend labels are set to 'run1' and 'run2'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve log1.json log2.json --keys bbox_mAP --legend run1 run2\n```\n\n----------------------------------------\n\nTITLE: Evaluate Single Model and Fusion Results (fuse_results.py)\nDESCRIPTION: This example simultaneously evaluates each single model and the fusion results.  It requires the paths to the JSON result files from each model and the ground truth annotation file.  The `--eval-single` flag enables evaluation of individual models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ./faster-rcnn_r50-caffe_fpn_1x_coco.json \\\n       ./retinanet_r50-caffe_fpn_1x_coco.json \\\n       ./cascade-rcnn_r50-caffe_fpn_1x_coco.json \\\n       --annotation ./annotation.json \\\n       --weights 1 2 3 \\\n       --eval-single\n```\n\n----------------------------------------\n\nTITLE: Using the override key to initialize specific modules\nDESCRIPTION: This example demonstrates using the `override` key to initialize a specific module with a different configuration. The `override` key allows users to specify a different initialization strategy for a named submodule within the model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# layers:\n# self.feat = nn.Conv1d(3, 1, 3)\n# self.reg = nn.Conv2d(3, 3, 3)\n# self.cls = nn.Linear(1,2)\n\ninit_cfg = dict(type='Constant',\n                  layer=['Conv1d','Conv2d'], val=1, bias=2,\n                  override=dict(type='Constant', name='reg', val=3, bias=4))\n# self.feat and self.cls 将被初始化为 dict(type='Constant', val=1, bias=2)\n# 叫 'reg' 的模块将被初始化为 dict(type='Constant', val=3, bias=4)\n```\n\n----------------------------------------\n\nTITLE: Training Command with Pre-trained Weights (Bash)\nDESCRIPTION: This bash script trains a Faster R-CNN model with a pre-trained FCOS RPN in MMDetection, specifying the configuration file and working directory. The pre-trained weights help accelerate training and improve performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/single_stage_as_rpn.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ntools/dist_train.sh\nconfigs/faster_rcnn/faster-rcnn_r50-caffe_fpn_fcos-rpn_1x_coco.py  \\\n--work-dir /work_dirs/faster-rcnn_r50-caffe_fpn_fcos-rpn_1x_coco\n```\n\n----------------------------------------\n\nTITLE: Initializing FooModel with init_cfg in mmcv Modules\nDESCRIPTION: This code shows how to initialize a model that contains `mmcv.Sequential` or `mmcv.ModuleList` with an `init_cfg`. This enables initialization configurations to be applied within sequential or module list structures.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmcv.runner import BaseModule, ModuleList\n\nclass FooModel(BaseModule)\n\tdef __init__(self,\n                \targ1,\n                \targ2,\n                \tinit_cfg=None):\n    \t\tsuper(FooModel, self).__init__(init_cfg)\n        \t...\n        \tself.conv1 = ModuleList(init_cfg=XXX)\n```\n\n----------------------------------------\n\nTITLE: Configure MeanTeacherHook in Python\nDESCRIPTION: This code snippet configures the MeanTeacherHook, which updates the teacher model using Exponential Moving Average (EMA) of the student model. It is a custom hook used during training to optimize the teacher model in conjunction with the student model's optimization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [dict(type='MeanTeacherHook')]\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection with MIM\nDESCRIPTION: Installs MMDetection as a dependency using MIM. This is suitable when using MMDetection as a third-party package.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmim install mmdet\n```\n\n----------------------------------------\n\nTITLE: Initializing Layers with Different Configurations (Python)\nDESCRIPTION: This snippet shows how to initialize different layers (`Conv1d`, `Conv2d`, `Linear`) with distinct configurations using a list of dictionaries in `init_cfg`. Each dictionary specifies the initializer type and value for a particular layer. This enables fine-grained control over the initialization of each layer within the module.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ninit_cfg = [dict(type='Constant', layer='Conv1d', val=1),\n            dict(type='Constant', layer='Conv2d', val=2),\n            dict(type='Constant', layer='Linear', val=3)]\n# nn.Conv1d will be initialized with dict(type='Constant', val=1)\n# nn.Conv2d will be initialized with dict(type='Constant', val=2)\n# nn.Linear will be initialized with dict(type='Constant', val=3)\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Mask RCNN model with a RegNetX-400MF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-400MF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-400MF_fpn_mstrain-poly_3x_coco_20210601_235443-8aac57a4.pth)\n```\n\n----------------------------------------\n\nTITLE: Training a model using the config file\nDESCRIPTION: This shell command initiates the training process for a model using the specified config file. The `train.py` script is located in the `tools` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/train.py configs/cityscapes/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes.py\n```\n\n----------------------------------------\n\nTITLE: Set PYTHONPATH for MMDetection (Shell)\nDESCRIPTION: This snippet modifies the `PYTHONPATH` environment variable to ensure that the script uses its own version of MMDetection. It prepends the directory above the script's directory to the existing `PYTHONPATH`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nPYTHONPATH=\"$(dirname $0)/..\":$PYTHONPATH\n```\n\n----------------------------------------\n\nTITLE: Convert MMDetection Model to ONNX Format\nDESCRIPTION: This section describes the process of converting an MMDetection model to the ONNX (Open Neural Network Exchange) format. Refer to mmdeploy for detailed instructions and capabilities, including comparing outputs between PyTorch and ONNX models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_25\n\n\n\n----------------------------------------\n\nTITLE: Inspecting MMDetection Config File - Python\nDESCRIPTION: This command uses the `print_config.py` tool to display the complete configuration of a given config file. This allows users to understand all parameters of a model and how it has been configured.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_config.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython tools/misc/print_config.py /PATH/TO/CONFIG\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Cascade Mask RCNN model with a RegNetX-800MF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_36\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/cascade_mask_rcnn_regnetx-800MF_fpn_mstrain_3x_coco/cascade_mask_rcnn_regnetx-800MF_fpn_mstrain_3x_coco_20210715_211616.log.json)\n```\n\n----------------------------------------\n\nTITLE: Evaluating on Zero-Shot COCO2017 val (Single Card)\nDESCRIPTION: This shell command evaluates the MM Grounding DINO model on the COCO2017 validation dataset in a zero-shot setting using a single GPU. It executes the `tools/test.py` script with the specified configuration file and model weights, generating evaluation metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for SparseInst\nDESCRIPTION: This BibTeX entry provides the citation information for the SparseInst paper. It includes the title, authors, conference, and year of publication. This should be used when referencing SparseInst in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/SparseInst/README.md#_snippet_3\n\nLANGUAGE: BibTeX\nCODE:\n```\n@inproceedings{Cheng2022SparseInst,\n  title     =   {Sparse Instance Activation for Real-Time Instance Segmentation},\n  author    =   {Cheng, Tianheng and Wang, Xinggang and Chen, Shaoyu and Zhang, Wenqiang and Zhang, Qian and Huang, Chang and Zhang, Zhaoxiang and Liu, Wenyu},\n  booktitle =   {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =   {2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained X-Decoder Weights\nDESCRIPTION: These commands download pre-trained weights for the X-Decoder model from a specified URL.  These weights are necessary for running the demo and achieving the reported performance. The weights are intended to be placed in the `mmdetection` root directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nwget https://download.openmmlab.com/mmdetection/v3.0/xdecoder/xdecoder_focalt_last_novg.pt\nwget https://download.openmmlab.com/mmdetection/v3.0/xdecoder/xdecoder_focalt_best_openseg.pt\n```\n\n----------------------------------------\n\nTITLE: Training with Multiple Datasets (8 Cards)\nDESCRIPTION: This shell script trains the MM Grounding DINO model using multiple datasets (obj365v1/goldg/grit/v3det) with distributed training on 8 GPUs. It executes the `tools/dist_train.sh` script with the specified configuration file and the number of GPUs (8).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_train.sh configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det.py 8\n```\n\n----------------------------------------\n\nTITLE: Converting iSAID JSON Format (Python)\nDESCRIPTION: This snippet shows how to convert the original iSAID JSON annotation format to a format compatible with MMDetection. It uses a Python script located in the `projects/iSAID` directory to perform the conversion. The script takes the path to the iSAID dataset as an argument.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/iSAID/README_zh-CN.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython projects/iSAID/isaid_json.py /path/to/iSAID\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN Testing on PASCAL VOC in MMDetection\nDESCRIPTION: This snippet demonstrates how to test the Faster R-CNN model on the PASCAL VOC dataset using the `tools/test.py` script. It specifies the configuration file and checkpoint file for Faster R-CNN on VOC 07+12.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n    configs/pascal_voc/faster-rcnn_r50_fpn_1x_voc0712.py \\\n    checkpoints/faster_rcnn_r50_fpn_1x_voc0712_20200624-c9895d40.pth\n```\n\n----------------------------------------\n\nTITLE: Configuring Class Aware Sampler in MMDetection\nDESCRIPTION: This snippet demonstrates how to configure and use the `ClassAwareSampler` in MMDetection for training on datasets with imbalanced classes. It sets the `num_sample_class` parameter within the `data` dictionary, which is part of the MMDetection configuration.  It shows how to integrate the `ClassAwareSampler` into the data loading pipeline.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata=dict(train_dataloader=dict(class_aware_sampler=dict(num_sample_class=1)))\n```\n\n----------------------------------------\n\nTITLE: Citation for Instaboost paper\nDESCRIPTION: This is the BibTeX entry for citing the InstaBoost paper in academic publications. It includes the title, authors, conference, pages, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/instaboost/README.md#_snippet_1\n\nLANGUAGE: Latex\nCODE:\n```\n@inproceedings{fang2019instaboost,\n  title={Instaboost: Boosting instance segmentation via probability map guided copy-pasting},\n  author={Fang, Hao-Shu and Sun, Jianhua and Wang, Runzhong and Gou, Minghao and Li, Yong-Lu and Lu, Cewu},\n  booktitle={Proceedings of the IEEE International Conference on Computer Vision},\n  pages={682--691},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation of Albumentations in LaTeX\nDESCRIPTION: This LaTeX code snippet provides the citation details for the Albumentations paper. It includes the author names, title, journal, eprint, and year of publication. This information is essential for properly attributing the use of Albumentations in research papers.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/albu_example/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{2018arXiv180906839B,\n  author = {A. Buslaev, A. Parinov, E. Khvedchenya, V.~I. Iglovikov and A.~A. Kalinin},\n  title = \"{Albumentations: fast and flexible image augmentations}\",\n  journal = {ArXiv e-prints},\n  eprint = {1809.06839},\n  year = 2018\n}\n```\n\n----------------------------------------\n\nTITLE: Adding MMDeploy package to CMake Project\nDESCRIPTION: This CMake code snippet shows how to find and link the MMDeploy package within a CMake project. It uses `find_package` to locate the MMDeploy library and `target_link_libraries` to link it to the target.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(MMDeploy REQUIRED)\ntarget_link_libraries(${name} PRIVATE mmdeploy ${OpenCV_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Cascade R-CNN S50 Training Log\nDESCRIPTION: URL pointing to the training log file in JSON format for a Cascade R-CNN model using ResNeSt-50 as the backbone. This file contains the training process details, including loss, metrics, and learning rate.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_14\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/resnest/cascade_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/cascade_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco-20201005_113242.log.json)\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Cascade Mask RCNN model with a RegNetX-400MF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_33\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/cascade_mask_rcnn_regnetx-400MF_fpn_mstrain_3x_coco/cascade_mask_rcnn_regnetx-400MF_fpn_mstrain_3x_coco_20210715_211619.log.json)\n```\n\n----------------------------------------\n\nTITLE: Analyzing COCO bbox error (Shell)\nDESCRIPTION: This command analyzes COCO bbox error results and saves the analysis images to a specified directory, using an annotation file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_error_analysis.py \\\n       results.bbox.json \\\n       results \\\n       --ann=data/coco/annotations/instances_val2017.json\n```\n\n----------------------------------------\n\nTITLE: Setting Roboflow API Key\nDESCRIPTION: This snippet demonstrates how to set the Roboflow API key as an environment variable, which is required for downloading datasets from the Roboflow platform. This key is crucial for authenticating the user and allowing access to the Roboflow 100 dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nexport ROBOFLOW_API_KEY = Your Private API Key\n```\n\n----------------------------------------\n\nTITLE: Displaying the Output Image using PIL\nDESCRIPTION: This Python snippet uses the PIL (Pillow) library to open and display the output image generated by the DetInferencer. It assumes the output image is saved at './output/vis/demo.jpg'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Show the output image\nfrom PIL import Image\nImage.open('./output/vis/demo.jpg')\n```\n\n----------------------------------------\n\nTITLE: Panoptic Segmentation Installation\nDESCRIPTION: This shell command shows how to install the necessary package for panoptic segmentation using git. It fetches and installs the panopticapi from the cocodataset repository.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/cocodataset/panopticapi.git\n```\n\n----------------------------------------\n\nTITLE: Inference on COCO Test-Dev with Single GPU and Config Overrides\nDESCRIPTION: This shell command runs inference on the COCO test-dev dataset using a single GPU and overrides the configuration settings using `--cfg-options`. This allows dynamic modification of dataset paths and other parameters directly from the command line.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# test with single gpu\nCUDA_VISIBLE_DEVICES=0 python tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=${WORK_DIR}/results\n```\n\n----------------------------------------\n\nTITLE: Setting Hook Priority in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to set the priority of a custom hook in the MMDetection configuration file. The `priority` key can be set to `'NORMAL'` or `'HIGHEST'`. By default, hooks are registered with `NORMAL` priority.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MyHook', a=a_value, b=b_value, priority='NORMAL')\n]\n```\n\n----------------------------------------\n\nTITLE: Fuse Predictions Using WBF (fuse_results.py)\nDESCRIPTION: This tool fuses predictions from different object detection models using Weighted Boxes Fusion (WBF). It supports COCO format only. It requires the paths to the detection results from different models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ${PRED_RESULTS} \\\n       [--annotation ${ANNOTATION}] \\\n       [--weights ${WEIGHTS}] \\\n       [--fusion-iou-thr ${FUSION_IOU_THR}] \\\n       [--skip-box-thr ${SKIP_BOX_THR}] \\\n       [--conf-type ${CONF_TYPE}] \\\n       [--eval-single ${EVAL_SINGLE}] \\\n       [--save-fusion-results ${SAVE_FUSION_RESULTS}] \\\n       [--out-dir ${OUT_DIR}]\n```\n\n----------------------------------------\n\nTITLE: Print Config (print_config.py)\nDESCRIPTION: This snippet demonstrates how to print the entire configuration file with all inherited relationships expanded using `tools/misc/print_config.py`. It takes the config file as an argument, along with optional parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/print_config.py ${CONFIG} [-h] [--options ${OPTIONS [OPTIONS...]}]\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Faster RCNN model using a RegNetX-400MF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[config](./faster-rcnn_regnetx-400MF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Run Image Demo from Source\nDESCRIPTION: Runs the image demo script using the downloaded configuration and checkpoint. It detects objects in the provided image ('demo/demo.jpg') using the CPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg rtmdet_tiny_8xb32-300e_coco.py --weights rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth --device cpu\n```\n\n----------------------------------------\n\nTITLE: Citation for YOLOX in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the YOLOX paper. It includes the title, authors, journal, and year of publication, allowing users to properly cite YOLOX in their research papers or projects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/yolox/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{yolox2021,\n  title={{YOLOX}: Exceeding YOLO Series in 2021},\n  author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},\n  journal={arXiv preprint arXiv:2107.08430},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing DeepSort MOT Model on Multi-GPU\nDESCRIPTION: This example demonstrates how to test the DeepSort MOT model on multiple GPUs. It requires both a detector and a re-identification (reid) checkpoint specified using the `--detector` and `--reid` arguments respectively.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_15\n\nLANGUAGE: shell 脚本\nCODE:\n```\nbash ./tools/dist_test_tracking.sh configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8 --detector ${CHECKPOINT_FILE} --reid ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Original Object365 v1 Dataset (Shell)\nDESCRIPTION: This script visualizes the original Object365 v1 dataset by generating images with bounding box labels. It is used to verify the correctness of the images and annotations before training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_raw.py data/object365_v1/ o365v1_train_od.json train --label-map-file o365v1_label_map.json -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: Define a New PAFPN Neck in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to define a custom PAFPN neck within the MMDetection framework. It involves creating a new Python file, defining a PAFPN class that inherits from nn.Module, and registering it using the @MODELS.register_module() decorator. The class includes an __init__ method for initialization with input channels, output channels, number of outputs, and other parameters, and a forward method for processing input feature maps. This module is intended to be placed in `mmdet/models/necks/pafpn.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\n\nfrom mmdet.registry import MODELS\n\n\n@MODELS.register_module()\nclass PAFPN(nn.Module):\n\n    def __init__(self,\n                in_channels,\n                out_channels,\n                num_outs,\n                start_level=0,\n                end_level=-1,\n                add_extra_convs=False):\n        pass\n\n    def forward(self, inputs):\n        # implementation is ignored\n        pass\n```\n\n----------------------------------------\n\nTITLE: DeepFashion Citation (LaTeX)\nDESCRIPTION: This snippet contains the LaTeX code for citing the DeepFashion dataset paper. It includes the author names, paper title, conference proceedings, publication month, and year. This citation is provided for users who wish to properly attribute the DeepFashion dataset in their research or publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepfashion/README.md#_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n@inproceedings{liuLQWTcvpr16DeepFashion,\n   author = {Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou},\n   title = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},\n   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n   month = {June},\n   year = {2016}\n}\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN S101 Model Download\nDESCRIPTION: URL to download the pre-trained model weights for Mask R-CNN using ResNeSt-101 as the backbone. These weights can be used for fine-tuning or inference tasks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/resnest/mask_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco/mask_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco_20201005_215831-af60cdf9.pth)\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Cascade Mask RCNN model with a RegNetX-400MF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_32\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/cascade_mask_rcnn_regnetx-400MF_fpn_mstrain_3x_coco/cascade_mask_rcnn_regnetx-400MF_fpn_mstrain_3x_coco_20210715_211619-5142f449.pth)\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Mask RCNN model using a RegNetX-400MF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n[config](./mask-rcnn_regnetx-400MF_fpn_ms-poly-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Configure Batch Inference\nDESCRIPTION: This shell snippet demonstrates how to configure batch inference in the testing data configuration file by setting the batch_size parameter. This enables processing multiple images simultaneously during testing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ndata = dict(train_dataloader=dict(...), val_dataloader=dict(...), test_dataloader=dict(batch_size=2, ...))\n```\n\n----------------------------------------\n\nTITLE: V3Det to ODVG Conversion\nDESCRIPTION: This command converts the V3Det dataset's JSON annotation file into the ODVG format, making it suitable for training the MM-GDINO-T model. It uses the `coco2odvg.py` script from the `tools/dataset_converters` directory, providing the path to the V3Det JSON file and the dataset name (`v3det`) as arguments. The script produces two new JSON files containing the converted annotations and a label map.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/v3det/annotations/v3det_2023_v1_train.json -d v3det\n```\n\n----------------------------------------\n\nTITLE: Citation LaTeX Entry for Cascade R-CNN\nDESCRIPTION: This LaTeX code provides the citation information for the Cascade R-CNN paper. It includes the title, authors, journal, year, and DOI. This can be used to properly cite the paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/cascade_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{Cai_2019,\n   title={Cascade R-CNN: High Quality Object Detection and Instance Segmentation},\n   ISSN={1939-3539},\n   url={http://dx.doi.org/10.1109/tpami.2019.2956516},\n   DOI={10.1109/tpami.2019.2956516},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Cai, Zhaowei and Vasconcelos, Nuno},\n   year={2019},\n   pages={1–1}\n}\n```\n\n----------------------------------------\n\nTITLE: COCO Segmentation Error Analysis (coco_error_analysis.py)\nDESCRIPTION: This example performs COCO segmentation error analysis per category and saves the analysis result images. It requires a result file (`results.segm.json`), an output directory (`results`), the annotation file (`data/coco/annotations/instances_val2017.json`), and the `--types='segm'` argument to specify segmentation analysis.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_error_analysis.py \\\n       results.segm.json \\\n       results \\\n       --ann=data/coco/annotations/instances_val2017.json \\\n       --types='segm'\n```\n\n----------------------------------------\n\nTITLE: Inference with MaskTrack R-CNN (Python/Shell)\nDESCRIPTION: This Python script performs inference on a video using a trained MaskTrack R-CNN model. It takes a video file, a configuration file, and a checkpoint file as input, and outputs a video with the predicted instance segmentations. It utilizes the `mot_demo.py` script. Requires the MMTracking environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/masktrack_rcnn/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/masktrack_rcnn/masktrack-rcnn_mask-rcnn_r50_fpn_8xb1-12e_youtubevis2021.py  --checkpoint {CHECKPOINT_PATH} --out vis.mp4\n```\n\n----------------------------------------\n\nTITLE: Configuring FPN with Swin Transformer in MMDetection (Python)\nDESCRIPTION: This snippet describes the configuration required when combining Swin Transformer with a one-stage detector like RetinaNet in MMDetection. Due to the layer norm at the Swin Transformer's output, the `start_level` in FPN must be set to 0, and the backbone's `out_indices` must be set to `[1,2,3]`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/swin/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nretinanet_swin-t-p4-w7_fpn_1x_coco.py\n```\n\n----------------------------------------\n\nTITLE: Single GPU Testing with TTA\nDESCRIPTION: This shell command runs the testing script with test-time augmentation (TTA) enabled. The `--tta` flag activates TTA, applying data augmentations during the testing phase.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n# 单 GPU 测试\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--tta]\n```\n\n----------------------------------------\n\nTITLE: Test and Store Detection Results for Occluded/Separated Recall\nDESCRIPTION: This describes how to test the model and store the detection results in a pkl file, which can then be used by the occluded/separated recall script.  It uses `tools/test.py` with the `--out` option to save the results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py ${CONFIG} ${MODEL_PATH} --out results.pkl\n```\n\n----------------------------------------\n\nTITLE: Citation of Generalized Focal Loss (GFL) in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Generalized Focal Loss paper.  It includes the title, authors, journal, and year of publication, adhering to standard LaTeX citation conventions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gfl/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{li2020generalized,\n  title={Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection},\n  author={Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},\n  journal={arXiv preprint arXiv:2006.04388},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Training Baseline with Box Supervision\nDESCRIPTION: This script starts the distributed training process using the specified configuration file and number of GPUs. It trains the baseline Detic model using box-supervised data.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbash ./tools/dist_train.sh projects/Detic_new/detic_centernet2_r50_fpn_4x_lvis_boxsup.py 8\n```\n\n----------------------------------------\n\nTITLE: ConvNeXt BibTeX Citation\nDESCRIPTION: This is the BibTeX entry for citing the ConvNeXt paper in academic publications. It contains information about the authors, title, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/convnext/README.md#_snippet_1\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{liu2022convnet,\n  title={A ConvNet for the 2020s},\n  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},\n  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for Prime Sample Attention\nDESCRIPTION: This LaTeX snippet provides the bibliographic information for citing the \"Prime Sample Attention in Object Detection\" paper presented at the IEEE Conference on Computer Vision and Pattern Recognition in 2020. It includes the title, authors, booktitle, and year of publication. It's intended to be used in academic papers or reports that utilize or reference this work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_7\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{cao2019prime,\n  title={Prime sample attention in object detection},\n  author={Cao, Yuhang and Chen, Kai and Loy, Chen Change and Lin, Dahua},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing bbox mAP from two runs (Shell)\nDESCRIPTION: This command compares the bounding box mAP from two different log files ('log1.json' and 'log2.json') and labels them as 'run1' and 'run2' respectively.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve log1.json log2.json --keys bbox_mAP --legend run1 run2\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation\nDESCRIPTION: This LaTeX code provides the citation details for the GCNet paper. It includes the title, authors, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_8\n\nLANGUAGE: latex\nCODE:\n```\n@article{cao2019GCNet,\n  title={GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond},\n  author={Cao, Yue and Xu, Jiarui and Lin, Stephen and Wei, Fangyun and Hu, Han},\n  journal={arXiv preprint arXiv:1904.11492},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: COCO BBox Error Analysis (coco_error_analysis.py)\nDESCRIPTION: This example performs COCO bbox error analysis per category and saves the analysis result images to the specified directory. It requires a result file (`results.bbox.json`), an output directory (`results`), and the annotation file (`data/coco/annotations/instances_val2017.json`).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_error_analysis.py \\\n       results.bbox.json \\\n       results \\\n       --ann=data/coco/annotations/instances_val2017.json\n```\n\n----------------------------------------\n\nTITLE: Installing CLIP Dependency\nDESCRIPTION: This command installs the CLIP (Contrastive Language-Image Pre-training) library from its GitHub repository, which is a prerequisite for using Detic.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install git+https://github.com/openai/CLIP.git\n```\n\n----------------------------------------\n\nTITLE: Seesaw Loss Citation in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Seesaw Loss paper, including the title, authors, conference, and year of publication. It is used to properly credit the original authors when using or referring to the Seesaw Loss in academic publications or research.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/seesaw_loss/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{wang2021seesaw,\n  title={Seesaw Loss for Long-Tailed Instance Segmentation},\n  author={Jiaqi Wang and Wenwei Zhang and Yuhang Zang and Yuhang Cao and Jiangmiao Pang and Tao Gong and Kai Chen and Ziwei Liu and Chen Change Loy and Dahua Lin},\n  booktitle={Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: YOLOv3 Citation LaTeX\nDESCRIPTION: LaTeX code snippet for citing the YOLOv3 paper. This includes the title, authors, year, and arXiv information for proper attribution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/yolo/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@misc{redmon2018yolov3,\n    title={YOLOv3: An Incremental Improvement},\n    author={Joseph Redmon and Ali Farhadi},\n    year={2018},\n    eprint={1804.02767},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: ConvNeXt-V2 BibTeX citation\nDESCRIPTION: This BibTeX entry provides the citation information for the ConvNeXt V2 paper, including the title, authors, year, and journal (arXiv preprint). It allows researchers and developers to properly credit the original authors when using ConvNeXt V2 in their work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/ConvNeXt-V2/README.md#_snippet_1\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{Woo2023ConvNeXtV2,\n  title={ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},\n  author={Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon and Saining Xie},\n  year={2023},\n  journal={arXiv preprint arXiv:2301.00808},\n}\n```\n\n----------------------------------------\n\nTITLE: Prepare Unlabeled Data Annotations (Python)\nDESCRIPTION: This script processes the `image_info_unlabeled2017.json` file by copying the `categories` information from the `instances_train2017.json` file. This is necessary because the unlabeled data annotation file lacks category information, which is required to initialize the `CocoDataset` for training. The result is saved as `instances_unlabeled2017.json`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom mmengine.fileio import load, dump\n\nanns_train = load('instances_train2017.json')\nanns_unlabeled = load('image_info_unlabeled2017.json')\nanns_unlabeled['categories'] = anns_train['categories']\ndump(anns_unlabeled, 'instances_unlabeled2017.json')\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN with PISA Configuration (COCO Dataset)\nDESCRIPTION: This snippet provides the configuration file name for Faster R-CNN with PISA enabled, trained on the COCO dataset. It details the specific settings used for training the Faster R-CNN model with a ResNet-50-FPN backbone, incorporating the PISA sampling strategy to improve detection performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[config](./faster-rcnn_r50_fpn_pisa_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Multi-machine Training Commands (Shell)\nDESCRIPTION: These shell commands illustrate how to run distributed training across multiple machines using Ethernet connection.  `NNODES` specifies the total number of machines, `NODE_RANK` is the rank of the current machine, `PORT` is the master port, and `MASTER_ADDR` is the IP address of the master node.  Replace `${CONFIG}` and `${GPUS}` with the appropriate values.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nNNODES=2 NODE_RANK=0 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR sh tools/dist_train.sh $CONFIG $GPUS\n```\n\nLANGUAGE: shell\nCODE:\n```\nNNODES=2 NODE_RANK=1 PORT=$MASTER_PORT MASTER_ADDR=$MASTER_ADDR sh tools/dist_train.sh $CONFIG $GPUS\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Config and Checkpoint Paths\nDESCRIPTION: This snippet initializes the `DetInferencer` by providing the paths to both the model configuration file (`config_path`) and the checkpoint file (`checkpoint`). This is useful when using custom configurations or specific weight files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Choose to use a config\nconfig_path = './configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py'\n\n# Setup a checkpoint file to load\ncheckpoint = './checkpoints/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n\n# Initialize the DetInferencer\ninferencer = DetInferencer(model=config_path, weights=checkpoint)\n```\n\n----------------------------------------\n\nTITLE: Configuring Automatic Learning Rate Scaling in MMDetection\nDESCRIPTION: This snippet shows how to configure automatic learning rate scaling in MMDetection based on the number of GPUs and samples per GPU. The `auto_scale_lr` dictionary contains the `enable` flag to activate the feature and `base_batch_size`, which should be the batch size used for the current learning rate in the config.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nauto_scale_lr = dict(enable=True, base_batch_size=N)\n```\n\n----------------------------------------\n\nTITLE: Fast R-CNN Citation (LaTeX)\nDESCRIPTION: This is the BibTeX entry for the Fast R-CNN paper. It provides the necessary information to cite the paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fast_rcnn/README.md#_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{girshick2015fast,\n  title={Fast r-cnn},\n  author={Girshick, Ross},\n  booktitle={Proceedings of the IEEE international conference on computer vision},\n  year={2015}\n}\n```\n\n----------------------------------------\n\nTITLE: DDOD ATSS Model Download\nDESCRIPTION: Links to download the pre-trained model for DDOD-ATSS trained on COCO dataset with ResNet-50 backbone.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ddod/README.md#_snippet_1\n\nLANGUAGE: URL\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/ddod/ddod_r50_fpn_1x_coco/ddod_r50_fpn_1x_coco_20220523_223737-29b2fc67.pth)\n```\n\n----------------------------------------\n\nTITLE: GoldG (Flickr30k) to ODVG Conversion\nDESCRIPTION: This command converts the Flickr30k portion of the GoldG dataset's JSON annotation file into the ODVG format, enabling its use for training the MM-GDINO-T model. It employs the `goldg2odvg.py` script from the `tools/dataset_converters` directory, with the path to the Flickr30k JSON file provided as an argument. The script outputs a new JSON file containing the converted annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/goldg2odvg.py data/flickr30k_entities/final_flickr_separateGT_train.json\n```\n\n----------------------------------------\n\nTITLE: Citing FPG in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Feature Pyramid Grids (FPG) paper. It can be used to properly cite the FPG algorithm in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fpg/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{chen2020feature,\n  title={Feature pyramid grids},\n  author={Chen, Kai and Cao, Yuhang and Loy, Chen Change and Lin, Dahua and Feichtenhofer, Christoph},\n  journal={arXiv preprint arXiv:2004.03580},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Detic BibTeX Citation\nDESCRIPTION: This BibTeX entry provides the citation information for the Detic paper, which should be used when referencing Detic in academic publications. The entry includes the title, authors, booktitle, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_8\n\nLANGUAGE: BibTeX\nCODE:\n```\n@inproceedings{zhou2022detecting,\n  title={Detecting Twenty-thousand Classes using Image-level Supervision},\n  author={Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Kr{\"a}henb{\"u}hl, Philipp and Misra, Ishan},\n  booktitle={ECCV},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: SSD300 with PISA Configuration (COCO Dataset)\nDESCRIPTION: This snippet provides the configuration file name for SSD300 with PISA enabled, trained on the COCO dataset. It outlines the training configuration for an SSD300 object detector with a VGG16 backbone, utilizing the PISA sampling strategy to improve the model's ability to detect objects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[config](./ssd300_pisa_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Citation of NAS-FPN in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the NAS-FPN paper. It includes the title, author, booktitle, pages, and year of publication. This citation can be used to properly attribute the NAS-FPN architecture in research papers.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/nas_fpn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{ghiasi2019fpn,\n  title={Nas-fpn: Learning scalable feature pyramid architecture for object detection},\n  author={Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={7036--7045},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Training QDTrack MOT Model on CPU\nDESCRIPTION: This example shows how to train the QDTrack MOT model on a CPU. It sets CUDA_VISIBLE_DEVICES to -1 to disable GPUs and then runs the `tools/train.py` script with the specified QDTrack configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_1\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/train.py configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py\n```\n\n----------------------------------------\n\nTITLE: Test model with Slurm in MMDetection\nDESCRIPTION: This snippet shows how to test a model on a Slurm-managed cluster using the slurm_test_tracking.sh script. The PARTITION, JOB_NAME, CONFIG_FILE variables specify the Slurm partition, job name, and configuration file, respectively. Optional arguments can be passed to customize the testing process, including specifying the checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_16\n\nLANGUAGE: shell script\nCODE:\n```\n[GPUS=${GPUS}] bash tools/slurm_test_tracking.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Training Configuration (2.x)\nDESCRIPTION: This code snippet illustrates the training configuration in MMDetection 2.x.  It defines the runner type, maximum epochs, and validation interval. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nrunner = dict(\n    type='EpochBasedRunner',  # 训练循环的类型\n    max_epochs=12)  # 最大训练轮次\nevaluation = dict(interval=2)  # 验证间隔。每 2 个 epoch 验证一次\n```\n\n----------------------------------------\n\nTITLE: Evaluator Configuration - Python\nDESCRIPTION: This configures the evaluator for calculating metrics on the validation and test datasets. It uses the CocoMetric to evaluate AR, AP, and mAP for object detection and instance segmentation.  It specifies the annotation file path and the metrics to calculate (bbox, segm). It also demonstrates how to configure the evaluator to format the results for submission if test dataset annotations are not available.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(  # 验证过程使用的评测器\n    type='CocoMetric',  # 用于评估检测和实例分割的 AR、AP 和 mAP 的 coco 评价指标\n    ann_file=data_root + 'annotations/instances_val2017.json',  # 标注文件路径\n    metric=['bbox', 'segm'],  # 需要计算的评价指标，`bbox` 用于检测，`segm` 用于实例分割\n    format_only=False)\ntest_evaluator = val_evaluator  # 测试过程使用的评测器\n```\n\nLANGUAGE: python\nCODE:\n```\n# 在测试集上推理，\n# 并将检测结果转换格式以用于提交结果\ntest_dataloader = dict(\n    batch_size=1,\n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file=data_root + 'annotations/image_info_test-dev2017.json',\n        data_prefix=dict(img='test2017/'),\n        test_mode=True,\n        pipeline=test_pipeline))\ntest_evaluator = dict(\n    type='CocoMetric',\n    ann_file=data_root + 'annotations/image_info_test-dev2017.json',\n    metric=['bbox', 'segm'],\n    format_only=True,  # 只将模型输出转换为 coco 的 JSON 格式并保存\n    outfile_prefix='./work_dirs/coco_detection/test')  # 要保存的 JSON 文件的前缀\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX\nDESCRIPTION: This LaTeX code snippet provides the citation information for the Weight Standardization paper. It includes the author, title, journal, year, and other relevant details for proper attribution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gn+ws/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{weightstandardization,\n  author    = {Siyuan Qiao and Huiyu Wang and Chenxi Liu and Wei Shen and Alan Yuille},\n  title     = {Weight Standardization},\n  journal   = {arXiv preprint arXiv:1903.10520},\n  year      = {2019},\n}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for SSD\nDESCRIPTION: This BibTeX entry provides the citation information for the original SSD paper, which should be cited when using or referencing the SSD model in academic works. It includes the title, authors, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ssd/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{Liu_2016,\n   title={SSD: Single Shot MultiBox Detector},\n   journal={ECCV},\n   author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},\n   year={2016},\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Video Instance Segmentation Paper (Latex)\nDESCRIPTION: This LaTeX snippet provides the citation information for the Video Instance Segmentation paper.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/masktrack_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{yang2019video,\n  title={Video instance segmentation},\n  author={Yang, Linjie and Fan, Yuchen and Xu, Ning},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={5188--5197},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Dataset Definition\nDESCRIPTION: This snippet shows how to create a custom dataset class in MMDetection by inheriting from `BaseDetDataset`. It overrides the `load_data_list` method to parse a custom annotation file format and extract image and bounding box information. The dataset is then registered using the `@DATASETS.register_module()` decorator.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport mmengine\nfrom mmdet.base_det_dataset import BaseDetDataset\nfrom mmdet.registry import DATASETS\n\n\n@DATASETS.register_module()\nclass MyDataset(BaseDetDataset):\n\n    METAINFO = {\n       'classes': ('person', 'bicycle', 'car', 'motorcycle'),\n        'palette': [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230)]\n    }\n\n    def load_data_list(self, self, ann_file):\n        ann_list = mmengine.list_from_file(ann_file)\n\n        data_infos = []\n        for i, ann_line in enumerate(ann_list):\n            if ann_line != '#':\n                continue\n\n            img_shape = ann_list[i + 2].split(' ')\n            width = int(img_shape[0])\n            height = int(img_shape[1])\n            bbox_number = int(ann_list[i + 3])\n\n            instances = []\n            for anns in ann_list[i + 4:i + 4 + bbox_number]:\n                instance = {}\n                instance['bbox'] = [float(ann) for ann in anns.split(' ')[:4]]\n                instance['bbox_label']=int(anns[4])\n \t\t\t\tinstances.append(instance)\n\n            data_infos.append(\n                dict(\n                    img_path=ann_list[i + 1],\n                    img_id=i,\n                    width=width,\n                    height=height,\n                    instances=instances\n                ))\n\n        return data_infos\n```\n\n----------------------------------------\n\nTITLE: Convert Objects365 v1 to ODVG format (coco2odvg.py)\nDESCRIPTION: This script converts the Objects365 v1 dataset from COCO format to the ODVG format required for training. It uses the coco2odvg.py script and generates `o365v1_train_od.json` and `o365v1_label_map.json` files in the `data/objects365v1` directory.\nDependencies: coco2odvg.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/objects365v1/objects365_train.json -d o365v1\n```\n\n----------------------------------------\n\nTITLE: Citation for PVTv2\nDESCRIPTION: This is the BibTeX entry for citing the PVTv2 paper, which presents improved baselines using the Pyramid Vision Transformer. It contains the title, authors, journal, year, and other details necessary for citing the work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pvt/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{wang2021pvtv2,\n  title={PVTv2: Improved Baselines with Pyramid Vision Transformer},\n  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},\n  journal={arXiv preprint arXiv:2106.13797},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Instaboost Installation\nDESCRIPTION: This shell command shows how to install the instaboostfast package, which is a dependency for Instaboost functionality in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install instaboostfast\n```\n\n----------------------------------------\n\nTITLE: Configuration for Double Head R-CNN (Python)\nDESCRIPTION: This code provides an example configuration for using Double Head R-CNN in MMDetection. It extends a base Faster R-CNN configuration and modifies the `roi_head` to use the `DoubleHeadRoIHead` with a custom `DoubleConvFCBBoxHead`. It also sets parameters such as `reg_roi_scale_factor`, number of convolutional and fully connected layers, and loss functions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n_base_ = '../faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        type='DoubleHeadRoIHead',\n        reg_roi_scale_factor=1.3,\n        bbox_head=dict(\n            _delete_=True,\n            type='DoubleConvFCBBoxHead',\n            num_convs=4,\n            num_fcs=2,\n            in_channels=256,\n            conv_out_channels=1024,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=2.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=2.0))))\n\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Faster RCNN model with a RegNetX-3.2GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-3_20210526_095152-e16a5227.pth)\n```\n\n----------------------------------------\n\nTITLE: Single GPU Training Script\nDESCRIPTION: This snippet demonstrates how to initiate single GPU training using the `dist_train.sh` script, specifying the configuration file and the number of GPUs. The first command trains with default save path, the second defines `my_work_dirs` as save directory.  It is necessary to navigate to the `projects/RF100-Benchmark/` directory first for the command to work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n# current path is projects/RF100-Benchmark/\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 1\n# Specify the save path\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 1 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Stopping TorchServe\nDESCRIPTION: This command stops the TorchServe server.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\ntorchserve --stop\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration (3.x Interval)\nDESCRIPTION: This code snippet illustrates how to configure the checkpoint saving interval in MMDetection 3.x. It sets the interval to 1 within the `CheckpointHook` in `default_hooks`, meaning a checkpoint is saved every epoch. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        interval=1))\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Faster RCNN model using a RegNetX-4GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n[config](./faster-rcnn_regnetx-4GF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Migrating Dataset Config (MMDetection 2.x)\nDESCRIPTION: This code configures the dataset in MMDetection 2.x. It defines parameters such as `samples_per_gpu`, `workers_per_gpu`, and dataset-specific settings for training, validation, and testing, including annotation files, image prefixes, and data pipelines.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline))\n\n```\n\n----------------------------------------\n\nTITLE: Convert gRefCOCO data to MDETR format\nDESCRIPTION: Converts the gRefCOCO dataset into the MDETR annotation format for use with MMDetection. It requires the gRefCOCO repository to be cloned.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncd gRefCOCO/mdetr\npython scripts/fine-tuning/grefexp_coco_format.py --data_path ../../data/coco/grefs --out_path ../../data/coco/mdetr_annotations/ --coco_path ../../data/coco\n```\n\n----------------------------------------\n\nTITLE: MOT Define Evaluation Metrics - Python\nDESCRIPTION: Defines the evaluation metrics to be recorded during the MOT parameter search. The `test_evaluator` dictionary specifies the metrics to be used, which can include 'HOTA', 'CLEAR', and 'Identity'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_analysis_tools.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntest_evaluator=dict(type='MOTChallengeMetrics', metric=['HOTA', 'CLEAR', 'Identity'])\n```\n\n----------------------------------------\n\nTITLE: Citing SOLOv2 in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the SOLOv2 paper, including the title, authors, journal, and year of publication. This should be included in any work that uses or references the SOLOv2 algorithm.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/solov2/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{wang2020solov2,\n  title={SOLOv2: Dynamic and Fast Instance Segmentation},\n  author={Wang, Xinlong and Zhang, Rufeng and  Kong, Tao and Li, Lei and Shen, Chunhua},\n  journal={Proc. Advances in Neural Information Processing Systems (NeurIPS)},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Model with BaseModule in Python\nDESCRIPTION: This code snippet demonstrates how to initialize a model inheriting from `mmcv.runner.BaseModule` and using `init_cfg`. It shows the basic structure of the `FooModel` class, including the `__init__` method with `init_cfg` as an argument and the call to the superclass constructor.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule\n\nclass FooModel(BaseModule):\n\tdef __init__(self,\n                 arg1,\n                 arg2,\n                 init_cfg=None):\n    \tsuper(FooModel, self).__init__(init_cfg)\n\t\t...\n```\n\n----------------------------------------\n\nTITLE: Using AvoidCUDAOOM as a Decorator in MMDetection\nDESCRIPTION: This snippet demonstrates how to use `AvoidCUDAOOM` as a decorator to handle GPU out-of-memory (OOM) errors during MMDetection training. The `@AvoidCUDAOOM.retry_if_cuda_oom` decorator ensures that the decorated function retries execution with OOM error handling.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.utils import AvoidCUDAOOM\n\n@AvoidCUDAOOM.retry_if_cuda_oom\ndef function(*args, **kwargs):\n    ...\n    return xxx\n```\n\n----------------------------------------\n\nTITLE: Citation of DAB-DETR in LaTeX\nDESCRIPTION: This is the BibTeX entry for citing the DAB-DETR paper. It includes the title, authors, booktitle, year, and URL.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dab_detr/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{\n  liu2022dabdetr,\n  title={{DAB}-{DETR}: Dynamic Anchor Boxes are Better Queries for {DETR}},\n  author={Shilong Liu and Feng Li and Hao Zhang and Xiao Yang and Xianbiao Qi and Hang Su and Jun Zhu and Lei Zhang},\n  booktitle={International Conference on Learning Representations},\n  year={2022},\n  url={https://openreview.net/forum?id=oMI9PjOb9Jl}\n}\n```\n\n----------------------------------------\n\nTITLE: Download BERT Weights - Python\nDESCRIPTION: This Python code downloads and saves the BERT model and tokenizer weights from Hugging Face. It imports necessary modules from the transformers library and saves the configuration, model, and tokenizer to a specified local path. This is useful for offline usage or when encountering network access issues.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BertConfig, BertModel\nfrom transformers import AutoTokenizer\n\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False, config=config)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nconfig.save_pretrained(\"your path/bert-base-uncased\")\nmodel.save_pretrained(\"your path/bert-base-uncased\")\ntokenizer.save_pretrained(\"your path/bert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: Convert YouTube-VIS 2021 Annotations to COCO format\nDESCRIPTION: This script converts the YouTube-VIS 2021 dataset annotations to the COCO format. It takes input and output directories as arguments, and specifies the dataset version. The output is saved as a JSON file compatible with COCO.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_dataset_prepare.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython ./tools/dataset_converters/youtubevis2coco.py -i ./data/youtube_vis_2021 -o ./data/youtube_vis_2021/annotations --version 2021\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Mask R-CNN Weights\nDESCRIPTION: This snippet sets the `load_from` variable to the URL of a pre-trained Mask R-CNN model's weights. Using pre-trained weights can improve model performance by initializing the model with already learned features.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Cascade Mask RCNN model with a RegNetX-1.6GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_38\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/cascade_mask_rcnn_regnetx-1.6GF_fpn_mstrain_3x_coco/cascade_mask_rcnn_regnetx-1_20210715_211616-75f29a61.pth)\n```\n\n----------------------------------------\n\nTITLE: Optimizer Configuration (2.x)\nDESCRIPTION: This code snippet shows the optimizer configuration in MMDetection 2.x. It defines the optimizer type, learning rate, momentum, weight decay, and gradient clipping configuration.  Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\noptimizer = dict(\n    type='SGD',  # 随机梯度下降优化器\n    lr=0.02,  # 基础学习率\n    momentum=0.9,  # 带动量的随机梯度下降\n    weight_decay=0.0001)  # 权重衰减\noptimizer_config = dict(grad_clip=None)  # 梯度裁剪的配置，设置为 None 关闭梯度裁剪\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX\nDESCRIPTION: This LaTeX code snippet provides the citation information for the CrowdDet paper. It includes the title, authors, booktitle, month, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/crowddet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{Chu_2020_CVPR,\n  title={Detection in Crowded Scenes: One Proposal, Multiple Predictions},\n  author={Chu, Xuangeng and Zheng, Anlin and Zhang, Xiangyu and Sun, Jian},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  year = {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for CARAFE Paper\nDESCRIPTION: This BibTeX entry provides the citation information for the CARAFE paper, including title, authors, conference, month, and year. It's used to properly cite the CARAFE algorithm in academic works.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/carafe/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{Wang_2019_ICCV,\n    title = {CARAFE: Content-Aware ReAssembly of FEatures},\n    author = {Wang, Jiaqi and Chen, Kai and Xu, Rui and Liu, Ziwei and Loy, Chen Change and Lin, Dahua},\n    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},\n    month = {October},\n    year = {2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Mask RCNN model with a RegNetX-1.6GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_24\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-1.6GF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-1_20210602_210641.log.json)\n```\n\n----------------------------------------\n\nTITLE: Extracting Training Logs (Python)\nDESCRIPTION: This command executes the `log_extract.py` script to collect and organize training results from the specified `work_dirs` directory. It requires the `pandas` and `openpyxl` libraries to be installed. The first argument is a string used for generating CSV titles, and the `--epoch` parameter specifies the number of epochs trained.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython scripts/log_extract.py faster_rcnn --epoch 25 --work-dirs my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Grounding DINO in Shell\nDESCRIPTION: This shell script demonstrates how to run inference using Grounding DINO with a specified image, configuration file, weights file, and text prompts.  It first changes the directory to the MMDETROOT directory. Then it downloads a pretrained model. Finally it executes the image_demo.py script, passing the path to a demo image, the path to the config file, the path to the model weights, and the text prompts.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ncd $MMDETROOT\n\nwget https://download.openmmlab.com/mmdetection/v3.0/grounding_dino/groundingdino_swint_ogc_mmdet-822d7e9d.pth\n\npython demo/image_demo.py \\\n\tdemo/demo.jpg \\\n\tconfigs/grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_cap4m.py \\\n\t--weights groundingdino_swint_ogc_mmdet-822d7e9d.pth \\\n\t--texts 'bench . car .'\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Imports\nDESCRIPTION: This snippet shows an alternative way to import the custom loss module by adding a `custom_imports` dictionary to the configuration file. This allows MMDetection to automatically import the `my_loss` module during initialization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports=dict(\n    imports=['mmdet.models.losses.my_loss'])\n```\n\n----------------------------------------\n\nTITLE: Inference with Image Path (Python)\nDESCRIPTION: This code snippet demonstrates how to perform inference with an image given its path. Requires the `mmdet` library.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninferencer('demo/demo.jpg')\n```\n\n----------------------------------------\n\nTITLE: Log Extraction Script Execution\nDESCRIPTION: This snippet demonstrates how to execute the `log_extract.py` script to collect training results and output them in CSV and XLSX formats.  The script requires the `pandas` and `openpyxl` libraries to be installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\npython scripts/log_extract.py faster_rcnn --epoch 25 --work-dirs my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Citing Faster R-CNN paper in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Faster R-CNN paper. It includes the title, journal, publisher, authors, year, and month of publication. This citation can be used when referencing the Faster R-CNN architecture in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/faster_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{Ren_2017,\n   title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},\n   year={2017},\n   month={Jun},\n}\n```\n\n----------------------------------------\n\nTITLE: Download Karpathy annotations for COCO Caption (Shell)\nDESCRIPTION: This set of commands downloads the Karpathy annotations for the COCO Caption dataset using `wget`. These annotations are required to use the COCO2014 dataset for captioning tasks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd data/coco/annotations\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val_gt.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test_gt.json\n```\n\n----------------------------------------\n\nTITLE: Convert LVIS to ODVG format for open-set fine-tuning\nDESCRIPTION: Converts the LVIS dataset annotations to the ODVG (Object Detection with Visual Grounding) format. This is necessary for open-set continued pretraining fine-tuning LVIS.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/lvis2odvg.py data/coco/annotations/lvis_v1_train.json\n```\n\n----------------------------------------\n\nTITLE: Mosaic Transformation Initialization in MMDetection (Python)\nDESCRIPTION: This code snippet shows the initialization of the `Mosaic` transformation, which is used for data augmentation. It highlights the `img_scale` parameter, emphasizing that it expects the order `(width, height)` for image scaling. The `transform` method shows how the `img_shape` which is `(height, width)` is being created.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/conventions.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@TRANSFORMS.register_module()\nclass Mosaic(BaseTransform):\n    def __init__(self,\n                img_scale: Tuple[int, int] = (640, 640),\n                center_ratio_range: Tuple[float, float] = (0.5, 1.5),\n                bbox_clip_border: bool = True,\n                pad_val: float = 114.0,\n                prob: float = 1.0) -> None:\n       ...\n\n       # img_scale 顺序应该是 (width, height)\n       self.img_scale = img_scale\n\n    def transform(self, results: dict) -> dict:\n        ...\n\n        results['img'] = mosaic_img\n        # (height, width)\n        results['img_shape'] = mosaic_img.shape[:2]\n```\n\n----------------------------------------\n\nTITLE: LVIS Dataset Installation\nDESCRIPTION: This shell command shows how to install the LVIS API using git. It's necessary for working with the LVIS dataset in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/lvis-dataset/lvis-api.git\n```\n\n----------------------------------------\n\nTITLE: RetinaNet with PISA Configuration (COCO Dataset, X101 Backbone)\nDESCRIPTION: This snippet provides the configuration file name for RetinaNet with PISA enabled, trained on the COCO dataset using a ResNeXt-101 backbone. It details the specific configuration for training a RetinaNet model, incorporating PISA sampling to improve detection accuracy when using a more powerful backbone.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[config](./retinanet_x101-32x4d_fpn_pisa_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for LAD\nDESCRIPTION: This LaTeX snippet provides the citation information for the Label Assignment Distillation (LAD) paper presented at WACV 2022. It includes the title, authors, booktitle, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/lad/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{nguyen2021improving,\n  title={Improving Object Detection by Label Assignment Distillation},\n  author={Chuong H. Nguyen and Thuy C. Nguyen and Tuan N. Tang and Nam L. H. Phan},\n  booktitle = {WACV},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Convert Objects365 v2 to ODVG format (coco2odvg.py)\nDESCRIPTION: This script converts the Objects365 v2 dataset (after fixing class names) from COCO format to the ODVG format required for training. It uses the coco2odvg.py script and generates `zhiyuan_objv2_train_fixname_od.json` and `o365v2_label_map.json` files in the `data/objects365v2` directory.\nDependencies: coco2odvg.py script and `zhiyuan_objv2_train_fixname.json` generated by `fix_o365_names.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/objects365v2/annotations/zhiyuan_objv2_train_fixname.json -d o365v2\n```\n\n----------------------------------------\n\nTITLE: Train MMDetection using SLURM\nDESCRIPTION: This script is designed to launch MMDetection training jobs on a SLURM cluster. The script requires placeholders for the partition, job name, configuration file, working directory, and the number of GPUs to be used. All the placeholders must be substituted with relevant values before execution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dsdl/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./tools/slurm_train.sh {partition} {job_name} {config_file} {work_dir} {gpu_nums}\n```\n\n----------------------------------------\n\nTITLE: Extract and Move ODinW Dataset (Bash)\nDESCRIPTION: This bash script extracts all zip files within the specified folder and then moves the extracted content to the `mmdetection/data` directory. It assumes the current working directory is the root of the mmdetection project.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\n#!/bin/bash\n\nfolder=\"./GLIP/odinw_35/\"\n\nfind \"$folder\" -type f -name \"*.zip\" | while read -r file; do\n    unzip \"$file\" -d \"$(dirname \"$file\")\"\ndone\n\nmv GLIP/odinw_35 data/\n```\n\n----------------------------------------\n\nTITLE: Download and Prepare COCO Dataset (Shell)\nDESCRIPTION: This script downloads the COCO test dataset images, test image information, and panoptic training/validation annotations. It then unzips these files and places them in the appropriate directories under `data/coco/`. Finally, it removes the downloaded zip files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test_results_submission.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# 假设 data/coco/ 不存在\nmkdir -pv data/coco/\n# 下载 test2017\nwget -P data/coco/ http://images.cocodataset.org/zips/test2017.zip\nwget -P data/coco/ http://images.cocodataset.org/annotations/image_info_test2017.zip\nwget -P data/coco/ http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\n# 解压缩它们\nunzip data/coco/test2017.zip -d data/coco/\nunzip data/coco/image_info_test2017.zip -d data/coco/\nunzip data/coco/panoptic_annotations_trainval2017.zip -d data/coco/\n# 删除 zip 文件(可选)\nrm -rf data/coco/test2017.zip data/coco/image_info_test2017.zip data/coco/panoptic_annotations_trainval2017.zip\n```\n\n----------------------------------------\n\nTITLE: Loading and Saving Pre-trained BERT Model Locally\nDESCRIPTION: This Python code snippet demonstrates how to load a pre-trained BERT model and tokenizer from Hugging Face Transformers, and then save them locally. This is useful when network access to Hugging Face is restricted.  It requires the `transformers` library to be installed. The code configures, loads, and saves the model and tokenizer, allowing for offline usage.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/glip/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BertConfig, BertModel\nfrom transformers import AutoTokenizer\n\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False, config=config)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nconfig.save_pretrained(\"your path/bert-base-uncased\")\nmodel.save_pretrained(\"your path/bert-base-uncased\")\ntokenizer.save_pretrained(\"your path/bert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: COCO Evaluator Configuration (2.x)\nDESCRIPTION: This code snippet shows the COCO dataset and evaluation configuration in MMDetection 2.x. It defines the dataset type and annotation file for the validation set, along with the evaluation metrics (bbox and segm).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type='CocoDataset',\n        ann_file=data_root + 'annotations/instances_val2017.json'))\nevaluation = dict(metric=['bbox', 'segm'])\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for VISION Datasets\nDESCRIPTION: Provides the BibTeX entry for citing the VISION Datasets paper.  This allows researchers to properly attribute the dataset in their publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/VISION-Datasets/README.md#_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n@article{vision-datasets,\n  title         = {VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON},\n  author        = {Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk Cinbis, Oncel Tuzel, Ping Huang, Jiulong Shan, Jianjun Shi, Meng Cao},\n  journal       = {arXiv preprint arXiv:2306.07890},\n  year          = {2023},\n}\n```\n\n----------------------------------------\n\nTITLE: Using override without layer to initialize specific modules\nDESCRIPTION: This code illustrates using the `override` key without the `layer` key in `init_cfg` to initialize a specific module. When `layer` is not defined, only modules specified in `override` are initialized using the `init_cfg` settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# layers:\n# self.feat = nn.Conv1d(3, 1, 3)\n# self.reg = nn.Conv2d(3, 3, 3)\n# self.cls = nn.Linear(1,2)\n\ninit_cfg = dict(type='Constant', val=1, bias=2, \toverride=dict(name='reg'))\n\n# self.feat and self.cls 将被 Pytorch 初始化\n# 叫 'reg' 的模块将被 dict(type='Constant', val=1, bias=2) 初始化\n```\n\n----------------------------------------\n\nTITLE: Configuring ReID dataset paths in MMDetection\nDESCRIPTION: This code snippet configures the dataset paths for training, validation, and testing within an MMDetection configuration file for a custom dataset. It specifies the data prefix (image directory) and annotation file paths.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/reid/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    train=dict(\n        data_prefix='data/Filelist/imgs',\n        ann_file='data/Filelist/meta/train.txt'),\n    val=dict(\n        data_prefix='data/Filelist/imgs',\n        ann_file='data/Filelist/meta/val.txt'),\n    test=dict(\n        data_prefix='data/Filelist/imgs',\n        ann_file='data/Filelist/meta/val.txt'),\n)\n```\n\n----------------------------------------\n\nTITLE: Process GRIT dataset (grit_processing.py)\nDESCRIPTION: This script processes the raw GRIT dataset downloaded using img2dataset. It converts the raw format to a processed format with annotations and image directories, placing results in `data/grit_processed`.\nDependencies: grit_processing.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/grit_processing.py data/grit_raw data/grit_processed\n```\n\n----------------------------------------\n\nTITLE: Configuration File Naming Before v2.25.0 (Mask2Former)\nDESCRIPTION: Before MMDetection v2.25.0, configuration files named 'mask2former_xxx_coco.py' were used to represent panoptic segmentation models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/compatibility.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n'mask2former_xxx_coco.py' 代表全景分割的配置文件\n```\n\n----------------------------------------\n\nTITLE: Extracting COCO data from mixed dataset\nDESCRIPTION: This script extracts COCO-specific data from a mixed dataset annotation file.  It takes the `final_mixed_train.json` file as input and creates a new file `final_mixed_train_only_coco.json` containing only the COCO data.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/extract_coco_from_mixed.py data/coco/mdetr_annotations/final_mixed_train.json\n```\n\n----------------------------------------\n\nTITLE: Citation of Localization Distillation Method (LD)\nDESCRIPTION: This LaTeX snippet provides the citation information for the Localization Distillation (LD) method, including the title, authors, conference, and year of publication. This snippet is intended for academic use when referencing the LD method.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ld/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@Inproceedings{zheng2022LD,\n  title={Localization Distillation for Dense Object Detection},\n  author= {Zheng, Zhaohui and Ye, Rongguang and Wang, Ping and Ren, Dongwei and Zuo, Wangmeng and Hou, Qibin and Cheng, Mingming},\n  booktitle={CVPR},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Removing COCO2017 train images from RefCOCO\nDESCRIPTION: This script removes overlapping images between the COCO2017 training set and the RefCOCO validation sets to prevent data leakage during evaluation of referring expression comprehension models.  It takes the MDETR annotations and COCO training annotations as input and generates a new annotation file without the overlapping images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/remove_cocotrain2017_from_refcoco.py data/coco/mdetr_annotations data/coco/annotations/instances_train2017.json\n```\n\n----------------------------------------\n\nTITLE: Reusing Variables from Base Configuration in MMDetection\nDESCRIPTION: This code snippet demonstrates how to reuse variables from a base configuration file in MMDetection. By using the `{{_base_.xxx}}` syntax, you can access and copy the value of a variable (e.g., `model`) defined in the base configuration into the current configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\n\na = {{_base_.model}}  # 变量 a 等于 _base_ 中定义的 model\n```\n\n----------------------------------------\n\nTITLE: Setting up MMDetWandbHook in MMDetection\nDESCRIPTION: This snippet demonstrates how to configure and use the `MMDetWandbHook` in MMDetection for logging training progress to Wandb. It initializes the hook with project name, logging interval, checkpoint logging, and number of evaluation images. It relies on the `cfg` object which is assumed to be defined elsewhere in the MMDetection configuration.  No specific inputs or outputs are described here, but the purpose is to integrate Wandb logging into the MMDetection training pipeline.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncfg.log_config.hooks = [\n    dict(type='MMDetWandbHook',\n         init_kwargs={'project': 'MMDetection-tutorial'},\n         interval=10,\n         log_checkpoint=True,\n         log_checkpoint_metadata=True,\n         num_eval_images=10)]\n```\n\n----------------------------------------\n\nTITLE: Generating ODVG Format Files (Python)\nDESCRIPTION: This Python script generates the initial ODVG (Object Detection with Visual Grounding) format files required for training. It reads image metadata (filename, height, width) from a directory, creates a JSON file with this information and also creates a label map JSON file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport cv2\nimport json\nimport jsonlines\n\ndata_root = 'data/cat'\nimages_path = os.path.join(data_root, 'images')\nout_path = os.path.join(data_root, 'cat_train_od.json')\nmetas = []\nfor files in os.listdir(images_path):\n    img = cv2.imread(os.path.join(images_path, files))\n    height, width, _ = img.shape\n    metas.append({\"filename\": files, \"height\": height, \"width\": width})\n\nwith jsonlines.open(out_path, mode='w') as writer:\n    writer.write_all(metas)\n\n# 生成 label_map.json，由于只有一个类别，所以只需要写一个 cat 即可\nlabel_map_path = os.path.join(data_root, 'cat_label_map.json')\nwith open(label_map_path, 'w') as f:\n    json.dump({'0': 'cat'}, f)\n```\n\n----------------------------------------\n\nTITLE: Keep Latest Model Configuration (2.x)\nDESCRIPTION: This snippet configures the maximum number of checkpoints to keep in MMDetection 2.x. The `max_keep_ckpts` parameter specifies the number of latest checkpoints to retain.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_config = dict(\n    max_keep_ckpts=3)\n```\n\n----------------------------------------\n\nTITLE: Convert V3Det to ODVG format (coco2odvg.py)\nDESCRIPTION: This script converts the V3Det dataset from COCO format to ODVG format, creating `v3det_2023_v1_train_od.json` and `v3det_2023_v1_label_map.json` within the `data/v3det/annotations` directory.\nDependencies: coco2odvg.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/v3det/annotations/v3det_2023_v1_train.json -d v3det\n```\n\n----------------------------------------\n\nTITLE: Freezing and Unfreezing Backbone Network in Faster R-CNN (Python)\nDESCRIPTION: This example shows how to freeze and later unfreeze a ResNet backbone network during training in MMDetection's Faster R-CNN. It freezes the initial stages of the backbone and then uses a custom hook, `UnfreezeBackboneEpochBasedHook`, to unfreeze the frozen stages after a specified number of training epochs. This allows for initial training with a frozen backbone, followed by fine-tuning of the entire network.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/how_to.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    # freeze one stage of the backbone network.\n    backbone=dict(frozen_stages=1),\n)\ncustom_hooks = [dict(type=\"UnfreezeBackboneEpochBasedHook\", unfreeze_epoch=1)]\n\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.model import is_model_wrapper\nfrom mmengine.hooks import Hook\nfrom mmdet.registry import HOOKS\n\n\n@HOOKS.register_module()\nclass UnfreezeBackboneEpochBasedHook(Hook):\n    \"\"\"Unfreeze backbone network Hook.\n\n    Args:\n        unfreeze_epoch (int): The epoch unfreezing the backbone network.\n    \"\"\"\n\n    def __init__(self, unfreeze_epoch=1):\n        self.unfreeze_epoch = unfreeze_epoch\n\n    def before_train_epoch(self, runner):\n        # Unfreeze the backbone network.\n        # Only valid for resnet.\n        if runner.epoch == self.unfreeze_epoch:\n            model = runner.model\n            if is_module_wrapper(model):\n                model = model.module\n            backbone = model.backbone\n            if backbone.frozen_stages >= 0:\n                if backbone.deep_stem:\n                    backbone.stem.train()\n                    for param in backbone.stem.parameters():\n                        param.requires_grad = True\n                else:\n                    backbone.norm1.train()\n                    for m in [backbone.conv1, backbone.norm1]:\n                        for param in m.parameters():\n                            param.requires_grad = True\n\n            for i in range(1, backbone.frozen_stages + 1):\n                m = getattr(backbone, f'layer{i}')\n                m.train()\n                for param in m.parameters():\n                    param.requires_grad = True\n\n```\n\n----------------------------------------\n\nTITLE: VFNet Configuration - R-101 Backbone, MDConv, MS-train, 2x Schedule\nDESCRIPTION: This configuration file configures VarifocalNet with a ResNet-101 backbone, utilizing Modulated Deformable Convolution (MDConv) in the C3-C5 stages. It's trained using multi-scale training and a 2x learning rate schedule on the COCO dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/vfnet/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[config](./vfnet_r101-mdconv-c3-c5_fpn_ms-2x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: JSON Annotation Example\nDESCRIPTION: This snippet demonstrates the format of a JSON annotation file for an intermediate data format.  It contains 'metainfo' (e.g., class names) and 'data_list', a list of data instances each containing image path, dimensions, and instance annotations (bounding boxes, labels, ignore flags).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    'metainfo':\n        {\n            'classes': ('person', 'bicycle', 'car', 'motorcycle'),\n            ...\n        },\n    'data_list':\n        [\n            {\n                \"img_path\": \"xxx/xxx_1.jpg\",\n                \"height\": 604,\n                \"width\": 640,\n                \"instances\":\n                [\n                  {\n                    \"bbox\": [0, 0, 10, 20],\n                    \"bbox_label\": 1,\n                    \"ignore_flag\": 0\n                  },\n                  {\n                    \"bbox\": [10, 10, 110, 120],\n                    \"bbox_label\": 2,\n                    \"ignore_flag\": 0\n                  }\n                ]\n              },\n            {\n                \"img_path\": \"xxx/xxx_2.jpg\",\n                \"height\": 320,\n                \"width\": 460,\n                \"instances\":\n                [\n                  {\n                    \"bbox\": [10, 0, 20, 20],\n                    \"bbox_label\": 3,\n                    \"ignore_flag\": 1\n                  }\n                ]\n              },\n            ...\n        ]\n}\n```\n\n----------------------------------------\n\nTITLE: Convert MOT dataset to ReID format\nDESCRIPTION: This script converts the MOT17 dataset into a ReID-friendly format, creating image crops and annotation files suitable for training a ReID model. It takes the input and output directories, validation split ratio, and visibility threshold as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/reid/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/mot2reid.py -i ./data/MOT17/ -o ./data/MOT17/reid --val-split 0.2 --vis-threshold 0.3\n```\n\n----------------------------------------\n\nTITLE: Running MMCV Import Check in Python\nDESCRIPTION: This snippet verifies if MMCV is correctly installed by importing the mmcv and mmcv.ops modules. If the import is successful, it indicates that MMCV is properly installed and accessible.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import mmcv; import mmcv.ops'\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for HRNet Pixel and Region Labeling\nDESCRIPTION: This LaTeX snippet provides the citation information for the paper on High-Resolution Representations for Labeling Pixels and Regions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/hrnet/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{SunZJCXLMWLW19,\n  title={High-Resolution Representations for Labeling Pixels and Regions},\n  author={Ke Sun and Yang Zhao and Borui Jiang and Tianheng Cheng and Bin Xiao\n  and Dong Liu and Yadong Mu and Xinggang Wang and Wenyu Liu and Jingdong Wang},\n  journal   = {CoRR},\n  volume    = {abs/1904.04514},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Training a Model with iSAID Dataset (MMDetection)\nDESCRIPTION: This command initiates the training process of a model using the iSAID dataset within the MMDetection framework.  It uses the `tools/train.py` script and specifies the configuration file for the model, which is located in the `projects/iSAID/configs` directory. The configuration file defines the model architecture, training parameters, and dataset paths.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/iSAID/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython tools/train.py projects/iSAID/configs/mask_rcnn_r50_fpn_1x_isaid.py\n```\n\n----------------------------------------\n\nTITLE: Res2Net BibTeX Citation\nDESCRIPTION: This BibTeX entry provides the citation information for the Res2Net paper, including the title, authors, journal, year, and DOI. It is used to properly credit the original authors when using Res2Net in research or publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/res2net/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{gao2019res2net,\n  title={Res2Net: A New Multi-scale Backbone Architecture},\n  author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n  journal={IEEE TPAMI},\n  year={2020},\n  doi={10.1109/TPAMI.2019.2938758},\n}\n```\n\n----------------------------------------\n\nTITLE: MOT Challenge to COCO Conversion\nDESCRIPTION: Converts MOT Challenge dataset annotations to COCO format.  This script uses `mot2coco.py` to convert MOT Challenge annotations to COCO format and `mot2reid.py` to create reid annotations.  It requires the path to the MOT Challenge dataset and the desired output path for the COCO annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_dataset_prepare.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# MOT17\n# 其他 MOT Challenge 数据集的处理方式与 MOT17 相同。\npython ./tools/dataset_converters/mot2coco.py -i ./data/MOT17/ -o ./data/MOT17/annotations --split-train --convert-det\npython ./tools/dataset_converters/mot2reid.py -i ./data/MOT17/ -o ./data/MOT17/reid --val-split 0.2 --vis-threshold 0.3\n```\n\n----------------------------------------\n\nTITLE: Split ODVG Dataset (split_odvg.py)\nDESCRIPTION: This script splits a large ODVG dataset into a smaller subset for easier inspection and debugging.  It creates a new directory with a limited number of images and corresponding JSON annotations. Requires specifying the input data directory, annotation file, output directory, label map file and the number of images to include in the split.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/split_odvg.py data/object365_v1/ o365v1_train_od.json train your_output_dir --label-map-file o365v1_label_map.json -n 200\n```\n\n----------------------------------------\n\nTITLE: Optimizer Configuration (3.x)\nDESCRIPTION: This code snippet demonstrates the optimizer configuration in MMDetection 3.x.  It defines the optim_wrapper type, optimizer type, learning rate, momentum, weight decay, and gradient clipping configuration. Uses the `OptimWrapper` class.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(  # 优化器封装的配置\n    type='OptimWrapper',  # 优化器封装的类型。可以切换至 AmpOptimWrapper 来启用混合精度训练\n    optimizer=dict(  # 优化器配置。支持 PyTorch 的各种优化器。请参考 https://pytorch.org/docs/stable/optim.html#algorithms\n        type='SGD',  # 随机梯度下降优化器\n        lr=0.02,  # 基础学习率\n        momentum=0.9,  # 带动量的随机梯度下降\n        weight_decay=0.0001),  # 权重衰减\n    clip_grad=None,  # 梯度裁剪的配置，设置为 None 关闭梯度裁剪。使用方法请见 https://mmengine.readthedocs.io/en/latest/tutorials/optimizer.html\n    )\n```\n\n----------------------------------------\n\nTITLE: Train Dataloader Configuration (Python)\nDESCRIPTION: This Python code configures the training data loader for semi-supervised object detection. It specifies the batch size, number of workers, and persistent worker settings.  A `GroupMultiSourceSampler` is used to sample data from both labeled and unlabeled datasets, with a specified `source_ratio` to control the proportion of labeled and unlabeled data in each batch. The `ConcatDataset` combines the labeled and unlabeled datasets into a single dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ntrain_dataloader = dict(\n    batch_size=batch_size,\n    num_workers=num_workers,\n    persistent_workers=True,\n    sampler=dict(\n        type='GroupMultiSourceSampler',\n        batch_size=batch_size,\n        source_ratio=[1, 4]),\n    dataset=dict(\n        type='ConcatDataset', datasets=[labeled_dataset, unlabeled_dataset]))\n```\n\n----------------------------------------\n\nTITLE: CityScapes Evaluator Configuration (2.x)\nDESCRIPTION: This code snippet shows the CityScapes dataset and evaluation configuration in MMDetection 2.x. It defines the dataset type, annotation file, image prefix, and pipeline for the validation set, along with the evaluation metrics (bbox and segm). Requires `data_root` and `test_pipeline` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type='CityScapesDataset',\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        img_prefix=data_root + 'leftImg8bit/val/',\n        pipeline=test_pipeline))\nevaluation = dict(metric=['bbox', 'segm'])\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Interval Configuration (2.x)\nDESCRIPTION: This snippet configures the checkpoint saving interval in MMDetection 2.x. The `interval` parameter specifies how often checkpoints are saved (in epochs).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_config = dict(\n    interval=1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataloader arguments in MMDetection\nDESCRIPTION: This snippet compares the old and new ways to configure dataloader arguments in MMDetection. The new method provides a clearer separation of data loading and dataset configuration. The older style works for the training dataloader but the new style allow more parameters to be set.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    samples_per_gpu=64, workers_per_gpu=4,\n    train=dict(type='xxx', ...),\n    val=dict(type='xxx', samples_per_gpu=4, ...),\n    test=dict(type='xxx', ...),\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# A recommended config that is clear\ndata = dict(\n    train=dict(type='xxx', ...),\n    val=dict(type='xxx', ...),\n    test=dict(type='xxx', ...),\n    # Use different batch size during inference.\n    train_dataloader=dict(samples_per_gpu=64, workers_per_gpu=4),\n    val_dataloader=dict(samples_per_gpu=8, workers_per_gpu=2),\n    test_dataloader=dict(samples_per_gpu=8, workers_per_gpu=2),\n)\n\n# Old style still works but allows to set more arguments about data loaders\ndata = dict(\n    samples_per_gpu=64,  # only works for train_dataloader\n    workers_per_gpu=4,  # only works for train_dataloader\n    train=dict(type='xxx', ...),\n    val=dict(type='xxx', ...),\n    test=dict(type='xxx', ...),\n    # Use different batch size during inference.\n    val_dataloader=dict(samples_per_gpu=8, workers_per_gpu=2),\n    test_dataloader=dict(samples_per_gpu=8, workers_per_gpu=2),\n)\n```\n\n----------------------------------------\n\nTITLE: Citation for Libra R-CNN (IJCV 2021)\nDESCRIPTION: This LaTeX snippet provides the citation information for the extended version of Libra R-CNN published in the International Journal of Computer Vision (IJCV) in 2021. It details the title, authors, journal, volume, number, pages, year, and publisher, enabling correct citation of the IJCV publication on Libra R-CNN.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/libra_rcnn/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{pang2021towards,\n  title={Towards Balanced Learning for Instance Recognition},\n  author={Pang, Jiangmiao and Chen, Kai and Li, Qi and Xu, Zhihai and Feng, Huajun and Shi, Jianping and Ouyang, Wanli and Lin, Dahua},\n  journal={International Journal of Computer Vision},\n  volume={129},\n  number={5},\n  pages={1376--1393},\n  year={2021},\n  publisher={Springer}\n}\n```\n\n----------------------------------------\n\nTITLE: GRIT-20M Data Processing\nDESCRIPTION: This command processes the raw GRIT-20M dataset downloaded using img2dataset. It uses the `grit_processing.py` script from the `tools/dataset_converters` directory. The script transforms the raw data located in `data/grit_raw` and stores the processed data in `data/grit_processed`. This involves restructuring the dataset for easier access and use in subsequent steps.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/grit_processing.py data/grit_raw data/grit_processed\n```\n\n----------------------------------------\n\nTITLE: Citation for Swin Transformer (LaTeX)\nDESCRIPTION: This snippet provides the LaTeX code to cite the Swin Transformer paper. It includes the title, authors, journal (arXiv preprint), and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/swin/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{liu2021Swin,\n    title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},\n    author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},\n    journal={arXiv preprint arXiv:2103.14030},\n    year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Directory Structure\nDESCRIPTION: This shows the required directory structure for the COCO dataset, including annotations, train/val/test images, and the stuffthingmaps.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/scnet/README.md#_snippet_0\n\nLANGUAGE: None\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n|   |   ├── stuffthingmaps\n```\n\n----------------------------------------\n\nTITLE: Retry Training (Shell)\nDESCRIPTION: This command retries training on a subset of datasets specified in a text file. It uses the `RETRY_PATH` environment variable to point to the text file containing the list of datasets to retry. This script allows re-running failed training runs, improving the benchmark's robustness.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nRETRY_PATH=failed_dataset_list.txt bash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration (2.x Visualisation)\nDESCRIPTION: This code snippet shows how to configure logging to TensorBoard and WandB in MMDetection 2.x. It includes hooks for `TextLoggerHook`, `TensorboardLoggerHook`, and `MMDetWandbHook`, configuring the project, group, interval, and other parameters for WandB logging. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook'),\n        dict(type='MMDetWandbHook',\n             init_kwargs={\n                'project': 'mmdetection',\n                'group': 'maskrcnn-r50-fpn-1x-coco'\n             },\n             interval=50,\n             log_checkpoint=True,\n             log_checkpoint_metadata=True,\n             num_eval_images=100)\n    ])\n```\n\n----------------------------------------\n\nTITLE: Citation Information\nDESCRIPTION: This LaTeX code provides the citation information for the XDecoder paper. It includes the authors, title, publisher, and year of publication, which is crucial for referencing the work in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_18\n\nLANGUAGE: latex\nCODE:\n```\n@article{zou2022xdecoder,\n  author      = {Zou*, Xueyan and Dou*, Zi-Yi and Yang*, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee*, Yong Jae and Gao*, Jianfeng},\n  title       = {Generalized Decoding for Pixel, Image and Language},\n  publisher   = {arXiv},\n  year        = {2022},\n}\n```\n\n----------------------------------------\n\nTITLE: Fix Object365 v2 Class Names (fix_o365_names.py)\nDESCRIPTION: This script fixes incorrect class names in the Objects365 v2 dataset. It generates a new annotation file (`zhiyuan_objv2_train_fixname.json`) with corrected names in the `data/objects365v2/annotations` directory.\nDependencies: fix_o365_names.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/fix_o365_names.py\n```\n\n----------------------------------------\n\nTITLE: Convert Self-Supervised Model to MMDetection Format (Bash)\nDESCRIPTION: This script converts models pretrained using self-supervised methods (MoCo, SwAV, etc.) into the PyTorch-style checkpoint format compatible with MMDetection. It takes the path to the original pretrained model and the desired output path as arguments, along with the selfsup method.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/selfsup_pretrain/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -u tools/model_converters/selfsup2mmdet.py ${PRETRAIN_PATH} ${STORE_PATH} --selfsup ${method}\n```\n\n----------------------------------------\n\nTITLE: Split Coco Dataset (Shell)\nDESCRIPTION: This script splits the COCO train2017 dataset into labeled and unlabeled subsets with varying percentages of labeled data (1%, 2%, 5%, and 10%). It generates annotation files for both labeled and unlabeled sets, facilitating cross-validation for semi-supervised learning experiments.  The script is configured to repeat each split five times for cross-validation purposes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/misc/split_coco.py\n```\n\n----------------------------------------\n\nTITLE: Configure Online Evaluation for Occluded/Separated Recall\nDESCRIPTION: This snippet shows how to configure the `CocoOccludedSeparatedMetric` evaluator for online evaluation during training.  This involves modifying the `val_evaluator` and `test_evaluator` in the configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='CocoOccludedSeparatedMetric',  # 修改此处\n    ann_file=data_root + 'annotations/instances_val2017.json',\n    metric=['bbox', 'segm'],\n    format_only=False)\ntest_evaluator = val_evaluator\n```\n\n----------------------------------------\n\nTITLE: Large Image Inference Demo Script\nDESCRIPTION: This script performs object detection inference on large images by dividing them into patches and merging the results. It supports specifying the device, showing the results, using test-time augmentation (TTA), setting the confidence threshold, patch size, overlap ratio, merge IOU threshold, merge NMS type, batch size, debugging mode, and saving patches.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\npython demo/large_image_demo.py \\\n\t${IMG_PATH} \\\n\t${CONFIG_FILE} \\\n\t${CHECKPOINT_FILE} \\\n\t--device ${GPU_ID}  \\\n\t--show \\\n\t--tta  \\\n\t--score-thr ${SCORE_THR} \\\n\t--patch-size ${PATCH_SIZE} \\\n\t--patch-overlap-ratio ${PATCH_OVERLAP_RATIO} \\\n\t--merge-iou-thr ${MERGE_IOU_THR} \\\n\t--merge-nms-type ${MERGE_NMS_TYPE} \\\n\t--batch-size ${BATCH_SIZE} \\\n\t--debug \\\n\t--save-patch\n```\n\n----------------------------------------\n\nTITLE: Citation for Timm\nDESCRIPTION: This is the BibTeX entry for citing the PyTorch Image Models (timm) library in academic works.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/timm_example/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n\n----------------------------------------\n\nTITLE: Convert ADE20K Annotations to COCO Format (Python)\nDESCRIPTION: These commands move annotation files and then convert ADE20K annotations into COCO format for panoptic and instance segmentation tasks using a Python script in MMDetection. This requires the ADE20K dataset to be downloaded first.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nmv data/annotations_instance data/ADEChallengeData2016/\nmv data/categoryMapping.txt data/ADEChallengeData2016/\nmv data/imgCatIds.json data/ADEChallengeData2016/\npython tools/dataset_converters/ade20k2coco.py data/ADEChallengeData2016 --task panoptic\npython tools/dataset_converters/ade20k2coco.py data/ADEChallengeData2016 --task instance\n```\n\n----------------------------------------\n\nTITLE: VISION Datasets Final Directory Structure\nDESCRIPTION: Illustrates the final directory structure of the VISION Datasets after the .tar.gz files have been extracted. Each dataset (e.g., Cable, Capacitor) will have its own directory containing train, val, and inference subsets, with COCO format annotations and images. It details how images and annotation files are organized after unpacking the archives.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/VISION-Datasets/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n|   │── VISION-Datasets\n|   │   ├── Cable.tar.gz\n|   │   ├── Capacitor.tar.gz\n|   │   ├── Casting.tar.gz\n|   |   ├── Console.tar.gz\n|   │   ├── Cylinder.tar.gz\n|   │   ├── Electronics.tar.gz\n|   │   ├── Groove.tar.gz\n|   │   ├── Hemisphere.tar.gz\n|   │   ├── Lens.tar.gz\n|   │   ├── PCB_1.tar.gz\n|   │   ├── PCB_2.tar.gz\n|   |   ├── README.md\n|   │   ├── Ring.tar.gz\n|   │   ├── Screw.tar.gz\n|   │   └── Wood.tar.gz\n|   │   ├── Cable\n|   │   |    |── train\n|   │   |    |     |──  _annotations.coco.json # COCO format annotation\n|   │   |    |     |──  000001.png # Images\n|   │   |    |     |──  000002.png\n|   │   |    |     |──  xxxxxx.png\n|   │   |    |── val\n|   │   |    |     |──  _annotations.coco.json # COCO format annotation\n|   │   |    |     |──  xxxxxx.png # Images\n|   │   |    |── inference\n|   │   |    |     |──  _annotations.coco.json # COCO format annotation with unlabeled image list only\n|   │   |    |     |──  xxxxxx.png # Images\n...\n```\n\n----------------------------------------\n\nTITLE: Labeled Data Pipeline Configuration (Python)\nDESCRIPTION: This Python code defines the data augmentation pipeline for labeled data in a semi-supervised object detection setting. It includes loading images and annotations, random resizing, random flipping, RandAugment for color space augmentation, and filtering annotations based on bounding box size. The final step packs the data into the format expected by the detection model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment labeled data,\n# which will be sent to student model for supervised training.\nsup_pipeline = [\n    dict(type='LoadImageFromFile',backend_args = backend_args),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='RandomResize', scale=scale, keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='RandAugment', aug_space=color_space, aug_num=1),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1e-2, 1e-2)),\n    dict(type='MultiBranch', sup=dict(type='PackDetInputs'))\n]\n```\n\n----------------------------------------\n\nTITLE: Configure RetinaNet with SemiBaseDetector for Semi-Supervised Learning (Python)\nDESCRIPTION: This snippet configures RetinaNet with SemiBaseDetector for semi-supervised learning. It demonstrates how to use SemiBaseDetector with other detection models besides Faster R-CNN. It sets up the model structure, data preprocessor, and semi-supervised training configurations, including pseudo-labeling thresholds and weight parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py', '../_base_/default_runtime.py',\n    '../_base_/datasets/semi_coco_detection.py'\n]\n\ndetector = _base_.model\n\nmodel = dict(\n    _delete_=True,\n    type='SemiBaseDetector',\n    detector=detector,\n    data_preprocessor=dict(\n        type='MultiBranchDataPreprocessor',\n        data_preprocessor=detector.data_preprocessor),\n    semi_train_cfg=dict(\n        freeze_teacher=True,\n        sup_weight=1.0,\n        unsup_weight=1.0,\n        cls_pseudo_thr=0.9,\n        min_pseudo_bbox_wh=(1e-2, 1e-2)),\n    semi_test_cfg=dict(predict_on='teacher'))\n```\n\n----------------------------------------\n\nTITLE: Pascal VOC Evaluator Configuration (3.x)\nDESCRIPTION: This code snippet shows the Pascal VOC evaluator configuration in MMDetection 3.x. It specifies the evaluator type as `VOCMetric`, the mAP metric, and the evaluation mode as `11points`. No annotation file is needed. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='VOCMetric',\n    metric='mAP',\n    eval_mode='11points')\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Mask RCNN model using a RegNetX-4GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_28\n\nLANGUAGE: text\nCODE:\n```\n[config](./mask-rcnn_regnetx-4GF_fpn_ms-poly-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Configuration (3.x)\nDESCRIPTION: This code snippet demonstrates the learning rate configuration in MMDetection 3.x.  It uses a list of parameter schedulers, including `LinearLR` for warmup and `MultiStepLR` for step-wise decay.  It defines the start factor, warmup iterations, milestones, and gamma for learning rate decay.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='LinearLR',  # 使用线性学习率预热\n        start_factor=0.001, # 学习率预热的系数\n        by_epoch=False,  # 按 iteration 更新预热学习率\n        begin=0,  # 从第一个 iteration 开始\n        end=500),  # 到第 500 个 iteration 结束\n    dict(\n        type='MultiStepLR',  # 在训练过程中使用 multi step 学习率策略\n        by_epoch=True,  # 按 epoch 更新学习率\n        begin=0,   # 从第一个 epoch 开始\n        end=12,  # 到第 12 个 epoch 结束\n        milestones=[8, 11],  # 在哪几个 epoch 进行学习率衰减\n        gamma=0.1)  # 学习率衰减系数\n]\n```\n\n----------------------------------------\n\nTITLE: Training with Slurm Job Scheduler\nDESCRIPTION: This snippet demonstrates how to train a model using the Slurm job scheduler with MMDetection. It uses the `tools/slurm_train.sh` script, specifying the partition, job name, configuration file, working directory, and number of GPUs. This script facilitates submitting training jobs to a Slurm-managed cluster.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_8\n\nLANGUAGE: shell 脚本\nCODE:\n```\nbash ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR} ${GPUS}\n```\n\n----------------------------------------\n\nTITLE: Convert GRIT dataset to ODVG format (grit2odvg.py)\nDESCRIPTION: This script converts the processed GRIT dataset to ODVG format, generating `grit20m_vg.json` in the `data/grit_processed` directory. The dataset contains approximately 9M data entries.\nDependencies: grit2odvg.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/grit2odvg.py data/grit_processed/\n```\n\n----------------------------------------\n\nTITLE: RandomChoiceResize Config (MMDetection)\nDESCRIPTION: This code snippet demonstrates the configuration for the RandomChoiceResize transform in both MMDetection 2.x and 3.x. The key change is the parameter name: `img_scale` in 2.x is replaced with `scales` in 3.x and `multiscale_mode` is removed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='Resize',\n    img_scale=[\n        (1333, 640), (1333, 672),\n        (1333, 704), (1333, 736),\n        (1333, 768), (1333, 800)],\n    multiscale_mode='value',\n    keep_ratio=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(\n    type='RandomChoiceResize',\n    scales=[\n        (1333, 640), (1333, 672),\n        (1333, 704), (1333, 736),\n        (1333, 768), (1333, 800)],\n    keep_ratio=True)\n```\n\n----------------------------------------\n\nTITLE: Migrating Test Pipeline Config (MMDetection 3.x)\nDESCRIPTION: This code shows the test pipeline configuration in MMDetection 3.x. It includes data loading, resizing, and packing data inputs, removing `Normalize`, `Pad`, and `MultiScaleFlipAug`.  It specifies which meta keys to retain.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\n\n```\n\n----------------------------------------\n\nTITLE: Unlabeled Dataset Configuration (Python)\nDESCRIPTION: This Python code configures the unlabeled dataset for semi-supervised object detection. It specifies the dataset type, root directory, annotation file, and data prefix, as well as filtering configurations and the data pipeline defined earlier. The 'filter_empty_gt' option is set to False, meaning even images without object annotations are included.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nunlabeled_dataset = dict(\n    type=dataset_type,\n    data_root=data_root,\n    ann_file='annotations/instances_unlabeled2017.json',\n    data_prefix=dict(img='unlabeled2017/'),\n    filter_cfg=dict(filter_empty_gt=False),\n    pipeline=unsup_pipeline)\n```\n\n----------------------------------------\n\nTITLE: CPU Testing with TTA\nDESCRIPTION: This shell command runs the testing script with TTA enabled, but disables GPU usage by setting `CUDA_VISIBLE_DEVICES` to -1, thus performing testing on the CPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n# CPU 测试：禁用 GPU 并运行单 GPU 测试脚本\nexport CUDA_VISIBLE_DEVICES=-1\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    [--out ${RESULT_FILE}] \\\n    [--tta]\n```\n\n----------------------------------------\n\nTITLE: Example: Train QDTrack on CPU\nDESCRIPTION: This snippet provides a concrete example of training the QDTrack model on the CPU using a specific configuration file. The CUDA_VISIBLE_DEVICES variable is set to -1 to disable the GPU, and the training script is executed with the config file for the QDTrack model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_1\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/train.py configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py\n```\n\n----------------------------------------\n\nTITLE: Citing MMDetection in BibTeX\nDESCRIPTION: This BibTeX entry provides the citation format for the MMDetection project. It includes the title, authors, journal, and year of publication, allowing researchers to properly credit the project in their work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/README.md#_snippet_0\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{mmdetection,\n  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},\n  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and\n             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and\n             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and\n             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and\n             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong\n             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},\n  journal= {arXiv preprint arXiv:1906.07155},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Install Label-Studio and label-studio-ml-backend\nDESCRIPTION: This snippet installs Label-Studio and its machine learning backend using pip. It installs specific versions of both packages (1.7.2 and 1.0.9 respectively).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# 安装 label-studio 需要一段时间,如果找不到版本请使用官方源\npip install label-studio==1.7.2\npip install label-studio-ml==1.0.9\n```\n\n----------------------------------------\n\nTITLE: Create and Activate Conda Environment\nDESCRIPTION: Creates a new conda virtual environment named 'rtmdet' with Python 3.9 and activates it. This isolates the project dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n rtmdet python=3.9 -y\nconda activate rtmdet\n```\n\n----------------------------------------\n\nTITLE: Distributed Inference on COCO Test-Dev with Multiple GPUs and Config Overrides\nDESCRIPTION: This shell script executes distributed inference on the COCO test-dev dataset using multiple GPUs. It utilizes `tools/dist_test.sh` for distributed execution and overrides configuration settings with `--cfg-options`.  It defines the number of GPUs to be used and sets dataset paths dynamically.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# test with four gpus\nCUDA_VISIBLE_DEVICES=0,1,3,4 bash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    8 \\  # eights gpus\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=${WORK_DIR}/results\n```\n\n----------------------------------------\n\nTITLE: Configuring Mask R-CNN with GC Block (r=4) and X-101-FPN in MMDetection\nDESCRIPTION: This config file specifies a Mask R-CNN model that utilizes an X-101-FPN backbone. A Global Context (GC) block with a reduction ratio of 4 is included after the 1x1 convolution layers of the c3-c5 stages of the backbone. It uses SyncBN and is trained for 1x epochs on COCO.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[config](./mask-rcnn_x101-32x4d-syncbn-gcb-r4-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Slurm Training Shell Command\nDESCRIPTION: This shell command demonstrates how to launch training jobs on a cluster managed by Slurm using the `slurm_train.sh` script.  `${PARTITION}` specifies the Slurm partition, `${JOB_NAME}` is the name of the job, `${CONFIG_FILE}` is the path to the configuration file, and `${WORK_DIR}` is the working directory for saving logs and checkpoints.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n[GPUS=${GPUS}] ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n```\n\n----------------------------------------\n\nTITLE: Cityscapes Citation (LaTeX)\nDESCRIPTION: This LaTeX snippet provides the citation information for the Cityscapes dataset, allowing users to properly credit the dataset in their research publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/cityscapes/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{Cordts2016Cityscapes,\n   title={The Cityscapes Dataset for Semantic Urban Scene Understanding},\n   author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},\n   booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n   year={2016}\n}\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Faster RCNN model with a RegNetX-3.2GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-3_20210526_095152.log.json)\n```\n\n----------------------------------------\n\nTITLE: Build MMDetection Docker Image\nDESCRIPTION: This command builds a Docker image for MMDetection based on the provided Dockerfile. Adjust the Dockerfile to specify desired PyTorch and CUDA versions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\ndocker build -t mmdetection docker/\n```\n\n----------------------------------------\n\nTITLE: Inheriting a Single Configuration File in MMDetection\nDESCRIPTION: This code snippet shows how to inherit from a single base configuration file in MMDetection. The `_base_` variable specifies the path to the base configuration file, allowing the current configuration to inherit its settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\n```\n\n----------------------------------------\n\nTITLE: Build MMDetection Docker Image (Shell)\nDESCRIPTION: This snippet builds a Docker image for MMDetection using the provided Dockerfile. It tags the image as `mmdetection`.  The Dockerfile should specify the base image (e.g., with PyTorch and CUDA), dependencies, and the MMDetection source code.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n# build an image with PyTorch 1.9, CUDA 11.1\n# If you prefer other versions, just modified the Dockerfile\ndocker build -t mmdetection docker/\n```\n\n----------------------------------------\n\nTITLE: Run Inference Demo with MIM (Python)\nDESCRIPTION: Initializes a detector using a config and checkpoint file, then runs inference on a demo image.  This is a Python-based verification method for MIM installations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.apis import init_detector, inference_detector\n\nconfig_file = 'rtmdet_tiny_8xb32-300e_coco.py'\ncheckpoint_file = 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\nmodel = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\ninference_detector(model, 'demo/demo.jpg')\n```\n\n----------------------------------------\n\nTITLE: OpenImages Evaluator Configuration (2.x)\nDESCRIPTION: This code snippet demonstrates the OpenImages dataset and evaluation configuration in MMDetection 2.x. It defines the dataset type, annotation files, image prefix, label file, hierarchy file, meta file, and image-level annotation file for the validation set, along with the mAP metric. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type='OpenImagesDataset',\n        ann_file=data_root + 'annotations/validation-annotations-bbox.csv',\n        img_prefix=data_root + 'OpenImages/validation/',\n        label_file=data_root + 'annotations/class-descriptions-boxable.csv',\n        hierarchy_file=data_root +\n        'annotations/bbox_labels_600_hierarchy.json',\n        meta_file=data_root + 'annotations/validation-image-metas.pkl',\n        image_level_ann_file=data_root +\n        'annotations/validation-annotations-human-imagelabels-boxable.csv'))\nevaluation = dict(interval=1, metric='mAP')\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessor Config in MMDetection 3.x\nDESCRIPTION: This code snippet presents the DataPreprocessor configuration introduced in MMDetection 3.x. It defines the normalization and padding parameters within the 'DetDataPreprocessor' type. This module replaces the 'Normalize' and 'Pad' modules used in earlier versions. It includes parameters for image normalization, BGR to RGB conversion, and image padding.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    data_preprocessor=dict(\n        type='DetDataPreprocessor',\n        # Image normalization parameters\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True,\n        # Image padding parameters\n        pad_mask=True,  # In instance segmentation, the mask needs to be padded\n        pad_size_divisor=32)  # Padding the image to multiples of 32\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Weights from MMEngine\nDESCRIPTION: This snippet initializes the DetInferencer by providing only the path to the weights file. This assumes that the weights file was trained using MMEngine and contains the configuration information. If the configuration file cannot be found in the weights, an error will be raised. Only ddq-detr-4scale_r50 model is able to be loaded in MMDetection model zoo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# 如果无法在权重中找到配置文件，则会引发错误。目前 MMDetection 模型库中只有 ddq-detr-4scale_r50 的权重可以这样加载。\ninferencer = DetInferencer(weights='https://download.openmmlab.com/mmdetection/v3.0/ddq/ddq-detr-4scale_r50_8xb2-12e_coco/ddq-detr-4scale_r50_8xb2-12e_coco_20230809_170711-42528127.pth')\n```\n\n----------------------------------------\n\nTITLE: Configure FCOS-RPN for Proposal Evaluation (Python)\nDESCRIPTION: This code configures an FCOS-based RPN model for evaluating proposal quality within the MMDetection framework. The configuration file includes model architecture, dataset settings, and evaluation metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/single_stage_as_rpn.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n_base_ = [\n    '../_base_/models/rpn_r50_fpn.py', '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nval_evaluator = dict(metric='proposal_fast')\ntest_evaluator = val_evaluator\nmodel = dict(\n    # 从 configs/fcos/fcos_r50-caffe_fpn_gn-head_1x_coco.py 复制\n    neck=dict(\n        start_level=1,\n        add_extra_convs='on_output',  # 使用 P5\n        relu_before_extra_convs=True),\n    rpn_head=dict(\n        _delete_=True,  # 忽略未使用的旧设置\n        type='FCOSHead',\n        num_classes=1,  # 对于 rpn, num_classes = 1，如果 num_classes >为1，它将在 rpn 中自动设置为1\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n```\n\n----------------------------------------\n\nTITLE: Printing the entire config\nDESCRIPTION: This command prints the whole config verbatim, expanding all its imports. It requires a config file path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/print_config.py ${CONFIG} [-h] [--options ${OPTIONS [OPTIONS...]}]\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Directory Structure\nDESCRIPTION: Shows the directory structure required for the COCO dataset when using PanopticFPN. This is necessary for training and evaluation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/panoptic_fpn/README.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   │   ├── panoptic_train2017.json\n│   │   │   ├── panoptic_train2017\n│   │   │   ├── panoptic_val2017.json\n│   │   │   ├── panoptic_val2017\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n```\n\n----------------------------------------\n\nTITLE: Configuring Epoch-Based Training Loop in MMDetection\nDESCRIPTION: This code snippet shows how to configure an epoch-based training loop in MMDetection using `EpochBasedTrainLoop`, specifying the maximum epochs, validation start epoch, and validation interval.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_begin=1, val_interval=1)\n\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Modules in Config\nDESCRIPTION: Imports custom modules directly in the configuration file using `custom_imports`. This avoids modifying the original MMDetection codebase.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(\n    imports=['mmdet.models.necks.augfpn'],\n    allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: RTMDet Single GPU Testing with Visualization\nDESCRIPTION: This shell command tests the RTMDet model using a specified configuration file and checkpoint file, while visualizing the results. The `--show` flag enables displaying the results in a new window.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n       configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n       checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n       --show\n```\n\n----------------------------------------\n\nTITLE: Installing imagecorruptions Python Package\nDESCRIPTION: This command installs the `imagecorruptions` Python package, which provides the image corruption functions used in the benchmark. This package needs to be installed before running the robustness tests.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install imagecorruptions\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for CornerNet\nDESCRIPTION: This BibTeX entry provides the citation information for the CornerNet paper, which describes the object detection method using paired keypoints.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/cornernet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{law2018cornernet,\n  title={Cornernet: Detecting objects as paired keypoints},\n  author={Law, Hei and Deng, Jia},\n  booktitle={15th European Conference on Computer Vision, ECCV 2018},\n  pages={765--781},\n  year={2018},\n  organization={Springer Verlag}\n}\n```\n\n----------------------------------------\n\nTITLE: Install DSDL via pip\nDESCRIPTION: This code snippet demonstrates how to install the DSDL (Data Set Description Language) library using pip, the Python package installer. This is the simplest method to get started with DSDL.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dsdl/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dsdl\n```\n\n----------------------------------------\n\nTITLE: Testing Instance Segmentation on ADE20K with X-Decoder\nDESCRIPTION: This command tests the X-Decoder model for instance segmentation on the ADE20K dataset. It uses the `dist_test.sh` script for distributed testing and specifies the configuration file, pre-trained weights, and number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-instance_ade20k.py xdecoder_focalt_best_openseg.pt 8\n```\n\n----------------------------------------\n\nTITLE: Configure MeanTeacherHook (Python)\nDESCRIPTION: This snippet configures the MeanTeacherHook, which updates the teacher model using Exponential Moving Average (EMA) of the student model. This hook is essential for the teacher-student training framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [dict(type='MeanTeacherHook')]\n```\n\n----------------------------------------\n\nTITLE: Renaming Mask2Former config files in MMDetection\nDESCRIPTION: This snippet illustrates the renaming of Mask2Former configuration files between MMDetection versions prior to 2.25.0 and version 2.25.0 and later, specifically distinguishing between configurations for panoptic segmentation and instance segmentation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/compatibility.md#_snippet_0\n\nLANGUAGE: None\nCODE:\n```\n'mask2former_xxx_coco.py' represents config files for **panoptic segmentation**.\n```\n\n----------------------------------------\n\nTITLE: Training MaskTrack R-CNN Shell Command\nDESCRIPTION: This shell command trains the MaskTrack R-CNN model on the YouTube-VIS-2021 dataset using 8 GPUs.  It uses the `dist_train.sh` script with a specific configuration file and GPU count. Requires the MMTracking environment to be set up correctly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/masktrack_rcnn/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Training MaskTrack R-CNN on YouTube-VIS-2021 dataset with following command.\n# The number after config file represents the number of GPUs used. Here we use 8 GPUs.\nbash tools/dist_train.sh configs/masktrack_rcnn/masktrack-rcnn_mask-rcnn_r50_fpn_8xb1-12e_youtubevis2021.py 8\n```\n\n----------------------------------------\n\nTITLE: Fusing model predictions and saving the results (Shell)\nDESCRIPTION: This command fuses predictions from three models, saves the fused results, and specifies an output directory for the saved results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/fuse_results.py \\\n       ./faster-rcnn_r50-caffe_fpn_1x_coco.json \\\n       ./retinanet_r50-caffe_fpn_1x_coco.json \\\n       ./cascade-rcnn_r50-caffe_fpn_1x_coco.json \\\n       --annotation ./annotation.json \\\n       --weights 1 2 3 \\\n       --save-fusion-results \\\n       --out-dir outputs/fusion\n```\n\n----------------------------------------\n\nTITLE: Optimize YOLO Anchors Example (optimize_anchors.py)\nDESCRIPTION: This is an example usage of the `optimize_anchors.py` script with the differential evolution algorithm, demonstrating how to optimize anchors for a YOLOv3 model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/optimize_anchors.py configs/yolo/yolov3_d53_8xb8-320-273e_coco.py --algorithm differential_evolution --input-shape 608 608 --device cuda --output-dir work_dirs\n```\n\n----------------------------------------\n\nTITLE: Use MobileNet Backbone in MMDetection Configuration (Python)\nDESCRIPTION: This snippet illustrates how to configure the MMDetection model to use the newly defined MobileNet backbone. The configuration specifies the type of the backbone as 'MobileNet' and includes any required arguments for its initialization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n    ...\n    backbone=dict(\n        type='MobileNet',\n        arg1=xxx,\n        arg2=xxx),\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing albumentations with specific version and options\nDESCRIPTION: This command installs albumentations version 0.3.2 or higher and specifies that pre-compiled binary wheels should not be used for qudida and albumentations. This ensures that the specified versions are used and that the installation process uses source builds if necessary.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/requirements/albu.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nalbumentations>=0.3.2 --no-binary qudida,albumentations\n```\n\n----------------------------------------\n\nTITLE: Configuring the Custom Transform in MMDetection (Python)\nDESCRIPTION: This code snippet demonstrates how to integrate the custom `MyTransform` into the MMDetection training pipeline configuration.  It uses the `custom_imports` dictionary to specify the path to the file containing the custom transform definition. It then adds an entry for `MyTransform` to the `train_pipeline` list, specifying the desired probability. This requires the custom module to be importable by the training script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_transforms.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(imports=['path.to.my_pipeline'], allow_failed_imports=False)\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='MyTransform', prob=0.2),\n    dict(type='PackDetInputs')\n]\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-x\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-x model. This config file is used to define the architecture, training parameters, and other settings for the RTMDet-x model variant.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_x_8xb32-300e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: SSD Model Conversion Script in MMDetection\nDESCRIPTION: This script is used to convert older SSD models to the refactored structure introduced in MMDetection v2.14.0. It takes the path to the old model and the path to save the converted model as input parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/compatibility.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython tools/model_converters/upgrade_ssd_version.py ${OLD_MODEL_PATH} ${NEW_MODEL_PATH}\n```\n\n----------------------------------------\n\nTITLE: Install MMEngine and MMCV with MIM\nDESCRIPTION: Installs MMEngine and MMCV, core dependencies for MMDetection, using the MIM package manager.  MIM simplifies the installation of MMCV by handling CUDA and PyTorch dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install -U openmim\nmim install mmengine\nmim install \"mmcv>=2.0.0\"\n```\n\n----------------------------------------\n\nTITLE: DDOD ATSS Training Log\nDESCRIPTION: Links to download the training logs for DDOD-ATSS trained on COCO dataset with ResNet-50 backbone.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ddod/README.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/ddod/ddod_r50_fpn_1x_coco/ddod_r50_fpn_1x_coco_20220523_223737.log.json)\n```\n\n----------------------------------------\n\nTITLE: Modifying Data Pipeline Configuration in MMDetection\nDESCRIPTION: This snippet demonstrates how to define new data pipelines (`train_pipeline`, `test_pipeline`) and apply them to the data loaders (`train_dataloader`, `val_dataloader`, `test_dataloader`) in MMDetection.  It involves overriding the pipeline settings within the dataset configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/config.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n_base_ = './mask-rcnn_r50_fpn_1x_coco.py'\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='RandomResize', scale=[(1333, 640), (1333, 800)],\n        keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(type='PackDetInputs')\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor'))\n]\ntrain_dataloader = dict(dataset=dict(pipeline=train_pipeline))\nval_dataloader = dict(dataset=dict(pipeline=test_pipeline))\ntest_dataloader = dict(dataset=dict(pipeline=test_pipeline))\n```\n\n----------------------------------------\n\nTITLE: Visualize Raw Grounding Data (browse_grounding_raw.py)\nDESCRIPTION: This script visualizes raw grounding data by overlaying bounding boxes and labels on images. It generates images with overlaid labels in a specified output directory. Requires specifying the data directory, annotation file, label map file, and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_raw.py data/object365_v1/ o365v1_train_od.json train --label-map-file o365v1_label_map.json -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: Modifying TTA Config with Scaling Enhancement\nDESCRIPTION: This code demonstrates how to modify the `tta_pipeline` to include scaling enhancement. It defines multiple image scales in `img_scales` and uses a list comprehension to create `Resize` transformations for each scale within the `transforms` list. The `tta_model` configuration remains the same as in the previous example.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ntta_model = dict(\n    type='DetTTAModel',\n    tta_cfg=dict(nms=dict(\n                   type='nms',\n                   iou_threshold=0.5),\n                   max_per_img=100))\n\nimg_scales = [(1333, 800), (666, 400), (2000, 1200)]\ntta_pipeline = [\n    dict(type='LoadImageFromFile',\n         backend_args=None),\n    dict(\n        type='TestTimeAug',\n        transforms=[[            dict(type='Resize', scale=s, keep_ratio=True) for s in img_scales\n        ], [\n            dict(type='RandomFlip', prob=1.),\n            dict(type='RandomFlip', prob=0.)\n        ], [\n            dict(\n               type='PackDetInputs',\n               meta_keys=('img_id', 'img_path', 'ori_shape',\n                       'img_shape', 'scale_factor', 'flip',\n                       'flip_direction'))\n       ]])]\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for Robustness Benchmark\nDESCRIPTION: This LaTeX snippet provides the citation information for the 'Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming' paper.  It defines the article's title, authors, journal, and year of publication, which is useful for referencing the benchmark in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{michaelis2019winter,\n  title={Benchmarking Robustness in Object Detection:\n    Autonomous Driving when Winter is Coming},\n  author={Michaelis, Claudio and Mitzkus, Benjamin and\n    Geirhos, Robert and Rusak, Evgenia and\n    Bringmann, Oliver and Ecker, Alexander S. and\n    Bethge, Matthias and Brendel, Wieland},\n  journal={arXiv:1907.07484},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Referring Expression Comprehension\nDESCRIPTION: This shell command executes referring expression comprehension with the MM Grounding DINO model. The `--tokens-positive -1` argument indicates that the model should perform referring expression comprehension without noun phrase extraction. The model attempts to understand the full input text and identify the corresponding object.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/apples.jpg \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'red apple.' \\\n        --tokens-positive -1\n```\n\n----------------------------------------\n\nTITLE: RTMDet Single GPU Testing with Image Saving\nDESCRIPTION: This shell command tests the RTMDet model and saves the visualized results to a specified directory.  The `--show-dir` flag allows saving the images without needing a GUI environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n       configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n       checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n       --show-dir rtmdet_l_8xb32-300e_coco_results\n```\n\n----------------------------------------\n\nTITLE: iSAID JSON Conversion Script Execution (MMDetection)\nDESCRIPTION: This command executes the `isaid_json.py` script to convert the iSAID dataset's JSON annotation format into a format compatible with MMDetection. The script is located within the `projects/iSAID` directory of the MMDetection repository and requires the path to the iSAID dataset as an argument. This conversion is essential before training or testing with MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/iSAID/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython projects/iSAID/isaid_json.py /path/to/iSAID\n```\n\n----------------------------------------\n\nTITLE: Using AvoidCUDAOOM decorator in MMDetection\nDESCRIPTION: This snippet demonstrates the usage of `AvoidCUDAOOM` as a decorator. It is used to make sure the function continues to run when GPU memory runs out. It assumes the `AvoidCUDAOOM` utility is available in the `mmdet.utils` module.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.utils import AvoidCUDAOOM\n\n@AvoidCUDAOOM.retry_if_cuda_oom\ndef function(*args, **kwargs):\n    ...\n    return xxx\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-l-Swin-B-P6\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-l-Swin-B-P6 model. The linked config file likely specifies the model's architecture and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_l_swin_b_p6_4xb16-100e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Proposals in CascadeRoIHead (Python)\nDESCRIPTION: This code demonstrates how MMDetection handles scenarios where there are no proposals generated. It shows both the case where the entire batch lacks proposals and the case where a single image within the batch has no proposals. When proposals are empty, placeholder results are created to avoid errors during processing. `np.zeros` is used to create an empty numpy array.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/conventions.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# 简单的测试\n...\n\n# 在整个 batch中 都没有 proposals\nif rois.shape[0] == 0:\n    bbox_results = [[\n        np.zeros((0, 5), dtype=np.float32)\n        for _ in range(self.bbox_head[-1].num_classes)\n    ]] * num_imgs\n    if self.with_mask:\n        mask_classes = self.mask_head[-1].num_classes\n        segm_results = [[[] for _ in range(mask_classes)]\n                        for _ in range(num_imgs)]\n        results = list(zip(bbox_results, segm_results))\n    else:\n        results = bbox_results\n    return results\n...\n\n# 在单张图片中没有 proposals\nfor i in range(self.num_stages):\n    ...\n    if i < self.num_stages - 1:\n          for j in range(num_imgs):\n                # 处理空 proposals\n                if rois[j].shape[0] > 0:\n                    bbox_label = cls_score[j][:, :-1].argmax(dim=1)\n                    refine_roi = self.bbox_head[i].regress_by_class(\n                         rois[j], bbox_label[j], bbox_pred[j], img_metas[j])\n                    refine_roi_list.append(refine_roi)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference on a Directory of Images\nDESCRIPTION: This snippet shows how to provide a directory path as input to the `DetInferencer`. The `DetInferencer` will automatically process all images found within the specified directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ninferencer('tests/data/')\n```\n\n----------------------------------------\n\nTITLE: Calculating average training speed (Shell)\nDESCRIPTION: This command calculates the average training speed from a log file and optionally includes outliers in the calculation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py cal_train_time log.json [--include-outliers]\n```\n\n----------------------------------------\n\nTITLE: Testing QDTrack using dist_test_tracking.sh\nDESCRIPTION: This command tests the trained QDTrack model on the motXX-half-val dataset using the distributed testing script. It specifies the configuration file, the number of GPUs, and the path to the checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/qdtrack/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8 --checkpoint ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Generating Initial ODVG Format File (Python)\nDESCRIPTION: This Python script generates an initial ODVG (Object Detection and Visual Grounding) format file for the 'cat' dataset. It reads image dimensions and stores them in a JSONLines file. Requires OpenCV (cv2) and jsonlines libraries.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport cv2\nimport json\nimport jsonlines\n\ndata_root = 'data/cat'\nimages_path = os.path.join(data_root, 'images')\nout_path = os.path.join(data_root, 'cat_train_od.json')\nmetas = []\nfor files in os.listdir(images_path):\n    img = cv2.imread(os.path.join(images_path, files))\n    height, width, _ = img.shape\n    metas.append({\"filename\": files, \"height\": height, \"width\": width})\n\nwith jsonlines.open(out_path, mode='w') as writer:\n    writer.write_all(metas)\n\n```\n\n----------------------------------------\n\nTITLE: Citing Mask2Former in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Mask2Former paper.  It includes the title, authors, journal, and year of publication for proper academic citation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask2former/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{cheng2021mask2former,\n  title={Masked-attention Mask Transformer for Universal Image Segmentation},\n  author={Bowen Cheng and Ishan Misra and Alexander G. Schwing and Alexander Kirillov and Rohit Girdhar},\n  journal={arXiv},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: OpenImages Evaluator Configuration (3.x)\nDESCRIPTION: This code snippet shows the OpenImages evaluator configuration in MMDetection 3.x. It specifies the evaluator type as `OpenImagesMetric`, along with IoU and IoA thresholds, and flags to use group_of and get_supercategory. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='OpenImagesMetric',\n    iou_thrs=0.5,\n    ioa_thrs=0.5,\n    use_group_of=True,\n    get_supercategory=True)\n```\n\n----------------------------------------\n\nTITLE: Install MMCV\nDESCRIPTION: Installs OpenMIM and then uses it to install MMCV. This also installs mmengine as a dependency.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -U openmim\nmim install \"mmcv>=2.0.0\"\n# Installing mmcv will automatically install mmengine\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection with Tracking Support via MIM\nDESCRIPTION: Installs MMDetection with tracking capabilities using MIM.  The '[tracking]' extra installs the necessary tracking-related dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nmim install mmdet[tracking]\n```\n\n----------------------------------------\n\nTITLE: Testing on CPU with MMDetection\nDESCRIPTION: This snippet demonstrates how to test a model on the CPU using MMDetection. Similar to training, it first disables GPU visibility by setting the CUDA_VISIBLE_DEVICES environment variable to -1, and then executes the testing script, specifying the configuration file and optional arguments. It is useful for verification or when GPUs are unavailable.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_10\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/test_tracking.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Transform in MMDetection (Python)\nDESCRIPTION: This code defines a custom data transform called `MyTransform` for MMDetection. It inherits from `BaseTransform` and uses the `@TRANSFORMS.register_module()` decorator to register the transform. The `transform` method modifies the input `results` dictionary based on a given probability. The `results` dictionary typically contains image data and annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_transforms.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nfrom mmcv.transforms import BaseTransform\nfrom mmdet.registry import TRANSFORMS\n\n\n@TRANSFORMS.register_module()\nclass MyTransform(BaseTransform):\n    \"\"\"Add your transform\n\n    Args:\n        p (float): Probability of shifts. Default 0.5.\n    \"\"\"\n\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def transform(self, results):\n        if random.random() > self.prob:\n            results['dummy'] = True\n        return results\n```\n\n----------------------------------------\n\nTITLE: Installing Roboflow Package (Shell)\nDESCRIPTION: This command installs the Roboflow Python package, which provides tools for interacting with the Roboflow platform, including dataset downloading and management. This package is a prerequisite for downloading the Roboflow 100 dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install roboflow\n```\n\n----------------------------------------\n\nTITLE: VISION Datasets Directory Structure\nDESCRIPTION: Illustrates the directory structure required for the VISION Datasets within the mmdetection project. This structure includes the location of the dataset archives (tar.gz files) before unzipping. It also highlights README.md file. This is the initial organization prior to the unzip operation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/VISION-Datasets/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n├── │── VISION-Datasets\n├── │   ├── Cable.tar.gz\n├── │   ├── Capacitor.tar.gz\n├── │   ├── Casting.tar.gz\n├── |   ├── Console.tar.gz\n├── │   ├── Cylinder.tar.gz\n├── │   ├── Electronics.tar.gz\n├── │   ├── Groove.tar.gz\n├── │   ├── Hemisphere.tar.gz\n├── │   ├── Lens.tar.gz\n├── │   ├── PCB_1.tar.gz\n├── │   ├── PCB_2.tar.gz\n├── |   ├── README.md\n├── │   ├── Ring.tar.gz\n├── │   ├── Screw.tar.gz\n├── │   └── Wood.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Analyzing COCO error (Shell)\nDESCRIPTION: This script analyzes COCO evaluation results for each category using different metrics, visualizing helpful information in charts. It requires a results file and an output directory and can be configured with annotation file and analysis types.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_error_analysis.py ${RESULT} ${OUT_DIR} [-h] [--ann ${ANN}] [--types ${TYPES[TYPES...]}]\n```\n\n----------------------------------------\n\nTITLE: Resize Config (MMDetection)\nDESCRIPTION: This code snippet demonstrates the configuration for the Resize transform in both MMDetection 2.x and 3.x. The key change is the parameter name: `img_scale` in 2.x is replaced with `scale` in 3.x.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndict(type='Resize',\n     img_scale=(1333, 800),\n     keep_ratio=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(type='Resize',\n     scale=(1333, 800),\n     keep_ratio=True)\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Faster RCNN model with a RegNetX-800MF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-800MF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-800MF_fpn_mstrain_3x_coco_20210526_095118.log.json)\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Model Name\nDESCRIPTION: This snippet initializes the DetInferencer with a pre-trained model name. The weights will be automatically downloaded from the OpenMMLab model zoo. This assumes the user has the MMDetection library installed and configured.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco')\n```\n\n----------------------------------------\n\nTITLE: WIDER FACE Dataset Directory Structure\nDESCRIPTION: This code snippet shows the expected directory structure for the WIDER FACE dataset after downloading and organizing it. The structure includes train and validation folders with images and annotations, along with train.txt and val.txt files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/wider_face/README.md#_snippet_0\n\nLANGUAGE: tree\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── WIDERFace\n│   │   ├── WIDER_train\n│   |   │   ├──0--Parade\n│   |   │   ├── ...\n│   |   │   ├── Annotations\n│   │   ├── WIDER_val\n│   |   │   ├──0--Parade\n│   |   │   ├── ...\n│   |   │   ├── Annotations\n│   │   ├── val.txt\n│   │   ├── train.txt\n```\n\n----------------------------------------\n\nTITLE: Citing Deformable DETR in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Deformable DETR paper. It includes the title, authors, conference, year, and URL. This citation can be used to properly attribute the work in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deformable_detr/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{\nzhu2021deformable,\ntitle={Deformable DETR: Deformable Transformers for End-to-End Object Detection},\nauthor={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=gZ9hCDWe6ke}\n}\n```\n\n----------------------------------------\n\nTITLE: Train MMDetection with single GPU\nDESCRIPTION: This command demonstrates how to initiate a training process with MMDetection using a single GPU. The `{config_file}` placeholder should be replaced with the path to the configuration file that defines the model architecture, dataset, and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dsdl/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython tools/train.py {config_file}\n```\n\n----------------------------------------\n\nTITLE: DINO BibTeX Citation\nDESCRIPTION: This is the BibTeX entry for citing the DINO paper, which introduces the DINO (DETR with Improved DeNoising Anchor Boxes) object detection model.  It includes the title, authors, year, and arXiv identifier.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dino/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@misc{zhang2022dino,\n  title={DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection},\n  author={Hao Zhang and Feng Li and Shilong Liu and Lei Zhang and Hang Su and Jun Zhu and Lionel M. Ni and Heung-Yeung Shum},\n  year={2022},\n  eprint={2203.03605},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}}\n```\n\n----------------------------------------\n\nTITLE: Modifying the config file to use AugFPN\nDESCRIPTION: This Python dictionary demonstrates how to modify the config file to use the newly implemented `AugFPN` neck. It defines the type, input channels, output channels, and the number of output features.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nneck=dict(\n    type='AugFPN',\n    in_channels=[256, 512, 1024, 2048],\n    out_channels=256,\n    num_outs=5)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Fine-tuned Model Performance (mmdetection)\nDESCRIPTION: These lines show the evaluation metrics for the fine-tuned Grounding DINO model after training on the cat dataset. It includes Average Precision (AP) and Average Recall (AR) values at different IoU thresholds and area sizes, indicating the model's detection accuracy.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 1.000\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.931\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.867\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.903\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.907\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.907\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.907\n```\n\n----------------------------------------\n\nTITLE: Removing COCO train2017 images from RefCOCO (Python)\nDESCRIPTION: This script removes overlapping images from the COCO2017 training set that are present in RefCOCO/RefCOCO+/RefCOCOg/gRefCOCO validation sets to prevent data leakage during evaluation of RefExp models. It generates a new annotation file without the overlapping images.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/remove_cocotrain2017_from_refcoco.py data/coco/mdetr_annotations data/coco/annotations/instances_train2017.json\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Testing on COCO test-dev (no ground truth)\nDESCRIPTION: This shell script tests Mask R-CNN on the COCO test-dev dataset using 8 GPUs when ground truth labels are unavailable. JSON files are generated.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n    configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py \\\n    checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n    8\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration (3.x Interval)\nDESCRIPTION: This code snippet demonstrates how to configure the logging interval in MMDetection 3.x. It sets the interval to 50 within the `LoggerHook` in `default_hooks`, meaning a log message is printed every 50 iterations. It also shows how to optionally configure the log processor to smooth the log values using a window of size 50. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    logger=dict(\n        type='LoggerHook',\n        interval=50))\n# 可选： 配置日志打印数值的平滑窗口大小\nlog_processor = dict(\n    type='LogProcessor',\n    window_size=50)\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Only Weights (MMEngine)\nDESCRIPTION: This snippet initializes the `DetInferencer` by only providing the path to a weight file. This works if the weight file contains the model's configuration, as is often the case with models trained in MMEngine. The code specifies a direct download link for ddq-detr-4scale_r50 weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# It will raise an error if the config file cannot be found in the weight. Currently, within the MMDetection model repository, only the weights of ddq-detr-4scale_r50 can be loaded in this manner.\ninferencer = DetInferencer(weights='https://download.openmmlab.com/mmdetection/v3.0/ddq/ddq-detr-4scale_r50_8xb2-12e_coco/ddq-detr-4scale_r50_8xb2-12e_coco_20230809_170711-42528127.pth')\n```\n\n----------------------------------------\n\nTITLE: Test-Time Augmentation Configuration\nDESCRIPTION: This shell snippet shows the configuration for test-time augmentation (TTA). It defines the `tta_model` type and configuration, as well as the `tta_pipeline` that specifies the transformations to apply during testing, such as resizing and flipping.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ntta_model = dict(\n    type='DetTTAModel',\n    tta_cfg=dict(nms=dict(\n                   type='nms',\n                   iou_threshold=0.5),\n                   max_per_img=100))\n\ntta_pipeline = [\n    dict(type='LoadImageFromFile',\n        backend_args=None),\n    dict(\n        type='TestTimeAug',\n        transforms=[[[\n            dict(type='Resize', scale=(1333, 800), keep_ratio=True)\n        ]], [[ # It uses 2 flipping transformations (flipping and not flipping).\n            dict(type='RandomFlip', prob=1.),\n            dict(type='RandomFlip', prob=0.)\n        ]], [[\n            dict(\n               type='PackDetInputs',\n               meta_keys=('img_id', 'img_path', 'ori_shape',\n                       'img_shape', 'scale_factor', 'flip',\n                       'flip_direction'))\n       ]]])]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Evaluation Results on a Referring Expression Dataset\nDESCRIPTION: This shell command visualizes the evaluation results of the MM Grounding DINO model on a referring expression dataset. It runs the `tools/test.py` script with the configuration file and model weights and saves the visualizations to the specified `refcoco_result/{timestamp}/save_path` directory. The `--work-dir` argument specifies the working directory, and the `--show-dir` argument specifies the directory to save the visualizations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/refcoco/grounding_dino_swin-t_pretrain_zeroshot_refexp \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth --work-dir refcoco_result --show-dir save_path\n```\n\n----------------------------------------\n\nTITLE: Distributed Inference on COCO Test-Dev with Slurm and Config Overrides\nDESCRIPTION: This shell script performs distributed inference on the COCO test-dev dataset using a Slurm job scheduler. It uses `tools/slurm_test.sh` to launch the job and overrides configuration settings using `--cfg-options`. It defines the partition, job name, and number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# test with slurm\nGPUS=8 tools/slurm_test.sh \\\n    ${Partition} \\\n    ${JOB_NAME} \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=${WORK_DIR}/results\n```\n\n----------------------------------------\n\nTITLE: Run MOT Demo from Source\nDESCRIPTION: Runs the MOT (Multi-Object Tracking) demo script using the downloaded ByteTrack configuration and checkpoint. It performs tracking on a video ('demo/demo_mot.mp4') and saves the output as 'mot.mp4'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 bytetrack_yolox_x_8xb4-amp-80e_crowdhuman-mot17halftrain_test-mot17halfval.py --checkpoint bytetrack_yolox_x_crowdhuman_mot17-private-half_20211218_205500-1985c9f0.pth --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Cat Dataset (Shell)\nDESCRIPTION: These commands download and extract the 'cat' dataset, which is used as an example for fine-tuning. It prepares the data for further processing and training. Requires wget and unzip tools.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\nwget https://download.openmmlab.com/mmyolo/data/cat_dataset.zip\nunzip cat_dataset.zip -d data/cat/\n```\n\n----------------------------------------\n\nTITLE: FreeAnchor Config - ResNet-50 Backbone - PyTorch\nDESCRIPTION: This configuration file specifies the settings for FreeAnchor with a ResNet-50 backbone, using the PyTorch framework. It defines the model architecture, training schedule, and other hyperparameters required for object detection on the COCO dataset. It is associated with a specific model and log file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/free_anchor/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[config](./freeanchor_r50_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Convert LVIS to OVD Format (Python)\nDESCRIPTION: This script converts the LVIS dataset to OVD format. It utilizes `lvis2ovd.py` script to perform the conversion.  The script converts the LVIS dataset to the OVD (Open Vocabulary Detection) format. It executes the `lvis2ovd.py` script from the `tools/dataset_converters/` directory, taking the COCO data directory as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/lvis2ovd.py data/coco/\n```\n\n----------------------------------------\n\nTITLE: COCO Format Annotation Example\nDESCRIPTION: This snippet shows the required fields for a COCO format JSON annotation file, including images, annotations, and categories. The 'images' array contains image metadata, the 'annotations' array contains instance annotations, and the 'categories' array defines the class names and IDs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n'images': [\n    {\n        'file_name': 'COCO_val2014_000000001268.jpg',\n        'height': 427,\n        'width': 640,\n        'id': 1268\n    },\n    ...\n],\n\n'annotations': [\n    {\n        'segmentation': [[192.81,\n            247.09,\n            ...\n            219.03,\n            249.06]],  # 如果有 mask 标签且为多边形 XY 点坐标格式，则需要保证至少包括 3 个点坐标，否则为无效多边形\n        'area': 1035.749,\n        'iscrowd': 0,\n        'image_id': 1268,\n        'bbox': [192.81, 224.8, 74.73, 33.43],\n        'category_id': 16,\n        'id': 42986\n    },\n    ...\n],\n\n'categories': [\n    {'id': 0, 'name': 'car'},\n ]\n```\n\n----------------------------------------\n\nTITLE: Training a Model\nDESCRIPTION: Command to train a model using a specified configuration file. The configuration file defines the model architecture, dataset, and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py configs/cityscapes/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes.py\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Faster RCNN model with a RegNetX-4GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-4GF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-4GF_fpn_mstrain_3x_coco_20210526_095201.log.json)\n```\n\n----------------------------------------\n\nTITLE: Migrating Image Normalization Config (MMDetection)\nDESCRIPTION: This code snippet demonstrates how to migrate the image normalization configuration from MMDetection 2.x to 3.x.  In 2.x, image normalization parameters were defined separately and used within the data pipeline. In 3.x, these parameters are moved to the `data_preprocessor` module within the model configuration.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# 图像归一化参数\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53],\n    std=[58.395, 57.12, 57.375],\n    to_rgb=True)\npipeline=[\n    ...,\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),  # 图像 padding 到 32 的倍数\n    ...\n]\n```\n\n----------------------------------------\n\nTITLE: Shell script for unzipping VISION Datasets\nDESCRIPTION: This shell script automates the extraction of all .tar.gz files within the data/VISION-Datasets/ directory. It iterates through each file and uses the tar command to extract its contents into the same directory. This script is designed to simplify the dataset preparation process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/VISION-Datasets/README_zh-CN.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n#!/usr/bin/env bash\n\nfor file in data/VISION-Datasets/*.tar.gz; do\n    tar -xzvzf \"$file\" -C data/VISION-Datasets/\ndone\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX entry for Guided Anchoring paper\nDESCRIPTION: This BibTeX entry provides the citation information for the Region Proposal by Guided Anchoring paper, which was presented at CVPR 2019. It includes the title, authors, conference, and year of publication, facilitating proper attribution in academic works.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/guided_anchoring/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{wang2019region,\n    title={Region Proposal by Guided Anchoring},\n    author={Jiaqi Wang and Kai Chen and Shuo Yang and Chen Change Loy and Dahua Lin},\n    booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n    year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing DeepSORT on MOT17-test (Shell)\nDESCRIPTION: This command tests DeepSORT on the motxx-test dataset to generate result files for the MOT Challenge submission.  It uses the `tools/dist_test_tracking.sh` script for testing.  It takes the config file, number of GPUs, detector checkpoint path, and ReID checkpoint path as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepsort/README.md#_snippet_2\n\nLANGUAGE: Shell Script\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/deepsort/deepsort_faster-rcnn_r50_fpn_8xb2-4e_mot17train_test-mot17test 8 --detector ${DETECTOR_CHECKPOINT_PATH} --reid ${REID_CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for GHM\nDESCRIPTION: This BibTeX entry provides the citation information for the Gradient Harmonized Single-stage Detector paper. It includes the title, authors, conference, and year of publication. This citation is used to properly attribute the GHM method in research papers.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ghm/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{li2019gradient,\n  title={Gradient Harmonized Single-stage Detector},\n  author={Li, Buyu and Liu, Yu and Wang, Xiaogang},\n  booktitle={AAAI Conference on Artificial Intelligence},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing After Fine-tuning (mmdetection)\nDESCRIPTION: This command visualizes the single image inference result after fine-tuning.  It uses the best model weights saved during training to perform object detection on the input image. The `cat_work_dir/best_coco_bbox_mAP_epoch_16.pth` file contains the weights of the best model at epoch 16.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\npython demo/image_demo.py data/cat/images/IMG_20211205_120756.jpg configs/grounding_dino/grounding_dino_swin-t_finetune_8xb2_20e_cat.py --weights cat_work_dir/best_coco_bbox_mAP_epoch_16.pth --texts cat.\n```\n\n----------------------------------------\n\nTITLE: Configure TeacherStudentValLoop in Python\nDESCRIPTION: This code snippet configures the TeacherStudentValLoop, replacing the standard ValLoop.  It is used in teacher-student joint training frameworks to evaluate the accuracy of both the teacher and student models during the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/semi_det.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nval_cfg = dict(type='TeacherStudentValLoop')\n```\n\n----------------------------------------\n\nTITLE: Referring Segmentation Evaluation\nDESCRIPTION: This shell command evaluates the XDecoder model for referring segmentation on the RefCOCO dataset. It specifies the configuration file, checkpoint file, and the 'val' split of the dataset. The evaluation is distributed across 8 processes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh  projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-ref-seg_refcocog.py xdecoder_focalt_last_novg.pt 8  --cfg-options test_dataloader.dataset.split='val'\n```\n\n----------------------------------------\n\nTITLE: Visualizing Object Detection Results\nDESCRIPTION: This shell command visualizes the object detection results generated by the previous step. It uses the `tools/analysis_tools/browse_grounding_raw.py` script to display the predicted bounding boxes and labels on the input images. The `--label-map-file` argument specifies the path to the `cat_label_map.json` file. The `--not-show` argument prevents the visualization from being displayed interactively. The `-o` argument specifies the output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_raw.py data/cat/ cat_train_od_v1.json images --label-map-file cat_label_map.json -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: Installing DDD Dataset using pip\nDESCRIPTION: This command installs the 'ddd-dataset' package using pip, which is likely required for utilizing the Zero-Shot Description Detection Dataset (DOD). The command ensures the necessary dependencies for working with the dataset are installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install ddd-dataset\n```\n\n----------------------------------------\n\nTITLE: Convert LVIS to ODVG Format (Python)\nDESCRIPTION: This script converts the LVIS training data to the ODVG format. It executes the `lvis2odvg.py` script from the `tools/dataset_converters/` directory, providing the path to the LVIS training JSON file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/lvis2odvg.py data/coco/annotations/lvis_v1_train.json\n```\n\n----------------------------------------\n\nTITLE: Training DINO with DeepSpeed in MMDetection (Bash)\nDESCRIPTION: This bash script illustrates the attempt to train the DINO object detection model with a Swin-L backbone using DeepSpeed on 8 GPUs with MMDetection. It highlights that this approach currently results in gradient overflow and low precision.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_largemodel/README_zh-CN.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd mmdetection\n./tools/dist_train.sh projects/example_largemodel/dino-5scale_swin-l_deepspeed_8xb2-12e_coco.py 8\n```\n\n----------------------------------------\n\nTITLE: JSON File Generation Testing with 8 GPUs in MMDetection\nDESCRIPTION: This snippet tests the Mask R-CNN model on the COCO test-dev split with 8 GPUs using the `tools/dist_test.sh` script. It generates JSON files (`test.bbox.json` and `test.segm.json`) in the `./work_dirs/coco_instance/` directory for submission to the official evaluation server. Note that some config changes might be needed as explained in the documentation\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n    configs/mask_rcnn/mask-rcnn_r50_fpn_1x_coco.py \\\n    checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n    8\n```\n\n----------------------------------------\n\nTITLE: Adjusting Post-Processing Settings in MMDetection config using Python\nDESCRIPTION: This Python snippet illustrates how to adjust post-processing settings within the MMDetection configuration, such as the score threshold (`score_thr`), number of boxes before NMS (`nms_pre`), and maximum detections per image (`max_per_img`). Adjusting these settings can reduce post-processing time.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# in MMDetection config\nmodel = dict(\n    test_cfg=dict(\n        nms_pre=1000,  # keep top-k score bboxes before nms\n        min_bbox_size=0,\n        score_thr=0.3,  # score threshold to filter bboxes\n        nms=dict(type='nms', iou_threshold=0.65),\n        max_per_img=100)  # only keep top-100 as the final results.\n)\n```\n\n----------------------------------------\n\nTITLE: Image Visualization (mmdetection)\nDESCRIPTION: This command performs single image visualization using a pre-trained Grounding DINO model. It takes an image path, configuration file, model weights, and text prompts as input. It uses `demo/image_demo.py` to run the inference and display the detected objects with corresponding labels.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grounding_dino/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\npython demo/image_demo.py data/cat/images/IMG_20211205_120756.jpg configs/grounding_dino/grounding_dino_swin-t_finetune_8xb2_20e_cat.py --weights https://download.openmmlab.com/mmdetection/v3.0/grounding_dino/groundingdino_swint_ogc_mmdet-822d7e9d.pth --texts cat.\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Inference Time with benchmark.py in Shell\nDESCRIPTION: This shell command uses the `benchmark.py` script in MMDetection to measure the inference time of a model. It requires specifying a configuration file path (${CONFIG}) and a checkpoint file path (${CHECKPOINT}). Optional arguments include `--log-interval` to change the log output frequency and `--fuse-conv-bn` to potentially improve inference speed by fusing convolution and batch normalization layers.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/model_zoo.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/benchmark.py ${CONFIG} ${CHECKPOINT} [--log-interval $[LOG-INTERVAL]] [--fuse-conv-bn]\n```\n\n----------------------------------------\n\nTITLE: Unlabeled Weak Data Pipeline Configuration (Python)\nDESCRIPTION: This Python code configures the data augmentation pipeline for unlabeled data in a semi-supervised object detection setup. It includes random resizing, random flipping and packing the data for the teacher model to predict pseudo labels. The `PackDetInputs` step prepares the data for input to the detection model, including metadata keys.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment unlabeled data weakly,\n# which will be sent to teacher model for predicting pseudo instances.\nweak_pipeline = [\n    dict(type='RandomResize', scale=scale, keep_ratio=True),\n    dict(type='RandomFlip', prob=0.5),\n    dict(\n        type='PackDetInputs',\n        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n                   'scale_factor', 'flip', 'flip_direction',\n                   'homography_matrix')),\n]\n```\n\n----------------------------------------\n\nTITLE: Convert YouTube-VIS 2019 Annotations to COCO format\nDESCRIPTION: This script converts the YouTube-VIS 2019 dataset annotations to the COCO format. It takes input and output directories as arguments, and specifies the dataset version. The output is saved as a JSON file compatible with COCO.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_dataset_prepare.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython ./tools/dataset_converters/youtubevis2coco.py -i ./data/youtube_vis_2019 -o ./data/youtube_vis_2019/annotations --version 2019\n```\n\n----------------------------------------\n\nTITLE: Example: Prepare Model for Release (publish_model.py)\nDESCRIPTION: This is an example usage of the `publish_model.py` script, demonstrating the conversion of a model checkpoint to a release-ready format.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/publish_model.py work_dirs/faster_rcnn/latest.pth faster_rcnn_r50_fpn_1x_20190801.pth\n```\n\n----------------------------------------\n\nTITLE: Convert COCO to ODVG format for open-set fine-tuning\nDESCRIPTION: Converts the COCO dataset annotations to the ODVG (Object Detection with Visual Grounding) format. This is necessary for open-set continued pretraining fine-tuning.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/coco/annotations/instances_train2017.json -d coco\n```\n\n----------------------------------------\n\nTITLE: StrongSORT Citation in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation format for the StrongSORT paper. It includes the title, authors, journal, and year of publication. This is useful for referencing the StrongSORT algorithm in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/strongsort/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{du2022strongsort,\n  title={Strongsort: Make deepsort great again},\n  author={Du, Yunhao and Song, Yang and Yang, Bo and Zhao, Yanyun},\n  journal={arXiv preprint arXiv:2202.13514},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: SCNet Citation (LaTeX)\nDESCRIPTION: LaTeX snippet for citing the SCNet paper. It provides the title, authors, booktitle, and year for proper attribution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/scnet/README.md#_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n@inproceedings{vu2019cascade,\n  title={SCNet: Training Inference Sample Consistency for Instance Segmentation},\n  author={Vu, Thang and Haeyong, Kang and Yoo, Chang D},\n  booktitle={AAAI},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Pascal VOC Evaluator Configuration (2.x)\nDESCRIPTION: This code snippet presents the Pascal VOC dataset and evaluation configuration in MMDetection 2.x. It defines the dataset type and annotation file for the validation set, along with the mAP metric.  Requires `dataset_type` and `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt'))\nevaluation = dict(metric='mAP')\n```\n\n----------------------------------------\n\nTITLE: Clone gRefCOCO repository\nDESCRIPTION: Clones the gRefCOCO repository to prepare data for referring expression tasks.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/henghuiding/gRefCOCO.git\n```\n\n----------------------------------------\n\nTITLE: DDOD BibTeX Citation\nDESCRIPTION: This is the BibTeX entry for citing the DDOD paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ddod/README.md#_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{chen2021disentangle,\ntitle={Disentangle Your Dense Object Detector},\nauthor={Chen, Zehui and Yang, Chenhongyi and Li, Qiaofei and Zhao, Feng and Zha, Zheng-Jun and Wu, Feng},\nbooktitle={Proceedings of the 29th ACM International Conference on Multimedia},\npages={4939--4948},\nyear={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Inference on COCO Test-Dev (Base Command)\nDESCRIPTION: This shell command initiates inference on the COCO test-dev dataset using a specified configuration file and checkpoint file. The configuration file defines the model and dataset settings, while the checkpoint file provides the trained model weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Create Conda Environment\nDESCRIPTION: Creates a new conda environment named 'openmmlab' with Python 3.8 and activates it. This isolates the MMDetection dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda create --name openmmlab python=3.8 -y\nconda activate openmmlab\n```\n\n----------------------------------------\n\nTITLE: Shell script to unzip VISION Datasets\nDESCRIPTION: This shell script automates the unzipping of all the .tar.gz archive files within the data/VISION-Datasets directory. The script iterates through each .tar.gz file and extracts its contents into the same directory, effectively preparing the datasets for use. It expects the script to be run from the mmdetection root directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/VISION-Datasets/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n#!/usr/bin/env bash\n\nfor file in data/VISION-Datasets/*.tar.gz; do\n    tar -xzvzf \"$file\" -C data/VISION-Datasets/\ndone\n```\n\n----------------------------------------\n\nTITLE: Extracting EfficientDet Weights using tar\nDESCRIPTION: This command extracts the downloaded EfficientDet weights from a tar archive. It assumes the archive is located at the path specified by `{EFFICIENTDET_WEIGHT}`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntar -xzvf {EFFICIENTDET_WEIGHT}\n```\n\n----------------------------------------\n\nTITLE: Valid Annotation Example\nDESCRIPTION: This snippet shows an example of a valid annotation in COCO format.  It emphasizes the importance of matching the length and order of classes in the configuration file and annotation file.  MMDetection automatically maps non-contiguous IDs in `categories` to contiguous indices.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n'annotations': [\n    {\n        'segmentation': [[192.81,\n            247.09,\n            ...\n            219.03,\n            249.06]],  # 如果有 mask 标签。\n        'area': 1035.749,\n        'iscrowd': 0,\n        'image_id': 1268,\n        'bbox': [192.81, 224.8, 74.73, 33.43],\n        'category_id': 16,\n        'id': 42986\n    },\n    ...\n],\n\n# MMDetection 会自动将 `categories` 中不连续的 `id` 映射成连续的索引。\n'categories': [\n    {'id': 1, 'name': 'a'}, {'id': 3, 'name': 'b'}, {'id': 4, 'name': 'c'}, {'id': 16, 'name': 'd'}, {'id': 17, 'name': 'e'},\n ]\n```\n\n----------------------------------------\n\nTITLE: Analyzing detection results (Shell)\nDESCRIPTION: This script calculates mAP for each image and visualizes top-k scoring predictions with bounding boxes.  It takes a config file, prediction path, and output directory as arguments, and supports options for showing results, wait time, top-k predictions, score threshold, and config overrides.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n      ${CONFIG} \\\n      ${PREDICTION_PATH} \\\n      ${SHOW_DIR} \\\n      [--show] \\\n      [--wait-time ${WAIT_TIME}] \\\n      [--topk ${TOPK}] \\\n      [--show-score-thr ${SHOW_SCORE_THR}] \\\n      [--cfg-options ${CFG_OPTIONS}]\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Mask RCNN model with a RegNetX-800MF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-800MF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-800MF_fpn_mstrain-poly_3x_coco_20210602_210641.log.json)\n```\n\n----------------------------------------\n\nTITLE: COCO Panoptic Test Dataloader Configuration (Python)\nDESCRIPTION: This Python code snippet shows the dataloader configuration for testing on the coco test-dev dataset. The `ann_file` specifies the path to the annotation file, `data_prefix` specifies the path to the test images, and `test_mode` is set to `True`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntest_dataloader = dict(\n    batch_size=1,\n    num_workers=1,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='annotations/panoptic_image_info_test-dev2017.json',\n        data_prefix=dict(img='test2017/'),\n        test_mode=True,\n        pipeline=test_pipeline))\ntest_evaluator = dict(\n    type='CocoPanopticMetric',\n    format_only=True,\n    ann_file=data_root + 'annotations/panoptic_image_info_test-dev2017.json',\n    outfile_prefix='./work_dirs/coco_panoptic/test')\n```\n\n----------------------------------------\n\nTITLE: Testing and Evaluation Shell Command\nDESCRIPTION: This shell command tests and evaluates the MaskTrack R-CNN model.  It utilizes the `dist_test_tracking.sh` script, a configuration file, GPU count, and a checkpoint path to generate result files for submission to the YouTube-VOS evaluation server. The results are stored in a zip file. Requires the MMTracking environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/masktrack_rcnn/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# The number after config file represents the number of GPUs used.\nbash tools/dist_test_tracking.sh configs/masktrack_rcnn/masktrack-rcnn_mask-rcnn_r50_fpn_8xb1-12e_youtubevis2021.py 8 --checkpoint ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Convert Cityscapes to JSON Format (Python)\nDESCRIPTION: This script converts the Cityscapes dataset annotations to the required JSON format using the `cityscapes.py` script located in `tools/dataset_converters/`. It requires the Cityscapes dataset to be downloaded and extracted, and the script generates new JSON annotation files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/cityscapes.py data/cityscapes/\n```\n\n----------------------------------------\n\nTITLE: Installing ImageCorruptions Library\nDESCRIPTION: This shell command installs the `imagecorruptions` Python library, which contains image corruption transformation functions.  This library is a dependency for evaluating model robustness against common image corruptions.  It can be installed using pip.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install imagecorruptions\n```\n\n----------------------------------------\n\nTITLE: MOT Error Visualization - Shell\nDESCRIPTION: Executes the `mot_error_visualize.py` script to visualize errors in multiple object tracking results. It takes the configuration file, input data, and result directory as arguments. Optional arguments allow for specifying the output directory, frames per second, showing the video on the fly, and choosing the visualization backend.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_analysis_tools.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/mot/mot_error_visualize.py \\\n    ${CONFIG_FILE}\\\n    --input ${INPUT} \\\n    --result-dir ${RESULT_DIR} \\\n    [--output-dir ${OUTPUT}] \\\n    [--fps ${FPS}] \\\n    [--show] \\\n    [--backend ${BACKEND}]\n```\n\n----------------------------------------\n\nTITLE: Detic BibTeX citation\nDESCRIPTION: This is the BibTeX entry to cite the Detic paper in academic publications. It includes the title, authors, conference, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic/README.md#_snippet_5\n\nLANGUAGE: BibTeX\nCODE:\n```\n@inproceedings{zhou2022detecting,\n  title={Detecting Twenty-thousand Classes using Image-level Supervision},\n  author={Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Kr{\"a}henb{\"u}hl, Philipp and Misra, Ishan},\n  booktitle={ECCV},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Start Label-Studio Web Service\nDESCRIPTION: This snippet starts the Label-Studio web service using the `label-studio start` command. This command launches the Label-Studio web application, which can be accessed through a web browser.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nlabel-studio start\n```\n\n----------------------------------------\n\nTITLE: Converting cityscapes annotations to COCO format\nDESCRIPTION: This shell command converts the Cityscapes annotations into the COCO format, which is a common format used in object detection. It utilizes the `cityscapes.py` script from the MMDetection tools, and requires the `cityscapesscripts` package.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npip install cityscapesscripts\npython tools/dataset_converters/cityscapes.py ./data/cityscapes --nproc 8 --out-dir ./data/cityscapes/annotations\n```\n\n----------------------------------------\n\nTITLE: CentripetalNet Citation (LaTeX)\nDESCRIPTION: This is the BibTeX entry for citing the CentripetalNet paper in academic publications. It includes author names, title, conference, year, and other relevant publication details.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/centripetalnet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@InProceedings{Dong_2020_CVPR,\nauthor = {Dong, Zhiwei and Li, Guoxuan and Liao, Yue and Wang, Fei and Ren, Pengju and Qian, Chen},\ntitle = {CentripetalNet: Pursuing High-Quality Keypoint Pairs for Object Detection},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cascade Mask R-CNN with DCN and X-101-FPN\nDESCRIPTION: This config file sets up a Cascade Mask R-CNN model featuring an X-101-FPN backbone.  It replaces the 3x3 convolutions with 3x3 Deformable Convolutions (DCN) in the c3-c5 stages of the backbone. SyncBN is employed, and the model is trained for 1x epochs on the COCO dataset. It does not include a Global Context block.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/gcnet/README.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n[config](./cascade-mask-rcnn_x101-32x4d-syncbn-dconv-c3-c5_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Citation for Conditional DETR (Latex)\nDESCRIPTION: This is a BibTeX entry for citing the Conditional DETR paper in academic publications. It includes the title, authors, conference, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/conditional_detr/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{meng2021-CondDETR,\n  title       = {Conditional DETR for Fast Training Convergence},\n  author      = {Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},\n  booktitle   = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n  year        = {2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Training Customized Faster R-CNN Model (Bash)\nDESCRIPTION: This bash command trains the customized Faster R-CNN model using distributed training across 8 GPUs.  It specifies the configuration file, the number of GPUs, and the working directory for storing training results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/single_stage_as_rpn.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash tools/dist_train.sh configs/faster_rcnn/faster-rcnn_r50_fpn_fcos-rpn_1x_coco.py \\\n    8 \\\n    --work-dir ./work_dirs/faster-rcnn_r50_fpn_fcos-rpn_1x_coco\n```\n\n----------------------------------------\n\nTITLE: Generating Label Map JSON\nDESCRIPTION: This snippet generates a `label_map.json` file containing a mapping from class IDs to class names.  It is written to the `data_root` directory with the filename `cat_label_map.json`. This file is used to map the numerical output of the model to human-readable class labels. This example assumes there is only one class named 'cat'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nlabel_map_path = os.path.join(data_root, 'cat_label_map.json')\nwith open(label_map_path, 'w') as f:\n    json.dump({'0': 'cat'}, f)\n```\n\n----------------------------------------\n\nTITLE: Rename and Zip Panoptic Segmentation Results\nDESCRIPTION: This shell script renames the panoptic segmentation result files according to the COCO submission convention and compresses them into a zip file.  It replaces '[algorithm_name]' placeholder with the actual algorithm name used for inference. This step is necessary for proper evaluation by the COCO evaluation server.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# In WORK_DIR, we have panoptic segmentation results: 'panoptic' and 'results.panoptic.json'.\ncd ${WORK_DIR}\n\n# replace '[algorithm_name]' with the name of algorithm you used.\nmv ./panoptic ./panoptic_test-dev2017_[algorithm_name]_results\nmv ./results.panoptic.json ./panoptic_test-dev2017_[algorithm_name]_results.json\nzip panoptic_test-dev2017_[algorithm_name]_results.zip -ur panoptic_test-dev2017_[algorithm_name]_results panoptic_test-dev2017_[algorithm_name]_results.json\n```\n\n----------------------------------------\n\nTITLE: Installing Instaboost Dependencies\nDESCRIPTION: Demonstrates how to install Instaboost dependencies using pip, a common requirement for certain algorithms in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install instaboostfast\n```\n\n----------------------------------------\n\nTITLE: Object Detection Format (OD) Example\nDESCRIPTION: This is an example of the Object Detection (OD) format for pretraining data. It contains information about the filename, height, width, and detection instances, including bounding boxes, labels, and categories. Requires the images to be present.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n{\"filename\": \"obj365_train_000000734304.jpg\",\n \"height\": 512,\n \"width\": 769,\n \"detection\": {\n    \"instances\": [\n          {\"bbox\": [109.4768676992, 346.0190429696, 135.1918335098, 365.3641967616], \"label\": 2, \"category\": \"chair\"},\n          {\"bbox\": [58.612365705900004, 323.2281494016, 242.6005859067, 451.4166870016], \"label\": 8, \"category\": \"car\"}\n                ]\n      }\n}\n```\n\n----------------------------------------\n\nTITLE: Run RetinaNet inference with TTA\nDESCRIPTION: Executes the large_image_demo.py script for object detection using the RetinaNet model. This script takes an image, configuration file, and checkpoint file as input. The `--tta` flag enables test-time augmentation for improved accuracy.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npython demo/large_image_demo.py \\\n    demo/large_image.jpg \\\n    configs/retinanet/retinanet_r50_fpn_1x_coco.py \\\n    checkpoint/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth --tta\n```\n\n----------------------------------------\n\nTITLE: ATSSHead Class Definition with get_targets method\nDESCRIPTION: This code snippet shows the definition of the `ATSSHead` class, which inherits from `AnchorHead`. It includes the `get_targets` method, which is responsible for generating `label_weights` and `bbox_weights` for the loss calculation.  These weights are then applied element-wise to the losses.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass ATSSHead(AnchorHead):\n\n    ...\n\n    def get_targets(self,\n                    anchor_list,\n                    valid_flag_list,\n                    gt_bboxes_list,\n                    img_metas,\n                    gt_bboxes_ignore_list=None,\n                    gt_labels_list=None,\n                    label_channels=1,\n                    unmap_outputs=True):\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the DDQ paper, including authors, title, conference, month, year, and pages.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ddq/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@InProceedings{Zhang_2023_CVPR,\n    author    = {Zhang, Shilong and Wang, Xinjiang and Wang, Jiaqi and Pang, Jiangmiao and Lyu, Chengqi and Zhang, Wenwei and Luo, Ping and Chen, Kai},\n    title     = {Dense Distinct Query for End-to-End Object Detection},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2023},\n    pages     = {7329-7338}\n}\n```\n\n----------------------------------------\n\nTITLE: Obtaining Channel Numbers of a New Backbone Network (Python)\nDESCRIPTION: This script shows how to obtain the channel numbers of a new backbone network in MMDetection by constructing the network, feeding it a dummy input, and printing the shape of the output at each stage. This helps determine the correct `in_channels` for subsequent layers like the neck in object detection models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/how_to.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.models import ResNet\nimport torch\nself = ResNet(depth=18)\nself.eval()\ninputs = torch.rand(1, 3, 32, 32)\nlevel_outputs = self.forward(inputs)\nfor level_out in level_outputs:\n    print(tuple(level_out.shape))\n\n```\n\nLANGUAGE: python\nCODE:\n```\n(1, 64, 8, 8)\n(1, 128, 4, 4)\n(1, 256, 2, 2)\n(1, 512, 1, 1)\n\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for HRNet Pose Estimation\nDESCRIPTION: This LaTeX snippet provides the citation information for the original paper on Deep High-Resolution Representation Learning for Human Pose Estimation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/hrnet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{SunXLW19,\n  title={Deep High-Resolution Representation Learning for Human Pose Estimation},\n  author={Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang},\n  booktitle={CVPR},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: CrowdHuman to COCO Conversion\nDESCRIPTION: Converts CrowdHuman dataset annotations to COCO format. This script utilizes `crowdhuman2coco.py` to transform the CrowdHuman annotations.  It requires the input path to the CrowdHuman dataset and the desired output path for the COCO annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_dataset_prepare.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# CrowdHuman\npython ./tools/dataset_converters/crowdhuman2coco.py -i ./data/crowdhuman -o ./data/crowdhuman/annotations\n```\n\n----------------------------------------\n\nTITLE: Unlabeled Data Pipeline Configuration (Python)\nDESCRIPTION: This Python code defines the overall unlabeled data pipeline, which combines the weak and strong augmentation pipelines using the `MultiBranch` transform. This allows the same unlabeled image to be processed with different augmentations for the teacher and student models in the semi-supervised training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# pipeline used to augment unlabeled data into different views\nunsup_pipeline = [\n    dict(type='LoadImageFromFile', backend_args = backend_args),\n    dict(type='LoadEmptyAnnotations'),\n    dict(\n        type='MultiBranch',\n        unsup_teacher=weak_pipeline,\n        unsup_student=strong_pipeline,\n    )\n]\n```\n\n----------------------------------------\n\nTITLE: Inheriting Base Configurations in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to inherit configurations from multiple existing files in MMDetection V3.0 to build a new configuration for fine-tuning a MaskRCNN model. It reduces redundancy and potential errors by inheriting from base model, dataset, runtime, and schedule configurations. It specifies the paths to the base configuration files to be inherited.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/finetune.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py',\n    '../_base_/schedules/schedule_1x.py'\n]\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN S50 Model Download\nDESCRIPTION: This URL links to the pre-trained weights for a Mask R-CNN model using a ResNeSt-50 backbone and FPN. These weights can be loaded into MMDetection to perform object detection and instance segmentation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/resnest/mask_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco/mask_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain_1x_coco_20200926_125503-8a2c3d47.pth)\n```\n\n----------------------------------------\n\nTITLE: MOT Define Tracker Parameters for Search - Python\nDESCRIPTION: Defines the tracker parameters to be searched during the MOT parameter search.  The `obj_score_thr` and `match_iou_thr` are configured as lists of values, and the script will evaluate all possible combinations of these parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_analysis_tools.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmodel=dict(\n    tracker=dict(\n        type='BaseTracker',\n        obj_score_thr=[0.4, 0.5, 0.6],\n        match_iou_thr=[0.4, 0.5, 0.6, 0.7]\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading Karpathy Annotations for COCO Caption\nDESCRIPTION: These commands download the COCO Caption dataset annotations from Google Cloud Storage, which are required for training COCO Caption models. The annotations are saved to the `data/coco/annotations` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/dataset_prepare.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd data/coco/annotations\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val_gt.json\nwget https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test_gt.json\n```\n\n----------------------------------------\n\nTITLE: Slurm Training with Port Option\nDESCRIPTION: This shell command demonstrates how to set the communication port when using Slurm by passing the `dist_params.port` configuration option via `--cfg-options`. This avoids modifying the original config files. `CUDA_VISIBLE_DEVICES` specifies the GPUs to use, and `GPUS` is the number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR} --cfg-options 'dist_params.port=29500'\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR} --cfg-options 'dist_params.port=29501'\n```\n\n----------------------------------------\n\nTITLE: Download ODinW Dataset from HuggingFace (Shell)\nDESCRIPTION: This snippet downloads the ODinW dataset from Hugging Face using `git lfs`. It first installs `git lfs` if it's not already installed, clones the repository, and then pulls the 'odinw_35' dataset. The `GIT_LFS_SKIP_SMUDGE=1` environment variable prevents the download of model weights.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\n\ngit lfs install\n# 我们不需要下载权重\nGIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/GLIPModel/GLIP\n\ncd GLIP\ngit lfs pull --include=\"odinw_35\"\n```\n\n----------------------------------------\n\nTITLE: Cascade R-CNN S101 Model Download\nDESCRIPTION: Link to download pre-trained weights for Cascade R-CNN with a ResNeSt-101 backbone within MMDetection. These weights facilitate object detection tasks using this specific model architecture.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/resnest/cascade_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/cascade_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco_20201005_113242-b9459f8f.pth)\n```\n\n----------------------------------------\n\nTITLE: Installing MMDetection Dependencies\nDESCRIPTION: This snippet installs the necessary dependencies for MMDetection, including openmim, mmengine, and mmcv. It then clones the MMDetection repository and installs it in editable mode. This prepares the environment for running MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# install dependencies\n%pip install -U openmim\n!mim install \"mmengine>=0.7.0\"\n!mim install \"mmcv>=2.0.0rc4\"\n\n# Install mmdetection\n!rm -rf mmdetection\n!git clone https://github.com/open-mmlab/mmdetection.git -b dev-3.x\n%cd mmdetection\n\n%pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Converting Object365 v2 to ODVG format (Python)\nDESCRIPTION: This script converts the Object365 v2 annotation file to the ODVG (Object Detection and Visual Grounding) format, which is required for training. It uses the coco2odvg.py conversion tool.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/coco2odvg.py data/objects365v2/annotations/zhiyuan_objv2_train_fixname.json -d o365v2\n```\n\n----------------------------------------\n\nTITLE: Migrating Test Pipeline Config (MMDetection 2.x)\nDESCRIPTION: This code demonstrates the test pipeline configuration in MMDetection 2.x, including loading images, multi-scale flipping augmentation, resizing, random flipping, normalization, padding, converting to tensors, and collecting relevant keys.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\n\n```\n\n----------------------------------------\n\nTITLE: Start Label-Studio Web Service\nDESCRIPTION: Starts the Label-Studio web service, making it accessible through a web browser.  By default, it runs on port 8080.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nlabel-studio start\n```\n\n----------------------------------------\n\nTITLE: EfficientDet BibTeX citation\nDESCRIPTION: This is the BibTeX entry for citing the EfficientDet paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_6\n\nLANGUAGE: BibTeX\nCODE:\n```\n@inproceedings{tan2020efficientdet,\n  title={Efficientdet: Scalable and efficient object detection},\n  author={Tan, Mingxing and Pang, Ruoming and Le, Quoc V},\n  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages={10781--10790},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: MMDetection BibTeX Citation\nDESCRIPTION: This BibTeX entry should be used when referencing MMDetection in research papers or other scholarly works. It includes the title, authors, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/README_zh-CN.md#_snippet_0\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{mmdetection,\n  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},\n  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and\n             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and\n             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and\n             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and\n             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong\n             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},\n  journal= {arXiv preprint arXiv:1906.07155},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Instaboostfast using pip\nDESCRIPTION: This command installs the `instaboostfast` package, which is required for using InstaBoost in MMDetection. It uses pip, the Python package installer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/instaboost/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install instaboostfast\n```\n\n----------------------------------------\n\nTITLE: MOT Model Inference Command\nDESCRIPTION: This command demonstrates how to run the `mot_demo.py` script for inference on a video or image sequence using a Multi-Object Tracking model.  It takes input video/image path, configuration file, and optional arguments for checkpoint, detector, reid, score threshold, device, output path, and display option.  The specific weight loading depends on the algorithm used.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_interference.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py \\\n    ${INPUTS}\n    ${CONFIG_FILE} \\\n    [--checkpoint ${CHECKPOINT_FILE}] \\\n    [--detector ${DETECTOR_FILE}] \\\n    [--reid ${REID_FILE}] \\\n    [--score-thr ${SCORE_THR}] \\\n    [--device ${DEVICE}] \\\n    [--out ${OUTPUT}] \\\n    [--show]\n```\n\n----------------------------------------\n\nTITLE: Train model on CPU in MMDetection\nDESCRIPTION: This snippet demonstrates how to train a model on a CPU by disabling GPU visibility. The CUDA_VISIBLE_DEVICES environment variable is set to -1, and then the tools/train.py script is executed with the desired configuration file and optional arguments. The config file specifies the model and dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_0\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=-1 python tools/train.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Installing LVIS Dataset Dependencies\nDESCRIPTION: Illustrates how to install the dependencies required for working with the LVIS dataset using pip and a direct GitHub repository link.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/lvis-dataset/lvis-api.git\n```\n\n----------------------------------------\n\nTITLE: Start MMYOLO Backend Inference Service\nDESCRIPTION: This snippet shows how to start the Label-Studio backend inference service, replacing RTMDet with MMYOLO. The user is expected to change the paths to point to correct MMYOLO config and weight files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\n\nlabel-studio-ml start projects/LabelStudio/backend_template --with \\\nconfig_file= path/to/mmyolo_config.py \\\ncheckpoint_file= path/to/mmyolo_weights.pth \\\ndevice=cpu \\\n--port 8003\n# device=cpu 为使用 CPU 推理，如果使用 GPU 推理，将 cpu 替换为 cuda:0\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation Metrics for MOT Parameter Search (Python)\nDESCRIPTION: This snippet shows how to define the evaluation metrics for the MOT parameter search tool.  It defines a `test_evaluator` dictionary specifying the type as 'MOTChallengeMetrics' and the metrics to be used for evaluation, such as 'HOTA', 'CLEAR', and 'Identity'. This is necessary for the parameter search script to properly evaluate the performance of different parameter combinations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_analysis_tools.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntest_evaluator=dict(type='MOTChallengeMetrics', metric=['HOTA', 'CLEAR', 'Identity'])\n```\n\n----------------------------------------\n\nTITLE: Evaluating a metric\nDESCRIPTION: This command evaluates certain metrics of a pkl result file according to a config file. It can evaluate metrics, format results and configure evaluation options.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/eval_metric.py ${CONFIG} ${PKL_RESULTS} [-h] [--format-only] [--eval ${EVAL[EVAL ...]}]\n                      [--cfg-options ${CFG_OPTIONS [CFG_OPTIONS ...]}]\n                      [--eval-options ${EVAL_OPTIONS [EVAL_OPTIONS ...]}]\n```\n\n----------------------------------------\n\nTITLE: Slurm Training Command (Shell)\nDESCRIPTION: This shell command demonstrates how to start a training job on a Slurm-managed cluster.  `${PARTITION}` is the Slurm partition name, `${JOB_NAME}` is the job name, `${CONFIG_FILE}` is the path to the configuration file, and `${WORK_DIR}` is the working directory. The `GPUS` environment variable specifies the number of GPUs to use. Replace with appropriate values.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n[GPUS=${GPUS}] ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${WORK_DIR}\n```\n\n----------------------------------------\n\nTITLE: Start RTMDet Backend Inference Service\nDESCRIPTION: Starts the Label-Studio ML backend, configuring it to use the specified RTMDet configuration file and checkpoint file for inference. The service listens on port 8003 and can be configured to use CPU or GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\n\nlabel-studio-ml start projects/LabelStudio/backend_template --with \\\nconfig_file=configs/rtmdet/rtmdet_m_8xb32-300e_coco.py \\\ncheckpoint_file=./work_dirs/rtmdet_m_8xb32-300e_coco_20220719_112220-229f527c.pth \\\ndevice=cpu \\\n--port 8003\n# Set device=cpu to use CPU inference, and replace cpu with cuda:0 to use GPU inference.\n```\n\n----------------------------------------\n\nTITLE: Large Image Inference Example Usage\nDESCRIPTION: This command provides an example of how to run the large image inference demo script with a specific image, configuration file, and checkpoint file. It also downloads a pre-trained model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n# inferecnce without tta\nwget -P checkpoint https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r101_fpn_2x_coco/faster_rcnn_r101_fpn_2x_coco_bbox_mAP-0.398_20200504_210455-1d2dac9c.pth\n\npython demo/large_image_demo.py \\\n    demo/large_image.jpg \\\n    configs/faster_rcnn/faster-rcnn_r101_fpn_2x_coco.py \\\n    checkpoint/faster_rcnn_r101_fpn_2x_coco_bbox_mAP-0.398_20200504_210455-1d2dac9c.pth\n```\n\n----------------------------------------\n\nTITLE: Directory structure for datasets\nDESCRIPTION: This shows the recommended directory structure for organizing datasets within the MMDetection project. It highlights the location of the coco, cityscapes and VOC datasets under the data directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_0\n\nLANGUAGE: None\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n│   ├── cityscapes\n│   │   ├── annotations\n│   │   ├── leftImg8bit\n│   │   │   ├── train\n│   │   │   ├── val\n│   │   ├── gtFine\n│   │   │   ├── train\n│   │   │   ├── val\n│   ├── VOCdevkit\n│   │   ├── VOC2007\n│   │   ├── VOC2012\n```\n\n----------------------------------------\n\nTITLE: Testing and Evaluation with dist_test_tracking.sh\nDESCRIPTION: This shell script command shows how to test and evaluate Mask2Former on the YouTube-VOS validation/test set using the `dist_test_tracking.sh` script. It requires specifying the configuration file and the path to the checkpoint file. The output is a submission file for evaluation on the YouTube-VIS dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask2former_vis/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# The number after config file represents the number of GPUs used.\nbash tools/dist_test_tracking.sh configs/mask2former_vis/mask2former_r50_8xb2-8e_youtubevis2021.py --checkpoint ${CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: COCO Panoptic Test Dataloader Update Configuration (Python)\nDESCRIPTION: This Python code snippet updates the dataloader configuration specifically for testing on the coco test-dev dataset by setting the correct ann_file and data_prefix. `_delete_=True` removes the original setting in `data_prefix` and uses the new value.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntest_dataloader = dict(\n    dataset=dict(\n        ann_file='annotations/panoptic_image_info_test-dev2017.json',\n        data_prefix=dict(img='test2017/', _delete_=True)))\ntest_evaluator = dict(\n    format_only=True,\n    ann_file=data_root + 'annotations/panoptic_image_info_test-dev2017.json',\n    outfile_prefix='./work_dirs/coco_panoptic/test')\n```\n\n----------------------------------------\n\nTITLE: Failed Dataset List Format (Text)\nDESCRIPTION: This is the format for the `failed_dataset_list.txt` file, used for retrying training on specific datasets. Each line represents the name of a dataset to be retrained, with an empty line at the end of the file. This text file allows the user to specify the list of datasets that need to be retrained.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nacl-x-ray\ntweeter-profile\nabdomen-mri\n\n\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for SORT\nDESCRIPTION: This LaTeX snippet provides the citation details for the Simple Online and Realtime Tracking (SORT) paper. It includes the title, authors, conference details, and year of publication. This citation is essential for properly referencing the SORT algorithm in academic works.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sort/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{bewley2016simple,\n  title={Simple online and realtime tracking},\n  author={Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},\n  booktitle={2016 IEEE International Conference on Image Processing (ICIP)},\n  pages={3464--3468},\n  year={2016},\n  organization={IEEE}\n}\n```\n\n----------------------------------------\n\nTITLE: Migrating Training and Testing Configs in MMDetection\nDESCRIPTION: Details the changes to the training loop configuration. `EpochBasedRunner` and `evaluation` are replaced with `train_cfg`, `val_cfg`, and `test_cfg`. `val_interval` is moved into `train_cfg`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nrunner = dict(\n    type='EpochBasedRunner',  # Type of training loop\n    max_epochs=12)  # Maximum number of training epochs\nevaluation = dict(interval=2)  # Interval for evaluation, check the performance every 2 epochs\n```\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(\n    type='EpochBasedTrainLoop',  # Type of training loop, please refer to https://github.com/open-mmlab/mmengine/blob/main/mmengine/runner/loops.py\n    max_epochs=12,  # Maximum number of training epochs\n    val_interval=2)  # Interval for validation, check the performance every 2 epochs\nval_cfg = dict(type='ValLoop')  # Type of validation loop\ntest_cfg = dict(type='TestLoop')  # Type of testing loop\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Hook in MMDetection\nDESCRIPTION: This code snippet shows how to define a custom hook `MyHook` within the MMDetection framework, implementing various methods for different stages of the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom mmengine.hooks import Hook\nfrom mmdet.registry import HOOKS\n\n\n@HOOKS.register_module()\nclass MyHook(Hook):\n\n    def __init__(self, a, b):\n\n    def before_run(self, runner) -> None:\n\n    def after_run(self, runner) -> None:\n\n    def before_train(self, runner) -> None:\n\n    def after_train(self, runner) -> None:\n\n    def before_train_epoch(self, runner) -> None:\n\n    def after_train_epoch(self, runner) -> None:\n\n    def before_train_iter(\n                          runner,\n                          batch_idx: int,\n                          data_batch: DATA_BATCH = None) -> None:\n\n    def after_train_iter(\n                         runner,\n                         batch_idx: int,\n                         data_batch: DATA_BATCH = None,\n                         outputs: Optional[dict] = None) -> None\n\n```\n\n----------------------------------------\n\nTITLE: Browsing Dataset (Shell)\nDESCRIPTION: This snippet shows how to use the `browse_dataset.py` script to visualize the training dataset for inspecting the dataset configuration. It requires the configuration file (`CONFIG_FILE`) as input, and allows specifying the display interval (`SHOW_INTERVAL`) in seconds.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_analysis_tools.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_dataset.py ${CONFIG_FILE} [--show-interval ${SHOW_INTERVAL}]\n```\n\n----------------------------------------\n\nTITLE: Converting OpenImages v6 to ODVG format (Python)\nDESCRIPTION: This script converts the OpenImages v6 dataset annotations to the ODVG format. It processes the bounding box annotations CSV file and creates JSON files suitable for training object detection models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npython tools/dataset_converters/openimages2odvg.py data/OpenImages/annotations\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX entry for FSAF\nDESCRIPTION: This is the BibTeX entry for citing the FSAF paper. It includes the title, authors, publication venue, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fsaf/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{zhu2019feature,\n  title={Feature Selective Anchor-Free Module for Single-Shot Object Detection},\n  author={Zhu, Chenchen and He, Yihui and Savvides, Marios},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={840--849},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation of Faster R-CNN in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Faster R-CNN paper, which introduces the Region Proposal Network (RPN). It includes the title, authors, booktitle, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rpn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{ren2015faster,\n  title={Faster r-cnn: Towards real-time object detection with region proposal networks},\n  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},\n  booktitle={Advances in neural information processing systems},\n  year={2015}\n}\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Directory Structure\nDESCRIPTION: This code snippet shows the expected directory structure for the COCO dataset when using MaskFormer.  It outlines the location of the annotations (panoptic JSON files and image folders) and the train/val/test image directories within the 'coco' folder.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/maskformer/README.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   │   ├── panoptic_train2017.json\n│   │   │   ├── panoptic_train2017\n│   │   │   ├── panoptic_val2017.json\n│   │   │   ├── panoptic_val2017\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Configuration for Balloon Dataset (Python)\nDESCRIPTION: This Python code snippet configures a Mask R-CNN model for training on the balloon dataset. It inherits from a base configuration and modifies the `num_classes` parameter in the `roi_head` to match the number of classes in the balloon dataset (1 in this case).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# 新配置继承了基本配置，并做了必要的修改\n_base_ = '../mask_rcnn/mask-rcnn_r50-caffe_fpn_ms-poly-1x_coco.py'\n\n# 我们还需要更改 head 中的 num_classes 以匹配数据集中的类别数\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(num_classes=1), mask_head=dict(num_classes=1)))\n```\n\n----------------------------------------\n\nTITLE: Importing the Custom Loss Module\nDESCRIPTION: This snippet demonstrates how to import the newly created `MyLoss` module into the `__init__.py` file within the `mmdet/models/losses` directory. This makes the custom loss function accessible for use within the MMDetection framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_models.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom .my_loss import MyLoss, my_loss\n```\n\n----------------------------------------\n\nTITLE: Prepare Unlabeled Annotations (Python)\nDESCRIPTION: This Python script prepares the COCO unlabeled2017 dataset for use in semi-supervised object detection. It loads the 'categories' information from the 'instances_train2017.json' file and adds it to the 'image_info_unlabeled2017.json' file, saving the result as 'instances_unlabeled2017.json'. This allows the unlabeled dataset to be used with the CocoDataset class.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/semi_det.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom mmengine.fileio import load, dump\n\nanns_train = load('instances_train2017.json')\nanns_unlabeled = load('image_info_unlabeled2017.json')\nanns_unlabeled['categories'] = anns_train['categories']\ndump(anns_unlabeled, 'instances_unlabeled2017.json')\n```\n\n----------------------------------------\n\nTITLE: Phrase Grounding with Automatic Noun Phrase Extraction\nDESCRIPTION: This shell command demonstrates phrase grounding using the MM Grounding DINO model, automatically extracting noun phrases from the input text with the NLTK library. It runs the `image_demo.py` script with a specified image, configuration file, model weights, and a text description. The model detects objects corresponding to the extracted noun phrases.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py images/apples.jpg \\\n        configs/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365.py \\\n        --weights grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth \\\n        --texts 'There are many apples here.'\n```\n\n----------------------------------------\n\nTITLE: Dataset Directory Structure\nDESCRIPTION: Shows the recommended directory structure for datasets when using MMDetection. It includes common datasets like COCO, Cityscapes, and VOC.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_0\n\nLANGUAGE: none\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n│   ├── cityscapes\n│   │   ├── annotations\n│   │   ├── leftImg8bit\n│   │   │   ├── train\n│   │   │   ├── val\n│   │   ├── gtFine\n│   │   │   ├── train\n│   │   │   ├── val\n│   ├── VOCdevkit\n│   │   ├── VOC2007\n│   │   ├── VOC2012\n```\n\n----------------------------------------\n\nTITLE: Training Command with Learning Rate Auto-Scaling (Shell)\nDESCRIPTION: This shell command demonstrates how to train a model using MMDetection's `tools/train.py` script with automatic learning rate scaling enabled.  The `${CONFIG_FILE}` placeholder should be replaced with the path to the configuration file for the model and dataset. The `--auto-scale-lr` flag enables the automatic learning rate scaling feature.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    --auto-scale-lr \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Citing SABL in LaTeX\nDESCRIPTION: This LaTeX snippet provides the correct format for citing the Side-Aware Boundary Localization paper in academic publications. It includes the title, authors, conference (ECCV), and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sabl/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{Wang_2020_ECCV,\n    title = {Side-Aware Boundary Localization for More Precise Object Detection},\n    author = {Jiaqi Wang and Wenwei Zhang and Yuhang Cao and Kai Chen and Jiangmiao Pang and Tao Gong and Jianping Shi and Chen Change Loy and Dahua Lin},\n    booktitle = {ECCV},\n    year = {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration (3.x Save Best)\nDESCRIPTION: This code snippet demonstrates how to configure the saving of the best model in MMDetection 3.x. It sets `save_best` to `auto` within the `CheckpointHook` in `default_hooks`, which automatically saves the best model based on the evaluation metric. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        save_best='auto'))\n```\n\n----------------------------------------\n\nTITLE: Install CLIP\nDESCRIPTION: This command installs CLIP (Contrastive Language-Image Pre-training) from the OpenAI GitHub repository. CLIP is a required dependency for Detic, as it enables the use of image-level labels for training object detectors.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/openai/CLIP.git\n```\n\n----------------------------------------\n\nTITLE: Get Raw Model Outputs (DataSamples) - Python\nDESCRIPTION: This code snippet demonstrates how to obtain the raw model outputs (DataSamples) from the MMDetection inferencer by setting the `return_datasamples` argument to `True`.  The original DataSample will be stored in the `predictions` field of the result.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nresult = inferencer('demo/demo.jpg', return_datasamples=True)\npprint(result, max_length=4)\n```\n\n----------------------------------------\n\nTITLE: Slurm Training with Config Files\nDESCRIPTION: This shell commands demonstrates launching two training jobs using different configuration files, each with a different communication port specified within the config file.  `CUDA_VISIBLE_DEVICES` specifies the GPUs to use, and `GPUS` is the number of GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR}\nCUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 ./tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR}\n```\n\n----------------------------------------\n\nTITLE: Slurm Training (Shell)\nDESCRIPTION: This command starts a training run using the Slurm workload manager and the `slurm_train.sh` script.  The script uses a provided configuration file and the number of GPUs.  An optional directory for output saving can be specified.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nbash scripts/slurm_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8\n# 如果想指定保存路径\nbash scripts/slurm_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Initializing layers with the same configuration\nDESCRIPTION: This snippet shows how to initialize multiple layers with the same configuration using the `layer` key.  All layers listed in the `layer` list (e.g., 'Conv1d', 'Conv2d', 'Linear') will be initialized with the provided `Constant` initializer and the specified `val` value.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninit_cfg = dict(type='Constant', layer=['Conv1d', 'Conv2d', 'Linear'], val=1)\n# ⽤相同的配置初始化整个模块\n```\n\n----------------------------------------\n\nTITLE: Train model on multiple GPUs (single node) in MMDetection\nDESCRIPTION: This snippet demonstrates how to train a model on multiple GPUs within a single node using the dist_train.sh script. The CONFIG_FILE and GPU_NUM variables specify the configuration file and the number of GPUs to use, respectively. Optional arguments can be passed to customize the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_4\n\nLANGUAGE: shell script\nCODE:\n```\nbash ./tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained Models in MMDetection (Python)\nDESCRIPTION: This snippet shows how to specify the path to a pre-trained model's weights for fine-tuning in MMDetection.  The `load_from` variable specifies the URL of the pre-trained model weights to be loaded before starting the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/finetune.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'  # noqa\n```\n\n----------------------------------------\n\nTITLE: Evaluating on Zero-Shot ODinW13 (Single Card)\nDESCRIPTION: This shell command evaluates the MM Grounding DINO model on the ODinW13 dataset in a zero-shot setting using a single GPU. It executes the `tools/test.py` script with the specified configuration file and model weights, generating evaluation metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/mm_grounding_dino/odinw/grounding_dino_swin-t_pretrain_odinw13.py \\\n        grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Multi-GPU Testing on COCO test-dev\nDESCRIPTION: This shell script tests Mask R-CNN on the COCO test-dev dataset using 8 GPUs, generating JSON files suitable for submission to the official evaluation server. The configuration files should be updated with test_evaluator and test_dataloader definitions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n       configs/cityscapes/mask-rcnn_r50_fpn_1x_cityscapes.py \\\n       checkpoints/mask_rcnn_r50_fpn_1x_cityscapes_20200227-afe51d5a.pth \\\n       8\n```\n\n----------------------------------------\n\nTITLE: Rename and Compress Results (Shell)\nDESCRIPTION: This script renames the panoptic segmentation results (JSON file and directory containing the mask) according to the COCO's naming convention. It compresses the renamed directory and JSON file into a zip file and renames the zip file.  The algorithm name is replaced with the used algorithm name.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test_results_submission.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# 在 WORK_DIR 中，我们有 panoptic 分割结果: 'panoptic' 和 'results. panoptical .json'。\ncd ${WORK_DIR}\n# 将 '[algorithm_name]' 替换为您使用的算法名称\nmv ./panoptic ./panoptic_test-dev2017_[algorithm_name]_results\nmv ./results.panoptic.json ./panoptic_test-dev2017_[algorithm_name]_results.json\nzip panoptic_test-dev2017_[algorithm_name]_results.zip -ur panoptic_test-dev2017_[algorithm_name]_results panoptic_test-dev2017_[algorithm_name]_results.json\n```\n\n----------------------------------------\n\nTITLE: Inference Example with MaskFormer (Shell)\nDESCRIPTION: This provides a specific example of how to run inference on the `test2017` dataset using a pre-trained MaskFormer model with a ResNet-50 backbone.  It includes the paths to the configuration file and checkpoint file, as well as the command-line arguments to update the data loader and evaluator settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test_results_submission.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# 单 gpu 测试\nCUDA_VISIBLE_DEVICES=0 python tools/test.py \\\n    configs/maskformer/maskformer_r50_mstrain_16x1_75e_coco.py \\\n    checkpoints/maskformer_r50_mstrain_16x1_75e_coco_20220221_141956-bc2699cb.pth \\\n    --cfg-options \\\n    test_dataloader.dataset.ann_file=annotations/panoptic_image_info_test-dev2017.json \\\n    test_dataloader.dataset.data_prefix.img=test2017 \\\n    test_dataloader.dataset.data_prefix._delete_=True \\\n    test_evaluator.format_only=True \\\n    test_evaluator.ann_file=data/coco/annotations/panoptic_image_info_test-dev2017.json \\\n    test_evaluator.outfile_prefix=work_dirs/maskformer/results\n```\n\n----------------------------------------\n\nTITLE: Compiling MMCV for RTX 30 Series GPUs\nDESCRIPTION: Provides a temporary solution for compiling MMCV on RTX 30 series GPUs by specifying the compute architecture during the pip installation process.  This ensures compatibility with older CUDA toolkits.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nMMCV_WITH_OPS=1 MMCV_CUDA_ARGS='-gencode=arch=compute_80,code=sm_80' pip install -e .\n```\n\n----------------------------------------\n\nTITLE: SORT Video Inference (Python)\nDESCRIPTION: This python script performs inference on a video using the SORT tracker with a Faster R-CNN detector. It takes a video file, the SORT configuration file, and the detector checkpoint path as input.  The output is saved as a video file named `mot.mp4`. Requires a single GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sort/README.md#_snippet_4\n\nLANGUAGE: shell script\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py --detector ${DETECTOR_CHECKPOINT_PATH}  --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Import MobileNet Backbone Module Using Custom Imports (Python)\nDESCRIPTION: This code shows an alternative method for importing the MobileNet backbone module using the `custom_imports` dictionary. This approach allows importing the module without modifying the original `__init__.py` file, providing a cleaner and more modular way to extend the framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(\n    imports=['mmdet.models.backbones.mobilenet'],\n    allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: Install PyTorch\nDESCRIPTION: This snippet installs PyTorch with CPU support or CUDA 11.3 support, along with torchvision and torchaudio. The `-f` flag specifies the URL to find pre-built wheels for PyTorch.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Linux and Windows CPU only\npip install torch==1.10.1+cpu torchvision==0.11.2+cpu torchaudio==0.10.1 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n# Linux and Windows CUDA 11.3\npip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n# OSX\npip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1\n```\n\n----------------------------------------\n\nTITLE: Panoptic FPN BibTeX Citation\nDESCRIPTION: Provides the BibTeX entry for citing the Panoptic FPN paper.  This is the standard citation format for academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/panoptic_fpn/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{kirillov2018panopticfpn,\n  author = {\n    Alexander Kirillov,\n    Ross Girshick,\n    Kaiming He,\n    Piotr Dollar,\n  },\n  title = {Panoptic Feature Pyramid Networks},\n  booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Run GLIP Inference Demo with Multiple Texts\nDESCRIPTION: Executes the `image_demo.py` script for multimodal object detection using the GLIP model with multiple text prompts. The `--texts` argument specifies a list of text prompts separated by `xx.`.  Here, it searches for both \"bench\" and \"car\".\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg glip_tiny_a_mmdet-b3654169.pth --texts 'bench. car'\n```\n\n----------------------------------------\n\nTITLE: Setting Random Seed in Python (Python)\nDESCRIPTION: This Python code snippet sets a fixed random seed for reproducibility of results. It sets the seed for the random number generator, PyTorch's random number generator, and CUDA's random number generator (for both single and multiple GPUs). This ensures that the same random numbers are generated each time the code is run, leading to consistent results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/DiffusionDet/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# hard set seed=0 before generating random boxes\n  seed = 0\n  random.seed(seed)\n  torch.manual_seed(seed)\n  # torch.cuda.manual_seed(seed)\n  torch.cuda.manual_seed_all(seed)\n  ...\n  noise_bboxes_raw = torch.randn(\n      (self.num_proposals, 4),\n      device=device)\n  ...\n```\n\n----------------------------------------\n\nTITLE: Specify Custom Optimizer in MMDetection Config (Python)\nDESCRIPTION: This code snippet shows how to specify the custom optimizer, `MyOptimizer`, in the MMDetection configuration file within the `optimizer` field in `optim_wrapper`. It replaces the default optimizer (SGD) with the custom optimizer and sets its specific parameters (a, b, c).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value))\n\n```\n\n----------------------------------------\n\nTITLE: Basic Inference using DetInferencer in Python\nDESCRIPTION: This snippet demonstrates the simplest way to perform object detection inference using the DetInferencer class. It initializes the inferencer with a pre-trained model name and runs inference on a sample image, displaying the results. The RTMDet model is used in this example.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.apis import DetInferencer\n\n# 初始化模型\ninferencer = DetInferencer('rtmdet_tiny_8xb32-300e_coco')\n\n# 推理示例图片\ninferencer('demo/demo.jpg', show=True)\n```\n\n----------------------------------------\n\nTITLE: Distributed Parameters Configuration\nDESCRIPTION: This Python code snippet shows how to configure the distributed training parameters, including the backend and port, directly within a configuration file. This is one way to set the communication port when using Slurm.  `backend` specifies the communication backend (e.g., 'nccl'), and `port` sets the port number.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndist_params = dict(backend='nccl', port=29500)\n```\n\nLANGUAGE: python\nCODE:\n```\ndist_params = dict(backend='nccl', port=29501)\n```\n\n----------------------------------------\n\nTITLE: Calculate Model FLOPs and Parameters (get_flops.py)\nDESCRIPTION: This snippet shows how to use `tools/analysis_tools/get_flops.py` to calculate the FLOPs (floating point operations per second) and the number of parameters of a given model configuration. It requires a model configuration file and optionally an input shape. The output displays the FLOPs in GFLOPs and the number of parameters in millions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/get_flops.py ${CONFIG_FILE} [--shape ${INPUT_SHAPE}]\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard and Wandb Backends\nDESCRIPTION: This snippet configures the Visualizer to store data in TensorBoard and Wandb in addition to the local backend. It modifies the `vis_backends` list to include `TensorboardVisBackend` and `WandbVisBackend` for logging training information to these platforms.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/visualization.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# https://mmengine.readthedocs.io/en/latest/api/visualization.html\n_base_.visualizer.vis_backends = [\n    dict(type='LocalVisBackend'), #\n    dict(type='TensorboardVisBackend'),\n    dict(type='WandbVisBackend'),]\n```\n\n----------------------------------------\n\nTITLE: Distributed Multi-GPU Training (Shell)\nDESCRIPTION: This command starts a distributed multi-GPU training run using the `dist_train.sh` script.  It requires a configuration file and the number of GPUs to use.  Optionally, it can specify a directory for saving the training outputs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8\n# 如果想指定保存路径\nbash scripts/dist_train.sh configs/faster-rcnn_r50_fpn_ms_8xb8_tweeter-profile.py 8 my_work_dirs\n```\n\n----------------------------------------\n\nTITLE: Video Inference Example Usage (GPU Accelerated)\nDESCRIPTION: This command demonstrates how to run the video inference demo script with GPU acceleration, specifying a video file, configuration file, checkpoint file, enabling nvdecode, and setting the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_gpuaccel_demo.py demo/demo.mp4 \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --nvdecode --out result.mp4\n```\n\n----------------------------------------\n\nTITLE: Slurm Training Example (Shell)\nDESCRIPTION: This shell command provides an example of how to use `slurm_train.sh` to train a Mask R-CNN model on the 'dev' partition with 16 GPUs, setting the working directory to a shared file system. Replace with appropriate values.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nGPUS=16 ./tools/slurm_train.sh dev mask_r50_1x configs/mask_rcnn_r50_fpn_1x_coco.py /nfs/xxxx/mask_rcnn_r50_fpn_1x\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN Training Log\nDESCRIPTION: This URL points to the training log file in JSON format for the specified Faster R-CNN model. The log contains detailed information about the training process, including loss values, metrics, and learning rate schedules.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/resnest/faster_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/faster_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco-20200926_125502.log.json)\n```\n\n----------------------------------------\n\nTITLE: Learning Rate Configuration (2.x)\nDESCRIPTION: This code snippet illustrates the learning rate configuration in MMDetection 2.x. It defines the learning rate policy, warmup strategy, warmup iterations, warmup ratio, step epochs, and gamma for learning rate decay.  Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nlr_config = dict(\n    policy='step',  # 在训练过程中使用 multi step 学习率策略\n    warmup='linear',  # 使用线性学习率预热\n    warmup_iters=500,  # 到第 500 个 iteration 结束预热\n    warmup_ratio=0.001,  # 学习率预热的系数\n    step=[8, 11],  # 在哪几个 epoch 进行学习率衰减\n    gamma=0.1)  # 学习率衰减系数\n```\n\n----------------------------------------\n\nTITLE: Citation of AlignDETR in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the AlignDETR paper. It includes the title, authors, year, eprint, archivePrefix, and primaryClass.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/AlignDETR/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@misc{cai2023aligndetr,\n      title={Align-DETR: Improving DETR with Simple IoU-aware BCE loss},\n      author={Zhi Cai and Songtao Liu and Guodong Wang and Zheng Ge and Xiangyu Zhang and Di Huang},\n      year={2023},\n      eprint={2304.07527},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Import MobileNet Backbone Module (Python)\nDESCRIPTION: This code shows how to import the newly created MobileNet backbone module into the MMDetection framework. It can be done either by adding a line to `mmdet/models/backbones/__init__.py` or using a custom imports dictionary in the configuration file, avoiding direct modifications to the original codebase.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom .mobilenet import MobileNet\n```\n\n----------------------------------------\n\nTITLE: Install MMCV\nDESCRIPTION: This snippet installs MMCV (Open MMLab Computer Vision) using pip and the `openmim` package manager.  It also mentions that `mmengine` will be installed automatically during the MMCV installation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install -U openmim\nmim install \"mmcv>=2.0.0\"\n# 安装 mmcv 的过程中会自动安装 mmengine\n```\n\n----------------------------------------\n\nTITLE: Modifying the Configuration\nDESCRIPTION: Shows how to modify the configuration file to use the custom `AugFPN` neck module.  This involves changing the `type` field in the `neck` dictionary to `'AugFPN'` and specifying the input and output channels.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nneck=dict(\n    type='AugFPN',\n    in_channels=[256, 512, 1024, 2048],\n    out_channels=256,\n    num_outs=5)\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Mask RCNN model with a RegNetX-1.6GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-1.6GF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-1_20210602_210641-6764cff5.pth)\n```\n\n----------------------------------------\n\nTITLE: Convert gRefCOCO to COCO Format (Python)\nDESCRIPTION: This script converts the gRefCOCO dataset to the COCO format using a conversion script provided by the gRefCOCO authors. It requires cloning the gRefCOCO repository and modifying a line in the script to generate the full JSON file.  The command uses the cloned repo's python script to convert the grefcoco dataset to COCO format, and places output JSON file in the specified directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n# 需要克隆官方 repo\ngit clone https://github.com/henghuiding/gRefCOCO.git\ncd gRefCOCO/mdetr\npython scripts/fine-tuning/grefexp_coco_format.py --data_path ../../data/coco/grefs --out_path ../../data/coco/mdetr_annotations/ --coco_path ../../data/coco\n```\n\n----------------------------------------\n\nTITLE: Import PAFPN Neck Module Using Custom Imports (Python)\nDESCRIPTION: This code shows an alternative method for importing the PAFPN neck module using the `custom_imports` dictionary. This approach allows importing the module without modifying the original `__init__.py` file, providing a cleaner and more modular way to extend the framework.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(\n    imports=['mmdet.models.necks.pafpn'],\n    allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: Multi-Datasets Configuration in Python\nDESCRIPTION: This Python code defines the configuration for using multiple datasets (LVIS and ImageNet) during training. It shows how to configure the datasets, dataloaders, samplers, and pipelines for mixed data training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic_new/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndataset_det = dict(\n    type='ClassBalancedDataset',\n    oversample_thr=1e-3,\n    dataset=dict(\n        type='LVISV1Dataset',\n        data_root='data/lvis/',\n        ann_file='annotations/lvis_v1_train.json',\n        data_prefix=dict(img=''),\n        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n        pipeline=train_pipeline_det,\n        backend_args=backend_args))\n\ndataset_cls = dict(\n    type='ImageNetLVISV1Dataset',\n    data_root='data/imagenet',\n    ann_file='annotations/imagenet_lvis_image_info.json',\n    data_prefix=dict(img='ImageNet-LVIS/'),\n    pipeline=train_pipeline_cls,\n    backend_args=backend_args)\n\ntrain_dataloader = dict(\n    batch_size=[8, 32],\n    num_workers=2,\n    persistent_workers=True,\n    sampler=dict(\n        type='MultiDataSampler',\n        dataset_ratio=[1, 4]),\n    batch_sampler=dict(\n        type='MultiDataAspectRatioBatchSampler',\n        num_datasets=2),\n    dataset=dict(\n        type='ConcatDataset',\n        datasets=[dataset_det, dataset_cls]))\n```\n\n----------------------------------------\n\nTITLE: Training on a Single GPU with MMDetection\nDESCRIPTION: This snippet shows how to train a model on a single GPU using MMDetection. It directly executes the `tools/train.py` script with the specified configuration file and optional arguments. The CUDA_VISIBLE_DEVICES environment variable can be used to select a specific GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_2\n\nLANGUAGE: shell 脚本\nCODE:\n```\npython tools/train.py ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Robustness Testing with Noise Corruptions\nDESCRIPTION: This command runs a robustness test with a specific category of image corruptions (`noise`). `${CONFIG_FILE}` and `${CHECKPOINT_FILE}` need to be replaced with the actual paths. `${RESULT_FILE}` is the desired name for the output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/robustness_benchmarking.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] --corruptions noise\n```\n\n----------------------------------------\n\nTITLE: Setting DYNAMO_CACHE_SIZE_LIMIT for Multi-GPU\nDESCRIPTION: This shell script shows how to set the `DYNAMO_CACHE_SIZE_LIMIT` environment variable to control the cache size used by TorchDynamo, and how to enable the `torch.compile` function for MMDetection when training with multiple GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nexport DYNAMO_CACHE_SIZE_LIMIT = 4\n./tools/dist_train.sh configs/rtmdet/rtmdet_s_8xb32-300e_coco.py 8 --cfg-options compile=True\n```\n\n----------------------------------------\n\nTITLE: FocalLoss Class Definition - Python\nDESCRIPTION: This Python code defines the `FocalLoss` class, inheriting from `nn.Module`, and registers it as a loss module in MMDetection. It initializes the loss function with parameters such as `use_sigmoid`, `gamma`, `alpha`, `reduction`, and `loss_weight`, allowing customization of the Focal Loss's behavior.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_losses.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@LOSSES.register_module()\nclass FocalLoss(nn.Module):\n\n    def __init__(self,\n                 use_sigmoid=True,\n                 gamma=2.0,\n                 alpha=0.25,\n                 reduction='mean',\n                 loss_weight=1.0):\n```\n\n----------------------------------------\n\nTITLE: Define a New MobileNet Backbone in MMDetection (Python)\nDESCRIPTION: This snippet demonstrates how to define a custom MobileNet backbone within the MMDetection framework. It involves creating a new Python file, defining a MobileNet class that inherits from nn.Module, and registering it using the @MODELS.register_module() decorator. The class includes an __init__ method for initialization and a forward method for processing input tensors. This module is intended to be placed in `mmdet/models/backbones/mobilenet.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\n\nfrom mmdet.registry import MODELS\n\n\n@MODELS.register_module()\nclass MobileNet(nn.Module):\n\n    def __init__(self, arg1, arg2):\n        pass\n\n    def forward(self, x):  # should return a tuple\n        pass\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for H-DETR\nDESCRIPTION: This LaTeX code provides the citation information for the H-DETR paper.  It includes the title, authors, journal, and year of publication. This citation can be used in academic papers and other publications to properly credit the authors of the H-DETR algorithm.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/HDINO/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{jia2022detrs,\n  title={DETRs with Hybrid Matching},\n  author={Jia, Ding and Yuan, Yuhui and He, Haodi and Wu, Xiaopei and Yu, Haojun and Lin, Weihong and Sun, Lei and Zhang, Chao and Hu, Han},\n  journal={arXiv preprint arXiv:2207.13080},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Testing on Multi-GPU with MMDetection\nDESCRIPTION: This snippet shows how to test a model on multiple GPUs using MMDetection. It uses the `tools/dist_test_tracking.sh` script, specifying the configuration file and the number of GPUs to use. This script facilitates distributed testing across multiple GPUs on a single node.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_14\n\nLANGUAGE: shell 脚本\nCODE:\n```\nbash ./tools/dist_test_tracking.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Custom Imports for DoubleHeadRoIHead (Python)\nDESCRIPTION: This snippet demonstrates how to import the newly created DoubleHeadRoIHead and DoubleConvFCBBoxHead modules into the MMDetection framework using custom imports. This allows importing the modules without modifying the original `__init__.py` files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports=dict(\n    imports=['mmdet.models.roi_heads.double_roi_head', 'mmdet.models.roi_heads.bbox_heads.double_bbox_head'])\n```\n\n----------------------------------------\n\nTITLE: Faster R-CNN S101 Training Log\nDESCRIPTION: Download link to the training log in JSON format of the Faster R-CNN model with ResNeSt-101. The log includes information such as loss, learning rate, and evaluation metrics during training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/resnest/faster_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/faster_rcnn_s101_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco-20201006_021058.log.json)\n```\n\n----------------------------------------\n\nTITLE: Configuring TrackVisualizationHook in Shell\nDESCRIPTION: This snippet shows how to configure the `TrackVisualizationHook` in your configuration file to draw predicted results. Setting `draw=True` enables the visualization of detection/tracking results during the training or testing process. This configuration needs to be added to the `default_hooks` section of your configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_visualization.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ndefault_hooks = dict(visualization=dict(type='TrackVisualizationHook', draw=True))\n```\n\n----------------------------------------\n\nTITLE: Enabling and Configuring DetVisualizationHook\nDESCRIPTION: This snippet demonstrates how to enable and configure the DetVisualizationHook to draw annotations and predictions during training or testing, as well as how to display the images. It updates the visualization settings in the configuration file to set `draw` and `show` to True.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/visualization.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nvisualization = _base_.default_hooks.visualization\nvisualization.update(dict(draw=True, show=True))\n```\n\n----------------------------------------\n\nTITLE: Utilizing AvoidCUDAOOM for OOM Error Handling in Python\nDESCRIPTION: This code shows how to use `AvoidCUDAOOM` to handle CUDA out-of-memory errors by retrying the function with different strategies (empty cache, FP16 conversion, CPU offloading). The retry_if_cuda_oom function allows the code to continue running even if the GPU runs out of memory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.utils import AvoidCUDAOOM\n\noutput = AvoidCUDAOOM.retry_if_cuda_oom(some_function)(input1, input2)\n```\n\n----------------------------------------\n\nTITLE: Update Test Image Information (Python)\nDESCRIPTION: This Python script updates the category information in the test image information file (`image_info_test-dev2017.json`) using the category information from the `panoptic_val2017.json` file. This is necessary because `image_info_test-dev2017.json` lacks the `isthing` attribute in its category information.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test_results_submission.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython tools/misc/gen_coco_panoptic_test_info.py data/coco/annotations\n```\n\n----------------------------------------\n\nTITLE: Gradient Clipping Configuration in MMDetection\nDESCRIPTION: This snippet shows how to configure gradient clipping in MMDetection's training configuration. Gradient clipping helps stabilize training by preventing excessively large gradients. The `max_norm` parameter specifies the maximum norm of the gradients, and `norm_type` specifies the type of norm to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noptimizer_config=dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer on a Specific GPU\nDESCRIPTION: This snippet initializes the DetInferencer and binds it to a specific GPU device. The `device` parameter is set to 'cuda:1', which specifies the second GPU (index 1). Requires CUDA and PyTorch to be configured correctly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cuda:1')\n```\n\n----------------------------------------\n\nTITLE: Using Gradient Clipping for Stable Training in MMDetection\nDESCRIPTION: This code snippet shows how to use gradient clipping in the `optim_wrapper` configuration to stabilize training, including settings for maximum norm and norm type. The `_delete_=True` ensures that a default `optim_wrapper` configuration is overwritten.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    _delete_=True, clip_grad=dict(max_norm=35, norm_type=2))\n\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Reduction Method in Focal Loss\nDESCRIPTION: This configuration snippet shows how to change the reduction method in Focal Loss from 'mean' (default) to 'sum'.  This affects how the individual element losses are aggregated into a single scalar loss value.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=2.0,\n    alpha=0.25,\n    loss_weight=1.0,\n    reduction='sum')\n```\n\n----------------------------------------\n\nTITLE: Optimize YOLO Anchors (optimize_anchors.py) - Differential Evolution\nDESCRIPTION: This snippet shows how to optimize YOLO anchors using the differential evolution algorithm with `tools/analysis_tools/optimize_anchors.py`. It requires a configuration file, the `--algorithm differential_evolution` option, the input shape, and an output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/optimize_anchors.py ${CONFIG} --algorithm differential_evolution --input-shape ${INPUT_SHAPE [WIDTH HEIGHT]} --output-dir ${OUTPUT_DIR}\n```\n\n----------------------------------------\n\nTITLE: Download RetinaNet checkpoint for inference with TTA\nDESCRIPTION: Downloads the pre-trained RetinaNet checkpoint file from a specified URL to the 'checkpoint' directory. This checkpoint is required for running the subsequent inference demo script with test-time augmentation (TTA).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nwget -P checkpoint https://download.openmmlab.com/mmdetection/v2.0/retinanet/retinanet_r50_fpn_1x_coco/retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth\n```\n\n----------------------------------------\n\nTITLE: Customize Epoch-Based Training Loop in MMDetection (Python)\nDESCRIPTION: This code snippet configures an `EpochBasedTrainLoop` in MMDetection, specifying the maximum number of epochs (max_epochs=12), the epoch to start validation (val_begin=1), and the interval between validation runs (val_interval=1).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/advanced_guides/customize_runtime.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrain_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_begin=1, val_interval=1)\n\n```\n\n----------------------------------------\n\nTITLE: Example: Test QDTrack on single GPU\nDESCRIPTION: This snippet provides an example of testing the QDTrack model on a single GPU (GPU ID 2) using tools/test_tracking.py with a specific configuration file and specifying the detector checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_13\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=2 python tools/test_tracking.py configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py --detector ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Convert GQA dataset to ODVG format (goldg2odvg.py)\nDESCRIPTION: This script converts the GQA portion of the GoldG dataset to the ODVG format. It utilizes the `goldg2odvg.py` script and creates the `final_mixed_train_no_coco_vg.json` file in the `data/gqa` directory.\nDependencies: goldg2odvg.py script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/goldg2odvg.py data/gqa/final_mixed_train_no_coco.json\n```\n\n----------------------------------------\n\nTITLE: DETR BibTeX Citation\nDESCRIPTION: This is the BibTeX entry for citing the DETR paper, which is titled \"End-to-End Object Detection with Transformers\". It includes the authors, title, booktitle (ECCV), and year (2020).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/detr/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{detr,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  booktitle = {ECCV},\n  year      = {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation for Pascal VOC Dataset (LaTeX)\nDESCRIPTION: LaTeX code snippet to cite the Pascal VOC dataset in academic publications. This snippet provides the standard BibTeX entry for referencing the original Pascal VOC challenge paper, including authors, title, journal, and publication details.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pascal_voc/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@Article{Everingham10,\n   author = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n   title = \"The Pascal Visual Object Classes (VOC) Challenge\",\n   journal = \"International Journal of Computer Vision\",\n   volume = \"88\",\n   year = \"2010\",\n   number = \"2\",\n   month = jun,\n   pages = \"303--338\",\n}\n```\n\n----------------------------------------\n\nTITLE: MMDetection Assigner GPU Configuration\nDESCRIPTION: This snippet shows how to configure the assigner in MMDetection to use CPU for IOU calculation when the number of ground truth boxes exceeds a threshold.  Setting `gpu_assign_thr=N` forces the assigner to use CPU for IOU calculation when there are more than N ground truth boxes, mitigating potential OOM errors on the GPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\ngpu_assign_thr=N\n```\n\n----------------------------------------\n\nTITLE: Setting Validation Interval (Python)\nDESCRIPTION: This Python code snippet shows how to configure the validation interval during training within a MMDetection configuration file. The `val_interval` parameter specifies how often (in epochs) the model should be evaluated on the validation dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 每 12 轮迭代进行一次测试评估\ntrain_cfg = dict(val_interval=12)\n```\n\n----------------------------------------\n\nTITLE: Checking nvcc and GCC versions with shell commands\nDESCRIPTION: This code snippet uses shell commands to check the versions of nvcc (NVIDIA CUDA Compiler) and GCC (GNU Compiler Collection). This is useful for ensuring the correct versions of compilers are installed for MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/demo/inference_demo.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!nvcc -V\n# Check GCC version\n!gcc --version\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer with Custom Weights\nDESCRIPTION: This snippet initializes the DetInferencer with a pre-trained model name and a custom path to the weights file. The `weights` parameter specifies the location of the custom checkpoint file. The model structure corresponds to the model specified in the `model` parameter.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', weights='path/to/rtmdet.pth')\n```\n\n----------------------------------------\n\nTITLE: Configuring CheckpointHook in MMDetection (Python)\nDESCRIPTION: This code snippet configures the CheckpointHook to save checkpoints periodically during training. The interval parameter specifies the frequency of checkpoint saving, max_keep_ckpts limits the number of checkpoints stored, and save_optimizer determines whether the optimizer's state dictionary is included in the checkpoint.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        interval=1,\n        max_keep_ckpts=3,\n        save_optimizer=True))\n\n```\n\n----------------------------------------\n\nTITLE: Setting Roboflow API Key (Shell)\nDESCRIPTION: This command sets the Roboflow API key as an environment variable, which is required for downloading datasets from the Roboflow platform.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README_zh-CN.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport ROBOFLOW_API_KEY = 你的 Private API Key\n```\n\n----------------------------------------\n\nTITLE: Citation Information - LaTeX\nDESCRIPTION: LaTeX code snippet providing the citation information for the paper A Tri-Layer Plugin to Improve Occluded Detection. This snippet should be used when utilizing the COCO occluded/separated metric.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_46\n\nLANGUAGE: LaTeX\nCODE:\n```\n@article{zhan2022triocc,\n    title={A Tri-Layer Plugin to Improve Occluded Detection},\n    author={Zhan, Guanqi and Xie, Weidi and Zisserman, Andrew},\n    journal={British Machine Vision Conference},\n    year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Renaming Mask2Former config files in MMDetection\nDESCRIPTION: This snippet illustrates the renaming of Mask2Former configuration files between MMDetection versions prior to 2.25.0 and version 2.25.0 and later, specifically distinguishing between configurations for panoptic segmentation and instance segmentation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/compatibility.md#_snippet_1\n\nLANGUAGE: None\nCODE:\n```\n'mask2former_xxx_coco.py' represents config files for **instance segmentation**.\n'mask2former_xxx_coco-panoptic.py' represents config files for **panoptic segmentation**.\n```\n\n----------------------------------------\n\nTITLE: Install MMCV without MIM\nDESCRIPTION: This command installs MMCV directly using pip, specifying the source for pre-built binaries that match the PyTorch and CUDA versions. This is useful when MIM is not used.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\npip install \"mmcv>=2.0.0\" -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.12.0/index.html\n```\n\n----------------------------------------\n\nTITLE: Configuring custom imports for AugFPN\nDESCRIPTION: This Python dictionary configures custom imports in the MMDetection config file. It specifies the path to the AugFPN module and allows for failed imports, avoiding modifications to the original codebase.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/new_model.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ncustom_imports = dict(\n    imports=['mmdet.models.necks.augfpn'],\n    allow_failed_imports=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing DetInferencer on CPU\nDESCRIPTION: This snippet initializes the DetInferencer and forces it to use the CPU for inference. The `device` parameter is set to 'cpu', which ensures that all computations are performed on the CPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninferencer = DetInferencer(model='rtmdet_tiny_8xb32-300e_coco', device='cpu')\n```\n\n----------------------------------------\n\nTITLE: Converting Cityscapes and Pascal VOC datasets to COCO format\nDESCRIPTION: These commands convert the Cityscapes dataset and Pascal VOC dataset to the COCO format. The first requires the path to the Cityscapes dataset, and the second requires the path to the Pascal VOC devkit.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/cityscapes.py ${CITYSCAPES_PATH} [-h] [--img-dir ${IMG_DIR}] [--gt-dir ${GT_DIR}] [-o ${OUT_DIR}] [--nproc ${NPROC}]\npython tools/dataset_converters/pascal_voc.py ${DEVKIT_PATH} [-h] [-o ${OUT_DIR}]\n```\n\n----------------------------------------\n\nTITLE: Testing and Inference\nDESCRIPTION: Command to test a trained model using a specified configuration file and checkpoint file. This evaluates the model's performance on a test dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/cityscapes/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes.py work_dirs/cascade-mask-rcnn_r50_augfpn_autoaug-10e_cityscapes/epoch_10.pth\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration (2.x Interval)\nDESCRIPTION: This code snippet shows how to configure the logging interval in MMDetection 2.x. It sets the interval to 50, meaning a log message is printed every 50 iterations. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nlog_config = dict(\n    interval=50)\n```\n\n----------------------------------------\n\nTITLE: Configuring the Head to Use the New Loss (Python)\nDESCRIPTION: Shows how to modify the `loss_bbox` field in the Head configuration to use the newly defined `MyLoss`. The `type` parameter specifies the name of the loss module, and `loss_weight` specifies the weight of the loss.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nloss_bbox=dict(type='MyLoss', loss_weight=1.0))\n```\n\n----------------------------------------\n\nTITLE: Single GPU Training Command (Shell)\nDESCRIPTION: This shell command demonstrates how to train a model on a single GPU using MMDetection's `tools/train.py` script. The `${CONFIG_FILE}` placeholder should be replaced with the path to the configuration file for the model and dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Migrating Pascal VOC Evaluator Configuration in MMDetection\nDESCRIPTION: Shows how to migrate the Pascal VOC evaluator configuration from MMDetection v2.x to v3.x.  The evaluator type is changed from relying on dataset parameters to using a dedicated `VOCMetric` evaluator.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndata = dict(\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt'))\nevaluation = dict(metric='mAP')\n```\n\nLANGUAGE: python\nCODE:\n```\nval_evaluator = dict(\n    type='VOCMetric',\n    metric='mAP',\n    eval_mode='11points')\n```\n\n----------------------------------------\n\nTITLE: FocalLoss Class Definition\nDESCRIPTION: This code defines the FocalLoss class in MMDetection. It inherits from `nn.Module` and initializes parameters such as `use_sigmoid`, `gamma`, `alpha`, `reduction`, and `loss_weight`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@LOSSES.register_module()\nclass FocalLoss(nn.Module):\n\n    def __init__(self,\n                 use_sigmoid=True,\n                 gamma=2.0,\n                 alpha=0.25,\n                 reduction='mean',\n                 loss_weight=1.0):\n```\n\n----------------------------------------\n\nTITLE: Browsing a detection dataset (Shell)\nDESCRIPTION: This script helps inspect the detection dataset (images and annotations) or save images to a specified directory.  It can skip certain types of annotations and control the display interval.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_dataset.py ${CONFIG} [-h] [--skip-type ${SKIP_TYPE[SKIP_TYPE...]}] [--output-dir ${OUTPUT_DIR}] [--not-show] [--show-interval ${SHOW_INTERVAL}]\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Interval Configuration (3.x)\nDESCRIPTION: This snippet configures the checkpoint saving interval in MMDetection 3.x using `CheckpointHook` within `default_hooks`. The `interval` parameter specifies how often checkpoints are saved (in epochs).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        interval=1))\n```\n\n----------------------------------------\n\nTITLE: Save Best Model Configuration (3.x)\nDESCRIPTION: This snippet configures saving the best model during evaluation in MMDetection 3.x using `CheckpointHook` within `default_hooks`. The `save_best` parameter determines when to save the model with the best evaluation metric.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        save_best='auto'))\n```\n\n----------------------------------------\n\nTITLE: DeepSORT Citation (Latex)\nDESCRIPTION: This LaTeX code provides the citation information for the original DeepSORT paper. It includes the title, authors, conference details, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/deepsort/README.md#_snippet_4\n\nLANGUAGE: Latex\nCODE:\n```\n@inproceedings{wojke2017simple,\n  title={Simple online and realtime tracking with a deep association metric},\n  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},\n  booktitle={2017 IEEE international conference on image processing (ICIP)},\n  pages={3645--3649},\n  year={2017},\n  organization={IEEE}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation in Latex\nDESCRIPTION: This LaTeX snippet provides the citation information for the TridentNet paper, including title, authors, journal, and year. It allows for easy integration of the citation into academic papers and reports.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/tridentnet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@InProceedings{li2019scale,\n  title={Scale-Aware Trident Networks for Object Detection},\n  author={Li, Yanghao and Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang},\n  journal={The International Conference on Computer Vision (ICCV)},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation of ResNet Improved Training Procedure (LaTeX)\nDESCRIPTION: This LaTeX code snippet provides the citation information for the 'Resnet strikes back: An improved training procedure in timm' research paper. It includes the title, authors, journal, and year of publication for proper academic citation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnet_strikes_back/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{wightman2021resnet,\ntitle={Resnet strikes back: An improved training procedure in timm},\nauthor={Ross Wightman, Hugo Touvron, Hervé Jégou},\njournal={arXiv preprint arXiv:2110.00476},\nyear={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Mask RCNN model using a RegNetX-1.6GF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n[config](./mask-rcnn_regnetx-1.6GF_fpn_ms-poly-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Downloading Objects365 V2 Dataset\nDESCRIPTION: This script downloads and extracts the Objects365 V2 dataset. It requires the dataset name, saving directory, and optional flags to unzip the downloaded file and delete the zip file after extraction. It utilizes `tools/misc/download_dataset.py` to perform the download.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/objects365/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name objects365v2 \\\n--save-dir ${SAVING PATH} \\\n--unzip \\\n--delete  # Optional, delete the download zip file\n```\n\n----------------------------------------\n\nTITLE: Single-GPU Training with MMPreTrain\nDESCRIPTION: This command initiates training using a single GPU. The `${CONFIG_FILE}` variable should be replaced with the path to the configuration file specifying the training parameters. Optional arguments can be added to customize the training process.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/classification/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/train.py \\\n    ${CONFIG_FILE} \\\n    [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Install MMPretrain using pip\nDESCRIPTION: This command installs the MMPretrain library using pip, which is required for using the ConvNeXt backbone in MMDetection. MMPretrain provides a collection of pre-trained backbones for various downstream tasks. This step is crucial before using ConvNeXt-V2 in MMDetection.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/ConvNeXt-V2/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install mmpretrain\n```\n\n----------------------------------------\n\nTITLE: SSD Model Conversion Script\nDESCRIPTION: This shell script converts old version SSD models to the new version for use with MMDetection v2.14.0 and later versions. It takes the old model path and the desired path for the converted model as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/compatibility.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/model_converters/upgrade_ssd_version.py ${OLD_MODEL_PATH} ${NEW_MODEL_PATH}\n```\n\n----------------------------------------\n\nTITLE: Initializing TrackLocalVisualizer in Python\nDESCRIPTION: This code snippet demonstrates how to initialize the `TrackLocalVisualizer` in Python. The visualizer is responsible for handling the actual drawing of the tracking results on the images. It is used internally by `TrackVisualizationHook` to perform the visualization.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_visualization.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nvisualizer = dict(type='TrackLocalVisualizer')\n```\n\n----------------------------------------\n\nTITLE: Listing Available Models with DetInferencer\nDESCRIPTION: This snippet uses the `list_models` method of the `DetInferencer` class to retrieve a list of available pre-trained models from the specified model zoo. The 'mmdet' argument specifies the MMDetection model zoo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# models 是一个模型名称列表，它们将自动打印\nmodels = DetInferencer.list_models('mmdet')\n```\n\n----------------------------------------\n\nTITLE: Convert COCO to OVD format for open-vocabulary fine-tuning\nDESCRIPTION: Converts the COCO dataset to the OVD (Open-Vocabulary Detection) format for open-vocabulary fine-tuning. The script handles dividing categories into base and novel classes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/coco2ovd.py data/coco/\n```\n\n----------------------------------------\n\nTITLE: Example of preparing a model for publishing\nDESCRIPTION: This command shows an example of how to prepare a model for publishing by converting model weights to CPU tensors, deleting optimizer states, and computing the hash of the checkpoint file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/publish_model.py work_dirs/faster_rcnn/latest.pth faster_rcnn_r50_fpn_1x_20190801.pth\n```\n\n----------------------------------------\n\nTITLE: COCO Error Analysis (coco_error_analysis.py)\nDESCRIPTION: This tool analyzes COCO results per category and by different criteria and makes a plot to provide useful information. It requires the result file and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_error_analysis.py ${RESULT} ${OUT_DIR} [-h] [--ann ${ANN}] [--types ${TYPES[TYPES...]}]\n```\n\n----------------------------------------\n\nTITLE: Compute Average Training Speed (analyze_logs.py)\nDESCRIPTION: This example calculates the average training speed from a training log file. The `log.json` file is analyzed, and the `--include-outliers` flag can be used to include outliers in the calculation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py cal_train_time log.json [--include-outliers]\n```\n\n----------------------------------------\n\nTITLE: Download RefCOCO dataset in MMDetection (Python)\nDESCRIPTION: This snippet downloads and unzips the RefCOCO dataset using a Python script provided in the MMDetection tools directory. The dataset is saved in the `data/coco` directory, which must exist prior to running.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name refcoco --save-dir data/coco --unzip\n```\n\n----------------------------------------\n\nTITLE: Prepare Model for Release (publish_model.py)\nDESCRIPTION: This snippet demonstrates how to prepare a model for release using `tools/model_converters/publish_model.py`. It takes the input and output filenames as arguments. Steps include converting the model to CPU tensors, removing optimizer states, and calculating the hash value of the checkpoint file to include in the filename.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/publish_model.py ${INPUT_FILENAME} ${OUTPUT_FILENAME}\n```\n\n----------------------------------------\n\nTITLE: Testing QDTrack MOT Model on Single GPU\nDESCRIPTION: This example shows how to test the QDTrack MOT model on a single GPU. It specifies the detector checkpoint using the `--detector` argument.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_13\n\nLANGUAGE: shell 脚本\nCODE:\n```\nCUDA_VISIBLE_DEVICES=2 python tools/test_tracking.py configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py --detector ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Installing TorchServe and dependencies (Shell)\nDESCRIPTION: This command installs TorchServe and its dependencies, including torch-model-archiver, torch-workflow-archiver, and nvgpu. Ensure that PyTorch and MMDetection are already installed in the Python environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install torchserve torch-model-archiver torch-workflow-archiver nvgpu\n```\n\n----------------------------------------\n\nTITLE: Configuring MemoryProfilerHook\nDESCRIPTION: This code snippet demonstrates how to configure the MemoryProfilerHook in the config file. The hook is added to the custom_hooks list and the interval parameter specifies the frequency at which memory information is recorded.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_hooks.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MemoryProfilerHook', interval=50)\n]\n```\n\n----------------------------------------\n\nTITLE: Example: Train QDTrack on single GPU\nDESCRIPTION: This snippet provides an example of training the QDTrack model on a single GPU (GPU ID 2) using tools/train.py with a specific configuration file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_train_test.md#_snippet_3\n\nLANGUAGE: shell script\nCODE:\n```\nCUDA_VISIBLE_DEVICES=2 python tools/train.py configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN Multi-GPU Testing\nDESCRIPTION: This shell script tests the Mask R-CNN model using 8 GPUs, outputting the results to a `.pkl` file. The `dist_test.sh` script handles distributed testing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh \\\n       configs/mask-rcnn_r50_fpn_1x_coco.py \\\n       checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth \\\n       8 \\\n       --out results.pkl\n```\n\n----------------------------------------\n\nTITLE: Using Polynomial Learning Rate Scheduling in MMDetection\nDESCRIPTION: This snippet shows how to configure a polynomial learning rate scheduler in MMDetection using `PolyLR`, specifying the power, minimum learning rate, and training epochs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nparam_scheduler = [\n    dict(\n        type='PolyLR',\n        power=0.9,\n        eta_min=1e-4,\n        begin=0,\n        end=8,\n        by_epoch=True)]\n\n```\n\n----------------------------------------\n\nTITLE: Testing Faster R-CNN with score threshold (Shell)\nDESCRIPTION: This command tests a Faster R-CNN model and filters predictions with a score threshold of 0.3, saving the results to the 'results/' directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       result.pkl \\\n       results \\\n       --show-score-thr 0.3\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Faster RCNN model with a RegNetX-1.6GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-1.6GF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-1_20210526_095325.log.json)\n```\n\n----------------------------------------\n\nTITLE: Configuration File Path for RTMDet-tiny\nDESCRIPTION: This snippet provides the relative path to the configuration file for the RTMDet-tiny model. This configuration file likely contains the model architecture, training parameters, and other settings required to train or evaluate the model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n[config](./rtmdet_tiny_8xb32-300e_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Import PAFPN Neck Module (Python)\nDESCRIPTION: This code shows how to import the newly created PAFPN neck module into the MMDetection framework. It can be done either by adding a line to `mmdet/models/necks/__init__.py` or using a custom imports dictionary in the configuration file, avoiding direct modifications to the original codebase.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_models.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom .pafpn import PAFPN\n```\n\n----------------------------------------\n\nTITLE: Training QDTrack MOT Model on Multi-GPU\nDESCRIPTION: This example shows how to train the QDTrack MOT model on multiple GPUs (8 in this case) using the `tools/dist_train.sh` script. It specifies the QDTrack configuration file and the number of GPUs to use.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_6\n\nLANGUAGE: shell脚本\nCODE:\n```\nbash ./tools/dist_train.sh configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py 8\n```\n\n----------------------------------------\n\nTITLE: Cascade RPN BibTeX Citation\nDESCRIPTION: This BibTeX entry provides the citation information for the Cascade RPN paper, including the title, authors, conference, and year of publication. It is used for referencing the Cascade RPN algorithm in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/cascade_rpn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{vu2019cascade,\n  title={Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution},\n  author={Vu, Thang and Jang, Hyunjun and Pham, Trung X and Yoo, Chang D},\n  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Inference with Numpy Array using DetInferencer\nDESCRIPTION: This snippet shows how to perform object detection inference using the DetInferencer with a numpy array representing an image as input. The image is loaded using mmcv.imread and passed to the inferencer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport mmcv\narray = mmcv.imread('demo/demo.jpg')\ninferencer(array)\n```\n\n----------------------------------------\n\nTITLE: Converting Cityscapes Annotations to COCO Format\nDESCRIPTION: Converts the Cityscapes dataset annotations to the COCO annotation format using the provided script.  It requires the cityscapesscripts package to be installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/new_model.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install cityscapesscripts\npython tools/dataset_converters/cityscapes.py ./data/cityscapes --nproc 8 --out-dir ./data/cityscapes/annotations\n```\n\n----------------------------------------\n\nTITLE: Plotting classification loss (Shell)\nDESCRIPTION: This command plots the classification loss curve from a log file named 'log.json', labeling the legend as 'loss_cls'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_logs.py plot_curve log.json --keys loss_cls --legend loss_cls\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Optimizer in MMDetection\nDESCRIPTION: This snippet illustrates how to define a custom optimizer named `MyOptimizer` within the MMDetection framework. It involves registering the optimizer using the `OPTIMIZERS` registry.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.registry import OPTIMIZERS\nfrom torch.optim import Optimizer\n\n\n@OPTIMIZERS.register_module()\nclass MyOptimizer(Optimizer):\n\n    def __init__(self, a, b, c)\n\n```\n\n----------------------------------------\n\nTITLE: CPU Training Configuration (Shell)\nDESCRIPTION: This shell command disables the use of GPUs by setting the `CUDA_VISIBLE_DEVICES` environment variable to -1. This forces MMDetection to use the CPU for training. This is mainly used for debugging purposes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/train.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport CUDA_VISIBLE_DEVICES=-1\n```\n\n----------------------------------------\n\nTITLE: Customizing Optimizer with ADAM in MMDetection\nDESCRIPTION: This code snippet demonstrates how to customize the optimizer using ADAM in MMDetection. It shows the minimal configuration required to switch to a different optimizer.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='Adam', lr=0.0003, weight_decay=0.0001))\n\n```\n\n----------------------------------------\n\nTITLE: Download COCO2014 Dataset in MMDetection (Python)\nDESCRIPTION: This snippet downloads and unzips the COCO2014 dataset using a Python script provided in the MMDetection tools directory. The dataset will be downloaded to the `data/coco` directory in the current path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/dataset_prepare.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/download_dataset.py --dataset-name coco2014 --unzip\n```\n\n----------------------------------------\n\nTITLE: MMDetection Config Name Convention\nDESCRIPTION: This code snippet illustrates the naming convention for configuration files in MMDetection V2.0. It defines the structure as [model]_(model setting)_[backbone]_[neck]_(norm setting)_(misc)_(gpu x batch)_[schedule]_[dataset].py, and mentions that `misc` includes DCN and GCBlock.  More details can be found in the config documentation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/compatibility.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n[model]_(model setting)_[backbone]_[neck]_(norm setting)_(misc)_(gpu x batch)_[schedule]_[dataset].py\n```\n\n----------------------------------------\n\nTITLE: Log Printing Interval Configuration (2.x)\nDESCRIPTION: This snippet configures the log printing interval in MMDetection 2.x. The `interval` parameter specifies how often logs are printed (in iterations).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nlog_config = dict(interval=50)\n```\n\n----------------------------------------\n\nTITLE: Converting MMDetection model to TorchServe format (Shell)\nDESCRIPTION: This script converts an MMDetection model to the TorchServe format. It requires a config file, checkpoint file, output folder, and model name as arguments.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython tools/deployment/mmdet2torchserve.py ${CONFIG_FILE} ${CHECKPOINT_FILE} \\\n--output-folder ${MODEL_STORE} \\\n--model-name ${MODEL_NAME}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Optimizer in MMDetection\nDESCRIPTION: This snippet shows how to configure the custom optimizer `MyOptimizer` within the `optim_wrapper` configuration in MMDetection, including its specific parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='MyOptimizer', a=a_value, b=b_value, c=c_value))\n\n```\n\n----------------------------------------\n\nTITLE: Initializing with a pretrained model\nDESCRIPTION: This snippet demonstrates initializing a model with weights from a pretrained model, specifically ResNet50 from torchvision. The `checkpoint` key specifies the path or identifier of the pretrained model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ninit_cfg = dict(type='Pretrained',\n                checkpoint='torchvision://resnet50')\n```\n\n----------------------------------------\n\nTITLE: Example output from get_flops.py\nDESCRIPTION: This is an example of the output generated by the get_flops.py script, showing the input shape, FLOPs, and the number of parameters of the model.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_27\n\nLANGUAGE: text\nCODE:\n```\n==============================\nInput shape: (3, 1280, 800)\nFlops: 239.32 GFLOPs\nParams: 37.74 M\n==============================\n```\n\n----------------------------------------\n\nTITLE: Citing ResNeSt with BibTeX\nDESCRIPTION: This BibTeX entry is used to cite the ResNeSt paper, which is the backbone architecture used in the Cascade Mask R-CNN models. It includes the title, authors, journal, and year of publication for proper academic citation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_18\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhang2020resnest,\ntitle={ResNeSt: Split-Attention Networks},\nauthor={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Zhang, Zhi and Lin, Haibin and Sun, Yue and He, Tong and Muller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},\njournal={arXiv preprint arXiv:2004.08955},\nyear={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration (3.x Max Keep)\nDESCRIPTION: This code snippet illustrates how to configure the maximum number of checkpoints to keep in MMDetection 3.x. It sets `max_keep_ckpts` to 3 within the `CheckpointHook` in `default_hooks`, meaning only the latest 3 checkpoints will be saved. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndefault_hooks = dict(\n    checkpoint=dict(\n        type='CheckpointHook',\n        max_keep_ckpts=3))\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Mask RCNN model with a RegNetX-400MF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-400MF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-400MF_fpn_mstrain-poly_3x_coco_20210601_235443.log.json)\n```\n\n----------------------------------------\n\nTITLE: Analyzing COCO segmentation error (Shell)\nDESCRIPTION: This command analyzes COCO segmentation error results and saves the analysis images to a specified directory, using an annotation file and specifying the analysis type as 'segm'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_error_analysis.py \\\n       results.segm.json \\\n       results \\\n       --ann=data/coco/annotations/instances_val2017.json \\\n       --types='segm'\n```\n\n----------------------------------------\n\nTITLE: Installing X-Decoder from Wheel Package\nDESCRIPTION: This command installs the mmdet[multimodal] package using mim, a package management tool within the MMDetection ecosystem.  This provides a convenient way to install the X-Decoder model and its dependencies. It assumes that the wheel package for multimodal support is available.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmim install mmdet[multimodal]\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Directory Structure\nDESCRIPTION: This snippet shows the expected directory structure for the COCO dataset when used with the HTC algorithm. It lists the subdirectories for annotations, training images, validation images, test images, and stuffthingmaps.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/htc/README.md#_snippet_0\n\nLANGUAGE: None\nCODE:\n```\nmmdetection\n├── mmdet\n├── tools\n├── configs\n├── data\n│   ├── coco\n│   │   ├── annotations\n│   │   ├── train2017\n│   │   ├── val2017\n│   │   ├── test2017\n|   |   ├── stuffthingmaps\n```\n\n----------------------------------------\n\nTITLE: Converting Detectron ResNet to PyTorch\nDESCRIPTION: This command converts keys in the original detectron pretrained ResNet models to PyTorch style, requiring source, destination and model depth.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/detectron2pytorch.py ${SRC} ${DST} ${DEPTH} [-h]\n```\n\n----------------------------------------\n\nTITLE: COCO Panoptic Annotation Format\nDESCRIPTION: This code snippet shows the structure of the COCO Panoptic annotation JSON file, including 'images', 'annotations', and 'categories' keys. The 'annotations' contain information about segments, their categories, and bounding boxes. Note that newlines are escaped with '\\n'.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_dataset.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n'images': [\n    {\n        'file_name': '000000001268.jpg',\n        'height': 427,\n        'width': 640,\n        'id': 1268\n    },\n    ...\n]\n\n'annotations': [\n    {\n        'filename': '000000001268.jpg',\n        'image_id': 1268,\n        'segments_info': [\n            {\n                'id':8345037,  # One-to-one correspondence with the id in the annotation map.\n                'category_id': 51,\n                'iscrowd': 0,\n                'bbox': (x1, y1, w, h),  # The bbox of the background is the outer rectangle of its mask.\n                'area': 24315\n            },\n            ...\n        ]\n    },\n    ...\n]\n\n'categories': [  # including both foreground categories and background categories\n    {'id': 0, 'name': 'person'},\n    ...\n ]\n```\n\n----------------------------------------\n\nTITLE: Testing Faster R-CNN and visualizing results (Shell)\nDESCRIPTION: This command tests a Faster R-CNN model, visualizes the results, and saves the images to the 'results/' directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/analyze_results.py \\\n       configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n       result.pkl \\\n       results \\\n       --show\n```\n\n----------------------------------------\n\nTITLE: Install MMEngine and MMCV with MIM\nDESCRIPTION: This snippet installs MMEngine and MMCV using the MIM package manager. MIM simplifies the installation of OpenMMLab packages by resolving dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npip install -U openmim\nmim install mmengine\nmim install \"mmcv>=2.0.0\"\n```\n\n----------------------------------------\n\nTITLE: Citation for FreeAnchor Algorithm\nDESCRIPTION: This LaTeX snippet provides the BibTeX entry for citing the FreeAnchor paper. It includes the title, authors, booktitle (Neural Information Processing Systems), and year of publication (2019). This is used to give credit to the original authors of the FreeAnchor algorithm.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/free_anchor/README.md#_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{zhang2019freeanchor,\n  title   =  {{FreeAnchor}: Learning to Match Anchors for Visual Object Detection},\n  author  =  {Zhang, Xiaosong and Wan, Fang and Liu, Chang and Ji, Rongrong and Ye, Qixiang},\n  booktitle =  {Neural Information Processing Systems},\n  year    =  {2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow GPU using pip\nDESCRIPTION: This command installs TensorFlow GPU version 2.6.0 using pip. This specific version is required for compatibility with the model conversion script.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/EfficientDet/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install tensorflow-gpu==2.6.0\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration (2.x Interval)\nDESCRIPTION: This code snippet shows how to configure the checkpoint saving interval in MMDetection 2.x. It sets the interval to 1, meaning a checkpoint is saved every epoch. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_config = dict(\n    interval=1)\n```\n\n----------------------------------------\n\nTITLE: Testing TorchServe deployment (Shell)\nDESCRIPTION: This command tests the TorchServe deployment by sending an image to the server and retrieving the prediction results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/3dogs.jpg\ncurl http://127.0.0.1:8080/predictions/${MODEL_NAME} -T 3dogs.jpg\n```\n\n----------------------------------------\n\nTITLE: EfficientNet Citation (LaTeX)\nDESCRIPTION: This LaTeX snippet provides the citation information for the EfficientNet paper. It includes the title, author, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/efficientnet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{tan2019efficientnet,\n  title={Efficientnet: Rethinking model scaling for convolutional neural networks},\n  author={Tan, Mingxing and Le, Quoc V},\n  journal={arXiv preprint arXiv:1905.11946},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Image Region Proposals (Python)\nDESCRIPTION: This snippet configures the `val_dataloader` and `test_dataloader` to use `DumpProposals` for extracting region proposals from the training set. It specifies the output directory and the proposals file name.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fast_rcnn/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# For training set\nval_dataloader = dict(\n    dataset=dict(\n        ann_file='data/coco/annotations/instances_train2017.json',\n        data_prefix=dict(img='val2017/')))\nval_dataloader = dict(\n    _delete_=True,\n    type='DumpProposals',\n    output_dir='data/coco/proposals/',\n    proposals_file='rpn_r50_fpn_1x_train2017.pkl')\ntest_dataloader = val_dataloader\ntest_evaluator = val_dataloader\n```\n\n----------------------------------------\n\nTITLE: Install Required Python Packages\nDESCRIPTION: This snippet lists the necessary Python packages to install for this project to work. Ensure that mmengine is at least version 0.9.0 and that both deepspeed and fairscale are installed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_largemodel/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmmengine >=0.9.0 # Example 1\ndeepspeed # Example 2\nfairscale # Example 2\n```\n\n----------------------------------------\n\nTITLE: Testing with Weather Corruptions\nDESCRIPTION: This shell command assesses model performance under weather-related image corruptions. The `test_robustness.py` script is used, with the configuration and checkpoint files, as well as optional arguments for output and evaluation. The `--corruptions weather` argument specifies the application of weather-based image corruptions.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/robustness_benchmarking.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# wetaher\npython tools/analysis_tools/test_robustness.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [--out ${RESULT_FILE}] [--eval ${EVAL_METRICS}] --corruptions weather\n```\n\n----------------------------------------\n\nTITLE: Citation for GRoIE in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the GRoIE paper. Use this snippet when referencing the GRoIE layer in academic publications or research reports.  It includes the title, authors, conference, pages, year, and organization details.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/groie/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{rossi2021novel,\n  title={A novel region of interest extraction layer for instance segmentation},\n  author={Rossi, Leonardo and Karimi, Akbar and Prati, Andrea},\n  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},\n  pages={2203--2209},\n  year={2021},\n  organization={IEEE}\n}\n```\n\n----------------------------------------\n\nTITLE: Using AvoidCUDAOOM to Handle GPU OOM Errors in MMDetection\nDESCRIPTION: This snippet demonstrates how to use `AvoidCUDAOOM` to handle GPU out-of-memory (OOM) errors during MMDetection training.  It provides both a direct function call and a decorator approach. `AvoidCUDAOOM` first attempts to clear the CUDA cache, then tries FP16 conversion, and finally moves computation to the CPU.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom mmdet.utils import AvoidCUDAOOM\n\noutput = AvoidCUDAOOM.retry_if_cuda_oom(some_function)(input1, input2)\n```\n\n----------------------------------------\n\nTITLE: Cascade R-CNN S50 Model Download\nDESCRIPTION: Download link for the pre-trained weights of a Cascade R-CNN model using a ResNeSt-50 backbone and FPN within the MMDetection framework. These weights are useful for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/resnest/cascade_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/cascade_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco_20201122_213640-763cc7b5.pth)\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Hook with custom_imports in MMDetection\nDESCRIPTION: This snippet demonstrates how to use `custom_imports` in the configuration file to import the custom hook module.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncustom_imports = dict(imports=['mmdet.engine.hooks.my_hook'], allow_failed_imports=False)\n\n```\n\n----------------------------------------\n\nTITLE: Installing TorchServe using pip\nDESCRIPTION: This command installs TorchServe and its dependencies using pip. TorchServe is used for serving PyTorch models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install torchserve torch-model-archiver torch-workflow-archiver nvgpu\n```\n\n----------------------------------------\n\nTITLE: Configuring NumClassCheckHook in MMDetection\nDESCRIPTION: This code snippet shows how to configure a built-in hook `NumClassCheckHook` to check the consistency of the number of classes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [dict(type='NumClassCheckHook')]\n\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX Entry\nDESCRIPTION: This is the BibTeX entry for citing the FoveaBox paper. It includes the title, authors, journal, and year of publication. This information is useful for academic publications that reference FoveaBox.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/foveabox/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{kong2019foveabox,\n  title={FoveaBox: Beyond Anchor-based Object Detector},\n  author={Kong, Tao and Sun, Fuchun and Liu, Huaping and Jiang, Yuning and Shi, Jianbo},\n  journal={arXiv preprint arXiv:1904.03797},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Verify MMDetection Installation (Python)\nDESCRIPTION: This snippet imports the `mmdet` module and prints its version to verify a successful installation. It checks if the module is importable and if the version attribute is accessible.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/get_started.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport mmdet\nprint(mmdet.__version__)\n# Example output: 3.0.0, or an another version.\n```\n\n----------------------------------------\n\nTITLE: Install MMEngine without MIM\nDESCRIPTION: This snippet installs MMEngine directly using pip, bypassing MIM. This is an alternative approach to installing MMEngine if MIM is not desired.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\npip install mmengine\n```\n\n----------------------------------------\n\nTITLE: Importing a Custom Optimizer in MMDetection Init\nDESCRIPTION: This code shows how to import the custom optimizer in the `__init__.py` file, making it accessible to the MMDetection registry.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom .my_optimizer import MyOptimizer\n\n```\n\n----------------------------------------\n\nTITLE: Cascade R-CNN S101 Training Log\nDESCRIPTION: Download URL for the training log in JSON format of a Cascade R-CNN model with a ResNeSt-101 backbone. This log contains details such as losses, metrics, and training parameters.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/resnest/README.md#_snippet_17\n\nLANGUAGE: JSON\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/resnest/cascade_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco/cascade_rcnn_s50_fpn_syncbn-backbone%2Bhead_mstrain-range_1x_coco-20201122_213640.log.json)\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the SoftTeacher paper, including the title, authors, journal, and year of publication. It is intended for use in academic publications or research reports to properly attribute the work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/soft_teacher/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{xu2021end,\n  title={End-to-End Semi-Supervised Object Detection with Soft Teacher},\n  author={Xu, Mengde and Zhang, Zheng and Hu, Han and Wang, Jianfeng and Wang, Lijuan and Wei, Fangyun and Bai, Xiang and Liu, Zicheng},\n  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Faster RCNN model with a RegNetX-1.6GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/faster_rcnn_regnetx-1.6GF_fpn_mstrain_3x_coco/faster_rcnn_regnetx-1_20210526_095325-94aa46cc.pth)\n```\n\n----------------------------------------\n\nTITLE: RandomFlip Config (MMDetection)\nDESCRIPTION: This code snippet demonstrates the configuration for the RandomFlip transform in both MMDetection 2.x and 3.x. The key change is the parameter name: `flip_ratio` in 2.x is replaced with `prob` in 3.x.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndict(type='RandomFlip',\n     flip_ratio=0.5)\n```\n\nLANGUAGE: python\nCODE:\n```\ndict(type='RandomFlip',\n     prob=0.5)\n```\n\n----------------------------------------\n\nTITLE: Importing a Custom Hook in MMDetection Init\nDESCRIPTION: This code shows how to import the custom hook in the `__init__.py` file, making it accessible to the MMDetection registry.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom .my_hook import MyHook\n\n```\n\n----------------------------------------\n\nTITLE: Download and Unzip Cat Dataset\nDESCRIPTION: This snippet downloads the cat dataset from a specified URL and unzips it. This provides the images that will be used for semi-automated labeling.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncd path/to/mmetection\nmkdir data && cd data\n\nwget https://download.openmmlab.com/mmyolo/data/cat_dataset.zip && unzip cat_dataset.zip\n```\n\n----------------------------------------\n\nTITLE: Normalize and Pad Config in MMDetection 2.x\nDESCRIPTION: This code snippet demonstrates the image normalization and padding configuration in MMDetection 2.x. It defines the image normalization parameters (mean, std, to_rgb) and utilizes 'Normalize' and 'Pad' modules within the pipeline.  The image is padded to multiples of 32.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/migration/config_migration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Image normalization parameters\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53],\n    std=[58.395, 57.12, 57.375],\n    to_rgb=True)\npipeline=[\n    ...,\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),  # Padding the image to multiples of 32\n    ...\n]\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Cascade Mask RCNN model using a RegNetX-400MF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_31\n\nLANGUAGE: text\nCODE:\n```\n[config](./cascade-mask-rcnn_regnetx-400MF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Citing Grid R-CNN Plus in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Grid R-CNN Plus technical report. It defines a bibitem entry that can be used in a LaTeX document to cite the arXiv preprint of the improved version.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grid_rcnn/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{lu2019grid,\n  title={Grid R-CNN Plus: Faster and Better},\n  author={Lu, Xin and Li, Buyu and Yue, Yuxin and Li, Quanquan and Yan, Junjie},\n  journal={arXiv preprint arXiv:1906.05688},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying CUDA Availability in PyTorch\nDESCRIPTION: Demonstrates how to check if CUDA is properly installed and available for use in PyTorch, using a Python command.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import torch; print(torch.cuda.is_available())'\n```\n\n----------------------------------------\n\nTITLE: Install MMDetection in Google Colab\nDESCRIPTION: These commands clone the MMDetection repository and install it in editable mode in a Google Colab environment.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\n!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection\n!pip install -e .\n```\n\n----------------------------------------\n\nTITLE: Verify MMDetection installation in Google Colab\nDESCRIPTION: This snippet verifies the MMDetection installation by importing the mmdet module and printing its version.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport mmdet\nprint(mmdet.__version__)\n```\n\n----------------------------------------\n\nTITLE: ViTDet Citation (LaTeX)\nDESCRIPTION: This LaTeX code provides the citation information for the ViTDet paper. It includes the title, authors, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/ViTDet/README.md#_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n@article{li2022exploring,\n  title={Exploring plain vision transformer backbones for object detection},\n  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},\n  journal={arXiv preprint arXiv:2203.16527},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for Double Heads\nDESCRIPTION: This is the LaTeX citation for the \"Rethinking Classification and Localization for Object Detection\" paper, introducing the Double Heads method. It provides the necessary information for academic referencing, including authors, title, year, and arXiv identifier.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/double_heads/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{wu2019rethinking,\n    title={Rethinking Classification and Localization for Object Detection},\n    author={Yue Wu and Yinpeng Chen and Lu Yuan and Zicheng Liu and Lijuan Wang and Hongzhi Li and Yun Fu},\n    year={2019},\n    eprint={1904.06493},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Running MOT Error Visualization (Shell)\nDESCRIPTION: This snippet demonstrates how to run the `mot_error_visualize.py` script for visualizing errors in multi-object tracking results.  It requires specifying the configuration file (`CONFIG_FILE`), the input file (`INPUT`), and the directory containing the tracking results (`RESULT_DIR`). Optional arguments include output directory (`OUTPUT`), frames per second (`FPS`), and backend (`BACKEND`).\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_analysis_tools.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/mot/mot_error_visualize.py \\\n    ${CONFIG_FILE}\\\n    --input ${INPUT} \\\n    --result-dir ${RESULT_DIR} \\\n    [--output-dir ${OUTPUT}] \\\n    [--fps ${FPS}] \\\n    [--show] \\\n    [--backend ${BACKEND}]\n```\n\n----------------------------------------\n\nTITLE: Installing Roboflow Python Package\nDESCRIPTION: This snippet shows how to install the Roboflow Python package using pip.  This package provides necessary tools and functions to interact with the Roboflow API and download datasets.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npip install roboflow\n```\n\n----------------------------------------\n\nTITLE: Defining FooModel with init_cfg\nDESCRIPTION: This snippet defines a custom model named `FooModel` inheriting from `BaseModule` and initializes it using `init_cfg`. The `init_cfg` parameter allows users to specify initialization settings for the model's layers or submodules.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule\n\nclass FooModel(BaseModule):\n\tdef __init__(self,\n                 arg1,\n                 arg2,\n                 init_cfg=None):\n    \tsuper(FooModel, self).__init__(init_cfg)\n\t\t...\n```\n\n----------------------------------------\n\nTITLE: Converting SSD Model Weights with Python\nDESCRIPTION: This script converts older version SSD model weights to be compatible with newer versions by refactoring the SSD neck and head. It loads the old model, converts the weights, and saves the converted model to a new file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ssd/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tools/model_converters/upgrade_ssd_version.py ${OLD_MODEL_PATH} ${NEW_MODEL_PATH}\n```\n\n----------------------------------------\n\nTITLE: Faster RCNN Config Path\nDESCRIPTION: This snippet defines the relative path to the configuration file for a Faster RCNN model using a RegNetX-800MF-FPN backbone. This configuration is used for training and inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n[config](./faster-rcnn_regnetx-800MF_fpn_ms-3x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX for NAS-FCOS\nDESCRIPTION: This LaTeX snippet provides the citation information for the NAS-FCOS paper. It includes the title, authors, journal, year, and arXiv preprint number, allowing for easy inclusion of the citation in academic documents.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/nas_fcos/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{wang2019fcos,\n  title={Nas-fcos: Fast neural architecture search for object detection},\n  author={Wang, Ning and Gao, Yang and Chen, Hao and Wang, Peng and Tian, Zhi and Shen, Chunhua},\n  journal={arXiv preprint arXiv:1906.04423},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Converting RegNet model to MMDetection style\nDESCRIPTION: This command converts keys in pycls pretrained RegNet models to MMDetection style.  It takes source and destination paths for the models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\npython tools/model_converters/regnet2mmdet.py ${SRC} ${DST} [-h]\n```\n\n----------------------------------------\n\nTITLE: Configuring Memory Profiler Hook in MMDetection\nDESCRIPTION: This snippet demonstrates how to configure the `MemoryProfilerHook` in MMDetection to monitor memory usage during training. The hook is added to the `custom_hooks` list and specifies the logging interval.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/changelog_v2.x.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MemoryProfilerHook', interval=50)\n]\n```\n\n----------------------------------------\n\nTITLE: FCOS Citation LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the FCOS paper, which should be used when referencing the FCOS algorithm in academic publications or technical reports. This is the standard way to cite the FCOS research work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/fcos/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{tian2019fcos,\n  title={FCOS: Fully Convolutional One-Stage Object Detection},\n  author={Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  journal={arXiv preprint arXiv:1904.01355},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SGD Optimizer in MMDetection\nDESCRIPTION: This code shows how to specify the SGD optimizer within the `optim_wrapper` configuration in MMDetection, including settings for learning rate, momentum, and weight decay.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001))\n\n```\n\n----------------------------------------\n\nTITLE: Setting Hook Priority in MMDetection\nDESCRIPTION: This code snippet demonstrates how to set the priority of a custom hook in MMDetection using the `priority` key.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_runtime.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncustom_hooks = [\n    dict(type='MyHook', a=a_value, b=b_value, priority='NORMAL')\n]\n\n```\n\n----------------------------------------\n\nTITLE: Citation of Deformable ConvNets v2 (DCNv2) in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Deformable ConvNets v2 paper, including title, authors, journal, and year.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dcnv2/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhu2018deformable,\n  title={Deformable ConvNets v2: More Deformable, Better Results},\n  author={Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},\n  journal={arXiv preprint arXiv:1811.11168},\n  year={2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Image Captioning Evaluation\nDESCRIPTION: This shell command evaluates the XDecoder model for image captioning on the COCO2014 dataset.  It uses the `dist_test.sh` script with the specified configuration file and checkpoint file, distributing the evaluation across 8 processes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_caption_coco2014.py xdecoder_focalt_last_novg.pt 8\n```\n\n----------------------------------------\n\nTITLE: Cityscapes JSON Conversion\nDESCRIPTION: This script converts the Cityscapes dataset to the required JSON format for MMDetection. It requires the Cityscapes dataset to be downloaded and extracted to the `data/cityscapes/` folder. The script `cityscapes.py` is located in the `tools/dataset_converters/` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/cityscapes.py data/cityscapes/\n```\n\n----------------------------------------\n\nTITLE: Citation (Faster R-CNN) - LaTeX\nDESCRIPTION: This is a BibTeX entry for citing the original Faster R-CNN paper. It includes details such as authors, title, journal, and publication year for proper academic referencing.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_project/README.md#_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\n@article{Ren_2017,\n   title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},\n   year={2017},\n   month={Jun},\n}\n```\n\n----------------------------------------\n\nTITLE: Install PyTorch with Conda (CPU)\nDESCRIPTION: This command installs PyTorch and torchvision for CPU-only systems using conda. It avoids the installation of CUDA-related dependencies.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/get_started.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nconda install pytorch torchvision cpuonly -c pytorch\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Testing with TTA\nDESCRIPTION: This shell script executes the distributed testing script with TTA enabled, utilizing multiple GPUs for faster testing. The `--tta` flag activates the test-time augmentation.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/test.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\n# 多 GPU 测试\nbash tools/dist_test.sh \\\n    ${CONFIG_FILE} \\\n    ${CHECKPOINT_FILE} \\\n    ${GPU_NUM} \\\n    [--tta]\n```\n\n----------------------------------------\n\nTITLE: Extracting COCO Data from Mixed Annotations (MDETR)\nDESCRIPTION: This script extracts the COCO portion of the data from the `final_mixed_train.json` file, which is part of the MDetr dataset preparation process. It creates a new JSON file containing only COCO annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/extract_coco_from_mixed.py data/coco/mdetr_annotations/final_mixed_train.json\n```\n\n----------------------------------------\n\nTITLE: Starting TorchServe (Shell)\nDESCRIPTION: This command starts the TorchServe server using the specified model store and model archive. The --ncs option enables the no-console mode.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ntorchserve --start --ncs \\\n  --model-store ${MODEL_STORE} \\\n  --models  ${MODEL_NAME}.mar\n```\n\n----------------------------------------\n\nTITLE: Citing YOLACT in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the YOLACT paper.  It includes the authors, title, booktitle (ICCV), and year of publication. This citation should be used when referencing YOLACT in academic work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/yolact/README.md#_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n@inproceedings{yolact-iccv2019,\n  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},\n  title     = {YOLACT: {Real-time} Instance Segmentation},\n  booktitle = {ICCV},\n  year      = {2019},\n}\n```\n\n----------------------------------------\n\nTITLE: WIDER FACE Citation (LaTeX)\nDESCRIPTION: This LaTeX snippet provides the citation information for the WIDER FACE dataset. It includes the authors, title, conference, and year of publication, allowing users to properly attribute the dataset in their research papers.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/wider_face/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{yang2016wider,\n   Author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},\n   Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n   Title = {WIDER FACE: A Face Detection Benchmark},\n   Year = {2016}\n}\n```\n\n----------------------------------------\n\nTITLE: Panoptic Segmentation Evaluation\nDESCRIPTION: This shell command is used to evaluate the XDecoder model for panoptic segmentation on the COCO2017 dataset. It utilizes the `dist_test.sh` script with the specified configuration file and checkpoint file, distributing the evaluation across 8 processes.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh projects/XDecoder/configs/xdecoder-tiny_zeroshot_open-vocab-panoptic_coco.py xdecoder_focalt_last_novg.pt 8\n```\n\n----------------------------------------\n\nTITLE: Stopping TorchServe (Shell)\nDESCRIPTION: This command stops the TorchServe server.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\ntorchserve --stop\n```\n\n----------------------------------------\n\nTITLE: Run Detic demo with existing vocabularies\nDESCRIPTION: This command runs the Detic demo script with pre-existing dataset vocabularies. It specifies the image path, configuration file path, model path, score threshold, and the dataset to use (LVIS). The `--show` argument enables displaying the inference results.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython demo.py \\\n  ${IMAGE_PATH} \\\n  ${CONFIG_PATH} \\\n  ${MODEL_PATH} \\\n  --show \\\n  --score-thr 0.5 \\\n  --dataset lvis\n```\n\n----------------------------------------\n\nTITLE: Setting Dynamo Cache Size Limit for Torch Compile\nDESCRIPTION: Illustrates how to set the `torch._dynamo.config.cache_size_limit` parameter using the `DYNAMO_CACHE_SIZE_LIMIT` environment variable to manage the compilation cache when using `torch.compile` with MMDetection.  This is recommended when input shape is fixed.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# 单卡\nexport DYNAMO_CACHE_SIZE_LIMIT = 4\npython tools/train.py configs/rtmdet/rtmdet_s_8xb32-300e_coco.py  --cfg-options compile=True\n\n# 单机 8 卡\nexport DYNAMO_CACHE_SIZE_LIMIT = 4\n./tools/dist_train.sh configs/rtmdet/rtmdet_s_8xb32-300e_coco.py 8 --cfg-options compile=True\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Loss Weight in Focal Loss\nDESCRIPTION: This configuration shows how to set the loss weight for Focal Loss to 0.5.  The loss weight controls the relative importance of this loss compared to other losses in a multi-task learning scenario.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/customize_losses.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nloss_cls=dict(\n    type='FocalLoss',\n    use_sigmoid=True,\n    gamma=2.0,\n    alpha=0.25,\n    loss_weight=0.5)\n```\n\n----------------------------------------\n\nTITLE: Example comparing TorchServe and PyTorch results (Shell)\nDESCRIPTION: This command provides an example of how to use the `test_torchserver.py` script to compare results and visualize differences, including example files, configurations, and model checkpoints.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\npython tools/deployment/test_torchserver.py \\\ndemo/demo.jpg \\\nconfigs/yolo/yolov3_d53_8xb8-320-273e_coco.py \\\ncheckpoint/yolov3_d53_320_273e_coco-421362b6.pth \\\nyolov3 \\\n--work-dir ./work-dir\n```\n\n----------------------------------------\n\nTITLE: Citation of Mask2Former in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation format for the Mask2Former paper. It includes the title, authors, journal, and year of publication. This is used to properly credit the original authors when using or referring to Mask2Former in academic work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mask2former_vis/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{cheng2021mask2former,\n  title={Masked-attention Mask Transformer for Universal Image Segmentation},\n  author={Bowen Cheng and Ishan Misra and Alexander G. Schwing and Alexander Kirillov and Rohit Girdhar},\n  journal={CVPR},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Cascade Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Cascade Mask RCNN model with a RegNetX-1.6GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_39\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/cascade_mask_rcnn_regnetx-1.6GF_fpn_mstrain_3x_coco/cascade_mask_rcnn_regnetx-1_20210715_211616.log.json)\n```\n\n----------------------------------------\n\nTITLE: Install Multimodal Dependencies\nDESCRIPTION: Installs the necessary dependencies for multimodal algorithms. The first command installs the dependencies listed in the `requirements/multimodal.txt` file. The second command installs the `mmdet[multimodal]` package, potentially from a wheel file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements/multimodal.txt\n```\n\nLANGUAGE: shell\nCODE:\n```\nmim install mmdet[multimodal]\n```\n\n----------------------------------------\n\nTITLE: Specify MMCV Dependency Version Range\nDESCRIPTION: Defines the acceptable version range for the mmcv dependency.  It requires mmcv to be greater than or equal to version 2.0.0rc4 but less than version 2.2.0. This ensures compatibility with specific features and bug fixes within that version range.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/requirements/mminstall.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmmcv>=2.0.0rc4,<2.2.0\n```\n\n----------------------------------------\n\nTITLE: Developing with multiple MMDetection versions\nDESCRIPTION: Shows an example shell script to remove the MMDetection code from the `PYTHONPATH` so that the environment installed default version is used rather than the current development version.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nPYTHONPATH=\"$(dirname $0)/..\":$PYTHONPATH\n```\n\n----------------------------------------\n\nTITLE: Installing Panoptic Segmentation Dependencies\nDESCRIPTION: Shows how to install the necessary dependencies for panoptic segmentation using pip and a direct GitHub repository link.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/cocodataset/panopticapi.git\n```\n\n----------------------------------------\n\nTITLE: GRIT-20M to ODVG Conversion\nDESCRIPTION: This command converts the processed GRIT-20M dataset into the ODVG format, which is required for training the MM-GDINO-T model. It uses the `grit2odvg.py` script from the `tools/dataset_converters` directory, specifying the directory containing the processed GRIT data (`data/grit_processed/`) as input. The script generates a new JSON file containing the converted annotations.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/grit2odvg.py data/grit_processed/\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX for RTMDet\nDESCRIPTION: This BibTeX entry provides the citation information for the RTMDet paper. It can be used to cite RTMDet in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/rtmdet/README.md#_snippet_9\n\nLANGUAGE: latex\nCODE:\n```\n@misc{lyu2022rtmdet,\n      title={RTMDet: An Empirical Study of Designing Real-Time Object Detectors},\n      author={Chengqi Lyu and Wenwei Zhang and Haian Huang and Yue Zhou and Yudong Wang and Yanyi Liu and Shilong Zhang and Kai Chen},\n      year={2022},\n      eprint={2212.07784},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Grid R-CNN (Original Paper) in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the original Grid R-CNN paper. It defines a bibitem entry that can be used in a LaTeX document to cite the paper correctly.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/grid_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{lu2019grid,\n  title={Grid r-cnn},\n  author={Lu, Xin and Li, Buyu and Yue, Yuxin and Li, Quanquan and Yan, Junjie},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Visualization Backends (Python)\nDESCRIPTION: This snippet demonstrates how to configure multiple visualization backends, such as LocalVisBackend, TensorboardVisBackend, and WandbVisBackend, to store data in various locations. This allows users to simultaneously log visualization results to local storage, TensorBoard, and Wandb for comprehensive analysis.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/visualization.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# https://mmengine.readthedocs.io/en/latest/api/visualization.html\n_base_.visualizer.vis_backends = [\n    dict(type='LocalVisBackend'), #\n    dict(type='TensorboardVisBackend'),\n    dict(type='WandbVisBackend'),]\n```\n\n----------------------------------------\n\nTITLE: Testing with Slurm Job Scheduler\nDESCRIPTION: This snippet demonstrates how to test a model using the Slurm job scheduler with MMDetection. It uses the `tools/slurm_test_tracking.sh` script, specifying the partition, job name, configuration file and any optional arguments. It shows how to execute test jobs managed by Slurm.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_16\n\nLANGUAGE: shell 脚本\nCODE:\n```\n[GPUS=${GPUS}] bash tools/slurm_test_tracking.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} [optional arguments]\n```\n\n----------------------------------------\n\nTITLE: Uninstalling mmcv-lite using pip\nDESCRIPTION: Shows how to uninstall `mmcv-lite` using pip to resolve module import errors when `mmcv` is required.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall mmcv-lite\n```\n\n----------------------------------------\n\nTITLE: LaTeX Citation for DyHead\nDESCRIPTION: This is the LaTeX citation for the DyHead paper, which can be used to reference the work in academic publications. It includes the authors, title, booktitle, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dyhead/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{DyHead_CVPR2021,\n  author    = {Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},\n  title     = {Dynamic Head: Unifying Object Detection Heads With Attentions},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year      = {2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Run Detic demo with custom vocabularies\nDESCRIPTION: This command executes the Detic demo script with a custom vocabulary provided through the `--class-name` argument. It allows the detector to recognize classes not present in the pre-defined datasets, by leveraging CLIP to associate text descriptions with visual features. The score threshold is set to 0.3.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/Detic/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython demo.py \\\n  ${IMAGE_PATH} \\\n  ${CONFIG_PATH} \\\n  ${MODEL_PATH} \\\n  --show \\\n  --score-thr 0.3 \\\n  --class-name headphone webcam paper coffe\n```\n\n----------------------------------------\n\nTITLE: LVIS Dataset Citation in LaTeX\nDESCRIPTION: This LaTeX code provides the citation information for the LVIS dataset. It includes the title, author, booktitle, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/lvis/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{gupta2019lvis,\n  title={{LVIS}: A Dataset for Large Vocabulary Instance Segmentation},\n  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},\n  booktitle={Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation of Deformable Convolutional Networks\nDESCRIPTION: This LaTeX snippet provides the citation information for the Deformable Convolutional Networks paper. It includes the title, authors, booktitle, and year of publication, which are required for properly attributing the work in academic contexts.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/dcn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{dai2017deformable,\n  title={Deformable Convolutional Networks},\n  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},\n  booktitle={Proceedings of the IEEE international conference on computer vision},\n  year={2017}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing FooModel with init_cfg in code\nDESCRIPTION: This example demonstrates initializing the `FooModel` directly in the code using `init_cfg`. This allows users to define initialization strategies within the model definition itself.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule\n# or directly inherit mmdet models\n\nclass FooModel(BaseModule)\n\tdef __init__(self,\n                  arg1,\n                  arg2,\n                  init_cfg=XXX):\n    \t\tsuper(FooModel, self).__init__(init_cfg)\n    \t    ...\n```\n\n----------------------------------------\n\nTITLE: Mask R-CNN with PISA Configuration (COCO Dataset)\nDESCRIPTION: This snippet provides the configuration file name for Mask R-CNN with PISA enabled, trained on the COCO dataset. It specifies the settings for training a Mask R-CNN model with a ResNet-50-FPN backbone, enhanced with the PISA sampling method to boost both object detection and instance segmentation performance.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pisa/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[config](./mask-rcnn_r50_fpn_pisa_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Testing SORT on MOTxx-test (Shell Script)\nDESCRIPTION: This shell script tests SORT on the MOTxx-test set to generate submission files for the MOT Challenge. The script utilizes `dist_test_tracking.sh`. The `--detector` argument provides the path to the trained detector model. The results will be saved in the `./mot_17_test_res` directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/sort/README.md#_snippet_3\n\nLANGUAGE: shell script\nCODE:\n```\nbash tools/dist_test_tracking.sh configs/sort/sort_faster-rcnn_r50_fpn_8xb2-4e_mot17train_test-mot17test.py 8 --detector ${DETECTOR_CHECKPOINT_PATH}\n```\n\n----------------------------------------\n\nTITLE: Converting GoldG data to ODVG format\nDESCRIPTION: This snippet converts the extracted COCO data (in GoldG format) to the ODVG (Object Detection with Visual Grounding) format.  The script `goldg2odvg.py` is used to process the extracted COCO annotations and create `final_mixed_train_only_coco_vg.json` which is used for training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/dataset_converters/goldg2odvg.py data/coco/mdetr_annotations/final_mixed_train_only_coco.json\n```\n\n----------------------------------------\n\nTITLE: Install Gradio Package\nDESCRIPTION: This command installs the Gradio package with a version greater than or equal to 3.31.0. Gradio is used to create the interactive web interface for the MMDetection demo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/gradio_demo/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install \"gradio>=3.31.0\"\n```\n\n----------------------------------------\n\nTITLE: Citation of CenterNet in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the CenterNet paper, including the title, authors, journal, and year of publication. It is used for referencing the original CenterNet paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/centernet/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhou2019objects,\n  title={Objects as Points},\n  author={Zhou, Xingyi and Wang, Dequan and Kr{\"a}henb{\"u}hl, Philipp},\n  booktitle={arXiv preprint arXiv:1904.07850},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: SOLO Citation (BibTeX)\nDESCRIPTION: This is the BibTeX entry for citing the SOLO paper in academic publications. It includes the title, authors, conference, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/solo/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{wang2020solo,\n  title     =  {{SOLO}: Segmenting Objects by Locations},\n  author    =  {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying Config Keys of Dict Chains - Shell\nDESCRIPTION: This command demonstrates how to modify configuration keys within nested dictionaries using the `--cfg-options` argument. It specifically changes the `norm_eval` parameter in the model's backbone to `False`, which affects the training mode of BatchNorm modules.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/tracking_config.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n--cfg-options model.detector.backbone.norm_eval=False\n```\n\n----------------------------------------\n\nTITLE: Inference with mot_demo.py\nDESCRIPTION: This command performs inference on a video using a single GPU and saves the output as a video. It specifies the input video, configuration file, checkpoint path, and output video path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/qdtrack/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython demo/mot_demo.py demo/demo_mot.mp4 configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py --checkpoint ${CHECKPOINT_PATH} --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Citation of AutoAssign in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the AutoAssign object detection algorithm. It includes the title, authors, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/autoassign/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{zhu2020autoassign,\n  title={AutoAssign: Differentiable Label Assignment for Dense Object Detection},\n  author={Zhu, Benjin and Wang, Jianfeng and Jiang, Zhengkai and Zong, Fuhang and Liu, Songtao and Li, Zeming and Sun, Jian},\n  journal={arXiv preprint arXiv:2007.03496},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Model Download Link\nDESCRIPTION: This snippet provides the URL to download the pre-trained weights for a Mask RCNN model with a RegNetX-3.2GF-FPN backbone, trained on the COCO dataset. These weights can be used for fine-tuning or direct inference.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_26\n\nLANGUAGE: text\nCODE:\n```\n[model](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco/mask_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco_20200521_202221-99879813.pth)\n```\n\n----------------------------------------\n\nTITLE: Running Image Captioning Demo (X-Decoder)\nDESCRIPTION: This command runs the demo script for image captioning using the X-Decoder model. It specifies the image, configuration file, and pre-trained weights. It requires the X-Decoder weights to be downloaded and the images to be placed in the correct directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/XDecoder/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd projects/XDecoder\npython demo.py ../../images/penguin.jpeg configs/xdecoder-tiny_zeroshot_caption_coco2014.py --weights ../../xdecoder_focalt_last_novg.pt\n```\n\n----------------------------------------\n\nTITLE: Configuration File Naming in v2.25.0 and Later (Mask2Former)\nDESCRIPTION: From MMDetection v2.25.0 onwards, configuration files named 'mask2former_xxx_coco.py' represent instance segmentation models, while 'mask2former_xxx_coco-panoptic.py' represents panoptic segmentation models.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/compatibility.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n'mask2former_xxx_coco.py' 代表实例分割的配置文件\n'mask2former_xxx_coco-panoptic.py' 代表全景分割的配置文件\n```\n\n----------------------------------------\n\nTITLE: Citation for PVT\nDESCRIPTION: This is the BibTeX entry for citing the original Pyramid Vision Transformer (PVT) paper.  It includes the title, authors, journal, year, and other relevant information for proper attribution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pvt/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{wang2021pyramid,\n  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},\n  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},\n  journal={arXiv preprint arXiv:2102.12122},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: BBoxHead Loss Function in MMDetection (Python)\nDESCRIPTION: This snippet illustrates the structure of the `loss` function within a `BBoxHead` module. It computes classification loss (`loss_cls`), classification accuracy (`acc`), and bounding box loss (`loss_bbox`). The returned dictionary contains these losses, but only keys containing 'loss' are used for backpropagation. The `accuracy` is being used as an evaluation metric.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/advanced_guides/conventions.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass BBoxHead(nn.Module):\n    ...\n    def loss(self, ...):\n        losses = dict()\n        # 分类损失\n        losses['loss_cls'] = self.loss_cls(...)\n        # 分类准确率\n        losses['acc'] = accuracy(...)\n        # 边界框损失\n        losses['loss_bbox'] = self.loss_bbox(...)\n        return losses\n```\n\n----------------------------------------\n\nTITLE: Citation of QueryInst\nDESCRIPTION: This LaTeX snippet provides the citation information for the QueryInst paper, including the authors, title, conference, and year of publication. It's used for properly attributing the work in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/queryinst/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@InProceedings{Fang_2021_ICCV,\n    author    = {Fang, Yuxin and Yang, Shusheng and Wang, Xinggang and Li, Yu and Fang, Chen and Shan, Ying and Feng, Bin and Liu, Wenyu},\n    title     = {Instances As Queries},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2021},\n    pages     = {6910-6919}\n}\n```\n\n----------------------------------------\n\nTITLE: YOLO Anchor Optimization Example - Shell\nDESCRIPTION: Example usage of the anchor optimization script using differential evolution on a specific YOLO configuration with a defined input shape, device, and output directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/useful_tools.md#_snippet_41\n\nLANGUAGE: Shell\nCODE:\n```\npython tools/analysis_tools/optimize_anchors.py configs/yolo/yolov3_d53_8xb8-320-273e_coco.py --algorithm differential_evolution --input-shape 608 608 --device cuda --output-dir work_dirs\n```\n\n----------------------------------------\n\nTITLE: Checkpoint Configuration (2.x Max Keep)\nDESCRIPTION: This code snippet shows how to configure the maximum number of checkpoints to keep in MMDetection 2.x. It sets `max_keep_ckpts` to 3, meaning only the latest 3 checkpoints will be saved. Requires `data_root` to be defined.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/migration/config_migration.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_config = dict(\n    max_keep_ckpts=3)\n```\n\n----------------------------------------\n\nTITLE: Training Command (Single GPU) - Bash\nDESCRIPTION: This command trains the dummy ResNet model using a single GPU. It executes the `tools/train.py` script with a specific configuration file, `projects/example_project/configs/faster-rcnn_dummy-resnet_fpn_1x_coco.py`, located within the project directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/example_project/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tools/train.py projects/example_project/configs/faster-rcnn_dummy-resnet_fpn_1x_coco.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing Phrase Grounding Results\nDESCRIPTION: This shell command visualizes the phrase grounding results.  It uses the `tools/analysis_tools/browse_grounding_raw.py` script to display the predicted bounding boxes associated with phrases on the input images from the flickr30k dataset. The results are saved to `your_output_dir`. The script uses the generated json file (`flickr_simple_train_vg_v1.json`) to overlay the bounding boxes and phrases.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage.md#_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/browse_grounding_raw.py data/flickr30k_entities/ flickr_simple_train_vg_v1.json flickr30k_images -o your_output_dir --not-show\n```\n\n----------------------------------------\n\nTITLE: Setting DYNAMO_CACHE_SIZE_LIMIT for Single GPU\nDESCRIPTION: This shell script demonstrates how to set the `DYNAMO_CACHE_SIZE_LIMIT` environment variable to control the cache size used by TorchDynamo, along with enabling `torch.compile` for single GPU training. It aims to reduce compilation time during training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/notes/faq.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport DYNAMO_CACHE_SIZE_LIMIT = 4\npython tools/train.py configs/rtmdet/rtmdet_s_8xb32-300e_coco.py  --cfg-options compile=True\n```\n\n----------------------------------------\n\nTITLE: MaskFormer BibTeX Citation\nDESCRIPTION: This BibTeX entry provides the citation information for the MaskFormer paper, including the title, authors, journal, and year of publication. This is used to properly credit the original authors when using MaskFormer in research or projects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/maskformer/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{cheng2021maskformer,\n  title={Per-Pixel Classification is Not All You Need for Semantic Segmentation},\n  author={Bowen Cheng and Alexander G. Schwing and Alexander Kirillov},\n  journal={NeurIPS},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: ResNet Style Configuration in MMDetection\nDESCRIPTION: This snippet illustrates the difference in stride placement between 'pytorch' and 'caffe' ResNet styles within the Bottleneck module. In 'caffe' style, the stride is applied in the first 1x1 convolution, while in 'pytorch' style, it's applied in the 3x3 convolution.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nif self.style == 'pytorch':\n      self.conv1_stride = 1\n      self.conv2_stride = stride\nelse:\n      self.conv1_stride = stride\n      self.conv2_stride = 1\n```\n\n----------------------------------------\n\nTITLE: Slurm Training Example\nDESCRIPTION: This shell command shows an example of using `slurm_train.sh` to train Mask R-CNN on a Slurm partition named `dev` with 16 GPUs. The working directory is set to `/nfs/xxxx/mask_rcnn_r50_fpn_1x`, and the configuration file is `configs/mask-rcnn_r50_fpn_1x_coco.py`.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/train.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nGPUS=16 ./tools/slurm_train.sh dev mask_r50_1x configs/mask-rcnn_r50_fpn_1x_coco.py /nfs/xxxx/mask_rcnn_r50_fpn_1x\n```\n\n----------------------------------------\n\nTITLE: Converting RefCOCO to ODVG format using Python\nDESCRIPTION: This Python script converts the RefCOCO dataset annotations into the ODVG (Object Detection with Visual Grounding) format, which is required for training specific models in MMDetection. The script processes JSON annotation files in the specified directory.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare_zh-CN.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\npython tools/dataset_converters/refcoco2odvg.py data/coco/mdetr_annotations\n```\n\n----------------------------------------\n\nTITLE: Run GLIP Inference Demo with a Sentence\nDESCRIPTION: Executes the `image_demo.py` script for multimodal object detection using the GLIP model with a sentence as text prompt. The `--texts` argument takes a full sentence describing the scene.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg glip_tiny_a_mmdet-b3654169.pth --texts 'There are a lot of cars here.'\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the YOLOF paper, including the title, authors, booktitle, and year.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/yolof/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{chen2021you,\n  title={You Only Look One-level Feature},\n  author={Chen, Qiang and Wang, Yingming and Yang, Tong and Zhang, Xiangyu and Cheng, Jian and Sun, Jian},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: OC-SORT Citation (LaTeX)\nDESCRIPTION: This LaTeX snippet provides the citation information for the Observation-Centric SORT (OC-SORT) paper.  It includes the title, authors, journal, and year of publication.  This is used for academic referencing of the OC-SORT algorithm.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ocsort/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@article{cao2022observation,\n  title={Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking},\n  author={Cao, Jinkun and Weng, Xinshuo and Khirodkar, Rawal and Pang, Jiangmiao and Kitani, Kris},\n  journal={arXiv preprint arXiv:2203.14360},\n  year={2022}\n}\n```\n\n----------------------------------------\n\nTITLE: Failed Datasets List Format\nDESCRIPTION: This snippet illustrates the expected format of the `failed_dataset_list.txt` file, which contains a list of dataset names, one per line, with a blank line at the end. This file is used by the training script to retry failed datasets.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/RF100-Benchmark/README.md#_snippet_7\n\nLANGUAGE: Text\nCODE:\n```\nacl-x-ray\ntweeter-profile\nabdomen-mri\n\n\n```\n\n----------------------------------------\n\nTITLE: FreeAnchor Config - ResNeXt-101-32x4d Backbone - PyTorch\nDESCRIPTION: This config file provides the configuration details for FreeAnchor using a ResNeXt-101-32x4d backbone, implemented in PyTorch. It defines the network architecture, data processing pipeline, and training parameters for object detection on the COCO dataset. Links to pretrained models and training logs are also provided.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/free_anchor/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[config](./freeanchor_x101-32x4d_fpn_1x_coco.py)\n```\n\n----------------------------------------\n\nTITLE: Citing MS R-CNN in LaTeX\nDESCRIPTION: This LaTeX code snippet provides the correct citation format for the Mask Scoring R-CNN paper.  It includes the title, authors, booktitle, and year of publication. Use this snippet to properly attribute the work in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/ms_rcnn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{huang2019msrcnn,\n    title={Mask Scoring R-CNN},\n    author={Zhaojin Huang and Lichao Huang and Yongchao Gong and Chang Huang and Xinggang Wang},\n    booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n    year={2019},\n}\n```\n\n----------------------------------------\n\nTITLE: Running GLIP Inference with a Sentence Prompt\nDESCRIPTION: This command runs the inference demo for the GLIP model using a demo image, model weights, and a sentence as the text prompt. The '--texts' argument specifies the sentence to use for detecting objects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\npython demo/image_demo.py demo/demo.jpg glip_tiny_a_mmdet-b3654169.pth --texts 'There are a lot of cars here.'\n```\n\n----------------------------------------\n\nTITLE: Citation: PAFPN in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Path Aggregation Network (PAFPN) paper, including authors, title, booktitle, and year.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/pafpn/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{liu2018path,\n  author = {Shu Liu and\n            Lu Qi and\n            Haifang Qin and\n            Jianping Shi and\n            Jiaya Jia},\n  title = {Path Aggregation Network for Instance Segmentation},\n  booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2018}\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading NLTK Weights for Phrase Grounding\nDESCRIPTION: This Python script downloads specific NLTK (Natural Language Toolkit) resources to a user-defined directory, addressing potential connectivity limitations for Phrase Grounding tasks. It downloads the 'punkt' and 'averaged_perceptron_tagger' models, essential for text processing, to the specified `~/nltk_data` directory, ensuring the models are available even without internet access.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nnltk.download('punkt', download_dir='~/nltk_data')\nnltk.download('averaged_perceptron_tagger', download_dir='~/nltk_data')\n```\n\n----------------------------------------\n\nTITLE: DetectoRS Citation in LaTeX\nDESCRIPTION: This code snippet provides the LaTeX code for citing the DetectoRS paper. It includes the title, authors, journal, and year of publication, allowing users to properly reference the DetectoRS algorithm in their work.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/detectors/README.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{qiao2020detectors,\n  title={DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution},\n  author={Qiao, Siyuan and Chen, Liang-Chieh and Yuille, Alan},\n  journal={arXiv preprint arXiv:2006.02334},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Splitting Object365 v1 Dataset (Shell)\nDESCRIPTION: This script splits the Object365 v1 dataset into a smaller subset for easier review. It creates a directory with a limited number of training images and their corresponding JSON files.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/dataset_prepare.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/split_odvg.py data/object365_v1/ o365v1_train_od.json train your_output_dir --label-map-file o365v1_label_map.json -n 200\n```\n\n----------------------------------------\n\nTITLE: Download GLIP Model Weights\nDESCRIPTION: This command downloads the GLIP model weights from a specified URL and saves them in the 'mmdetection' directory. This is a necessary step before running the GLIP inference demo.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/inference.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\ncd mmdetection\nwget https://download.openmmlab.com/mmdetection/v3.0/glip/glip_tiny_a_mmdet-b3654169.pth\n```\n\n----------------------------------------\n\nTITLE: Create and Activate Conda Environment\nDESCRIPTION: This snippet creates a new conda environment named 'rtmdet' with Python 3.9 and then activates it. This ensures that the dependencies installed are isolated from other projects.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/label_studio.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda create -n rtmdet python=3.9 -y\nconda activate rtmdet\n```\n\n----------------------------------------\n\nTITLE: Citation of PAA Algorithm in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the Probabilistic Anchor Assignment (PAA) algorithm. It includes the title, authors, booktitle (ECCV), and year of publication. This information can be used to properly cite the PAA paper in academic works.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/paa/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{paa-eccv2020,\n  title={Probabilistic Anchor Assignment with IoU Prediction for Object Detection},\n  author={Kim, Kang and Lee, Hee Seok},\n  booktitle = {ECCV},\n  year={2020}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation of PointRend in LaTeX\nDESCRIPTION: This LaTeX snippet provides the citation information for the PointRend paper. It includes the title, authors, journal, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/point_rend/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@InProceedings{kirillov2019pointrend,\n  title={{PointRend}: Image Segmentation as Rendering},\n  author={Alexander Kirillov and Yuxin Wu and Kaiming He and Ross Girshick},\n  journal={ArXiv:1912.08193},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: MOT Model Inference Example (Checkpoint)\nDESCRIPTION: This example demonstrates MOT inference using a specified checkpoint file.  It utilizes a QDTrack model. The command specifies the input video, configuration file, checkpoint weights, and output file.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_interference.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# 示例 2：使用 --checkpoint\npython demo/mot_demo.py \\\n    demo/demo_mot.mp4 \\\n    configs/qdtrack/qdtrack_faster-rcnn_r50_fpn_8xb2-4e_mot17halftrain_test-mot17halfval.py \\\n    --checkpoint https://download.openmmlab.com/mmtracking/mot/qdtrack/mot_dataset/qdtrack_faster-rcnn_r50_fpn_4e_mot17_20220315_145635-76f295ef.pth \\\n    --out mot.mp4\n```\n\n----------------------------------------\n\nTITLE: Calculate Occluded/Separated Mask Recall (coco_occluded_separated_recall.py)\nDESCRIPTION: This snippet showcases how to calculate the occluded and separated mask recall using `tools/analysis_tools/coco_occluded_separated_recall.py`.  It takes the path to the pkl file containing the detection results as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/useful_tools.md#_snippet_42\n\nLANGUAGE: shell\nCODE:\n```\npython tools/analysis_tools/coco_occluded_separated_recall.py results.pkl --out occluded_separated_recall.json\n```\n\n----------------------------------------\n\nTITLE: Initializing FooModel with init_cfg in config\nDESCRIPTION: This demonstrates initializing a model using an `init_cfg` defined in a configuration file. The `type` key specifies the model class, and `init_cfg` defines the initialization settings.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/init_cfg.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = dict(\n\t...\n    \tmodel = dict(\n        \ttype='FooModel',\n        \targ1=XXX,\n        \targ2=XXX,\n        \tinit_cfg=XXX),\n            ...\n```\n\n----------------------------------------\n\nTITLE: Validate GLIP Model with Multiple GPUs\nDESCRIPTION: Executes the `dist_test.sh` script to validate the GLIP model's accuracy using multiple GPUs (in this case, 8). It takes a configuration file, a checkpoint file, and the number of GPUs as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\n./tools/dist_test.sh configs/glip/glip_atss_swin-t_fpn_dyhead_pretrain_obj365.py glip_tiny_a_mmdet-b3654169.pth 8\n```\n\n----------------------------------------\n\nTITLE: Download and Prepare COCO Test Dataset\nDESCRIPTION: This shell script downloads the necessary COCO test dataset images, testing image info, and panoptic annotations. It then extracts these files into the `data/coco/` directory and removes the original zip archives.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/test_results_submission.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# suppose data/coco/ does not exist\nmkdir -pv data/coco/\n\n# download test2017\nwget -P data/coco/ http://images.cocodataset.org/zips/test2017.zip\nwget -P data/coco/ http://images.cocodataset.org/annotations/image_info_test2017.zip\nwget -P data/coco/ http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip\n\n# unzip them\nunzip data/coco/test2017.zip -d data/coco/\nunzip data/coco/image_info_test2017.zip -d data/coco/\nunzip data/coco/panoptic_annotations_trainval2017.zip -d data/coco/\n\n# remove zip files (optional)\nrm -rf data/coco/test2017.zip data/coco/image_info_test2017.zip data/coco/panoptic_annotations_trainval2017.zip\n```\n\n----------------------------------------\n\nTITLE: Install PyTorch\nDESCRIPTION: Installs PyTorch, torchvision, and torchaudio with specific versions using pip. The appropriate command is chosen based on the system and desired CUDA support. The -f flag specifies the index URL to search for packages.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/label_studio.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Linux and Windows CPU only\npip install torch==1.10.1+cpu torchvision==0.11.2+cpu torchaudio==0.10.1 -f https://download.pytorch.org/whl/cpu/torch_stable.html\n# Linux and Windows CUDA 11.3\npip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n# OSX\npip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1\n```\n\n----------------------------------------\n\nTITLE: Mask RCNN Log Download Link\nDESCRIPTION: This snippet provides the URL to download the training logs for a Mask RCNN model with a RegNetX-4GF-FPN backbone. The logs contain information about the training process, such as loss curves and performance metrics.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/regnet/README.md#_snippet_30\n\nLANGUAGE: text\nCODE:\n```\n[log](https://download.openmmlab.com/mmdetection/v2.0/regnet/mask_rcnn_regnetx-4GF_fpn_mstrain-poly_3x_coco/mask_rcnn_regnetx-4GF_fpn_mstrain-poly_3x_coco_20210602_032621.log.json)\n```\n\n----------------------------------------\n\nTITLE: Video Inference Example Usage\nDESCRIPTION: This command demonstrates how to run the video inference demo script with a video file, configuration file, checkpoint file, and output file specified.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\npython demo/video_demo.py demo/demo.mp4 \\\n    configs/rtmdet/rtmdet_l_8xb32-300e_coco.py \\\n    checkpoints/rtmdet_l_8xb32-300e_coco_20220719_112030-5a0be7c4.pth \\\n    --out result.mp4\n```\n\n----------------------------------------\n\nTITLE: Validate GLIP Model with a Single GPU\nDESCRIPTION: Executes the `test.py` script to validate the GLIP model's accuracy using a single GPU. It takes a configuration file and a checkpoint file as input.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/inference.md#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\npython tools/test.py configs/glip/glip_atss_swin-t_fpn_dyhead_pretrain_obj365.py glip_tiny_a_mmdet-b3654169.pth\n```\n\n----------------------------------------\n\nTITLE: Testing Mask2former with Slurm\nDESCRIPTION: This example demonstrates how to test the Mask2former VIS model using the Slurm job scheduler. It specifies the configuration file and checkpoint file and uses 8 GPUs.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/user_guides/tracking_train_test_zh_cn.md#_snippet_17\n\nLANGUAGE: shell 脚本\nCODE:\n```\nGPUS=8\nbash tools/slurm_test_tracking.sh \\\nmypartition \\\nvis \\\nconfigs/mask2former_vis/mask2former_r50_8xb2-8e_youtubevis2021.py \\\n--checkpoint ${CHECKPOINT_FILE}\n```\n\n----------------------------------------\n\nTITLE: Citation in LaTeX for RepPoints paper\nDESCRIPTION: This LaTeX snippet provides the citation information for the RepPoints object detection paper, including the title, authors, conference, and year of publication. It is used to properly cite the paper in academic publications.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/reppoints/README.md#_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\n@inproceedings{yang2019reppoints,\n  title={RepPoints: Point Set Representation for Object Detection},\n  author={Yang, Ze and Liu, Shaohui and Hu, Han and Wang, Liwei and Lin, Stephen},\n  booktitle={The IEEE International Conference on Computer Vision (ICCV)},\n  month={Oct},\n  year={2019}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Module with Same Configuration (Python)\nDESCRIPTION: This example demonstrates how to initialize a module with the same configuration for multiple layers (`Conv1d`, `Conv2d`, `Linear`). The `init_cfg` dictionary specifies the initializer type as 'Constant' and sets the value to 1 for the weights of all specified layers. This approach is efficient when you want a uniform initialization across multiple layer types.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/en/user_guides/init_cfg.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninit_cfg = dict(type='Constant', layer=['Conv1d', 'Conv2d', 'Linear'], val=1)\n# initialize whole module with same configuration\n```\n\n----------------------------------------\n\nTITLE: Setting find_unused_parameters for DDP in MMDetection\nDESCRIPTION: This snippet shows how to set `find_unused_parameters = True` in the config file for training with DistributedDataParallel (DDP) in MMDetection. This is useful when some parameters are not used in the forward pass, which can cause errors with DDP. Setting this to `True` can slow down training.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/docs/zh_cn/notes/faq.md#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nfind_unused_parameters = True\n```\n\n----------------------------------------\n\nTITLE: Generating Image Metas for Open Images Dataset using get_image_metas.py\nDESCRIPTION: This script generates image metas (width and height) for the Open Images dataset which are used during training and testing. It requires specifying the configuration file, dataset type (train, val, or test), and the output file name.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/openimages/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython tools/misc/get_image_metas.py ${CONFIG} \\\n--dataset ${DATASET TYPE} \\  # train or val or test\n--out ${OUTPUT FILE NAME}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Saving BERT Weights Locally\nDESCRIPTION: This Python script downloads the BERT model weights and tokenizer from Hugging Face and saves them to a local directory. This is useful when network access is restricted. The script uses the `transformers` library to download the pre-trained \"bert-base-uncased\" model configuration, model weights, and tokenizer, and then saves them to the specified local path.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/usage_zh-CN.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BertConfig, BertModel\nfrom transformers import AutoTokenizer\n\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\", add_pooling_layer=False, config=config)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nconfig.save_pretrained(\"your path/bert-base-uncased\")\nmodel.save_pretrained(\"your path/bert-base-uncased\")\ntokenizer.save_pretrained(\"your path/bert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: CondInst Citation\nDESCRIPTION: BibTeX citation for the CondInst paper, which should be used when referencing this work. It contains the title, authors, conference, and year of publication.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/configs/condinst/README.md#_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n@inproceedings{tian2020conditional,\n  title     =  {Conditional Convolutions for Instance Segmentation},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}\n```\n\n----------------------------------------\n\nTITLE: LaTeX citation for VISION Datasets\nDESCRIPTION: This LaTeX code provides the citation information for the VISION Datasets. It includes the title, authors, journal, year, and other relevant details needed to properly cite the dataset in academic publications. This ensures proper attribution and allows others to easily find and reference the dataset.\nSOURCE: https://github.com/open-mmlab/mmdetection/blob/main/projects/VISION-Datasets/README_zh-CN.md#_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n@article{vision-datasets,\n  title         = {VISION Datasets: A Benchmark for Vision-based InduStrial InspectiON},\n  author        = {Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk Cinbis, Oncel Tuzel, Ping Huang, Jiulong Shan, Jianjun Shi, Meng Cao},\n  journal       = {arXiv preprint arXiv:2306.07890},\n  year          = {2023},\n}\n```"
  }
]
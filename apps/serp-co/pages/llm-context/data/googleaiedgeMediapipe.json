[
  {
    "owner": "google-ai-edge",
    "repo": "mediapipe",
    "content": "TITLE: Initializing and Using Audio Classifier in MediaPipe with JavaScript\nDESCRIPTION: This code shows how to initialize the Audio Classifier task from MediaPipe and perform classification on audio data. It first loads the necessary WASM modules using FilesetResolver, creates an AudioClassifier instance from a YAMNet model, and then classifies audio data to detect sounds.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/audio/README.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst audio = await FilesetResolver.forAudioTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio/wasm\"\n);\nconst audioClassifier = await AudioClassifier.createFromModelPath(audio,\n    \"https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite\n);\nconst classifications = audioClassifier.classify(audioData);\n```\n\n----------------------------------------\n\nTITLE: Initializing Interactive Segmenter with MediaPipe\nDESCRIPTION: This code shows how to initialize an Interactive Segmenter using MediaPipe Tasks Vision. It loads the vision tasks, creates an interactive segmenter from a model path, and performs segmentation based on a specified region of interest in an image. The user can select points to guide the segmentation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_9\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst interactiveSegmenter = await InteractiveSegmenter.createFromModelPath(\n    vision,\n    \"https://storage.googleapis.com/mediapipe-models/interactive_segmenter/magic_touch/float32/1/magic_touch.tflite\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\ninteractiveSegmenter.segment(image, { keypoint: { x: 0.1, y: 0.2 } },\n    (masks, width, height) => { ... }\n);\n```\n\n----------------------------------------\n\nTITLE: Face Mesh Detection and Visualization in Python using MediaPipe\nDESCRIPTION: Python script using MediaPipe's Face Mesh solution to detect and visualize facial landmarks in static images. Processes images to detect face landmarks, draw mesh tessellation, contours, and iris connections with configurable parameters for detection confidence and refinement.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_face_mesh = mp.solutions.face_mesh\n\n# For static images:\nIMAGE_FILES = []\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\nwith mp_face_mesh.FaceMesh(\n    static_image_mode=True,\n    max_num_faces=1,\n    refine_landmarks=True,\n    min_detection_confidence=0.5) as face_mesh:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    # Convert the BGR image to RGB before processing.\n    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Print and draw face mesh landmarks on the image.\n    if not results.multi_face_landmarks:\n      continue\n    annotated_image = image.copy()\n    for face_landmarks in results.multi_face_landmarks:\n      print('face_landmarks:', face_landmarks)\n      mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_TESSELATION,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles\n          .get_default_face_mesh_tesselation_style())\n      mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_CONTOURS,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles\n          .get_default_face_mesh_contours_style())\n      mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACEMESH_IRISES,\n          landmark_drawing_spec=None,\n          connection_drawing_spec=mp_drawing_styles\n          .get_default_face_mesh_iris_connections_style())\n    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n```\n\n----------------------------------------\n\nTITLE: Initializing Image Classifier with MediaPipe\nDESCRIPTION: This code demonstrates how to initialize an Image Classifier using MediaPipe Tasks Vision. It loads the vision tasks, creates an image classifier from a model path, and performs classification on an image element. The classifier returns category predictions for the image content.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst imageClassifier = await ImageClassifier.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst classifications = imageClassifier.classify(image);\n```\n\n----------------------------------------\n\nTITLE: Processing Static Images with MediaPipe Hands in Python\nDESCRIPTION: This snippet demonstrates how to use the MediaPipe Hands solution to process static images. It initializes the Hands object, reads images, detects hand landmarks, and draws them on the images. It also prints handedness information and landmark coordinates.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_hands = mp.solutions.hands\n\n# For static images:\nIMAGE_FILES = []\nwith mp_hands.Hands(\n    static_image_mode=True,\n    max_num_hands=2,\n    min_detection_confidence=0.5) as hands:\n  for idx, file in enumerate(IMAGE_FILES):\n    # Read an image, flip it around y-axis for correct handedness output (see\n    # above).\n    image = cv2.flip(cv2.imread(file), 1)\n    # Convert the BGR image to RGB before processing.\n    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Print handedness and draw hand landmarks on the image.\n    print('Handedness:', results.multi_handedness)\n    if not results.multi_hand_landmarks:\n      continue\n    image_height, image_width, _ = image.shape\n    annotated_image = image.copy()\n    for hand_landmarks in results.multi_hand_landmarks:\n      print('hand_landmarks:', hand_landmarks)\n      print(\n          f'Index finger tip coordinates: (',\n          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n      )\n      mp_drawing.draw_landmarks(\n          annotated_image,\n          hand_landmarks,\n          mp_hands.HAND_CONNECTIONS,\n          mp_drawing_styles.get_default_hand_landmarks_style(),\n          mp_drawing_styles.get_default_hand_connections_style())\n    cv2.imwrite(\n        '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n    # Draw hand world landmarks.\n    if not results.multi_hand_world_landmarks:\n      continue\n    for hand_world_landmarks in results.multi_hand_world_landmarks:\n      mp_drawing.plot_landmarks(\n        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n```\n\n----------------------------------------\n\nTITLE: JavaScript Implementation of MediaPipe Hands for Web\nDESCRIPTION: This JavaScript code implements MediaPipe Hands in a web environment. It sets up the Hands solution, configures options, processes video frames, and renders the results on a canvas. The code includes camera setup and continuous frame processing.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n<script type=\"module\">\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\n\nfunction onResults(results) {\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n  if (results.multiHandLandmarks) {\n    for (const landmarks of results.multiHandLandmarks) {\n      drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS,\n                     {color: '#00FF00', lineWidth: 5});\n      drawLandmarks(canvasCtx, landmarks, {color: '#FF0000', lineWidth: 2});\n    }\n  }\n  canvasCtx.restore();\n}\n\nconst hands = new Hands({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;\n}});\nhands.setOptions({\n  maxNumHands: 2,\n  modelComplexity: 1,\n  minDetectionConfidence: 0.5,\n  minTrackingConfidence: 0.5\n});\nhands.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await hands.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n</script>\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Detector with MediaPipe\nDESCRIPTION: This code demonstrates how to initialize a Face Detector using MediaPipe Tasks Vision. It loads the vision tasks, creates a face detector from a model path, and performs face detection on an image element. The detector returns face detection results with bounding boxes.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst faceDetector = await FaceDetector.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst detections = faceDetector.detect(image);\n```\n\n----------------------------------------\n\nTITLE: Initializing MediaPipe Face Mesh for Image Input in Android\nDESCRIPTION: This snippet demonstrates how to set up MediaPipe Face Mesh for processing static images in an Android app. It includes options configuration, result listener setup, and image selection from the gallery.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#2025-04-23_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nFaceMeshOptions faceMeshOptions =\n    FaceMeshOptions.builder()\n        .setStaticImageMode(true)\n        .setRefineLandmarks(true)\n        .setMaxNumFaces(1)\n        .setRunOnGpu(true).build();\nFaceMesh faceMesh = new FaceMesh(this, faceMeshOptions);\n\nFaceMeshResultImageView imageView = new FaceMeshResultImageView(this);\nfaceMesh.setResultListener(\n    faceMeshResult -> {\n      int width = faceMeshResult.inputBitmap().getWidth();\n      int height = faceMeshResult.inputBitmap().getHeight();\n      NormalizedLandmark noseLandmark =\n          result.multiFaceLandmarks().get(0).getLandmarkList().get(1);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Face Mesh nose coordinates (pixel values): x=%f, y=%f\",\n              noseLandmark.getX() * width, noseLandmark.getY() * height));\n      imageView.setFaceMeshResult(faceMeshResult);\n      runOnUiThread(() -> imageView.update());\n    });\nfaceMesh.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Face Mesh error:\" + message));\n\nActivityResultLauncher<Intent> imageGetter =\n    registerForActivityResult(\n        new ActivityResultContracts.StartActivityForResult(),\n        result -> {\n          Intent resultIntent = result.getData();\n          if (resultIntent != null && result.getResultCode() == RESULT_OK) {\n            Bitmap bitmap = null;\n            try {\n              bitmap =\n                  MediaStore.Images.Media.getBitmap(\n                      this.getContentResolver(), resultIntent.getData());\n            } catch (IOException e) {\n              Log.e(TAG, \"Bitmap reading error:\" + e);\n            }\n            if (bitmap != null) {\n              faceMesh.send(bitmap);\n            }\n          }\n        });\nIntent pickImageIntent = new Intent(Intent.ACTION_PICK);\npickImageIntent.setDataAndType(MediaStore.Images.Media.INTERNAL_CONTENT_URI, \"image/*\");\nimageGetter.launch(pickImageIntent);\n```\n\n----------------------------------------\n\nTITLE: Implementing Face Mesh Detection with MediaPipe in Python\nDESCRIPTION: This snippet demonstrates how to set up face mesh detection using MediaPipe's FaceMesh solution in Python. It processes webcam input, detects facial landmarks, and renders face mesh annotations on the video stream.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# For webcam input:\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\ncap = cv2.VideoCapture(0)\nwith mp_face_mesh.FaceMesh(\n    max_num_faces=1,\n    refine_landmarks=True,\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as face_mesh:\n  while cap.isOpened():\n    success, image = cap.read()\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = face_mesh.process(image)\n\n    # Draw the face mesh annotations on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    if results.multi_face_landmarks:\n      for face_landmarks in results.multi_face_landmarks:\n        mp_drawing.draw_landmarks(\n            image=image,\n            landmark_list=face_landmarks,\n            connections=mp_face_mesh.FACEMESH_TESSELATION,\n            landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles\n            .get_default_face_mesh_tesselation_style())\n        mp_drawing.draw_landmarks(\n            image=image,\n            landmark_list=face_landmarks,\n            connections=mp_face_mesh.FACEMESH_CONTOURS,\n            landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles\n            .get_default_face_mesh_contours_style())\n        mp_drawing.draw_landmarks(\n            image=image,\n            landmark_list=face_landmarks,\n            connections=mp_face_mesh.FACEMESH_IRISES,\n            landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles\n            .get_default_face_mesh_iris_connections_style())\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n    if cv2.waitKey(5) & 0xFF == 27:\n      break\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Initializing Gesture Recognizer with MediaPipe\nDESCRIPTION: This code shows how to initialize a Gesture Recognizer using MediaPipe Tasks Vision. It loads the vision tasks, creates a gesture recognizer from a model path, and performs gesture recognition on an image element. The recognizer returns detected hand gestures and their landmarks.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst gestureRecognizer = await GestureRecognizer.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst recognitions = gestureRecognizer.recognize(image);\n```\n\n----------------------------------------\n\nTITLE: Implementing MediaPipe Objectron with JavaScript\nDESCRIPTION: JavaScript implementation for initializing and using the Objectron API. This code sets up the camera input, processes video frames through Objectron, and renders detected 3D objects with bounding boxes and centroids on a canvas.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n<script type=\"module\">\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\n\nfunction onResults(results) {\n  canvasCtx.save();\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n  if (!!results.objectDetections) {\n    for (const detectedObject of results.objectDetections) {\n      // Reformat keypoint information as landmarks, for easy drawing.\n      const landmarks: mpObjectron.Point2D[] =\n          detectedObject.keypoints.map(x => x.point2d);\n      // Draw bounding box.\n      drawingUtils.drawConnectors(canvasCtx, landmarks,\n          mpObjectron.BOX_CONNECTIONS, {color: '#FF0000'});\n      // Draw centroid.\n      drawingUtils.drawLandmarks(canvasCtx, [landmarks[0]], {color: '#FFFFFF'});\n    }\n  }\n  canvasCtx.restore();\n}\n\nconst objectron = new Objectron({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/objectron/${file}`;\n}});\nobjectron.setOptions({\n  modelName: 'Chair',\n  maxNumObjects: 3,\n});\nobjectron.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await objectron.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n</script>\n```\n\n----------------------------------------\n\nTITLE: Face Detection with JavaScript and HTML\nDESCRIPTION: This snippet shows how to implement MediaPipe Face Detection using JavaScript and HTML. It sets up a video input, processes frames using the Face Detection model, and draws the results on a canvas element.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_detection.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/face_detection.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n  </div>\n</body>\n</html>\n```\n\nLANGUAGE: javascript\nCODE:\n```\n<script type=\"module\">\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\nconst drawingUtils = window;\n\nfunction onResults(results) {\n  // Draw the overlays.\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n  if (results.detections.length > 0) {\n    drawingUtils.drawRectangle(\n        canvasCtx, results.detections[0].boundingBox,\n        {color: 'blue', lineWidth: 4, fillColor: '#00000000'});\n    drawingUtils.drawLandmarks(canvasCtx, results.detections[0].landmarks, {\n      color: 'red',\n      radius: 5,\n    });\n  }\n  canvasCtx.restore();\n}\n\nconst faceDetection = new FaceDetection({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.0/${file}`;\n}});\nfaceDetection.setOptions({\n  model: 'short',\n  minDetectionConfidence: 0.5\n});\nfaceDetection.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await faceDetection.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n</script>\n```\n\n----------------------------------------\n\nTITLE: Implementing MediaPipe Pose Detection in Python\nDESCRIPTION: This code demonstrates how to use MediaPipe Pose for both static images and webcam input. It shows initialization with various configuration options, processing images, extracting landmark coordinates, applying segmentation, and visualizing the results with drawing utilities.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/pose.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_pose = mp.solutions.pose\n\n# For static images:\nIMAGE_FILES = []\nBG_COLOR = (192, 192, 192) # gray\nwith mp_pose.Pose(\n    static_image_mode=True,\n    model_complexity=2,\n    enable_segmentation=True,\n    min_detection_confidence=0.5) as pose:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    image_height, image_width, _ = image.shape\n    # Convert the BGR image to RGB before processing.\n    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    if not results.pose_landmarks:\n      continue\n    print(\n        f'Nose coordinates: ('\n        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '\n        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_height})'\n    )\n\n    annotated_image = image.copy()\n    # Draw segmentation on the image.\n    # To improve segmentation around boundaries, consider applying a joint\n    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n    bg_image = np.zeros(image.shape, dtype=np.uint8)\n    bg_image[:] = BG_COLOR\n    annotated_image = np.where(condition, annotated_image, bg_image)\n    # Draw pose landmarks on the image.\n    mp_drawing.draw_landmarks(\n        annotated_image,\n        results.pose_landmarks,\n        mp_pose.POSE_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n    # Plot pose world landmarks.\n    mp_drawing.plot_landmarks(\n        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n\n# For webcam input:\ncap = cv2.VideoCapture(0)\nwith mp_pose.Pose(\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as pose:\n  while cap.isOpened():\n    success, image = cap.read()\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = pose.process(image)\n\n    # Draw the pose annotation on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    mp_drawing.draw_landmarks(\n        image,\n        results.pose_landmarks,\n        mp_pose.POSE_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n    if cv2.waitKey(5) & 0xFF == 27:\n      break\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Implementing Face Mesh Detection with MediaPipe in Android (Java)\nDESCRIPTION: This snippet demonstrates how to set up face mesh detection using MediaPipe's FaceMesh solution in Android (Java). It processes camera input, detects facial landmarks, and renders face mesh annotations using OpenGL.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#2025-04-23_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n// For camera input and result rendering with OpenGL.\nFaceMeshOptions faceMeshOptions =\n    FaceMeshOptions.builder()\n        .setStaticImageMode(false)\n        .setRefineLandmarks(true)\n        .setMaxNumFaces(1)\n        .setRunOnGpu(true).build();\nFaceMesh faceMesh = new FaceMesh(this, faceMeshOptions);\nfaceMesh.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Face Mesh error:\" + message));\n\n// Initializes a new CameraInput instance and connects it to MediaPipe Face Mesh Solution.\nCameraInput cameraInput = new CameraInput(this);\ncameraInput.setNewFrameListener(\n    textureFrame -> faceMesh.send(textureFrame));\n\n// Initializes a new GlSurfaceView with a ResultGlRenderer<FaceMeshResult> instance\n// that provides the interfaces to run user-defined OpenGL rendering code.\n// See mediapipe/examples/android/solutions/facemesh/src/main/java/com/google/mediapipe/examples/facemesh/FaceMeshResultGlRenderer.java\n// as an example.\nSolutionGlSurfaceView<FaceMeshResult> glSurfaceView =\n    new SolutionGlSurfaceView<>(\n        this, faceMesh.getGlContext(), faceMesh.getGlMajorVersion());\nglSurfaceView.setSolutionResultRenderer(new FaceMeshResultGlRenderer());\nglSurfaceView.setRenderInputImage(true);\n\nfaceMesh.setResultListener(\n    faceMeshResult -> {\n      NormalizedLandmark noseLandmark =\n          result.multiFaceLandmarks().get(0).getLandmarkList().get(1);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Face Mesh nose normalized coordinates (value range: [0, 1]): x=%f, y=%f\",\n              noseLandmark.getX(), noseLandmark.getY()));\n      // Request GL rendering.\n      glSurfaceView.setRenderData(faceMeshResult);\n      glSurfaceView.requestRender();\n    });\n\n// The runnable to start camera after the GLSurfaceView is attached.\nglSurfaceView.post(\n    () ->\n        cameraInput.start(\n            this,\n            faceMesh.getGlContext(),\n            CameraInput.CameraFacing.FRONT,\n            glSurfaceView.getWidth(),\n            glSurfaceView.getHeight()));\n```\n\n----------------------------------------\n\nTITLE: JavaScript Implementation of MediaPipe Holistic Web Demo\nDESCRIPTION: This JavaScript snippet implements the MediaPipe Holistic model in a web environment. It sets up the model, processes video frames, and draws the resulting landmarks and segmentation mask on a canvas.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md#2025-04-23_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n<script type=\"module\">\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\n\nfunction onResults(results) {\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(results.segmentationMask, 0, 0,\n                      canvasElement.width, canvasElement.height);\n\n  // Only overwrite existing pixels.\n  canvasCtx.globalCompositeOperation = 'source-in';\n  canvasCtx.fillStyle = '#00FF00';\n  canvasCtx.fillRect(0, 0, canvasElement.width, canvasElement.height);\n\n  // Only overwrite missing pixels.\n  canvasCtx.globalCompositeOperation = 'destination-atop';\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n\n  canvasCtx.globalCompositeOperation = 'source-over';\n  drawConnectors(canvasCtx, results.poseLandmarks, POSE_CONNECTIONS,\n                 {color: '#00FF00', lineWidth: 4});\n  drawLandmarks(canvasCtx, results.poseLandmarks,\n                {color: '#FF0000', lineWidth: 2});\n  drawConnectors(canvasCtx, results.faceLandmarks, FACEMESH_TESSELATION,\n                 {color: '#C0C0C070', lineWidth: 1});\n  drawConnectors(canvasCtx, results.leftHandLandmarks, HAND_CONNECTIONS,\n                 {color: '#CC0000', lineWidth: 5});\n  drawLandmarks(canvasCtx, results.leftHandLandmarks,\n                {color: '#00FF00', lineWidth: 2});\n  drawConnectors(canvasCtx, results.rightHandLandmarks, HAND_CONNECTIONS,\n                 {color: '#00CC00', lineWidth: 5});\n  drawLandmarks(canvasCtx, results.rightHandLandmarks,\n                {color: '#FF0000', lineWidth: 2});\n  canvasCtx.restore();\n}\n\nconst holistic = new Holistic({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;\n}});\nholistic.setOptions({\n  modelComplexity: 1,\n  smoothLandmarks: true,\n  enableSegmentation: true,\n  smoothSegmentation: true,\n  refineFaceLandmarks: true,\n  minDetectionConfidence: 0.5,\n  minTrackingConfidence: 0.5\n});\nholistic.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await holistic.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n</script>\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Landmarker with MediaPipe\nDESCRIPTION: This code shows how to initialize a Face Landmarker using MediaPipe Tasks Vision. It loads the vision tasks, creates a face landmarker from a model path, and performs face landmark detection on an image element. The landmarker returns facial landmarks for rendering visual effects.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst faceLandmarker = await FaceLandmarker.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst landmarks = faceLandmarker.detect(image);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Audio Embedding in MediaPipe with JavaScript\nDESCRIPTION: This code demonstrates how to set up and use the Audio Embedding task in MediaPipe. It loads the required WASM modules via FilesetResolver, initializes an AudioEmbedder with a YAMNet embedding model, and extracts feature embeddings from audio data that can be used for downstream tasks.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/audio/README.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst audio = await FilesetResolver.forAudioTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-audio/wasm\"\n);\nconst audioEmbedder = await AudioEmbedder.createFromModelPath(audio,\n    \"https://storage.googleapis.com/mediapipe-assets/yamnet_embedding_metadata.tflite?generation=1668295071595506\"\n);\nconst embeddings = audioEmbedder.embed(audioData);\n```\n\n----------------------------------------\n\nTITLE: Implementing a GPU-based Luminance Calculator with OpenGL ES in MediaPipe\nDESCRIPTION: This code snippet demonstrates a complete implementation of a GPU calculator that converts RGB images to luminance images using OpenGL ES shaders. It shows the structure of a calculator derived from GlSimpleCalculator, with the focus on the GlRender method that performs the actual OpenGL drawing operations.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/gpu.md#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n// Converts RGB images into luminance images, still stored in RGB format.\n// See GlSimpleCalculator for inputs, outputs and input side packets.\nclass LuminanceCalculator : public GlSimpleCalculator {\n public:\n  absl::Status GlSetup() override;\n  absl::Status GlRender(const GlTexture& src,\n                        const GlTexture& dst) override;\n  absl::Status GlTeardown() override;\n\n private:\n  GLuint program_ = 0;\n  GLint frame_;\n};\nREGISTER_CALCULATOR(LuminanceCalculator);\n\nabsl::Status LuminanceCalculator::GlRender(const GlTexture& src,\n                                           const GlTexture& dst) {\n  static const GLfloat square_vertices[] = {\n      -1.0f, -1.0f,  // bottom left\n      1.0f,  -1.0f,  // bottom right\n      -1.0f, 1.0f,   // top left\n      1.0f,  1.0f,   // top right\n  };\n  static const GLfloat texture_vertices[] = {\n      0.0f, 0.0f,  // bottom left\n      1.0f, 0.0f,  // bottom right\n      0.0f, 1.0f,  // top left\n      1.0f, 1.0f,  // top right\n  };\n\n  // program\n  glUseProgram(program_);\n  glUniform1i(frame_, 1);\n\n  // vertex storage\n  GLuint vbo[2];\n  glGenBuffers(2, vbo);\n  GLuint vao;\n  glGenVertexArrays(1, &vao);\n  glBindVertexArray(vao);\n\n  // vbo 0\n  glBindBuffer(GL_ARRAY_BUFFER, vbo[0]);\n  glBufferData(GL_ARRAY_BUFFER, 4 * 2 * sizeof(GLfloat), square_vertices,\n               GL_STATIC_DRAW);\n  glEnableVertexAttribArray(ATTRIB_VERTEX);\n  glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, nullptr);\n\n  // vbo 1\n  glBindBuffer(GL_ARRAY_BUFFER, vbo[1]);\n  glBufferData(GL_ARRAY_BUFFER, 4 * 2 * sizeof(GLfloat), texture_vertices,\n               GL_STATIC_DRAW);\n  glEnableVertexAttribArray(ATTRIB_TEXTURE_POSITION);\n  glVertexAttribPointer(ATTRIB_TEXTURE_POSITION, 2, GL_FLOAT, 0, 0, nullptr);\n\n  // draw\n  glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);\n\n  // cleanup\n  glDisableVertexAttribArray(ATTRIB_VERTEX);\n  glDisableVertexAttribArray(ATTRIB_TEXTURE_POSITION);\n  glBindBuffer(GL_ARRAY_BUFFER, 0);\n  glBindVertexArray(0);\n  glDeleteVertexArrays(1, &vao);\n  glDeleteBuffers(2, vbo);\n\n  return absl::OkStatus();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Face Mesh Detection with MediaPipe in JavaScript\nDESCRIPTION: This snippet shows how to set up face mesh detection using MediaPipe's FaceMesh solution in JavaScript. It processes video input, detects facial landmarks, and renders face mesh annotations on a canvas element.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#2025-04-23_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n  </div>\n</body>\n</html>\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\n<script type=\"module\">\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\n\nfunction onResults(results) {\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n  if (results.multiFaceLandmarks) {\n    for (const landmarks of results.multiFaceLandmarks) {\n      drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION,\n                     {color: '#C0C0C070', lineWidth: 1});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, {color: '#FF3030'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYEBROW, {color: '#FF3030'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_IRIS, {color: '#FF3030'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, {color: '#30FF30'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYEBROW, {color: '#30FF30'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_IRIS, {color: '#30FF30'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_FACE_OVAL, {color: '#E0E0E0'});\n      drawConnectors(canvasCtx, landmarks, FACEMESH_LIPS, {color: '#E0E0E0'});\n    }\n  }\n  canvasCtx.restore();\n}\n\nconst faceMesh = new FaceMesh({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;\n}});\nfaceMesh.setOptions({\n  maxNumFaces: 1,\n  refineLandmarks: true,\n  minDetectionConfidence: 0.5,\n  minTrackingConfidence: 0.5\n});\nfaceMesh.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await faceMesh.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n</script>\n```\n\n----------------------------------------\n\nTITLE: Face Detection with Python and OpenCV\nDESCRIPTION: This snippet demonstrates how to use MediaPipe Face Detection with Python and OpenCV. It includes examples for both static images and webcam input, showing how to initialize the face detection model, process images, and draw face detections.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_detection.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nmp_face_detection = mp.solutions.face_detection\nmp_drawing = mp.solutions.drawing_utils\n\n# For static images:\nIMAGE_FILES = []\nwith mp_face_detection.FaceDetection(\n    model_selection=1, min_detection_confidence=0.5) as face_detection:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Draw face detections of each face.\n    if not results.detections:\n      continue\n    annotated_image = image.copy()\n    for detection in results.detections:\n      print('Nose tip:')\n      print(mp_face_detection.get_key_point(\n          detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n      mp_drawing.draw_detection(annotated_image, detection)\n    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n\n# For webcam input:\ncap = cv2.VideoCapture(0)\nwith mp_face_detection.FaceDetection(\n    model_selection=0, min_detection_confidence=0.5) as face_detection:\n  while cap.isOpened():\n    success, image = cap.read()\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = face_detection.process(image)\n\n    # Draw the face detection annotations on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    if results.detections:\n      for detection in results.detections:\n        mp_drawing.draw_detection(image, detection)\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n    if cv2.waitKey(5) & 0xFF == 27:\n      break\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Initializing Holistic Landmarker with MediaPipe\nDESCRIPTION: This code shows how to initialize a Holistic Landmarker using MediaPipe Tasks Vision. It loads the vision tasks, creates a holistic landmarker from a model path, and performs detection on an image element. The holistic landmarker combines pose, face, and hand landmark detection in one pipeline.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst holisticLandmarker = await HolisticLandmarker.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/holistic_landmarker/holistic_landmarker/float16/1/hand_landmark.task\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst landmarks = holisticLandmarker.detect(image);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Text Classifier in MediaPipe (JavaScript)\nDESCRIPTION: This code shows how to implement text classification using MediaPipe. It initializes the text tasks module, creates a TextClassifier instance with a BERT-based model, and classifies input text into predefined categories.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/text/README.md#2025-04-23_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst text = await FilesetResolver.forTextTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-text/wasm\"\n);\nconst textClassifier = await TextClassifier.createFromModelPath(text,\n    \"https://storage.googleapis.com/mediapipe-models/text_classifier/bert_classifier/float32/1/bert_classifier.tflite\"\n);\nconst classifications = textClassifier.classify(textData);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using LLM Inference in MediaPipe Tasks GenAI\nDESCRIPTION: This code snippet demonstrates how to initialize the LLM Inference task using a FilesetResolver and a model URL, then generate a response from input text. It requires a pre-downloaded or converted LLM model compatible with MediaPipe's GPU backend.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/genai/README.md#2025-04-23_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst genai = await FilesetResolver.forGenAiTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai/wasm\"\n);\nconst llmInference = await LlmInference.createFromModelPath(genai, MODEL_URL);\nconst response = await llmInference.generateResponse(inputText);\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for MediaPipe Hands Web Implementation\nDESCRIPTION: This HTML snippet sets up the necessary structure for implementing MediaPipe Hands in a web environment. It includes video input, canvas output, and the required JavaScript libraries for MediaPipe functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n  </div>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Initializing Hand Landmarker with MediaPipe\nDESCRIPTION: This code demonstrates how to initialize a Hand Landmarker using MediaPipe Tasks Vision. It loads the vision tasks, creates a hand landmarker from a model path, and performs hand landmark detection on an image element. The landmarker returns key points of detected hands for visual rendering.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst handLandmarker = await HandLandmarker.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst landmarks = handLandmarker.detect(image);\n```\n\n----------------------------------------\n\nTITLE: Building a Calculator Graph with Optimal Readability Using Utility Functions\nDESCRIPTION: The most readable approach for calculator graph construction that extracts node creation into utility functions. This method provides clear scoping for each node while maintaining a concise, easy-to-follow main function.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_18\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).Cast<A>();\n\n  Stream<B> b = RunCalculator1(a, graph);\n  Stream<C> c = RunCalculator2(b, graph);\n  Stream<D> d = RunCalculator3(b, c, graph);\n  Stream<E> e = RunCalculator4(b, c, d, graph);\n\n  // Outputs.\n  b.SetName(\"b\").ConnectTo(graph.Out(0));\n  c.SetName(\"c\").ConnectTo(graph.Out(1));\n  d.SetName(\"d\").ConnectTo(graph.Out(2));\n  e.SetName(\"e\").ConnectTo(graph.Out(3));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Webcam Input with MediaPipe Holistic in Python\nDESCRIPTION: This snippet shows how to use the MediaPipe Holistic model for real-time processing of webcam input in Python. It includes setting up the model, capturing video frames, and drawing landmarks on the output image.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# For webcam input:\ncap = cv2.VideoCapture(0)\nwith mp_holistic.Holistic(\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as holistic:\n  while cap.isOpened():\n    success, image = cap.read()\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = holistic.process(image)\n\n    # Draw landmark annotation on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    mp_drawing.draw_landmarks(\n        image,\n        results.face_landmarks,\n        mp_holistic.FACEMESH_CONTOURS,\n        landmark_drawing_spec=None,\n        connection_drawing_spec=mp_drawing_styles\n        .get_default_face_mesh_contours_style())\n    mp_drawing.draw_landmarks(\n        image,\n        results.pose_landmarks,\n        mp_holistic.POSE_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles\n        .get_default_pose_landmarks_style())\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n    if cv2.waitKey(5) & 0xFF == 27:\n      break\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Real-time Hand Tracking with MediaPipe Hands in Python\nDESCRIPTION: This snippet shows how to use MediaPipe Hands for real-time hand tracking using a webcam. It captures video frames, processes them to detect hand landmarks, and displays the results in real-time. The code includes performance optimizations and proper resource management.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# For webcam input:\ncap = cv2.VideoCapture(0)\nwith mp_hands.Hands(\n    model_complexity=0,\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5) as hands:\n  while cap.isOpened():\n    success, image = cap.read()\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = hands.process(image)\n\n    # Draw the hand annotations on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    if results.multi_hand_landmarks:\n      for hand_landmarks in results.multi_hand_landmarks:\n        mp_drawing.draw_landmarks(\n            image,\n            hand_landmarks,\n            mp_hands.HAND_CONNECTIONS,\n            mp_drawing_styles.get_default_hand_landmarks_style(),\n            mp_drawing_styles.get_default_hand_connections_style())\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n    if cv2.waitKey(5) & 0xFF == 27:\n      break\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Processing Static Images with MediaPipe Holistic in Python\nDESCRIPTION: This snippet demonstrates how to use the MediaPipe Holistic model to process static images in Python. It includes setting up the model, processing images, and drawing landmarks and segmentation masks.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_holistic = mp.solutions.holistic\n\n# For static images:\nIMAGE_FILES = []\nBG_COLOR = (192, 192, 192) # gray\nwith mp_holistic.Holistic(\n    static_image_mode=True,\n    model_complexity=2,\n    enable_segmentation=True,\n    refine_face_landmarks=True) as holistic:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    image_height, image_width, _ = image.shape\n    # Convert the BGR image to RGB before processing.\n    results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    if results.pose_landmarks:\n      print(\n          f'Nose coordinates: ('\n          f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, '\n          f'{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_height})'\n      )\n\n    annotated_image = image.copy()\n    # Draw segmentation on the image.\n    # To improve segmentation around boundaries, consider applying a joint\n    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n    bg_image = np.zeros(image.shape, dtype=np.uint8)\n    bg_image[:] = BG_COLOR\n    annotated_image = np.where(condition, annotated_image, bg_image)\n    # Draw pose, left and right hands, and face landmarks on the image.\n    mp_drawing.draw_landmarks(\n        annotated_image,\n        results.face_landmarks,\n        mp_holistic.FACEMESH_TESSELATION,\n        landmark_drawing_spec=None,\n        connection_drawing_spec=mp_drawing_styles\n        .get_default_face_mesh_tesselation_style())\n    mp_drawing.draw_landmarks(\n        annotated_image,\n        results.pose_landmarks,\n        mp_holistic.POSE_CONNECTIONS,\n        landmark_drawing_spec=mp_drawing_styles.\n        get_default_pose_landmarks_style())\n    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n    # Plot pose world landmarks.\n    mp_drawing.plot_landmarks(\n        results.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS)\n```\n\n----------------------------------------\n\nTITLE: Implementing CalculatorBase Class in C++\nDESCRIPTION: Code snippet showing the core methods that must be implemented when creating a calculator in MediaPipe: GetContract(), Open(), Process(), and Close(). These methods define the calculator's input/output types and handle its lifecycle during graph execution.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nclass CalculatorBase {\n public:\n  ...\n\n  // The subclasses of CalculatorBase must implement GetContract.\n  // ...\n  static absl::Status GetContract(CalculatorContract* cc);\n\n  // Open is called before any Process() calls, on a freshly constructed\n  // calculator.  Subclasses may override this method to perform necessary\n  // setup, and possibly output Packets and/or set output streams' headers.\n  // ...\n  virtual absl::Status Open(CalculatorContext* cc) {\n    return absl::OkStatus();\n  }\n\n  // Processes the incoming inputs. May call the methods on cc to access\n  // inputs and produce outputs.\n  // ...\n  virtual absl::Status Process(CalculatorContext* cc) = 0;\n\n  // Is called if Open() was called and succeeded.  Is called either\n  // immediately after processing is complete or after a graph run has ended\n  // (if an error occurred in the graph).  ...\n  virtual absl::Status Close(CalculatorContext* cc) {\n    return absl::OkStatus();\n  }\n\n  ...\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Pipeline with MediaPipe for LLM Knowledge Augmentation in JavaScript\nDESCRIPTION: This example demonstrates how to set up and use a RAG (Retrieval-Augmented Generation) Pipeline to enhance an LLM with factual knowledge. The code initializes the necessary filesets, creates an LLM inference instance, and sets up a RAG pipeline with an embedding model to augment the LLM with recorded knowledge.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/genai_experimental/README.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst genaiFileset = await FilesetResolver.forGenAiTasks();\nconst genaiExperimentalFileset =\n  await FilesetResolver.forGenAiExperimentalTasks();\nconst llmInference = await LlmInference.createFromModelPath(genaiFileset, ...);\nconst ragPipeline = await RagPipeline.createWithEmbeddingModel(\n  genaiExperimentalFileset,\n  llmInference,\n  EMBEDDING_MODEL_URL,\n);\nawait ragPipeline.recordBatchedMemory([\n  'Paris is the capital of France.',\n  'Berlin is the capital of Germany.',\n]);\nconst result = await ragPipeline.generateResponse(\n  'What is the capital of France?',\n);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: AutoFlip MediaPipe Graph Configuration\nDESCRIPTION: Complete configuration for the AutoFlip processing graph. This protobuf configuration defines the full pipeline including video preparation, feature detection (borders, faces, objects), signal fusion, cropping, and encoding.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/autoflip.md#2025-04-23_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\n# Autoflip graph that only renders the final cropped video. For use with\n# end user applications.\nmax_queue_size: -1\n\n# VIDEO_PREP: Decodes an input video file into images and a video header.\nnode {\n  calculator: \"OpenCvVideoDecoderCalculator\"\n  input_side_packet: \"INPUT_FILE_PATH:input_video_path\"\n  output_stream: \"VIDEO:video_raw\"\n  output_stream: \"VIDEO_PRESTREAM:video_header\"\n  output_side_packet: \"SAVED_AUDIO_PATH:audio_path\"\n}\n\n# VIDEO_PREP: Scale the input video before feature extraction.\nnode {\n  calculator: \"ScaleImageCalculator\"\n  input_stream: \"FRAMES:video_raw\"\n  input_stream: \"VIDEO_HEADER:video_header\"\n  output_stream: \"FRAMES:video_frames_scaled\"\n  node_options: {\n    [type.googleapis.com/mediapipe.ScaleImageCalculatorOptions]: {\n      preserve_aspect_ratio: true\n      output_format: SRGB\n      target_width: 480\n      algorithm: DEFAULT_WITHOUT_UPSCALE\n    }\n  }\n}\n\n# VIDEO_PREP: Create a low frame rate stream for feature extraction.\nnode {\n  calculator: \"PacketThinnerCalculator\"\n  input_stream: \"video_frames_scaled\"\n  output_stream: \"video_frames_scaled_downsampled\"\n  node_options: {\n    [type.googleapis.com/mediapipe.PacketThinnerCalculatorOptions]: {\n      thinner_type: ASYNC\n      period: 200000\n    }\n  }\n}\n\n# DETECTION: find borders around the video and major background color.\nnode {\n  calculator: \"BorderDetectionCalculator\"\n  input_stream: \"VIDEO:video_raw\"\n  output_stream: \"DETECTED_BORDERS:borders\"\n}\n\n# DETECTION: find shot/scene boundaries on the full frame rate stream.\nnode {\n  calculator: \"ShotBoundaryCalculator\"\n  input_stream: \"VIDEO:video_frames_scaled\"\n  output_stream: \"IS_SHOT_CHANGE:shot_change\"\n  options {\n    [type.googleapis.com/mediapipe.autoflip.ShotBoundaryCalculatorOptions] {\n      min_shot_span: 0.2\n      min_motion: 0.3\n      window_size: 15\n      min_shot_measure: 10\n      min_motion_with_shot_measure: 0.05\n    }\n  }\n}\n\n# DETECTION: find faces on the down sampled stream\nnode {\n  calculator: \"AutoFlipFaceDetectionSubgraph\"\n  input_stream: \"VIDEO:video_frames_scaled_downsampled\"\n  output_stream: \"DETECTIONS:face_detections\"\n}\nnode {\n  calculator: \"FaceToRegionCalculator\"\n  input_stream: \"VIDEO:video_frames_scaled_downsampled\"\n  input_stream: \"FACES:face_detections\"\n  output_stream: \"REGIONS:face_regions\"\n}\n\n# DETECTION: find objects on the down sampled stream\nnode {\n  calculator: \"AutoFlipObjectDetectionSubgraph\"\n  input_stream: \"VIDEO:video_frames_scaled_downsampled\"\n  output_stream: \"DETECTIONS:object_detections\"\n}\nnode {\n  calculator: \"LocalizationToRegionCalculator\"\n  input_stream: \"DETECTIONS:object_detections\"\n  output_stream: \"REGIONS:object_regions\"\n  options {\n    [type.googleapis.com/mediapipe.autoflip.LocalizationToRegionCalculatorOptions] {\n      output_all_signals: true\n    }\n  }\n}\n\n# SIGNAL FUSION: Combine detections (with weights) on each frame\nnode {\n  calculator: \"SignalFusingCalculator\"\n  input_stream: \"shot_change\"\n  input_stream: \"face_regions\"\n  input_stream: \"object_regions\"\n  output_stream: \"salient_regions\"\n  options {\n    [type.googleapis.com/mediapipe.autoflip.SignalFusingCalculatorOptions] {\n      signal_settings {\n        type { standard: FACE_CORE_LANDMARKS }\n        min_score: 0.85\n        max_score: 0.9\n        is_required: false\n      }\n      signal_settings {\n        type { standard: FACE_ALL_LANDMARKS }\n        min_score: 0.8\n        max_score: 0.85\n        is_required: false\n      }\n      signal_settings {\n        type { standard: FACE_FULL }\n        min_score: 0.8\n        max_score: 0.85\n        is_required: false\n      }\n      signal_settings {\n        type: { standard: HUMAN }\n        min_score: 0.75\n        max_score: 0.8\n        is_required: false\n      }\n      signal_settings {\n        type: { standard: PET }\n        min_score: 0.7\n        max_score: 0.75\n        is_required: false\n      }\n      signal_settings {\n        type: { standard: CAR }\n        min_score: 0.7\n        max_score: 0.75\n        is_required: false\n      }\n      signal_settings {\n        type: { standard: OBJECT }\n        min_score: 0.1\n        max_score: 0.2\n        is_required: false\n      }\n    }\n  }\n}\n\n# CROPPING: make decisions about how to crop each frame.\nnode {\n  calculator: \"SceneCroppingCalculator\"\n  input_side_packet: \"EXTERNAL_ASPECT_RATIO:aspect_ratio\"\n  input_stream: \"VIDEO_FRAMES:video_raw\"\n  input_stream: \"KEY_FRAMES:video_frames_scaled_downsampled\"\n  input_stream: \"DETECTION_FEATURES:salient_regions\"\n  input_stream: \"STATIC_FEATURES:borders\"\n  input_stream: \"SHOT_BOUNDARIES:shot_change\"\n  output_stream: \"CROPPED_FRAMES:cropped_frames\"\n  node_options: {\n    [type.googleapis.com/mediapipe.autoflip.SceneCroppingCalculatorOptions]: {\n      max_scene_size: 600\n      key_frame_crop_options: {\n        score_aggregation_type: CONSTANT\n      }\n      scene_camera_motion_analyzer_options: {\n        motion_stabilization_threshold_percent: 0.5\n        salient_point_bound: 0.499\n      }\n      padding_parameters: {\n        blur_cv_size: 200\n        overlay_opacity: 0.6\n      }\n      target_size_type: MAXIMIZE_TARGET_DIMENSION\n    }\n  }\n}\n\n# ENCODING(required): encode the video stream for the final cropped output.\nnode {\n  calculator: \"VideoPreStreamCalculator\"\n  # Fetch frame format and dimension from input frames.\n  input_stream: \"FRAME:cropped_frames\"\n  # Copying frame rate and duration from original video.\n  input_stream: \"VIDEO_PRESTREAM:video_header\"\n  output_stream: \"output_frames_video_header\"\n}\n\nnode {\n  calculator: \"OpenCvVideoEncoderCalculator\"\n  input_stream: \"VIDEO:cropped_frames\"\n  input_stream: \"VIDEO_PRESTREAM:output_frames_video_header\"\n  input_side_packet: \"OUTPUT_FILE_PATH:output_video_path\"\n  input_side_packet: \"AUDIO_FILE_PATH:audio_path\"\n  node_options: {\n    [type.googleapis.com/mediapipe.OpenCvVideoEncoderCalculatorOptions]: {\n      codec: \"avc1\"\n      video_format: \"mp4\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Text Embedder in MediaPipe (JavaScript)\nDESCRIPTION: This code demonstrates how to extract text embeddings using MediaPipe's Text Embedder. It sets up the text tasks environment, creates a TextEmbedder with the Universal Sentence Encoder model, and generates embeddings from the provided text data.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/text/README.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst text = await FilesetResolver.forTextTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-text/wasm\"\n);\nconst textEmbedder = await TextEmbedder.createFromModelPath(text,\n    \"https://storage.googleapis.com/mediapipe-models/text_embedder/universal_sentence_encoder/float32/1/universal_sentence_encoder.tflite\"\n);\nconst embeddings = textEmbedder.embed(textData);\n```\n\n----------------------------------------\n\nTITLE: Building and Installing MediaPipe from Source\nDESCRIPTION: Builds the MediaPipe Python package from source code and installs it with OpenCV linking.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n(mp_env)mediapipe$ python3 setup.py install --link-opencv\n```\n\n----------------------------------------\n\nTITLE: Python Implementation of MediaPipe Selfie Segmentation\nDESCRIPTION: Python code demonstrating how to use MediaPipe Selfie Segmentation for both static images and webcam input. Includes image processing, mask generation, and visualization functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/selfie_segmentation.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nmp_drawing = mp.solutions.drawing_utils\nmp_selfie_segmentation = mp.solutions.selfie_segmentation\n\n# For static images:\nIMAGE_FILES = []\nBG_COLOR = (192, 192, 192) # gray\nMASK_COLOR = (255, 255, 255) # white\nwith mp_selfie_segmentation.SelfieSegmentation(\n    model_selection=0) as selfie_segmentation:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    image_height, image_width, _ = image.shape\n    # Convert the BGR image to RGB before processing.\n    results = selfie_segmentation.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Draw selfie segmentation on the background image.\n    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n    fg_image = np.zeros(image.shape, dtype=np.uint8)\n    fg_image[:] = MASK_COLOR\n    bg_image = np.zeros(image.shape, dtype=np.uint8)\n    bg_image[:] = BG_COLOR\n    output_image = np.where(condition, fg_image, bg_image)\n    cv2.imwrite('/tmp/selfie_segmentation_output' + str(idx) + '.png', output_image)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using MediaPipe Objectron in Python\nDESCRIPTION: This code demonstrates how to use the MediaPipe Objectron solution in Python for both static images and webcam input. It shows how to configure the Objectron object, process images, and visualize the detected 3D bounding boxes.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport cv2\nimport mediapipe as mp\nmp_drawing = mp.solutions.drawing_utils\nmp_objectron = mp.solutions.objectron\n\n# For static images:\nIMAGE_FILES = []\nwith mp_objectron.Objectron(static_image_mode=True,\n                            max_num_objects=5,\n                            min_detection_confidence=0.5,\n                            model_name='Shoe') as objectron:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    # Convert the BGR image to RGB and process it with MediaPipe Objectron.\n    results = objectron.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Draw box landmarks.\n    if not results.detected_objects:\n      print(f'No box landmarks detected on {file}')\n      continue\n    print(f'Box landmarks of {file}:')\n    annotated_image = image.copy()\n    for detected_object in results.detected_objects:\n      mp_drawing.draw_landmarks(\n          annotated_image, detected_object.landmarks_2d, mp_objectron.BOX_CONNECTIONS)\n      mp_drawing.draw_axis(annotated_image, detected_object.rotation,\n                           detected_object.translation)\n      cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n\n# For webcam input:\ncap = cv2.VideoCapture(0)\nwith mp_objectron.Objectron(static_image_mode=False,\n                            max_num_objects=5,\n                            min_detection_confidence=0.5,\n                            min_tracking_confidence=0.99,\n                            model_name='Shoe') as objectron:\n  while cap.isOpened():\n    success, image = cap.read()\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = objectron.process(image)\n\n    # Draw the box landmarks on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    if results.detected_objects:\n        for detected_object in results.detected_objects:\n            mp_drawing.draw_landmarks(\n              image, detected_object.landmarks_2d, mp_objectron.BOX_CONNECTIONS)\n            mp_drawing.draw_axis(image, detected_object.rotation,\n                                 detected_object.translation)\n    # Flip the image horizontally for a selfie-view display.\n    cv2.imshow('MediaPipe Objectron', cv2.flip(image, 1))\n    if cv2.waitKey(5) & 0xFF == 27:\n      break\ncap.release()\n```\n\n----------------------------------------\n\nTITLE: Implementing Image Input for Face Detection in Android\nDESCRIPTION: Implements face detection for static images from gallery with custom ImageView rendering. Includes image selection via ActivityResultLauncher, bitmap processing, and detection result handling with pixel coordinate conversion.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_detection.md#2025-04-23_snippet_3\n\nLANGUAGE: java\nCODE:\n```\n// For reading images from gallery and drawing the output in an ImageView.\nFaceDetectionOptions faceDetectionOptions =\n    FaceDetectionOptions.builder()\n        .setStaticImageMode(true)\n        .setModelSelection(0).build();\nFaceDetection faceDetection = new FaceDetection(this, faceDetectionOptions);\n\n// Connects MediaPipe Face Detection Solution to the user-defined ImageView\n// instance that allows users to have the custom drawing of the output landmarks\n// on it. See mediapipe/examples/android/solutions/facedetection/src/main/java/com/google/mediapipe/examples/facedetection/FaceDetectionResultImageView.java\n// as an example.\nFaceDetectionResultImageView imageView = new FaceDetectionResultImageView(this);\nfaceDetection.setResultListener(\n    faceDetectionResult -> {\n      if (faceDetectionResult.multiFaceDetections().isEmpty()) {\n        return;\n      }\n      int width = faceDetectionResult.inputBitmap().getWidth();\n      int height = faceDetectionResult.inputBitmap().getHeight();\n      RelativeKeypoint noseTip =\n          faceDetectionResult\n              .multiFaceDetections()\n              .get(0)\n              .getLocationData()\n              .getRelativeKeypoints(FaceKeypoint.NOSE_TIP);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Face Detection nose tip coordinates (pixel values): x=%f, y=%f\",\n              noseTip.getX() * width, noseTip.getY() * height));\n      // Request canvas drawing.\n      imageView.setFaceDetectionResult(faceDetectionResult);\n      runOnUiThread(() -> imageView.update());\n    });\nfaceDetection.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Face Detection error:\" + message));\n\n// ActivityResultLauncher to get an image from the gallery as Bitmap.\nActivityResultLauncher<Intent> imageGetter =\n    registerForActivityResult(\n        new ActivityResultContracts.StartActivityForResult(),\n        result -> {\n          Intent resultIntent = result.getData();\n          if (resultIntent != null && result.getResultCode() == RESULT_OK) {\n            Bitmap bitmap = null;\n            try {\n              bitmap =\n                  MediaStore.Images.Media.getBitmap(\n                      this.getContentResolver(), resultIntent.getData());\n              // Please also rotate the Bitmap based on its orientation.\n            } catch (IOException e) {\n              Log.e(TAG, \"Bitmap reading error:\" + e);\n            }\n            if (bitmap != null) {\n              faceDetection.send(bitmap);\n            }\n          }\n        });\nIntent pickImageIntent = new Intent(Intent.ACTION_PICK);\npickImageIntent.setDataAndType(MediaStore.Images.Media.INTERNAL_CONTENT_URI, \"image/*\");\nimageGetter.launch(pickImageIntent);\n```\n\n----------------------------------------\n\nTITLE: Configuring MediaPipe Hands in Python\nDESCRIPTION: Example of how to configure the MediaPipe Hands solution with various parameters like static_image_mode, max_num_hands, model_complexity, min_detection_confidence, and min_tracking_confidence.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport mediapipe as mp\n\nmp_hands = mp.solutions.hands\nhands = mp_hands.Hands(\n    static_image_mode=False,\n    max_num_hands=2,\n    model_complexity=1,\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5\n)\n```\n\n----------------------------------------\n\nTITLE: Defining MediaPipe Graph for GPU Sobel Edge Detection\nDESCRIPTION: This MediaPipe graph defines the processing pipeline for performing GPU-based Sobel edge detection on a live video stream. It consists of two nodes: LuminanceCalculator for converting RGB images to luminance, and SobelEdgesCalculator for applying the Sobel filter to detect edges.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\n# MediaPipe graph that performs GPU Sobel edge detection on a live video stream.\n# Used in the examples\n# mediapipe/examples/android/src/java/com/google/mediapipe/apps/basic:helloworld\n# and mediapipe/examples/ios/helloworld.\n\n# Images coming into and out of the graph.\ninput_stream: \"input_video\"\noutput_stream: \"output_video\"\n\n# Converts RGB images into luminance images, still stored in RGB format.\nnode: {\n  calculator: \"LuminanceCalculator\"\n  input_stream: \"input_video\"\n  output_stream: \"luma_video\"\n}\n\n# Applies the Sobel filter to luminance images stored in RGB format.\nnode: {\n  calculator: \"SobelEdgesCalculator\"\n  input_stream: \"luma_video\"\n  output_stream: \"output_video\"\n}\n```\n\n----------------------------------------\n\nTITLE: Calculator Graph Configuration in Protocol Buffers\nDESCRIPTION: Example of a calculator graph configuration using Protocol Buffers. This shows how to connect calculators by defining nodes with input and output streams using tags to identify connections between processors.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_1\n\nLANGUAGE: proto\nCODE:\n```\n# Graph describing calculator SomeAudioVideoCalculator\nnode {\n  calculator: \"SomeAudioVideoCalculator\"\n  input_stream: \"INPUT:combined_input\"\n  output_stream: \"VIDEO:video_stream\"\n}\nnode {\n  calculator: \"SomeVideoCalculator\"\n  input_stream: \"VIDEO_IN:video_stream\"\n  output_stream: \"VIDEO_OUT:processed_video\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe with TensorFlow GPU Support\nDESCRIPTION: Complete bazel build command for MediaPipe with TensorFlow GPU support, including CUDA configuration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/gpu_support.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config=cuda --spawn_strategy=local \\\n    --define no_aws_support=true --copt -DMESA_EGL_NO_X11_HEADERS \\\n    mediapipe/examples/desktop/object_detection:object_detection_tensorflow\n```\n\n----------------------------------------\n\nTITLE: Initializing Face Stylizer with MediaPipe\nDESCRIPTION: This code demonstrates how to initialize a Face Stylizer using MediaPipe Tasks Vision. It loads the vision tasks, creates a face stylizer from a model path, and applies stylization to a face in an image element. The stylizer returns a modified version of the image with stylistic effects applied to faces.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst faceStylizer = await FaceStylizer.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/face_stylizer/blaze_face_stylizer/float32/1/blaze_face_stylizer.task\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst stylizedImage = faceStylizer.stylize(image);\n```\n\n----------------------------------------\n\nTITLE: Initializing Image Embedder with MediaPipe\nDESCRIPTION: This code shows how to initialize an Image Embedder using MediaPipe Tasks Vision. It loads the vision tasks, creates an image embedder from a model path, and extracts embeddings from an image element. The embedder produces vector representations of images for similarity comparisons.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst imageEmbedder = await ImageEmbedder.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/image_embedder/mobilenet_v3_small/float32/1/mobilenet_v3_small.tflite\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst embeddings = imageSegmenter.embed(image);\n```\n\n----------------------------------------\n\nTITLE: JavaScript Implementation of MediaPipe Selfie Segmentation\nDESCRIPTION: JavaScript code for implementing real-time selfie segmentation using MediaPipe, including camera setup and result processing.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/selfie_segmentation.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\n\nfunction onResults(results) {\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(results.segmentationMask, 0, 0,\n                      canvasElement.width, canvasElement.height);\n\n  // Only overwrite existing pixels.\n  canvasCtx.globalCompositeOperation = 'source-in';\n  canvasCtx.fillStyle = '#00FF00';\n  canvasCtx.fillRect(0, 0, canvasElement.width, canvasElement.height);\n\n  // Only overwrite missing pixels.\n  canvasCtx.globalCompositeOperation = 'destination-atop';\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n\n  canvasCtx.restore();\n}\n\nconst selfieSegmentation = new SelfieSegmentation({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/${file}`;\n}});\nselfieSegmentation.setOptions({\n  modelSelection: 1,\n});\nselfieSegmentation.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await selfieSegmentation.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n```\n\n----------------------------------------\n\nTITLE: Building a Graph Configuration with C++ Graph Builder\nDESCRIPTION: A C++ function that constructs the equivalent CalculatorGraphConfig for the inference graph shown in the protocol buffer example. It demonstrates connecting streams and side packets between graph inputs, calculator nodes, and graph outputs.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Graph inputs.\n  Stream<std::vector<Tensor>> input_tensors =\n      graph.In(0).SetName(\"input_tensors\").Cast<std::vector<Tensor>>();\n  SidePacket<TfLiteModelPtr> model =\n      graph.SideIn(0).SetName(\"model\").Cast<TfLiteModelPtr>();\n\n  auto& inference_node = graph.AddNode(\"InferenceCalculator\");\n  auto& inference_opts =\n      inference_node.GetOptions<InferenceCalculatorOptions>();\n  // Requesting GPU delegate.\n  inference_opts.mutable_delegate()->mutable_gpu();\n  input_tensors.ConnectTo(inference_node.In(\"TENSORS\"));\n  model.ConnectTo(inference_node.SideIn(\"MODEL\"));\n  Stream<std::vector<Tensor>> output_tensors =\n      inference_node.Out(\"TENSORS\").Cast<std::vector<Tensor>>();\n\n  // Graph outputs.\n  output_tensors.SetName(\"output_tensors\").ConnectTo(graph.Out(0));\n\n  // Get `CalculatorGraphConfig` to pass it into `CalculatorGraph`\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a MediaPipe Graph in Protobuf\nDESCRIPTION: This snippet shows how to define a simple MediaPipe graph using CalculatorGraphConfig in protobuf format. It creates a series of PassThroughCalculators connected in sequence.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_0\n\nLANGUAGE: proto\nCODE:\n```\n# This graph named main_pass_throughcals_nosubgraph.pbtxt contains 4\n# passthrough calculators.\ninput_stream: \"in\"\noutput_stream: \"out\"\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"in\"\n    output_stream: \"out1\"\n}\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"out1\"\n    output_stream: \"out2\"\n}\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"out2\"\n    output_stream: \"out3\"\n}\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"out3\"\n    output_stream: \"out\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing MediaPipe Hands with Image Input in Android\nDESCRIPTION: Configures MediaPipe Hands to process static images from the device gallery. This code sets up the hands detector in static image mode, creates a custom image view for result visualization, and registers an activity launcher to select images from the gallery.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_6\n\nLANGUAGE: java\nCODE:\n```\n// For reading images from gallery and drawing the output in an ImageView.\nHandsOptions handsOptions =\n    HandsOptions.builder()\n        .setStaticImageMode(true)\n        .setMaxNumHands(2)\n        .setRunOnGpu(true).build();\nHands hands = new Hands(this, handsOptions);\n\n// Connects MediaPipe Hands Solution to the user-defined ImageView instance that\n// allows users to have the custom drawing of the output landmarks on it.\n// See mediapipe/examples/android/solutions/hands/src/main/java/com/google/mediapipe/examples/hands/HandsResultImageView.java\n// as an example.\nHandsResultImageView imageView = new HandsResultImageView(this);\nhands.setResultListener(\n    handsResult -> {\n      if (result.multiHandLandmarks().isEmpty()) {\n        return;\n      }\n      int width = handsResult.inputBitmap().getWidth();\n      int height = handsResult.inputBitmap().getHeight();\n      NormalizedLandmark wristLandmark =\n          handsResult.multiHandLandmarks().get(0).getLandmarkList().get(HandLandmark.WRIST);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Hand wrist coordinates (pixel values): x=%f, y=%f\",\n              wristLandmark.getX() * width, wristLandmark.getY() * height));\n      // Request canvas drawing.\n      imageView.setHandsResult(handsResult);\n      runOnUiThread(() -> imageView.update());\n    });\nhands.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Hands error:\" + message));\n\n// ActivityResultLauncher to get an image from the gallery as Bitmap.\nActivityResultLauncher<Intent> imageGetter =\n    registerForActivityResult(\n        new ActivityResultContracts.StartActivityForResult(),\n        result -> {\n          Intent resultIntent = result.getData();\n          if (resultIntent != null && result.getResultCode() == RESULT_OK) {\n            Bitmap bitmap = null;\n            try {\n              bitmap =\n                  MediaStore.Images.Media.getBitmap(\n                      this.getContentResolver(), resultIntent.getData());\n              // Please also rotate the Bitmap based on its orientation.\n            } catch (IOException e) {\n              Log.e(TAG, \"Bitmap reading error:\" + e);\n            }\n            if (bitmap != null) {\n              hands.send(bitmap);\n            }\n          }\n        });\nIntent pickImageIntent = new Intent(Intent.ACTION_PICK);\npickImageIntent.setDataAndType(MediaStore.Images.Media.INTERNAL_CONTENT_URI, \"image/*\");\nimageGetter.launch(pickImageIntent);\n```\n\n----------------------------------------\n\nTITLE: Adding Bounding Box Data in C++\nDESCRIPTION: C++ example demonstrating how to initialize a sequence and add bounding box data with timestamps, labels and tracking IDs. Shows usage of MediaSequence's C++ API for object tracking tasks.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\n// C++: functions from media_sequence.h\ntensorflow::SequenceExample sequence;\nSetClipDataPath(\"path_to_video\", &sequence);\nSetClipStartTimestamp(1000000, &sequence);\nSetClipEndTimestamp(6000000, &sequence);\n\n// For an object tracking task with action labels:\nstd::vector<mediapipe::Location> locations_on_frame_1;\nAddBBox(locations_on_frame_1, &sequence);\nAddBBoxTimestamp(3000000, &sequence);\nAddBBoxLabelIndex({4, 3}, &sequence);\nAddBBoxLabelString({\"run\", \"jump\"}, &sequence);\nAddBBoxTrackString({\"id_0\", \"id_1\"}, &sequence);\n// AddBBoxClassString({\"cls_0\", \"cls_0\"}, &sequence); // if required\nstd::vector<mediapipe::Location> locations_on_frame_2;\nAddBBox(locations_on_frame_2, &sequence);\nAddBBoxTimestamp(5000000, &sequence);\nAddBBoxLabelIndex({3}, &sequence);\nAddBBoxLabelString({\"jump\"}, &sequence);\nAddBBoxTrackString({\"id_0\"}, &sequence);\n// AddBBoxClassString({\"cls_0\"}, &sequence); // if required\n```\n\n----------------------------------------\n\nTITLE: Configuring MediaPipe Edge Detection Graph in Protobuf\nDESCRIPTION: Defines a MediaPipe graph for GPU Sobel edge detection that processes video input through luminance calculation and edge detection nodes.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\n# MediaPipe graph that performs GPU Sobel edge detection on a live video stream.\n# Used in the examples in\n# mediapipe/examples/android/src/java/com/mediapipe/apps/basic and\n# mediapipe/examples/ios/edgedetectiongpu.\n\n# Images coming into and out of the graph.\ninput_stream: \"input_video\"\noutput_stream: \"output_video\"\n\n# Converts RGB images into luminance images, still stored in RGB format.\nnode: {\n  calculator: \"LuminanceCalculator\"\n  input_stream: \"input_video\"\n  output_stream: \"luma_video\"\n}\n\n# Applies the Sobel filter to luminance images stored in RGB format.\nnode: {\n  calculator: \"SobelEdgesCalculator\"\n  input_stream: \"luma_video\"\n  output_stream: \"output_video\"\n}\n```\n\n----------------------------------------\n\nTITLE: Calculator Graph with Multiple Input/Output Streams\nDESCRIPTION: Example of a more complex calculator graph configuration showing how to handle multiple inputs and outputs with different tags and indices, demonstrating stream identification methods in MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_2\n\nLANGUAGE: proto\nCODE:\n```\n# Graph describing calculator SomeAudioVideoCalculator\nnode {\n  calculator: \"SomeAudioVideoCalculator\"\n  input_stream: \"combined_input\"\n  output_stream: \"VIDEO:video_stream\"\n  output_stream: \"AUDIO:0:audio_left\"\n  output_stream: \"AUDIO:1:audio_right\"\n}\n\nnode {\n  calculator: \"SomeAudioCalculator\"\n  input_stream: \"audio_left\"\n  input_stream: \"audio_right\"\n  output_stream: \"audio_energy\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a MediaPipe Graph in C++\nDESCRIPTION: This snippet demonstrates how to define the same MediaPipe graph using C++ representation. It creates a series of PassThroughCalculators using a lambda function.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraphConfig() {\n  Graph graph;\n\n  // Graph inputs\n  Stream<AnyType> in = graph.In(0).SetName(\"in\");\n\n  auto pass_through_fn = [](Stream<AnyType> in,\n                            Graph& graph) -> Stream<AnyType> {\n    auto& node = graph.AddNode(\"PassThroughCalculator\");\n    in.ConnectTo(node.In(0));\n    return node.Out(0);\n  };\n\n  Stream<AnyType> out1 = pass_through_fn(in, graph);\n  Stream<AnyType> out2 = pass_through_fn(out1, graph);\n  Stream<AnyType> out3 = pass_through_fn(out2, graph);\n  Stream<AnyType> out4 = pass_through_fn(out3, graph);\n\n  // Graph outputs\n  out4.SetName(\"out\").ConnectTo(graph.Out(0));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Pose Landmarker with MediaPipe\nDESCRIPTION: This code shows how to initialize a Pose Landmarker using MediaPipe Tasks Vision. It loads the vision tasks, creates a pose landmarker from a model path, and performs body pose landmark detection on an image element. The landmarker returns key points of the body for posture analysis and visual rendering.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_11\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst poseLandmarker = await PoseLandmarker.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst landmarks = poseLandmarker.detect(image);\n```\n\n----------------------------------------\n\nTITLE: Implementing MediaPipe Pose Tracking in JavaScript\nDESCRIPTION: JavaScript implementation for MediaPipe pose tracking that processes video input and visualizes pose landmarks on a canvas. It configures the pose model with specific options, handles camera input, and renders pose landmarks with connections. The code also includes segmentation mask visualization.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/pose.md#2025-04-23_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n<script type=\"module\">\nconst videoElement = document.getElementsByClassName('input_video')[0];\nconst canvasElement = document.getElementsByClassName('output_canvas')[0];\nconst canvasCtx = canvasElement.getContext('2d');\nconst landmarkContainer = document.getElementsByClassName('landmark-grid-container')[0];\nconst grid = new LandmarkGrid(landmarkContainer);\n\nfunction onResults(results) {\n  if (!results.poseLandmarks) {\n    grid.updateLandmarks([]);\n    return;\n  }\n\n  canvasCtx.save();\n  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);\n  canvasCtx.drawImage(results.segmentationMask, 0, 0,\n                      canvasElement.width, canvasElement.height);\n\n  // Only overwrite existing pixels.\n  canvasCtx.globalCompositeOperation = 'source-in';\n  canvasCtx.fillStyle = '#00FF00';\n  canvasCtx.fillRect(0, 0, canvasElement.width, canvasElement.height);\n\n  // Only overwrite missing pixels.\n  canvasCtx.globalCompositeOperation = 'destination-atop';\n  canvasCtx.drawImage(\n      results.image, 0, 0, canvasElement.width, canvasElement.height);\n\n  canvasCtx.globalCompositeOperation = 'source-over';\n  drawConnectors(canvasCtx, results.poseLandmarks, POSE_CONNECTIONS,\n                 {color: '#00FF00', lineWidth: 4});\n  drawLandmarks(canvasCtx, results.poseLandmarks,\n                {color: '#FF0000', lineWidth: 2});\n  canvasCtx.restore();\n\n  grid.updateLandmarks(results.poseWorldLandmarks);\n}\n\nconst pose = new Pose({locateFile: (file) => {\n  return `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`;\n}});\npose.setOptions({\n  modelComplexity: 1,\n  smoothLandmarks: true,\n  enableSegmentation: true,\n  smoothSegmentation: true,\n  minDetectionConfidence: 0.5,\n  minTrackingConfidence: 0.5\n});\npose.onResults(onResults);\n\nconst camera = new Camera(videoElement, {\n  onFrame: async () => {\n    await pose.send({image: videoElement});\n  },\n  width: 1280,\n  height: 720\n});\ncamera.start();\n</script>\n```\n\n----------------------------------------\n\nTITLE: Specifying MediaPipe Project Dependencies\nDESCRIPTION: A requirements file listing all necessary Python packages for the MediaPipe project with specific version constraints to ensure compatibility. The file includes TensorFlow and its ecosystem packages, along with support libraries like NumPy and OpenCV.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/model_maker/requirements_bazel.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py\nnumpy<2\nopencv-python\nsetuptools==70.3.0 # needed due to https://github.com/pypa/setuptools/issues/4487\ntensorflow>=2.10,<2.16\ntensorflow-addons\ntensorflow-datasets\ntensorflow-hub\ntensorflow-model-optimization<0.8.0\ntensorflow-text\ntf-models-official>=2.13.2,<2.16.0\n```\n\n----------------------------------------\n\nTITLE: Adding Bounding Box Data in Python\nDESCRIPTION: Example showing how to add bounding box data including locations, timestamps, labels and tracking IDs to a sequence in Python. Demonstrates usage of MediaSequence's Python API for object tracking tasks.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# ms.add_bbox_class_string((\"cls_0\", \"cls_0\"), sequence)  # if required\nlocations_on_frame_2 = locations_on_frame_1[0]\nms.add_bbox(locations_on_frame_2, sequence)\nms.add_bbox_timestamp(5000000, sequence)\nms.add_bbox_label_index((3), sequence)\nms.add_bbox_label_string((b\"jump\",), sequence)\nms.add_bbox_track_string((b\"id_0\",), sequence)\n# ms.add_bbox_class_string((\"cls_0\",), sequence)  # if required\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running MediaPipe Graph in C++\nDESCRIPTION: This snippet illustrates how to initialize a CalculatorGraph, set up an OutputStreamPoller, and start the graph run. It's a crucial step in setting up the MediaPipe graph for execution.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_cpp.md#2025-04-23_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraph graph;\nMP_RETURN_IF_ERROR(graph.Initialize(config));\nMP_ASSIGN_OR_RETURN(OutputStreamPoller poller,\n                    graph.AddOutputStreamPoller(\"out\"));\nMP_RETURN_IF_ERROR(graph.StartRun({}));\n```\n\n----------------------------------------\n\nTITLE: Defining MediaPipe Graph Configuration in C++\nDESCRIPTION: This snippet demonstrates how to define a simple MediaPipe graph using the CalculatorGraphConfig proto. The graph consists of two PassThroughCalculators connected in series, with one input stream and one output stream.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_cpp.md#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nabsl::Status PrintHelloWorld() {\n  // Configures a simple graph, which concatenates 2 PassThroughCalculators.\n  CalculatorGraphConfig config = ParseTextProtoOrDie<CalculatorGraphConfig>(R\"(\n    input_stream: \"in\"\n    output_stream: \"out\"\n    node {\n      calculator: \"PassThroughCalculator\"\n      input_stream: \"in\"\n      output_stream: \"out1\"\n    }\n    node {\n      calculator: \"PassThroughCalculator\"\n      input_stream: \"out1\"\n      output_stream: \"out\"\n    }\n  )\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Camera Start with CameraXPreviewHelper\nDESCRIPTION: This Java code demonstrates how to start the camera using MediaPipe's CameraXPreviewHelper and set up a listener for when the camera is started.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\npublic void startCamera() {\n  cameraHelper = new CameraXPreviewHelper();\n  cameraHelper.setOnCameraStartedListener(\n    surfaceTexture -> {\n      previewFrameTexture = surfaceTexture;\n      // Make the display view visible to start showing the preview.\n      previewDisplayView.setVisibility(View.VISIBLE);\n    });\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple Inference Steps with Reusable Function\nDESCRIPTION: An example showing how to chain multiple model inference steps using the reusable RunInference function. This demonstrates the clear advantage of code reuse for complex graph construction.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n  // Run first inference.\n  Stream<std::vector<Tensor>> output_tensors =\n      RunInference(input_tensors, model, delegate, graph);\n  // Run second inference on the output of the first one.\n  Stream<std::vector<Tensor>> extra_output_tensors =\n      RunInference(output_tensors, extra_model, delegate, graph);\n```\n\n----------------------------------------\n\nTITLE: Implementing Video Input for Face Detection in Android\nDESCRIPTION: Sets up face detection for video files with OpenGL rendering. Handles video selection via ActivityResultLauncher, configures video input processing, and manages detection results with GL surface view updates.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_detection.md#2025-04-23_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n// For video input and result rendering with OpenGL.\nFaceDetectionOptions faceDetectionOptions =\n    FaceDetectionOptions.builder()\n        .setStaticImageMode(false)\n        .setModelSelection(0).build();\nFaceDetection faceDetection = new FaceDetection(this, faceDetectionOptions);\nfaceDetection.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Face Detection error:\" + message));\n\n// Initializes a new VideoInput instance and connects it to MediaPipe Face Detection Solution.\nVideoInput videoInput = new VideoInput(this);\nvideoInput.setNewFrameListener(\n    textureFrame -> faceDetection.send(textureFrame));\n\n// Initializes a new GlSurfaceView with a ResultGlRenderer<FaceDetectionResult> instance\n// that provides the interfaces to run user-defined OpenGL rendering code.\n// See mediapipe/examples/android/solutions/facedetection/src/main/java/com/google/mediapipe/examples/facedetection/FaceDetectionResultGlRenderer.java\n// as an example.\nSolutionGlSurfaceView<FaceDetectionResult> glSurfaceView =\n    new SolutionGlSurfaceView<>(\n        this, faceDetection.getGlContext(), faceDetection.getGlMajorVersion());\nglSurfaceView.setSolutionResultRenderer(new FaceDetectionResultGlRenderer());\nglSurfaceView.setRenderInputImage(true);\n\nfaceDetection.setResultListener(\n    faceDetectionResult -> {\n      if (faceDetectionResult.multiFaceDetections().isEmpty()) {\n        return;\n      }\n      RelativeKeypoint noseTip =\n          faceDetectionResult\n              .multiFaceDetections()\n              .get(0)\n              .getLocationData()\n              .getRelativeKeypoints(FaceKeypoint.NOSE_TIP);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Face Detection nose tip normalized coordinates (value range: [0, 1]): x=%f, y=%f\",\n              noseTip.getX(), noseTip.getY()));\n      // Request GL rendering.\n      glSurfaceView.setRenderData(faceDetectionResult);\n      glSurfaceView.requestRender();\n    });\n\nActivityResultLauncher<Intent> videoGetter =\n    registerForActivityResult(\n        new ActivityResultContracts.StartActivityForResult(),\n        result -> {\n          Intent resultIntent = result.getData();\n          if (resultIntent != null) {\n            if (result.getResultCode() == RESULT_OK) {\n              glSurfaceView.post(\n                  () ->\n                      videoInput.start(\n                          this,\n                          resultIntent.getData(),\n                          faceDetection.getGlContext(),\n                          glSurfaceView.getWidth(),\n                          glSurfaceView.getHeight()));\n            }\n          }\n        });\nIntent pickVideoIntent = new Intent(Intent.ACTION_PICK);\npickVideoIntent.setDataAndType(MediaStore.Video.Media.INTERNAL_CONTENT_URI, \"video/*\");\nvideoGetter.launch(pickVideoIntent);\n```\n\n----------------------------------------\n\nTITLE: Importing MediaPipe and Using Face Mesh Solution\nDESCRIPTION: Demonstrates how to import the MediaPipe package and initialize the Face Mesh solution.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport mediapipe as mp\nmp_face_mesh = mp.solutions.face_mesh\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Printing Output from MediaPipe Graph in C++\nDESCRIPTION: This snippet shows how to use an OutputStreamPoller to retrieve packets from the graph's output stream. It iterates through the output packets, extracts the string content, and prints it to the output log.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_cpp.md#2025-04-23_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\nmediapipe::Packet packet;\nwhile (poller.Next(&packet)) {\n  ABSL_LOG(INFO) << packet.Get<string>();\n}\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Graph for Video Frame Extraction\nDESCRIPTION: A MediaPipe graph configuration that processes video data by extracting frames, encoding them, and storing them in SequenceExample format. The graph includes options for frame rate and encoding quality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Convert the string input into a decoded SequenceExample.\nnode {\n  calculator: \"StringToSequenceExampleCalculator\"\n  input_side_packet: \"STRING:input_sequence_example\"\n  output_side_packet: \"SEQUENCE_EXAMPLE:parsed_sequence_example\"\n}\n\n# Unpack the data path and clip timing from the SequenceExample.\nnode {\n  calculator: \"UnpackMediaSequenceCalculator\"\n  input_side_packet: \"SEQUENCE_EXAMPLE:parsed_sequence_example\"\n  output_side_packet: \"DATA_PATH:input_video_path\"\n  output_side_packet: \"RESAMPLER_OPTIONS:packet_resampler_options\"\n  options {\n    [type.googleapis.com/mediapipe.UnpackMediaSequenceCalculatorOptions]: {\n      base_packet_resampler_options {\n        frame_rate: 24.0\n        base_timestamp: 0\n      }\n    }\n  }\n}\n\n# Decode the entire video.\nnode {\n  calculator: \"OpenCvVideoDecoderCalculator\"\n  input_side_packet: \"INPUT_FILE_PATH:input_video_path\"\n  output_stream: \"VIDEO:decoded_frames\"\n}\n\n# Extract the subset of frames we want to keep.\nnode {\n  calculator: \"PacketResamplerCalculator\"\n  input_stream: \"decoded_frames\"\n  output_stream: \"sampled_frames\"\n  input_side_packet: \"OPTIONS:packet_resampler_options\"\n}\n\n# Encode the images to store in the SequenceExample.\nnode {\n  calculator: \"OpenCvImageEncoderCalculator\"\n  input_stream: \"sampled_frames\"\n  output_stream: \"encoded_frames\"\n  node_options {\n    [type.googleapis.com/mediapipe.OpenCvImageEncoderCalculatorOptions]: {\n      quality: 80\n    }\n  }\n}\n\n# Store the images in the SequenceExample.\nnode {\n  calculator: \"PackMediaSequenceCalculator\"\n  input_side_packet: \"SEQUENCE_EXAMPLE:parsed_sequence_example\"\n  output_side_packet: \"SEQUENCE_EXAMPLE:sequence_example_to_serialize\"\n  input_stream: \"IMAGE:encoded_frames\"\n}\n\n# Serialize the SequenceExample to a string for storage.\nnode {\n  calculator: \"StringToSequenceExampleCalculator\"\n  input_side_packet: \"SEQUENCE_EXAMPLE:sequence_example_to_serialize\"\n  output_side_packet: \"STRING:output_sequence_example\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Packets to MediaPipe Graph Input Stream in C++\nDESCRIPTION: This code demonstrates how to create and add packets to the graph's input stream. It creates 10 packets, each containing the string \"Hello World!\" with timestamps from 0 to 9, and adds them to the graph.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_cpp.md#2025-04-23_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nfor (int i = 0; i < 10; ++i) {\n  MP_RETURN_IF_ERROR(graph.AddPacketToInputStream(\"in\",\n                     MakePacket<std::string>(\"Hello World!\").At(Timestamp(i))));\n}\nMP_RETURN_IF_ERROR(graph.CloseInputStream(\"in\"));\n```\n\n----------------------------------------\n\nTITLE: Reusable Node Construction Functions in C++\nDESCRIPTION: This snippet shows how to create reusable functions for node construction, further improving code modularity and reusability across different graphs.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nStream<B> RunCalculator1(Stream<A> a, Graph& graph) {\n  auto& node = graph.AddNode(\"Calculator1\");\n  a.ConnectTo(node.In(\"INPUT\"));\n  return node.Out(\"OUTPUT\").Cast<B>();\n}\n\nStream<C> RunCalculator2(Stream<B> b, Graph& graph) {\n  auto& node = graph.AddNode(\"Calculator2\");\n  b.ConnectTo(node.In(\"INPUT\"));\n  return node.Out(\"OUTPUT\").Cast<C>();\n}\n\nStream<D> RunCalculator3(Stream<B> b, Stream<C> c, Graph& graph) {\n  auto& node = graph.AddNode(\"Calculator3\");\n  b.ConnectTo(node.In(\"INPUT_B\"));\n  c.ConnectTo(node.In(\"INPUT_C\"));\n  return node.Out(\"OUTPUT\").Cast<D>();\n}\n\nStream<E> RunCalculator4(Stream<B> b, Stream<C> c, Stream<D> d, Graph& graph) {\n  auto& node = graph.AddNode(\"Calculator4\");\n  b.ConnectTo(node.In(\"INPUT_B\"));\n  c.ConnectTo(node.In(\"INPUT_C\"));\n  d.ConnectTo(node.In(\"INPUT_D\"));\n  return node.Out(\"OUTPUT\").Cast<E>();\n}\n\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).Cast<A>();\n\n  Stream<B> b = RunCalculator1(a, graph);\n  Stream<C> c = RunCalculator2(b, graph);\n  Stream<D> d = RunCalculator3(b, c, graph);\n  Stream<E> e = RunCalculator4(b, c, d, graph);\n\n  // Outputs.\n  b.SetName(\"b\").ConnectTo(graph.Out(0));\n  c.SetName(\"c\").ConnectTo(graph.Out(1));\n  d.SetName(\"d\").ConnectTo(graph.Out(2));\n  e.SetName(\"e\").ConnectTo(graph.Out(3));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Starting MediaPipe Graph in Objective-C\nDESCRIPTION: Starts the MediaPipe graph after obtaining camera access permission.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_25\n\nLANGUAGE: Objective-C\nCODE:\n```\n[_cameraSource requestCameraAccessWithCompletionHandler:^void(BOOL granted) {\n  if (granted) {\n    // Start running self.mediapipeGraph.\n    NSError* error;\n    if (![self.mediapipeGraph startWithError:&error]) {\n      NSLog(@\"Failed to start graph: %@\", error);\n    }\n    else if (![self.mediapipeGraph waitUntilIdleWithError:&error]) {\n      NSLog(@\"Failed to complete graph initial run: %@\", error);\n    }\n\n    dispatch_async(_videoQueue, ^{\n      [_cameraSource start];\n    });\n  }\n}];\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Inference Graph with Protocol Buffers\nDESCRIPTION: A protocol buffer configuration sample that defines a simple inference graph with input/output streams, a model side packet, and an InferenceCalculator node configured to use GPU delegation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_0\n\nLANGUAGE: proto\nCODE:\n```\n# Graph inputs.\ninput_stream: \"input_tensors\"\ninput_side_packet: \"model\"\n\n# Graph outputs.\noutput_stream: \"output_tensors\"\n\nnode {\n  calculator: \"InferenceCalculator\"\n  input_stream: \"TENSORS:input_tensors\"\n  input_side_packet: \"MODEL:model\"\n  output_stream: \"TENSORS:output_tensors\"\n  node_options: {\n    [type.googleapis.com/mediapipe.InferenceCalculatorOptions] {\n      # Requesting GPU delegate.\n      delegate { gpu {} }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MediaPipe Face Mesh for Video Input in Android\nDESCRIPTION: This code snippet shows how to set up MediaPipe Face Mesh for processing video input in an Android app. It includes configuration for video input, OpenGL rendering, and result handling.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_mesh.md#2025-04-23_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nFaceMeshOptions faceMeshOptions =\n    FaceMeshOptions.builder()\n        .setStaticImageMode(false)\n        .setRefineLandmarks(true)\n        .setMaxNumFaces(1)\n        .setRunOnGpu(true).build();\nFaceMesh faceMesh = new FaceMesh(this, faceMeshOptions);\nfaceMesh.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Face Mesh error:\" + message));\n\nVideoInput videoInput = new VideoInput(this);\nvideoInput.setNewFrameListener(\n    textureFrame -> faceMesh.send(textureFrame));\n\nSolutionGlSurfaceView<FaceMeshResult> glSurfaceView =\n    new SolutionGlSurfaceView<>(\n        this, faceMesh.getGlContext(), faceMesh.getGlMajorVersion());\nglSurfaceView.setSolutionResultRenderer(new FaceMeshResultGlRenderer());\nglSurfaceView.setRenderInputImage(true);\n\nfaceMesh.setResultListener(\n    faceMeshResult -> {\n      NormalizedLandmark noseLandmark =\n          result.multiFaceLandmarks().get(0).getLandmarkList().get(1);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Face Mesh nose normalized coordinates (value range: [0, 1]): x=%f, y=%f\",\n              noseLandmark.getX(), noseLandmark.getY()));\n      glSurfaceView.setRenderData(faceMeshResult);\n      glSurfaceView.requestRender();\n    });\n\nActivityResultLauncher<Intent> videoGetter =\n    registerForActivityResult(\n        new ActivityResultContracts.StartActivityForResult(),\n        result -> {\n          Intent resultIntent = result.getData();\n          if (resultIntent != null) {\n            if (result.getResultCode() == RESULT_OK) {\n              glSurfaceView.post(\n                  () ->\n                      videoInput.start(\n                          this,\n                          resultIntent.getData(),\n                          faceMesh.getGlContext(),\n                          glSurfaceView.getWidth(),\n                          glSurfaceView.getHeight()));\n            }\n          }\n        });\nIntent pickVideoIntent = new Intent(Intent.ACTION_PICK);\npickVideoIntent.setDataAndType(MediaStore.Video.Media.INTERNAL_CONTENT_URI, \"video/*\");\nvideoGetter.launch(pickVideoIntent);\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Language Detector in MediaPipe (JavaScript)\nDESCRIPTION: This code demonstrates how to create a Language Detector instance using MediaPipe. It loads the necessary WASM modules via FilesetResolver, initializes a LanguageDetector with a pre-trained model, and detects the language of provided text data.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/text/README.md#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst text = await FilesetResolver.forTextTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-text/wasm\"\n);\nconst languageDetector = await LanguageDetector.createFromModelPath(text,\n    \"https://storage.googleapis.com/mediapipe-models/language_detector/language_detector/float32/1/language_detector.tflite\n);\nconst result = languageDetector.detect(textData);\n```\n\n----------------------------------------\n\nTITLE: Creating a Utility Class for PassThroughCalculator Node Building\nDESCRIPTION: A utility class implementation that simplifies the construction of PassThroughCalculator nodes by automatically managing stream connections and indexes. This class preserves type information and reduces error potential.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\nclass PassThroughNodeBuilder {\n public:\n  explicit PassThroughNodeBuilder(Graph& graph)\n      : node_(graph.AddNode(\"PassThroughCalculator\")) {}\n\n  template <typename T>\n  Stream<T> PassThrough(Stream<T> stream) {\n    stream.ConnectTo(node_.In(index_));\n    return node_.Out(index_++).Cast<T>();\n  }\n\n private:\n  int index_ = 0;\n  GenericNode& node_;\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing PacketClonerCalculator in C++\nDESCRIPTION: Complete implementation of PacketClonerCalculator showing GetContract(), Open(), and Process() methods for packet synchronization.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\n// This takes packets from N+1 streams, A_1, A_2, ..., A_N, B.\n// For every packet that appears in B, outputs the most recent packet from each\n// of the A_i on a separate stream.\n\n#include <vector>\n\n#include \"absl/strings/str_cat.h\"\n#include \"mediapipe/framework/calculator_framework.h\"\n\nnamespace mediapipe {\n\nclass PacketClonerCalculator : public CalculatorBase {\n public:\n  static absl::Status GetContract(CalculatorContract* cc) {\n    const int tick_signal_index = cc->Inputs().NumEntries() - 1;\n    for (int i = 0; i < tick_signal_index; ++i) {\n      cc->Inputs().Index(i).SetAny();\n      cc->Outputs().Index(i).SetSameAs(&cc->Inputs().Index(i));\n    }\n    cc->Inputs().Index(tick_signal_index).SetAny();\n    return absl::OkStatus();\n  }\n\n  absl::Status Open(CalculatorContext* cc) final {\n    tick_signal_index_ = cc->Inputs().NumEntries() - 1;\n    current_.resize(tick_signal_index_);\n    for (int i = 0; i < tick_signal_index_; ++i) {\n      if (!cc->Inputs().Index(i).Header().IsEmpty()) {\n        cc->Outputs().Index(i).SetHeader(cc->Inputs().Index(i).Header());\n      }\n    }\n    return absl::OkStatus();\n  }\n\n  absl::Status Process(CalculatorContext* cc) final {\n    for (int i = 0; i < tick_signal_index_; ++i) {\n      if (!cc->Inputs().Index(i).Value().IsEmpty()) {\n        current_[i] = cc->Inputs().Index(i).Value();\n      }\n    }\n\n    if (!cc->Inputs().Index(tick_signal_index_).Value().IsEmpty()) {\n      for (int i = 0; i < tick_signal_index_; ++i) {\n        if (!current_[i].IsEmpty()) {\n          cc->Outputs().Index(i).AddPacket(\n              current_[i].At(cc->InputTimestamp()));\n        } else {\n          cc->Outputs().Index(i).SetNextTimestampBound(\n              cc->InputTimestamp().NextAllowedInStream());\n        }\n      }\n    }\n    return absl::OkStatus();\n  }\n\n private:\n  std::vector<Packet> current_;\n  int tick_signal_index_;\n};\n\nREGISTER_CALCULATOR(PacketClonerCalculator);\n}  // namespace mediapipe\n```\n\n----------------------------------------\n\nTITLE: Converting Frozen Graph to TFLite Model\nDESCRIPTION: This command converts the exported frozen graph to a TFLite model using the tflite_convert tool. It specifies input and output details for the object detection model.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection_saved_model.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ tflite_convert --  \\\n  --graph_def_file=${PATH_TO_MODEL}/tflite_graph.pb \\\n  --output_file=${PATH_TO_MODEL}/model.tflite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shapes=1,320,320,3 \\\n  --input_arrays=normalized_input_image_tensor \\\n  --output_arrays=raw_outputs/box_encodings,raw_outputs/class_predictions\n```\n\n----------------------------------------\n\nTITLE: Setting up HTML Structure for MediaPipe Pose Tracking\nDESCRIPTION: HTML structure required to implement MediaPipe pose tracking. It includes necessary script imports for camera utilities, control utilities, drawing utilities, and the pose module. The body contains containers for video input, canvas output, and landmark visualization.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/pose.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils_3d/control_utils_3d.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n    <div class=\"landmark-grid-container\"></div>\n  </div>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Building Model Inference Binary in Bash\nDESCRIPTION: Command to build the MediaPipe binary for model inference.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define='MEDIAPIPE_DISABLE_GPU=1' --linkopt=-s \\\n  mediapipe/examples/desktop/youtube8m:model_inference\n```\n\n----------------------------------------\n\nTITLE: Implementing GetContract() Method for Type Specification\nDESCRIPTION: Example implementation of the GetContract() method for a calculator that processes audio and video. Shows how to define input and output stream types, and how to access streams by index, tag, or combination of both.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n// c++ Code snippet describing the SomeAudioVideoCalculator GetContract() method\nclass SomeAudioVideoCalculator : public CalculatorBase {\n public:\n  static absl::Status GetContract(CalculatorContract* cc) {\n    cc->Inputs().Index(0).SetAny();\n    // SetAny() is used to specify that whatever the type of the\n    // stream is, it's acceptable.  This does not mean that any\n    // packet is acceptable.  Packets in the stream still have a\n    // particular type.  SetAny() has the same effect as explicitly\n    // setting the type to be the stream's type.\n    cc->Outputs().Tag(\"VIDEO\").Set<ImageFrame>();\n    cc->Outputs().Get(\"AUDIO\", 0).Set<Matrix>();\n    cc->Outputs().Get(\"AUDIO\", 1).Set<Matrix>();\n    return absl::OkStatus();\n  }\n```\n\n----------------------------------------\n\nTITLE: Initializing Image Segmenter with MediaPipe\nDESCRIPTION: This code demonstrates how to initialize an Image Segmenter using MediaPipe Tasks Vision. It loads the vision tasks, creates an image segmenter from a model path, and performs segmentation on an image element. The segmenter returns masks categorizing different parts of the image.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst imageSegmenter = await ImageSegmenter.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/image_segmenter/deeplab_v3/float32/1/deeplab_v3.tflite\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nimageSegmenter.segment(image, (masks, width, height) => {\n  ...\n});\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Calculator Graph\nDESCRIPTION: Example showing how to start a graph run, add packets to input streams including text and image data, and properly close the graph.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngraph.start_run()\n\ngraph.add_packet_to_input_stream(\n    'in_stream', mp.packet_creator.create_string('abc').at(0))\n\nrgb_img = cv2.cvtColor(cv2.imread('/path/to/your/image.png'), cv2.COLOR_BGR2RGB)\ngraph.add_packet_to_input_stream(\n    'in_stream',\n    mp.packet_creator.create_image_frame(image_format=mp.ImageFormat.SRGB,\n                                         data=rgb_img).at(1))\n\ngraph.close()\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for MediaPipe Holistic Web Demo\nDESCRIPTION: This HTML snippet sets up the structure for a web demo using the MediaPipe Holistic model. It includes the necessary script imports and defines video and canvas elements for input and output.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md#2025-04-23_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n  </div>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Process Method in C++\nDESCRIPTION: Example of a basic Process() method implementation for a MediaPipe calculator. Shows how to handle input data, allocate memory for output, and return status.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nabsl::Status MyCalculator::Process() {\n  const Matrix& input = Input()->Get<Matrix>();\n  std::unique_ptr<Matrix> output(new Matrix(input.rows(), input.cols()));\n  // do your magic here....\n  //    output->row(n) =  ...\n  Output()->Add(output.release(), InputTimestamp());\n  return absl::OkStatus();\n}\n```\n\n----------------------------------------\n\nTITLE: Simplified Output Writing with Node API\nDESCRIPTION: Modern approach for sending output packets using the new Node API.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nkPair(cc).Send({kIn(cc)[0].packet(), kIn(cc)[1].packet()});\n```\n\n----------------------------------------\n\nTITLE: Binding Custom Types in MediaPipe with Pybind11\nDESCRIPTION: Shows how to create pybind11 class bindings or custom type casters for new data types in MediaPipe. This code demonstrates the basic structure for binding a custom type to make it usable in Python.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n#include \"path/to/my_type/header/file.h\"\n#include \"pybind11/pybind11.h\"\n\nnamespace py = pybind11;\n\nPYBIND11_MODULE(my_type_binding, m) {\n  // Write binding code or a custom type caster for MyType.\n  py::class_<MyType>(m, \"MyType\")\n      .def(py::init<>())\n      .def(...);\n}\n```\n\n----------------------------------------\n\nTITLE: Including MediaPipe Libraries via CDN\nDESCRIPTION: HTML code snippet showing how to include MediaPipe drawing utilities and holistic packages via jsDelivr CDN. This method allows you to use MediaPipe without installing packages locally.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/javascript.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<head>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.1/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/holistic@0.1/holistic.js\" crossorigin=\"anonymous\"></script>\n</head>\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Hello World Example in Bash\nDESCRIPTION: This snippet shows how to clone the MediaPipe repository, set up the environment, and run the 'hello world' example using Bazel. It demonstrates the command-line instructions to execute the C++ example.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_cpp.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/google/mediapipe.git\n$ cd mediapipe\n\n$ export GLOG_logtostderr=1\n# Need bazel flag 'MEDIAPIPE_DISABLE_GPU=1' as desktop GPU is not supported currently.\n$ bazel run --define MEDIAPIPE_DISABLE_GPU=1 \\\n    mediapipe/examples/desktop/hello_world:hello_world\n\n# It should print 10 rows of Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata for Spatiotemporal Detection in Python\nDESCRIPTION: Example showing how to create metadata for object tracking or spatiotemporal detection using the MediaSequence Python API. It sets the clip metadata and adds bounding boxes, timestamps, labels, and tracking information.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Python: functions from media_sequence.py as ms\nsequence = tf.train.SequenceExample()\nms.set_clip_data_path(b\"path_to_video\", sequence)\nms.set_clip_start_timestamp(1000000, sequence)\nms.set_clip_end_timestamp(6000000, sequence)\n\n# For an object tracking task with action labels:\nloctions_on_frame_1 = np.array([[0.1, 0.2, 0.3 0.4],\n                                [0.2, 0.3, 0.4, 0.5]])\nms.add_bbox(locations_on_frame_1, sequence)\nms.add_bbox_timestamp(3000000, sequence)\nms.add_bbox_label_index((4, 3), sequence)\nms.add_bbox_label_string((b\"run\", b\"jump\"), sequence)\nms.add_bbox_track_string((b\"id_0\", b\"id_1\"), sequence)\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection TensorFlow Example on Desktop\nDESCRIPTION: Command to run the object detection application with a TensorFlow model, specifying the graph configuration and input/output video paths. This uses the TensorFlow model for inference on CPU.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/object_detection/object_detection_tflite \\\n  --calculator_graph_config_file=mediapipe/graphs/object_detection/object_detection_desktop_tensorflow_graph.pbtxt \\\n  --input_side_packets=input_video_path=<input video path>,output_video_path=<output video path>\n```\n\n----------------------------------------\n\nTITLE: Initializing MediaPipe Hands with Camera Input in Android\nDESCRIPTION: Sets up the MediaPipe Hands solution with camera input and OpenGL rendering. This snippet configures the Hands detector for real-time processing with multiple hand tracking and GPU acceleration, then connects it to the camera input and display surface.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n// For camera input and result rendering with OpenGL.\nHandsOptions handsOptions =\n    HandsOptions.builder()\n        .setStaticImageMode(false)\n        .setMaxNumHands(2)\n        .setRunOnGpu(true).build();\nHands hands = new Hands(this, handsOptions);\nhands.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Hands error:\" + message));\n\n// Initializes a new CameraInput instance and connects it to MediaPipe Hands Solution.\nCameraInput cameraInput = new CameraInput(this);\ncameraInput.setNewFrameListener(\n    textureFrame -> hands.send(textureFrame));\n\n// Initializes a new GlSurfaceView with a ResultGlRenderer<HandsResult> instance\n// that provides the interfaces to run user-defined OpenGL rendering code.\n// See mediapipe/examples/android/solutions/hands/src/main/java/com/google/mediapipe/examples/hands/HandsResultGlRenderer.java\n// as an example.\nSolutionGlSurfaceView<HandsResult> glSurfaceView =\n    new SolutionGlSurfaceView<>(\n        this, hands.getGlContext(), hands.getGlMajorVersion());\nglSurfaceView.setSolutionResultRenderer(new HandsResultGlRenderer());\nglSurfaceView.setRenderInputImage(true);\n\nhands.setResultListener(\n    handsResult -> {\n      if (result.multiHandLandmarks().isEmpty()) {\n        return;\n      }\n      NormalizedLandmark wristLandmark =\n          handsResult.multiHandLandmarks().get(0).getLandmarkList().get(HandLandmark.WRIST);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Hand wrist normalized coordinates (value range: [0, 1]): x=%f, y=%f\",\n              wristLandmark.getX(), wristLandmark.getY()));\n      // Request GL rendering.\n      glSurfaceView.setRenderData(handsResult);\n      glSurfaceView.requestRender();\n    });\n\n// The runnable to start camera after the GLSurfaceView is attached.\nglSurfaceView.post(\n    () ->\n        cameraInput.start(\n            this,\n            hands.getGlContext(),\n            CameraInput.CameraFacing.FRONT,\n            glSurfaceView.getWidth(),\n            glSurfaceView.getHeight()));\n```\n\n----------------------------------------\n\nTITLE: New Type-Safe Port Declaration in MediaPipe Node API\nDESCRIPTION: Modern approach using the new Node API to declare typed input ports with optional values.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstatic constexpr Input<int>::Optional kSelect{\"SELECT\"};\n```\n\n----------------------------------------\n\nTITLE: Creating MediaPipe Packets using MakePacket\nDESCRIPTION: Demonstrates how to create a new packet containing data and how to create another packet with the same data but a different timestamp using MakePacket<T>().\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/packets.md#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n// Create a packet containing some new data.\nPacket p = MakePacket<MyDataClass>(\"constructor_argument\");\n// Make a new packet with the same data and a different timestamp.\nPacket p2 = p.At(Timestamp::PostStream());\n```\n\n----------------------------------------\n\nTITLE: Loading MediaPipe Graph from Resource in Objective-C\nDESCRIPTION: Implements a function to load the MediaPipe graph from a binary protocol buffer file.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_23\n\nLANGUAGE: Objective-C\nCODE:\n```\n+ (MPPGraph*)loadGraphFromResource:(NSString*)resource {\n  // Load the graph config resource.\n  NSError* configLoadError = nil;\n  NSBundle* bundle = [NSBundle bundleForClass:[self class]];\n  if (!resource || resource.length == 0) {\n    return nil;\n  }\n  NSURL* graphURL = [bundle URLForResource:resource withExtension:@\"binarypb\"];\n  NSData* data = [NSData dataWithContentsOfURL:graphURL options:0 error:&configLoadError];\n  if (!data) {\n    NSLog(@\"Failed to load MediaPipe graph config: %@\", configLoadError);\n    return nil;\n  }\n\n  // Parse the graph config resource into mediapipe::CalculatorGraphConfig proto object.\n  mediapipe::CalculatorGraphConfig config;\n  config.ParseFromArray(data.bytes, data.length);\n\n  // Create MediaPipe graph with mediapipe::CalculatorGraphConfig proto object.\n  MPPGraph* newGraph = [[MPPGraph alloc] initWithGraphConfig:config];\n  [newGraph addFrameOutputStream:kOutputStream outputPacketType:MPPPacketTypePixelBuffer];\n  return newGraph;\n}\n```\n\n----------------------------------------\n\nTITLE: Building KNIFT Index Files for Custom Template Images\nDESCRIPTION: Commands to build an index file from custom template images. This process extracts KNIFT features from images in a specified directory and generates an index file that can be used for template matching.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/knift.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 \\\nmediapipe/examples/desktop/template_matching:template_matching_tflite\n```\n\nLANGUAGE: bash\nCODE:\n```\nbazel-bin/mediapipe/examples/desktop/template_matching/template_matching_tflite \\\n--calculator_graph_config_file=mediapipe/graphs/template_matching/index_building.pbtxt \\\n--input_side_packets=\"file_directory=<template image directory>,file_suffix=png,output_index_filename=<output index filename>\"\n```\n\n----------------------------------------\n\nTITLE: Installing MediaPipe Holistic Package with NPM\nDESCRIPTION: Command to install the MediaPipe Holistic package locally using NPM. This allows you to incorporate holistic body tracking functionality into your JavaScript application.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/javascript.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mediapipe/holistic.\n```\n\n----------------------------------------\n\nTITLE: Improved PassThroughCalculator Implementation with Utility Class\nDESCRIPTION: An improved implementation of the PassThroughCalculator graph using the PassThroughNodeBuilder utility class. This approach maintains type safety, automates index management, and improves code readability.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Graph inputs.\n  Stream<float> float_value = graph.In(0).SetName(\"float_value\").Cast<float>();\n  Stream<int> int_value = graph.In(1).SetName(\"int_value\").Cast<int>();\n  Stream<bool> bool_value = graph.In(2).SetName(\"bool_value\").Cast<bool>();\n\n  PassThroughNodeBuilder pass_node_builder(graph);\n  Stream<float> passed_float_value = pass_node_builder.PassThrough(float_value);\n  Stream<int> passed_int_value = pass_node_builder.PassThrough(int_value);\n  Stream<bool> passed_bool_value = pass_node_builder.PassThrough(bool_value);\n\n  // Graph outputs.\n  passed_float_value.SetName(\"passed_float_value\").ConnectTo(graph.Out(0));\n  passed_int_value.SetName(\"passed_int_value\").ConnectTo(graph.Out(1));\n  passed_bool_value.SetName(\"passed_bool_value\").ConnectTo(graph.Out(2));\n\n  // Get `CalculatorGraphConfig` to pass it into `CalculatorGraph`\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a MediaPipe Subgraph\nDESCRIPTION: This snippet shows how to define a MediaPipe subgraph named 'TwoPassThroughSubgraph'. The subgraph contains two PassThroughCalculators connected in sequence.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_2\n\nLANGUAGE: proto\nCODE:\n```\n# This subgraph is defined in two_pass_through_subgraph.pbtxt\n# and is registered as \"TwoPassThroughSubgraph\"\n\ntype: \"TwoPassThroughSubgraph\"\ninput_stream: \"out1\"\noutput_stream: \"out3\"\n\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"out1\"\n    output_stream: \"out2\"\n}\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"out2\"\n    output_stream: \"out3\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Iris Tracking with Video Input\nDESCRIPTION: Command to run the MediaPipe Iris tracking application with video input and output paths.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/iris.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel-bin/mediapipe/examples/desktop/iris_tracking/iris_tracking_cpu_video_input \\\n  --calculator_graph_config_file=mediapipe/graphs/iris_tracking/iris_tracking_cpu_video_input.pbtxt \\\n  --input_side_packets=input_video_path=<input video path>,output_video_path=<output video path>\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Packet Methods in MediaPipe\nDESCRIPTION: Demonstrates how to implement custom packet creator and getter methods for new data types in MediaPipe. This code shows the basic structure for adding packet handling capabilities for custom types.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n#include \"path/to/my_type/header/file.h\"\n#include \"mediapipe/framework/packet.h\"\n#include \"pybind11/pybind11.h\"\n\nnamespace mediapipe {\nnamespace py = pybind11;\n\nPYBIND11_MODULE(my_packet_methods, m) {\n  m.def(\n      \"create_my_type\",\n      [](const MyType& my_type) { return MakePacket<MyType>(my_type); });\n\n  m.def(\n      \"get_my_type\",\n      [](const Packet& packet) {\n        if(!packet.ValidateAsType<MyType>().ok()) {\n          PyErr_SetString(PyExc_ValueError, \"Packet data type mismatch.\");\n\n```\n\n----------------------------------------\n\nTITLE: Initializing FrameProcessor for MediaPipe Graph in Java\nDESCRIPTION: This code initializes the FrameProcessor with necessary context and metadata. It should be called in the onCreate method after initializing eglManager.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_30\n\nLANGUAGE: Java\nCODE:\n```\nprocessor =\n    new FrameProcessor(\n        this,\n        eglManager.getNativeContext(),\n        applicationInfo.metaData.getString(\"binaryGraphName\"),\n        applicationInfo.metaData.getString(\"inputVideoStreamName\"),\n        applicationInfo.metaData.getString(\"outputVideoStreamName\"));\n```\n\n----------------------------------------\n\nTITLE: Implementing Unit Delay Calculator in C++\nDESCRIPTION: A MediaPipe calculator implementation that adds a unit delay to input packets and provides initial packet output. The calculator processes integer inputs and outputs them with a one-time-unit delay, assuming input timestamps of 0, 1, 2, 3...\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nclass UnitDelayCalculator : public Calculator {\n public:\n  static absl::Status FillExpectations(\n      const CalculatorOptions& extendable_options, PacketTypeSet* inputs,\n      PacketTypeSet* outputs, PacketTypeSet* input_side_packets) {\n    inputs->Index(0)->Set<int>(\"An integer.\");\n    outputs->Index(0)->Set<int>(\"The input delayed by one time unit.\");\n    return absl::OkStatus();\n  }\n\n  absl::Status Open() final {\n    Output()->Add(new int(0), Timestamp(0));\n    return absl::OkStatus();\n  }\n\n  absl::Status Process() final {\n    const Packet& packet = Input()->Value();\n    Output()->AddPacket(packet.At(packet.Timestamp().NextAllowedInStream()));\n    return absl::OkStatus();\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Declaring FrameProcessor for MediaPipe Graph Processing in Java\nDESCRIPTION: This code snippet declares a FrameProcessor object that will be used to send camera frames to the MediaPipe graph, run the graph, and prepare the output for display.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_29\n\nLANGUAGE: Java\nCODE:\n```\nprivate FrameProcessor processor;\n```\n\n----------------------------------------\n\nTITLE: Setting up Camera Preview Display with SurfaceHolder Callbacks\nDESCRIPTION: Implementing SurfaceHolder.Callback to connect the camera preview texture to the ExternalTextureConverter when the surface changes.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_22\n\nLANGUAGE: Java\nCODE:\n```\npreviewDisplayView\n .getHolder()\n .addCallback(\n     new SurfaceHolder.Callback() {\n       @Override\n       public void surfaceCreated(SurfaceHolder holder) {}\n\n       @Override\n       public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {\n         // (Re-)Compute the ideal size of the camera-preview display (the area that the\n         // camera-preview frames get rendered onto, potentially with scaling and rotation)\n         // based on the size of the SurfaceView that contains the display.\n         Size viewSize = new Size(width, height);\n         Size displaySize = cameraHelper.computeDisplaySizeFromViewSize(viewSize);\n\n         // Connect the converter to the camera-preview frames as its input (via\n         // previewFrameTexture), and configure the output width and height as the computed\n         // display size.\n         converter.setSurfaceTextureAndAttachToGLContext(\n             previewFrameTexture, displaySize.getWidth(), displaySize.getHeight());\n       }\n\n       @Override\n       public void surfaceDestroyed(SurfaceHolder holder) {}\n     });\n```\n\n----------------------------------------\n\nTITLE: Building and Running Feature Extraction in Bash\nDESCRIPTION: Commands to build the MediaPipe binary for feature extraction and run it on the input video, generating feature output.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --linkopt=-s \\\n  --define MEDIAPIPE_DISABLE_GPU=1 --define no_aws_support=true \\\n  mediapipe/examples/desktop/youtube8m:extract_yt8m_features\n\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/youtube8m/extract_yt8m_features \\\n  --calculator_graph_config_file=mediapipe/graphs/youtube8m/feature_extraction.pbtxt \\\n  --input_side_packets=input_sequence_example=/tmp/mediapipe/metadata.pb  \\\n  --output_side_packets=output_sequence_example=/tmp/mediapipe/features.pb\n```\n\n----------------------------------------\n\nTITLE: Running Python Web Server for Model Inference\nDESCRIPTION: Command to run a Python web server for YouTube-8M model inference with a web interface.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython mediapipe/examples/desktop/youtube8m/viewer/server.py --root `pwd`\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Hands on CPU in C++\nDESCRIPTION: Command to run the built MediaPipe Hands application on CPU. It configures logging to stderr and specifies the calculator graph configuration file for desktop live hand tracking.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/cpp.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/hand_tracking/hand_tracking_cpu \\\n  --calculator_graph_config_file=mediapipe/graphs/hand_tracking/hand_tracking_desktop_live.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Implementing Camera Input for Face Detection in Android\nDESCRIPTION: Sets up real-time face detection using device camera input with OpenGL rendering. Initializes FaceDetection with camera input, configures GL surface view for rendering, and handles detection results including nose tip coordinate tracking.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/face_detection.md#2025-04-23_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n// For camera input and result rendering with OpenGL.\nFaceDetectionOptions faceDetectionOptions =\n    FaceDetectionOptions.builder()\n        .setStaticImageMode(false)\n        .setModelSelection(0).build();\nFaceDetection faceDetection = new FaceDetection(this, faceDetectionOptions);\nfaceDetection.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Face Detection error:\" + message));\n\n// Initializes a new CameraInput instance and connects it to MediaPipe Face Detection Solution.\nCameraInput cameraInput = new CameraInput(this);\ncameraInput.setNewFrameListener(\n    textureFrame -> faceDetection.send(textureFrame));\n\n// Initializes a new GlSurfaceView with a ResultGlRenderer<FaceDetectionResult> instance\n// that provides the interfaces to run user-defined OpenGL rendering code.\n// See mediapipe/examples/android/solutions/facedetection/src/main/java/com/google/mediapipe/examples/facedetection/FaceDetectionResultGlRenderer.java\n// as an example.\nSolutionGlSurfaceView<FaceDetectionResult> glSurfaceView =\n    new SolutionGlSurfaceView<>(\n        this, faceDetection.getGlContext(), faceDetection.getGlMajorVersion());\nglSurfaceView.setSolutionResultRenderer(new FaceDetectionResultGlRenderer());\nglSurfaceView.setRenderInputImage(true);\nfaceDetection.setResultListener(\n    faceDetectionResult -> {\n      if (faceDetectionResult.multiFaceDetections().isEmpty()) {\n        return;\n      }\n      RelativeKeypoint noseTip =\n          faceDetectionResult\n              .multiFaceDetections()\n              .get(0)\n              .getLocationData()\n              .getRelativeKeypoints(FaceKeypoint.NOSE_TIP);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Face Detection nose tip normalized coordinates (value range: [0, 1]): x=%f, y=%f\",\n              noseTip.getX(), noseTip.getY()));\n      // Request GL rendering.\n      glSurfaceView.setRenderData(faceDetectionResult);\n      glSurfaceView.requestRender();\n    });\n\n// The runnable to start camera after the GLSurfaceView is attached.\nglSurfaceView.post(\n    () ->\n        cameraInput.start(\n            this,\n            faceDetection.getGlContext(),\n            CameraInput.CameraFacing.FRONT,\n            glSurfaceView.getWidth(),\n            glSurfaceView.getHeight()));\n```\n\n----------------------------------------\n\nTITLE: Configuring Tracing and Profiling in MediaPipe (Protobuf)\nDESCRIPTION: This snippet shows how to enable tracing and profiling in a MediaPipe graph configuration. It sets up basic tracing and keeps 100 seconds of timing events.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/tracing_and_profiling.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nprofiler_config {\n  trace_enabled: true\n  enable_profiler: true\n  trace_log_interval_count: 200\n  trace_log_path: \"/sdcard/Download/\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using a MediaPipe Subgraph in a Main Graph\nDESCRIPTION: This snippet shows how to use a registered subgraph ('TwoPassThroughSubgraph') in a main MediaPipe graph. The subgraph is used as a node in the main graph alongside other calculators.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_4\n\nLANGUAGE: proto\nCODE:\n```\n# This main graph is defined in main_pass_throughcals.pbtxt\n# using subgraph called \"TwoPassThroughSubgraph\"\n\ninput_stream: \"in\"\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"in\"\n    output_stream: \"out1\"\n}\nnode {\n    calculator: \"TwoPassThroughSubgraph\"\n    input_stream: \"out1\"\n    output_stream: \"out3\"\n}\nnode {\n    calculator: \"PassThroughCalculator\"\n    input_stream: \"out3\"\n    output_stream: \"out4\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Time-Aligned Text Content in MediaPipe (Python/C++)\nDESCRIPTION: Adds time-aligned segments of text to the MediaPipe feature list. This is used for captions, transcripts, or other text aligned with media.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nadd_text_content\n```\n\nLANGUAGE: c++\nCODE:\n```\nAddTextContent\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe AAR with Bazel Command\nDESCRIPTION: Command line instruction for building the MediaPipe AAR with optimizations for ARM processors and various compiler options.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_archive_library.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --strip=ALWAYS \\\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n    --fat_apk_cpu=arm64-v8a,armeabi-v7a \\\n    --legacy_whole_archive=0 \\\n    --features=-legacy_whole_archive \\\n    --copt=-fvisibility=hidden \\\n    --copt=-ffunction-sections \\\n    --copt=-fdata-sections \\\n    --copt=-fstack-protector \\\n    --copt=-Oz \\\n    --copt=-fomit-frame-pointer \\\n    --copt=-DABSL_MIN_LOG_LEVEL=2 \\\n    --linkopt=-Wl,--gc-sections,--strip-all \\\n    //path/to/the/aar/build/file:aar_name.aar\n```\n\n----------------------------------------\n\nTITLE: Implementing Video Frame Processing Method\nDESCRIPTION: Processes incoming video frames from the camera source, ensuring frames are displayed on the main thread using the renderer.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_14\n\nLANGUAGE: Objective-C\nCODE:\n```\n// Must be invoked on _videoQueue.\n- (void)processVideoFrame:(CVPixelBufferRef)imageBuffer\n                timestamp:(CMTime)timestamp\n               fromSource:(MPPInputSource*)source {\n  if (source != _cameraSource) {\n    NSLog(@\"Unknown source: %@\", source);\n    return;\n  }\n  // Display the captured image on the screen.\n  CFRetain(imageBuffer);\n  dispatch_async(dispatch_get_main_queue(), ^{\n    [_renderer renderPixelBuffer:imageBuffer];\n    CFRelease(imageBuffer);\n  });\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Extracted Features in Python\nDESCRIPTION: Python code snippet to read and print the extracted features from the output file.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\n\nsequence_example = open('/tmp/mediapipe/features.pb', 'rb').read()\nprint(tf.train.SequenceExample.FromString(sequence_example))\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection TFLite Example on Desktop\nDESCRIPTION: Command to run the object detection application with a TFLite model, specifying the graph configuration and input/output video paths. GLOG_logtostderr=1 enables logging to standard error.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/object_detection/object_detection_tflite \\\n  --calculator_graph_config_file=mediapipe/graphs/object_detection/object_detection_desktop_tflite_graph.pbtxt \\\n  --input_side_packets=input_video_path=<input video path>,output_video_path=<output video path>\n```\n\n----------------------------------------\n\nTITLE: Initializing MediaPipe Calculator Graph with Binary Protobuf\nDESCRIPTION: Example demonstrating how to initialize a MediaPipe CalculatorGraph using a binary protobuf file and setting up output stream observation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mediapipe as mp\n# resources dependency\n\ngraph = mp.CalculatorGraph(\n    binary_graph=os.path.join(\n        resources.GetRunfilesDir(), 'path/to/your/graph.binarypb'))\ngraph.observe_output_stream(\n    'out_stream',\n    lambda stream_name, packet: print(f'Get {packet} from {stream_name}'))\n```\n\n----------------------------------------\n\nTITLE: Initializing MediaPipe Calculator Graph with Config\nDESCRIPTION: Example showing how to initialize a MediaPipe CalculatorGraph using a text config and setting up output stream observation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mediapipe as mp\n\nconfig_text = \"\"\"\n  input_stream: 'in_stream'\n  output_stream: 'out_stream'\n  node {\n    calculator: 'PassThroughCalculator'\n    input_stream: 'in_stream'\n    output_stream: 'out_stream'\n  }\n\"\"\"\ngraph = mp.CalculatorGraph(graph_config=config_text)\noutput_packets = []\ngraph.observe_output_stream(\n    'out_stream',\n    lambda stream_name, packet:\n        output_packets.append(mp.packet_getter.get_str(packet)))\n```\n\n----------------------------------------\n\nTITLE: Setting Up MediaPipe Hands with Video Input in Android\nDESCRIPTION: Implements MediaPipe Hands detection for processing video input from the device gallery. This code configures the hands detector for video processing with GPU acceleration, creates a video input system, and sets up activity launchers to select and process videos.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hands.md#2025-04-23_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n// For video input and result rendering with OpenGL.\nHandsOptions handsOptions =\n    HandsOptions.builder()\n        .setStaticImageMode(false)\n        .setMaxNumHands(2)\n        .setRunOnGpu(true).build();\nHands hands = new Hands(this, handsOptions);\nhands.setErrorListener(\n    (message, e) -> Log.e(TAG, \"MediaPipe Hands error:\" + message));\n\n// Initializes a new VideoInput instance and connects it to MediaPipe Hands Solution.\nVideoInput videoInput = new VideoInput(this);\nvideoInput.setNewFrameListener(\n    textureFrame -> hands.send(textureFrame));\n\n// Initializes a new GlSurfaceView with a ResultGlRenderer<HandsResult> instance\n// that provides the interfaces to run user-defined OpenGL rendering code.\n// See mediapipe/examples/android/solutions/hands/src/main/java/com/google/mediapipe/examples/hands/HandsResultGlRenderer.java\n// as an example.\nSolutionGlSurfaceView<HandsResult> glSurfaceView =\n    new SolutionGlSurfaceView<>(\n        this, hands.getGlContext(), hands.getGlMajorVersion());\nglSurfaceView.setSolutionResultRenderer(new HandsResultGlRenderer());\nglSurfaceView.setRenderInputImage(true);\n\nhands.setResultListener(\n    handsResult -> {\n      if (result.multiHandLandmarks().isEmpty()) {\n        return;\n      }\n      NormalizedLandmark wristLandmark =\n          handsResult.multiHandLandmarks().get(0).getLandmarkList().get(HandLandmark.WRIST);\n      Log.i(\n          TAG,\n          String.format(\n              \"MediaPipe Hand wrist normalized coordinates (value range: [0, 1]): x=%f, y=%f\",\n              wristLandmark.getX(), wristLandmark.getY()));\n      // Request GL rendering.\n      glSurfaceView.setRenderData(handsResult);\n      glSurfaceView.requestRender();\n    });\n\nActivityResultLauncher<Intent> videoGetter =\n    registerForActivityResult(\n        new ActivityResultContracts.StartActivityForResult(),\n        result -> {\n          Intent resultIntent = result.getData();\n          if (resultIntent != null) {\n            if (result.getResultCode() == RESULT_OK) {\n              glSurfaceView.post(\n                  () ->\n                      videoInput.start(\n                          this,\n                          resultIntent.getData(),\n                          hands.getGlContext(),\n                          glSurfaceView.getWidth(),\n                          glSurfaceView.getHeight()));\n            }\n          }\n        });\nIntent pickVideoIntent = new Intent(Intent.ACTION_PICK);\npickVideoIntent.setDataAndType(MediaStore.Video.Media.INTERNAL_CONTENT_URI, \"video/*\");\nvideoGetter.launch(pickVideoIntent);\n```\n\n----------------------------------------\n\nTITLE: Declaring Camera Source in ViewController Implementation\nDESCRIPTION: Creates a camera source object in the ViewController implementation block to handle camera access via AVCaptureSession.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_5\n\nLANGUAGE: Objective-C\nCODE:\n```\n@implementation ViewController {\n  // Handles camera access via AVCaptureSession library.\n  MPPCameraInputSource* _cameraSource;\n}\n```\n\n----------------------------------------\n\nTITLE: Example of Calculator Open and Process Methods\nDESCRIPTION: Demonstrates implementation of Open and Process methods in a MediaPipe calculator to handle timestamp bound propagation using ProcessTimestampBounds.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/realtime_streams.md#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nabsl::Status Open(CalculatorContext* cc) {\n  cc->SetProcessTimestampBounds(true);\n}\n\nabsl::Status Process(CalculatorContext* cc) {\n  cc->Outputs.Tag(\"OUT\").SetNextTimestampBound(\n      cc->InputTimestamp().NextAllowedInStream());\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Running Model Inference\nDESCRIPTION: Commands to build and run the MediaPipe binary for YouTube-8M model inference on the downloaded dataset.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define='MEDIAPIPE_DISABLE_GPU=1' --linkopt=-s \\\nmediapipe/examples/desktop/youtube8m:model_inference\n\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/youtube8m/model_inference \\\n  --calculator_graph_config_file=mediapipe/graphs/youtube8m/yt8m_dataset_model_inference.pbtxt \\\n  --input_side_packets=tfrecord_path=/tmp/mediapipe/trainpj.tfrecord,record_index=0,desired_segment_size=5 \\\n  --output_stream=annotation_summary \\\n  --output_stream_file=/tmp/summary \\\n  --output_side_packets=yt8m_id \\\n  --output_side_packets_file=/tmp/yt8m_id\n```\n\n----------------------------------------\n\nTITLE: Initializing MediaPipe Graph in Objective-C\nDESCRIPTION: Initializes the MediaPipe graph in the viewDidLoad method of ViewController.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_24\n\nLANGUAGE: Objective-C\nCODE:\n```\nself.mediapipeGraph = [[self class] loadGraphFromResource:kGraphName];\nself.mediapipeGraph.delegate = self;\nself.mediapipeGraph.maxFramesInFlight = 2;\n```\n\n----------------------------------------\n\nTITLE: Building and Running Charades Dataset Pipeline\nDESCRIPTION: Commands to build and execute the Charades dataset generation pipeline. Requires TensorFlow and compliance with Allen Institute's license for the Charades dataset.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt mediapipe/examples/desktop/media_sequence:media_sequence_demo \\\n  --define MEDIAPIPE_DISABLE_GPU=1\n\npython -m mediapipe.examples.desktop.media_sequence.charades_dataset \\\n  --alsologtostderr \\\n  --path_to_charades_data=/tmp/charades_data/ \\\n  --path_to_mediapipe_binary=bazel-bin/mediapipe/examples/desktop/\\\nmedia_sequence/media_sequence_demo  \\\n  --path_to_graph_directory=mediapipe/graphs/media_sequence/\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe Wheel Package\nDESCRIPTION: Alternative command to build a MediaPipe wheel package for distribution.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n(mp_env)mediapipe$ python3 setup.py bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Creating MediaPipe Packets using Adopt\nDESCRIPTION: Shows an alternative method of creating packets using Adopt() where the packet takes ownership of dynamically allocated data.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/packets.md#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// Create some new data.\nauto data = absl::make_unique<MyDataClass>(\"constructor_argument\");\n// Create a packet to own the data.\nPacket p = Adopt(data.release()).At(Timestamp::PostStream());\n```\n\n----------------------------------------\n\nTITLE: Configuring Object Detection and Tracking Graph in MediaPipe\nDESCRIPTION: This snippet demonstrates the configuration of an object detection and tracking graph in MediaPipe. It includes subgraphs for object detection, tracking, and rendering.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/box_tracking.md#2025-04-23_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\n# mediapipe/graphs/tracking/object_detection_tracking_mobile_gpu.pbtxt\n\ninput_stream: \"input_video\"\n\nnode {\n  calculator: \"PacketResampler\"\n  input_stream: \"input_video\"\n  output_stream: \"sampled_video\"\n  node_options: {\n    [type.googleapis.com/mediapipe.PacketResampler.Options] {\n      frame_rate: 0.5\n    }\n  }\n}\n\nnode {\n  calculator: \"ObjectDetectionSubgraph\"\n  input_stream: \"IMAGE:sampled_video\"\n  output_stream: \"DETECTIONS:detections\"\n}\n\nnode {\n  calculator: \"ObjectTrackingSubgraph\"\n  input_stream: \"IMAGE:input_video\"\n  input_stream: \"DETECTIONS:detections\"\n  output_stream: \"TRACKED_DETECTIONS:tracked_detections\"\n}\n\nnode {\n  calculator: \"RendererSubgraph\"\n  input_stream: \"IMAGE:input_video\"\n  input_stream: \"DETECTIONS:tracked_detections\"\n  output_stream: \"IMAGE:output_video\"\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing MediaPipe Image Classifier Benchmark Execution\nDESCRIPTION: This example demonstrates how to run the benchmark with custom parameters. It specifies a custom model file and sets the number of iterations for the benchmark.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/python/benchmark/vision/image_classifier/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel run -c opt :image_classifier_benchmark -- \\\n  --model classifier.tflite \\\n  --iterations 200\n```\n\n----------------------------------------\n\nTITLE: Initializing Camera Source in viewDidLoad\nDESCRIPTION: Sets up the camera source with appropriate configuration in the viewDidLoad method, specifying session preset, camera position, and orientation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_6\n\nLANGUAGE: Objective-C\nCODE:\n```\n-(void)viewDidLoad {\n  [super viewDidLoad];\n\n  _cameraSource = [[MPPCameraInputSource alloc] init];\n  _cameraSource.sessionPreset = AVCaptureSessionPresetHigh;\n  _cameraSource.cameraPosition = AVCaptureDevicePositionBack;\n  // The frame's native format is rotated with respect to the portrait orientation.\n  _cameraSource.orientation = AVCaptureVideoOrientationPortrait;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Basic MediaPipe Graph Node Configuration\nDESCRIPTION: Demonstrates the configuration of two calculator nodes A and B in a MediaPipe graph using protobuf syntax. Shows how nodes can be connected through input and output streams.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/realtime_streams.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nnode {\n   calculator: \"A\"\n   input_stream: \"alpha_in\"\n   output_stream: \"alpha\"\n}\nnode {\n   calculator: \"B\"\n   input_stream: \"alpha\"\n   input_stream: \"foo\"\n   output_stream: \"beta\"\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Demo Dataset with Python\nDESCRIPTION: Python command to download and prepare a demo dataset using the MediaSequence library. This processes video data and stores it in TFRecord format.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m mediapipe.examples.desktop.media_sequence.demo_dataset \\\n  --path_to_demo_data=/tmp/demo_data/ \\\n  --path_to_mediapipe_binary=bazel-bin/mediapipe/examples/desktop/media_sequence/media_sequence_demo \\\n  --path_to_graph_directory=mediapipe/graphs/media_sequence/\n```\n\n----------------------------------------\n\nTITLE: Handling Camera Permission Results in MainActivity\nDESCRIPTION: This Java code shows how to handle the user's response to camera permission requests and start the camera if permissions are granted.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_9\n\nLANGUAGE: Java\nCODE:\n```\n@Override\npublic void onRequestPermissionsResult(\n    int requestCode, String[] permissions, int[] grantResults) {\n  super.onRequestPermissionsResult(requestCode, permissions, grantResults);\n  PermissionHelper.onRequestPermissionsResult(requestCode, permissions, grantResults);\n}\n\n@Override\nprotected void onResume() {\n  super.onResume();\n  if (PermissionHelper.cameraPermissionsGranted(this)) {\n    startCamera();\n  }\n}\n\npublic void startCamera() {}\n```\n\n----------------------------------------\n\nTITLE: Building and Running AutoFlip Video Cropping Tool\nDESCRIPTION: Commands to build the run_autoflip binary and execute it to process a local video. This snippet includes the build command and the execution command with necessary parameters.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/autoflip/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 \\\n  mediapipe/examples/desktop/autoflip:run_autoflip\n\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/autoflip/run_autoflip \\\n  --calculator_graph_config_file=mediapipe/examples/desktop/autoflip/autoflip_graph.pbtxt \\\n  --input_side_packets=input_video_path=/absolute/path/to/the/local/video/file,output_video_path=/absolute/path/to/save/the/output/video/file,aspect_ratio=width:height\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference on Local Video in Bash\nDESCRIPTION: Command to run model inference on a local video file, specifying input and output paths, and segment parameters.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/youtube8m/model_inference \\\n  --calculator_graph_config_file=mediapipe/graphs/youtube8m/local_video_model_inference.pbtxt \\\n  --input_side_packets=input_sequence_example_path=/tmp/mediapipe/features.pb,input_video_path=/absolute/path/to/the/local/video/file,output_video_path=/tmp/mediapipe/annotated_video.mp4,segment_size=5,overlap=4\n```\n\n----------------------------------------\n\nTITLE: Specifying Graph Options in a MediaPipe Graph\nDESCRIPTION: This snippet demonstrates how to specify graph options for a subgraph and calculator in a MediaPipe graph. It shows how to set options for FlowLimiterCalculator and FaceDetectionSubgraph.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_5\n\nLANGUAGE: proto\nCODE:\n```\nnode {\n  calculator: \"FlowLimiterCalculator\"\n  input_stream: \"image\"\n  output_stream: \"throttled_image\"\n  node_options: {\n    [type.googleapis.com/mediapipe.FlowLimiterCalculatorOptions] {\n      max_in_flight: 1\n    }\n  }\n}\n\nnode {\n  calculator: \"FaceDetectionSubgraph\"\n  input_stream: \"IMAGE:throttled_image\"\n  node_options: {\n    [type.googleapis.com/mediapipe.FaceDetectionOptions] {\n      tensor_width: 192\n      tensor_height: 192\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Extracted Features in Python\nDESCRIPTION: Python code snippet to read and print the extracted features from the output file.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\n\nsequence_example = open('/tmp/mediapipe/features.pb', 'rb').read()\nprint(tf.train.SequenceExample.FromString(sequence_example))\n```\n\n----------------------------------------\n\nTITLE: HTML Setup for MediaPipe Selfie Segmentation\nDESCRIPTION: HTML configuration for implementing MediaPipe Selfie Segmentation in a web application, including necessary script imports and canvas setup.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/selfie_segmentation.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/selfie_segmentation.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n  </div>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Simplified Input Access with Node API\nDESCRIPTION: Modern approach for accessing input values using the new Node API.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nint select = kSelect(cc).Get();  // alternative: *kSelect(cc)\n```\n\n----------------------------------------\n\nTITLE: Implementing onPause for ExternalTextureConverter Cleanup\nDESCRIPTION: Overriding the onPause method to properly close the ExternalTextureConverter when the app is paused.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\n@Override\nprotected void onPause() {\n  super.onPause();\n  converter.close();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SceneCroppingCalculator with Visualization Outputs in MediaPipe\nDESCRIPTION: This code snippet demonstrates how to modify the SceneCroppingCalculator node in MediaPipe to enable additional output streams for visualization. It includes settings for aspect ratio, video frames, detection features, and various cropping and motion analysis options.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/autoflip.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnode {\n  calculator: \"SceneCroppingCalculator\"\n  input_side_packet: \"EXTERNAL_ASPECT_RATIO:aspect_ratio\"\n  input_stream: \"VIDEO_FRAMES:video_raw\"\n  input_stream: \"KEY_FRAMES:video_frames_scaled_downsampled\"\n  input_stream: \"DETECTION_FEATURES:salient_regions\"\n  input_stream: \"STATIC_FEATURES:borders\"\n  input_stream: \"SHOT_BOUNDARIES:shot_change\"\n  output_stream: \"CROPPED_FRAMES:cropped_frames\"\n  output_stream: \"KEY_FRAME_CROP_REGION_VIZ_FRAMES:key_frame_crop_viz_frames\"\n  output_stream: \"SALIENT_POINT_FRAME_VIZ_FRAMES:salient_point_viz_frames\"\n  node_options: {\n    [type.googleapis.com/mediapipe.autoflip.SceneCroppingCalculatorOptions]: {\n      max_scene_size: 600\n      key_frame_crop_options: {\n        score_aggregation_type: CONSTANT\n      }\n      scene_camera_motion_analyzer_options: {\n        motion_stabilization_threshold_percent: 0.5\n        salient_point_bound: 0.499\n      }\n      padding_parameters: {\n        blur_cv_size: 200\n        overlay_opacity: 0.6\n      }\n      target_size_type: MAXIMIZE_TARGET_DIMENSION\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Running Desktop Objectron Application\nDESCRIPTION: Bash commands for building and running the MediaPipe Objectron desktop application with CPU processing, including options for different object categories.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/object_detection_3d:objectron_cpu\n```\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/object_detection_3d/objectron_cpu \\\n  --calculator_graph_config_file=mediapipe/graphs/object_detection_3d/objectron_desktop_cpu.pbtxt \\\n  --input_side_packets=input_video_path=<input video path>,output_video_path=<output video path>,box_landmark_model_path=<landmark model path>,allowed_labels=<allowed labels>\n```\n\n----------------------------------------\n\nTITLE: Building Face Detection for Coral USB\nDESCRIPTION: Bazel build command for compiling face detection example with USB Edge TPU support. Requires libusb and configures MediaPipe for optimized Edge TPU execution.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/coral/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build \\\n  --compilation_mode=opt \\\n  --define darwinn_portable=1 \\\n  --define MEDIAPIPE_DISABLE_GPU=1 \\\n  --define MEDIAPIPE_EDGE_TPU=usb \\\n  --linkopt=-l:libusb-1.0.so \\\n  mediapipe/examples/coral:face_detection_tpu build\n```\n\n----------------------------------------\n\nTITLE: Using Graph Options to Populate Calculator Options\nDESCRIPTION: This snippet shows how to accept graph options and use them to populate calculator options in a MediaPipe graph. It demonstrates setting options for ImageToTensorCalculator and InferenceCalculator using FaceDetectionOptions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_6\n\nLANGUAGE: proto\nCODE:\n```\ngraph_options: {\n  [type.googleapis.com/mediapipe.FaceDetectionOptions] {}\n}\n\nnode: {\n  calculator: \"ImageToTensorCalculator\"\n  input_stream: \"IMAGE:image\"\n  node_options: {\n    [type.googleapis.com/mediapipe.ImageToTensorCalculatorOptions] {\n        keep_aspect_ratio: true\n        border_mode: BORDER_ZERO\n    }\n  }\n  option_value: \"output_tensor_width:options/tensor_width\"\n  option_value: \"output_tensor_height:options/tensor_height\"\n}\n\nnode {\n  calculator: \"InferenceCalculator\"\n  node_options: {\n    [type.googleapis.com/mediapipe.InferenceCalculatorOptions] {}\n  }\n  option_value: \"delegate:options/delegate\"\n  option_value: \"model_path:options/model_path\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up HTML structure for MediaPipe Objectron\nDESCRIPTION: Basic HTML structure for implementing MediaPipe Objectron with necessary script imports and video/canvas elements for input/output display.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/control_utils_3d/control_utils_3d.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js\" crossorigin=\"anonymous\"></script>\n  <script src=\"https://cdn.jsdelivr.net/npm/@mediapipe/objectron/objectron.js\" crossorigin=\"anonymous\"></script>\n</head>\n\n<body>\n  <div class=\"container\">\n    <video class=\"input_video\"></video>\n    <canvas class=\"output_canvas\" width=\"1280px\" height=\"720px\"></canvas>\n  </div>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Defining Streams in MediaPipe Graph (Protobuf)\nDESCRIPTION: Example of defining input and output streams in a MediaPipe calculator graph configuration. Streams are used to pass data between calculators.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/visualizer.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nnode {\n  calculator: \"PassThroughCalculator\"\n  input_stream: \"input_stream\"\n  output_stream: \"output_stream\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe Calculator Dependencies in BUILD File\nDESCRIPTION: Adds the required calculator dependencies for the edge detection graph to the deps section in the Bazel BUILD file.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_19\n\nLANGUAGE: BUILD\nCODE:\n```\n\"//mediapipe/graphs/edge_detection:mobile_calculators\",\n```\n\n----------------------------------------\n\nTITLE: Generating MediaSequence Metadata in Python\nDESCRIPTION: Python command to generate MediaSequence metadata from an input video, specifying the video path and clip duration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m mediapipe.examples.desktop.youtube8m.generate_input_sequence_example \\\n  --path_to_input_video=/absolute/path/to/the/local/video/file \\\n  --clip_end_time_sec=120\n```\n\n----------------------------------------\n\nTITLE: Defining MediaPipe AAR Target in Bazel\nDESCRIPTION: Bazel build configuration to create a MediaPipe AAR target for face detection, specifying calculator dependencies.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_archive_library.md#2025-04-23_snippet_0\n\nLANGUAGE: bazel\nCODE:\n```\nload(\"//mediapipe/java/com/google/mediapipe:mediapipe_aar.bzl\", \"mediapipe_aar\")\n\nmediapipe_aar(\n    name = \"mediapipe_face_detection\",\n    calculators = [\"//mediapipe/graphs/face_detection:mobile_calculators\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting TFLite SSD Graph for Object Detection Model\nDESCRIPTION: This command exports the TensorFlow object detection model to a frozen graph format suitable for TFLite conversion. It uses the export_tflite_ssd_graph.py script from the TensorFlow Object Detection API.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection_saved_model.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ PATH_TO_MODEL=path/to/the/model\n$ bazel run object_detection:export_tflite_ssd_graph -- \\\n    --pipeline_config_path ${PATH_TO_MODEL}/pipeline.config \\\n    --trained_checkpoint_prefix ${PATH_TO_MODEL}/model.ckpt \\\n    --output_directory ${PATH_TO_MODEL} \\\n    --add_postprocessing_op=False\n```\n\n----------------------------------------\n\nTITLE: Using Optional Graph Inputs in C++\nDESCRIPTION: This snippet demonstrates how to use std::optional for graph inputs that may not always be defined, placing them at the beginning of the function.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nstd::optional<Stream<A>> a;\nif (needs_a) {\n  a = graph.In(0).SetName(a).Cast<A>();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata for Clip Classification in C++\nDESCRIPTION: Example showing how to create metadata for a clip classification task using the MediaSequence C++ API. It sets the clip data path, start and end timestamps, and adds label indices and strings for classification.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// C++: functions from media_sequence.h\ntensorflow::SequenceExample sequence;\nSetClipDataPath(\"path_to_video\", &sequence);\nSetClipStartTimestamp(1000000, &sequence);\nSetClipEndTimestamp(6000000, &sequence);\nSetClipLabelIndex({4, 3}, &sequence);\nSetClipLabelString({\"run\", \"jump\"}, &sequence);\n```\n\n----------------------------------------\n\nTITLE: Working with Singular Feature Lists in Python and C++\nDESCRIPTION: API reference for functions to manipulate singular feature lists in TensorFlow SequenceExamples. These functions allow checking feature presence, getting feature size, accessing features at specific indices, clearing features, adding features, getting feature keys, and getting default parsers.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n| python call | c++ call | description |\n|-------------|----------|-------------|\n|`has_feature(example [, prefix])`|`HasFeature([const string& prefix,] const tf::SE& example)`|Returns a boolean if the feature is present.|\n|`get_feature_size(example [, prefix])`|`GetFeatureSize([const string& prefix,] const tf::SE&(example)`|Returns the number of features under this key. Will be 0 if the feature is absent.|\n|`get_feature_at(index, example [, prefix])`|`GetFeatureAt([const string& prefix,] const tf::SE& example, const int index)`|Returns a single feature of the appropriate type (string, int64, float) at position index of the feature list.|\n|`clear_feature(example [, prefix])`|`ClearFeature([const string& prefix,] tf::SE* example)`|Clears the entire feature.|\n|`add_feature(value, example [, prefix])`|`AddFeature([const string& prefix,], const TYPE& value, tf::SE* example)`|Appends a feature of the appropriate type to the feature list.|\n|`get_feature_key([prefix])`|`GetFeatureKey([const string& prefix])`|Returns the key used by related functions.|\n|`get_feature_default_parser()`| | Returns the tf.io.FixedLenSequenceFeature for this type. (Python only.) |\n```\n\n----------------------------------------\n\nTITLE: Defining Special Nodes in MediaPipe Graph (Protobuf)\nDESCRIPTION: Example of defining special input and output nodes in a MediaPipe calculator graph configuration. These nodes represent graph-level inputs and outputs.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/visualizer.md#2025-04-23_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\ninput_stream: \"input_frames\"\n\nnode {\n  calculator: \"ImageTransformationCalculator\"\n  input_stream: \"IMAGE:input_frames\"\n  output_stream: \"IMAGE:transformed_frames\"\n}\n\noutput_stream: \"OUTPUT:transformed_frames\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Target System\nDESCRIPTION: Commands for installing required OpenCV and libusb dependencies on the target system running the compiled MediaPipe applications.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/coral/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install -y \\\n    libopencv-core-dev \\\n    libopencv-highgui-dev \\\n    libopencv-calib3d-dev \\\n    libopencv-features2d-dev \\\n    libopencv-imgproc-dev \\\n    libopencv-video-dev\n\nsudo apt-get install -y \\\n   libusb-1.0-0\n```\n\n----------------------------------------\n\nTITLE: Generating VGGish Frozen Graph\nDESCRIPTION: Commands to install required Python package and generate the VGGish frozen graph for audio feature extraction.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd -\n\npip3 install tf_slim\npython -m mediapipe.examples.desktop.youtube8m.generate_vggish_frozen_graph\n```\n\n----------------------------------------\n\nTITLE: Android Gradle Dependencies Configuration\nDESCRIPTION: Gradle configuration showing required dependencies for using MediaPipe AAR in an Android Studio project, including CameraX and other supporting libraries.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_archive_library.md#2025-04-23_snippet_2\n\nLANGUAGE: gradle\nCODE:\n```\ndependencies {\n    implementation fileTree(dir: 'libs', include: ['*.jar', '*.aar'])\n    implementation 'androidx.appcompat:appcompat:1.0.2'\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'androidx.test.ext:junit:1.1.0'\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.1.1'\n    // MediaPipe deps\n    implementation 'com.google.flogger:flogger:latest.release'\n    implementation 'com.google.flogger:flogger-system-backend:latest.release'\n    implementation 'com.google.code.findbugs:jsr305:latest.release'\n    implementation 'com.google.guava:guava:27.0.1-android'\n    implementation 'com.google.protobuf:protobuf-javalite:3.19.1'\n    // CameraX core library\n    def camerax_version = \"1.0.0-beta10\"\n    implementation \"androidx.camera:camera-core:$camerax_version\"\n    implementation \"androidx.camera:camera-camera2:$camerax_version\"\n    implementation \"androidx.camera:camera-lifecycle:$camerax_version\"\n    // AutoValue\n    def auto_value_version = \"1.8.1\"\n    implementation \"com.google.auto.value:auto-value-annotations:$auto_value_version\"\n    annotationProcessor \"com.google.auto.value:auto-value:$auto_value_version\"\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading PCA and Model Data in Bash\nDESCRIPTION: Series of commands to create a temporary directory and download necessary PCA matrices and model data for feature extraction.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir /tmp/mediapipe\ncd /tmp/mediapipe\ncurl -O http://data.yt8m.org/pca_matrix_data/inception3_mean_matrix_data.pb\ncurl -O http://data.yt8m.org/pca_matrix_data/inception3_projection_matrix_data.pb\ncurl -O http://data.yt8m.org/pca_matrix_data/vggish_mean_matrix_data.pb\ncurl -O http://data.yt8m.org/pca_matrix_data/vggish_projection_matrix_data.pb\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\ntar -xvf /tmp/mediapipe/inception-2015-12-05.tgz\n```\n\n----------------------------------------\n\nTITLE: Setting Next Timestamp Bound in MediaPipe Calculator\nDESCRIPTION: Shows how to set the next timestamp bound for an output stream in a calculator using the CalculatorContext API. This is used when no packet is output for the current timestamp.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/realtime_streams.md#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncc->Outputs().Tag(\"output_frame\").SetNextTimestampBound(\n  cc->InputTimestamp().NextAllowedInStream());\n```\n\n----------------------------------------\n\nTITLE: Building Iris Depth Estimation Application\nDESCRIPTION: Bash command to build the MediaPipe Iris depth estimation application for single image processing.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/iris.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/iris_tracking:iris_depth_from_image_desktop\n```\n\n----------------------------------------\n\nTITLE: Adding Text Embedding in MediaPipe (Python/C++)\nDESCRIPTION: Adds a floating point vector representation for text tokens. This is used for storing semantic embeddings of text content.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nadd_text_embedding\n```\n\nLANGUAGE: c++\nCODE:\n```\nAddTextEmbedding\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Inputs in C++ (Best Practice)\nDESCRIPTION: This snippet shows the recommended way of defining graph inputs at the beginning of the graph builder function, improving readability and maintainability.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nStream<D> RunSomething(Stream<A> a, Stream<B> b, Stream<C> c, Graph& graph) {\n  // ...\n}\n\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).SetName(\"a\").Cast<A>();\n  Stream<B> b = graph.In(1).SetName(\"b\").Cast<B>();\n  Stream<C> c = graph.In(2).SetName(\"c\").Cast<C>();\n\n  // 10/100/N lines of code.\n  Stream<D> d = RunSomething(a, b, c, graph);\n  // ...\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Cross-compilation Docker Setup\nDESCRIPTION: Commands to set up Docker environment for cross-compilation targeting ARM32 (Raspberry Pi) or ARM64 (Coral Dev Board) platforms.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/coral/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# For ARM32 (e.g. Raspberry Pi)\nmake -C mediapipe/examples/coral PLATFORM=armhf docker\n\n# For ARM64 (e.g. Coral Dev Board)\nmake -C mediapipe/examples/coral PLATFORM=arm64 docker\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV in MediaPipe WORKSPACE\nDESCRIPTION: Example configuration for the WORKSPACE file to properly link to a custom OpenCV installation in WSL.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\nnew_local_repository(\n    name = \"linux_opencv\",\n    build_file = \"@//third_party:opencv_linux.BUILD\",\n    path = \"/usr/local\",\n)\n\ncc_library(\n    name = \"opencv\",\n    srcs = glob(\n        [\n            \"lib/libopencv_core.so\",\n            \"lib/libopencv_highgui.so\",\n            \"lib/libopencv_imgcodecs.so\",\n            \"lib/libopencv_imgproc.so\",\n            \"lib/libopencv_video.so\",\n            \"lib/libopencv_videoio.so\",\n        ],\n    ),\n    hdrs = glob([\"include/opencv4/**/*.h*\"]),\n    includes = [\"include/opencv4/\"],\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Bazel Build for Android\nDESCRIPTION: Bazel build configuration for compiling the Android application with required dependencies.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nandroid_library(\n    name = \"basic_lib\",\n    srcs = glob([\"*.java\"]),\n    manifest = \"AndroidManifest.xml\",\n    resource_files = glob([\"res/**\"]),\n    deps = [\n        \"@maven//:androidx_appcompat_appcompat\",\n        \"@maven//:androidx_constraintlayout_constraintlayout\",\n    ],\n)\n\nandroid_binary(\n    name = \"helloworld\",\n    manifest = \"AndroidManifest.xml\",\n    manifest_values = {\n        \"applicationId\": \"com.google.mediapipe.apps.basic\",\n        \"appName\": \"Hello World\",\n        \"mainActivity\": \".MainActivity\",\n    },\n    multidex = \"native\",\n    deps = [\n        \":basic_lib\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Generating VGGish Frozen Graph in Bash and Python\nDESCRIPTION: Commands to install required Python package and generate the VGGish frozen graph for audio feature extraction.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd -\n\npip3 install tf_slim\npython -m mediapipe.examples.desktop.youtube8m.generate_vggish_frozen_graph\n```\n\n----------------------------------------\n\nTITLE: Creating Reusable Inference Function for C++ Graph Builder\nDESCRIPTION: A utility function pattern that extracts inference node construction into a reusable component. This improves readability and enables code reuse for complex graphs with multiple inference steps.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\n// Updates graph to run inference.\nStream<std::vector<Tensor>> RunInference(\n    Stream<std::vector<Tensor>> tensors, SidePacket<TfLiteModelPtr> model,\n    const InferenceCalculatorOptions::Delegate& delegate, Graph& graph) {\n  auto& inference_node = graph.AddNode(\"InferenceCalculator\");\n  auto& inference_opts =\n      inference_node.GetOptions<InferenceCalculatorOptions>();\n  *inference_opts.mutable_delegate() = delegate;\n  tensors.ConnectTo(inference_node.In(\"TENSORS\"));\n  model.ConnectTo(inference_node.SideIn(\"MODEL\"));\n  return inference_node.Out(\"TENSORS\").Cast<std::vector<Tensor>>();\n}\n\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Graph inputs.\n  Stream<std::vector<Tensor>> input_tensors =\n      graph.In(0).SetName(\"input_tensors\").Cast<std::vector<Tensor>>();\n  SidePacket<TfLiteModelPtr> model =\n      graph.SideIn(0).SetName(\"model\").Cast<TfLiteModelPtr>();\n\n  InferenceCalculatorOptions::Delegate delegate;\n  delegate.mutable_gpu();\n  Stream<std::vector<Tensor>> output_tensors =\n      RunInference(input_tensors, model, delegate, graph);\n\n  // Graph outputs.\n  output_tensors.SetName(\"output_tensors\").ConnectTo(graph.Out(0));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Converting NDC to Pixel Space in MediaPipe\nDESCRIPTION: Formula for converting coordinates from Normalized Device Coordinates (NDC) space to pixel space, where the origin is at the upper-left corner of the image.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nx_pixel = (1 + x_ndc) / 2.0 * image_width\ny_pixel = (1 - y_ndc) / 2.0 * image_height\n```\n\n----------------------------------------\n\nTITLE: Region/BBox Keys in MediaPipe\nDESCRIPTION: Key-value pairs for handling bounding box coordinates, timestamps, annotations and region properties. Includes normalized coordinates (xmin, ymin, xmax, ymax) and metadata like timestamps and annotation flags.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nregion/bbox/ymin - feature list float list\nregion/bbox/xmin - feature list float list\nregion/bbox/ymax - feature list float list\nregion/bbox/xmax - feature list float list\nregion/bbox/* - special\nregion/timestamp - feature list int\nregion/num_regions - feature list int\nregion/is_annotated - feature list int\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV on macOS with Homebrew\nDESCRIPTION: Commands to install OpenCV version 3 using Homebrew package manager on macOS, with a workaround for the glog dependency issue.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n$ brew install opencv@3\n\n# There is a known issue caused by the glog dependency. Uninstall glog.\n$ brew uninstall --ignore-dependencies glog\n```\n\n----------------------------------------\n\nTITLE: Building iOS Application with Bazel\nDESCRIPTION: Bazel build command for compiling the MediaPipe iOS application. The command builds an optimized version of the application configured for ARM64 iOS devices.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config=ios_arm64 <$APPLICATION_PATH>:HelloWorldApp\n```\n\n----------------------------------------\n\nTITLE: Downloading YT8M Baseline Model\nDESCRIPTION: Commands to download and extract the YouTube-8M baseline model for inference.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl -o /tmp/mediapipe/yt8m_baseline_saved_model.tar.gz http://data.yt8m.org/models/baseline/saved_model.tar.gz\n\ntar -xvf /tmp/mediapipe/yt8m_baseline_saved_model.tar.gz -C /tmp/mediapipe\n```\n\n----------------------------------------\n\nTITLE: Configuring PacketClonerCalculator Graph in Proto\nDESCRIPTION: Example MediaPipe graph configuration showing input/output stream setup for PacketClonerCalculator.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_8\n\nLANGUAGE: proto\nCODE:\n```\ninput_stream: \"room_mic_signal\"\ninput_stream: \"room_lighting_sensor\"\ninput_stream: \"room_video_tick_signal\"\n\nnode {\n   calculator: \"PacketClonerCalculator\"\n   input_stream: \"room_mic_signal\"\n   input_stream: \"room_lighting_sensor\"\n   input_stream: \"room_video_tick_signal\"\n   output_stream: \"cloned_room_mic_signal\"\n   output_stream: \"cloned_lighting_sensor\"\n}\n```\n\n----------------------------------------\n\nTITLE: Debugging Tensors, cv::Mats, and ImageFrames in C++\nDESCRIPTION: These code snippets show how to use debug logging functions to visualize the contents of tensors, cv::Mats, and ImageFrames in the command line terminal.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\ndebug::LogTensor(tensor)\ndebug::LogMat(mat);\ndebug::LogImage(image_frame);\n```\n\n----------------------------------------\n\nTITLE: Documenting Objectron Subgraphs in Markdown\nDESCRIPTION: This markdown table describes two subgraphs in the Objectron module: ObjectronCpuSubgraph and ObjectronGpuSubgraph. It provides links to their respective .pbtxt files and briefly explains their functionality and execution environment.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/modules/objectron/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# objectron\n\nSubgraphs|Details\n:--- | :---\n[`ObjectronCpuSubgraph`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/objectron/objectron_cpu.pbtxt)| Detects and tracks 3D bounding boxes for objects. (CPU input, and inference is executed on CPU.)\n[`ObjectronGpuSubgraph`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/objectron/objectron_gpu.pbtxt)| Detects and tracks 3D bounding boxes for objects. (GPU input, and inference is executed on GPU.)\n```\n\n----------------------------------------\n\nTITLE: Projecting 3D Points to NDC Space in MediaPipe\nDESCRIPTION: Formula for projecting 3D points in camera coordinate system (X, Y, Z) to Normalized Device Coordinates (NDC) space using camera parameters (fx, fy) and (px, py).\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nx_ndc = -fx * X / Z + px\ny_ndc = -fy * Y / Z + py\nz_ndc = 1 / Z\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV on macOS with MacPorts\nDESCRIPTION: Command to install OpenCV using the MacPorts package manager on macOS.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n$ port install opencv\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3 and Dependencies on macOS\nDESCRIPTION: Commands to install Python 3 using Homebrew, set it as the default Python interpreter, and install the required 'six' library.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\n$ brew install python\n$ sudo ln -s -f /usr/local/bin/python3.7 /usr/local/bin/python\n$ python --version\nPython 3.7.4\n$ pip3 install --user six\n```\n\n----------------------------------------\n\nTITLE: Requesting Camera Access and Starting Camera\nDESCRIPTION: Requests user permission to access the camera and starts the camera capture session if granted, using the dedicated video queue.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_16\n\nLANGUAGE: Objective-C\nCODE:\n```\n[_cameraSource requestCameraAccessWithCompletionHandler:^void(BOOL granted) {\n  if (granted) {\n    dispatch_async(_videoQueue, ^{\n      [_cameraSource start];\n    });\n  }\n}];\n```\n\n----------------------------------------\n\nTITLE: Custom OpenCV 4 WORKSPACE Configuration\nDESCRIPTION: Bazel WORKSPACE configuration for manually built OpenCV 4 installed in /usr/local. This is used when building OpenCV 4 from source.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# WORKSPACE\nnew_local_repository(\n  name = \"linux_opencv\",\n  build_file = \"@//third_party:opencv_linux.BUILD\",\n  path = \"/usr/local\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe Android Solutions Dependencies in Gradle\nDESCRIPTION: Configuration for adding MediaPipe solution dependencies to an Android Studio project's Gradle file. Includes the core solution dependency and optional solution-specific dependencies like Face Detection, Face Mesh, and Hands.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/android_solutions.md#2025-04-23_snippet_0\n\nLANGUAGE: gradle\nCODE:\n```\ndependencies {\n    // MediaPipe solution-core is the foundation of any MediaPipe Solutions.\n    implementation 'com.google.mediapipe:solution-core:latest.release'\n    // Optional: MediaPipe Face Detection Solution.\n    implementation 'com.google.mediapipe:facedetection:latest.release'\n    // Optional: MediaPipe Face Mesh Solution.\n    implementation 'com.google.mediapipe:facemesh:latest.release'\n    // Optional: MediaPipe Hands Solution.\n    implementation 'com.google.mediapipe:hands:latest.release'\n}\n```\n\n----------------------------------------\n\nTITLE: Generating MediaSequence Metadata\nDESCRIPTION: Command to generate MediaSequence metadata from an input video, specifying the video path and clip duration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m mediapipe.examples.desktop.youtube8m.generate_input_sequence_example \\\n  --path_to_input_video=/absolute/path/to/the/local/video/file \\\n  --clip_end_time_sec=120\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Hello World Example\nDESCRIPTION: Commands to run the Hello World example in MediaPipe, demonstrating both CPU-only and GPU-enabled execution. This verifies that the MediaPipe installation is working correctly.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ export GLOG_logtostderr=1\n\n# if you are running on Linux desktop with CPU only\n$ bazel run --define MEDIAPIPE_DISABLE_GPU=1 \\\n    mediapipe/examples/desktop/hello_world:hello_world\n\n# If you are running on Linux desktop with GPU support enabled (via mesa drivers)\n$ bazel run --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\\n    mediapipe/examples/desktop/hello_world:hello_world\n\n# Should print:\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n```\n\n----------------------------------------\n\nTITLE: Converting Focal Length from Pixel to NDC Space in MediaPipe\nDESCRIPTION: Formula for converting camera focal length parameters from pixel space to Normalized Device Coordinates (NDC) space based on image dimensions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nfx = fx_pixel * 2.0 / image_width\nfy = fy_pixel * 2.0 / image_height\n```\n\n----------------------------------------\n\nTITLE: Converting Principal Point from Pixel to NDC Space in MediaPipe\nDESCRIPTION: Formula for converting camera principal point parameters from pixel space to Normalized Device Coordinates (NDC) space based on image dimensions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\npx = -px_pixel * 2.0 / image_width  + 1.0\npy = -py_pixel * 2.0 / image_height + 1.0\n```\n\n----------------------------------------\n\nTITLE: Configuring FFmpeg Library in BUILD File for Linux\nDESCRIPTION: Configuration for the FFmpeg cc_library rule in the ffmpeg_linux.BUILD file, specifying library paths, header files, and linker options.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncc_library(\n    name = \"libffmpeg\",\n    srcs = glob(\n        [\n            \"lib/libav*.so\",\n        ],\n    ),\n    hdrs = glob([\"include/libav*/*.h\"]),\n    includes = [\"include\"],\n    linkopts = [\n        \"-lavcodec\",\n        \"-lavformat\",\n        \"-lavutil\",\n    ],\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode Command Line Tools for MediaPipe iOS Development\nDESCRIPTION: Command to install the Xcode Command Line Tools, which are required for iOS development with MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Defining Video Queue Label Constant\nDESCRIPTION: Creates a constant string identifier for the video processing queue.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_10\n\nLANGUAGE: Objective-C\nCODE:\n```\nstatic const char* kVideoQueueLabel = \"com.google.mediapipe.example.videoQueue\";\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Library in BUILD File for MacPorts\nDESCRIPTION: Configuration for the OpenCV cc_library rule in the opencv_macos.BUILD file for MacPorts installation, specifying library paths, header files, and include directories.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ncc_library(\n    name = \"opencv\",\n    srcs = glob(\n        [\n            \"local/lib/libopencv_core.dylib\",\n            \"local/lib/libopencv_highgui.dylib\",\n            \"local/lib/libopencv_imgcodecs.dylib\",\n            \"local/lib/libopencv_imgproc.dylib\",\n            \"local/lib/libopencv_video.dylib\",\n            \"local/lib/libopencv_videoio.dylib\",\n        ],\n    ),\n    hdrs = glob([\"local/include/opencv2/**/*.h*\"]),\n    includes = [\"local/include/\"],\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Defining MediaPipe JNI Binary and Library in Bazel BUILD File\nDESCRIPTION: Creating cc_binary and cc_library rules to build and use the MediaPipe JNI framework in an Android application.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_23\n\nLANGUAGE: Bazel\nCODE:\n```\ncc_binary(\n    name = \"libmediapipe_jni.so\",\n    linkshared = 1,\n    linkstatic = 1,\n    deps = [\n        \"//mediapipe/java/com/google/mediapipe/framework/jni:mediapipe_framework_jni\",\n    ],\n)\n\ncc_library(\n    name = \"mediapipe_jni_lib\",\n    srcs = [\":libmediapipe_jni.so\"],\n    alwayslink = 1,\n)\n```\n\n----------------------------------------\n\nTITLE: Building Android Objectron Example with Single-stage Model\nDESCRIPTION: Bash commands for building the MediaPipe Objectron Android application with single-stage models for different object categories including shoes and chairs.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config android_arm64 --define shoe_1stage=true mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetection3d:objectdetection3d\n```\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config android_arm64 --define chair_1stage=true mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetection3d:objectdetection3d\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe Hands for CPU in C++\nDESCRIPTION: Command to build the MediaPipe Hands solution for CPU execution using Bazel. The MEDIAPIPE_DISABLE_GPU flag ensures the application runs without GPU acceleration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/cpp.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/hand_tracking:hand_tracking_cpu\n```\n\n----------------------------------------\n\nTITLE: Requesting Camera Permissions in MainActivity\nDESCRIPTION: This Java code demonstrates how to request camera permissions using MediaPipe's PermissionHelper in the MainActivity's onCreate method.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nPermissionHelper.checkAndRequestCameraPermissions(this);\n```\n\n----------------------------------------\n\nTITLE: Configuring FFmpeg Library in BUILD File for MacPorts\nDESCRIPTION: Configuration for the FFmpeg cc_library rule in the ffmpeg_macos.BUILD file for MacPorts installation, specifying library paths, header files, and linker options.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ncc_library(\n    name = \"libffmpeg\",\n    srcs = glob(\n        [\n            \"local/lib/libav*.dylib\",\n        ],\n    ),\n    hdrs = glob([\"local/include/libav*/*.h\"]),\n    includes = [\"local/include/\"],\n    linkopts = [\n        \"-lavcodec\",\n        \"-lavformat\",\n        \"-lavutil\",\n    ],\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Bazelisk via Homebrew for MediaPipe iOS Development\nDESCRIPTION: Commands to install Bazelisk using Homebrew, which is the recommended build system for MediaPipe projects.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbrew install bazelisk\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Dependencies on Debian/Ubuntu\nDESCRIPTION: Command to install the required OpenCV libraries using apt-get package manager. These packages are essential for MediaPipe to work with computer vision functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo apt-get install -y \\\n    libopencv-core-dev \\\n    libopencv-highgui-dev \\\n    libopencv-calib3d-dev \\\n    libopencv-features2d-dev \\\n    libopencv-imgproc-dev \\\n    libopencv-video-dev\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Hello World Example on CentOS\nDESCRIPTION: Commands to set up logging and run the Hello World example application using Bazel on CentOS, with GPU disabled for CPU-only environments.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n$ export GLOG_logtostderr=1\n# Need bazel flag 'MEDIAPIPE_DISABLE_GPU=1' if you are running on Linux desktop with CPU only\n$ bazel run --define MEDIAPIPE_DISABLE_GPU=1 \\\n    mediapipe/examples/desktop/hello_world:hello_world\n\n# Should print:\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe Hands for GPU in C++\nDESCRIPTION: Command to build the MediaPipe Hands solution for GPU execution on Linux. The build includes specific compiler options to handle OpenGL ES without X11 dependencies.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/cpp.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 \\\n  mediapipe/examples/desktop/hand_tracking:hand_tracking_gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Additional OpenCV Dependencies for Debian 11/Ubuntu 21.04\nDESCRIPTION: Command to install the OpenCV contrib library for Debian 11 or Ubuntu 21.04, which is required for OpenCV 4.5 compatibility with MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo apt-get install -y libopencv-contrib-dev\n```\n\n----------------------------------------\n\nTITLE: Decoupled Node Construction in C++ (Best Practice)\nDESCRIPTION: This snippet demonstrates the recommended way of constructing graph nodes with proper decoupling, improving code readability, maintainability, and reusability.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).Cast<A>();\n\n  // `node1` usage is limited to 3 lines below.\n  auto& node1 = graph.AddNode(\"Calculator1\");\n  a.ConnectTo(node1.In(\"INPUT\"));\n  Stream<B> b = node1.Out(\"OUTPUT\").Cast<B>();\n\n  // `node2` usage is limited to 3 lines below.\n  auto& node2 = graph.AddNode(\"Calculator2\");\n  b.ConnectTo(node2.In(\"INPUT\"));\n  Stream<C> c = node2.Out(\"OUTPUT\").Cast<C>();\n\n  // `node3` usage is limited to 4 lines below.\n  auto& node3 = graph.AddNode(\"Calculator3\");\n  b.ConnectTo(node3.In(\"INPUT_B\"));\n  c.ConnectTo(node3.In(\"INPUT_C\"));\n  Stream<D> d = node3.Out(\"OUTPUT\").Cast<D>();\n\n  // `node4` usage is limited to 5 lines below.\n  auto& node4 = graph.AddNode(\"Calculator4\");\n  b.ConnectTo(node4.In(\"INPUT_B\"));\n  c.ConnectTo(node4.In(\"INPUT_C\"));\n  d.ConnectTo(node4.In(\"INPUT_D\"));\n  Stream<E> e = node4.Out(\"OUTPUT\").Cast<E>();\n\n  // Outputs.\n  b.SetName(\"b\").ConnectTo(graph.Out(0));\n  c.SetName(\"c\").ConnectTo(graph.Out(1));\n  d.SetName(\"d\").ConnectTo(graph.Out(2));\n  e.SetName(\"e\").ConnectTo(graph.Out(3));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Camera with Specified Facing in MainActivity\nDESCRIPTION: This Java code starts the camera using the facing direction specified in the application metadata.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_17\n\nLANGUAGE: Java\nCODE:\n```\nCameraHelper.CameraFacing cameraFacing =\n    applicationInfo.metaData.getBoolean(\"cameraFacingFront\", false)\n        ? CameraHelper.CameraFacing.FRONT\n        : CameraHelper.CameraFacing.BACK;\ncameraHelper.startCamera(this, cameraFacing, /*unusedSurfaceTexture=*/ null);\n```\n\n----------------------------------------\n\nTITLE: Configuring WORKSPACE for Custom OpenCV and FFmpeg Paths\nDESCRIPTION: Configuration settings in the WORKSPACE file to specify custom paths for OpenCV and FFmpeg libraries when manually installed.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nnew_local_repository(\n    name = \"linux_opencv\",\n    build_file = \"@//third_party:opencv_linux.BUILD\",\n    path = \"/usr/local\",\n)\n\nnew_local_repository(\n    name = \"linux_ffmpeg\",\n    build_file = \"@//third_party:ffmpeg_linux.BUILD\",\n    path = \"/usr/local\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Symlink for Custom Provisioning Profile in MediaPipe\nDESCRIPTION: Command to symlink a custom provisioning profile to the MediaPipe directory, which is required for custom provisioning setup.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd mediapipe\nln -s ~/Downloads/MyProvisioningProfile.mobileprovision mediapipe/provisioning_profile.mobileprovision\n```\n\n----------------------------------------\n\nTITLE: WORKSPACE Configuration for OpenCV 2/3\nDESCRIPTION: Bazel WORKSPACE configuration for OpenCV 2/3 installed from Debian packages. This defines the repository location for the OpenCV libraries.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# WORKSPACE\nnew_local_repository(\n  name = \"linux_opencv\",\n  build_file = \"@//third_party:opencv_linux.BUILD\",\n  path = \"/usr\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Bazel BUILD File for iOS MediaPipe Application\nDESCRIPTION: Bazel BUILD file that defines the iOS application target and its dependencies. It sets up the iOS application build rules including bundle identifier, deployment target, and includes the necessary source files and frameworks for the HelloWorld app.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_3\n\nLANGUAGE: starlark\nCODE:\n```\nMIN_IOS_VERSION = \"12.0\"\n\nload(\n    \"@build_bazel_rules_apple//apple:ios.bzl\",\n    \"ios_application\",\n)\n\nios_application(\n    name = \"HelloWorldApp\",\n    bundle_id = \"com.google.mediapipe.HelloWorld\",\n    families = [\n        \"iphone\",\n        \"ipad\",\n    ],\n    infoplists = [\"Info.plist\"],\n    minimum_os_version = MIN_IOS_VERSION,\n    provisioning_profile = \"//mediapipe/examples/ios:developer_provisioning_profile\",\n    deps = [\":HelloWorldAppLibrary\"],\n)\n\nobjc_library(\n    name = \"HelloWorldAppLibrary\",\n    srcs = [\n        \"AppDelegate.m\",\n        \"ViewController.m\",\n        \"main.m\",\n    ],\n    hdrs = [\n        \"AppDelegate.h\",\n        \"ViewController.h\",\n    ],\n    data = [\n        \"Base.lproj/LaunchScreen.storyboard\",\n        \"Base.lproj/Main.storyboard\",\n    ],\n    sdk_frameworks = [\n        \"UIKit\",\n    ],\n    deps = [],\n)\n```\n\n----------------------------------------\n\nTITLE: Building a MediaPipe Android Example in Docker\nDESCRIPTION: Commands to set up Android SDK/NDK in the Docker container and build a MediaPipe Android object detection example with GPU support for ARM64 architecture.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\n$ docker run -it --name mediapipe mediapipe:latest\n\nroot@bca08b91ff63:/mediapipe# bash ./setup_android_sdk_and_ndk.sh\n\nroot@bca08b91ff63:/mediapipe# bazel build -c opt --config=android_arm64 mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetectiongpu:objectdetectiongpu\n```\n\n----------------------------------------\n\nTITLE: Cross-compilation Build Commands\nDESCRIPTION: Bazel build commands for cross-compiling MediaPipe applications for ARM32 and ARM64 architectures with Edge TPU support.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/coral/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# For ARM32 (e.g. Raspberry Pi)\nbazel build \\\n    --crosstool_top=@crosstool//:toolchains \\\n    --compiler=gcc \\\n    --cpu=armv7a \\\n    --define darwinn_portable=1 \\\n    --define MEDIAPIPE_DISABLE_GPU=1 \\\n    --define MEDIAPIPE_EDGE_TPU=usb \\\n    --linkopt=-l:libusb-1.0.so \\\n    mediapipe/examples/coral:face_detection_tpu build\n\n# For ARM64 (e.g. Coral Dev Board)\nbazel build \\\n    --crosstool_top=@crosstool//:toolchains \\\n    --compiler=gcc \\\n    --cpu=aarch64 \\\n    --define darwinn_portable=1 \\\n    --define MEDIAPIPE_DISABLE_GPU=1 \\\n    --define MEDIAPIPE_EDGE_TPU=usb \\\n    --linkopt=-l:libusb-1.0.so \\\n    mediapipe/examples/coral:face_detection_tpu build\n```\n\n----------------------------------------\n\nTITLE: Initializing Android Asset Manager for MediaPipe in Java\nDESCRIPTION: This code snippet initializes the Android asset manager to allow MediaPipe native libraries to access app assets, such as binary graphs. It should be called in the onCreate method before initializing eglManager.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_28\n\nLANGUAGE: Java\nCODE:\n```\n// Initialize asset manager so that MediaPipe native libraries can access the app assets, e.g.,\n// binary graphs.\nAndroidAssetUtil.initializeNativeAssetManager(this);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Preview Display View in MainActivity\nDESCRIPTION: This Java method sets up the preview display view by adding it to the FrameLayout defined in the layout XML.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_13\n\nLANGUAGE: Java\nCODE:\n```\nprivate void setupPreviewDisplayView() {\n  previewDisplayView.setVisibility(View.GONE);\n  ViewGroup viewGroup = findViewById(R.id.preview_display_layout);\n  viewGroup.addView(previewDisplayView);\n}\n```\n\n----------------------------------------\n\nTITLE: FFmpeg BUILD Configuration\nDESCRIPTION: BUILD file configuration for FFmpeg libraries installed from Debian packages. This defines the linker options for the FFmpeg libraries.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# ffmpeg_linux.BUILD for FFmpeg installed from Debian package\ncc_library(\n  name = \"libffmpeg\",\n  linkopts = [\n    \"-l:libavcodec.so\",\n    \"-l:libavformat.so\",\n    \"-l:libavutil.so\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Timestamp Change Configuration in Node API\nDESCRIPTION: Example of configuring timestamp behavior and stream handler in the new Node API.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nMEDIAPIPE_NODE_CONTRACT(kMain, kLoop, kPrevLoop,\n                        StreamHandler(\"ImmediateInputStreamHandler\"),\n                        TimestampChange::Arbitrary());\n```\n\n----------------------------------------\n\nTITLE: Custom OpenCV 2/3 WORKSPACE Configuration\nDESCRIPTION: Bazel WORKSPACE configuration for manually built OpenCV 2/3 installed in /usr/local. This is needed when building OpenCV from source.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# WORKSPACE\nnew_local_repository(\n  name = \"linux_opencv\",\n  build_file = \"@//third_party:opencv_linux.BUILD\",\n  path = \"/usr/local\",\n)\n```\n\n----------------------------------------\n\nTITLE: Adding iOS Framework Dependencies in BUILD File\nDESCRIPTION: Specifies the required iOS frameworks and MediaPipe dependencies in the Bazel BUILD file for camera functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_17\n\nLANGUAGE: BUILD\nCODE:\n```\nsdk_frameworks = [\n    \"AVFoundation\",\n    \"CoreGraphics\",\n    \"CoreMedia\",\n],\ndeps = [\n    \"//mediapipe/objc:mediapipe_framework_ios\",\n    \"//mediapipe/objc:mediapipe_input_sources_ios\",\n    \"//mediapipe/objc:mediapipe_layer_renderer\",\n],\n```\n\n----------------------------------------\n\nTITLE: Processing Video Frames with MediaPipe Graph in Objective-C\nDESCRIPTION: Sends video frames to the MediaPipe graph for processing.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_26\n\nLANGUAGE: Objective-C\nCODE:\n```\n- (void)processVideoFrame:(CVPixelBufferRef)imageBuffer\n                timestamp:(CMTime)timestamp\n               fromSource:(MPPInputSource*)source {\n  if (source != _cameraSource) {\n    NSLog(@\"Unknown source: %@\", source);\n    return;\n  }\n  [self.mediapipeGraph sendPixelBuffer:imageBuffer\n                            intoStream:kInputStream\n                            packetType:MPPPacketTypePixelBuffer];\n}\n```\n\n----------------------------------------\n\nTITLE: Installing msvc-runtime on Windows\nDESCRIPTION: Command to install Visual C++ runtime dependencies via pip to resolve DLL load failures on Windows systems.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install msvc-runtime\n```\n\n----------------------------------------\n\nTITLE: Building a Calculator Graph with Improved Readability Using Blank Lines\nDESCRIPTION: Improved version of the calculator graph construction with blank lines to visually separate node definitions. This approach better aligns with the CalculatorGraphConfig proto representation and makes the code much easier to read.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_17\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).Cast<A>();\n\n  auto& node1 = graph.AddNode(\"Calculator1\");\n  a.ConnectTo(node1.In(\"INPUT\"));\n  Stream<B> b = node1.Out(\"OUTPUT\").Cast<B>();\n\n  auto& node2 = graph.AddNode(\"Calculator2\");\n  b.ConnectTo(node2.In(\"INPUT\"));\n  Stream<C> c = node2.Out(\"OUTPUT\").Cast<C>();\n\n  auto& node3 = graph.AddNode(\"Calculator3\");\n  b.ConnectTo(node3.In(\"INPUT_B\"));\n  c.ConnectTo(node3.In(\"INPUT_C\"));\n  Stream<D> d = node3.Out(\"OUTPUT\").Cast<D>();\n\n  auto& node4 = graph.AddNode(\"Calculator4\");\n  b.ConnectTo(node4.In(\"INPUT_B\"));\n  c.ConnectTo(node4.In(\"INPUT_C\"));\n  d.ConnectTo(node4.In(\"INPUT_D\"));\n  Stream<E> e = node4.Out(\"OUTPUT\").Cast<E>();\n\n  // Outputs.\n  b.SetName(\"b\").ConnectTo(graph.Out(0));\n  c.SetName(\"c\").ConnectTo(graph.Out(1));\n  d.SetName(\"d\").ConnectTo(graph.Out(2));\n  e.SetName(\"e\").ConnectTo(graph.Out(3));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing ExternalTextureConverter in onResume Method\nDESCRIPTION: Creating an ExternalTextureConverter instance in the onResume method using the OpenGL context from EglManager.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nconverter = new ExternalTextureConverter(eglManager.getContext());\n```\n\n----------------------------------------\n\nTITLE: Custom OpenCV 2/3 BUILD Configuration\nDESCRIPTION: BUILD file configuration for manually built OpenCV 2/3 installed in /usr/local. This defines the library paths and linker options for custom-built OpenCV.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# opencv_linux.BUILD for OpenCV 2/3 installed to /usr/local\ncc_library(\n  name = \"opencv\",\n  linkopts = [\n    \"-L/usr/local/lib\",\n    \"-l:libopencv_core.so\",\n    \"-l:libopencv_calib3d.so\",\n    \"-l:libopencv_features2d.so\",\n    \"-l:libopencv_highgui.so\",\n    \"-l:libopencv_imgcodecs.so\",\n    \"-l:libopencv_imgproc.so\",\n    \"-l:libopencv_video.so\",\n    \"-l:libopencv_videoio.so\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Legacy Port Declaration in MediaPipe Calculator\nDESCRIPTION: Traditional approach for declaring and checking input ports using string tags in MediaPipe calculators.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nconstexpr char kSelectTag[] = \"SELECT\";\nif (cc->Inputs().HasTag(kSelectTag)) {\n  cc->Inputs().Tag(kSelectTag).Set<int>();\n}\n```\n\n----------------------------------------\n\nTITLE: FFmpeg WORKSPACE Configuration\nDESCRIPTION: Bazel WORKSPACE configuration for FFmpeg libraries. This defines the repository location for FFmpeg dependencies required by MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# WORKSPACE\nnew_local_repository(\n  name = \"linux_ffmpeg\",\n  build_file = \"@//third_party:ffmpeg_linux.BUILD\",\n  path = \"/usr\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe Graph Data Dependency in BUILD File\nDESCRIPTION: Adds the binary graph dependency for the edge detection example to the data section in the Bazel BUILD file.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_18\n\nLANGUAGE: BUILD\nCODE:\n```\n\"//mediapipe/graphs/edge_detection:mobile_gpu_binary_graph\",\n```\n\n----------------------------------------\n\nTITLE: Building a MediaPipe Hands iOS App Using Bazel\nDESCRIPTION: Command to build the MediaPipe Hands iOS application using Bazel with optimization for arm64 architecture.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config=ios_arm64 mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Palm Detection Subgraphs Reference Table\nDESCRIPTION: A markdown table listing the available palm detection subgraphs in MediaPipe with links to their implementation files and brief descriptions. The table includes CPU and GPU variants of the palm detection functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/modules/palm_detection/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# palm_detection\n\nSubgraphs|Details\n:--- | :---\n[`PalmDetectionCpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/palm_detection/palm_detection_cpu.pbtxt)| Detects palms/hands. (CPU input.)\n[`PalmDetectionGpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/palm_detection/palm_detection_gpu.pbtxt)| Detects palms/hands. (GPU input.)\n```\n\n----------------------------------------\n\nTITLE: Registering a MediaPipe Subgraph\nDESCRIPTION: This snippet demonstrates how to register a MediaPipe subgraph using the BUILD rule 'mediapipe_simple_subgraph'. It registers the 'TwoPassThroughSubgraph' for use in other graphs.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_3\n\nLANGUAGE: proto\nCODE:\n```\n# Small section of BUILD file for registering the \"TwoPassThroughSubgraph\"\n# subgraph for use by main graph main_pass_throughcals.pbtxt\n\nmediapipe_simple_subgraph(\n    name = \"twopassthrough_subgraph\",\n    graph = \"twopassthrough_subgraph.pbtxt\",\n    register_as = \"TwoPassThroughSubgraph\",\n    deps = [\n            \"//mediapipe/calculators/core:pass_through_calculator\",\n            \"//mediapipe/framework:calculator_graph\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Processing 3D Object Assets for MediaPipe Objectron\nDESCRIPTION: Bash commands for processing 3D object assets from .obj files to the custom .uuu format used by MediaPipe Objectron for rendering bounding boxes via the GlAnimationOverlayCalculator.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./mediapipe/graphs/object_detection_3d/obj_parser/obj_cleanup.sh [INPUT_DIR] [INTERMEDIATE_OUTPUT_DIR]\n```\n\nLANGUAGE: bash\nCODE:\n```\nbazel run -c opt mediapipe/graphs/object_detection_3d/obj_parser:ObjParser -- input_dir=[INTERMEDIATE_OUTPUT_DIR] output_dir=[OUTPUT_DIR]\n```\n\n----------------------------------------\n\nTITLE: OpenCV 2/3 BUILD Configuration\nDESCRIPTION: BUILD file configuration for OpenCV 2/3 installed from Debian packages. This defines the linker options for the OpenCV libraries.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# opencv_linux.BUILD for OpenCV 2/3 installed from Debian package\ncc_library(\n  name = \"opencv\",\n  linkopts = [\n    \"-l:libopencv_core.so\",\n    \"-l:libopencv_calib3d.so\",\n    \"-l:libopencv_features2d.so\",\n    \"-l:libopencv_highgui.so\",\n    \"-l:libopencv_imgcodecs.so\",\n    \"-l:libopencv_imgproc.so\",\n    \"-l:libopencv_video.so\",\n    \"-l:libopencv_videoio.so\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Application Metadata in MainActivity\nDESCRIPTION: This Java code retrieves the application metadata specified in the AndroidManifest.xml file to determine which camera to use.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_16\n\nLANGUAGE: Java\nCODE:\n```\ntry {\n  applicationInfo =\n      getPackageManager().getApplicationInfo(getPackageName(), PackageManager.GET_META_DATA);\n} catch (NameNotFoundException e) {\n  Log.e(TAG, \"Cannot find application info: \" + e);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing GPU Support Libraries\nDESCRIPTION: Command to install the required libraries for GPU acceleration on Linux. These packages enable MediaPipe to utilize GPU capabilities for improved performance.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Requires a GPU with EGL driver support.\n# Can use mesa GPU libraries for desktop, (or Nvidia/AMD equivalent).\nsudo apt-get install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\n\n# To compile with GPU support, replace\n--define MEDIAPIPE_DISABLE_GPU=1\n# with\n--copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11\n# when building GPU examples.\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe Graph Property in Objective-C\nDESCRIPTION: Declares a property for the MediaPipe graph in the ViewController interface.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_22\n\nLANGUAGE: Objective-C\nCODE:\n```\n// The MediaPipe graph currently in use. Initialized in viewDidLoad, started in viewWillAppear: and\n// sent video frames on _videoQueue.\n@property(nonatomic) MPPGraph* mediapipeGraph;\n```\n\n----------------------------------------\n\nTITLE: Updating ViewController Interface in Objective-C\nDESCRIPTION: Updates the ViewController interface to conform to MPPGraphDelegate and MPPInputSourceDelegate protocols.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_28\n\nLANGUAGE: Objective-C\nCODE:\n```\n@interface ViewController () <MPPGraphDelegate, MPPInputSourceDelegate>\n```\n\n----------------------------------------\n\nTITLE: Defining Side Packets in MediaPipe Graph (Protobuf)\nDESCRIPTION: Example of defining input and output side packets in a MediaPipe calculator graph configuration. Side packets are used for passing static data between calculators.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/visualizer.md#2025-04-23_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nnode {\n  calculator: \"SomeCalculator\"\n  input_side_packet: \"input_side_packet\"\n  output_side_packet: \"output_side_packet\"\n}\n```\n\n----------------------------------------\n\nTITLE: Registering New Calculator in BUILD\nDESCRIPTION: Bazel BUILD configuration example showing how to properly link a new calculator into MediaPipe with alwayslink option.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncc_library(\n    name = \"our_new_calculator\",\n    srcs = [\"our_new_calculator.cc\"],\n    deps = [ ... ],\n    alwayslink = True,\n)\n```\n\n----------------------------------------\n\nTITLE: Updating Layout XML for Camera Preview\nDESCRIPTION: This XML snippet updates the layout file to include a FrameLayout for camera preview and a TextView for displaying a message when camera access is not granted.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_10\n\nLANGUAGE: XML\nCODE:\n```\n<FrameLayout\n    android:id=\"@+id/preview_display_layout\"\n    android:layout_width=\"fill_parent\"\n    android:layout_height=\"fill_parent\"\n    android:layout_weight=\"1\">\n    <TextView\n        android:id=\"@+id/no_camera_access_view\"\n        android:layout_height=\"fill_parent\"\n        android:layout_width=\"fill_parent\"\n        android:gravity=\"center\"\n        android:text=\"@string/no_camera_access\" />\n</FrameLayout>\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe Binary Graph as an Asset in BUILD File\nDESCRIPTION: Configuring the Android binary rule to include the MediaPipe binary graph as an asset.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_25\n\nLANGUAGE: Bazel\nCODE:\n```\nassets = [\n  \"//mediapipe/graphs/edge_detection:mobile_gpu_binary_graph\",\n],\nassets_dir = \"\",\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe Calculator Dependencies to BUILD File\nDESCRIPTION: Adding dependencies to edge detection calculators in the MediaPipe JNI binary build rule.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_24\n\nLANGUAGE: Bazel\nCODE:\n```\n\"//mediapipe/graphs/edge_detection:mobile_calculators\",\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV on CentOS with Package Manager\nDESCRIPTION: Command to install OpenCV development packages using yum package manager on CentOS.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo yum install opencv-devel\n```\n\n----------------------------------------\n\nTITLE: Configuring Layer Renderer in viewDidLoad\nDESCRIPTION: Initializes and configures the renderer to display camera frames in the live view with appropriate scaling.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_13\n\nLANGUAGE: Objective-C\nCODE:\n```\n_renderer = [[MPPLayerRenderer alloc] init];\n_renderer.layer.frame = _liveView.layer.bounds;\n[_liveView.layer addSublayer:_renderer.layer];\n_renderer.frameScaleMode = MPPFrameScaleModeFillAndCrop;\n```\n\n----------------------------------------\n\nTITLE: Building and Running MediaPipe Hello World on Windows\nDESCRIPTION: Commands to build and run the MediaPipe Hello World example on Windows, demonstrating basic functionality of the framework.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_30\n\nLANGUAGE: batch\nCODE:\n```\nC:\\Users\\Username\\mediapipe_repo>bazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 --action_env PYTHON_BIN_PATH=\"C://python_36//python.exe\" mediapipe/examples/desktop/hello_world\n\nC:\\Users\\Username\\mediapipe_repo>set GLOG_logtostderr=1\n\nC:\\Users\\Username\\mediapipe_repo>bazel-bin\\mediapipe\\examples\\desktop\\hello_world\\hello_world.exe\n\n# should print:\n# I20200514 20:43:12.277598  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.278597  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.279618  1200 hello_world.cc:56] Hello World!\n# I20200514 20:43:12.280613  1200 hello_world.cc:56] Hello World!\n```\n\n----------------------------------------\n\nTITLE: Downloading PCA and Model Data\nDESCRIPTION: Commands to create a temporary directory and download necessary PCA matrices and model data for YouTube-8M feature extraction.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir /tmp/mediapipe\ncd /tmp/mediapipe\ncurl -O http://data.yt8m.org/pca_matrix_data/inception3_mean_matrix_data.pb\ncurl -O http://data.yt8m.org/pca_matrix_data/inception3_projection_matrix_data.pb\ncurl -O http://data.yt8m.org/pca_matrix_data/vggish_mean_matrix_data.pb\ncurl -O http://data.yt8m.org/pca_matrix_data/vggish_projection_matrix_data.pb\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\ntar -xvf /tmp/mediapipe/inception-2015-12-05.tgz\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Graph Configuration in Protobuf\nDESCRIPTION: This snippet shows the CalculatorGraphConfig content in protobuf format. It defines the graph structure with input and output streams, and two PassThroughCalculator nodes.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_cpp.md#2025-04-23_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\ninput_stream: \"in\"\noutput_stream: \"out\"\nnode {\n  calculator: \"PassThroughCalculator\"\n  input_stream: \"in\"\n  output_stream: \"out1\"\n}\nnode {\n  calculator: \"PassThroughCalculator\"\n  input_stream: \"out1\"\n  output_stream: \"out\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running AutoFlip with Custom Video and Aspect Ratio\nDESCRIPTION: Command to run the AutoFlip tool with a specified input video, output path, and target aspect ratio. The aspect_ratio parameter accepts width:height format for both landscape-to-portrait and portrait-to-landscape conversions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/autoflip.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/autoflip/run_autoflip \\\n  --calculator_graph_config_file=mediapipe/examples/desktop/autoflip/autoflip_graph.pbtxt \\\n  --input_side_packets=input_video_path=/absolute/path/to/the/local/video/file,output_video_path=/absolute/path/to/save/the/output/video/file,aspect_ratio=1:1\n```\n\n----------------------------------------\n\nTITLE: Adding Camera Permissions to AndroidManifest.xml\nDESCRIPTION: This snippet shows how to add camera permissions and features to the AndroidManifest.xml file. It also sets the minimum and target SDK versions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_7\n\nLANGUAGE: XML\nCODE:\n```\n<!-- For using the camera -->\n<uses-permission android:name=\"android.permission.CAMERA\" />\n<uses-feature android:name=\"android.hardware.camera\" />\n```\n\nLANGUAGE: XML\nCODE:\n```\n<uses-sdk\n    android:minSdkVersion=\"21\"\n    android:targetSdkVersion=\"34\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing Android Main Activity\nDESCRIPTION: Basic Android activity implementation that loads the main layout.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npackage com.google.mediapipe.apps.basic;\n\nimport android.os.Bundle;\nimport androidx.appcompat.app.AppCompatActivity;\n\n/** Bare-bones main activity. */\npublic class MainActivity extends AppCompatActivity {\n\n  @Override\n  protected void onCreate(Bundle savedInstanceState) {\n    super.onCreate(savedInstanceState);\n    setContentView(R.layout.activity_main);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Hello World Example on macOS\nDESCRIPTION: Commands to set up logging and run the Hello World example application using Bazel on macOS, with GPU disabled as desktop GPU is not currently supported.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\n$ export GLOG_logtostderr=1\n# Need bazel flag 'MEDIAPIPE_DISABLE_GPU=1' as desktop GPU is currently not supported\n$ bazel run --define MEDIAPIPE_DISABLE_GPU=1 \\\n    mediapipe/examples/desktop/hello_world:hello_world\n\n# Should print:\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n```\n\n----------------------------------------\n\nTITLE: Handling MediaPipe Graph Output in Objective-C\nDESCRIPTION: Implements a delegate method to receive and display processed frames from the MediaPipe graph.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_27\n\nLANGUAGE: Objective-C\nCODE:\n```\n- (void)mediapipeGraph:(MPPGraph*)graph\n   didOutputPixelBuffer:(CVPixelBufferRef)pixelBuffer\n             fromStream:(const std::string&)streamName {\n  if (streamName == kOutputStream) {\n    // Display the captured image on the screen.\n    CVPixelBufferRetain(pixelBuffer);\n    dispatch_async(dispatch_get_main_queue(), ^{\n      [_renderer renderPixelBuffer:pixelBuffer];\n      CVPixelBufferRelease(pixelBuffer);\n    });\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom OpenCV 4 BUILD Configuration\nDESCRIPTION: BUILD file configuration for manually built OpenCV 4 installed in /usr/local. This defines header files, include paths, and linker options specific to custom-built OpenCV 4.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# opencv_linux.BUILD for OpenCV 4 installed to /usr/local\ncc_library(\n  name = \"opencv\",\n  hdrs = glob([\n    \"include/opencv4/opencv2/**/*.h*\",\n  ]),\n  includes = [\n    \"include/opencv4/\",\n  ],\n  linkopts = [\n    \"-L/usr/local/lib\",\n    \"-l:libopencv_core.so\",\n    \"-l:libopencv_calib3d.so\",\n    \"-l:libopencv_features2d.so\",\n    \"-l:libopencv_highgui.so\",\n    \"-l:libopencv_imgcodecs.so\",\n    \"-l:libopencv_imgproc.so\",\n    \"-l:libopencv_video.so\",\n    \"-l:libopencv_videoio.so\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Working with List Feature Lists in Python and C++\nDESCRIPTION: API reference for functions to manipulate list feature lists in TensorFlow SequenceExamples. These functions allow checking feature presence, getting feature size, accessing features at specific indices, clearing features, adding features, getting feature keys, and getting default parsers for sequences of features.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n| python call | c++ call | description |\n|-------------|----------|-------------|\n|`has_feature(example [, prefix])`|`HasFeature([const string& prefix,] const tf::SE& example)`|Returns a boolean if the feature is present.|\n|`get_feature_size(example [, prefix])`|`GetFeatureSize([const string& prefix,] const tf::SE& example)`|Returns the number of feature sequences under this key. Will be 0 if the feature is absent.|\n|`get_feature_at(index, example [, prefix])`|`GetFeatureAt([const string& prefix,] const tf::SE& example, const int index)`|Returns a repeated feature of the appropriate type (comparable to list/vector of string, int64, float) at position index of the feature list.|\n|`clear_feature(example [, prefix])`|`ClearFeature([const string& prefix,] tf::SE* example)`|Clears the entire feature.|\n|`add_feature(value, example [, prefix])`|`AddFeature([const string& prefix,], const vector<TYPE>& value, tf::SE* example)`|Appends a sequence of features of the appropriate type to the feature list.|\n|`get_feature_key([prefix])`|`GetFeatureKey([const string& prefix])`|Returns the key used by related functions.|\n|`get_feature_default_parser()`| | Returns the tf.io.VarLenFeature for this type. (Python only.) |\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe with Mesa EGL Support\nDESCRIPTION: Bazel build command for MediaPipe with Mesa EGL support on Linux desktop, enabling GPU compute and rendering capabilities.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/gpu_support.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 <my-target>\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Building MediaPipe\nDESCRIPTION: Installs the required Python packages for building MediaPipe from source.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n(mp_env)mediapipe$ pip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing MPPCameraInputSource in iOS\nDESCRIPTION: Adds the import statement for MediaPipe's camera input source class to access the camera functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_4\n\nLANGUAGE: Objective-C\nCODE:\n```\n#import \"mediapipe/objc/MPPCameraInputSource.h\"\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Coral Examples\nDESCRIPTION: Commands for executing the compiled face detection and object detection examples using Edge TPU acceleration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/coral/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Face Detection\nGLOG_logtostderr=1 ./face_detection_tpu --calculator_graph_config_file \\\n    mediapipe/examples/coral/graphs/face_detection_desktop_live.pbtxt\n\n# Object Detection\nGLOG_logtostderr=1 ./object_detection_tpu --calculator_graph_config_file \\\n    mediapipe/examples/coral/graphs/object_detection_desktop_live.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for MediaPipe on WSL\nDESCRIPTION: Command to update package repositories and install essential build dependencies for MediaPipe in Windows Subsystem for Linux (WSL).\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nusername@DESKTOP-TMVLBJ1:~$ sudo apt-get update && sudo apt-get install -y build-essential git python zip adb openjdk-8-jdk\n```\n\n----------------------------------------\n\nTITLE: Initializing SurfaceView in MainActivity\nDESCRIPTION: This Java code initializes a SurfaceView for displaying camera preview in the MainActivity's onCreate method.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_12\n\nLANGUAGE: Java\nCODE:\n```\npreviewDisplayView = new SurfaceView(this);\nsetupPreviewDisplayView();\n```\n\n----------------------------------------\n\nTITLE: Defining MediaPipe Proto Library Build Rule\nDESCRIPTION: Build configuration for creating a calculator proto library using mediapipe_proto_library rule.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_6\n\nLANGUAGE: proto\nCODE:\n```\nmediapipe_proto_library(\n    name = \"packet_cloner_calculator_proto\",\n    srcs = [\"packet_cloner_calculator.proto\"],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"//mediapipe/framework:calculator_options_proto\",\n        \"//mediapipe/framework:calculator_proto\",\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Basic PassThroughCalculator Implementation with C++ Graph Builder\nDESCRIPTION: A basic C++ implementation of PassThroughCalculator configuration using the Graph Builder API. This approach requires manual management of input/output indexes and explicit type casting.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Graph inputs.\n  Stream<float> float_value = graph.In(0).SetName(\"float_value\").Cast<float>();\n  Stream<int> int_value = graph.In(1).SetName(\"int_value\").Cast<int>();\n  Stream<bool> bool_value = graph.In(2).SetName(\"bool_value\").Cast<bool>();\n\n  auto& pass_node = graph.AddNode(\"PassThroughCalculator\");\n  float_value.ConnectTo(pass_node.In(\"\")[0]);\n  int_value.ConnectTo(pass_node.In(\"\")[1]);\n  bool_value.ConnectTo(pass_node.In(\"\")[2]);\n  Stream<float> passed_float_value = pass_node.Out(\"\")[0].Cast<float>();\n  Stream<int> passed_int_value = pass_node.Out(\"\")[1].Cast<int>();\n  Stream<bool> passed_bool_value = pass_node.Out(\"\")[2].Cast<bool>();\n\n  // Graph outputs.\n  passed_float_value.SetName(\"passed_float_value\").ConnectTo(graph.Out(0));\n  passed_int_value.SetName(\"passed_int_value\").ConnectTo(graph.Out(1));\n  passed_bool_value.SetName(\"passed_bool_value\").ConnectTo(graph.Out(2));\n\n  // Get `CalculatorGraphConfig` to pass it into `CalculatorGraph`\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Building MediaPipe on Debian/Ubuntu\nDESCRIPTION: Installs the required system dependencies for building MediaPipe from source on Debian or Ubuntu.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo apt install python3-dev\n$ sudo apt install python3-venv\n$ sudo apt install -y protobuf-compiler\n\n# If you need to build opencv from source.\n$ sudo apt install cmake\n```\n\n----------------------------------------\n\nTITLE: Adding Storage Permissions for Android (XML)\nDESCRIPTION: This XML snippet shows how to add storage permissions to an Android app's manifest file, which is necessary for writing trace logs on Android devices.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/tracing_and_profiling.md#2025-04-23_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<uses-permission android:name=\"android.permission.MANAGE_EXTERNAL_STORAGE\" />\n```\n\n----------------------------------------\n\nTITLE: Building Object Detection TensorFlow Example on Desktop\nDESCRIPTION: Command to build the object detection application using TensorFlow model on desktop with CPU inference. This command builds TensorFlow targets from scratch which may take up to 30 minutes for the first time.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 --define no_aws_support=true --linkopt=-s \\\nmediapipe/examples/desktop/object_detection:object_detection_tensorflow\n```\n\n----------------------------------------\n\nTITLE: Building MediaPipe without GL Compute\nDESCRIPTION: Bazel build command for MediaPipe without GL compute support, suitable for systems that don't support OpenGL ES 3.1+.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/gpu_support.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel build --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 --copt -DMEDIAPIPE_DISABLE_GL_COMPUTE <my-target>\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository in WSL\nDESCRIPTION: Commands to clone the MediaPipe repository with limited depth and navigate to the project directory in Windows Subsystem for Linux.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nusername@DESKTOP-TMVLBJ1:~$ git clone --depth 1 https://github.com/google/mediapipe.git\n\nusername@DESKTOP-TMVLBJ1:~$ cd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Adding Camera Facing Metadata to AndroidManifest.xml\nDESCRIPTION: This XML snippet adds metadata to the AndroidManifest.xml file to specify which camera (front or back) should be used by default.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_15\n\nLANGUAGE: XML\nCODE:\n```\n      ...\n      <meta-data android:name=\"cameraFacingFront\" android:value=\"${cameraFacingFront}\"/>\n  </application>\n</manifest>\n```\n\n----------------------------------------\n\nTITLE: Declaring Constants for MediaPipe Graph in Objective-C\nDESCRIPTION: Defines static constants for the graph name, input stream, and output stream.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_21\n\nLANGUAGE: Objective-C\nCODE:\n```\nstatic NSString* const kGraphName = @\"mobile_gpu\";\n\nstatic const char* kInputStream = \"input_video\";\nstatic const char* kOutputStream = \"output_video\";\n```\n\n----------------------------------------\n\nTITLE: PassThroughCalculator Example in Protocol Buffers\nDESCRIPTION: A protocol buffer configuration for a simple graph using PassThroughCalculator to forward multiple typed inputs to corresponding outputs. The ordering of inputs and outputs must be maintained explicitly.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_4\n\nLANGUAGE: proto\nCODE:\n```\ninput_stream: \"float_value\"\ninput_stream: \"int_value\"\ninput_stream: \"bool_value\"\n\noutput_stream: \"passed_float_value\"\noutput_stream: \"passed_int_value\"\noutput_stream: \"passed_bool_value\"\n\nnode {\n  calculator: \"PassThroughCalculator\"\n  input_stream: \"float_value\"\n  input_stream: \"int_value\"\n  input_stream: \"bool_value\"\n  # The order must be the same as for inputs (or you can use explicit indexes)\n  output_stream: \"passed_float_value\"\n  output_stream: \"passed_int_value\"\n  output_stream: \"passed_bool_value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Node Contract Declaration in MediaPipe\nDESCRIPTION: Declarative way to set up calculator contracts using the new Node API.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nMEDIAPIPE_NODE_CONTRACT(kInput, kOutput);\n```\n\n----------------------------------------\n\nTITLE: Downloading YT8M Dataset\nDESCRIPTION: Command to download a shard of the YouTube-8M training data for model inference.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://us.data.yt8m.org/2/frame/train/trainpj.tfrecord --output /tmp/mediapipe/trainpj.tfrecord\n```\n\n----------------------------------------\n\nTITLE: Setting TensorFlow CUDA Paths\nDESCRIPTION: Export command to specify CUDA library paths for TensorFlow integration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/gpu_support.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport TF_CUDA_PATHS=/usr/local/cuda-10.1,/usr/lib/x86_64-linux-gnu,/usr/include\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Building MediaPipe on macOS\nDESCRIPTION: Installs the required system dependencies for building MediaPipe from source on macOS using Homebrew.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ brew install protobuf\n\n# If you need to build opencv from source.\n$ brew install cmake\n```\n\n----------------------------------------\n\nTITLE: Configuring Cyclic Graph in MediaPipe Proto\nDESCRIPTION: Proto configuration for a cyclic graph including back edge annotation and custom input stream handler. Defines nodes for counting, adding integers, and implementing delay in a cyclic graph structure.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/graphs.md#2025-04-23_snippet_8\n\nLANGUAGE: proto\nCODE:\n```\nnode {\n  calculator: 'GlobalCountSourceCalculator'\n  input_side_packet: 'global_counter'\n  output_stream: 'integers'\n}\nnode {\n  calculator: 'IntAdderCalculator'\n  input_stream: 'integers'\n  input_stream: 'old_sum'\n  input_stream_info: {\n    tag_index: ':1'  # 'old_sum'\n    back_edge: true\n  }\n  output_stream: 'sum'\n  input_stream_handler {\n    input_stream_handler: 'EarlyCloseInputStreamHandler'\n  }\n}\nnode {\n  calculator: 'UnitDelayCalculator'\n  input_stream: 'sum'\n  output_stream: 'old_sum'\n}\n```\n\n----------------------------------------\n\nTITLE: Coupled Node Construction in C++ (Poor Practice)\nDESCRIPTION: This snippet shows a poor practice of constructing graph nodes with tight coupling, making the code difficult to refactor and maintain.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).Cast<A>();\n\n  auto& node1 = graph.AddNode(\"Calculator1\");\n  a.ConnectTo(node1.In(\"INPUT\"));\n\n  auto& node2 = graph.AddNode(\"Calculator2\");\n  node1.Out(\"OUTPUT\").ConnectTo(node2.In(\"INPUT\"));  // Bad.\n\n  auto& node3 = graph.AddNode(\"Calculator3\");\n  node1.Out(\"OUTPUT\").ConnectTo(node3.In(\"INPUT_B\"));  // Bad.\n  node2.Out(\"OUTPUT\").ConnectTo(node3.In(\"INPUT_C\"));  // Bad.\n\n  auto& node4 = graph.AddNode(\"Calculator4\");\n  node1.Out(\"OUTPUT\").ConnectTo(node4.In(\"INPUT_B\"));  // Bad.\n  node2.Out(\"OUTPUT\").ConnectTo(node4.In(\"INPUT_C\"));  // Bad.\n  node3.Out(\"OUTPUT\").ConnectTo(node4.In(\"INPUT_D\"));  // Bad.\n\n  // Outputs.\n  node1.Out(\"OUTPUT\").SetName(\"b\").ConnectTo(graph.Out(0));  // Bad.\n  node2.Out(\"OUTPUT\").SetName(\"c\").ConnectTo(graph.Out(1));  // Bad.\n  node3.Out(\"OUTPUT\").SetName(\"d\").ConnectTo(graph.Out(2));  // Bad.\n  node4.Out(\"OUTPUT\").SetName(\"e\").ConnectTo(graph.Out(3));  // Bad.\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Hair Segmentation Graph Configuration\nDESCRIPTION: Protocol buffer configuration for the hair segmentation mobile GPU graph that defines the processing pipeline\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/hair_segmentation.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nmediapipe/graphs/hair_segmentation/hair_segmentation_mobile_gpu.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Implementing SurfaceHolder.Callback for MediaPipe Output Display in Java\nDESCRIPTION: These function definitions handle surface creation and destruction for displaying the MediaPipe processor output. They should be added to the custom SurfaceHolder.Callback implementation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_32\n\nLANGUAGE: Java\nCODE:\n```\n@Override\npublic void surfaceCreated(SurfaceHolder holder) {\n  processor.getVideoSurfaceOutput().setSurface(holder.getSurface());\n}\n\n@Override\npublic void surfaceDestroyed(SurfaceHolder holder) {\n  processor.getVideoSurfaceOutput().setSurface(null);\n}\n```\n\n----------------------------------------\n\nTITLE: Building Specific iOS Example with Bazel\nDESCRIPTION: Example Bazel build command that specifically targets the HelloWorldApp in the MediaPipe examples directory. This command builds the application for deployment to iOS ARM64 devices.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config=ios_arm64 mediapipe/examples/ios/helloworld:HelloWorldApp\n```\n\n----------------------------------------\n\nTITLE: Downloading TFLite Model for MediaPipe Image Classifier Benchmark\nDESCRIPTION: These commands change the directory to the benchmark location and download the EfficientNet Lite0 TFLite model for image classification.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/python/benchmark/vision/image_classifier/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd mediapipe/mediapipe/tasks/python/benchmark/vision/image_classifier\nwget -O classifier.tflite -q https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite0/float32/1/efficientnet_lite0.tflite\n```\n\n----------------------------------------\n\nTITLE: Installing MediaPipe and Launching Python Interpreter\nDESCRIPTION: Installs the MediaPipe Python package from PyPI and starts the Python interpreter.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n(mp_env)$ pip install mediapipe\n(mp_env)$ python3\n```\n\n----------------------------------------\n\nTITLE: Using Custom Type Bindings in Python\nDESCRIPTION: Example of importing and using custom type bindings in Python to create and manipulate MediaPipe packets with custom types.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport my_type_binding\nimport my_packet_methods\n\npacket = my_packet_methods.create_my_type(my_type_binding.MyType())\nmy_type = my_packet_methods.get_my_type(packet)\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata for Clip Classification in Python\nDESCRIPTION: Example showing how to create metadata for a clip classification task using the MediaSequence Python API. It sets the clip data path, start and end timestamps, and adds label indices and strings for classification.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Python: functions from media_sequence.py as ms\nsequence = tf.train.SequenceExample()\nms.set_clip_data_path(b\"path_to_video\", sequence)\nms.set_clip_start_timestamp(1000000, sequence)\nms.set_clip_end_timestamp(6000000, sequence)\nms.set_clip_label_index((4, 3), sequence)\nms.set_clip_label_string((b\"run\", b\"jump\"), sequence)\n```\n\n----------------------------------------\n\nTITLE: Building and Running Demo Dataset Pipeline\nDESCRIPTION: Commands to build the MediaPipe demo dataset binary and execute the dataset generation script. Requires TensorFlow installation and builds the media_sequence_demo binary with GPU disabled.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt mediapipe/examples/desktop/media_sequence:media_sequence_demo \\\n  --define MEDIAPIPE_DISABLE_GPU=1\n\npython -m mediapipe.examples.desktop.media_sequence.demo_dataset \\\n  --alsologtostderr \\\n  --path_to_demo_data=/tmp/demo_data/ \\\n  --path_to_mediapipe_binary=bazel-bin/mediapipe/examples/desktop/\\\nmedia_sequence/media_sequence_demo  \\\n  --path_to_graph_directory=mediapipe/graphs/media_sequence/\n```\n\n----------------------------------------\n\nTITLE: Adding Input Source Delegate Protocol to ViewController\nDESCRIPTION: Updates the ViewController interface to adopt the MPPInputSourceDelegate protocol to receive camera frames.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_7\n\nLANGUAGE: Objective-C\nCODE:\n```\n@interface ViewController () <MPPInputSourceDelegate>\n```\n\n----------------------------------------\n\nTITLE: Setting Camera Delegate and Queue\nDESCRIPTION: Configures the camera source to use the ViewController as its delegate and processes frames on a dedicated queue.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_8\n\nLANGUAGE: Objective-C\nCODE:\n```\n[_cameraSource setDelegate:self queue:_videoQueue];\n```\n\n----------------------------------------\n\nTITLE: Building Iris Tracking CPU Application\nDESCRIPTION: Bash command to build the MediaPipe Iris tracking application for CPU usage with video input.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/iris.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/iris_tracking:iris_tracking_cpu_video_input\n```\n\n----------------------------------------\n\nTITLE: Direct Projection from Camera to Pixel Coordinates in MediaPipe\nDESCRIPTION: Formula for directly projecting from camera coordinate system to pixel coordinates using camera parameters (fx_pixel, fy_pixel) and (px_pixel, py_pixel) defined in pixel space.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nx_pixel = -fx_pixel * X / Z + px_pixel\ny_pixel =  fy_pixel * Y / Z + py_pixel\n```\n\n----------------------------------------\n\nTITLE: Building Object Detection TFLite Example on Desktop\nDESCRIPTION: Command to build the object detection application using TFLite model on desktop with CPU inference. This disables GPU support and builds the application in optimized mode.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/object_detection:object_detection_tflite\n```\n\n----------------------------------------\n\nTITLE: Enabling Graph Runtime Monitoring in MediaPipe Configuration\nDESCRIPTION: This snippet demonstrates how to enable graph runtime monitoring in the MediaPipe graph configuration. It enables background capturing of graph runtime information which is written to LOG(INFO).\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_5\n\nLANGUAGE: protobuf\nCODE:\n```\ngraph {\n  runtime_info {\n    enable_graph_runtime_info: true\n  }\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata for Temporal Detection in Python\nDESCRIPTION: Example showing how to create metadata for a temporal detection task using the MediaSequence Python API. It sets the clip metadata and defines segment timestamps with corresponding labels for event detection.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Python: functions from media_sequence.py as ms\nsequence = tf.train.SequenceExample()\nms.set_clip_data_path(b\"path_to_video\", sequence)\nms.set_clip_start_timestamp(1000000, sequence)\nms.set_clip_end_timestamp(6000000, sequence)\n\nms.set_segment_start_timestamp((2000000, 4000000), sequence)\nms.set_segment_end_timestamp((3500000, 6000000), sequence)\nms.set_segment_label_index((4, 3), sequence)\nms.set_segment_label_string((b\"run\", b\"jump\"), sequence)\n```\n\n----------------------------------------\n\nTITLE: Creating High-Priority Video Processing Queue\nDESCRIPTION: Creates a serial dispatch queue with user-interactive QoS for processing camera frames efficiently.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_9\n\nLANGUAGE: Objective-C\nCODE:\n```\ndispatch_queue_attr_t qosAttribute = dispatch_queue_attr_make_with_qos_class(\n      DISPATCH_QUEUE_SERIAL, QOS_CLASS_USER_INTERACTIVE, /*relative_priority=*/0);\n_videoQueue = dispatch_queue_create(kVideoQueueLabel, qosAttribute);\n```\n\n----------------------------------------\n\nTITLE: Declaring Live View and Renderer in ViewController\nDESCRIPTION: Adds view and renderer objects to the ViewController implementation for displaying camera preview frames.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_12\n\nLANGUAGE: Objective-C\nCODE:\n```\n// Display the camera preview frames.\nIBOutlet UIView* _liveView;\n// Render frames in a layer.\nMPPLayerRenderer* _renderer;\n```\n\n----------------------------------------\n\nTITLE: Building Android Objectron Example with Two-stage Model\nDESCRIPTION: Bash commands for building the MediaPipe Objectron Android application with two-stage models for different object categories including shoes, chairs, cups, and cameras.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/objectron.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config android_arm64 mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetection3d:objectdetection3d\n```\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config android_arm64 --define chair=true mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetection3d:objectdetection3d\n```\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config android_arm64 --define cup=true mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetection3d:objectdetection3d\n```\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --config android_arm64 --define camera=true mediapipe/examples/android/src/java/com/google/mediapipe/apps/objectdetection3d:objectdetection3d\n```\n\n----------------------------------------\n\nTITLE: Configuring Calculator Options with Proto3\nDESCRIPTION: Example configuration for TfLiteInferenceCalculator showing how to specify node options using proto3 syntax.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/calculators.md#2025-04-23_snippet_5\n\nLANGUAGE: proto\nCODE:\n```\nnode {\n  calculator: \"TfLiteInferenceCalculator\"\n  input_stream: \"TENSORS:main_model_input\"\n  output_stream: \"TENSORS:main_model_output\"\n  node_options: {\n    [type.googleapis.com/mediapipe.TfLiteInferenceCalculatorOptions] {\n      model_path: \"mediapipe/models/detection_model.tflite\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Image Classifier Benchmark with Bazel\nDESCRIPTION: This command runs the image classifier benchmark using Bazel. It compiles and executes the benchmark in optimized mode.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/python/benchmark/vision/image_classifier/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel run -c opt //mediapipe/tasks/python/benchmark/vision/image_classifier:image_classifier_benchmark\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Flags for Clang Compatibility in MediaPipe\nDESCRIPTION: Adds a build flag to disable avxvnniint8 support in the CPU backend when using Clang 18 or older versions. This configuration should be added to the .bazelrc file to ensure compatibility.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_7\n\nLANGUAGE: bazelrc\nCODE:\n```\nbuild --define=xnn_enable_avxvnniint8=false\n```\n\n----------------------------------------\n\nTITLE: Adding Text Timestamp in MediaPipe (Python/C++)\nDESCRIPTION: Adds a timestamp for when a text token occurs, measured in microseconds. This is used to temporally align text with media content.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nadd_text_timestamp\n```\n\nLANGUAGE: c++\nCODE:\n```\nAddTextTimestamp\n```\n\n----------------------------------------\n\nTITLE: Setting Up Tulsi for MediaPipe Xcode Project Generation\nDESCRIPTION: Commands to clone, configure, and build Tulsi, which is a tool for generating Xcode projects from Bazel build configurations.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# cd out of the mediapipe directory, then:\ngit clone https://github.com/bazelbuild/tulsi.git\ncd tulsi\n# remove Xcode version from Tulsi's .bazelrc (see http://github.com/bazelbuild/tulsi#building-and-installing):\nsed -i .orig '/xcode_version/d' .bazelrc\n# build and run Tulsi:\nsh build_and_run.sh\n```\n\n----------------------------------------\n\nTITLE: Implementing View Appearance Method\nDESCRIPTION: Sets up the viewWillAppear method to prepare for camera activation when the view is about to be displayed.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_15\n\nLANGUAGE: Objective-C\nCODE:\n```\n-(void)viewWillAppear:(BOOL)animated {\n  [super viewWillAppear:animated];\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Running Feature Extraction\nDESCRIPTION: Commands to build and run the MediaPipe binary for extracting YouTube-8M features from the input video.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --linkopt=-s \\\n  --define MEDIAPIPE_DISABLE_GPU=1 --define no_aws_support=true \\\n  mediapipe/examples/desktop/youtube8m:extract_yt8m_features\n\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/youtube8m/extract_yt8m_features \\\n  --calculator_graph_config_file=mediapipe/graphs/youtube8m/feature_extraction.pbtxt \\\n  --input_side_packets=input_sequence_example=/tmp/mediapipe/metadata.pb  \\\n  --output_side_packets=output_sequence_example=/tmp/mediapipe/features.pb\n```\n\n----------------------------------------\n\nTITLE: Setting Text Language in MediaPipe (Python/C++)\nDESCRIPTION: Sets the language for the corresponding text content. This function is used to specify the language of text data in the MediaPipe context.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nset_text_langage\n```\n\nLANGUAGE: c++\nCODE:\n```\nSetTextLanguage\n```\n\n----------------------------------------\n\nTITLE: Building AutoFlip in MediaPipe\nDESCRIPTION: Command to build the AutoFlip pipeline with OpenCV 3 and CPU support. Note that AutoFlip currently only works with OpenCV 3.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/autoflip.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/autoflip:run_autoflip\n```\n\n----------------------------------------\n\nTITLE: Disabling OpenGL ES Support in MediaPipe Build\nDESCRIPTION: Bazel build command to disable OpenGL ES support when building MediaPipe targets on platforms where OpenGL ES is not available.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/gpu_support.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build --define MEDIAPIPE_DISABLE_GPU=1 <my-target>\n```\n\n----------------------------------------\n\nTITLE: Adding Text Token ID in MediaPipe (Python/C++)\nDESCRIPTION: Adds an integer ID for a text token. This is used for tokenized representations of text where each token has a unique identifier.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nadd_text_token_id\n```\n\nLANGUAGE: c++\nCODE:\n```\nAddTextTokenId\n```\n\n----------------------------------------\n\nTITLE: Setting Bazel Environment Variables for Windows\nDESCRIPTION: Commands to set necessary Bazel environment variables for building MediaPipe with MSVC on Windows. Points to Visual Studio and Windows SDK installations.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_28\n\nLANGUAGE: batch\nCODE:\n```\n# Please find the exact paths and version numbers from your local version.\nC:\\> set BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\nC:\\> set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\nC:\\> set BAZEL_VC_FULL_VERSION=<Your local VC version>\nC:\\> set BAZEL_WINSDK_FULL_VERSION=<Your local WinSDK version>\n```\n\n----------------------------------------\n\nTITLE: Setting FrameProcessor as Consumer for Converted Frames in Java\nDESCRIPTION: This code sets the FrameProcessor as the consumer for converted frames from the converter. It should be added to the onResume method after initializing the converter.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_31\n\nLANGUAGE: Java\nCODE:\n```\nconverter.setConsumer(processor);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Exported TFLite Graph with Graph Tool\nDESCRIPTION: This optional command uses the TensorFlow graph_transforms tool to inspect the input and output details of the exported TFLite graph.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/object_detection_saved_model.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ bazel run graph_transforms:summarize_graph -- \\\n    --in_graph=${PATH_TO_MODEL}/tflite_graph.pb\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Baseline Model in Bash\nDESCRIPTION: Commands to download and extract the YouTube-8M baseline model for inference.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -o /tmp/mediapipe/yt8m_baseline_saved_model.tar.gz http://data.yt8m.org/models/baseline/saved_model.tar.gz\n\ntar -xvf /tmp/mediapipe/yt8m_baseline_saved_model.tar.gz -C /tmp/mediapipe\n```\n\n----------------------------------------\n\nTITLE: Region Points and 3D Points in MediaPipe\nDESCRIPTION: Keys for handling 2D and 3D point coordinates, including radius values. Supports both individual coordinate access and batch operations through special accessor functions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nregion/point/x - feature list float list\nregion/point/y - feature list float list\nregion/point/* - special\nregion/radius - feature list float list\nregion/3d_point/x - feature list float list\nregion/3d_point/y - feature list float list\nregion/3d_point/z - feature list float list\nregion/3d_point/* - special\n```\n\n----------------------------------------\n\nTITLE: Cloning the MediaPipe Repository in Bash\nDESCRIPTION: Command to clone the MediaPipe repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/mediapipe.git\ncd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Defining Box Tracking Subgraph in MediaPipe\nDESCRIPTION: This snippet shows the structure of the box tracking subgraph in MediaPipe. It includes components for motion analysis, flow packaging, and box tracking.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/box_tracking.md#2025-04-23_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\n# mediapipe/graphs/tracking/subgraphs/box_tracking_gpu.pbtxt\n\ntype: \"BoxTrackingSubgraph\"\ninput_stream: \"IMAGE:input_video\"\ninput_stream: \"BOXES:start_pos\"\noutput_stream: \"BOXES:boxes\"\n\nnode {\n  calculator: \"MotionAnalysisCalculator\"\n  input_stream: \"IMAGE:input_video\"\n  output_stream: \"MOTION_METADATA:motion_metadata\"\n}\n\nnode {\n  calculator: \"FlowPackagerCalculator\"\n  input_stream: \"MOTION_METADATA:motion_metadata\"\n  output_stream: \"FLOW_PACKAGE:flow_package\"\n}\n\nnode {\n  calculator: \"BoxTrackerCalculator\"\n  input_stream: \"FLOW_PACKAGE:flow_package\"\n  input_stream: \"BOXES:start_pos\"\n  output_stream: \"BOXES:boxes\"\n}\n```\n\n----------------------------------------\n\nTITLE: Legacy Output Writing in MediaPipe\nDESCRIPTION: Traditional approach for sending output packets in MediaPipe calculators.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ncc->Outputs().Index(0).Add(\n    new std::pair<Packet, Packet>(cc->Inputs().Index(0).Value(),\n                                  cc->Inputs().Index(1).Value()),\n    cc->InputTimestamp());\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Python Virtual Environment for MediaPipe\nDESCRIPTION: Creates a Python virtual environment named 'mp_env' and activates it to prepare for MediaPipe installation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 -m venv mp_env && source mp_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Declaring ExternalTextureConverter Components in MainActivity\nDESCRIPTION: Declaring the necessary EglManager and ExternalTextureConverter objects in the MainActivity class for handling OpenGL texture conversion.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_18\n\nLANGUAGE: Java\nCODE:\n```\nprivate EglManager eglManager;\nprivate ExternalTextureConverter converter;\n```\n\n----------------------------------------\n\nTITLE: Running Iris Depth Estimation\nDESCRIPTION: Command to run the MediaPipe Iris depth estimation application with input and output image paths.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/iris.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/iris_tracking/iris_depth_from_image_desktop \\\n  --input_image_path=<input image path> --output_image_path=<output image path>\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference on Local Video\nDESCRIPTION: Command to run YouTube-8M model inference on a local video file using MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/youtube8m/model_inference \\\n  --calculator_graph_config_file=mediapipe/graphs/youtube8m/local_video_model_inference.pbtxt \\\n  --input_side_packets=input_sequence_example_path=/tmp/mediapipe/features.pb,input_video_path=/absolute/path/to/the/local/video/file,output_video_path=/tmp/mediapipe/annotated_video.mp4,segment_size=5,overlap=4\n```\n\n----------------------------------------\n\nTITLE: Segment-Related Keys in TensorFlow SequenceExamples\nDESCRIPTION: Reference for keys related to segments of clips in TensorFlow SequenceExamples. These include timestamps, indices, labels, and confidences for managing segments within multimedia clips.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_11\n\nLANGUAGE: markdown\nCODE:\n```\n| key | type | python call / c++ call | description |\n|-----|------|------------------------|-------------|\n|`segment/start/timestamp`|context int list|`set_segment_start_timestamp` / `SetSegmentStartTimestamp`|A list of segment start times in microseconds.|\n|`segment/start/index`|context int list|`set_segment_start_index` / `SetSegmentstartIndex`|A list of indices marking the first frame index >= the start time.|\n|`segment/end/timestamp`|context int list|`set_segment_end_timestamp` / `SetSegmentEndTimestamp`|A list of segment end times in microseconds.|\n|`segment/end/index`|context int list|`set_segment_end_index` / `SetSegmentEndIndex`|A list of indices marking the last frame index <= the end time.|\n|`segment/label/index`|context int list|`set_segment_label_index` / `SetSegmentLabelIndex`|A list with the label index for each segment. Multiple labels for the same segment are encoded as repeated segments.|\n|`segment/label/string`|context bytes list|`set_segment_label_string` / `SetSegmentLabelString`|A list with the label string for each segment. Multiple labels for the same segment are encoded as repeated segments.|\n|`segment/label/confidence`|context float list|`set_segment_label_confidence` / `SetSegmentLabelConfidence`|A list with the label confidence for each segment. Multiple labels for the same segment are encoded as repeated segments.|\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository\nDESCRIPTION: Commands to clone the MediaPipe repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone --depth 1 https://github.com/google/mediapipe.git\n\n# Change directory into MediaPipe root directory\n$ cd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Displaying Pose Detection Subgraphs in Markdown Table\nDESCRIPTION: A markdown table showcasing the available pose detection subgraphs in MediaPipe, including their names and brief descriptions. It lists both CPU and GPU versions of the pose detection subgraph.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/modules/pose_detection/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# pose_detection\n\nSubgraphs|Details\n:--- | :---\n[`PoseDetectionCpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/pose_detection/pose_detection_cpu.pbtxt)| Detects poses. (CPU input, and inference is executed on CPU.)\n[`PoseDetectionGpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/pose_detection/pose_detection_gpu.pbtxt)| Detects poses. (GPU input, and inference is executed on GPU.)\n```\n\n----------------------------------------\n\nTITLE: Logging Packet Addition and Input Queue Status in C++\nDESCRIPTION: This snippet demonstrates the output of DebugInputStreamHandler when adding a packet to an input stream and displaying the current status of input queues.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n[INFO] SomeCalculator: Adding packet (ts:2, type:int) to stream INPUT_B:0:input_b\n[INFO] SomeCalculator: INPUT_A:0:input_a num_packets: 0 min_ts: 2\n[INFO] SomeCalculator: INPUT_B:0:input_b num_packets: 1 min_ts: 2\n```\n\n----------------------------------------\n\nTITLE: Generating a Unique Bundle ID Prefix for MediaPipe iOS Apps\nDESCRIPTION: Python script command to generate a unique bundle ID prefix for MediaPipe iOS demo apps to avoid ID conflicts between different users.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 mediapipe/examples/ios/link_local_profiles.py\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Outputs in C++ (Best Practice)\nDESCRIPTION: This snippet demonstrates the recommended way of defining graph outputs at the end of the graph builder function, improving readability and maintainability.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nStream<F> RunSomething(Stream<Input> input, Graph& graph) {\n  // ...\n  return node.Out(\"OUTPUT_F\").Cast<F>();\n}\n\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // 10/100/N lines of code.\n  Stream<D> d = node.Out(\"OUTPUT_D\").Cast<D>();\n  // 10/100/N lines of code.\n  Stream<E> e = node.Out(\"OUTPUT_E\").Cast<E>();\n  // 10/100/N lines of code.\n  Stream<F> f = RunSomething(input, graph);\n  // ...\n\n  // Outputs.\n  d.SetName(\"output_d\").ConnectTo(graph.Out(0));\n  e.SetName(\"output_e\").ConnectTo(graph.Out(1));\n  f.SetName(\"output_f\").ConnectTo(graph.Out(2));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Converting 3D Assets to OpenGL-Ready Format with Bazel\nDESCRIPTION: Bazel command to run the ObjParser tool that converts preprocessed 3D assets into a .uuu format required by GlAnimationOverlayCalculator. This step generates the final OpenGL-compatible asset files.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/instant_motion_tracking.md#2025-04-23_snippet_1\n\nLANGUAGE: bazel\nCODE:\n```\nbazel run -c opt mediapipe/graphs/object_detection_3d/obj_parser:ObjParser -- input_dir=[INTERMEDIATE_OUTPUT_DIR] output_dir=[OUTPUT_DIR]\n```\n\n----------------------------------------\n\nTITLE: Legacy Input Access in MediaPipe Calculator\nDESCRIPTION: Traditional method for accessing input values in Process method.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nint select = cc->Inputs().Tag(kSelectTag).Get<int>();\n```\n\n----------------------------------------\n\nTITLE: Example-Related Keys in TensorFlow SequenceExamples\nDESCRIPTION: Reference for keys related to the entire example in TensorFlow SequenceExamples. These include identifiers, dataset names, and flags for managing multimedia data examples.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\n| key | type | python call / c++ call | description |\n|-----|------|------------------------|-------------|\n|`example/id`|context bytes|`set_example_id` / `SetExampleId`|A unique identifier for each example.|\n|`example/dataset_name`|context bytes|`set_example_dataset_name` / `SetExampleDatasetName`|The name of the data set, including the version.|\n|`example/dataset/flag/string`|context bytes list|`set_example_dataset_flag_string` / `SetExampleDatasetFlagString`|A list of bytes for dataset related attributes or flags for this example.\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository\nDESCRIPTION: Commands to clone the MediaPipe repository and navigate to the project directory.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/autoflip/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/mediapipe.git\ncd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Legacy Multiple Port Configuration\nDESCRIPTION: Traditional approach for configuring multiple input ports in MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int i = 0; i < cc->Inputs().NumEntries(); ++i) {\n  cc->Inputs().Index(i).SetAny();\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Incomplete Input Set in DebugInputStreamHandler in C++\nDESCRIPTION: This code snippet shows the output of DebugInputStreamHandler when a Calculator::Process call is triggered with an incomplete input set due to a timestamp bound increase.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n[INFO] SomeCalculator: Filled input set at ts: 1 with MISSING packets in input streams: INPUT_B:0:input_b.\n```\n\n----------------------------------------\n\nTITLE: Special Token List for MediaPipe Tokenization\nDESCRIPTION: A comprehensive list of special tokens including padding, unknown, classification, separator, and mask tokens, along with 839 reserved unused tokens for future use in the tokenization vocabulary.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/testdata/metadata/mobilebert_vocab.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n[PAD]\n[unused0]\n[unused1]\n...\n[UNK]\n[CLS]\n[SEP]\n[MASK]\n...\n[unused838]\n```\n\n----------------------------------------\n\nTITLE: Setting Text Duration in MediaPipe (Python/C++)\nDESCRIPTION: Sets the duration in microseconds for text tokens. This defines how long each text segment is displayed or relevant.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nadd_text_duration\n```\n\nLANGUAGE: c++\nCODE:\n```\nSetTextDuration\n```\n\n----------------------------------------\n\nTITLE: OpenCV 4 BUILD Configuration\nDESCRIPTION: BUILD file configuration for OpenCV 4 installed from Debian packages. This includes header files, include paths, and linker options specific to OpenCV 4.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# opencv_linux.BUILD for OpenCV 4 installed from Debian package\ncc_library(\n  name = \"opencv\",\n  hdrs = glob([\n    # Uncomment according to your multiarch value (gcc -print-multiarch):\n    #  \"include/aarch64-linux-gnu/opencv4/opencv2/cvconfig.h\",\n    #  \"include/arm-linux-gnueabihf/opencv4/opencv2/cvconfig.h\",\n    #  \"include/x86_64-linux-gnu/opencv4/opencv2/cvconfig.h\",\n    \"include/opencv4/opencv2/**/*.h*\",\n  ]),\n  includes = [\n    # Uncomment according to your multiarch value (gcc -print-multiarch):\n    #  \"include/aarch64-linux-gnu/opencv4/\",\n    #  \"include/arm-linux-gnueabihf/opencv4/\",\n    #  \"include/x86_64-linux-gnu/opencv4/\",\n    \"include/opencv4/\",\n  ],\n  linkopts = [\n    \"-l:libopencv_core.so\",\n    \"-l:libopencv_calib3d.so\",\n    \"-l:libopencv_features2d.so\",\n    \"-l:libopencv_highgui.so\",\n    \"-l:libopencv_imgcodecs.so\",\n    \"-l:libopencv_imgproc.so\",\n    \"-l:libopencv_video.so\",\n    \"-l:libopencv_videoio.so\",\n  ],\n)\n```\n\n----------------------------------------\n\nTITLE: Processing 3D Assets with Obj Cleanup Shell Script\nDESCRIPTION: Shell command for preprocessing 3D assets before they can be rendered by the GlAnimationOverlayCalculator. This first step cleans up the original OBJ files and prepares them for conversion.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/instant_motion_tracking.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./mediapipe/graphs/object_detection_3d/obj_parser/obj_cleanup.sh [INPUT_DIR] [INTERMEDIATE_OUTPUT_DIR]\n```\n\n----------------------------------------\n\nTITLE: Defining Normalized Anchor Coordinates and Dimensions for MediaPipe Model\nDESCRIPTION: A data file specifying anchor points used in a MediaPipe model, likely for object detection or pose estimation. Each row contains four values: the first two represent x,y position coordinates (normalized between 0-1), and the last two represent width/height or similar dimensional attributes. The anchors form a regular grid pattern with systematic variations in size and aspect ratio.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_1.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n0.342105 0.289474 0.282843 0.141421\n0.342105 0.289474 0.141421 0.282843\n0.394737 0.289474 0.1 0.1\n0.394737 0.289474 0.282843 0.141421\n0.394737 0.289474 0.141421 0.282843\n0.447368 0.289474 0.1 0.1\n0.447368 0.289474 0.282843 0.141421\n0.447368 0.289474 0.141421 0.282843\n0.5 0.289474 0.1 0.1\n0.5 0.289474 0.282843 0.141421\n0.5 0.289474 0.141421 0.282843\n0.552632 0.289474 0.1 0.1\n0.552632 0.289474 0.282843 0.141421\n0.552632 0.289474 0.141421 0.282843\n0.605263 0.289474 0.1 0.1\n0.605263 0.289474 0.282843 0.141421\n0.605263 0.289474 0.141421 0.282843\n0.657895 0.289474 0.1 0.1\n0.657895 0.289474 0.282843 0.141421\n0.657895 0.289474 0.141421 0.282843\n0.710526 0.289474 0.1 0.1\n0.710526 0.289474 0.282843 0.141421\n0.710526 0.289474 0.141421 0.282843\n0.763158 0.289474 0.1 0.1\n0.763158 0.289474 0.282843 0.141421\n0.763158 0.289474 0.141421 0.282843\n0.81579 0.289474 0.1 0.1\n0.81579 0.289474 0.282843 0.141421\n0.81579 0.289474 0.141421 0.282843\n0.868421 0.289474 0.1 0.1\n0.868421 0.289474 0.282843 0.141421\n0.868421 0.289474 0.141421 0.282843\n0.921053 0.289474 0.1 0.1\n0.921053 0.289474 0.282843 0.141421\n0.921053 0.289474 0.141421 0.282843\n0.973684 0.289474 0.1 0.1\n0.973684 0.289474 0.282843 0.141421\n0.973684 0.289474 0.141421 0.282843\n0.0263158 0.342105 0.1 0.1\n0.0263158 0.342105 0.282843 0.141421\n0.0263158 0.342105 0.141421 0.282843\n0.0789474 0.342105 0.1 0.1\n0.0789474 0.342105 0.282843 0.141421\n0.0789474 0.342105 0.141421 0.282843\n0.131579 0.342105 0.1 0.1\n0.131579 0.342105 0.282843 0.141421\n0.131579 0.342105 0.141421 0.282843\n0.184211 0.342105 0.1 0.1\n0.184211 0.342105 0.282843 0.141421\n0.184211 0.342105 0.141421 0.282843\n0.236842 0.342105 0.1 0.1\n0.236842 0.342105 0.282843 0.141421\n0.236842 0.342105 0.141421 0.282843\n0.289474 0.342105 0.1 0.1\n0.289474 0.342105 0.282843 0.141421\n0.289474 0.342105 0.141421 0.282843\n0.342105 0.342105 0.1 0.1\n0.342105 0.342105 0.282843 0.141421\n0.342105 0.342105 0.141421 0.282843\n0.394737 0.342105 0.1 0.1\n0.394737 0.342105 0.282843 0.141421\n0.394737 0.342105 0.141421 0.282843\n0.447368 0.342105 0.1 0.1\n0.447368 0.342105 0.282843 0.141421\n0.447368 0.342105 0.141421 0.282843\n0.5 0.342105 0.1 0.1\n0.5 0.342105 0.282843 0.141421\n0.5 0.342105 0.141421 0.282843\n0.552632 0.342105 0.1 0.1\n0.552632 0.342105 0.282843 0.141421\n0.552632 0.342105 0.141421 0.282843\n0.605263 0.342105 0.1 0.1\n0.605263 0.342105 0.282843 0.141421\n0.605263 0.342105 0.141421 0.282843\n0.657895 0.342105 0.1 0.1\n0.657895 0.342105 0.282843 0.141421\n0.657895 0.342105 0.141421 0.282843\n0.710526 0.342105 0.1 0.1\n0.710526 0.342105 0.282843 0.141421\n0.710526 0.342105 0.141421 0.282843\n0.763158 0.342105 0.1 0.1\n0.763158 0.342105 0.282843 0.141421\n0.763158 0.342105 0.141421 0.282843\n0.81579 0.342105 0.1 0.1\n0.81579 0.342105 0.282843 0.141421\n0.81579 0.342105 0.141421 0.282843\n0.868421 0.342105 0.1 0.1\n0.868421 0.342105 0.282843 0.141421\n0.868421 0.342105 0.141421 0.282843\n0.921053 0.342105 0.1 0.1\n0.921053 0.342105 0.282843 0.141421\n0.921053 0.342105 0.141421 0.282843\n0.973684 0.342105 0.1 0.1\n0.973684 0.342105 0.282843 0.141421\n0.973684 0.342105 0.141421 0.282843\n0.0263158 0.394737 0.1 0.1\n0.0263158 0.394737 0.282843 0.141421\n0.0263158 0.394737 0.141421 0.282843\n0.0789474 0.394737 0.1 0.1\n0.0789474 0.394737 0.282843 0.141421\n0.0789474 0.394737 0.141421 0.282843\n0.131579 0.394737 0.1 0.1\n0.131579 0.394737 0.282843 0.141421\n0.131579 0.394737 0.141421 0.282843\n0.184211 0.394737 0.1 0.1\n0.184211 0.394737 0.282843 0.141421\n0.184211 0.394737 0.141421 0.282843\n0.236842 0.394737 0.1 0.1\n0.236842 0.394737 0.282843 0.141421\n0.236842 0.394737 0.141421 0.282843\n0.289474 0.394737 0.1 0.1\n0.289474 0.394737 0.282843 0.141421\n0.289474 0.394737 0.141421 0.282843\n0.342105 0.394737 0.1 0.1\n0.342105 0.394737 0.282843 0.141421\n0.342105 0.394737 0.141421 0.282843\n0.394737 0.394737 0.1 0.1\n0.394737 0.394737 0.282843 0.141421\n0.394737 0.394737 0.141421 0.282843\n0.447368 0.394737 0.1 0.1\n0.447368 0.394737 0.282843 0.141421\n0.447368 0.394737 0.141421 0.282843\n0.5 0.394737 0.1 0.1\n0.5 0.394737 0.282843 0.141421\n0.5 0.394737 0.141421 0.282843\n0.552632 0.394737 0.1 0.1\n0.552632 0.394737 0.282843 0.141421\n0.552632 0.394737 0.141421 0.282843\n0.605263 0.394737 0.1 0.1\n0.605263 0.394737 0.282843 0.141421\n0.605263 0.394737 0.141421 0.282843\n0.657895 0.394737 0.1 0.1\n0.657895 0.394737 0.282843 0.141421\n0.657895 0.394737 0.141421 0.282843\n0.710526 0.394737 0.1 0.1\n0.710526 0.394737 0.282843 0.141421\n0.710526 0.394737 0.141421 0.282843\n0.763158 0.394737 0.1 0.1\n0.763158 0.394737 0.282843 0.141421\n0.763158 0.394737 0.141421 0.282843\n0.81579 0.394737 0.1 0.1\n0.81579 0.394737 0.282843 0.141421\n0.81579 0.394737 0.141421 0.282843\n0.868421 0.394737 0.1 0.1\n0.868421 0.394737 0.282843 0.141421\n0.868421 0.394737 0.141421 0.282843\n0.921053 0.394737 0.1 0.1\n0.921053 0.394737 0.282843 0.141421\n0.921053 0.394737 0.141421 0.282843\n0.973684 0.394737 0.1 0.1\n0.973684 0.394737 0.282843 0.141421\n0.973684 0.394737 0.141421 0.282843\n0.0263158 0.447368 0.1 0.1\n0.0263158 0.447368 0.282843 0.141421\n0.0263158 0.447368 0.141421 0.282843\n0.0789474 0.447368 0.1 0.1\n0.0789474 0.447368 0.282843 0.141421\n0.0789474 0.447368 0.141421 0.282843\n0.131579 0.447368 0.1 0.1\n0.131579 0.447368 0.282843 0.141421\n0.131579 0.447368 0.141421 0.282843\n0.184211 0.447368 0.1 0.1\n0.184211 0.447368 0.282843 0.141421\n0.184211 0.447368 0.141421 0.282843\n0.236842 0.447368 0.1 0.1\n0.236842 0.447368 0.282843 0.141421\n0.236842 0.447368 0.141421 0.282843\n0.289474 0.447368 0.1 0.1\n0.289474 0.447368 0.282843 0.141421\n0.289474 0.447368 0.141421 0.282843\n0.342105 0.447368 0.1 0.1\n0.342105 0.447368 0.282843 0.141421\n0.342105 0.447368 0.141421 0.282843\n0.394737 0.447368 0.1 0.1\n0.394737 0.447368 0.282843 0.141421\n0.394737 0.447368 0.141421 0.282843\n0.447368 0.447368 0.1 0.1\n0.447368 0.447368 0.282843 0.141421\n0.447368 0.447368 0.141421 0.282843\n0.5 0.447368 0.1 0.1\n0.5 0.447368 0.282843 0.141421\n0.5 0.447368 0.141421 0.282843\n0.552632 0.447368 0.1 0.1\n0.552632 0.447368 0.282843 0.141421\n0.552632 0.447368 0.141421 0.282843\n0.605263 0.447368 0.1 0.1\n0.605263 0.447368 0.282843 0.141421\n0.605263 0.447368 0.141421 0.282843\n0.657895 0.447368 0.1 0.1\n0.657895 0.447368 0.282843 0.141421\n0.657895 0.447368 0.141421 0.282843\n0.710526 0.447368 0.1 0.1\n0.710526 0.447368 0.282843 0.141421\n0.710526 0.447368 0.141421 0.282843\n0.763158 0.447368 0.1 0.1\n0.763158 0.447368 0.282843 0.141421\n0.763158 0.447368 0.141421 0.282843\n0.81579 0.447368 0.1 0.1\n0.81579 0.447368 0.282843 0.141421\n0.81579 0.447368 0.141421 0.282843\n0.868421 0.447368 0.1 0.1\n0.868421 0.447368 0.282843 0.141421\n0.868421 0.447368 0.141421 0.282843\n0.921053 0.447368 0.1 0.1\n0.921053 0.447368 0.282843 0.141421\n0.921053 0.447368 0.141421 0.282843\n0.973684 0.447368 0.1 0.1\n0.973684 0.447368 0.282843 0.141421\n0.973684 0.447368 0.141421 0.282843\n0.0263158 0.5 0.1 0.1\n0.0263158 0.5 0.282843 0.141421\n0.0263158 0.5 0.141421 0.282843\n0.0789474 0.5 0.1 0.1\n0.0789474 0.5 0.282843 0.141421\n0.0789474 0.5 0.141421 0.282843\n0.131579 0.5 0.1 0.1\n0.131579 0.5 0.282843 0.141421\n0.131579 0.5 0.141421 0.282843\n0.184211 0.5 0.1 0.1\n0.184211 0.5 0.282843 0.141421\n0.184211 0.5 0.141421 0.282843\n0.236842 0.5 0.1 0.1\n0.236842 0.5 0.282843 0.141421\n0.236842 0.5 0.141421 0.282843\n0.289474 0.5 0.1 0.1\n0.289474 0.5 0.282843 0.141421\n0.289474 0.5 0.141421 0.282843\n0.342105 0.5 0.1 0.1\n0.342105 0.5 0.282843 0.141421\n0.342105 0.5 0.141421 0.282843\n0.394737 0.5 0.1 0.1\n0.394737 0.5 0.282843 0.141421\n0.394737 0.5 0.141421 0.282843\n0.447368 0.5 0.1 0.1\n0.447368 0.5 0.282843 0.141421\n0.447368 0.5 0.141421 0.282843\n0.5 0.5 0.1 0.1\n0.5 0.5 0.282843 0.141421\n0.5 0.5 0.141421 0.282843\n0.552632 0.5 0.1 0.1\n0.552632 0.5 0.282843 0.141421\n0.552632 0.5 0.141421 0.282843\n0.605263 0.5 0.1 0.1\n0.605263 0.5 0.282843 0.141421\n0.605263 0.5 0.141421 0.282843\n0.657895 0.5 0.1 0.1\n0.657895 0.5 0.282843 0.141421\n0.657895 0.5 0.141421 0.282843\n0.710526 0.5 0.1 0.1\n0.710526 0.5 0.282843 0.141421\n0.710526 0.5 0.141421 0.282843\n0.763158 0.5 0.1 0.1\n0.763158 0.5 0.282843 0.141421\n0.763158 0.5 0.141421 0.282843\n0.81579 0.5 0.1 0.1\n0.81579 0.5 0.282843 0.141421\n0.81579 0.5 0.141421 0.282843\n0.868421 0.5 0.1 0.1\n0.868421 0.5 0.282843 0.141421\n0.868421 0.5 0.141421 0.282843\n0.921053 0.5 0.1 0.1\n0.921053 0.5 0.282843 0.141421\n0.921053 0.5 0.141421 0.282843\n0.973684 0.5 0.1 0.1\n0.973684 0.5 0.282843 0.141421\n0.973684 0.5 0.141421 0.282843\n0.0263158 0.552632 0.1 0.1\n0.0263158 0.552632 0.282843 0.141421\n0.0263158 0.552632 0.141421 0.282843\n0.0789474 0.552632 0.1 0.1\n0.0789474 0.552632 0.282843 0.141421\n0.0789474 0.552632 0.141421 0.282843\n0.131579 0.552632 0.1 0.1\n0.131579 0.552632 0.282843 0.141421\n0.131579 0.552632 0.141421 0.282843\n0.184211 0.552632 0.1 0.1\n0.184211 0.552632 0.282843 0.141421\n0.184211 0.552632 0.141421 0.282843\n0.236842 0.552632 0.1 0.1\n0.236842 0.552632 0.282843 0.141421\n0.236842 0.552632 0.141421 0.282843\n0.289474 0.552632 0.1 0.1\n0.289474 0.552632 0.282843 0.141421\n0.289474 0.552632 0.141421 0.282843\n0.342105 0.552632 0.1 0.1\n0.342105 0.552632 0.282843 0.141421\n0.342105 0.552632 0.141421 0.282843\n0.394737 0.552632 0.1 0.1\n0.394737 0.552632 0.282843 0.141421\n0.394737 0.552632 0.141421 0.282843\n0.447368 0.552632 0.1 0.1\n0.447368 0.552632 0.282843 0.141421\n0.447368 0.552632 0.141421 0.282843\n0.5 0.552632 0.1 0.1\n0.5 0.552632 0.282843 0.141421\n0.5 0.552632 0.141421 0.282843\n0.552632 0.552632 0.1 0.1\n0.552632 0.552632 0.282843 0.141421\n0.552632 0.552632 0.141421 0.282843\n0.605263 0.552632 0.1 0.1\n0.605263 0.552632 0.282843 0.141421\n0.605263 0.552632 0.141421 0.282843\n0.657895 0.552632 0.1 0.1\n0.657895 0.552632 0.282843 0.141421\n0.657895 0.552632 0.141421 0.282843\n0.710526 0.552632 0.1 0.1\n0.710526 0.552632 0.282843 0.141421\n0.710526 0.552632 0.141421 0.282843\n0.763158 0.552632 0.1 0.1\n0.763158 0.552632 0.282843 0.141421\n0.763158 0.552632 0.141421 0.282843\n0.81579 0.552632 0.1 0.1\n0.81579 0.552632 0.282843 0.141421\n0.81579 0.552632 0.141421 0.282843\n0.868421 0.552632 0.1 0.1\n0.868421 0.552632 0.282843 0.141421\n0.868421 0.552632 0.141421 0.282843\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Outputs in C++ (Poor Practice)\nDESCRIPTION: This snippet shows a poor practice of defining graph outputs scattered throughout the code, making it difficult to understand the graph structure and maintain the code.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nvoid RunSomething(Stream<Input> input, Graph& graph) {\n  // ...\n  node.Out(\"OUTPUT_F\")\n      .SetName(\"output_f\").ConnectTo(graph.Out(2));  // Bad.\n}\n\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // 10/100/N lines of code.\n  node.Out(\"OUTPUT_D\")\n      .SetName(\"output_d\").ConnectTo(graph.Out(0));  // Bad.\n  // 10/100/N lines of code.\n  node.Out(\"OUTPUT_E\")\n      .SetName(\"output_e\").ConnectTo(graph.Out(1));  // Bad.\n  // 10/100/N lines of code.\n  RunSomething(input, graph);\n  // ...\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Multiple Port Declaration with Node API\nDESCRIPTION: Simplified declaration of multiple input ports using the new Node API.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/api2/README.md#2025-04-23_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nstatic constexpr Input<AnyType>::Multiple kIn{\"\"};\n```\n\n----------------------------------------\n\nTITLE: Defining MediaPipe Python Package Dependencies\nDESCRIPTION: Specifies the required Python packages and their version constraints needed to run MediaPipe applications. Includes core ML frameworks like TensorFlow, computer vision libraries like OpenCV, and MediaPipe specific dependencies.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/model_maker/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nabsl-py\nmediapipe>=0.10.0\nnumpy<2\nopencv-python\ntensorflow>=2.10,<2.16\ntensorflow-addons\ntensorflow-datasets\ntensorflow-hub\ntensorflow-model-optimization<0.8.0\ntensorflow-text\ntf-models-official>=2.13.2,<2.16.0\n```\n\n----------------------------------------\n\nTITLE: Adding String Resource for Camera Access Message\nDESCRIPTION: This XML snippet adds a string resource for the message displayed when camera access is not granted.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_11\n\nLANGUAGE: XML\nCODE:\n```\n<string name=\"no_camera_access\" translatable=\"false\">Please grant camera permissions.</string>\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Path in Bazel Build\nDESCRIPTION: Example command showing how to specify Python binary path when building MediaPipe with Bazel to resolve missing Python binary issues.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/troubleshooting.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt \\\n  --define MEDIAPIPE_DISABLE_GPU=1 \\\n  --action_env PYTHON_BIN_PATH=$(which python3) \\\n  mediapipe/examples/desktop/hello_world\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Trace Log Path on Android (Protobuf)\nDESCRIPTION: This snippet demonstrates how to set a custom trace log path for Android devices in the MediaPipe graph configuration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/tracing_and_profiling.md#2025-04-23_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\nprofiler_config {\n  trace_enabled: true\n  enable_profiler: true\n  trace_log_path: \"/sdcard/Download/profiles/\"\n}\n```\n\n----------------------------------------\n\nTITLE: Normalized Coordinate Grid with Uniform Weights\nDESCRIPTION: A dataset representing a regular grid of 2D points in normalized coordinate space (0-1 range). Each point is defined by four values: x-coordinate, y-coordinate, followed by two weight values that are consistently set to 1. The grid points are arranged in a systematic pattern with equal spacing.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_0.txt#2025-04-23_snippet_0\n\nLANGUAGE: data\nCODE:\n```\n0.015625 0.015625 1 1\n0.015625 0.015625 1 1\n0.046875 0.015625 1 1\n0.046875 0.015625 1 1\n0.078125 0.015625 1 1\n0.078125 0.015625 1 1\n0.109375 0.015625 1 1\n0.109375 0.015625 1 1\n0.140625 0.015625 1 1\n0.140625 0.015625 1 1\n0.171875 0.015625 1 1\n0.171875 0.015625 1 1\n0.203125 0.015625 1 1\n0.203125 0.015625 1 1\n0.234375 0.015625 1 1\n0.234375 0.015625 1 1\n0.265625 0.015625 1 1\n0.265625 0.015625 1 1\n0.296875 0.015625 1 1\n0.296875 0.015625 1 1\n0.328125 0.015625 1 1\n0.328125 0.015625 1 1\n0.359375 0.015625 1 1\n0.359375 0.015625 1 1\n0.390625 0.015625 1 1\n0.390625 0.015625 1 1\n0.421875 0.015625 1 1\n0.421875 0.015625 1 1\n0.453125 0.015625 1 1\n0.453125 0.015625 1 1\n0.484375 0.015625 1 1\n0.484375 0.015625 1 1\n0.515625 0.015625 1 1\n0.515625 0.015625 1 1\n0.546875 0.015625 1 1\n0.546875 0.015625 1 1\n0.578125 0.015625 1 1\n0.578125 0.015625 1 1\n0.609375 0.015625 1 1\n0.609375 0.015625 1 1\n0.640625 0.015625 1 1\n0.640625 0.015625 1 1\n0.671875 0.015625 1 1\n0.671875 0.015625 1 1\n0.703125 0.015625 1 1\n0.703125 0.015625 1 1\n0.734375 0.015625 1 1\n0.734375 0.015625 1 1\n0.765625 0.015625 1 1\n0.765625 0.015625 1 1\n0.796875 0.015625 1 1\n0.796875 0.015625 1 1\n0.828125 0.015625 1 1\n0.828125 0.015625 1 1\n0.859375 0.015625 1 1\n0.859375 0.015625 1 1\n0.890625 0.015625 1 1\n0.890625 0.015625 1 1\n0.921875 0.015625 1 1\n0.921875 0.015625 1 1\n0.953125 0.015625 1 1\n0.953125 0.015625 1 1\n0.984375 0.015625 1 1\n0.984375 0.015625 1 1\n0.015625 0.046875 1 1\n0.015625 0.046875 1 1\n0.046875 0.046875 1 1\n0.046875 0.046875 1 1\n0.078125 0.046875 1 1\n0.078125 0.046875 1 1\n0.109375 0.046875 1 1\n0.109375 0.046875 1 1\n0.140625 0.046875 1 1\n0.140625 0.046875 1 1\n0.171875 0.046875 1 1\n0.171875 0.046875 1 1\n0.203125 0.046875 1 1\n0.203125 0.046875 1 1\n0.234375 0.046875 1 1\n0.234375 0.046875 1 1\n0.265625 0.046875 1 1\n0.265625 0.046875 1 1\n0.296875 0.046875 1 1\n0.296875 0.046875 1 1\n0.328125 0.046875 1 1\n0.328125 0.046875 1 1\n0.359375 0.046875 1 1\n0.359375 0.046875 1 1\n0.390625 0.046875 1 1\n0.390625 0.046875 1 1\n0.421875 0.046875 1 1\n0.421875 0.046875 1 1\n0.453125 0.046875 1 1\n0.453125 0.046875 1 1\n0.484375 0.046875 1 1\n0.484375 0.046875 1 1\n0.515625 0.046875 1 1\n0.515625 0.046875 1 1\n0.546875 0.046875 1 1\n0.546875 0.046875 1 1\n0.578125 0.046875 1 1\n0.578125 0.046875 1 1\n0.609375 0.046875 1 1\n0.609375 0.046875 1 1\n0.640625 0.046875 1 1\n0.640625 0.046875 1 1\n0.671875 0.046875 1 1\n0.671875 0.046875 1 1\n0.703125 0.046875 1 1\n0.703125 0.046875 1 1\n0.734375 0.046875 1 1\n0.734375 0.046875 1 1\n0.765625 0.046875 1 1\n0.765625 0.046875 1 1\n0.796875 0.046875 1 1\n0.796875 0.046875 1 1\n0.828125 0.046875 1 1\n0.828125 0.046875 1 1\n0.859375 0.046875 1 1\n0.859375 0.046875 1 1\n0.890625 0.046875 1 1\n0.890625 0.046875 1 1\n0.921875 0.046875 1 1\n0.921875 0.046875 1 1\n0.953125 0.046875 1 1\n0.953125 0.046875 1 1\n0.984375 0.046875 1 1\n0.984375 0.046875 1 1\n0.015625 0.078125 1 1\n0.015625 0.078125 1 1\n0.046875 0.078125 1 1\n0.046875 0.078125 1 1\n0.078125 0.078125 1 1\n0.078125 0.078125 1 1\n0.109375 0.078125 1 1\n0.109375 0.078125 1 1\n0.140625 0.078125 1 1\n0.140625 0.078125 1 1\n0.171875 0.078125 1 1\n0.171875 0.078125 1 1\n0.203125 0.078125 1 1\n0.203125 0.078125 1 1\n0.234375 0.078125 1 1\n0.234375 0.078125 1 1\n0.265625 0.078125 1 1\n0.265625 0.078125 1 1\n0.296875 0.078125 1 1\n0.296875 0.078125 1 1\n0.328125 0.078125 1 1\n0.328125 0.078125 1 1\n0.359375 0.078125 1 1\n0.359375 0.078125 1 1\n0.390625 0.078125 1 1\n0.390625 0.078125 1 1\n0.421875 0.078125 1 1\n0.421875 0.078125 1 1\n0.453125 0.078125 1 1\n0.453125 0.078125 1 1\n0.484375 0.078125 1 1\n0.484375 0.078125 1 1\n0.515625 0.078125 1 1\n0.515625 0.078125 1 1\n0.546875 0.078125 1 1\n0.546875 0.078125 1 1\n0.578125 0.078125 1 1\n0.578125 0.078125 1 1\n0.609375 0.078125 1 1\n0.609375 0.078125 1 1\n0.640625 0.078125 1 1\n0.640625 0.078125 1 1\n0.671875 0.078125 1 1\n0.671875 0.078125 1 1\n0.703125 0.078125 1 1\n0.703125 0.078125 1 1\n0.734375 0.078125 1 1\n0.734375 0.078125 1 1\n0.765625 0.078125 1 1\n0.765625 0.078125 1 1\n0.796875 0.078125 1 1\n0.796875 0.078125 1 1\n0.828125 0.078125 1 1\n0.828125 0.078125 1 1\n0.859375 0.078125 1 1\n0.859375 0.078125 1 1\n0.890625 0.078125 1 1\n0.890625 0.078125 1 1\n0.921875 0.078125 1 1\n0.921875 0.078125 1 1\n0.953125 0.078125 1 1\n0.953125 0.078125 1 1\n0.984375 0.078125 1 1\n0.984375 0.078125 1 1\n0.015625 0.109375 1 1\n0.015625 0.109375 1 1\n0.046875 0.109375 1 1\n0.046875 0.109375 1 1\n0.078125 0.109375 1 1\n0.078125 0.109375 1 1\n0.109375 0.109375 1 1\n0.109375 0.109375 1 1\n0.140625 0.109375 1 1\n0.140625 0.109375 1 1\n0.171875 0.109375 1 1\n0.171875 0.109375 1 1\n0.203125 0.109375 1 1\n0.203125 0.109375 1 1\n0.234375 0.109375 1 1\n0.234375 0.109375 1 1\n0.265625 0.109375 1 1\n0.265625 0.109375 1 1\n0.296875 0.109375 1 1\n0.296875 0.109375 1 1\n0.328125 0.109375 1 1\n0.328125 0.109375 1 1\n0.359375 0.109375 1 1\n0.359375 0.109375 1 1\n0.390625 0.109375 1 1\n0.390625 0.109375 1 1\n0.421875 0.109375 1 1\n0.421875 0.109375 1 1\n0.453125 0.109375 1 1\n0.453125 0.109375 1 1\n0.484375 0.109375 1 1\n0.484375 0.109375 1 1\n0.515625 0.109375 1 1\n0.515625 0.109375 1 1\n0.546875 0.109375 1 1\n0.546875 0.109375 1 1\n0.578125 0.109375 1 1\n0.578125 0.109375 1 1\n0.609375 0.109375 1 1\n0.609375 0.109375 1 1\n0.640625 0.109375 1 1\n0.640625 0.109375 1 1\n0.671875 0.109375 1 1\n0.671875 0.109375 1 1\n0.703125 0.109375 1 1\n0.703125 0.109375 1 1\n0.734375 0.109375 1 1\n0.734375 0.109375 1 1\n0.765625 0.109375 1 1\n0.765625 0.109375 1 1\n0.796875 0.109375 1 1\n0.796875 0.109375 1 1\n0.828125 0.109375 1 1\n0.828125 0.109375 1 1\n0.859375 0.109375 1 1\n0.859375 0.109375 1 1\n0.890625 0.109375 1 1\n0.890625 0.109375 1 1\n0.921875 0.109375 1 1\n0.921875 0.109375 1 1\n0.953125 0.109375 1 1\n0.953125 0.109375 1 1\n0.984375 0.109375 1 1\n0.984375 0.109375 1 1\n0.015625 0.140625 1 1\n0.015625 0.140625 1 1\n0.046875 0.140625 1 1\n0.046875 0.140625 1 1\n0.078125 0.140625 1 1\n0.078125 0.140625 1 1\n0.109375 0.140625 1 1\n0.109375 0.140625 1 1\n0.140625 0.140625 1 1\n0.140625 0.140625 1 1\n0.171875 0.140625 1 1\n0.171875 0.140625 1 1\n0.203125 0.140625 1 1\n0.203125 0.140625 1 1\n0.234375 0.140625 1 1\n0.234375 0.140625 1 1\n0.265625 0.140625 1 1\n0.265625 0.140625 1 1\n0.296875 0.140625 1 1\n0.296875 0.140625 1 1\n0.328125 0.140625 1 1\n0.328125 0.140625 1 1\n0.359375 0.140625 1 1\n0.359375 0.140625 1 1\n0.390625 0.140625 1 1\n0.390625 0.140625 1 1\n0.421875 0.140625 1 1\n0.421875 0.140625 1 1\n0.453125 0.140625 1 1\n0.453125 0.140625 1 1\n0.484375 0.140625 1 1\n0.484375 0.140625 1 1\n0.515625 0.140625 1 1\n0.515625 0.140625 1 1\n0.546875 0.140625 1 1\n0.546875 0.140625 1 1\n0.578125 0.140625 1 1\n0.578125 0.140625 1 1\n0.609375 0.140625 1 1\n0.609375 0.140625 1 1\n0.640625 0.140625 1 1\n0.640625 0.140625 1 1\n0.671875 0.140625 1 1\n0.671875 0.140625 1 1\n0.703125 0.140625 1 1\n0.703125 0.140625 1 1\n0.734375 0.140625 1 1\n0.734375 0.140625 1 1\n0.765625 0.140625 1 1\n0.765625 0.140625 1 1\n0.796875 0.140625 1 1\n0.796875 0.140625 1 1\n0.828125 0.140625 1 1\n0.828125 0.140625 1 1\n0.859375 0.140625 1 1\n0.859375 0.140625 1 1\n0.890625 0.140625 1 1\n0.890625 0.140625 1 1\n0.921875 0.140625 1 1\n0.921875 0.140625 1 1\n0.953125 0.140625 1 1\n0.953125 0.140625 1 1\n0.984375 0.140625 1 1\n0.984375 0.140625 1 1\n0.015625 0.171875 1 1\n0.015625 0.171875 1 1\n0.046875 0.171875 1 1\n0.046875 0.171875 1 1\n0.078125 0.171875 1 1\n0.078125 0.171875 1 1\n0.109375 0.171875 1 1\n0.109375 0.171875 1 1\n0.140625 0.171875 1 1\n0.140625 0.171875 1 1\n0.171875 0.171875 1 1\n0.171875 0.171875 1 1\n0.203125 0.171875 1 1\n0.203125 0.171875 1 1\n0.234375 0.171875 1 1\n0.234375 0.171875 1 1\n0.265625 0.171875 1 1\n0.265625 0.171875 1 1\n0.296875 0.171875 1 1\n0.296875 0.171875 1 1\n0.328125 0.171875 1 1\n0.328125 0.171875 1 1\n0.359375 0.171875 1 1\n0.359375 0.171875 1 1\n0.390625 0.171875 1 1\n0.390625 0.171875 1 1\n0.421875 0.171875 1 1\n0.421875 0.171875 1 1\n0.453125 0.171875 1 1\n0.453125 0.171875 1 1\n0.484375 0.171875 1 1\n0.484375 0.171875 1 1\n0.515625 0.171875 1 1\n0.515625 0.171875 1 1\n0.546875 0.171875 1 1\n0.546875 0.171875 1 1\n0.578125 0.171875 1 1\n0.578125 0.171875 1 1\n0.609375 0.171875 1 1\n0.609375 0.171875 1 1\n0.640625 0.171875 1 1\n0.640625 0.171875 1 1\n0.671875 0.171875 1 1\n0.671875 0.171875 1 1\n0.703125 0.171875 1 1\n0.703125 0.171875 1 1\n0.734375 0.171875 1 1\n0.734375 0.171875 1 1\n0.765625 0.171875 1 1\n0.765625 0.171875 1 1\n0.796875 0.171875 1 1\n0.796875 0.171875 1 1\n0.828125 0.171875 1 1\n0.828125 0.171875 1 1\n0.859375 0.171875 1 1\n0.859375 0.171875 1 1\n0.890625 0.171875 1 1\n0.890625 0.171875 1 1\n0.921875 0.171875 1 1\n0.921875 0.171875 1 1\n0.953125 0.171875 1 1\n0.953125 0.171875 1 1\n0.984375 0.171875 1 1\n0.984375 0.171875 1 1\n0.015625 0.203125 1 1\n0.015625 0.203125 1 1\n0.046875 0.203125 1 1\n0.046875 0.203125 1 1\n0.078125 0.203125 1 1\n0.078125 0.203125 1 1\n0.109375 0.203125 1 1\n0.109375 0.203125 1 1\n0.140625 0.203125 1 1\n0.140625 0.203125 1 1\n0.171875 0.203125 1 1\n0.171875 0.203125 1 1\n0.203125 0.203125 1 1\n0.203125 0.203125 1 1\n0.234375 0.203125 1 1\n0.234375 0.203125 1 1\n0.265625 0.203125 1 1\n0.265625 0.203125 1 1\n0.296875 0.203125 1 1\n0.296875 0.203125 1 1\n0.328125 0.203125 1 1\n0.328125 0.203125 1 1\n0.359375 0.203125 1 1\n0.359375 0.203125 1 1\n0.390625 0.203125 1 1\n0.390625 0.203125 1 1\n0.421875 0.203125 1 1\n0.421875 0.203125 1 1\n0.453125 0.203125 1 1\n0.453125 0.203125 1 1\n0.484375 0.203125 1 1\n0.484375 0.203125 1 1\n0.515625 0.203125 1 1\n0.515625 0.203125 1 1\n0.546875 0.203125 1 1\n0.546875 0.203125 1 1\n0.578125 0.203125 1 1\n0.578125 0.203125 1 1\n0.609375 0.203125 1 1\n0.609375 0.203125 1 1\n0.640625 0.203125 1 1\n0.640625 0.203125 1 1\n0.671875 0.203125 1 1\n0.671875 0.203125 1 1\n0.703125 0.203125 1 1\n0.703125 0.203125 1 1\n0.734375 0.203125 1 1\n0.734375 0.203125 1 1\n0.765625 0.203125 1 1\n0.765625 0.203125 1 1\n0.796875 0.203125 1 1\n0.796875 0.203125 1 1\n0.828125 0.203125 1 1\n0.828125 0.203125 1 1\n0.859375 0.203125 1 1\n0.859375 0.203125 1 1\n0.890625 0.203125 1 1\n0.890625 0.203125 1 1\n0.921875 0.203125 1 1\n0.921875 0.203125 1 1\n0.953125 0.203125 1 1\n0.953125 0.203125 1 1\n0.984375 0.203125 1 1\n0.984375 0.203125 1 1\n0.015625 0.234375 1 1\n0.015625 0.234375 1 1\n0.046875 0.234375 1 1\n0.046875 0.234375 1 1\n0.078125 0.234375 1 1\n0.078125 0.234375 1 1\n```\n\n----------------------------------------\n\nTITLE: Special Token Mappings\nDESCRIPTION: Core special tokens used for text processing with their numerical indices. These tokens handle padding, sequence starts, unknown words, and unused indices.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/testdata/text/vocab_for_regex_tokenizer.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<PAD> 0\n<START> 1\n<UNKNOWN> 2\n<UNUSED> 3\n```\n\n----------------------------------------\n\nTITLE: Image Keys in MediaPipe\nDESCRIPTION: Keys for managing image data, including encoded image content, timestamps, dimensions, format and metadata. Supports single and multi-image storage with associated labels and confidence values.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_14\n\nLANGUAGE: text\nCODE:\n```\nimage/encoded - feature list bytes\nimage/timestamp - feature list int\nimage/multi_encoded - feature list bytes list\nimage/format - context bytes\nimage/channels - context int\nimage/colorspace - context bytes\nimage/height - context int\nimage/width - context int\nimage/frame_rate - context float\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Inputs in C++ (Poor Practice)\nDESCRIPTION: This snippet demonstrates a poor practice of defining graph inputs scattered throughout the code, making it difficult to understand the graph structure and maintain the code.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nStream<D> RunSomething(Stream<A> a, Stream<B> b, Graph& graph) {\n  Stream<C> c = graph.In(2).SetName(\"c\").Cast<C>();  // Bad.\n  // ...\n}\n\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  Stream<A> a = graph.In(0).SetName(\"a\").Cast<A>();\n  // 10/100/N lines of code.\n  Stream<B> b = graph.In(1).SetName(\"b\").Cast<B>()  // Bad.\n  Stream<D> d = RunSomething(a, b, graph);\n  // ...\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Importing MediaPipe Graph in Objective-C\nDESCRIPTION: Imports the MediaPipe graph header file in the ViewController.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_20\n\nLANGUAGE: Objective-C\nCODE:\n```\n#import \"mediapipe/objc/MPPGraph.h\"\n```\n\n----------------------------------------\n\nTITLE: Importing MPPLayerRenderer for Frame Display\nDESCRIPTION: Adds import for MediaPipe's layer renderer utility to display camera frames on screen.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_ios.md#2025-04-23_snippet_11\n\nLANGUAGE: Objective-C\nCODE:\n```\n#import \"mediapipe/objc/MPPLayerRenderer.h\"\n```\n\n----------------------------------------\n\nTITLE: Setting up CUDA Environment Variables\nDESCRIPTION: Shell commands to configure PATH and LD_LIBRARY_PATH for CUDA support in TensorFlow with MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/gpu_support.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64,/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\nsudo ldconfig\n```\n\n----------------------------------------\n\nTITLE: Setting Text Context Embedding in MediaPipe (Python/C++)\nDESCRIPTION: Stores text embeddings in the MediaPipe context. This function handles large blocks of text represented as embedding vectors.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nset_text_context_embedding\n```\n\nLANGUAGE: c++\nCODE:\n```\nSetTextContextEmbedding\n```\n\n----------------------------------------\n\nTITLE: Parsing 2D Coordinate Data for MediaPipe\nDESCRIPTION: This data represents a series of 2D coordinates with associated values. Each line contains four numbers: x-coordinate, y-coordinate, and two constant values (1 1). The coordinates appear to be normalized between 0 and 1, incrementing in steps of 0.03125 (1/32) for both x and y values.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_0.txt#2025-04-23_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n0.109375 0.234375 1 1\n0.109375 0.234375 1 1\n0.140625 0.234375 1 1\n0.140625 0.234375 1 1\n0.171875 0.234375 1 1\n0.171875 0.234375 1 1\n0.203125 0.234375 1 1\n0.203125 0.234375 1 1\n0.234375 0.234375 1 1\n0.234375 0.234375 1 1\n```\n\n----------------------------------------\n\nTITLE: Vocabulary Mapping for Text Processing in MediaPipe\nDESCRIPTION: This file maps common English words to numeric IDs for text processing. It begins with special tokens (<PAD>, <START>, <UNKNOWN>, <UNUSED>) followed by common English words arranged by frequency, creating a vocabulary for NLP tasks.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/testdata/metadata/regex_vocab.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<PAD> 0\n<START> 1\n<UNKNOWN> 2\n<UNUSED> 3\nthe 4\nand 5\na 6\nof 7\nto 8\nis 9\nbr 10\nin 11\nit 12\ni 13\nthis 14\nthat 15\nwas 16\nas 17\nfor 18\nwith 19\nmovie 20\nbut 21\nfilm 22\non 23\nnot 24\nyou 25\nare 26\nhis 27\nhave 28\nhe 29\nbe 30\none 31\nall 32\nat 33\nby 34\nan 35\nthey 36\nwho 37\nso 38\nfrom 39\nlike 40\nher 41\nor 42\njust 43\nabout 44\nit's 45\nout 46\nhas 47\nif 48\nsome 49\nthere 50\nwhat 51\ngood 52\nmore 53\nwhen 54\nvery 55\nup 56\nno 57\ntime 58\nshe 59\neven 60\nmy 61\nwould 62\nwhich 63\nonly 64\nstory 65\nreally 66\nsee 67\ntheir 68\nhad 69\ncan 70\nwere 71\nme 72\nwell 73\nthan 74\nwe 75\nmuch 76\nbeen 77\nbad 78\nget 79\nwill 80\ndo 81\nalso 82\ninto 83\npeople 84\nother 85\nfirst 86\ngreat 87\nbecause 88\nhow 89\nhim 90\nmost 91\ndon't 92\nmade 93\nits 94\nthen 95\nway 96\nmake 97\nthem 98\ntoo 99\ncould 100\nany 101\nmovies 102\nafter 103\nthink 104\ncharacters 105\nwatch 106\ntwo 107\nfilms 108\ncharacter 109\nseen 110\nmany 111\nbeing 112\nlife 113\nplot 114\nnever 115\nacting 116\nlittle 117\nbest 118\nlove 119\nover 120\nwhere 121\ndid 122\nshow 123\nknow 124\noff 125\never 126\ndoes 127\nbetter 128\nyour 129\nend 130\nstill 131\nman 132\nhere 133\nthese 134\nsay 135\nscene 136\nwhile 137\nwhy 138\nscenes 139\ngo 140\nsuch 141\nsomething 142\nthrough 143\nshould 144\nback 145\ni'm 146\nreal 147\nthose 148\nwatching 149\nnow 150\nthough 151\ndoesn't 152\nyears 153\nold 154\nthing 155\nactors 156\nwork 157\n10 158\nbefore 159\nanother 160\ndidn't 161\nnew 162\nfunny 163\nnothing 164\nactually 165\nmakes 166\ndirector 167\nlook 168\nfind 169\ngoing 170\nfew 171\nsame 172\npart 173\nagain 174\nevery 175\nlot 176\ncast 177\nus 178\nquite 179\ndown 180\nwant 181\nworld 182\nthings 183\npretty 184\nyoung 185\nseems 186\naround 187\ngot 188\nhorror 189\nhowever 190\ncan't 191\nfact 192\ntake 193\nbig 194\nenough 195\nlong 196\nthought 197\nthat's 198\nboth 199\nbetween 200\nseries 201\ngive 202\nmay 203\noriginal 204\nown 205\naction 206\ni've 207\nright 208\nwithout 209\nalways 210\ntimes 211\ncomedy 212\npoint 213\ngets 214\nmust 215\ncome 216\nrole 217\nisn't 218\nsaw 219\nalmost 220\ninteresting 221\nleast 222\nfamily 223\ndone 224\nthere's 225\nwhole 226\nbit 227\nmusic 228\nscript 229\nfar 230\nmaking 231\nguy 232\nanything 233\nminutes 234\nfeel 235\nlast 236\nsince 237\nmight 238\nperformance 239\nhe's 240\n2 241\nprobably 242\nkind 243\nam 244\naway 245\nyet 246\nrather 247\ntv 248\nworst 249\ngirl 250\nday 251\nsure 252\nfun 253\nhard 254\nwoman 255\nplayed 256\neach 257\nfound 258\nanyone 259\nhaving 260\nalthough 261\nespecially 262\nour 263\nbelieve 264\ncourse 265\ncomes 266\nlooking 267\nscreen 268\ntrying 269\nset 270\ngoes 271\nlooks 272\nplace 273\nbook 274\ndifferent 275\nput 276\nending 277\nmoney 278\nmaybe 279\nonce 280\nsense 281\nreason 282\ntrue 283\nactor 284\neverything 285\nwasn't 286\nshows 287\ndvd 288\nthree 289\nworth 290\nyear 291\njob 292\nmain 293\nsomeone 294\ntogether 295\nwatched 296\nplay 297\namerican 298\nplays 299\n1 300\nsaid 301\neffects 302\nlater 303\ntakes 304\ninstead 305\nseem 306\nbeautiful 307\njohn 308\nhimself 309\nversion 310\naudience 311\nhigh 312\nhouse 313\nnight 314\nduring 315\neveryone 316\nleft 317\nspecial 318\nseeing 319\nhalf 320\nexcellent 321\nwife 322\nstar 323\nshot 324\nwar 325\nidea 326\nnice 327\nblack 328\nless 329\nmind 330\nsimply 331\nread 332\nsecond 333\nelse 334\nyou're 335\nfather 336\nfan 337\npoor 338\nhelp 339\ncompletely 340\ndeath 341\n3 342\nused 343\nhome 344\neither 345\nshort 346\nline 347\ngiven 348\nmen 349\ntop 350\ndead 351\nbudget 352\ntry 353\nperformances 354\nwrong 355\nclassic 356\nboring 357\nenjoy 358\nneed 359\nrest 360\nuse 361\nkids 362\nhollywood 363\nlow 364\nproduction 365\nuntil 366\nalong 367\nfull 368\nfriends 369\ncamera 370\ntruly 371\nwomen 372\nawful 373\nvideo 374\nnext 375\ntell 376\nremember 377\ncouple 378\nstupid 379\nstart 380\nstars 381\nperhaps 382\nsex 383\nmean 384\ncame 385\nrecommend 386\nlet 387\nmoments 388\nwonderful 389\nepisode 390\nunderstand 391\nsmall 392\nface 393\nterrible 394\nplaying 395\nschool 396\ngetting 397\nwritten 398\ndoing 399\noften 400\nkeep 401\nearly 402\nname 403\nperfect 404\nstyle 405\nhuman 406\ndefinitely 407\ngives 408\nothers 409\nitself 410\nlines 411\nlive 412\nbecome 413\ndialogue 414\nperson 415\nlost 416\nfinally 417\npiece 418\nhead 419\ncase 420\nfelt 421\nyes 422\nliked 423\nsupposed 424\ntitle 425\ncouldn't 426\nabsolutely 427\nwhite 428\nagainst 429\nboy 430\npicture 431\nsort 432\nworse 433\ncertainly 434\nwent 435\nentire 436\nwaste 437\ncinema 438\nproblem 439\nhope 440\nentertaining 441\nshe's 442\nmr 443\noverall 444\nevil 445\ncalled 446\nloved 447\nbased 448\noh 449\nseveral 450\nfans 451\nmother 452\ndrama 453\nbeginning 454\nkiller 455\nlives 456\n5 457\ndirection 458\ncare 459\nalready 460\nbecomes 461\nlaugh 462\nexample 463\nfriend 464\ndark 465\ndespite 466\nunder 467\nseemed 468\nthroughout 469\n4 470\nturn 471\nunfortunately 472\nwanted 473\ni'd 474\n 475\nchildren 476\nfinal 477\nfine 478\nhistory 479\namazing 480\nsound 481\nguess 482\nheart 483\ntotally 484\nlead 485\nhumor 486\nwriting 487\nmichael 488\nquality 489\nyou'll 490\nclose 491\nson 492\nguys 493\nwants 494\nworks 495\nbehind 496\ntries 497\nart 498\nside 499\ngame 500\npast 501\nable 502\nb 503\ndays 504\nturns 505\nchild 506\nthey're 507\nhand 508\nflick 509\nenjoyed 510\nact 511\ngenre 512\ntown 513\nfavorite 514\nsoon 515\nkill 516\nstarts 517\nsometimes 518\ncar 519\ngave 520\nrun 521\nlate 522\neyes 523\nactress 524\netc 525\ndirected 526\nhorrible 527\nwon't 528\nviewer 529\nbrilliant 530\nparts 531\nself 532\nthemselves 533\nhour 534\nexpect 535\nthinking 536\nstories 537\nstuff 538\ngirls 539\nobviously 540\nblood 541\ndecent 542\ncity 543\nvoice 544\nhighly 545\nmyself 546\nfeeling 547\nfight 548\nexcept 549\nslow 550\nmatter 551\ntype 552\nanyway 553\nkid 554\nroles 555\nkilled 556\nheard 557\ngod 558\nage 559\nsays 560\nmoment 561\ntook 562\nleave 563\nwriter 564\nstrong 565\ncannot 566\nviolence 567\npolice 568\nhit 569\nstop 570\nhappens 571\nparticularly 572\nknown 573\ninvolved 574\nhappened 575\nextremely 576\ndaughter 577\nobvious 578\ntold 579\nchance 580\nliving 581\ncoming 582\nlack 583\nalone 584\nexperience 585\nwouldn't 586\nincluding 587\nmurder 588\nattempt 589\ns 590\nplease 591\njames 592\nhappen 593\nwonder 594\ncrap 595\nago 596\nbrother 597\nfilm's 598\ngore 599\nnone 600\ncomplete 601\ninterest 602\nscore 603\ngroup 604\ncut 605\nsimple 606\nsave 607\nok 608\nhell 609\nlooked 610\ncareer 611\nnumber 612\nsong 613\npossible 614\nseriously 615\nannoying 616\nshown 617\nexactly 618\nsad 619\nrunning 620\nmusical 621\nserious 622\ntaken 623\nyourself 624\nwhose 625\nreleased 626\ncinematography 627\ndavid 628\nscary 629\nends 630\nenglish 631\nhero 632\nusually 633\nhours 634\nreality 635\nopening 636\ni'll 637\nacross 638\ntoday 639\njokes 640\nlight 641\nhilarious 642\nsomewhat 643\nusual 644\nstarted 645\ncool 646\nridiculous 647\nbody 648\nrelationship 649\nview 650\nlevel 651\nopinion 652\nchange 653\nhappy 654\nmiddle 655\ntaking 656\nwish 657\nhusband 658\nfinds 659\nsaying 660\norder 661\ntalking 662\nones 663\ndocumentary 664\nshots 665\nhuge 666\nnovel 667\nfemale 668\nmostly 669\nrobert 670\npower 671\nepisodes 672\nroom 673\nimportant 674\nrating 675\ntalent 676\nfive 677\nmajor 678\nturned 679\nstrange 680\nword 681\nmodern 682\ncall 683\napparently 684\ndisappointed 685\nsingle 686\nevents 687\ndue 688\nfour 689\nsongs 690\nbasically 691\nattention 692\n7 693\nknows 694\nclearly 695\nsupporting 696\nknew 697\nbritish 698\ntelevision 699\ncomic 700\nnon 701\nfast 702\nearth 703\ncountry 704\nfuture 705\ncheap 706\nclass 707\nthriller 708\n8 709\nsilly 710\nking 711\nproblems 712\naren't 713\neasily 714\nwords 715\ntells 716\nmiss 717\njack 718\nlocal 719\nsequence 720\nbring 721\nentertainment 722\npaul 723\nbeyond 724\nupon 725\nwhether 726\npredictable 727\nmoving 728\nsimilar 729\nstraight 730\nromantic 731\nsets 732\nreview 733\nfalls 734\noscar 735\nmystery 736\nenjoyable 737\nneeds 738\nappears 739\ntalk 740\nrock 741\ngeorge 742\ngiving 743\neye 744\nrichard 745\nwithin 746\nten 747\nanimation 748\nmessage 749\ntheater 750\nnear 751\nabove 752\ndull 753\nnearly 754\nsequel 755\ntheme 756\npoints 757\n' 758\nstand 759\nmention 760\nlady 761\nbunch 762\nadd 763\nfeels 764\nherself 765\nrelease 766\nred 767\nteam 768\nstoryline 769\nsurprised 770\nways 771\nusing 772\nnamed 773\nhaven't 774\nlots 775\neasy 776\nfantastic 777\nbegins 778\nactual 779\nworking 780\neffort 781\nyork 782\ndie 783\nhate 784\nfrench 785\nminute 786\ntale 787\nclear 788\nstay 789\n9 790\nelements 791\nfeature 792\namong 793\nfollow 794\ncomments 795\nre 796\nviewers 797\navoid 798\nsister 799\nshowing 800\ntypical 801\nediting 802\nwhat's 803\nfamous 804\ntried 805\nsorry 806\ndialog 807\ncheck 808\nfall 809\nperiod 810\nseason 811\nform 812\ncertain 813\nfilmed 814\nweak 815\nsoundtrack 816\nmeans 817\nbuy 818\nmaterial 819\nsomehow 820\nrealistic 821\nfigure 822\ncrime 823\ndoubt 824\ngone 825\npeter 826\ntom 827\nkept 828\nviewing 829\nt 830\ngeneral 831\nleads 832\ngreatest 833\nspace 834\nlame 835\nsuspense 836\ndance 837\nimagine 838\nbrought 839\nthird 840\natmosphere 841\nhear 842\nparticular 843\nsequences 844\nwhatever 845\nparents 846\nmove 847\nlee 848\nindeed 849\nlearn 850\nrent 851\nde 852\neventually 853\nnote 854\ndeal 855\naverage 856\nreviews 857\nwait 858\nforget 859\njapanese 860\nsexual 861\npoorly 862\npremise 863\nokay 864\nzombie 865\nsurprise 866\nbelievable 867\nstage 868\npossibly 869\nsit 870\nwho's 871\ndecided 872\nexpected 873\nyou've 874\nsubject 875\nnature 876\nbecame 877\ndifficult 878\nfree 879\nkilling 880\nscreenplay 881\ntruth 882\nromance 883\ndr 884\nnor 885\nreading 886\nneeded 887\nquestion 888\nleaves 889\nstreet 890\n20 891\nmeets 892\nhot 893\nunless 894\nbegin 895\nbaby 896\nsuperb 897\ncredits 898\nimdb 899\notherwise 900\nwrite 901\nshame 902\nlet's 903\nsituation 904\ndramatic 905\nmemorable 906\ndirectors 907\nearlier 908\nmeet 909\ndisney 910\nopen 911\ndog 912\nbadly 913\njoe 914\nmale 915\nweird 916\nacted 917\nforced 918\nlaughs 919\nsci 920\nemotional 921\nolder 922\nrealize 923\nfi 924\ndream 925\nsociety 926\nwriters 927\ninterested 928\nfootage 929\nforward 930\ncomment 931\ncrazy 932\ndeep 933\nsounds 934\nplus 935\nbeauty 936\nwhom 937\namerica 938\nfantasy 939\ndirecting 940\nkeeps 941\nask 942\ndevelopment 943\nfeatures 944\nair 945\nquickly 946\nmess 947\ncreepy 948\ntowards 949\nperfectly 950\nmark 951\nworked 952\nbox 953\ncheesy 954\nunique 955\nsetting 956\nhands 957\nplenty 958\nresult 959\nprevious 960\nbrings 961\neffect 962\ne 963\ntotal 964\npersonal 965\nincredibly 966\nrate 967\nfire 968\nmonster 969\nbusiness 970\nleading 971\napart 972\ncasting 973\nadmit 974\njoke 975\npowerful 976\nappear 977\n```\n\n----------------------------------------\n\nTITLE: Setting Text Context Content in MediaPipe (Python/C++)\nDESCRIPTION: Stores large blocks of text in the MediaPipe context. This is useful for text data that isn't time-aligned with media.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nset_text_context_content\n```\n\nLANGUAGE: c++\nCODE:\n```\nSetTextContextContent\n```\n\n----------------------------------------\n\nTITLE: Defining Android App Colors\nDESCRIPTION: XML resource file defining the application's color scheme.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<resources>\n    <color name=\"colorPrimary\">#008577</color>\n    <color name=\"colorPrimaryDark\">#00574B</color>\n    <color name=\"colorAccent\">#D81B60</color>\n</resources>\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package Dependencies for MediaPipe Edge TPU\nDESCRIPTION: Command to install required Python packages for MediaPipe in Edge TPU Runtime environment. This includes numpy, protobuf, and opencv-python which are core dependencies for MediaPipe functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/model_maker/python/core/utils/testdata/test.txt#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install numpy protobuf==3.20.0 opencv-python\n```\n\n----------------------------------------\n\nTITLE: Processing Charades Dataset with Python\nDESCRIPTION: Commands to build the MediaSequence demo binary and process the Charades dataset for human action recognition.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt mediapipe/examples/desktop/media_sequence:media_sequence_demo --define MEDIAPIPE_DISABLE_GPU=1\n\npython -m mediapipe.examples.desktop.media_sequence.charades_dataset \\\n  --alsologtostderr \\\n  --path_to_charades_data=/tmp/demo_data/ \\\n  --path_to_mediapipe_binary=bazel-bin/mediapipe/examples/desktop/media_sequence/media_sequence_demo \\\n  --path_to_graph_directory=mediapipe/graphs/media_sequence/\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata for Temporal Detection in C++\nDESCRIPTION: Example showing how to create metadata for a temporal detection task using the MediaSequence C++ API. It sets the clip metadata and defines segment timestamps with corresponding labels for event detection.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n// C++: functions from media_sequence.h\ntensorflow::SequenceExample sequence;\nSetClipDataPath(\"path_to_video\", &sequence);\nSetClipStartTimestamp(1000000, &sequence);\nSetClipEndTimestamp(6000000, &sequence);\n\nSetSegmentStartTimestamp({2000000, 4000000}, &sequence);\nSetSegmentEndTimestamp({3500000, 6000000}, &sequence);\nSetSegmentLabelIndex({4, 3}, &sequence);\nSetSegmentLabelString({\"run\", \"jump\"}, &sequence);\n```\n\n----------------------------------------\n\nTITLE: Normalized Grid Coordinates with Confidence Values for MediaPipe\nDESCRIPTION: This dataset contains normalized grid coordinates with associated confidence values. Each line represents a point with four values: x-coordinate, y-coordinate, followed by two confidence values (both set to 1 in this dataset). The coordinates appear to follow a systematic grid pattern with regular intervals (0.03125, 0.09375, etc.)\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_0.txt#2025-04-23_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n0.65625 0.46875 1 1\n0.65625 0.46875 1 1\n0.71875 0.46875 1 1\n0.71875 0.46875 1 1\n0.78125 0.46875 1 1\n0.78125 0.46875 1 1\n0.84375 0.46875 1 1\n0.84375 0.46875 1 1\n0.90625 0.46875 1 1\n0.90625 0.46875 1 1\n0.96875 0.46875 1 1\n0.96875 0.46875 1 1\n0.03125 0.53125 1 1\n0.03125 0.53125 1 1\n0.09375 0.53125 1 1\n0.09375 0.53125 1 1\n0.15625 0.53125 1 1\n0.15625 0.53125 1 1\n0.21875 0.53125 1 1\n0.21875 0.53125 1 1\n0.28125 0.53125 1 1\n0.28125 0.53125 1 1\n0.34375 0.53125 1 1\n0.34375 0.53125 1 1\n0.40625 0.53125 1 1\n0.40625 0.53125 1 1\n0.46875 0.53125 1 1\n0.46875 0.53125 1 1\n0.53125 0.53125 1 1\n0.53125 0.53125 1 1\n0.59375 0.53125 1 1\n0.59375 0.53125 1 1\n0.65625 0.53125 1 1\n0.65625 0.53125 1 1\n0.71875 0.53125 1 1\n0.71875 0.53125 1 1\n0.78125 0.53125 1 1\n0.78125 0.53125 1 1\n0.84375 0.53125 1 1\n0.84375 0.53125 1 1\n0.90625 0.53125 1 1\n0.90625 0.53125 1 1\n0.96875 0.53125 1 1\n0.96875 0.53125 1 1\n0.03125 0.59375 1 1\n0.03125 0.59375 1 1\n0.09375 0.59375 1 1\n0.09375 0.59375 1 1\n0.15625 0.59375 1 1\n0.15625 0.59375 1 1\n0.21875 0.59375 1 1\n0.21875 0.59375 1 1\n0.28125 0.59375 1 1\n0.28125 0.59375 1 1\n0.34375 0.59375 1 1\n0.34375 0.59375 1 1\n0.40625 0.59375 1 1\n0.40625 0.59375 1 1\n0.46875 0.59375 1 1\n0.46875 0.59375 1 1\n0.53125 0.59375 1 1\n0.53125 0.59375 1 1\n0.59375 0.59375 1 1\n0.59375 0.59375 1 1\n0.65625 0.59375 1 1\n0.65625 0.59375 1 1\n0.71875 0.59375 1 1\n0.71875 0.59375 1 1\n0.78125 0.59375 1 1\n0.78125 0.59375 1 1\n0.84375 0.59375 1 1\n0.84375 0.59375 1 1\n0.90625 0.59375 1 1\n0.90625 0.59375 1 1\n0.96875 0.59375 1 1\n0.96875 0.59375 1 1\n```\n\n----------------------------------------\n\nTITLE: Clip-Related Keys in TensorFlow SequenceExamples\nDESCRIPTION: Reference for keys related to clips in TensorFlow SequenceExamples. These include paths, timestamps, labels, media identifiers, and encoded media data for managing multimedia clips.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_10\n\nLANGUAGE: markdown\nCODE:\n```\n| key | type | python call / c++ call | description |\n|-----|------|------------------------|-------------|\n|`clip/data_path`|context bytes|`set_clip_data_path` / `SetClipDataPath`|The relative path to the data on disk from some root directory.|\n|`clip/start/timestamp`|context int|`set_clip_start_timestamp` / `SetClipStartTimestamp`|The start time, in microseconds, for the start of the clip in the media.|\n|`clip/end/timestamp`|context int|`set_clip_end_timestamp` / `SetClipEndTimestamp`|The end time, in microseconds, for the end of the clip in the media.|\n|`clip/label/index`|context int list|`set_clip_label_index` / `SetClipLabelIndex`|A list of label indices for this clip.|\n|`clip/label/string`|context string list|`set_clip_label_string` / `SetClipLabelString`|A list of label strings for this clip.|\n|`clip/label/confidence`|context float list|`set_clip_label_confidence` / `SetClipLabelConfidence`|A list of label confidences for this clip.|\n|`clip/media_id`|context bytes|`set_clip_media_id` / `SetClipMediaId`|Any identifier for the media beyond the data path.|\n|`clip/alternative_media_id`|context bytes|`set_clip_alternative_media_id` / `SetClipAlternativeMediaId`|Yet another alternative identifier.|\n|`clip/encoded_media_bytes`|context bytes|`set_clip_encoded_media_bytes` / `SetClipEncodedMediaBytes`|The encoded bytes for storing media directly in the SequenceExample.|\n|`clip/encoded_media_start_timestamp`|context int|`set_clip_encoded_media_start_timestamp` / `SetClipEncodedMediaStartTimestamp`|The start time for the encoded media if not preserved during encoding.|\n```\n\n----------------------------------------\n\nTITLE: Setting Text Context Token IDs in MediaPipe (Python/C++)\nDESCRIPTION: Stores large blocks of text as token IDs in the MediaPipe context. This is useful for tokenized text data not time-aligned with media.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nset_text_context_token_id\n```\n\nLANGUAGE: c++\nCODE:\n```\nSetTextContextTokenId\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Documentation Link in Markdown\nDESCRIPTION: Simple markdown document providing a reference link to the official MediaPipe documentation.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nMediaPipe\\n=====================================\\nPlease see https://developers.google.com/mediapipe/\n```\n\n----------------------------------------\n\nTITLE: Installing MSYS2 Packages on Windows for MediaPipe\nDESCRIPTION: Command to install necessary MSYS2 packages (git, patch, unzip) required for MediaPipe on Windows.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_27\n\nLANGUAGE: batch\nCODE:\n```\nC:\\> pacman -S git patch unzip\n```\n\n----------------------------------------\n\nTITLE: Adding Bazel Build Rules for Custom Type Bindings\nDESCRIPTION: Bazel build configuration for creating pybind11 extensions for custom type binding and packet methods. Defines build targets with their source files and dependencies.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/python_framework.md#2025-04-23_snippet_2\n\nLANGUAGE: bazel\nCODE:\n```\nload(\"@pybind11_bazel//:build_defs.bzl\", \"pybind_extension\")\n\npybind_extension(\n    name = \"my_type_binding\",\n    srcs = [\"my_type_binding.cc\"],\n    deps = [\":my_type\"],\n)\n\npybind_extension(\n    name = \"my_packet_methods\",\n    srcs = [\"my_packet_methods.cc\"],\n    deps = [\n        \":my_type\",\n        \"//mediapipe/framework:packet\"\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Building the MediaSequence Demo Binary in Bash\nDESCRIPTION: Command to compile the C++ binary for MediaSequence demo with GPU disabled.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel build -c opt mediapipe/examples/desktop/media_sequence:media_sequence_demo --define MEDIAPIPE_DISABLE_GPU=1\n```\n\n----------------------------------------\n\nTITLE: Generating Custom Kinetics Format Dataset\nDESCRIPTION: Commands to download a sample video and generate a dataset in Kinetics format. Includes creation of a custom CSV file and execution of the dataset generation script.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/media_sequence/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\necho \"Credit for this video belongs to: ESA/Hubble; Music: Johan B. Monell\"\nwget https://cdn.spacetelescope.org/archives/videos/medium_podcast/heic1608c.mp4 -O /tmp/heic1608c.mp4\nCUSTOM_CSV=/tmp/custom_kinetics.csv\nVIDEO_PATH=/tmp/heic1608c.mp4\necho -e \"video,time_start,time_end,split\\n${VIDEO_PATH},0,10,custom\" > ${CUSTOM_CSV}\n\nbazel build -c opt mediapipe/examples/desktop/media_sequence:media_sequence_demo \\\n  --define MEDIAPIPE_DISABLE_GPU=1\n\npython -m mediapipe.examples.desktop.media_sequence.kinetics_dataset \\\n  --alsologtostderr \\\n  --splits_to_process=custom \\\n  --path_to_custom_csv=${CUSTOM_CSV} \\\n  --video_path_format_string={video} \\\n  --path_to_kinetics_data=/tmp/ms/kinetics/ \\\n  --path_to_mediapipe_binary=bazel-bin/mediapipe/examples/desktop/\\\nmedia_sequence/media_sequence_demo  \\\n  --path_to_graph_directory=mediapipe/graphs/media_sequence/\n```\n\n----------------------------------------\n\nTITLE: Generating MediaPipe Requirements Lock File with pip-compile\nDESCRIPTION: Command used to generate the requirements lock file using pip-compile. This command takes the requirements.txt file as input and outputs the pinned dependencies to requirements_lock.txt.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/requirements_lock.txt#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip-compile --output-file=mediapipe/opensource_only/requirements_lock.txt mediapipe/opensource_only/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Face Landmark Detection Table\nDESCRIPTION: Markdown table showing different face landmark detection subgraphs with their respective implementations for CPU and GPU, including links to implementation files.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/modules/face_landmark/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nSubgraphs|Details\n:--- | :---\n[`FaceLandmarkCpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/face_landmark/face_landmark_cpu.pbtxt)| Detects landmarks on a single face. (CPU input, and inference is executed on CPU.)\n[`FaceLandmarkGpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/face_landmark/face_landmark_gpu.pbtxt)| Detects landmarks on a single face. (GPU input, and inference is executed on GPU)\n[`FaceLandmarkFrontCpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/face_landmark/face_landmark_front_cpu.pbtxt)| Detects and tracks landmarks on multiple faces. (CPU input, and inference is executed on CPU)\n[`FaceLandmarkFrontGpu`](https://github.com/google-ai-edge/mediapipe/tree/master/mediapipe/modules/face_landmark/face_landmark_front_gpu.pbtxt)| Detects and tracks landmarks on multiple faces. (GPU input, and inference is executed on GPU.)\n```\n\n----------------------------------------\n\nTITLE: Adding Text Confidence Score in MediaPipe (Python/C++)\nDESCRIPTION: Adds a confidence score indicating how likely the text is correct. Useful for ASR results or other probabilistic text features.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/util/sequence/README.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nadd_text_confidence\n```\n\nLANGUAGE: c++\nCODE:\n```\nAddTextConfidence\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Contents Configuration\nDESCRIPTION: Configuration for generating a table of contents in the markdown document.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/solutions.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n1. TOC\n{:toc}\n```\n\n----------------------------------------\n\nTITLE: Adding MediaPipe to Python Path in Bash\nDESCRIPTION: Command to add the current directory to the Python path to enable importing MediaPipe modules.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/media_sequence.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=\"${PYTHONPATH};\"+`pwd`\n```\n\n----------------------------------------\n\nTITLE: Running Python Web Server for Model Inference\nDESCRIPTION: Command to run a Python web server for the model inference interface.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython mediapipe/examples/desktop/youtube8m/viewer/server.py --root `pwd`\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for MediaPipe\nDESCRIPTION: Commands to clone the MediaPipe repository and build a Docker image with the tag 'mediapipe'. This creates an isolated environment for MediaPipe development.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_36\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone --depth 1 https://github.com/google/mediapipe.git\n$ cd mediapipe\n$ docker build --tag=mediapipe .\n```\n\n----------------------------------------\n\nTITLE: Object Detection Class Labels - Plain Text\nDESCRIPTION: A list of 20 standard object categories used for detection and classification tasks in computer vision. These labels are commonly used in datasets like PASCAL VOC and object detection models.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/testdata/metadata/segmenter_labelmap.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbackground\naeroplane\nbicycle\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ndining table\ndog\nhorse\nmotorbike\nperson\npotted plant\nsheep\nsofa\ntrain\ntv\n```\n\n----------------------------------------\n\nTITLE: Object Detection Labels\nDESCRIPTION: A categorical list of object detection labels used for classifying and identifying common objects in images or video streams.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/modules/objectron/object_detection_oidv4_labelmap.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n???\nBicycle\nBoot\nLaptop\nPerson\nChair\nCattle\nDesk\nCat\nComputer mouse\nComputer monitor\nBox\nMug\nCoffee cup\nStationary bicycle\nTable\nBottle\nHigh heels\nVehicle\nFootwear\nDog\nBook\nCamera\nCar\n```\n\n----------------------------------------\n\nTITLE: Navigating to Profiler Directory\nDESCRIPTION: Command to navigate to the MediaPipe profiler reporter directory\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/profiler/reporter/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n//mediapipe/framework/profiler/reporter\n```\n\n----------------------------------------\n\nTITLE: Normalized Coordinate Points Grid for MediaPipe\nDESCRIPTION: A dataset of normalized 2D coordinate points with weight values. Each row contains four numbers: x-coordinate, y-coordinate, and two weight values (both set to 1). The points appear to form a grid pattern with points spaced at regular intervals between 0.0625 and 0.9375 for both x and y coordinates.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_0.txt#2025-04-23_snippet_4\n\nLANGUAGE: data\nCODE:\n```\n0.3125 0.6875 1 1\n0.4375 0.6875 1 1\n0.4375 0.6875 1 1\n0.4375 0.6875 1 1\n0.4375 0.6875 1 1\n0.4375 0.6875 1 1\n0.4375 0.6875 1 1\n0.5625 0.6875 1 1\n0.5625 0.6875 1 1\n0.5625 0.6875 1 1\n0.5625 0.6875 1 1\n0.5625 0.6875 1 1\n0.5625 0.6875 1 1\n0.6875 0.6875 1 1\n0.6875 0.6875 1 1\n0.6875 0.6875 1 1\n0.6875 0.6875 1 1\n0.6875 0.6875 1 1\n0.6875 0.6875 1 1\n0.8125 0.6875 1 1\n0.8125 0.6875 1 1\n0.8125 0.6875 1 1\n0.8125 0.6875 1 1\n0.8125 0.6875 1 1\n0.8125 0.6875 1 1\n0.9375 0.6875 1 1\n0.9375 0.6875 1 1\n0.9375 0.6875 1 1\n0.9375 0.6875 1 1\n0.9375 0.6875 1 1\n0.9375 0.6875 1 1\n0.0625 0.8125 1 1\n0.0625 0.8125 1 1\n0.0625 0.8125 1 1\n0.0625 0.8125 1 1\n0.0625 0.8125 1 1\n0.0625 0.8125 1 1\n0.1875 0.8125 1 1\n0.1875 0.8125 1 1\n0.1875 0.8125 1 1\n0.1875 0.8125 1 1\n0.1875 0.8125 1 1\n0.1875 0.8125 1 1\n0.3125 0.8125 1 1\n0.3125 0.8125 1 1\n0.3125 0.8125 1 1\n0.3125 0.8125 1 1\n0.3125 0.8125 1 1\n0.3125 0.8125 1 1\n0.4375 0.8125 1 1\n0.4375 0.8125 1 1\n0.4375 0.8125 1 1\n0.4375 0.8125 1 1\n0.4375 0.8125 1 1\n0.4375 0.8125 1 1\n0.5625 0.8125 1 1\n0.5625 0.8125 1 1\n0.5625 0.8125 1 1\n0.5625 0.8125 1 1\n0.5625 0.8125 1 1\n0.5625 0.8125 1 1\n0.6875 0.8125 1 1\n0.6875 0.8125 1 1\n0.6875 0.8125 1 1\n0.6875 0.8125 1 1\n0.6875 0.8125 1 1\n0.6875 0.8125 1 1\n0.8125 0.8125 1 1\n0.8125 0.8125 1 1\n0.8125 0.8125 1 1\n0.8125 0.8125 1 1\n0.8125 0.8125 1 1\n0.8125 0.8125 1 1\n0.9375 0.8125 1 1\n0.9375 0.8125 1 1\n0.9375 0.8125 1 1\n0.9375 0.8125 1 1\n0.9375 0.8125 1 1\n0.9375 0.8125 1 1\n0.0625 0.9375 1 1\n0.0625 0.9375 1 1\n0.0625 0.9375 1 1\n0.0625 0.9375 1 1\n0.0625 0.9375 1 1\n0.0625 0.9375 1 1\n0.1875 0.9375 1 1\n0.1875 0.9375 1 1\n0.1875 0.9375 1 1\n0.1875 0.9375 1 1\n0.1875 0.9375 1 1\n0.1875 0.9375 1 1\n0.3125 0.9375 1 1\n0.3125 0.9375 1 1\n0.3125 0.9375 1 1\n0.3125 0.9375 1 1\n0.3125 0.9375 1 1\n0.3125 0.9375 1 1\n0.4375 0.9375 1 1\n0.4375 0.9375 1 1\n0.4375 0.9375 1 1\n0.4375 0.9375 1 1\n0.4375 0.9375 1 1\n0.4375 0.9375 1 1\n0.5625 0.9375 1 1\n0.5625 0.9375 1 1\n0.5625 0.9375 1 1\n0.5625 0.9375 1 1\n0.5625 0.9375 1 1\n0.5625 0.9375 1 1\n0.6875 0.9375 1 1\n0.6875 0.9375 1 1\n0.6875 0.9375 1 1\n0.6875 0.9375 1 1\n0.6875 0.9375 1 1\n0.6875 0.9375 1 1\n0.8125 0.9375 1 1\n0.8125 0.9375 1 1\n0.8125 0.9375 1 1\n0.8125 0.9375 1 1\n0.8125 0.9375 1 1\n0.8125 0.9375 1 1\n0.9375 0.9375 1 1\n0.9375 0.9375 1 1\n0.9375 0.9375 1 1\n0.9375 0.9375 1 1\n0.9375 0.9375 1 1\n0.9375 0.9375 1 1\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCV Dependencies in WSL\nDESCRIPTION: Command to install pre-compiled OpenCV libraries and dependencies required for MediaPipe in WSL using the package manager.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nusername@DESKTOP-TMVLBJ1:~/mediapipe$ sudo apt-get install libopencv-core-dev libopencv-highgui-dev \\\n                           libopencv-calib3d-dev libopencv-features2d-dev \\\n                           libopencv-imgproc-dev libopencv-video-dev\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Python Requirements Lock File for Python 3.12\nDESCRIPTION: A pip-compiled requirements lock file that specifies exact versions of all direct and transitive dependencies needed to run MediaPipe's open source components. Generated from a source requirements.txt file using pip-compile.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/requirements_lock_3_12.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip-requirements\nCODE:\n```\nabsl-py==2.1.0\nattrs==24.2.0\ncffi==1.17.1\ncontourpy==1.3.0\ncycler==0.12.1\nflatbuffers==24.3.25\nfonttools==4.54.1\njax==0.4.30\njaxlib==0.4.30\nkiwisolver==1.4.7\nmatplotlib==3.9.2\nml-dtypes==0.5.0\nnumpy==1.26.4\nopencv-contrib-python==4.10.0.84\nopt-einsum==3.4.0\npackaging==24.1\npillow==10.4.0\nprotobuf==4.25.5\npycparser==2.22\npyparsing==3.1.4\npython-dateutil==2.9.0.post0\nscipy==1.13.1\nsentencepiece==0.2.0\nsix==1.16.0\nsounddevice==0.5.0\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository on Windows\nDESCRIPTION: Commands to clone the MediaPipe repository with limited depth and navigate to the project directory on Windows.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_29\n\nLANGUAGE: batch\nCODE:\n```\nC:\\Users\\Username\\mediapipe_repo> git clone --depth 1 https://github.com/google/mediapipe.git\n\n# Change directory into MediaPipe root directory\nC:\\Users\\Username\\mediapipe_repo> cd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Running the Hello World C++ Example in Docker\nDESCRIPTION: Commands to run the MediaPipe container and execute the Hello World C++ example. The GLOG_logtostderr flag enables logging to stderr.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_37\n\nLANGUAGE: bash\nCODE:\n```\n$ docker run -it --name mediapipe mediapipe:latest\n\nroot@bca08b91ff63:/mediapipe# GLOG_logtostderr=1 bazel run --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/hello_world\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents in Markdown\nDESCRIPTION: This snippet demonstrates how to create a table of contents in Markdown using a numbered list and a special syntax to generate the TOC.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/getting_started.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. TOC\n{:toc}\n```\n\n----------------------------------------\n\nTITLE: Running the Profile Printer\nDESCRIPTION: Command to execute the profile printer using Bazel\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/profiler/reporter/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbazel run :print_profile\n```\n\n----------------------------------------\n\nTITLE: Generating MediaPipe Requirements File with pip-compile\nDESCRIPTION: This command uses pip-compile to generate a locked requirements file for MediaPipe's open-source components, specifying Python 3.10 as the target version.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/requirements_lock_3_10.txt#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip-compile --output-file=mediapipe/opensource_only/requirements_lock_3_10.txt mediapipe/opensource_only/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Parsing MediaPipe Normalized Coordinate Data with Size Parameters\nDESCRIPTION: This data represents a grid of normalized coordinates with various size parameters used for MediaPipe model training or testing. Each line contains four floating-point values: X position, Y position, width, and height. Three different dimension configurations are provided for each position.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_1.txt#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n0.921053 0.552632 0.1 0.1\n0.921053 0.552632 0.282843 0.141421\n0.921053 0.552632 0.141421 0.282843\n0.973684 0.552632 0.1 0.1\n0.973684 0.552632 0.282843 0.141421\n0.973684 0.552632 0.141421 0.282843\n0.0263158 0.605263 0.1 0.1\n0.0263158 0.605263 0.282843 0.141421\n0.0263158 0.605263 0.141421 0.282843\n0.0789474 0.605263 0.1 0.1\n0.0789474 0.605263 0.282843 0.141421\n0.0789474 0.605263 0.141421 0.282843\n0.131579 0.605263 0.1 0.1\n0.131579 0.605263 0.282843 0.141421\n0.131579 0.605263 0.141421 0.282843\n0.184211 0.605263 0.1 0.1\n0.184211 0.605263 0.282843 0.141421\n0.184211 0.605263 0.141421 0.282843\n0.236842 0.605263 0.1 0.1\n0.236842 0.605263 0.282843 0.141421\n0.236842 0.605263 0.141421 0.282843\n0.289474 0.605263 0.1 0.1\n0.289474 0.605263 0.282843 0.141421\n0.289474 0.605263 0.141421 0.282843\n0.342105 0.605263 0.1 0.1\n0.342105 0.605263 0.282843 0.141421\n0.342105 0.605263 0.141421 0.282843\n0.394737 0.605263 0.1 0.1\n0.394737 0.605263 0.282843 0.141421\n0.394737 0.605263 0.141421 0.282843\n0.447368 0.605263 0.1 0.1\n0.447368 0.605263 0.282843 0.141421\n0.447368 0.605263 0.141421 0.282843\n```\n\n----------------------------------------\n\nTITLE: Building a Calculator Graph with Poor Node Separation\nDESCRIPTION: Example of a calculator graph construction with poor readability due to lack of visual separation between node definitions. This implementation makes it difficult to identify where each node begins and ends.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/framework_concepts/building_graphs_cpp.md#2025-04-23_snippet_16\n\nLANGUAGE: c++\nCODE:\n```\nCalculatorGraphConfig BuildGraph() {\n  Graph graph;\n\n  // Inputs.\n  Stream<A> a = graph.In(0).Cast<A>();\n  auto& node1 = graph.AddNode(\"Calculator1\");\n  a.ConnectTo(node1.In(\"INPUT\"));\n  Stream<B> b = node1.Out(\"OUTPUT\").Cast<B>();\n  auto& node2 = graph.AddNode(\"Calculator2\");\n  b.ConnectTo(node2.In(\"INPUT\"));\n  Stream<C> c = node2.Out(\"OUTPUT\").Cast<C>();\n  auto& node3 = graph.AddNode(\"Calculator3\");\n  b.ConnectTo(node3.In(\"INPUT_B\"));\n  c.ConnectTo(node3.In(\"INPUT_C\"));\n  Stream<D> d = node3.Out(\"OUTPUT\").Cast<D>();\n  auto& node4 = graph.AddNode(\"Calculator4\");\n  b.ConnectTo(node4.In(\"INPUT_B\"));\n  c.ConnectTo(node4.In(\"INPUT_C\"));\n  d.ConnectTo(node4.In(\"INPUT_D\"));\n  Stream<E> e = node4.Out(\"OUTPUT\").Cast<E>();\n  // Outputs.\n  b.SetName(\"b\").ConnectTo(graph.Out(0));\n  c.SetName(\"c\").ConnectTo(graph.Out(1));\n  d.SetName(\"d\").ConnectTo(graph.Out(2));\n  e.SetName(\"e\").ConnectTo(graph.Out(3));\n\n  return graph.GetConfig();\n}\n```\n\n----------------------------------------\n\nTITLE: Linking Local Provisioning Profiles for MediaPipe iOS Apps\nDESCRIPTION: Command to run a Python script that finds and links the provisioning profiles for all applications with automatic provisioning enabled in Xcode.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 mediapipe/examples/ios/link_local_profiles.py\n```\n\n----------------------------------------\n\nTITLE: Testing MediaPipe Installation with Python Import\nDESCRIPTION: Python code to verify successful installation of MediaPipe by importing the package. This simple test confirms that the package is accessible in the Python environment without errors.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/model_maker/python/core/utils/testdata/test.txt#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 -c \"import mediapipe as mp; print(mp.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: Jekyll front matter configuration for the MediaPipe legacy solutions documentation page, defining layout, target URL, and navigation settings.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/solutions.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: forward\ntarget: https://developers.google.com/mediapipe/solutions/guide#legacy\ntitle: MediaPipe Legacy Solutions\nnav_order: 3\nhas_children: true\nhas_toc: false\n---\n```\n\n----------------------------------------\n\nTITLE: MediaPipe Numerical Coordinate Data\nDESCRIPTION: A dataset of numerical coordinates arranged in a grid pattern. Each line contains four floating-point values, possibly representing normalized x,y coordinates and width/height parameters for bounding boxes or anchor points used in MediaPipe vision models.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_1.txt#2025-04-23_snippet_3\n\nLANGUAGE: data\nCODE:\n```\n0.15 0.35 0.41833 0.41833\n0.25 0.35 0.35 0.35\n0.25 0.35 0.494975 0.247487\n0.25 0.35 0.247487 0.494975\n0.25 0.35 0.606218 0.202073\n0.25 0.35 0.202062 0.606248\n0.25 0.35 0.41833 0.41833\n0.35 0.35 0.35 0.35\n0.35 0.35 0.494975 0.247487\n0.35 0.35 0.247487 0.494975\n0.35 0.35 0.606218 0.202073\n0.35 0.35 0.202062 0.606248\n0.35 0.35 0.41833 0.41833\n0.45 0.35 0.35 0.35\n0.45 0.35 0.494975 0.247487\n0.45 0.35 0.247487 0.494975\n0.45 0.35 0.606218 0.202073\n0.45 0.35 0.202062 0.606248\n0.45 0.35 0.41833 0.41833\n0.55 0.35 0.35 0.35\n0.55 0.35 0.494975 0.247487\n0.55 0.35 0.247487 0.494975\n0.55 0.35 0.606218 0.202073\n0.55 0.35 0.202062 0.606248\n0.55 0.35 0.41833 0.41833\n0.65 0.35 0.35 0.35\n0.65 0.35 0.494975 0.247487\n0.65 0.35 0.247487 0.494975\n0.65 0.35 0.606218 0.202073\n0.65 0.35 0.202062 0.606248\n0.65 0.35 0.41833 0.41833\n0.75 0.35 0.35 0.35\n0.75 0.35 0.494975 0.247487\n0.75 0.35 0.247487 0.494975\n0.75 0.35 0.606218 0.202073\n0.75 0.35 0.202062 0.606248\n0.75 0.35 0.41833 0.41833\n0.85 0.35 0.35 0.35\n0.85 0.35 0.494975 0.247487\n0.85 0.35 0.247487 0.494975\n0.85 0.35 0.606218 0.202073\n0.85 0.35 0.202062 0.606248\n0.85 0.35 0.41833 0.41833\n0.95 0.35 0.35 0.35\n0.95 0.35 0.494975 0.247487\n0.95 0.35 0.247487 0.494975\n0.95 0.35 0.606218 0.202073\n0.95 0.35 0.202063 0.606248\n0.95 0.35 0.41833 0.41833\n0.05 0.45 0.35 0.35\n0.05 0.45 0.494975 0.247487\n0.05 0.45 0.247487 0.494975\n0.05 0.45 0.606218 0.202073\n0.05 0.45 0.202062 0.606248\n0.05 0.45 0.41833 0.41833\n0.15 0.45 0.35 0.35\n0.15 0.45 0.494975 0.247487\n0.15 0.45 0.247487 0.494975\n0.15 0.45 0.606218 0.202073\n0.15 0.45 0.202062 0.606248\n0.15 0.45 0.41833 0.41833\n0.25 0.45 0.35 0.35\n0.25 0.45 0.494975 0.247487\n0.25 0.45 0.247487 0.494975\n0.25 0.45 0.606218 0.202073\n0.25 0.45 0.202062 0.606248\n0.25 0.45 0.41833 0.41833\n0.35 0.45 0.35 0.35\n0.35 0.45 0.494975 0.247487\n0.35 0.45 0.247487 0.494975\n0.35 0.45 0.606218 0.202073\n0.35 0.45 0.202062 0.606248\n0.35 0.45 0.41833 0.41833\n0.45 0.45 0.35 0.35\n0.45 0.45 0.494975 0.247487\n0.45 0.45 0.247487 0.494975\n0.45 0.45 0.606218 0.202073\n0.45 0.45 0.202062 0.606248\n0.45 0.45 0.41833 0.41833\n0.55 0.45 0.35 0.35\n0.55 0.45 0.494975 0.247487\n0.55 0.45 0.247487 0.494975\n0.55 0.45 0.606218 0.202073\n0.55 0.45 0.202062 0.606248\n0.55 0.45 0.41833 0.41833\n0.65 0.45 0.35 0.35\n0.65 0.45 0.494975 0.247487\n0.65 0.45 0.247487 0.494975\n0.65 0.45 0.606218 0.202073\n0.65 0.45 0.202062 0.606248\n0.65 0.45 0.41833 0.41833\n0.75 0.45 0.35 0.35\n0.75 0.45 0.494975 0.247487\n0.75 0.45 0.247487 0.494975\n0.75 0.45 0.606218 0.202073\n0.75 0.45 0.202062 0.606248\n0.75 0.45 0.41833 0.41833\n0.85 0.45 0.35 0.35\n0.85 0.45 0.494975 0.247487\n0.85 0.45 0.247487 0.494975\n0.85 0.45 0.606218 0.202073\n0.85 0.45 0.202062 0.606248\n0.85 0.45 0.41833 0.41833\n0.95 0.45 0.35 0.35\n0.95 0.45 0.494975 0.247487\n0.95 0.45 0.247487 0.494975\n0.95 0.45 0.606218 0.202073\n0.95 0.45 0.202063 0.606248\n0.95 0.45 0.41833 0.41833\n0.05 0.55 0.35 0.35\n0.05 0.55 0.494975 0.247487\n0.05 0.55 0.247487 0.494975\n0.05 0.55 0.606218 0.202073\n0.05 0.55 0.202062 0.606248\n0.05 0.55 0.41833 0.41833\n0.15 0.55 0.35 0.35\n0.15 0.55 0.494975 0.247487\n0.15 0.55 0.247487 0.494975\n0.15 0.55 0.606218 0.202073\n0.15 0.55 0.202062 0.606248\n0.15 0.55 0.41833 0.41833\n0.25 0.55 0.35 0.35\n0.25 0.55 0.494975 0.247487\n0.25 0.55 0.247487 0.494975\n0.25 0.55 0.606218 0.202073\n0.25 0.55 0.202062 0.606248\n0.25 0.55 0.41833 0.41833\n0.35 0.55 0.35 0.35\n0.35 0.55 0.494975 0.247487\n0.35 0.55 0.247487 0.494975\n0.35 0.55 0.606218 0.202073\n0.35 0.55 0.202062 0.606248\n0.35 0.55 0.41833 0.41833\n0.45 0.55 0.35 0.35\n0.45 0.55 0.494975 0.247487\n0.45 0.55 0.247487 0.494975\n0.45 0.55 0.606218 0.202073\n0.45 0.55 0.202062 0.606248\n0.45 0.55 0.41833 0.41833\n0.55 0.55 0.35 0.35\n0.55 0.55 0.494975 0.247487\n0.55 0.55 0.247487 0.494975\n0.55 0.55 0.606218 0.202073\n0.55 0.55 0.202062 0.606248\n0.55 0.55 0.41833 0.41833\n0.65 0.55 0.35 0.35\n0.65 0.55 0.494975 0.247487\n0.65 0.55 0.247487 0.494975\n0.65 0.55 0.606218 0.202073\n0.65 0.55 0.202062 0.606248\n0.65 0.55 0.41833 0.41833\n0.75 0.55 0.35 0.35\n0.75 0.55 0.494975 0.247487\n0.75 0.55 0.247487 0.494975\n0.75 0.55 0.606218 0.202073\n0.75 0.55 0.202062 0.606248\n0.75 0.55 0.41833 0.41833\n0.85 0.55 0.35 0.35\n0.85 0.55 0.494975 0.247487\n0.85 0.55 0.247487 0.494975\n0.85 0.55 0.606218 0.202073\n0.85 0.55 0.202062 0.606248\n0.85 0.55 0.41833 0.41833\n0.95 0.55 0.35 0.35\n0.95 0.55 0.494975 0.247487\n0.95 0.55 0.247487 0.494975\n0.95 0.55 0.606218 0.202073\n0.95 0.55 0.202063 0.606248\n0.95 0.55 0.41833 0.41833\n0.05 0.65 0.35 0.35\n0.05 0.65 0.494975 0.247487\n0.05 0.65 0.247487 0.494975\n0.05 0.65 0.606218 0.202073\n0.05 0.65 0.202062 0.606248\n0.05 0.65 0.41833 0.41833\n0.15 0.65 0.35 0.35\n0.15 0.65 0.494975 0.247487\n0.15 0.65 0.247487 0.494975\n0.15 0.65 0.606218 0.202073\n0.15 0.65 0.202062 0.606248\n0.15 0.65 0.41833 0.41833\n0.25 0.65 0.35 0.35\n0.25 0.65 0.494975 0.247487\n0.25 0.65 0.247487 0.494975\n0.25 0.65 0.606218 0.202073\n0.25 0.65 0.202062 0.606248\n0.25 0.65 0.41833 0.41833\n0.35 0.65 0.35 0.35\n0.35 0.65 0.494975 0.247487\n0.35 0.65 0.247487 0.494975\n0.35 0.65 0.606218 0.202073\n0.35 0.65 0.202062 0.606248\n0.35 0.65 0.41833 0.41833\n0.45 0.65 0.35 0.35\n0.45 0.65 0.494975 0.247487\n0.45 0.65 0.247487 0.494975\n0.45 0.65 0.606218 0.202073\n0.45 0.65 0.202062 0.606248\n0.45 0.65 0.41833 0.41833\n0.55 0.65 0.35 0.35\n0.55 0.65 0.494975 0.247487\n0.55 0.65 0.247487 0.494975\n0.55 0.65 0.606218 0.202073\n0.55 0.65 0.202062 0.606248\n0.55 0.65 0.41833 0.41833\n0.65 0.65 0.35 0.35\n0.65 0.65 0.494975 0.247487\n0.65 0.65 0.247487 0.494975\n0.65 0.65 0.606218 0.202073\n0.65 0.65 0.202062 0.606248\n0.65 0.65 0.41833 0.41833\n0.75 0.65 0.35 0.35\n0.75 0.65 0.494975 0.247487\n0.75 0.65 0.247487 0.494975\n0.75 0.65 0.606218 0.202073\n0.75 0.65 0.202062 0.606248\n0.75 0.65 0.41833 0.41833\n0.85 0.65 0.35 0.35\n0.85 0.65 0.494975 0.247487\n0.85 0.65 0.247487 0.494975\n0.85 0.65 0.606218 0.202073\n0.85 0.65 0.202062 0.606248\n0.85 0.65 0.41833 0.41833\n0.95 0.65 0.35 0.35\n0.95 0.65 0.494975 0.247487\n0.95 0.65 0.247487 0.494975\n0.95 0.65 0.606218 0.202073\n0.95 0.65 0.202063 0.606248\n0.95 0.65 0.41833 0.41833\n0.05 0.75 0.35 0.35\n0.05 0.75 0.494975 0.247487\n0.05 0.75 0.247487 0.494975\n0.05 0.75 0.606218 0.202073\n0.05 0.75 0.202062 0.606248\n0.05 0.75 0.41833 0.41833\n0.15 0.75 0.35 0.35\n0.15 0.75 0.494975 0.247487\n0.15 0.75 0.247487 0.494975\n0.15 0.75 0.606218 0.202073\n0.15 0.75 0.202062 0.606248\n0.15 0.75 0.41833 0.41833\n0.25 0.75 0.35 0.35\n0.25 0.75 0.494975 0.247487\n0.25 0.75 0.247487 0.494975\n0.25 0.75 0.606218 0.202073\n0.25 0.75 0.202062 0.606248\n0.25 0.75 0.41833 0.41833\n0.35 0.75 0.35 0.35\n0.35 0.75 0.494975 0.247487\n0.35 0.75 0.247487 0.494975\n0.35 0.75 0.606218 0.202073\n0.35 0.75 0.202062 0.606248\n0.35 0.75 0.41833 0.41833\n0.45 0.75 0.35 0.35\n0.45 0.75 0.494975 0.247487\n0.45 0.75 0.247487 0.494975\n0.45 0.75 0.606218 0.202073\n0.45 0.75 0.202062 0.606248\n0.45 0.75 0.41833 0.41833\n0.55 0.75 0.35 0.35\n0.55 0.75 0.494975 0.247487\n0.55 0.75 0.247487 0.494975\n0.55 0.75 0.606218 0.202073\n0.55 0.75 0.202062 0.606248\n0.55 0.75 0.41833 0.41833\n0.65 0.75 0.35 0.35\n0.65 0.75 0.494975 0.247487\n0.65 0.75 0.247487 0.494975\n0.65 0.75 0.606218 0.202073\n0.65 0.75 0.202062 0.606248\n0.65 0.75 0.41833 0.41833\n0.75 0.75 0.35 0.35\n0.75 0.75 0.494975 0.247487\n0.75 0.75 0.247487 0.494975\n0.75 0.75 0.606218 0.202073\n0.75 0.75 0.202062 0.606248\n0.75 0.75 0.41833 0.41833\n0.85 0.75 0.35 0.35\n0.85 0.75 0.494975 0.247487\n0.85 0.75 0.247487 0.494975\n0.85 0.75 0.606218 0.202073\n0.85 0.75 0.202062 0.606248\n0.85 0.75 0.41833 0.41833\n0.95 0.75 0.35 0.35\n0.95 0.75 0.494975 0.247487\n0.95 0.75 0.247487 0.494975\n0.95 0.75 0.606218 0.202073\n0.95 0.75 0.202063 0.606248\n0.95 0.75 0.41833 0.41833\n0.05 0.85 0.35 0.35\n0.05 0.85 0.494975 0.247487\n0.05 0.85 0.247487 0.494975\n0.05 0.85 0.606218 0.202073\n0.05 0.85 0.202062 0.606248\n0.05 0.85 0.41833 0.41833\n0.15 0.85 0.35 0.35\n0.15 0.85 0.494975 0.247487\n0.15 0.85 0.247487 0.494975\n0.15 0.85 0.606218 0.202073\n0.15 0.85 0.202062 0.606248\n0.15 0.85 0.41833 0.41833\n0.25 0.85 0.35 0.35\n0.25 0.85 0.494975 0.247487\n0.25 0.85 0.247487 0.494975\n0.25 0.85 0.606218 0.202073\n0.25 0.85 0.202062 0.606248\n0.25 0.85 0.41833 0.41833\n0.35 0.85 0.35 0.35\n0.35 0.85 0.494975 0.247487\n0.35 0.85 0.247487 0.494975\n0.35 0.85 0.606218 0.202073\n0.35 0.85 0.202062 0.606248\n0.35 0.85 0.41833 0.41833\n0.45 0.85 0.35 0.35\n0.45 0.85 0.494975 0.247487\n0.45 0.85 0.247487 0.494975\n0.45 0.85 0.606218 0.202073\n0.45 0.85 0.202062 0.606248\n0.45 0.85 0.41833 0.41833\n0.55 0.85 0.35 0.35\n0.55 0.85 0.494975 0.247487\n0.55 0.85 0.247487 0.494975\n0.55 0.85 0.606218 0.202073\n0.55 0.85 0.202062 0.606248\n0.55 0.85 0.41833 0.41833\n0.65 0.85 0.35 0.35\n0.65 0.85 0.494975 0.247487\n0.65 0.85 0.247487 0.494975\n0.65 0.85 0.606218 0.202073\n0.65 0.85 0.202062 0.606248\n0.65 0.85 0.41833 0.41833\n0.75 0.85 0.35 0.35\n0.75 0.85 0.494975 0.247487\n0.75 0.85 0.247487 0.494975\n0.75 0.85 0.606218 0.202073\n0.75 0.85 0.202062 0.606248\n0.75 0.85 0.41833 0.41833\n0.85 0.85 0.35 0.35\n0.85 0.85 0.494975 0.247487\n0.85 0.85 0.247487 0.494975\n0.85 0.85 0.606218 0.202073\n0.85 0.85 0.202062 0.606248\n0.85 0.85 0.41833 0.41833\n0.95 0.85 0.35 0.35\n0.95 0.85 0.494975 0.247487\n0.95 0.85 0.247487 0.494975\n0.95 0.85 0.606218 0.202073\n0.95 0.85 0.202063 0.606248\n0.95 0.85 0.41833 0.41833\n0.05 0.95 0.35 0.35\n0.05 0.95 0.494975 0.247487\n0.05 0.95 0.247487 0.494975\n0.05 0.95 0.606218 0.202073\n0.05 0.95 0.202062 0.606248\n0.05 0.95 0.41833 0.41833\n0.15 0.95 0.35 0.35\n0.15 0.95 0.494975 0.247487\n0.15 0.95 0.247487 0.494975\n0.15 0.95 0.606218 0.202073\n0.15 0.95 0.202062 0.606248\n0.15 0.95 0.41833 0.41833\n0.25 0.95 0.35 0.35\n0.25 0.95 0.494975 0.247487\n0.25 0.95 0.247487 0.494975\n0.25 0.95 0.606218 0.202073\n0.25 0.95 0.202062 0.606248\n0.25 0.95 0.41833 0.41833\n0.35 0.95 0.35 0.35\n0.35 0.95 0.494975 0.247487\n0.35 0.95 0.247487 0.494975\n0.35 0.95 0.606218 0.202073\n0.35 0.95 0.202062 0.606248\n0.35 0.95 0.41833 0.41833\n0.45 0.95 0.35 0.35\n0.45 0.95 0.494975 0.247487\n0.45 0.95 0.247487 0.494975\n0.45 0.95 0.606218 0.202073\n0.45 0.95 0.202062 0.606248\n0.45 0.95 0.41833 0.41833\n0.55 0.95 0.35 0.35\n```\n\n----------------------------------------\n\nTITLE: Creating Android Activity Layout XML\nDESCRIPTION: Defines the main activity layout with a TextView displaying 'Hello World!' using ConstraintLayout.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<android.support.constraint.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\">\n\n  <TextView\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:text=\"Hello World!\"\n    app:layout_constraintBottom_toBottomOf=\"parent\"\n    app:layout_constraintLeft_toLeftOf=\"parent\"\n    app:layout_constraintRight_toRightOf=\"parent\"\n    app:layout_constraintTop_toTopOf=\"parent\" />\n\n</android.support.constraint.ConstraintLayout>\n```\n\n----------------------------------------\n\nTITLE: Normalized Coordinate Dataset for Object Detection in MediaPipe\nDESCRIPTION: A comprehensive set of normalized coordinates used for object detection or tracking in MediaPipe. Each line contains four float values representing position (first two values) and dimensions (last two values), all normalized between 0 and 1.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_1.txt#2025-04-23_snippet_2\n\nLANGUAGE: data\nCODE:\n```\n0.342105 0.868421 0.1 0.1\n0.342105 0.868421 0.282843 0.141421\n0.342105 0.868421 0.141421 0.282843\n0.394737 0.868421 0.1 0.1\n0.394737 0.868421 0.282843 0.141421\n0.394737 0.868421 0.141421 0.282843\n0.447368 0.868421 0.1 0.1\n0.447368 0.868421 0.282843 0.141421\n0.447368 0.868421 0.141421 0.282843\n0.5 0.868421 0.1 0.1\n0.5 0.868421 0.282843 0.141421\n0.5 0.868421 0.141421 0.282843\n0.552632 0.868421 0.1 0.1\n0.552632 0.868421 0.282843 0.141421\n0.552632 0.868421 0.141421 0.282843\n0.605263 0.868421 0.1 0.1\n0.605263 0.868421 0.282843 0.141421\n0.605263 0.868421 0.141421 0.282843\n0.657895 0.868421 0.1 0.1\n0.657895 0.868421 0.282843 0.141421\n0.657895 0.868421 0.141421 0.282843\n0.710526 0.868421 0.1 0.1\n0.710526 0.868421 0.282843 0.141421\n0.710526 0.868421 0.141421 0.282843\n0.763158 0.868421 0.1 0.1\n0.763158 0.868421 0.282843 0.141421\n0.763158 0.868421 0.141421 0.282843\n0.81579 0.868421 0.1 0.1\n0.81579 0.868421 0.282843 0.141421\n0.81579 0.868421 0.141421 0.282843\n0.868421 0.868421 0.1 0.1\n0.868421 0.868421 0.282843 0.141421\n0.868421 0.868421 0.141421 0.282843\n0.921053 0.868421 0.1 0.1\n0.921053 0.868421 0.282843 0.141421\n0.921053 0.868421 0.141421 0.282843\n0.973684 0.868421 0.1 0.1\n0.973684 0.868421 0.282843 0.141421\n0.973684 0.868421 0.141421 0.282843\n0.0263158 0.921053 0.1 0.1\n0.0263158 0.921053 0.282843 0.141421\n0.0263158 0.921053 0.141421 0.282843\n0.0789474 0.921053 0.1 0.1\n0.0789474 0.921053 0.282843 0.141421\n0.0789474 0.921053 0.141421 0.282843\n0.131579 0.921053 0.1 0.1\n0.131579 0.921053 0.282843 0.141421\n0.131579 0.921053 0.141421 0.282843\n0.184211 0.921053 0.1 0.1\n0.184211 0.921053 0.282843 0.141421\n0.184211 0.921053 0.141421 0.282843\n0.236842 0.921053 0.1 0.1\n0.236842 0.921053 0.282843 0.141421\n0.236842 0.921053 0.141421 0.282843\n0.289474 0.921053 0.1 0.1\n0.289474 0.921053 0.282843 0.141421\n0.289474 0.921053 0.141421 0.282843\n0.342105 0.921053 0.1 0.1\n0.342105 0.921053 0.282843 0.141421\n0.342105 0.921053 0.141421 0.282843\n0.394737 0.921053 0.1 0.1\n0.394737 0.921053 0.282843 0.141421\n0.394737 0.921053 0.141421 0.282843\n0.447368 0.921053 0.1 0.1\n0.447368 0.921053 0.282843 0.141421\n0.447368 0.921053 0.141421 0.282843\n0.5 0.921053 0.1 0.1\n0.5 0.921053 0.282843 0.141421\n0.5 0.921053 0.141421 0.282843\n0.552632 0.921053 0.1 0.1\n0.552632 0.921053 0.282843 0.141421\n0.552632 0.921053 0.141421 0.282843\n0.605263 0.921053 0.1 0.1\n0.605263 0.921053 0.282843 0.141421\n0.605263 0.921053 0.141421 0.282843\n0.657895 0.921053 0.1 0.1\n0.657895 0.921053 0.282843 0.141421\n0.657895 0.921053 0.141421 0.282843\n0.710526 0.921053 0.1 0.1\n0.710526 0.921053 0.282843 0.141421\n0.710526 0.921053 0.141421 0.282843\n0.763158 0.921053 0.1 0.1\n0.763158 0.921053 0.282843 0.141421\n0.763158 0.921053 0.141421 0.282843\n0.81579 0.921053 0.1 0.1\n0.81579 0.921053 0.282843 0.141421\n0.81579 0.921053 0.141421 0.282843\n0.868421 0.921053 0.1 0.1\n0.868421 0.921053 0.282843 0.141421\n0.868421 0.921053 0.141421 0.282843\n0.921053 0.921053 0.1 0.1\n0.921053 0.921053 0.282843 0.141421\n0.921053 0.921053 0.141421 0.282843\n0.973684 0.921053 0.1 0.1\n0.973684 0.921053 0.282843 0.141421\n0.973684 0.921053 0.141421 0.282843\n0.0263158 0.973684 0.1 0.1\n0.0263158 0.973684 0.282843 0.141421\n0.0263158 0.973684 0.141421 0.282843\n0.0789474 0.973684 0.1 0.1\n0.0789474 0.973684 0.282843 0.141421\n0.0789474 0.973684 0.141421 0.282843\n0.131579 0.973684 0.1 0.1\n0.131579 0.973684 0.282843 0.141421\n0.131579 0.973684 0.141421 0.282843\n0.184211 0.973684 0.1 0.1\n0.184211 0.973684 0.282843 0.141421\n0.184211 0.973684 0.141421 0.282843\n0.236842 0.973684 0.1 0.1\n0.236842 0.973684 0.282843 0.141421\n0.236842 0.973684 0.141421 0.282843\n0.289474 0.973684 0.1 0.1\n0.289474 0.973684 0.282843 0.141421\n0.289474 0.973684 0.141421 0.282843\n0.342105 0.973684 0.1 0.1\n0.342105 0.973684 0.282843 0.141421\n0.342105 0.973684 0.141421 0.282843\n0.394737 0.973684 0.1 0.1\n0.394737 0.973684 0.282843 0.141421\n0.394737 0.973684 0.141421 0.282843\n0.447368 0.973684 0.1 0.1\n0.447368 0.973684 0.282843 0.141421\n0.447368 0.973684 0.141421 0.282843\n0.5 0.973684 0.1 0.1\n0.5 0.973684 0.282843 0.141421\n0.5 0.973684 0.141421 0.282843\n0.552632 0.973684 0.1 0.1\n0.552632 0.973684 0.282843 0.141421\n0.552632 0.973684 0.141421 0.282843\n0.605263 0.973684 0.1 0.1\n0.605263 0.973684 0.282843 0.141421\n0.605263 0.973684 0.141421 0.282843\n0.657895 0.973684 0.1 0.1\n0.657895 0.973684 0.282843 0.141421\n0.657895 0.973684 0.141421 0.282843\n0.710526 0.973684 0.1 0.1\n0.710526 0.973684 0.282843 0.141421\n0.710526 0.973684 0.141421 0.282843\n0.763158 0.973684 0.1 0.1\n0.763158 0.973684 0.282843 0.141421\n0.763158 0.973684 0.141421 0.282843\n0.81579 0.973684 0.1 0.1\n0.81579 0.973684 0.282843 0.141421\n0.81579 0.973684 0.141421 0.282843\n0.868421 0.973684 0.1 0.1\n0.868421 0.973684 0.282843 0.141421\n0.868421 0.973684 0.141421 0.282843\n0.921053 0.973684 0.1 0.1\n0.921053 0.973684 0.282843 0.141421\n0.921053 0.973684 0.141421 0.282843\n0.973684 0.973684 0.1 0.1\n0.973684 0.973684 0.282843 0.141421\n0.973684 0.973684 0.141421 0.282843\n0.05 0.05 0.35 0.35\n0.05 0.05 0.494975 0.247487\n0.05 0.05 0.247487 0.494975\n0.05 0.05 0.606218 0.202073\n0.05 0.05 0.202062 0.606248\n0.05 0.05 0.41833 0.41833\n0.15 0.05 0.35 0.35\n0.15 0.05 0.494975 0.247487\n0.15 0.05 0.247487 0.494975\n0.15 0.05 0.606218 0.202073\n0.15 0.05 0.202062 0.606248\n0.15 0.05 0.41833 0.41833\n0.25 0.05 0.35 0.35\n0.25 0.05 0.494975 0.247487\n0.25 0.05 0.247487 0.494975\n0.25 0.05 0.606218 0.202073\n0.25 0.05 0.202062 0.606248\n0.25 0.05 0.41833 0.41833\n0.35 0.05 0.35 0.35\n0.35 0.05 0.494975 0.247487\n0.35 0.05 0.247487 0.494975\n0.35 0.05 0.606218 0.202073\n0.35 0.05 0.202062 0.606248\n0.35 0.05 0.41833 0.41833\n0.45 0.05 0.35 0.35\n0.45 0.05 0.494975 0.247487\n0.45 0.05 0.247487 0.494975\n0.45 0.05 0.606218 0.202073\n0.45 0.05 0.202062 0.606248\n0.45 0.05 0.41833 0.41833\n0.55 0.05 0.35 0.35\n0.55 0.05 0.494975 0.247487\n0.55 0.05 0.247487 0.494975\n0.55 0.05 0.606218 0.202073\n0.55 0.05 0.202062 0.606248\n0.55 0.05 0.41833 0.41833\n0.65 0.05 0.35 0.35\n0.65 0.05 0.494975 0.247487\n0.65 0.05 0.247487 0.494975\n0.65 0.05 0.606218 0.202073\n0.65 0.05 0.202062 0.606248\n0.65 0.05 0.41833 0.41833\n0.75 0.05 0.35 0.35\n0.75 0.05 0.494975 0.247487\n0.75 0.05 0.247487 0.494975\n0.75 0.05 0.606218 0.202073\n0.75 0.05 0.202062 0.606248\n0.75 0.05 0.41833 0.41833\n0.85 0.05 0.35 0.35\n0.85 0.05 0.494975 0.247487\n0.85 0.05 0.247487 0.494975\n0.85 0.05 0.606218 0.202073\n0.85 0.05 0.202062 0.606248\n0.85 0.05 0.41833 0.41833\n0.95 0.05 0.35 0.35\n0.95 0.05 0.494975 0.247487\n0.95 0.05 0.247487 0.494975\n0.95 0.05 0.606218 0.202073\n0.95 0.05 0.202063 0.606248\n0.95 0.05 0.41833 0.41833\n0.05 0.15 0.35 0.35\n0.05 0.15 0.494975 0.247487\n0.05 0.15 0.247487 0.494975\n0.05 0.15 0.606218 0.202073\n0.05 0.15 0.202062 0.606248\n0.05 0.15 0.41833 0.41833\n0.15 0.15 0.35 0.35\n0.15 0.15 0.494975 0.247487\n0.15 0.15 0.247487 0.494975\n0.15 0.15 0.606218 0.202073\n0.15 0.15 0.202062 0.606248\n0.15 0.15 0.41833 0.41833\n0.25 0.15 0.35 0.35\n0.25 0.15 0.494975 0.247487\n0.25 0.15 0.247487 0.494975\n0.25 0.15 0.606218 0.202073\n0.25 0.15 0.202062 0.606248\n0.25 0.15 0.41833 0.41833\n0.35 0.15 0.35 0.35\n0.35 0.15 0.494975 0.247487\n0.35 0.15 0.247487 0.494975\n0.35 0.15 0.606218 0.202073\n0.35 0.15 0.202062 0.606248\n0.35 0.15 0.41833 0.41833\n0.45 0.15 0.35 0.35\n0.45 0.15 0.494975 0.247487\n0.45 0.15 0.247487 0.494975\n0.45 0.15 0.606218 0.202073\n0.45 0.15 0.202062 0.606248\n0.45 0.15 0.41833 0.41833\n0.55 0.15 0.35 0.35\n0.55 0.15 0.494975 0.247487\n0.55 0.15 0.247487 0.494975\n0.55 0.15 0.606218 0.202073\n0.55 0.15 0.202062 0.606248\n0.55 0.15 0.41833 0.41833\n0.65 0.15 0.35 0.35\n0.65 0.15 0.494975 0.247487\n0.65 0.15 0.247487 0.494975\n0.65 0.15 0.606218 0.202073\n0.65 0.15 0.202062 0.606248\n0.65 0.15 0.41833 0.41833\n0.75 0.15 0.35 0.35\n0.75 0.15 0.494975 0.247487\n0.75 0.15 0.247487 0.494975\n0.75 0.15 0.606218 0.202073\n0.75 0.15 0.202062 0.606248\n0.75 0.15 0.41833 0.41833\n0.85 0.15 0.35 0.35\n0.85 0.15 0.494975 0.247487\n0.85 0.15 0.247487 0.494975\n0.85 0.15 0.606218 0.202073\n0.85 0.15 0.202062 0.606248\n0.85 0.15 0.41833 0.41833\n0.95 0.15 0.35 0.35\n0.95 0.15 0.494975 0.247487\n0.95 0.15 0.247487 0.494975\n0.95 0.15 0.606218 0.202073\n0.95 0.15 0.202063 0.606248\n0.95 0.15 0.41833 0.41833\n0.05 0.25 0.35 0.35\n0.05 0.25 0.494975 0.247487\n0.05 0.25 0.247487 0.494975\n0.05 0.25 0.606218 0.202073\n0.05 0.25 0.202062 0.606248\n0.05 0.25 0.41833 0.41833\n0.15 0.25 0.35 0.35\n0.15 0.25 0.494975 0.247487\n0.15 0.25 0.247487 0.494975\n0.15 0.25 0.606218 0.202073\n0.15 0.25 0.202062 0.606248\n0.15 0.25 0.41833 0.41833\n0.25 0.25 0.35 0.35\n0.25 0.25 0.494975 0.247487\n0.25 0.25 0.247487 0.494975\n0.25 0.25 0.606218 0.202073\n0.25 0.25 0.202062 0.606248\n0.25 0.25 0.41833 0.41833\n0.35 0.25 0.35 0.35\n0.35 0.25 0.494975 0.247487\n0.35 0.25 0.247487 0.494975\n0.35 0.25 0.606218 0.202073\n0.35 0.25 0.202062 0.606248\n0.35 0.25 0.41833 0.41833\n0.45 0.25 0.35 0.35\n0.45 0.25 0.494975 0.247487\n0.45 0.25 0.247487 0.494975\n0.45 0.25 0.606218 0.202073\n0.45 0.25 0.202062 0.606248\n0.45 0.25 0.41833 0.41833\n0.55 0.25 0.35 0.35\n0.55 0.25 0.494975 0.247487\n0.55 0.25 0.247487 0.494975\n0.55 0.25 0.606218 0.202073\n0.55 0.25 0.202062 0.606248\n0.55 0.25 0.41833 0.41833\n0.65 0.25 0.35 0.35\n0.65 0.25 0.494975 0.247487\n0.65 0.25 0.247487 0.494975\n0.65 0.25 0.606218 0.202073\n0.65 0.25 0.202062 0.606248\n0.65 0.25 0.41833 0.41833\n0.75 0.25 0.35 0.35\n0.75 0.25 0.494975 0.247487\n0.75 0.25 0.247487 0.494975\n0.75 0.25 0.606218 0.202073\n0.75 0.25 0.202062 0.606248\n0.75 0.25 0.41833 0.41833\n0.85 0.25 0.35 0.35\n0.85 0.25 0.494975 0.247487\n0.85 0.25 0.247487 0.494975\n0.85 0.25 0.606218 0.202073\n0.85 0.25 0.202062 0.606248\n0.85 0.25 0.41833 0.41833\n0.95 0.25 0.35 0.35\n0.95 0.25 0.494975 0.247487\n0.95 0.25 0.247487 0.494975\n0.95 0.25 0.606218 0.202073\n0.95 0.25 0.202063 0.606248\n0.95 0.25 0.41833 0.41833\n0.05 0.35 0.35 0.35\n0.05 0.35 0.494975 0.247487\n0.05 0.35 0.247487 0.494975\n0.05 0.35 0.606218 0.202073\n0.05 0.35 0.202062 0.606248\n0.05 0.35 0.41833 0.41833\n0.15 0.35 0.35 0.35\n0.15 0.35 0.494975 0.247487\n0.15 0.35 0.247487 0.494975\n0.15 0.35 0.606218 0.202073\n0.15 0.35 0.202062 0.606248\n```\n\n----------------------------------------\n\nTITLE: Listing Anchor Box Coordinates for Object Detection\nDESCRIPTION: This snippet shows a portion of the anchor box coordinates used for object detection. Each line contains four floating-point numbers representing the x and y coordinates of the top-left and bottom-right corners of an anchor box, normalized to the image dimensions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_2.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n0.05 0.05 0.3 0.3\n0.05 0.05 0.377976 0.377976\n0.05 0.05 0.47622 0.47622\n0.05 0.05 0.424264 0.212132\n0.05 0.05 0.534539 0.26727\n0.05 0.05 0.673477 0.336739\n0.05 0.05 0.212132 0.424264\n0.05 0.05 0.26727 0.534539\n0.05 0.05 0.336739 0.673477\n```\n\n----------------------------------------\n\nTITLE: Installing MediaPipe Python Package with Pip\nDESCRIPTION: Command to install the MediaPipe Python package using pip. This ensures that the latest version compatible with Edge TPU Runtime is installed in the Python environment.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/model_maker/python/core/utils/testdata/test.txt#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install mediapipe\n```\n\n----------------------------------------\n\nTITLE: Coordinate Data Matrix\nDESCRIPTION: A matrix of floating point coordinates organized in rows of 4 values each. Each row appears to contain x,y position coordinates followed by width/height dimensions.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_2.txt#2025-04-23_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n0.25 0.45 0.3 0.3\n0.25 0.45 0.377976 0.377976\n0.25 0.45 0.47622 0.47622\n0.25 0.45 0.424264 0.212132\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Numerical Parameters for MediaPipe\nDESCRIPTION: A data file containing coordinate pairs and dimensional values organized in four columns. Each line defines a set of positions and dimensions, likely used for vision processing, detection windows, or transforms in the MediaPipe framework.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_1.txt#2025-04-23_snippet_4\n\nLANGUAGE: data\nCODE:\n```\n0.55 0.95 0.494975 0.247487\n0.55 0.95 0.247487 0.494975\n0.55 0.95 0.606218 0.202073\n0.55 0.95 0.202062 0.606248\n0.55 0.95 0.41833 0.41833\n0.65 0.95 0.35 0.35\n0.65 0.95 0.494975 0.247487\n0.65 0.95 0.247487 0.494975\n0.65 0.95 0.606218 0.202073\n0.65 0.95 0.202062 0.606248\n0.65 0.95 0.41833 0.41833\n0.75 0.95 0.35 0.35\n0.75 0.95 0.494975 0.247487\n0.75 0.95 0.247487 0.494975\n0.75 0.95 0.606218 0.202073\n0.75 0.95 0.202062 0.606248\n0.75 0.95 0.41833 0.41833\n0.85 0.95 0.35 0.35\n0.85 0.95 0.494975 0.247487\n0.85 0.95 0.247487 0.494975\n0.85 0.95 0.606218 0.202073\n0.85 0.95 0.202062 0.606248\n0.85 0.95 0.41833 0.41833\n0.95 0.95 0.35 0.35\n0.95 0.95 0.494975 0.247487\n0.95 0.95 0.247487 0.494975\n0.95 0.95 0.606218 0.202073\n0.95 0.95 0.202063 0.606248\n0.95 0.95 0.41833 0.41833\n0.1 0.1 0.5 0.5\n0.1 0.1 0.707107 0.353553\n0.1 0.1 0.353553 0.707107\n0.1 0.1 0.866025 0.288675\n0.1 0.1 0.288661 0.866069\n0.1 0.1 0.570088 0.570088\n0.3 0.1 0.5 0.5\n0.3 0.1 0.707107 0.353553\n0.3 0.1 0.353553 0.707107\n0.3 0.1 0.866025 0.288675\n0.3 0.1 0.288661 0.866069\n0.3 0.1 0.570088 0.570088\n0.5 0.1 0.5 0.5\n0.5 0.1 0.707107 0.353553\n0.5 0.1 0.353553 0.707107\n0.5 0.1 0.866025 0.288675\n0.5 0.1 0.288661 0.866069\n0.5 0.1 0.570088 0.570088\n0.7 0.1 0.5 0.5\n0.7 0.1 0.707107 0.353553\n0.7 0.1 0.353553 0.707107\n0.7 0.1 0.866025 0.288675\n0.7 0.1 0.288661 0.866069\n0.7 0.1 0.570088 0.570088\n0.9 0.1 0.5 0.5\n0.9 0.1 0.707107 0.353553\n0.9 0.1 0.353553 0.707107\n0.9 0.1 0.866025 0.288675\n0.9 0.1 0.288661 0.866069\n0.9 0.1 0.570088 0.570088\n0.1 0.3 0.5 0.5\n0.1 0.3 0.707107 0.353553\n0.1 0.3 0.353553 0.707107\n0.1 0.3 0.866025 0.288675\n0.1 0.3 0.288661 0.866069\n0.1 0.3 0.570088 0.570088\n0.3 0.3 0.5 0.5\n0.3 0.3 0.707107 0.353553\n0.3 0.3 0.353553 0.707107\n0.3 0.3 0.866025 0.288675\n0.3 0.3 0.288661 0.866069\n0.3 0.3 0.570088 0.570088\n0.5 0.3 0.5 0.5\n0.5 0.3 0.707107 0.353553\n0.5 0.3 0.353553 0.707107\n0.5 0.3 0.866025 0.288675\n0.5 0.3 0.288661 0.866069\n0.5 0.3 0.570088 0.570088\n0.7 0.3 0.5 0.5\n0.7 0.3 0.707107 0.353553\n0.7 0.3 0.353553 0.707107\n0.7 0.3 0.866025 0.288675\n0.7 0.3 0.288661 0.866069\n0.7 0.3 0.570088 0.570088\n0.9 0.3 0.5 0.5\n0.9 0.3 0.707107 0.353553\n0.9 0.3 0.353553 0.707107\n0.9 0.3 0.866025 0.288675\n0.9 0.3 0.288661 0.866069\n0.9 0.3 0.570088 0.570088\n0.1 0.5 0.5 0.5\n0.1 0.5 0.707107 0.353553\n0.1 0.5 0.353553 0.707107\n0.1 0.5 0.866025 0.288675\n0.1 0.5 0.288661 0.866069\n0.1 0.5 0.570088 0.570088\n0.3 0.5 0.5 0.5\n0.3 0.5 0.707107 0.353553\n0.3 0.5 0.353553 0.707107\n0.3 0.5 0.866025 0.288675\n0.3 0.5 0.288661 0.866069\n0.3 0.5 0.570088 0.570088\n0.5 0.5 0.5 0.5\n0.5 0.5 0.707107 0.353553\n0.5 0.5 0.353553 0.707107\n0.5 0.5 0.866025 0.288675\n0.5 0.5 0.288661 0.866069\n0.5 0.5 0.570088 0.570088\n0.7 0.5 0.5 0.5\n0.7 0.5 0.707107 0.353553\n0.7 0.5 0.353553 0.707107\n0.7 0.5 0.866025 0.288675\n0.7 0.5 0.288661 0.866069\n0.7 0.5 0.570088 0.570088\n0.9 0.5 0.5 0.5\n0.9 0.5 0.707107 0.353553\n0.9 0.5 0.353553 0.707107\n0.9 0.5 0.866025 0.288675\n0.9 0.5 0.288661 0.866069\n0.9 0.5 0.570088 0.570088\n0.1 0.7 0.5 0.5\n0.1 0.7 0.707107 0.353553\n0.1 0.7 0.353553 0.707107\n0.1 0.7 0.866025 0.288675\n0.1 0.7 0.288661 0.866069\n0.1 0.7 0.570088 0.570088\n0.3 0.7 0.5 0.5\n0.3 0.7 0.707107 0.353553\n0.3 0.7 0.353553 0.707107\n0.3 0.7 0.866025 0.288675\n0.3 0.7 0.288661 0.866069\n0.3 0.7 0.570088 0.570088\n0.5 0.7 0.5 0.5\n0.5 0.7 0.707107 0.353553\n0.5 0.7 0.353553 0.707107\n0.5 0.7 0.866025 0.288675\n0.5 0.7 0.288661 0.866069\n0.5 0.7 0.570088 0.570088\n0.7 0.7 0.5 0.5\n0.7 0.7 0.707107 0.353553\n0.7 0.7 0.353553 0.707107\n0.7 0.7 0.866025 0.288675\n0.7 0.7 0.288661 0.866069\n0.7 0.7 0.570088 0.570088\n0.9 0.7 0.5 0.5\n0.9 0.7 0.707107 0.353553\n0.9 0.7 0.353553 0.707107\n0.9 0.7 0.866025 0.288675\n0.9 0.7 0.288661 0.866069\n0.9 0.7 0.570088 0.570088\n0.1 0.9 0.5 0.5\n0.1 0.9 0.707107 0.353553\n0.1 0.9 0.353553 0.707107\n0.1 0.9 0.866025 0.288675\n0.1 0.9 0.288661 0.866069\n0.1 0.9 0.570088 0.570088\n0.3 0.9 0.5 0.5\n0.3 0.9 0.707107 0.353553\n0.3 0.9 0.353553 0.707107\n0.3 0.9 0.866025 0.288675\n0.3 0.9 0.288661 0.866069\n0.3 0.9 0.570088 0.570088\n0.5 0.9 0.5 0.5\n0.5 0.9 0.707107 0.353553\n0.5 0.9 0.353553 0.707107\n0.5 0.9 0.866025 0.288675\n0.5 0.9 0.288661 0.866069\n0.5 0.9 0.570088 0.570088\n0.7 0.9 0.5 0.5\n0.7 0.9 0.707107 0.353553\n0.7 0.9 0.353553 0.707107\n0.7 0.9 0.866025 0.288675\n0.7 0.9 0.288661 0.866069\n0.7 0.9 0.570088 0.570088\n0.9 0.9 0.5 0.5\n0.9 0.9 0.707107 0.353553\n0.9 0.9 0.353553 0.707107\n0.9 0.9 0.866025 0.288675\n0.9 0.9 0.288661 0.866069\n0.9 0.9 0.570088 0.570088\n0.166667 0.166667 0.65 0.65\n0.166667 0.166667 0.919239 0.459619\n0.166667 0.166667 0.459619 0.919239\n0.166667 0.166667 1.12583 0.375278\n0.166667 0.166667 0.375259 1.12589\n0.166667 0.166667 0.72111 0.72111\n0.5 0.166667 0.65 0.65\n0.5 0.166667 0.919239 0.459619\n0.5 0.166667 0.459619 0.919239\n0.5 0.166667 1.12583 0.375278\n0.5 0.166667 0.375259 1.12589\n0.5 0.166667 0.72111 0.72111\n0.833333 0.166667 0.65 0.65\n0.833333 0.166667 0.919239 0.459619\n0.833333 0.166667 0.459619 0.919239\n0.833333 0.166667 1.12583 0.375278\n0.833333 0.166667 0.375259 1.12589\n0.833333 0.166667 0.72111 0.72111\n0.166667 0.5 0.65 0.65\n0.166667 0.5 0.919239 0.459619\n0.166667 0.5 0.459619 0.919239\n0.166667 0.5 1.12583 0.375278\n0.166667 0.5 0.375259 1.12589\n0.166667 0.5 0.72111 0.72111\n0.5 0.5 0.65 0.65\n0.5 0.5 0.919239 0.459619\n0.5 0.5 0.459619 0.919239\n0.5 0.5 1.12583 0.375278\n0.5 0.5 0.375259 1.12589\n0.5 0.5 0.72111 0.72111\n0.833333 0.5 0.65 0.65\n0.833333 0.5 0.919239 0.459619\n0.833333 0.5 0.459619 0.919239\n0.833333 0.5 1.12583 0.375278\n0.833333 0.5 0.375259 1.12589\n0.833333 0.5 0.72111 0.72111\n0.166667 0.833333 0.65 0.65\n0.166667 0.833333 0.919239 0.459619\n0.166667 0.833333 0.459619 0.919239\n0.166667 0.833333 1.12583 0.375278\n0.166667 0.833333 0.375259 1.12589\n0.166667 0.833333 0.72111 0.72111\n0.5 0.833333 0.65 0.65\n0.5 0.833333 0.919239 0.459619\n0.5 0.833333 0.459619 0.919239\n0.5 0.833333 1.12583 0.375278\n0.5 0.833333 0.375259 1.12589\n0.5 0.833333 0.72111 0.72111\n0.833333 0.833333 0.65 0.65\n0.833333 0.833333 0.919239 0.459619\n0.833333 0.833333 0.459619 0.919239\n0.833333 0.833333 1.12583 0.375278\n0.833333 0.833333 0.375259 1.12589\n0.833333 0.833333 0.72111 0.72111\n0.25 0.25 0.8 0.8\n0.25 0.25 1.13137 0.565686\n0.25 0.25 0.565685 1.13137\n0.25 0.25 1.38564 0.46188\n0.25 0.25 0.461857 1.38571\n0.25 0.25 0.87178 0.87178\n0.75 0.25 0.8 0.8\n0.75 0.25 1.13137 0.565686\n0.75 0.25 0.565685 1.13137\n0.75 0.25 1.38564 0.46188\n0.75 0.25 0.461857 1.38571\n0.75 0.25 0.87178 0.87178\n0.25 0.75 0.8 0.8\n0.25 0.75 1.13137 0.565686\n0.25 0.75 0.565685 1.13137\n0.25 0.75 1.38564 0.46188\n0.25 0.75 0.461857 1.38571\n0.25 0.75 0.87178 0.87178\n0.75 0.75 0.8 0.8\n0.75 0.75 1.13137 0.565686\n0.75 0.75 0.565685 1.13137\n0.75 0.75 1.38564 0.46188\n0.75 0.75 0.461857 1.38571\n0.75 0.75 0.87178 0.87178\n0.5 0.5 0.95 0.95\n0.5 0.5 1.3435 0.671751\n0.5 0.5 0.671751 1.3435\n0.5 0.5 1.64545 0.548483\n0.5 0.5 0.548455 1.64553\n0.5 0.5 0.974679 0.974679\n```\n\n----------------------------------------\n\nTITLE: Switching to OpenCV 4 for Template Matching in Android\nDESCRIPTION: Commands to temporarily switch from OpenCV 3 to OpenCV 4 to avoid knnMatch issues with NDK 17+, build and install the template matching app, and then switch back to OpenCV 3.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/knift.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Switch to OpenCV 4\nsed -i -e 's:3.4.3/opencv-3.4.3:4.0.1/opencv-4.0.1:g' WORKSPACE\nsed -i -e 's:libopencv_java3:libopencv_java4:g' third_party/opencv_android.BUILD\n\n# Build and install app\nbazel build -c opt --config=android_arm64 mediapipe/examples/android/src/java/com/google/mediapipe/apps/templatematchingcpu\nadb install -r bazel-bin/mediapipe/examples/android/src/java/com/google/mediapipe/apps/templatematchingcpu/templatematchingcpu.apk\n\n# Switch back to OpenCV 3\nsed -i -e 's:4.0.1/opencv-4.0.1:3.4.3/opencv-3.4.3:g' WORKSPACE\nsed -i -e 's:libopencv_java4:libopencv_java3:g' third_party/opencv_android.BUILD\n```\n\n----------------------------------------\n\nTITLE: Grid Coordinate Data\nDESCRIPTION: A series of 2D coordinates forming a grid pattern, with each coordinate followed by two additional values (likely width and height). The coordinates range from 0 to 1 in both x and y directions, with varying increments.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/anchor_golden_file_0.txt#2025-04-23_snippet_2\n\nLANGUAGE: data\nCODE:\n```\n0.390625 0.890625 1 1\n0.390625 0.890625 1 1\n0.421875 0.890625 1 1\n0.421875 0.890625 1 1\n0.453125 0.890625 1 1\n0.453125 0.890625 1 1\n0.484375 0.890625 1 1\n0.484375 0.890625 1 1\n0.515625 0.890625 1 1\n0.515625 0.890625 1 1\n...\n```\n\n----------------------------------------\n\nTITLE: Building and Running MediaPipe Hello World in WSL\nDESCRIPTION: Commands to set up logging and run the MediaPipe Hello World example in WSL with GPU disabled, demonstrating basic functionality.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\nusername@DESKTOP-TMVLBJ1:~/mediapipe$ export GLOG_logtostderr=1\n\n# Need bazel flag 'MEDIAPIPE_DISABLE_GPU=1' as desktop GPU is currently not supported\nusername@DESKTOP-TMVLBJ1:~/mediapipe$ bazel run --define MEDIAPIPE_DISABLE_GPU=1 \\\n    mediapipe/examples/desktop/hello_world:hello_world\n\n# Should print:\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n# Hello World!\n```\n\n----------------------------------------\n\nTITLE: Generating MediaPipe Model Maker Requirements with pip-compile\nDESCRIPTION: Command used to generate the requirements file using pip-compile. It specifies the output file and input file, allowing unsafe packages.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/model_maker/requirements_lock.txt#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip-compile --allow-unsafe --output-file=mediapipe/opensource_only/model_maker_requirements_lock.txt mediapipe/opensource_only/model_maker_requirements_bazel.txt\n```\n\n----------------------------------------\n\nTITLE: Markdown Page Header Definition\nDESCRIPTION: YAML front matter defining page layout, target URL, title and navigation properties\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/models.md#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n---\nlayout: forward\ntarget: https://developers.google.com/mediapipe/solutions/guide#legacy\ntitle: Models and Model Cards\nparent: MediaPipe Legacy Solutions\nnav_order: 30\n---\n```\n\n----------------------------------------\n\nTITLE: Token to Integer Mapping in MediaPipe\nDESCRIPTION: A simple mapping table that associates token identifiers with integer values. This mapping could be used for enumeration, indexing, or reference purposes within the MediaPipe project.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/testdata/text/vocab_with_index.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntoken1 0\ntoken2 1\ntoken3 2\n```\n\n----------------------------------------\n\nTITLE: Configuring WORKSPACE for MacPorts OpenCV and FFmpeg\nDESCRIPTION: Configuration settings for the WORKSPACE file to specify MacPorts paths for OpenCV and FFmpeg libraries.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nnew_local_repository(\n    name = \"macos_opencv\",\n    build_file = \"@//third_party:opencv_macos.BUILD\",\n    path = \"/opt\",\n)\n\nnew_local_repository(\n    name = \"macos_ffmpeg\",\n    build_file = \"@//third_party:ffmpeg_macos.BUILD\",\n    path = \"/opt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring Multiple Classes in MediaPipe\nDESCRIPTION: This code snippet defines three classes: classA, classB, and classC. Without additional context, it's unclear what specific functionality these classes provide within the MediaPipe project. They may represent different components, algorithms, or data structures used in media processing pipelines.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tensor/testdata/labelmap.txt#2025-04-23_snippet_0\n\nLANGUAGE: Unknown\nCODE:\n```\nclassA\nclassB\nclassC\n```\n\n----------------------------------------\n\nTITLE: Cloning the MediaPipe Repository for iOS Development\nDESCRIPTION: Command to clone the MediaPipe repository from GitHub to your local machine.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/mediapipe.git\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCV Library in BUILD File for Linux\nDESCRIPTION: Configuration for the OpenCV cc_library rule in the opencv_linux.BUILD file, specifying library paths, header files, and include directories.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncc_library(\n    name = \"opencv\",\n    srcs = glob(\n        [\n            \"lib/libopencv_core.so\",\n            \"lib/libopencv_highgui.so\",\n            \"lib/libopencv_imgcodecs.so\",\n            \"lib/libopencv_imgproc.so\",\n            \"lib/libopencv_video.so\",\n            \"lib/libopencv_videoio.so\",\n        ],\n    ),\n    hdrs = glob([\n        # For OpenCV 3.x\n        \"include/opencv2/**/*.h*\",\n        # For OpenCV 4.x\n        # \"include/opencv4/opencv2/**/*.h*\",\n    ]),\n    includes = [\n        # For OpenCV 3.x\n        \"include/\",\n        # For OpenCV 4.x\n        # \"include/opencv4/\",\n    ],\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for MediaPipe iOS Development\nDESCRIPTION: Command to install the Python 'six' library, which is required for TensorFlow dependencies in MediaPipe.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/ios.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --user six\n```\n\n----------------------------------------\n\nTITLE: Configuring Android Manifest\nDESCRIPTION: Android manifest configuration with basic app settings and main activity declaration.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.google.mediapipe.apps.basic\">\n\n  <uses-sdk\n      android:minSdkVersion=\"19\"\n      android:targetSdkVersion=\"34\" />\n\n  <application\n      android:allowBackup=\"true\"\n      android:label=\"${appName}\"\n      android:supportsRtl=\"true\"\n      android:theme=\"@style/AppTheme\">\n      <activity\n          android:name=\"${mainActivity}\"\n          android:exported=\"true\"\n          android:screenOrientation=\"portrait\">\n          <intent-filter>\n              <action android:name=\"android.intent.action.MAIN\" />\n              <category android:name=\"android.intent.category.LAUNCHER\" />\n          </intent-filter>\n      </activity>\n  </application>\n\n</manifest>\n```\n\n----------------------------------------\n\nTITLE: Defining Android App Styles\nDESCRIPTION: XML resource file defining the application's theme and styles.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<resources>\n\n    <!-- Base application theme. -->\n    <style name=\"AppTheme\" parent=\"Theme.AppCompat.Light.DarkActionBar\">\n        <!-- Customize your theme here. -->\n        <item name=\"colorPrimary\">@color/colorPrimary</item>\n        <item name=\"colorPrimaryDark\">@color/colorPrimaryDark</item>\n        <item name=\"colorAccent\">@color/colorAccent</item>\n    </style>\n\n</resources>\n```\n\n----------------------------------------\n\nTITLE: Generating MediaPipe Requirements Lock File with pip-compile\nDESCRIPTION: This command generates the requirements lock file for MediaPipe using pip-compile. It specifies the Python version and input/output files for the compilation process.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/requirements_lock_3_11.txt#2025-04-23_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n#    pip-compile --output-file=mediapipe/opensource_only/requirements_lock_3_11.txt mediapipe/opensource_only/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running Profile Reporter Tests\nDESCRIPTION: Command to run the reporter test suite using Bazel\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/profiler/reporter/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbazel test :reporter_test\n```\n\n----------------------------------------\n\nTITLE: Filtering Profile Columns\nDESCRIPTION: Example command showing how to filter specific columns (time and total) and specify log files for analysis\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/framework/profiler/reporter/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbazel run :print_profile -- --cols \"*time*,*total*\" --logfiles \"<path-to-log>,<path-to-another-log>\"\n```\n\n----------------------------------------\n\nTITLE: Processing CSV Data Points in MediaPipe\nDESCRIPTION: Raw data output containing 4 columns of floating point values. Each row appears to contain a confidence score (first column), coordinate values (second and third columns), and a threshold or measurement value (fourth column).\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/testdata/metadata/score_calibration.txt#2025-04-23_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\n0.7237641215324402,0.2756437361240387,-0.10612351447343826,0.019999999552965164\n0.9793540239334106,0.5117573738098145,0.8033715486526489,0.01953125\n0.9825188517570496,0.3965616822242737,0.17742407321929932,0.019999999552965164\n...\n```\n\n----------------------------------------\n\nTITLE: Initializing Object Detector with MediaPipe\nDESCRIPTION: This code demonstrates how to initialize an Object Detector using MediaPipe Tasks Vision. It loads the vision tasks, creates an object detector from a model path, and performs object detection on an image element. The detector returns bounding boxes and classifications for detected objects.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/tasks/web/vision/README.md#2025-04-23_snippet_10\n\nLANGUAGE: TypeScript\nCODE:\n```\nconst vision = await FilesetResolver.forVisionTasks(\n    \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm\"\n);\nconst objectDetector = await ObjectDetector.createFromModelPath(vision,\n    \"https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/float16/1/efficientdet_lite0.tflite\"\n);\nconst image = document.getElementById(\"image\") as HTMLImageElement;\nconst detections = objectDetector.detect(image);\n```\n\n----------------------------------------\n\nTITLE: Markdown TOC Structure\nDESCRIPTION: Basic table of contents structure using markdown syntax\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/models.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. TOC\n{:toc}\n```\n\n----------------------------------------\n\nTITLE: Running MediaPipe Hands on GPU in C++\nDESCRIPTION: Command to run the built MediaPipe Hands application with GPU acceleration. It configures logging to stderr and uses a GPU-specific calculator graph configuration file for desktop live hand tracking.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/cpp.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/hand_tracking/hand_tracking_gpu \\\n  --calculator_graph_config_file=mediapipe/graphs/hand_tracking/hand_tracking_desktop_live_gpu.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents in Markdown\nDESCRIPTION: A markdown snippet that creates a table of contents using Jekyll's TOC functionality. The first line specifies the TOC marker, and the second line applies the toc formatting.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/tools/tools.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. TOC\n{:toc}\n```\n\n----------------------------------------\n\nTITLE: Defining Classes in MediaPipe\nDESCRIPTION: This code snippet defines three classes: classA, classB, and classC. The classes are likely part of a larger MediaPipe project structure, potentially representing different components or modules within the framework.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/calculators/tflite/testdata/labelmap.txt#2025-04-23_snippet_0\n\nLANGUAGE: Unknown\nCODE:\n```\nclassA\nclassB\nclassC\n```\n\n----------------------------------------\n\nTITLE: Setting Android Manifest Values for MediaPipe Graph Configuration\nDESCRIPTION: Defining manifest values to specify the MediaPipe graph configuration including graph name and stream names.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_26\n\nLANGUAGE: Bazel\nCODE:\n```\nmanifest_values = {\n    \"applicationId\": \"com.google.mediapipe.apps.basic\",\n    \"appName\": \"Hello World\",\n    \"mainActivity\": \".MainActivity\",\n    \"cameraFacingFront\": \"False\",\n    \"binaryGraphName\": \"mobile_gpu.binarypb\",\n    \"inputVideoStreamName\": \"input_video\",\n    \"outputVideoStreamName\": \"output_video\",\n},\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository in Bash\nDESCRIPTION: Commands to clone the MediaPipe repository from GitHub and navigate to the project directory. This is the first step in setting up MediaPipe on your local machine.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/install.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd $HOME\n$ git clone --depth 1 https://github.com/google/mediapipe.git\n\n# Change directory into MediaPipe root directory\n$ cd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Loading Native Libraries in MainActivity\nDESCRIPTION: Loading the required native libraries (MediaPipe JNI and OpenCV) in the MainActivity static block.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_27\n\nLANGUAGE: Java\nCODE:\n```\nstatic {\n  // Load all native libraries needed by the app.\n  System.loadLibrary(\"mediapipe_jni\");\n  System.loadLibrary(\"opencv_java4\");\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository in Bash\nDESCRIPTION: Commands to clone the MediaPipe repository and navigate to its directory.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/youtube_8m.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/mediapipe.git\ncd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Cloning MediaPipe Repository\nDESCRIPTION: Commands to clone the MediaPipe repository and navigate to its directory.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/mediapipe/examples/desktop/youtube8m/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/mediapipe.git\ncd mediapipe\n```\n\n----------------------------------------\n\nTITLE: Initializing EglManager in onCreate Method\nDESCRIPTION: Initializing the EglManager object in the onCreate method to manage the OpenGL context for texture conversion.\nSOURCE: https://github.com/google-ai-edge/mediapipe/blob/master/docs/getting_started/hello_world_android.md#2025-04-23_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\neglManager = new EglManager(null);\n```"
  }
]
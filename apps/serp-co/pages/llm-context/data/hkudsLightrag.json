[
  {
    "owner": "hkuds",
    "repo": "lightrag",
    "content": "TITLE: Configuring Faiss Vector Storage in Python\nDESCRIPTION: Shows how to configure LightRAG to use Faiss as the vector database storage. Includes defining an embedding function and initializing LightRAG with FaissVectorDBStorage.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(texts, convert_to_numpy=True)\n    return embeddings\n\n# Initialize LightRAG with the LLM model function and embedding function\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=8192,\n        func=embedding_func,\n    ),\n    vector_storage=\"FaissVectorDBStorage\",\n    vector_db_storage_cls_kwargs={\n        \"cosine_better_than_threshold\": 0.3  # Your desired threshold\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing LightRAG with OpenAI-like API Integration\nDESCRIPTION: This code demonstrates how to set up LightRAG with OpenAI-compatible APIs, including custom LLM completion and embedding functions. It initializes the necessary storage components and prepares the RAG system for document processing.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs\n    )\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"solar-embedding-1-large-query\",\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\"\n    )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=4096,\n            max_token_size=8192,\n            func=embedding_func\n        )\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n```\n\n----------------------------------------\n\nTITLE: Integrating LightRAG with LlamaIndex\nDESCRIPTION: Complete example of initializing and using LightRAG with LlamaIndex integration, demonstrating how to set up the RAG system and perform different types of searches (naive, local, global, hybrid).\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# 使用LlamaIndex直接访问OpenAI\nimport asyncio\nfrom lightrag import LightRAG\nfrom lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger\n\n# 为LightRAG设置日志处理程序\nsetup_logger(\"lightrag\", level=\"INFO\")\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=\"your/path\",\n        llm_model_func=llama_index_complete_if_cache,  # LlamaIndex兼容的完成函数\n        embedding_func=EmbeddingFunc(    # LlamaIndex兼容的嵌入函数\n            embedding_dim=1536,\n            max_token_size=8192,\n            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)\n        ),\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n\ndef main():\n    # 初始化RAG实例\n    rag = asyncio.run(initialize_rag())\n\n    with open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n        rag.insert(f.read())\n\n    # 执行朴素搜索\n    print(\n        rag.query(\"这个故事的主要主题是什么？\", param=QueryParam(mode=\"naive\"))\n    )\n\n    # 执行本地搜索\n    print(\n        rag.query(\"这个故事的主要主题是什么？\", param=QueryParam(mode=\"local\"))\n    )\n\n    # 执行全局搜索\n    print(\n        rag.query(\"这个故事的主要主题是什么？\", param=QueryParam(mode=\"global\"))\n    )\n\n    # 执行混合搜索\n    print(\n        rag.query(\"这个故事的主要主题是什么？\", param=QueryParam(mode=\"hybrid\"))\n    )\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Integrating LightRAG with OpenAI-compatible API\nDESCRIPTION: Example of initializing LightRAG with functions that use OpenAI-compatible APIs for LLM completion and embedding. The example shows how to structure the required async functions and pass them to the LightRAG initialization.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasync def llm_model_func(\n    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n) -> str:\n    return await openai_complete_if_cache(\n        \"solar-mini\",\n        prompt,\n        system_prompt=system_prompt,\n        history_messages=history_messages,\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\",\n        **kwargs\n    )\n\nasync def embedding_func(texts: list[str]) -> np.ndarray:\n    return await openai_embed(\n        texts,\n        model=\"solar-embedding-1-large-query\",\n        api_key=os.getenv(\"UPSTAGE_API_KEY\"),\n        base_url=\"https://api.upstage.ai/v1/solar\"\n    )\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=llm_model_func,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=4096,\n            max_token_size=8192,\n            func=embedding_func\n        )\n    )\n\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n\n    return rag\n```\n\n----------------------------------------\n\nTITLE: LightRAG Core Basic Usage Example\nDESCRIPTION: A complete Python script demonstrating how to initialize LightRAG, insert text, and perform queries using the Core functionality. It includes setting up the logger, initializing storage, and executing a hybrid search query.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed\nfrom lightrag.kg.shared_storage import initialize_pipeline_status\nfrom lightrag.utils import setup_logger\n\nsetup_logger(\"lightrag\", level=\"INFO\")\n\nWORKING_DIR = \"./rag_storage\"\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        embedding_func=openai_embed,\n        llm_model_func=gpt_4o_mini_complete,\n    )\n    await rag.initialize_storages()\n    await initialize_pipeline_status()\n    return rag\n\nasync def main():\n    try:\n        # Initialize RAG instance\n        rag = await initialize_rag()\n        rag.insert(\"Your text\")\n\n        # Perform hybrid search\n        mode=\"hybrid\"\n        print(\n          await rag.query(\n              \"What are the top themes in this story?\",\n              param=QueryParam(mode=mode)\n          )\n        )\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        if rag:\n            await rag.finalize_storages()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Using TokenTracker with Context Manager for Automatic Token Tracking in Python\nDESCRIPTION: This snippet demonstrates how to use TokenTracker with a context manager to automatically track token usage across multiple LLM calls. The context manager approach is recommended for scenarios requiring automatic token usage tracking.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom lightrag.utils import TokenTracker\n\n# Create TokenTracker instance\ntoken_tracker = TokenTracker()\n\n# Method 1: Using context manager (Recommended)\n# Suitable for scenarios requiring automatic token usage tracking\nwith token_tracker:\n    result1 = await llm_model_func(\"your question 1\")\n    result2 = await llm_model_func(\"your question 2\")\n```\n\n----------------------------------------\n\nTITLE: Configuring LightRAG with Hugging Face Models\nDESCRIPTION: This code shows how to initialize LightRAG using Hugging Face models for both text generation and embeddings. It specifies the model names and embedding dimensions required for the RAG system.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Initialize LightRAG with Hugging Face model\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation\n    llm_model_name='meta-llama/Llama-3.1-8B-Instruct',  # Model name from Hugging Face\n    # Use Hugging Face embedding function\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embed(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\"),\n            embed_model=AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n        )\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Prompts with LightRAG\nDESCRIPTION: Examples demonstrating how to use custom prompts with LightRAG to control system behavior. Shows both default prompt usage and a custom prompt that provides specific formatting instructions for environmental science responses.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# 创建查询参数\nquery_param = QueryParam(\n    mode=\"hybrid\",  # 或其他模式：\"local\"、\"global\"、\"hybrid\"、\"mix\"和\"naive\"\n)\n\n# 示例1：使用默认系统提示\nresponse_default = rag.query(\n    \"可再生能源的主要好处是什么？\",\n    param=query_param\n)\nprint(response_default)\n\n# 示例2：使用自定义提示\ncustom_prompt = \"\"\"\n您是环境科学领域的专家助手。请提供详细且结构化的答案，并附带示例。\n---对话历史---\n{history}\n\n---知识库---\n{context_data}\n\n---响应规则---\n\n- 目标格式和长度：{response_type}\n\"\"\"\nresponse_custom = rag.query(\n    \"可再生能源的主要好处是什么？\",\n    param=query_param,\n    system_prompt=custom_prompt  # 传递自定义提示\n)\nprint(response_custom)\n```\n\n----------------------------------------\n\nTITLE: Setting Up LightRAG with Ollama Models\nDESCRIPTION: Example demonstrating how to initialize LightRAG with Ollama models for text generation and embeddings. Includes the basic setup as well as configuration for increasing context size and handling low RAM GPUs.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 使用Ollama模型初始化LightRAG\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,  # 使用Ollama模型进行文本生成\n    llm_model_name='your_model_name', # 您的模型名称\n    # 使用Ollama嵌入函数\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embed(\n            texts,\n            embed_model=\"nomic-embed-text\"\n        )\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing LightRAG with Direct OpenAI Access via LlamaIndex\nDESCRIPTION: Python code showing how to initialize LightRAG with direct OpenAI access using LlamaIndex. Includes implementation of the LLM model function and embedding configuration.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/llm/Readme.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightrag import LightRAG\nfrom lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom lightrag.utils import EmbeddingFunc\n\n# Initialize with direct OpenAI access\nasync def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n    try:\n        # Initialize OpenAI if not in kwargs\n        if 'llm_instance' not in kwargs:\n            llm_instance = OpenAI(\n                model=\"gpt-4\",\n                api_key=\"your-openai-key\",\n                temperature=0.7,\n            )\n            kwargs['llm_instance'] = llm_instance\n\n        response = await llama_index_complete_if_cache(\n            kwargs['llm_instance'],\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            **kwargs,\n        )\n        return response\n    except Exception as e:\n        logger.error(f\"LLM request failed: {str(e)}\")\n        raise\n\n# Initialize LightRAG with OpenAI\nrag = LightRAG(\n    working_dir=\"your/path\",\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1536,\n        max_token_size=8192,\n        func=lambda texts: llama_index_embed(\n            texts,\n            embed_model=OpenAIEmbedding(\n                model=\"text-embedding-3-large\",\n                api_key=\"your-openai-key\"\n            )\n        ),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing LightRAG with Hugging Face Models\nDESCRIPTION: Example showing how to set up LightRAG with Hugging Face models for both LLM completion and embeddings. The code references an implementation from 'lightrag_hf_demo.py' and demonstrates the configuration structure.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# 使用Hugging Face模型初始化LightRAG\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=hf_model_complete,  # 使用Hugging Face模型进行文本生成\n    llm_model_name='meta-llama/Llama-3.1-8B-Instruct',  # Hugging Face的模型名称\n    # 使用Hugging Face嵌入函数\n    embedding_func=EmbeddingFunc(\n        embedding_dim=384,\n        max_token_size=5000,\n        func=lambda texts: hf_embed(\n            texts,\n            tokenizer=AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\"),\n            embed_model=AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n        )\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Defining QueryParam Class for LightRAG Retrieval Control\nDESCRIPTION: This class defines the parameters that control how LightRAG performs retrievals. It includes settings for retrieval mode, context preferences, token limits, and model function override capabilities.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass QueryParam:\n    mode: Literal[\"local\", \"global\", \"hybrid\", \"naive\", \"mix\"] = \"global\"\n    \"\"\"Specifies the retrieval mode:\n    - \"local\": Focuses on context-dependent information.\n    - \"global\": Utilizes global knowledge.\n    - \"hybrid\": Combines local and global retrieval methods.\n    - \"naive\": Performs a basic search without advanced techniques.\n    - \"mix\": Integrates knowledge graph and vector retrieval. Mix mode combines knowledge graph and vector search:\n        - Uses both structured (KG) and unstructured (vector) information\n        - Provides comprehensive answers by analyzing relationships and context\n        - Supports image content through HTML img tags\n        - Allows control over retrieval depth via top_k parameter\n    \"\"\"\n    only_need_context: bool = False\n    \"\"\"If True, only returns the retrieved context without generating a response.\"\"\"\n    response_type: str = \"Multiple Paragraphs\"\n    \"\"\"Defines the response format. Examples: 'Multiple Paragraphs', 'Single Paragraph', 'Bullet Points'.\"\"\"\n    top_k: int = 60\n    \"\"\"Number of top items to retrieve. Represents entities in 'local' mode and relationships in 'global' mode.\"\"\"\n    max_token_for_text_unit: int = 4000\n    \"\"\"Maximum number of tokens allowed for each retrieved text chunk.\"\"\"\n    max_token_for_global_context: int = 4000\n    \"\"\"Maximum number of tokens allocated for relationship descriptions in global retrieval.\"\"\"\n    max_token_for_local_context: int = 4000\n    \"\"\"Maximum number of tokens allocated for entity descriptions in local retrieval.\"\"\"\n    ids: list[str] | None = None # ONLY SUPPORTED FOR PG VECTOR DBs\n    \"\"\"List of ids to filter the RAG.\"\"\"\n    model_func: Callable[..., object] | None = None\n    \"\"\"Optional override for the LLM model function to use for this specific query.\n    If provided, this will be used instead of the global model function.\n    This allows using different models for different query modes.\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing LightRAG with LiteLLM Proxy via LlamaIndex\nDESCRIPTION: Python code showing how to initialize LightRAG using a LiteLLM proxy with LlamaIndex. Includes implementation of the LLM model function and embedding configuration for proxy access.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/llm/Readme.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightrag import LightRAG\nfrom lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed\nfrom llama_index.llms.litellm import LiteLLM\nfrom llama_index.embeddings.litellm import LiteLLMEmbedding\nfrom lightrag.utils import EmbeddingFunc\n\n# Initialize with LiteLLM proxy\nasync def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):\n    try:\n        # Initialize LiteLLM if not in kwargs\n        if 'llm_instance' not in kwargs:\n            llm_instance = LiteLLM(\n                model=f\"openai/{settings.LLM_MODEL}\",  # Format: \"provider/model_name\"\n                api_base=settings.LITELLM_URL,\n                api_key=settings.LITELLM_KEY,\n                temperature=0.7,\n            )\n            kwargs['llm_instance'] = llm_instance\n\n        response = await llama_index_complete_if_cache(\n            kwargs['llm_instance'],\n            prompt,\n            system_prompt=system_prompt,\n            history_messages=history_messages,\n            **kwargs,\n        )\n        return response\n    except Exception as e:\n        logger.error(f\"LLM request failed: {str(e)}\")\n        raise\n\n# Initialize LightRAG with LiteLLM\nrag = LightRAG(\n    working_dir=\"your/path\",\n    llm_model_func=llm_model_func,\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1536,\n        max_token_size=8192,\n        func=lambda texts: llama_index_embed(\n            texts,\n            embed_model=LiteLLMEmbedding(\n                model_name=f\"openai/{settings.EMBEDDING_MODEL}\",\n                api_base=settings.LITELLM_URL,\n                api_key=settings.LITELLM_KEY,\n            )\n        ),\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Neo4J Storage in Python\nDESCRIPTION: Demonstrates how to configure LightRAG to use Neo4J as the knowledge graph storage backend. Includes environment variable setup and initialization with the Neo4JStorage implementation.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nexport NEO4J_URI=\"neo4j://localhost:7687\"\nexport NEO4J_USERNAME=\"neo4j\"\nexport NEO4J_PASSWORD=\"password\"\n\n# Setup logger for LightRAG\nsetup_logger(\"lightrag\", level=\"INFO\")\n\n# When you launch the project be sure to override the default KG: NetworkX\n# by specifying kg=\"Neo4JStorage\".\n\n# Note: Default settings use NetworkX\n# Initialize LightRAG with Neo4J implementation.\nasync def initialize_rag():\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model\n        graph_storage=\"Neo4JStorage\", #<-----------override KG default\n    )\n\n    # Initialize database connections\n    await rag.initialize_storages()\n    # Initialize pipeline status for document processing\n    await initialize_pipeline_status()\n\n    return rag\n```\n\n----------------------------------------\n\nTITLE: Running LightRAG Server with OpenAI Backend\nDESCRIPTION: This snippet demonstrates how to run the LightRAG server using OpenAI as the backend for LLM and embedding. It requires specific configuration in the .env or config.ini file.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# 使用 openai 运行 lightrag，llm 使用 GPT-4o-mini，嵌入使用 text-embedding-3-small\n# 在 .env 或 config.ini 中配置：\n# LLM_BINDING=openai\n# LLM_MODEL=GPT-4o-mini\n# EMBEDDING_BINDING=openai\n# EMBEDDING_MODEL=text-embedding-3-small\nlightrag-server\n\n# 使用认证密钥\nlightrag-server --key my-key\n```\n\n----------------------------------------\n\nTITLE: Building and Starting Docker Container for LightRAG\nDESCRIPTION: Command to build and start the Docker container for LightRAG using docker-compose. This works on all platforms with Docker Desktop installed.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose up -d\n```\n\n----------------------------------------\n\nTITLE: Complete LightRAG Environment Configuration Example\nDESCRIPTION: Comprehensive example of a .env file for LightRAG configuration, including server settings, document indexing parameters, LLM and embedding configurations, and authentication options.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n### Server Configuration\n# HOST=0.0.0.0\nPORT=9621\nWORKERS=2\n\n### Settings for document indexing\nENABLE_LLM_CACHE_FOR_EXTRACT=true\nSUMMARY_LANGUAGE=Chinese\nMAX_PARALLEL_INSERT=2\n\n### LLM Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nTIMEOUT=200\nTEMPERATURE=0.0\nMAX_ASYNC=4\nMAX_TOKENS=32768\n\nLLM_BINDING=openai\nLLM_MODEL=gpt-4o-mini\nLLM_BINDING_HOST=https://api.openai.com/v1\nLLM_BINDING_API_KEY=your-api-key\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\n\n### For JWT Auth\n# AUTH_ACCOUNTS='admin:admin123,user1:pass456'\n# TOKEN_SECRET=your-key-for-LightRAG-API-Server-xxx\n```\n\n----------------------------------------\n\nTITLE: Running LightRAG OpenAI Demo\nDESCRIPTION: A series of bash commands to set up and run the LightRAG demo with OpenAI integration. This includes setting the API key, downloading a sample document, and running the demo script.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n### you should run the demo code with project folder\ncd LightRAG\n### provide your API-KEY for OpenAI\nexport OPENAI_API_KEY=\"sk-...your_opeai_key...\"\n### download the demo document of \"A Christmas Carol\" by Charles Dickens\ncurl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt > ./book.txt\n### run the demo code\npython examples/lightrag_openai_demo.py\n```\n\n----------------------------------------\n\nTITLE: Running LightRAG Server with Ollama Backend\nDESCRIPTION: This snippet demonstrates how to run the LightRAG server using Ollama as the default backend for LLM and embedding. It includes examples with and without an authentication key.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# 使用 ollama 运行 lightrag，llm 使用 mistral-nemo:latest，嵌入使用 bge-m3:latest\nlightrag-server\n\n# 使用认证密钥\nlightrag-server --key my-key\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LightRAG API Server\nDESCRIPTION: Bash commands to set up environment variables for configuring the LightRAG API server, including index directory, API keys, and model selections.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport RAG_DIR=\"your_index_directory\"  # Optional: Defaults to \"index_default\"\nexport OPENAI_BASE_URL=\"Your OpenAI API base URL\"  # Optional: Defaults to \"https://api.openai.com/v1\"\nexport OPENAI_API_KEY=\"Your OpenAI API key\"  # Required\nexport LLM_MODEL=\"Your LLM model\" # Optional: Defaults to \"gpt-4o-mini\"\nexport EMBEDDING_MODEL=\"Your embedding model\" # Optional: Defaults to \"text-embedding-3-large\"\n```\n\n----------------------------------------\n\nTITLE: Enqueuing Documents for RAG Processing in Python\nDESCRIPTION: Demonstrates how to enqueue documents for processing in a LightRAG pipeline. This is typically used in a loop to process multiple input documents.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nawait rag.apipeline_process_enqueue_documents(input)\n```\n\n----------------------------------------\n\nTITLE: Uploading Single File to RAG\nDESCRIPTION: Upload a single file to the RAG system with optional description using multipart form data\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/documents/file\" \\\n    -F \"file=@/path/to/your/document.txt\" \\\n    -F \"description=Optional description\"\n```\n\n----------------------------------------\n\nTITLE: Creating Entities and Relations in Knowledge Graph\nDESCRIPTION: Demonstrates how to create new entities and establish relationships between them in the knowledge graph. This allows for building a custom knowledge structure.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Create new entity\nentity = rag.create_entity(\"Google\", {\n    \"description\": \"Google is a multinational technology company specializing in internet-related services and products.\",\n    \"entity_type\": \"company\"\n})\n\n# Create another entity\nproduct = rag.create_entity(\"Gmail\", {\n    \"description\": \"Gmail is an email service developed by Google.\",\n    \"entity_type\": \"product\"\n})\n\n# Create relation between entities\nrelation = rag.create_relation(\"Google\", \"Gmail\", {\n    \"description\": \"Google develops and operates Gmail.\",\n    \"keywords\": \"develops operates service\",\n    \"weight\": 2.0\n})\n```\n\n----------------------------------------\n\nTITLE: Checking Server Health\nDESCRIPTION: Check the health status and configuration of the server\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://localhost:9621/health\"\n```\n\n----------------------------------------\n\nTITLE: Batch File Upload to RAG\nDESCRIPTION: Upload multiple files simultaneously to the RAG system\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/documents/batch\" \\\n    -F \"files=@/path/to/doc1.txt\" \\\n    -F \"files=@/path/to/doc2.txt\"\n```\n\n----------------------------------------\n\nTITLE: Batch Insertion with LightRAG\nDESCRIPTION: Examples showing how to perform batch insertion of multiple documents into LightRAG, including configuration of parallel processing limits for document indexing.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# 基本批量插入：一次插入多个文本\nrag.insert([\"文本1\", \"文本2\",...])\n\n# 带有自定义批量大小配置的批量插入\nrag = LightRAG(\n    ...\n    working_dir=WORKING_DIR,\n    max_parallel_insert = 4\n)\n\nrag.insert([\"文本1\", \"文本2\", \"文本3\", ...])  # 文档将以4个为一批进行处理\n```\n\n----------------------------------------\n\nTITLE: Inserting Text into LightRAG\nDESCRIPTION: JSON structure for the insert text endpoint request body, containing the text content to be inserted into the RAG system.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"Your text content here\"\n}\n```\n\n----------------------------------------\n\nTITLE: Inserting Contexts into LightRAG\nDESCRIPTION: Function to insert extracted contexts into the LightRAG system with retry logic. Handles insertion failures with up to 3 retry attempts.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef insert_text(rag, file_path):\n    with open(file_path, mode='r') as f:\n        unique_contexts = json.load(f)\n\n    retries = 0\n    max_retries = 3\n    while retries < max_retries:\n        try:\n            rag.insert(unique_contexts)\n            break\n        except Exception as e:\n            retries += 1\n            print(f\"Insertion failed, retrying ({retries}/{max_retries}), error: {e}\")\n            time.sleep(10)\n    if retries == max_retries:\n        print(\"Insertion failed after exceeding the maximum number of retries\")\n```\n\n----------------------------------------\n\nTITLE: Sending Query Request to LightRAG API\nDESCRIPTION: cURL command example for sending a POST request to the query endpoint of the LightRAG API server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://127.0.0.1:8020/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"What are the main themes?\", \"mode\": \"hybrid\"}'\n```\n\n----------------------------------------\n\nTITLE: Inserting Contexts into LightRAG System with Retry Logic in Python\nDESCRIPTION: This function inserts extracted contexts into the LightRAG system. It includes retry logic to handle potential insertion failures, with a maximum of 3 retries and a 10-second delay between attempts.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef insert_text(rag, file_path):\n    with open(file_path, mode='r') as f:\n        unique_contexts = json.load(f)\n\n    retries = 0\n    max_retries = 3\n    while retries < max_retries:\n        try:\n            rag.insert(unique_contexts)\n            break\n        except Exception as e:\n            retries += 1\n            print(f\"插入失败，重试（{retries}/{max_retries}），错误：{e}\")\n            time.sleep(10)\n    if retries == max_retries:\n        print(\"超过最大重试次数后插入失败\")\n```\n\n----------------------------------------\n\nTITLE: Running LightRAG Server with Azure OpenAI Backend\nDESCRIPTION: This snippet shows how to run the LightRAG server using Azure OpenAI as the backend for LLM and embedding. It requires specific configuration in the .env or config.ini file.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# 使用 azure_openai 运行 lightrag\n# 在 .env 或 config.ini 中配置：\n# LLM_BINDING=azure_openai\n# LLM_MODEL=your-model\n# EMBEDDING_BINDING=azure_openai\n# EMBEDDING_MODEL=your-embedding-model\nlightrag-server\n\n# 使用认证密钥\nlightrag-server --key my-key\n```\n\n----------------------------------------\n\nTITLE: Entity Merging with Custom Merge Strategy in LightRAG\nDESCRIPTION: This code shows how to merge entities with a custom merge strategy for different fields. It allows control over how specific attributes are combined during the entity merging process, with options like concatenating texts or keeping values from specific entities.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Define custom merge strategy for different fields\nrag.merge_entities(\n    source_entities=[\"John Smith\", \"Dr. Smith\", \"J. Smith\"],\n    target_entity=\"John Smith\",\n    merge_strategy={\n        \"description\": \"concatenate\",  # Combine all descriptions\n        \"entity_type\": \"keep_first\",   # Keep the entity type from the first entity\n        \"source_id\": \"join_unique\"     # Combine all unique source IDs\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Query Endpoint Request Body (JSON)\nDESCRIPTION: This JSON payload represents the request body for the `/query` endpoint.  The `query` parameter specifies the question to be answered. The `mode` parameter defines the RAG retrieval mode (naive, local, global, or hybrid). The `only_need_context` parameter controls whether the LLM answer or just the referenced context is returned. Defaults to false.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"query\": \"Your question here\",\n    \"mode\": \"hybrid\",  // Can be \"naive\", \"local\", \"global\", or \"hybrid\"\n    \"only_need_context\": true // Optional: Defaults to false, if true, only the referenced context will be returned, otherwise the llm answer will be returned\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI LLM with Ollama Embedding\nDESCRIPTION: Environment variable configuration for using OpenAI as the LLM provider and Ollama for embeddings in LightRAG.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nLLM_BINDING=openai\nLLM_MODEL=gpt-4o\nLLM_BINDING_HOST=https://api.openai.com/v1\nLLM_BINDING_API_KEY=your_api_key\n### Max tokens sent to LLM (less than model context size)\nMAX_TOKENS=32768\n\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\n# EMBEDDING_BINDING_API_KEY=your_api_key\n```\n\n----------------------------------------\n\nTITLE: Basic Entity Merging in LightRAG\nDESCRIPTION: This snippet demonstrates how to merge multiple related entities into a single entity in LightRAG. The system automatically handles all relationships between the merged entities, combining them under the target entity.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Basic entity merging\nrag.merge_entities(\n    source_entities=[\"Artificial Intelligence\", \"AI\", \"Machine Intelligence\"],\n    target_entity=\"AI Technology\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring LightRAG API Key Authentication\nDESCRIPTION: Environment variables for setting up API key authentication and path whitelisting in LightRAG Server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nLIGHTRAG_API_KEY=your-secure-api-key-here\nWHITELIST_PATHS=/health,/api/*\n```\n\n----------------------------------------\n\nTITLE: Including Vector Embeddings in LightRAG Data Export\nDESCRIPTION: This snippet shows how to include vector embeddings when exporting data from LightRAG. This option is useful when the exported data needs to retain the vector representations for further analysis or processing.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nrag.export_data(\"complete_data.csv\", include_vector_data=True)\n```\n\n----------------------------------------\n\nTITLE: Using Conversation History with LightRAG\nDESCRIPTION: Example showing how to add conversation history support to enable multi-turn dialogues with LightRAG. The code demonstrates creating a conversation history array and passing it to the query parameters.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# 创建对话历史\nconversation_history = [\n    {\"role\": \"user\", \"content\": \"主角对圣诞节的态度是什么？\"},\n    {\"role\": \"assistant\", \"content\": \"在故事开始时，埃比尼泽·斯克鲁奇对圣诞节持非常消极的态度...\"},\n    {\"role\": \"user\", \"content\": \"他的态度是如何改变的？\"}\n]\n\n# 创建带有对话历史的查询参数\nquery_param = QueryParam(\n    mode=\"mix\",  # 或其他模式：\"local\"、\"global\"、\"hybrid\"\n    conversation_history=conversation_history,  # 添加对话历史\n    history_turns=3  # 考虑最近的对话轮数\n)\n\n# 进行考虑对话历史的查询\nresponse = rag.query(\n    \"是什么导致了他性格的这种变化？\",\n    param=query_param\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting LightRAG Data in Different File Formats\nDESCRIPTION: This snippet demonstrates how to export knowledge graph data from LightRAG in various file formats including CSV, Excel, Markdown, and Text. It shows the flexibility of the export system to accommodate different output needs.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#Export data in CSV format\nrag.export_data(\"graph_data.csv\", file_format=\"csv\")\n\n# Export data in Excel sheet\nrag.export_data(\"graph_data.xlsx\", file_format=\"excel\")\n\n# Export data in markdown format\nrag.export_data(\"graph_data.md\", file_format=\"md\")\n\n# Export data in Text\nrag.export_data(\"graph_data.txt\", file_format=\"txt\")\n```\n\n----------------------------------------\n\nTITLE: Sending Insert Text Request to LightRAG API\nDESCRIPTION: cURL command example for sending a POST request to insert text into the LightRAG system via the API.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://127.0.0.1:8020/insert\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"text\": \"Content to be inserted into RAG\"}'\n```\n\n----------------------------------------\n\nTITLE: Editing Entities and Relations in Knowledge Graph\nDESCRIPTION: Shows how to edit existing entities and relationships in the knowledge graph, including updating attributes, renaming entities, and modifying relationship properties.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Edit an existing entity\nupdated_entity = rag.edit_entity(\"Google\", {\n    \"description\": \"Google is a subsidiary of Alphabet Inc., founded in 1998.\",\n    \"entity_type\": \"tech_company\"\n})\n\n# Rename an entity (with all its relationships properly migrated)\nrenamed_entity = rag.edit_entity(\"Gmail\", {\n    \"entity_name\": \"Google Mail\",\n    \"description\": \"Google Mail (formerly Gmail) is an email service.\"\n})\n\n# Edit a relation between entities\nupdated_relation = rag.edit_relation(\"Google\", \"Google Mail\", {\n    \"description\": \"Google created and maintains Google Mail service.\",\n    \"keywords\": \"creates maintains email service\",\n    \"weight\": 3.0\n})\n```\n\n----------------------------------------\n\nTITLE: Processing Multi-file Types with Textract in Python\nDESCRIPTION: Shows how to extract text from different file formats like PDF, DOCX, PPTX, CSV, and TXT using the textract library and insert the content into the RAG system.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport textract\n\nfile_path = 'TEXT.pdf'\ntext_content = textract.process(file_path)\n\nrag.insert(text_content.decode('utf-8'))\n```\n\n----------------------------------------\n\nTITLE: Insert Text Endpoint Request Body (JSON)\nDESCRIPTION: This JSON payload represents the request body for the `/insert` endpoint. The `text` parameter specifies the text content to be inserted into the RAG system.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text\": \"Your text content here\"\n}\n```\n\n----------------------------------------\n\nTITLE: Using Citation Functionality in Python\nDESCRIPTION: Shows how to insert documents with file path information to enable citation functionality. This ensures that sources can be traced back to their original documents.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Define documents and their file paths\ndocuments = [\"Document content 1\", \"Document content 2\"]\nfile_paths = [\"path/to/doc1.txt\", \"path/to/doc2.txt\"]\n\n# Insert documents with file paths\nrag.insert(documents, file_paths=file_paths)\n```\n\n----------------------------------------\n\nTITLE: Starting LightRAG Server with Gunicorn (Production Mode)\nDESCRIPTION: Command to start LightRAG Server in production mode using Gunicorn with multiple worker processes.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nlightrag-gunicorn --workers 4\n```\n\n----------------------------------------\n\nTITLE: Advanced Entity Merging Combining Strategy and Custom Data in LightRAG\nDESCRIPTION: This code demonstrates an advanced entity merging approach that combines both a merge strategy for specific fields and custom data for other fields. This provides maximum flexibility in controlling how entities are merged and what the final entity looks like.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Merge company entities with both strategy and custom data\nrag.merge_entities(\n    source_entities=[\"Microsoft Corp\", \"Microsoft Corporation\", \"MSFT\"],\n    target_entity=\"Microsoft\",\n    merge_strategy={\n        \"description\": \"concatenate\",  # Combine all descriptions\n        \"source_id\": \"join_unique\"     # Combine source IDs\n    },\n    target_entity_data={\n        \"entity_type\": \"ORGANIZATION\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Managing Documents in LightRAG\nDESCRIPTION: This snippet shows various curl commands for managing documents in the LightRAG system, including adding text, uploading files, batch uploading, scanning for new documents, and clearing all documents.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/documents/text\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"text\": \"您的文本内容\", \"description\": \"可选描述\"}'\n\ncurl -X POST \"http://localhost:9621/documents/file\" \\\n    -F \"file=@/path/to/your/document.txt\" \\\n    -F \"description=可选描述\"\n\ncurl -X POST \"http://localhost:9621/documents/batch\" \\\n    -F \"files=@/path/to/doc1.txt\" \\\n    -F \"files=@/path/to/doc2.txt\"\n\ncurl -X POST \"http://localhost:9621/documents/scan\" --max-time 1800\n\ncurl -X DELETE \"http://localhost:9621/documents\"\n```\n\n----------------------------------------\n\nTITLE: Starting LightRAG Server with Uvicorn\nDESCRIPTION: Simple command to start the LightRAG Server in single-process Uvicorn mode.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nlightrag-server\n```\n\n----------------------------------------\n\nTITLE: Entity Merging with Custom Target Entity Data in LightRAG\nDESCRIPTION: This snippet demonstrates how to merge entities while specifying exact values for the merged target entity. This allows for complete control over the properties of the resulting entity after the merge operation.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Specify exact values for the merged entity\nrag.merge_entities(\n    source_entities=[\"New York\", \"NYC\", \"Big Apple\"],\n    target_entity=\"New York City\",\n    target_entity_data={\n        \"entity_type\": \"LOCATION\",\n        \"description\": \"New York City is the most populous city in the United States.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Using Ollama Emulation Endpoints\nDESCRIPTION: This snippet demonstrates how to use the Ollama emulation endpoints provided by LightRAG, including version information, available models, and chat completion requests.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:9621/api/version\n\ncurl http://localhost:9621/api/tags\n\ncurl -N -X POST http://localhost:9621/api/chat -H \"Content-Type: application/json\" -d \\\n  '{\"model\":\"lightrag:latest\",\"messages\":[{\"role\":\"user\",\"content\":\"猪八戒是谁\"}],\"stream\":true}'\n```\n\n----------------------------------------\n\nTITLE: Configuring LightRAG with OpenAI\nDESCRIPTION: Example configuration for using LightRAG with OpenAI as the LLM and embedding backend. This configuration requires an OpenAI API key.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nLLM_BINDING=openai\nLLM_MODEL=gpt-3.5-turbo\nEMBEDDING_BINDING=openai\nEMBEDDING_MODEL=text-embedding-ada-002\nOPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Querying RAG System with POST Request\nDESCRIPTION: Send a query to the RAG system with configurable search modes using POST /query endpoint\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/query\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"Your question here\", \"mode\": \"hybrid\"}'\n```\n\n----------------------------------------\n\nTITLE: Configuring LightRAG with Ollama\nDESCRIPTION: Example configuration for using LightRAG with Ollama as the LLM and embedding backend. This configuration uses host.docker.internal to access localhost services from within Docker.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nLLM_BINDING=ollama\nLLM_BINDING_HOST=http://host.docker.internal:11434\nLLM_MODEL=mistral\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://host.docker.internal:11434\nEMBEDDING_MODEL=bge-m3\n```\n\n----------------------------------------\n\nTITLE: Configuring JWT Authentication in LightRAG\nDESCRIPTION: Environment variables for setting up JWT-based authentication in LightRAG, including account credentials, token secret, and expiration time.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# For jwt auth\nAUTH_ACCOUNTS='admin:admin123,user1:pass456'\nTOKEN_SECRET='your-key'\nTOKEN_EXPIRE_HOURS=4\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG Server from PyPI\nDESCRIPTION: Command to install the LightRAG Server component from PyPI with API support. The server provides Web UI and API functionality.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"lightrag-hku[api]\"\n```\n\n----------------------------------------\n\nTITLE: Inserting File into LightRAG\nDESCRIPTION: JSON structure for the insert file endpoint request body, specifying the file path to be inserted into the RAG system.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"file_path\": \"path/to/your/file.txt\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG Core from Source\nDESCRIPTION: Commands to install the LightRAG Core component from source code, which is the recommended installation method.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd LightRAG\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Insert File Endpoint Example (cURL)\nDESCRIPTION: This cURL command demonstrates how to send a POST request to the `/insert_file` endpoint to insert the content of a file into the LightRAG system. It sets the `Content-Type` header to `application/json` and includes a JSON payload containing the file path.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://127.0.0.1:8020/insert_file\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"file_path\": \"./book.txt\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG Server from Source\nDESCRIPTION: Commands to install the LightRAG Server from source code in editable mode with API support.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# create a Python virtual enviroment if neccesary\n# Install in editable mode with API support\npip install -e \".[api]\"\n```\n\n----------------------------------------\n\nTITLE: Querying LightRAG API\nDESCRIPTION: This snippet demonstrates how to use curl to query the LightRAG API, including both regular and streaming query endpoints.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/query\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"您的问题\", \"mode\": \"hybrid\", \"\"}'\n\ncurl -X POST \"http://localhost:9621/query/stream\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"您的问题\", \"mode\": \"hybrid\"}'\n```\n\n----------------------------------------\n\nTITLE: Sending Insert File Request to LightRAG API\nDESCRIPTION: cURL command example for sending a POST request to insert a file into the LightRAG system via the API.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://127.0.0.1:8020/insert_file\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"file_path\": \"./book.txt\"}'\n```\n\n----------------------------------------\n\nTITLE: Install FastAPI Dependencies\nDESCRIPTION: This command installs the necessary Python packages for running the LightRAG API server, including FastAPI, Uvicorn (an ASGI server), and Pydantic (for data validation and settings management).  These packages are essential for building and running the API.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install fastapi uvicorn pydantic\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG from Source Code\nDESCRIPTION: Steps to clone the LightRAG repository from GitHub and install it in editable mode with API support.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/HKUDS/lightrag.git\n\n# Change to the repository directory\ncd lightrag\n\n# create a Python virtual environment if necessary\n# Install in editable mode with API support\npip install -e \".[api]\"\n```\n\n----------------------------------------\n\nTITLE: Streaming RAG System Responses\nDESCRIPTION: Stream responses from the RAG system using POST /query/stream endpoint\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/query/stream\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"Your question here\", \"mode\": \"hybrid\"}'\n```\n\n----------------------------------------\n\nTITLE: Citation Feature with LightRAG\nDESCRIPTION: Example showing how to provide file paths when inserting documents to ensure sources can be traced back to their original documents for citation purposes.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# 定义文档及其文件路径\ndocuments = [\"文档内容1\", \"文档内容2\"]\nfile_paths = [\"path/to/doc1.txt\", \"path/to/doc2.txt\"]\n```\n\n----------------------------------------\n\nTITLE: Starting LightRAG WebUI Development Server\nDESCRIPTION: Command to start the development server for LightRAG WebUI, enabling live development and testing of the interface.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag_webui/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbun run dev\n```\n\n----------------------------------------\n\nTITLE: Inserting Text Documents into RAG\nDESCRIPTION: Upload text content directly into the RAG system with optional description\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/documents/text\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"text\": \"Your text content here\", \"description\": \"Optional description\"}'\n```\n\n----------------------------------------\n\nTITLE: Starting LightRAG Server\nDESCRIPTION: Command to start the LightRAG server with PostgreSQL storage options.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nlightrag-server --port 9621 --key sk-somepassword --kv-storage PGKVStorage --graph-storage PGGraphStorage --vector-storage PGVectorStorage --doc-status-storage PGDocStatusStorage\n```\n\n----------------------------------------\n\nTITLE: Insert Text Endpoint Example (cURL)\nDESCRIPTION: This cURL command demonstrates how to send a POST request to the `/insert` endpoint to insert text into the LightRAG system. It sets the `Content-Type` header to `application/json` and includes a JSON payload containing the text to be inserted.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://127.0.0.1:8020/insert\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"text\": \"Content to be inserted into RAG\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG from PyPI with API Support\nDESCRIPTION: Command to install the LightRAG package from PyPI with API support components included.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"lightrag-hku[api]\"\n```\n\n----------------------------------------\n\nTITLE: Run LightRAG API Server\nDESCRIPTION: This command executes the Python script that starts the LightRAG API server. It assumes the script `examples/lightrag_api_openai_compatible_demo.py` is present in the specified path and configured to use the environment variables set previously.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython examples/lightrag_api_openai_compatible_demo.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama LLM with Ollama Embedding\nDESCRIPTION: Environment variable configuration for using Ollama for both LLM and embedding services in LightRAG.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nLLM_BINDING=ollama\nLLM_MODEL=mistral-nemo:latest\nLLM_BINDING_HOST=http://localhost:11434\n# LLM_BINDING_API_KEY=your_api_key\n### Max tokens sent to LLM (based on your Ollama Server capacity)\nMAX_TOKENS=8192\n\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\n# EMBEDDING_BINDING_API_KEY=your_api_key\n```\n\n----------------------------------------\n\nTITLE: Inserting Custom Knowledge into LightRAG\nDESCRIPTION: Example demonstrating how to insert custom knowledge graph data into LightRAG with predefined entities, relationships, and content chunks. Shows the structure required for creating custom knowledge entries.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncustom_kg = {\n    \"chunks\": [\n        {\n            \"content\": \"Alice和Bob正在合作进行量子计算研究。\",\n            \"source_id\": \"doc-1\"\n        }\n    ],\n    \"entities\": [\n        {\n            \"entity_name\": \"Alice\",\n            \"entity_type\": \"person\",\n            \"description\": \"Alice是一位专门研究量子物理的研究员。\",\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"entity_name\": \"Bob\",\n            \"entity_type\": \"person\",\n            \"description\": \"Bob是一位数学家。\",\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"entity_name\": \"量子计算\",\n            \"entity_type\": \"technology\",\n            \"description\": \"量子计算利用量子力学现象进行计算。\",\n            \"source_id\": \"doc-1\"\n        }\n    ],\n    \"relationships\": [\n        {\n            \"src_id\": \"Alice\",\n            \"tgt_id\": \"Bob\",\n            \"description\": \"Alice和Bob是研究伙伴。\",\n            \"keywords\": \"合作 研究\",\n            \"weight\": 1.0,\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"src_id\": \"Alice\",\n            \"tgt_id\": \"量子计算\",\n            \"description\": \"Alice进行量子计算研究。\",\n            \"keywords\": \"研究 专业\",\n            \"weight\": 1.0,\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"src_id\": \"Bob\",\n            \"tgt_id\": \"量子计算\",\n            \"description\": \"Bob研究量子计算。\",\n            \"keywords\": \"研究 应用\",\n            \"weight\": 1.0,\n            \"source_id\": \"doc-1\"\n        }\n    ]\n}\n\nrag.insert_custom_kg(custom_kg)\n```\n\n----------------------------------------\n\nTITLE: Configuring Concurrent Processing Parameters\nDESCRIPTION: Environment variables for controlling concurrent processing of documents and LLM requests in LightRAG Server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n### Number of worker processes, not greater than (2 x number_of_cores) + 1\nWORKERS=2\n### Number of parallel files to process in one batch\nMAX_PARALLEL_INSERT=2\n### Max concurrent requests to the LLM\nMAX_ASYNC=4\n```\n\n----------------------------------------\n\nTITLE: Checking LightRAG Server Health\nDESCRIPTION: This snippet shows how to use curl to check the health status and configuration of the LightRAG server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl \"http://localhost:9621/health\"\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG with Visualization Tools\nDESCRIPTION: Commands for installing LightRAG package with visualization tools. Offers two installation options: visualization tools only or both API and visualization tools.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/tools/lightrag_visualizer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightrag-hku[tools]  # Install with visualization tool only\n# or\npip install lightrag-hku[api,tools]  # Install with both API and visualization tools\n```\n\n----------------------------------------\n\nTITLE: Increasing Context Size for Ollama Models via API Options\nDESCRIPTION: Python code example showing how to configure an Ollama model with increased context size (32768) by passing options through the LightRAG initialization parameters.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=ollama_model_complete,  # 使用Ollama模型进行文本生成\n    llm_model_name='your_model_name', # 您的模型名称\n    llm_model_kwargs={\"options\": {\"num_ctx\": 32768}},\n    # 使用Ollama嵌入函数\n    embedding_func=EmbeddingFunc(\n        embedding_dim=768,\n        max_token_size=8192,\n        func=lambda texts: ollama_embedding(\n            texts,\n            embed_model=\"nomic-embed-text\"\n        )\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: LightRAG Configuration File Setup\nDESCRIPTION: Example configuration file content for PostgreSQL connection settings.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\n[postgres]\nhost = localhost\nport = 5432\nuser = your_role_name\npassword = your_password\ndatabase = your_database\nworkspace = default\n```\n\n----------------------------------------\n\nTITLE: Generating Summary Tokens for Queries\nDESCRIPTION: Function to generate summaries from context by extracting and combining tokens from the first and second half of the text using GPT-2 tokenizer.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\ndef get_summary(context, tot_tokens=2000):\n    tokens = tokenizer.tokenize(context)\n    half_tokens = tot_tokens // 2\n\n    start_tokens = tokens[1000:1000 + half_tokens]\n    end_tokens = tokens[-(1000 + half_tokens):1000]\n\n    summary_tokens = start_tokens + end_tokens\n    summary = tokenizer.convert_tokens_to_string(summary_tokens)\n\n    return summary\n```\n\n----------------------------------------\n\nTITLE: Querying LightRAG API\nDESCRIPTION: JSON structure for the query endpoint request body, including the query text, mode, and option for context-only response.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"query\": \"Your question here\",\n    \"mode\": \"hybrid\",  // Can be \"naive\", \"local\", \"global\", or \"hybrid\"\n    \"only_need_context\": true // Optional: Defaults to false, if true, only the referenced context will be returned, otherwise the llm answer will be returned\n}\n```\n\n----------------------------------------\n\nTITLE: Increasing Context Size for Ollama Models via Modelfile\nDESCRIPTION: Shell commands for pulling an Ollama model, viewing its Modelfile, modifying the context size parameter, and creating a modified model with increased context window.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nollama pull qwen2\n```\n\nLANGUAGE: bash\nCODE:\n```\nollama show --modelfile qwen2 > Modelfile\n```\n\nLANGUAGE: bash\nCODE:\n```\nPARAMETER num_ctx 32768\n```\n\nLANGUAGE: bash\nCODE:\n```\nollama create -f Modelfile qwen2m\n```\n\n----------------------------------------\n\nTITLE: Generating Evaluation Prompts for RAG Systems in Python\nDESCRIPTION: This code snippet defines a prompt template used for evaluating two RAG systems' performance on advanced queries. It specifies the role, objectives, and evaluation criteria for comparing answers based on comprehensiveness, diversity, and empowerment.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n---角色---\n您是一位专家，负责根据三个标准评估同一问题的两个答案：**全面性**、**多样性**和**赋能性**。\n---目标---\n您将根据三个标准评估同一问题的两个答案：**全面性**、**多样性**和**赋能性**。\n\n- **全面性**：答案提供了多少细节来涵盖问题的所有方面和细节？\n- **多样性**：答案在提供关于问题的不同视角和见解方面有多丰富多样？\n- **赋能性**：答案在多大程度上帮助读者理解并对主题做出明智判断？\n\n对于每个标准，选择更好的答案（答案1或答案2）并解释原因。然后，根据这三个类别选择总体赢家。\n\n这是问题：\n{query}\n\n这是两个答案：\n\n**答案1：**\n{answer1}\n\n**答案2：**\n{answer2}\n\n使用上述三个标准评估两个答案，并为每个标准提供详细解释。\n\n以下列JSON格式输出您的评估：\n\n{{\n    \"全面性\": {{\n        \"获胜者\": \"[答案1或答案2]\",\n        \"解释\": \"[在此提供解释]\"\n    }},\n    \"赋能性\": {{\n        \"获胜者\": \"[答案1或答案2]\",\n        \"解释\": \"[在此提供解释]\"\n    }},\n    \"总体获胜者\": {{\n        \"获胜者\": \"[答案1或答案2]\",\n        \"解释\": \"[根据三个标准总结为什么这个答案是总体获胜者]\"\n    }}\n}}\n```\n\n----------------------------------------\n\nTITLE: Triggering Document Scan\nDESCRIPTION: Initiate a scan for new documents in the input directory with configurable timeout\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/documents/scan\" --max-time 1800\n```\n\n----------------------------------------\n\nTITLE: Building LightRAG WebUI Project\nDESCRIPTION: Command to build the LightRAG WebUI project for production deployment, outputting the built files to the lightrag/api/webui directory with an empty output directory.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag_webui/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbun run build --emptyOutDir\n```\n\n----------------------------------------\n\nTITLE: Launching LightRAG Viewer\nDESCRIPTION: Simple command to start the LightRAG 3D graph viewer application.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/tools/lightrag_visualizer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlightrag-viewer\n```\n\n----------------------------------------\n\nTITLE: Cloning LightRAG Repository in Linux/MacOS\nDESCRIPTION: Commands to clone the LightRAG repository and navigate to the project directory in Linux or MacOS environments.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/HKUDS/LightRAG.git\ncd LightRAG\n```\n\n----------------------------------------\n\nTITLE: Ollama Chat Completion Request\nDESCRIPTION: Send a chat completion request through LightRAG with query mode selection based on prefix\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\ncurl -N -X POST http://localhost:9621/api/chat -H \"Content-Type: application/json\" -d \\\n  '{\"model\":\"lightrag:latest\",\"messages\":[{\"role\":\"user\",\"content\":\"猪八戒是谁\"}],\"stream\":true}'\n```\n\n----------------------------------------\n\nTITLE: Cloning LightRAG Repository in Windows PowerShell\nDESCRIPTION: Commands to clone the LightRAG repository and navigate to the project directory using Windows PowerShell.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\ngit clone https://github.com/HKUDS/LightRAG.git\ncd LightRAG\n```\n\n----------------------------------------\n\nTITLE: Using Keyword Extraction with LightRAG\nDESCRIPTION: Example showing how to use the query_with_separate_keyword_extraction function to enhance keyword extraction by separating the user query from formatting prompts, resulting in more relevant extracted keywords.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrag.query_with_separate_keyword_extraction(\n    query=\"解释重力定律\",\n    prompt=\"提供适合学习物理的高中生的详细解释。\",\n    param=QueryParam(mode=\"hybrid\")\n)\n```\n\n----------------------------------------\n\nTITLE: Health Check Endpoint Example (cURL)\nDESCRIPTION: This cURL command demonstrates how to send a GET request to the `/health` endpoint to check the health status of the LightRAG API server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"http://127.0.0.1:8020/health\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment in Linux/MacOS\nDESCRIPTION: Commands to copy the example environment file and prepare it for editing in Linux or MacOS.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp .env.example .env\n# Edit .env with your preferred configuration\n```\n\n----------------------------------------\n\nTITLE: Performing Health Check on LightRAG API\nDESCRIPTION: cURL command example for sending a GET request to the health check endpoint of the LightRAG API server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X GET \"http://127.0.0.1:8020/health\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment in Windows PowerShell\nDESCRIPTION: Commands to copy the example environment file and prepare it for editing in Windows PowerShell.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nCopy-Item .env.example .env\n# Edit .env with your preferred configuration\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Ollama Models\nDESCRIPTION: Get a list of available Ollama models\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:9621/api/tags\n```\n\n----------------------------------------\n\nTITLE: Installing PostgreSQL on Ubuntu\nDESCRIPTION: Commands to install PostgreSQL and its contrib packages on Ubuntu, including starting and enabling the service.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install postgresql postgresql-contrib\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl start postgresql\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo systemctl enable postgresql\n```\n\n----------------------------------------\n\nTITLE: Updating Docker Container for LightRAG\nDESCRIPTION: Commands to update the LightRAG Docker container, including pulling the latest image and rebuilding the container.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ndocker-compose pull\ndocker-compose up -d --build\n```\n\n----------------------------------------\n\nTITLE: Extracting Queries from Formatted Text File in Python\nDESCRIPTION: This function extracts queries from a formatted text file. It removes asterisks and uses regular expressions to find and collect questions labeled with 'Question X:' format.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef extract_queries(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read()\n\n    data = data.replace('**', '')\n\n    queries = re.findall(r'- Question \\d+: (.+)', data)\n\n    return queries\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Command Not Found\nDESCRIPTION: Commands to verify LightRAG installation and check package presence in pip list.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/tools/lightrag_visualizer/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure you installed with the 'tools' option\npip install lightrag-hku[tools]\n\n# Verify installation\npip list | grep lightrag-hku\n```\n\n----------------------------------------\n\nTITLE: Clearing LightRAG Cache with Different Modes\nDESCRIPTION: This snippet shows how to clear the LLM response cache in LightRAG with different modes. It demonstrates both asynchronous and synchronous methods to clear caches for different search strategies like local, global, and hybrid searches.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Clear all cache\nawait rag.aclear_cache()\n\n# Clear local mode cache\nawait rag.aclear_cache(modes=[\"local\"])\n\n# Clear extraction cache\nawait rag.aclear_cache(modes=[\"default\"])\n\n# Clear multiple modes\nawait rag.aclear_cache(modes=[\"local\", \"global\", \"hybrid\"])\n\n# Synchronous version\nrag.clear_cache(modes=[\"local\"])\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Lightrag-viewer\nDESCRIPTION: This command installs the lightrag-hku package with the 'tools' extra dependencies and then runs the lightrag-viewer application. It's the primary way to start the viewer.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/tools/lightrag_visualizer/README-zh.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightrag-hku[tools]\nlightrag-viewer\n```\n\n----------------------------------------\n\nTITLE: Getting Ollama Version\nDESCRIPTION: Retrieve version information for Ollama compatibility\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:9621/api/version\n```\n\n----------------------------------------\n\nTITLE: Inserting Custom Knowledge Graph in Python\nDESCRIPTION: Demonstrates how to insert a custom-defined knowledge graph into LightRAG. The knowledge graph contains chunks, entities, and relationships with detailed metadata.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncustom_kg = {\n    \"chunks\": [\n        {\n            \"content\": \"Alice and Bob are collaborating on quantum computing research.\",\n            \"source_id\": \"doc-1\"\n        }\n    ],\n    \"entities\": [\n        {\n            \"entity_name\": \"Alice\",\n            \"entity_type\": \"person\",\n            \"description\": \"Alice is a researcher specializing in quantum physics.\",\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"entity_name\": \"Bob\",\n            \"entity_type\": \"person\",\n            \"description\": \"Bob is a mathematician.\",\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"entity_name\": \"Quantum Computing\",\n            \"entity_type\": \"technology\",\n            \"description\": \"Quantum computing utilizes quantum mechanical phenomena for computation.\",\n            \"source_id\": \"doc-1\"\n        }\n    ],\n    \"relationships\": [\n        {\n            \"src_id\": \"Alice\",\n            \"tgt_id\": \"Bob\",\n            \"description\": \"Alice and Bob are research partners.\",\n            \"keywords\": \"collaboration research\",\n            \"weight\": 1.0,\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"src_id\": \"Alice\",\n            \"tgt_id\": \"Quantum Computing\",\n            \"description\": \"Alice conducts research on quantum computing.\",\n            \"keywords\": \"research expertise\",\n            \"weight\": 1.0,\n            \"source_id\": \"doc-1\"\n        },\n        {\n            \"src_id\": \"Bob\",\n            \"tgt_id\": \"Quantum Computing\",\n            \"description\": \"Bob researches quantum computing.\",\n            \"keywords\": \"research application\",\n            \"weight\": 1.0,\n            \"source_id\": \"doc-1\"\n        }\n    ]\n}\n\nrag.insert_custom_kg(custom_kg)\n```\n\n----------------------------------------\n\nTITLE: Clearing All Documents\nDESCRIPTION: Remove all documents from the RAG system\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X DELETE \"http://localhost:9621/documents\"\n```\n\n----------------------------------------\n\nTITLE: Creating Azure OpenAI Resources with Azure CLI\nDESCRIPTION: Azure CLI commands to create resource groups, provision OpenAI services, deploy models, and retrieve endpoints and keys for LightRAG integration.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Change the resource group name, location, and OpenAI resource name as needed\nRESOURCE_GROUP_NAME=LightRAG\nLOCATION=swedencentral\nRESOURCE_NAME=LightRAG-OpenAI\n\naz login\naz group create --name $RESOURCE_GROUP_NAME --location $LOCATION\naz cognitiveservices account create --name $RESOURCE_NAME --resource-group $RESOURCE_GROUP_NAME  --kind OpenAI --sku S0 --location swedencentral\naz cognitiveservices account deployment create --resource-group $RESOURCE_GROUP_NAME  --model-format OpenAI --name $RESOURCE_NAME --deployment-name gpt-4o --model-name gpt-4o --model-version \"2024-08-06\"  --sku-capacity 100 --sku-name \"Standard\"\naz cognitiveservices account deployment create --resource-group $RESOURCE_GROUP_NAME  --model-format OpenAI --name $RESOURCE_NAME --deployment-name text-embedding-3-large --model-name text-embedding-3-large --model-version \"1\"  --sku-capacity 80 --sku-name \"Standard\"\naz cognitiveservices account show --name $RESOURCE_NAME --resource-group $RESOURCE_GROUP_NAME --query \"properties.endpoint\"\naz cognitiveservices account keys list --name $RESOURCE_NAME -g $RESOURCE_GROUP_NAME\n```\n\n----------------------------------------\n\nTITLE: ID-based Insertion with LightRAG\nDESCRIPTION: Examples showing how to insert documents with custom IDs in LightRAG. The number of documents and IDs must match, and both single and multiple document insertion patterns are demonstrated.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# 插入单个文本，并为其提供ID\nrag.insert(\"文本1\", ids=[\"文本1的ID\"])\n\n# 插入多个文本，并为它们提供ID\nrag.insert([\"文本1\", \"文本2\",...], ids=[\"文本1的ID\", \"文本2的ID\"])\n```\n\n----------------------------------------\n\nTITLE: Basic Data Export in LightRAG with Default Format\nDESCRIPTION: This code shows how to export knowledge graph data from LightRAG using the default CSV format. It demonstrates the simplest way to export data for analysis, sharing, or backup purposes.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Basic CSV export (default format)\nrag.export_data(\"knowledge_graph.csv\")\n\n# Specify any format\nrag.export_data(\"output.xlsx\", file_format=\"excel\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Unique Contexts from JSONL Files in Python\nDESCRIPTION: This function extracts unique context entries from JSONL files in a given input directory and saves them as JSON files in an output directory. It handles multiple files, error checking, and provides progress updates.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef extract_unique_contexts(input_directory, output_directory):\n\n    os.makedirs(output_directory, exist_ok=True)\n\n    jsonl_files = glob.glob(os.path.join(input_directory, '*.jsonl'))\n    print(f\"找到{len(jsonl_files)}个JSONL文件。\")\n\n    for file_path in jsonl_files:\n        filename = os.path.basename(file_path)\n        name, ext = os.path.splitext(filename)\n        output_filename = f\"{name}_unique_contexts.json\"\n        output_path = os.path.join(output_directory, output_filename)\n\n        unique_contexts_dict = {}\n\n        print(f\"处理文件：{filename}\")\n\n        try:\n            with open(file_path, 'r', encoding='utf-8') as infile:\n                for line_number, line in enumerate(infile, start=1):\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        json_obj = json.loads(line)\n                        context = json_obj.get('context')\n                        if context and context not in unique_contexts_dict:\n                            unique_contexts_dict[context] = None\n                    except json.JSONDecodeError as e:\n                        print(f\"文件{filename}第{line_number}行JSON解码错误：{e}\")\n        except FileNotFoundError:\n            print(f\"未找到文件：{filename}\")\n            continue\n        except Exception as e:\n            print(f\"处理文件{filename}时发生错误：{e}\")\n            continue\n\n        unique_contexts_list = list(unique_contexts_dict.keys())\n        print(f\"文件{filename}中有{len(unique_contexts_list)}个唯一的`context`条目。\")\n\n        try:\n            with open(output_path, 'w', encoding='utf-8') as outfile:\n                json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=4)\n            print(f\"唯一的`context`条目已保存到：{output_filename}\")\n        except Exception as e:\n            print(f\"保存到文件{output_filename}时发生错误：{e}\")\n\n    print(\"所有文件已处理完成。\")\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG Core from PyPI\nDESCRIPTION: Command to install the LightRAG Core component from PyPI package repository.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install lightrag-hku\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for OpenAI Direct Usage\nDESCRIPTION: Bash environment variable configuration for direct OpenAI API access with LlamaIndex integration.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/llm/Readme.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your-openai-key\n```\n\n----------------------------------------\n\nTITLE: Running LightRAG Server with LoLLMs Backend\nDESCRIPTION: This snippet shows how to run the LightRAG server using LoLLMs as the backend for LLM and embedding. It requires configuration in the .env or config.ini file.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# 使用 lollms 运行 lightrag，llm 使用 mistral-nemo:latest，嵌入使用 bge-m3:latest\n# 在 .env 或 config.ini 中配置 LLM_BINDING=lollms 和 EMBEDDING_BINDING=lollms\nlightrag-server\n\n# 使用认证密钥\nlightrag-server --key my-key\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for LiteLLM Proxy Integration\nDESCRIPTION: Bash environment variable configuration for using LiteLLM proxy with LlamaIndex integration, including proxy connection details and model specifications.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/llm/Readme.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# LiteLLM Configuration\nLITELLM_URL=http://litellm:4000\nLITELLM_KEY=your-litellm-key\n\n# Model Configuration\nLLM_MODEL=gpt-4\nEMBEDDING_MODEL=text-embedding-3-large\nEMBEDDING_MAX_TOKEN_SIZE=8192\n```\n\n----------------------------------------\n\nTITLE: Querying LightRAG API using curl\nDESCRIPTION: Example curl command to send a query to the LightRAG API. It sets the necessary headers, including the API key, and sends a POST request with a JSON payload.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://localhost:9621/query\" \\\n     -H \"X-API-Key: your-api-key\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"your question here\"}'\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Configuration in .env File\nDESCRIPTION: Environment variables for configuring LightRAG to use Azure OpenAI for LLM and embedding services, including endpoints, API keys, and model names.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Azure OpenAI Configuration in .env:\nLLM_BINDING=azure_openai\nLLM_BINDING_HOST=your-azure-endpoint\nLLM_MODEL=your-model-deployment-name\nLLM_BINDING_API_KEY=your-azure-api-key\n### API version is optional, defaults to latest version\nAZURE_OPENAI_API_VERSION=2024-08-01-preview\n\n### If using Azure OpenAI for embeddings\nEMBEDDING_BINDING=azure_openai\nEMBEDDING_MODEL=your-embedding-deployment-name\n```\n\n----------------------------------------\n\nTITLE: Manually Tracking Token Usage with TokenTracker in Python\nDESCRIPTION: This snippet shows how to manually track token usage with TokenTracker. This method provides more granular control over token statistics and is suitable for scenarios where specific operations need to be tracked separately.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Method 2: Manually adding token usage records\n# Suitable for scenarios requiring more granular control over token statistics\ntoken_tracker.reset()\n\nrag.insert()\n\nrag.query(\"your question 1\", param=QueryParam(mode=\"naive\"))\nrag.query(\"your question 2\", param=QueryParam(mode=\"mix\"))\n\n# Display total token usage (including insert and query operations)\nprint(\"Token usage:\", token_tracker.get_usage())\n```\n\n----------------------------------------\n\nTITLE: Database Creation and Extension Activation\nDESCRIPTION: Commands for creating a new database and enabling the pgvector extension.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo -u postgres createdb your_database\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo -u postgres psql -d your_database\n```\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION vector;\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT extversion FROM pg_extension WHERE extname = 'vector';\n\\q\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LightRAG\nDESCRIPTION: Command to install the required dependencies for LightRAG using pip. This works on both Linux/MacOS and Windows platforms.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Set Environment Variables for API Server\nDESCRIPTION: These commands set the required environment variables for configuring the LightRAG API server.  `RAG_DIR` specifies the directory for the RAG index, `OPENAI_BASE_URL` sets the OpenAI API base URL, `OPENAI_API_KEY` provides the OpenAI API key, `LLM_MODEL` defines the LLM model to use, and `EMBEDDING_MODEL` defines the embedding model to use.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport RAG_DIR=\"your_index_directory\"  # Optional: Defaults to \"index_default\"\nexport OPENAI_BASE_URL=\"Your OpenAI API base URL\"  # Optional: Defaults to \"https://api.openai.com/v1\"\nexport OPENAI_API_KEY=\"Your OpenAI API key\"  # Required\nexport LLM_MODEL=\"Your LLM model\" # Optional: Defaults to \"gpt-4o-mini\"\nexport EMBEDDING_MODEL=\"Your embedding model\" # Optional: Defaults to \"text-embedding-3-large\"\n```\n\n----------------------------------------\n\nTITLE: Query Endpoint Example (cURL)\nDESCRIPTION: This cURL command demonstrates how to send a POST request to the `/query` endpoint to query the LightRAG system.  It sets the `Content-Type` header to `application/json` and includes a JSON payload containing the query and mode.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST \"http://127.0.0.1:8020/query\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"query\": \"What are the main themes?\", \"mode\": \"hybrid\"}'\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Dependencies for LightRAG\nDESCRIPTION: Command to install the required Python packages for using LlamaIndex with LightRAG, including support for LiteLLM proxies.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/llm/Readme.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install llama-index-llms-litellm llama-index-embeddings-litellm\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL User Permissions\nDESCRIPTION: SQL commands for setting up user roles and passwords in PostgreSQL.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo -u postgres psql\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER USER postgres WITH PASSWORD 'your_secure_password';\n\\q\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo -u postgres createuser --interactive\n```\n\nLANGUAGE: sql\nCODE:\n```\nALTER USER your_new_role WITH PASSWORD 'your_secure_password';\n\\q\n```\n\n----------------------------------------\n\nTITLE: Configuring LightRAG Server Settings\nDESCRIPTION: This snippet shows the configuration settings for the LightRAG server, including server port, document indexing parameters, LLM and embedding configurations, and authentication options.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/README-zh.md#2025-04-23_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\n### Server Configuration\n# HOST=0.0.0.0\nPORT=9621\nWORKERS=2\n\n### Settings for document indexing\nENABLE_LLM_CACHE_FOR_EXTRACT=true\nSUMMARY_LANGUAGE=Chinese\nMAX_PARALLEL_INSERT=2\n\n### LLM Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nTIMEOUT=200\nTEMPERATURE=0.0\nMAX_ASYNC=4\nMAX_TOKENS=32768\n\nLLM_BINDING=openai\nLLM_MODEL=gpt-4o-mini\nLLM_BINDING_HOST=https://api.openai.com/v1\nLLM_BINDING_API_KEY=your-api-key\n\n### Embedding Configuration (Use valid host. For local services installed with docker, you can use host.docker.internal)\nEMBEDDING_MODEL=bge-m3:latest\nEMBEDDING_DIM=1024\nEMBEDDING_BINDING=ollama\nEMBEDDING_BINDING_HOST=http://localhost:11434\n\n### For JWT Auth\n# AUTH_ACCOUNTS='admin:admin123,user1:pass456'\n# TOKEN_SECRET=your-key-for-LightRAG-API-Server-xxx\n# TOKEN_EXPIRE_HOURS=48\n\n# LIGHTRAG_API_KEY=your-secure-api-key-here-123\n# WHITELIST_PATHS=/api/*\n# WHITELIST_PATHS=/health,/api/*\n```\n\n----------------------------------------\n\nTITLE: Insert File Endpoint Request Body (JSON)\nDESCRIPTION: This JSON payload represents the request body for the `/insert_file` endpoint. The `file_path` parameter specifies the path to the file to be inserted into the RAG system.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README_zh.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"file_path\": \"path/to/your/file.txt\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment in Linux/MacOS\nDESCRIPTION: Commands to create and activate a Python virtual environment in Linux or MacOS.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Installing PGVector and Age Extensions\nDESCRIPTION: Commands for installing PGVector and Age extensions for PostgreSQL.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install postgresql-server-dev-all\ncd /tmp\ngit clone --branch v0.8.0 https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nsudo make install\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get install build-essential libpq-dev\ncd /tmp\ngit clone https://github.com/apache/age.git\ncd age\nmake\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: Basic Insertion with LightRAG\nDESCRIPTION: Simple example showing how to insert a single text document into LightRAG.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# 基本插入\nrag.insert(\"文本\")\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG WebUI Dependencies with Bun\nDESCRIPTION: Command to install the project dependencies using Bun package manager with frozen lockfile to ensure consistent installations across environments.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag_webui/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbun install --frozen-lockfile\n```\n\n----------------------------------------\n\nTITLE: Pipeline-based Insertion with LightRAG\nDESCRIPTION: Example showing how to use asynchronous pipeline functions to incrementally insert documents into the knowledge graph while allowing the main thread to continue execution.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nrag = LightRAG(..)\n\nawait rag.apipeline_enqueue_documents(input)\n# 您的循环例程\nawait rag.apipeline_process_enqueue_documents(input)\n```\n\n----------------------------------------\n\nTITLE: Installing LightRAG API Package\nDESCRIPTION: Command to install LightRAG with API support using pip.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/api/docs/LightRagWithPostGRESQL.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install \"lightrag-hku[api]\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LightRAG API Server\nDESCRIPTION: Command to install the required Python packages for running the LightRAG API server.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install fastapi uvicorn pydantic\n```\n\n----------------------------------------\n\nTITLE: Querying LightRAG API using PowerShell\nDESCRIPTION: Example PowerShell script to send a query to the LightRAG API. It sets the necessary headers, including the API key, and sends a POST request with a JSON payload.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_10\n\nLANGUAGE: powershell\nCODE:\n```\n$headers = @{\n    \"X-API-Key\" = \"your-api-key\"\n    \"Content-Type\" = \"application/json\"\n}\n$body = @{\n    query = \"your question here\"\n} | ConvertTo-Json\n\nInvoke-RestMethod -Uri \"http://localhost:9621/query\" -Method Post -Headers $headers -Body $body\n```\n\n----------------------------------------\n\nTITLE: Running LightRAG API Server\nDESCRIPTION: Command to start the LightRAG API server using the provided Python script.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/examples/openai_README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython examples/lightrag_api_openai_compatible_demo.py\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Virtual Environment in Windows PowerShell\nDESCRIPTION: Commands to create and activate a Python virtual environment using Windows PowerShell.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\npython -m venv venv\n.\\venv\\Scripts\\Activate\n```\n\n----------------------------------------\n\nTITLE: Multi-file Type Support with LightRAG\nDESCRIPTION: Example showing how to use textract to read different file types (TXT, DOCX, PPTX, CSV, PDF) and insert their content into LightRAG.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport textract\n\nfile_path = 'TEXT.pdf'\ntext_content = textract.process(file_path)\n\nrag.insert(text_content.decode('utf-8'))\n```\n\n----------------------------------------\n\nTITLE: Updating Native Installation of LightRAG in Linux/MacOS\nDESCRIPTION: Commands to update a native installation of LightRAG in Linux or MacOS, including pulling the latest code, activating the virtual environment, and updating dependencies.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ngit pull\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Extracting Unique Contexts in Python\nDESCRIPTION: Function to extract and save unique context entries from JSONL files. Handles multiple files, includes error handling and progress reporting. Outputs unique contexts to JSON files.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef extract_unique_contexts(input_directory, output_directory):\n\n    os.makedirs(output_directory, exist_ok=True)\n\n    jsonl_files = glob.glob(os.path.join(input_directory, '*.jsonl'))\n    print(f\"Found {len(jsonl_files)} JSONL files.\")\n\n    for file_path in jsonl_files:\n        filename = os.path.basename(file_path)\n        name, ext = os.path.splitext(filename)\n        output_filename = f\"{name}_unique_contexts.json\"\n        output_path = os.path.join(output_directory, output_filename)\n\n        unique_contexts_dict = {}\n\n        print(f\"Processing file: {filename}\")\n\n        try:\n            with open(file_path, 'r', encoding='utf-8') as infile:\n                for line_number, line in enumerate(infile, start=1):\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        json_obj = json.loads(line)\n                        context = json_obj.get('context')\n                        if context and context not in unique_contexts_dict:\n                            unique_contexts_dict[context] = None\n                    except json.JSONDecodeError as e:\n                        print(f\"JSON decoding error in file {filename} at line {line_number}: {e}\")\n        except FileNotFoundError:\n            print(f\"File not found: {filename}\")\n            continue\n        except Exception as e:\n            print(f\"An error occurred while processing file {filename}: {e}\")\n            continue\n\n        unique_contexts_list = list(unique_contexts_dict.keys())\n        print(f\"There are {len(unique_contexts_list)} unique `context` entries in the file {filename}.\")\n\n        try:\n            with open(output_path, 'w', encoding='utf-8') as outfile:\n                json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=4)\n            print(f\"Unique `context` entries have been saved to: {output_filename}\")\n        except Exception as e:\n            print(f\"An error occurred while saving to the file {output_filename}: {e}\")\n\n    print(\"All files have been processed.\")\n```\n\n----------------------------------------\n\nTITLE: Checking OpenGL Version for ModernGL Issues\nDESCRIPTION: Commands to check OpenGL version and verify graphics driver compatibility for ModernGL initialization issues.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/lightrag/tools/lightrag_visualizer/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Check OpenGL version\nglxinfo | grep \"OpenGL version\"\n\n# Update graphics drivers if needed\n```\n\n----------------------------------------\n\nTITLE: Generating Query Summaries from Context Using GPT-2 Tokenizer in Python\nDESCRIPTION: This function creates a summary of a given context by extracting tokens from its first and second half. It uses the GPT-2 tokenizer to process the text and combines tokens to form a summary within a specified token limit.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README-zh.md#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\ndef get_summary(context, tot_tokens=2000):\n    tokens = tokenizer.tokenize(context)\n    half_tokens = tot_tokens // 2\n\n    start_tokens = tokens[1000:1000 + half_tokens]\n    end_tokens = tokens[-(1000 + half_tokens):1000]\n\n    summary_tokens = start_tokens + end_tokens\n    summary = tokenizer.convert_tokens_to_string(summary_tokens)\n\n    return summary\n```\n\n----------------------------------------\n\nTITLE: Extracting Queries from Text\nDESCRIPTION: Function to extract queries from formatted text files by finding patterns matching 'Question X:' format and cleaning the text.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/README.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef extract_queries(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read()\n\n    data = data.replace('**', '')\n\n    queries = re.findall(r'- Question \\d+: (.+)', data)\n\n    return queries\n```\n\n----------------------------------------\n\nTITLE: Updating Native Installation of LightRAG in Windows PowerShell\nDESCRIPTION: Commands to update a native installation of LightRAG using Windows PowerShell, including pulling the latest code, activating the virtual environment, and updating dependencies.\nSOURCE: https://github.com/hkuds/lightrag/blob/main/docs/DockerDeployment.md#2025-04-23_snippet_14\n\nLANGUAGE: powershell\nCODE:\n```\ngit pull\n.\\venv\\Scripts\\Activate\npip install -r requirements.txt\n```"
  }
]
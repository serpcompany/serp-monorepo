[
  {
    "owner": "akariasai",
    "repo": "openscholar",
    "content": "TITLE: Running a LoRA Fine-tuning Job with torchtune\nDESCRIPTION: Command to launch a LoRA fine-tuning job for the Llama2 7B model using torchtune, with sample output showing the initialization progress and training start. Uses a predefined recipe with epoch override.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/first_finetune_tutorial.rst#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ tune run lora_finetune_single_device --config llama2/7B_lora_single_device epochs=1\nINFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\nWriting logs to /tmp/lora_finetune_output/log_1713194212.txt\nINFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\nINFO:torchtune.utils.logging:Tokenizer is initialized from file.\nINFO:torchtune.utils.logging:Optimizer and loss are initialized.\nINFO:torchtune.utils.logging:Loss is initialized.\nINFO:torchtune.utils.logging:Dataset and Sampler are initialized.\nINFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n1|52|Loss: 2.3697006702423096:   0%|‚ñè                     | 52/25880 [00:24<3:55:01,  1.83it/s]\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU Full Finetuning on Llama3.1 8B Model\nDESCRIPTION: Command to execute full fine-tuning on 4 GPUs for the Llama3.1 8B model, using the distributed training configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 4 full_finetune_distributed --config llama3_1/8B_full\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Llama3-8B-Instruct with QLoRA\nDESCRIPTION: This snippet demonstrates how to fine-tune Llama3-8B-Instruct using QLoRA on a single device with torchtune, which uses less memory than standard LoRA.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama3/8B_qlora_single_device\n```\n\n----------------------------------------\n\nTITLE: Running QLoRA Finetuning on Llama2-7B\nDESCRIPTION: Command to run a QLoRA finetuning job on Llama2-7B using torchtune's built-in recipe with a QLoRA-specific configuration, enabling finetuning with reduced memory requirements.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama2/7B_qlora_single_device\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Llama3-8B-Instruct with LoRA on Single Device\nDESCRIPTION: This snippet demonstrates how to fine-tune the Llama3-8B-Instruct model using LoRA on a single device with torchtune. It includes options for specifying checkpoint directories and tokenizer path.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama3/8B_lora_single_device \\\n    checkpointer.checkpoint_dir=<checkpoint_dir> \\\n    tokenizer.path=<checkpoint_dir>/tokenizer.model \\\n    checkpointer.output_dir=<checkpoint_dir>\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU LoRA Finetuning on Llama3.1 8B Model\nDESCRIPTION: Command to execute LoRA fine-tuning on 2 GPUs for the Llama3.1 8B model, using the distributed training configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 2 lora_finetune_distributed --config llama3_1/8B_lora\n```\n\n----------------------------------------\n\nTITLE: Running Single GPU Full Finetuning on Llama3.1 8B Model\nDESCRIPTION: Command to execute full fine-tuning on a single GPU for the Llama3.1 8B model, using the predefined configuration for full model training.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntune run full_finetune_single_device --config llama3_1/8B_full_single_device\n```\n\n----------------------------------------\n\nTITLE: Configuring LoRA Parameters in Python\nDESCRIPTION: Code to fetch LoRA parameters, set trainability flags, and calculate the proportion of trainable parameters in the model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Fetch all params from the model that are associated with LoRA.\nlora_params = get_adapter_params(lora_model)\n\n# Set requires_grad=True on lora_params, and requires_grad=False on all others.\nset_trainable_params(lora_model, lora_params)\n\n# Print the total number of parameters\ntotal_params = sum([p.numel() for p in lora_model.parameters()])\ntrainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])\nprint(\n  f\"\"\"\n  {total_params} total params,\n  {trainable_params}\" trainable params,\n  {(100.0 * trainable_params / total_params):.2f}% of all params are trainable.\n  \"\"\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running Multi-GPU LoRA Finetuning on Llama3.1 70B Model\nDESCRIPTION: Command to execute LoRA fine-tuning on 8 GPUs for the Llama3.1 70B model, using the distributed training configuration with YAML specification.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 8 lora_finetune_distributed --config llama3_1/70B_lora.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing Llama2-7B with QLoRA in torchtune\nDESCRIPTION: Shows how to initialize a Llama2-7B model with QLoRA applied to specific attention modules (q_proj and v_proj), enabling memory-efficient finetuning while maintaining model quality.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.models.llama2 import qlora_llama2_7b\n\nqlora_model = qlora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\n```\n\n----------------------------------------\n\nTITLE: Distributed Fine-tuning of Llama3-8B-Instruct with LoRA\nDESCRIPTION: This snippet shows how to perform distributed fine-tuning of Llama3-8B-Instruct using LoRA on multiple GPUs with torchtune.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 2 lora_finetune_distributed --config llama3/8B_lora\n```\n\n----------------------------------------\n\nTITLE: Applying QAT to Llama3 Models in Python\nDESCRIPTION: Shows how to apply Quantization-Aware Training transformations to Llama3 models using torchtune. It prepares the model by inserting fake quantize operations into linear layers.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom torchtune.utils.quantization import Int8DynActInt4WeightQATQuantizer\nfrom torchtune.models.llama3 import llama3_8b\n\nmodel = llama3_8b()\n\n# Quantizer for int8 dynamic per token activations +\n# int4 grouped per channel weights, only for linear layers\nquantizer = Int8DynActInt4WeightQATQuantizer()\n\n# Insert \"fake quantize\" operations into linear layers.\n# These operations simulate quantization numerics during\n# fine-tuning without performing any dtype casting\nprepared_model = quantizer.prepare(model)\n```\n\n----------------------------------------\n\nTITLE: Simulating Quantization in Python\nDESCRIPTION: Demonstrates the difference between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) in terms of how values are quantized.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# PTQ: x_q is quantized and cast to int8\n# scale and zero point (zp) refer to parameters used to quantize x_float\n# qmin and qmax refer to the range of quantized values\nx_q = (x_float / scale + zp).round().clamp(qmin, qmax).cast(int8)\n\n# QAT: x_fq is still in float\n# Fake quantize simulates the numerics of quantize + dequantize\nx_fq = (x_float / scale + zp).round().clamp(qmin, qmax)\nx_fq = (x_fq - zp) * scale\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Llama2 Chat Format\nDESCRIPTION: Example showing the Llama2 chat model prompt template with special tags for system and user instructions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n<s>[INST] <<SYS>>\nYou are a helpful, respectful, and honest assistant.\n<</SYS>>\n\nHi! I am a human. [/INST] Hello there! Nice to meet you! I'm Meta AI, your friendly AI assistant </s>\n```\n\n----------------------------------------\n\nTITLE: Running Single GPU QLoRA Finetuning on Llama3.1 8B Model\nDESCRIPTION: Command to execute QLoRA fine-tuning on a single GPU for the Llama3.1 8B model, using the predefined configuration for quantized LoRA.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama3_1/8B_qlora_single_device\n```\n\n----------------------------------------\n\nTITLE: Launching Llama3 Fine-Tuning with Command Line Interface\nDESCRIPTION: This bash command launches the Llama3 fine-tuning process using the 'tune run' CLI tool with the LoRA single device recipe. It specifies the custom configuration file and sets the number of training epochs to 15.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ tune run lora_finetune_single_device --config custom_8B_lora_single_device.yaml epochs=15\n```\n\n----------------------------------------\n\nTITLE: Running QLoRA Finetuning with Torch Compile\nDESCRIPTION: Bash command to run a QLoRA finetuning job with torch.compile enabled for performance optimization using the torchtune framework.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama2/7B_qlora_single_device compile=True\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Llama3 Chat Format\nDESCRIPTION: Example showing the updated Llama3 Instruct format for multiturn conversations with new special tokens and structure.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful, and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHi! I am a human.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHello there! Nice to meet you! I'm Meta AI, your friendly AI assistant<|eot_id|>\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Weights into LoRA Model\nDESCRIPTION: Example showing how to load pretrained Llama2 weights into a LoRA-enabled model using torchtune's built-in functionality.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Assuming that base_model already has the pretrained Llama2 weights,\n# this will directly load them into your LoRA model without any conversion necessary.\nlora_model.load_state_dict(base_model.state_dict(), strict=False)\n```\n\n----------------------------------------\n\nTITLE: Running LoRA fine-tuning on a single GPU\nDESCRIPTION: Command to run a LoRA fine-tuning recipe for Llama3 8B on a single GPU using torchtune's run command with a predefined configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama2/7B_lora_single_device\n```\n\n----------------------------------------\n\nTITLE: Comparing Memory Usage Between LoRALinear and QLORALinear\nDESCRIPTION: Demonstrates the memory savings of QLoRA by comparing the memory consumption of a regular LoRALinear layer versus a quantized one, showing approximately 8x memory reduction with QLoRA.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchtune.modules.peft import LoRALinear\n\ntorch.set_default_device(\"cuda\")\nqlora_linear = LoRALinear(512, 512, rank=8, alpha=0.1, quantize_base=True)\nprint(torch.cuda.memory_allocated())  # 177,152 bytes\ndel qlora_linear\ntorch.cuda.empty_cache()\nlora_linear = LoRALinear(512, 512, rank=8, alpha=0.1, quantize_base=False)\nprint(torch.cuda.memory_allocated()) # 1,081,344 bytes\n```\n\n----------------------------------------\n\nTITLE: Running Quantization-Aware Training in torchtune\nDESCRIPTION: Command to run QAT with finetuning using the llama3/8B_qat_full configuration across 4 processes per node.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/quantization.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 4 qat_distributed --config llama3/8B_qat_full\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Chat Format and Dataset in Python\nDESCRIPTION: This snippet demonstrates how to create a custom chat format and use it with a chat dataset in torchtune. It includes the definition of a CustomChatFormat class and configuration of the chat dataset.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import chat_dataset\nfrom torchtune.data import ChatFormat\n\nclass CustomChatFormat(ChatFormat):\n    # Define templates for system, user, assistant messages\n    # as strings with {} as placeholders for message content\n    system = ...\n    user = ...\n    assistant = ...\n\n    # Implement this method\n    @classmethod\n    def format(\n        cls,\n        sample: List[Message],\n    ) -> List[Message]:\n        ...\n\n# Load in tokenizer\ntokenizer = ...\ndataset = chat_dataset(\n    tokenizer=tokenizer,\n    source=\"my/dataset/path\",\n    split=\"train\",\n    conversation_style=\"openai\",\n    chat_format=\"import.path.to.CustomChatFormat\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Full Finetuning with Tune CLI for Llama3 8B\nDESCRIPTION: Command to run a distributed full finetuning process on a Llama3 8B model across 4 GPU nodes. Uses the BitsAndBytes 8-bit Adam optimizer with paged memory and disables the foreach implementation for better memory efficiency.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full \\\n    optimizer=bitsandbytes.optim.PagedAdamW8bit ~optimizer.foreach\n```\n\n----------------------------------------\n\nTITLE: Running QAT Fine-tuning in Bash\nDESCRIPTION: Command to run fine-tuning with QAT using the specified configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ntune run --nnodes 1 --nproc_per_node 6 qat_distributed --config custom_8B_qat_full.yaml\n```\n\n----------------------------------------\n\nTITLE: Training English Monolingual Contriever Model\nDESCRIPTION: Command for training the English Contriever model using 32 GPUs with specific hyperparameters including average pooling, delete augmentation, and linear scheduling. Uses bert-base-uncased as the base model with contrastive learning setup.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\npython train.py \\\n        --retriever_model_id bert-base-uncased --pooling average \\\n        --augmentation delete --prob_augmentation 0.1 \\\n        --train_data \"data/wiki/ data/cc-net/\" --loading_mode split \\\n        --ratio_min 0.1 --ratio_max 0.5 --chunk_length 256 \\\n        --momentum 0.9995 --moco_queue 131072 --temperature 0.05 \\\n        --warmup_steps 20000 --total_steps 500000 --lr 0.00005 \\\n        --scheduler linear --optim adamw --per_gpu_batch_size 64 \\\n        --output_dir /checkpoint/gizacard/contriever/xling/contriever \\\n```\n\n----------------------------------------\n\nTITLE: Applying LoRA to Llama2 Models with Torchtune\nDESCRIPTION: Example of creating Llama2 models with and without LoRA using torchtune, showing how to specify which attention modules should use LoRA adaptation.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.models.llama2 import llama2_7b, lora_llama2_7b\n\n# Build Llama2 without any LoRA layers\nbase_model = llama2_7b()\n\n# The default settings for lora_llama2_7b will match those for llama2_7b\n# We just need to define which layers we want LoRA applied to.\n# Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\n# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n# layers outside of the self-attention.\nlora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Dataset in YAML for Llama3 Fine-Tuning\nDESCRIPTION: This YAML configuration specifies the custom dataset builder and its parameters for use in the Llama3 fine-tuning process. It references the path to the custom dataset builder function and sets the maximum sequence length.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ndataset:\n  _component_: path.to.my.custom_dataset\n  max_seq_len: 2048\n```\n\n----------------------------------------\n\nTITLE: Running LoRA Experiment with Modified Parameters\nDESCRIPTION: Bash command showing how to run LoRA fine-tuning with custom attention modules, rank, and alpha values.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora \\\nlora_attn_modules=['q_proj','k_proj','v_proj','output_proj'] \\\nlora_rank=32 lora_alpha=64 output_dir=./lora_experiment_1\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama2 7B Model Using tune CLI\nDESCRIPTION: Uses the tune CLI to download the Hugging Face format of the Llama2 7B model. Requires specifying an output directory and HuggingFace access token.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntune download \\\nmeta-llama/Llama-2-7b-hf \\\n--output-dir <checkpoint_dir> \\\n--hf-token <ACCESS TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Running a fine-tuning recipe with a custom configuration\nDESCRIPTION: Command to run a torchtune fine-tuning recipe using a local custom configuration file that was previously created and modified.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ntune run full_finetune_distributed --config ./my_custom_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring EleutherAI Evaluation for Quantized Models in YAML\nDESCRIPTION: YAML configuration showing necessary settings for evaluating a quantized model, including model specification, checkpoint location, evaluation tasks, and quantizer settings.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# Model arguments\nmodel:\n  _component_: torchtune.models.llama3.llama3_8b\n\ncheckpointer:\n  _component_: torchtune.utils.FullModelTorchTuneCheckpointer\n  checkpoint_dir: <your quantized model checkpoint dir>\n  checkpoint_files: [meta_model_0-8da4w.pt]\n  recipe_checkpoint: null\n  output_dir: <your quantized model checkpoint dir>\n  model_type: LLAMA3\n\n...\n\n# EleutherAI specific eval args\ntasks: [\"hellaswag\", \"wikitext\"]\nlimit: null\nmax_seq_length: 8192\nbatch_size: 8\n\nquantizer:\n  _component_: torchtune.utils.quantization.Int8DynActInt4WeightQuantizer\n  groupsize: 256\n```\n\n----------------------------------------\n\nTITLE: Running Single GPU LoRA Finetuning on Llama3.1 8B Model\nDESCRIPTION: Command to execute LoRA fine-tuning on a single GPU for the Llama3.1 8B model, using the predefined configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama3_1/8B_lora_single_device\n```\n\n----------------------------------------\n\nTITLE: Single Device LoRA Fine-tuning Command\nDESCRIPTION: Bash command for running LoRA fine-tuning on a single GPU device.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama2/7B_lora_single_device\n```\n\n----------------------------------------\n\nTITLE: Loading and Using TorchTune Checkpoints with Llama2 Model\nDESCRIPTION: Python script demonstrating how to load TorchTune checkpoints, create a Llama2 model, and perform a forward pass using the loaded weights.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchtune.utils import FullModelHFCheckpointer, ModelType\nfrom torchtune.models.llama2 import llama2_13b\n\ncheckpoint_dir = 'Llama-2-13b-hf/'\npytorch_files = [\n    'pytorch_model-00001-of-00003.bin',\n    'pytorch_model-00002-of-00003.bin',\n    'pytorch_model-00003-of-00003.bin'\n]\n\ncheckpointer = FullModelHFCheckpointer(\n    checkpoint_dir=checkpoint_dir,\n    checkpoint_files=pytorch_files,\n    output_dir=checkpoint_dir,\n    model_type=ModelType.LLAMA2\n)\ntorchtune_sd = checkpointer.load_checkpoint()\n\nmodel = llama2_13b()\n\nmodel.load_state_dict(torchtune_sd[\"model\"])\n\nx = torch.randint(0, 32000, (1, 70))\n\nwith torch.no_grad():\n    model(x)\n```\n\n----------------------------------------\n\nTITLE: Listing Available Recipes and Configs in torchtune\nDESCRIPTION: Command to list all available recipes and their corresponding configuration files in torchtune. This helps users find appropriate starting points for their fine-tuning jobs.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/first_finetune_tutorial.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ tune ls\nRECIPE                                   CONFIG\nfull_finetune_single_device              llama2/7B_full_low_memory\n                                         mistral/7B_full_low_memory\nfull_finetune_distributed                llama2/7B_full\n                                         llama2/13B_full\n                                         mistral/7B_full\nlora_finetune_single_device              llama2/7B_lora_single_device\n                                         llama2/7B_qlora_single_device\n                                         mistral/7B_lora_single_device\n...\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation on Quantized Models\nDESCRIPTION: Command to run evaluation on a quantized model using the eleuther_evaluation configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/quantization.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntune run eleuther_eval --config eleuther_evaluation\n```\n\n----------------------------------------\n\nTITLE: Running distributed fine-tuning on multiple GPUs\nDESCRIPTION: Command to run a full fine-tuning recipe for Llama3 8B on two GPUs using torchtune's distributed training capabilities with torchrun integration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nproc_per_node 2 full_finetune_distributed --config llama2/7B_full\n```\n\n----------------------------------------\n\nTITLE: Configuring QAT Fine-tuning in YAML\nDESCRIPTION: YAML configuration for QAT fine-tuning, including dataset settings, epochs, and fake quantization parameters.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n# Dataset\ndataset:\n  _component_: torchtune.datasets.text_completion_dataset\n  source: allenai/c4\n  max_seq_len: 8192\n  column: text\n  name: en\n  split: train\nseed: null\nshuffle: True\n\n...\n\nepochs: 1\nmax_steps_per_epoch: 2000\nfake_quant_after_n_steps: 1000\nmemory_efficient_fsdp_wrap: False\n```\n\n----------------------------------------\n\nTITLE: Converting QAT Model to Quantized Model in Python\nDESCRIPTION: Demonstrates how to convert a QAT-prepared model to an actual quantized model after fine-tuning.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Fine-tune as before\ntrain_loop(prepared_model)\n\n# Convert fake quantize to actual quantize operations\nconverted_model = quantizer.convert(prepared_model)\n```\n\n----------------------------------------\n\nTITLE: Overriding Recipe Configuration Parameters from Command Line\nDESCRIPTION: Example showing how to override a configuration parameter (epochs) when running a torchtune recipe. This allows for quick parameter adjustments without modifying the config file.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/first_finetune_tutorial.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntune run <RECIPE> --config <CONFIG> epochs=1\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with TorchTune CLI\nDESCRIPTION: Bash command to run text generation using the TorchTune CLI with a custom configuration file and a prompt about Bay Area attractions. This uses default sampling settings with top_k=300 and temperature=0.8.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntune run generate --config ./custom_generation_config.yaml \\\npromptt=\"What are some interesting sites to visit in the Bay Area?\"\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with Quantized Models\nDESCRIPTION: Command to run text generation using a quantized model with the generation configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/quantization.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntune run generate --config generation\n```\n\n----------------------------------------\n\nTITLE: Creating Custom PreferenceDataset in Python\nDESCRIPTION: This code snippet demonstrates how to create a custom PreferenceDataset for RLHF preference data in torchtune. It shows the process of tokenizing and labeling chosen and rejected samples.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nchosen_message = [\n    Message(role=\"user\", content=prompt, masked=True),\n    Message(role=\"assistant\", content=transformed_sample[key_chosen]),\n]\nrejected_message = [\n    Message(role=\"user\", content=prompt, masked=True),\n    Message(role=\"assistant\", content=transformed_sample[key_rejected]),\n]\n\nchosen_input_ids, c_masks = self._tokenizer.tokenize_messages(\n    chosen_message, self.max_seq_len\n)\nchosen_labels = list(\n    np.where(c_masks, CROSS_ENTROPY_IGNORE_IDX, chosen_input_ids)\n)\n\nrejected_input_ids, r_masks = self._tokenizer.tokenize_messages(\n    rejected_message, self.max_seq_len\n)\nrejected_labels = list(\n    np.where(r_masks, CROSS_ENTROPY_IGNORE_IDX, rejected_input_ids)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization in YAML\nDESCRIPTION: YAML configuration for quantizing the QAT-trained model, specifying model arguments, checkpointer settings, and quantizer details.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\n# Model arguments\nmodel:\n  _component_: torchtune.models.llama3.llama3_8b\n\ncheckpointer:\n  _component_: torchtune.utils.FullModelMetaCheckpointer\n  checkpoint_dir: <your QAT checkpoint dir>\n  checkpoint_files: [meta_model_0.pt]\n  recipe_checkpoint: null\n  output_dir: <your QAT checkpoint dir>\n  model_type: LLAMA3\n\n...\n\nquantizer:\n  _component_: torchtune.utils.quantization.Int8DynActInt4WeightQATQuantizer\n  groupsize: 256\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation with EleutherAI's Eval Harness\nDESCRIPTION: This snippet shows how to run the evaluation using the custom config file with EleutherAI's evaluation harness in torchtune.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune run eleuther_eval --config ./custom_eval_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Converting CSV Rows to Llama3 Message Format\nDESCRIPTION: This function converts a single row from the CSV file into the Message format expected by the Llama3 tokenizer. It creates a user message containing the input text and an assistant message containing the output text, with optional masking based on the train_on_input parameter.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef message_converter(sample: Mapping[str, Any], train_on_input: bool) -> List[Message]:\n    input_msg = sample[\"input\"]\n    output_msg = sample[\"output\"]\n\n    user_message = Message(\n        role=\"user\",\n        content=input_msg,\n        masked=not train_on_input,  # Mask if not training on prompt\n    )\n    assistant_message = Message(\n        role=\"assistant\",\n        content=output_msg,\n        masked=False,\n    )\n    # A single turn conversation\n    messages = [user_message, assistant_message]\n\n    return messages\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dataset Builder Function in Python\nDESCRIPTION: This code snippet shows how to create a custom dataset builder function for a specific dataset configuration. It demonstrates the creation of a PreferenceDataset for a paired dataset from Hugging Face.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef stack_exchanged_paired_dataset(\n    tokenizer: ModelTokenizer,\n    max_seq_len: int = 1024,\n) -> PreferenceDataset:\n    return PreferenceDataset(\n        tokenizer=tokenizer,\n        source=\"lvwerra/stack-exchange-paired\",\n        template=StackExchangedPairedTemplate(),\n        column_map={\n            \"prompt\": \"question\",\n            \"chosen\": \"response_j\",\n            \"rejected\": \"response_k\",\n        },\n        max_seq_len=max_seq_len,\n        split=\"train\",\n        data_dir=\"data/rl\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Recipe Training Script\nDESCRIPTION: Example showing how to initialize process groups, setup a recipe instance, and execute the training process.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/recipe_deepdive.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the process group\ninit_process_group(backend=\"gloo\" if cfg.device == \"cpu\" else \"nccl\")\n\n# Setup the recipe and train the model\nrecipe = FullFinetuneRecipeDistributed(cfg=cfg)\nrecipe.setup(cfg=cfg)\nrecipe.train()\nrecipe.cleanup()\n\n# Other stuff to do after training is complete\n...\n```\n\n----------------------------------------\n\nTITLE: Extended LoRA Configuration with MLP Layers\nDESCRIPTION: YAML configuration showing how to enable LoRA for additional model layers including MLP and output projections.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# Model Arguments\nmodel:\n  _component_: lora_llama2_7b\n  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj', 'output_proj']\n  apply_lora_to_mlp: True\n  apply_lora_to_output: True\n```\n\n----------------------------------------\n\nTITLE: Installing OpenScholar Dependencies\nDESCRIPTION: Commands to create and configure a Python environment for OpenScholar, including installing required packages and downloading necessary language models.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconda create -n os_env python=3.10.0\nconda activate os_env\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n\n----------------------------------------\n\nTITLE: Logging Model Checkpoints to W&B in Python\nDESCRIPTION: Python code snippet showing how to modify the save_checkpoint method to log model checkpoints to Weights & Biases. This creates a W&B artifact for each checkpoint with associated metadata.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/wandb_logging.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef save_checkpoint(self, epoch: int) -> None:\n    ...\n    ## Let's save the checkpoint to W&B\n    ## depending on the Checkpointer Class the file will be named differently\n    ## Here is an example for the full_finetune case\n    checkpoint_file = Path.joinpath(\n        self._checkpointer._output_dir, f\"torchtune_model_{epoch}\"\n    ).with_suffix(\".pt\")\n    wandb_at = wandb.Artifact(\n        name=f\"torchtune_model_{epoch}\",\n        type=\"model\",\n        # description of the model checkpoint\n        description=\"Model checkpoint\",\n        # you can add whatever metadata you want as a dict\n        metadata={\n            utils.SEED_KEY: self.seed,\n            utils.EPOCHS_KEY: self.epochs_run,\n            utils.TOTAL_EPOCHS_KEY: self.total_epochs,\n            utils.MAX_STEPS_KEY: self.max_steps_per_epoch,\n        }\n    )\n    wandb_at.add_file(checkpoint_file)\n    wandb.log_artifact(wandb_at)\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Llama2 7B with LoRA Using tune CLI\nDESCRIPTION: Runs the LoRA fine-tuning recipe for Llama2 7B using the tune CLI. Specifies the config and overrides checkpoint and tokenizer paths.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device \\\n--config llama2/7B_lora_single_device \\\ncheckpointer.checkpoint_dir=<checkpoint_dir> \\\ntokenizer.path=<checkpoint_dir>/tokenizer.model \\\ncheckpointer.output_dir=<checkpoint_dir>\n```\n\n----------------------------------------\n\nTITLE: Quantizing Fine-tuned Models with TorchAO\nDESCRIPTION: Python code snippet for quantizing a fine-tuned model using TorchAO's quantization API. This example uses int4_weight_only quantization technique to reduce model size and improve inference speed.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# we also support `int8_weight_only()` and `int8_dynamic_activation_int8_weight()`, see\n# https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques\n# for a full list of techniques that we support\nfrom torchao.quantization.quant_api import quantize\\_, int4_weight_only\nquantize\\_(model, int4_weight_only())\n```\n\n----------------------------------------\n\nTITLE: Configuring Local and Remote Datasets in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up datasets from local and remote sources using torchtune. It includes examples for both local CSV files and remote JSON files.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config - local files\ndataset:\n  _component_: torchtune.datasets.instruct_dataset\n  source: csv\n  template: import.path.to.CustomTemplate\n  data_files: path/to/my/data.csv\n\n# YAML config - remote files\ndataset:\n  _component_: torchtune.datasets.instruct_dataset\n  source: json\n  template: import.path.to.CustomTemplate\n  data_files: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n  field: data\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Dataset via Command Line in torchtune\nDESCRIPTION: This bash command demonstrates how to configure a local dataset using torchtune's command-line interface. It specifies the dataset source, template, and file path.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# Command line - local files\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\ndataset=torchtune.datasets.chat_dataset dataset.source=csv \\\ndataset.template=import.path.to.CustomTemplate dataset.data_files=path/to/my/data.csv\n```\n\n----------------------------------------\n\nTITLE: Formatting Messages with Llama2ChatFormat in Python\nDESCRIPTION: This code snippet shows how to use the Llama2ChatFormat class to format messages for a chat dataset. It demonstrates the structure of system, user, and assistant messages.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.data import Llama2ChatFormat, Message\n\nmessages = [\n    Message(\n        role=\"system\",\n        content=\"You are a helpful, respectful, and honest assistant.\",\n    ),\n    Message(\n        role=\"user\",\n        content=\"I am going to Paris, what should I see?\",\n    ),\n    Message(\n        role=\"assistant\",\n        content=\"Paris, the capital of France, is known for its stunning architecture...\"\n    ),\n]\nformatted_messages = Llama2ChatFormat.format(messages)\nprint(formatted_messages)\n# [\n#     Message(\n#         role=\"user\",\n#         content=\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant.\\n<</SYS>>\\n\\n\"\n#         \"I am going to Paris, what should I see? [/INST] \",\n#     ),\n#     Message(\n#         role=\"assistant\",\n#         content=\"Paris, the capital of France, is known for its stunning architecture...\"\n#     ),\n# ]\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Custom Instruct Dataset with Templates in Python, YAML, and Command Line\nDESCRIPTION: Demonstrates how to create and use a custom instruction template for specialized tasks and configure it with various methods.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import instruct_dataset\nfrom torchtune.data import InstructTemplate\n\nclass CustomTemplate(InstructTemplate):\n    # Define the template as string with {} as placeholders for data columns\n    template = ...\n\n    # Implement this method\n    @classmethod\n    def format(\n        cls, sample: Mapping[str, Any], column_map: Optional[Dict[str, str]] = None\n    ) -> str:\n        ...\n\n# Load in tokenizer\ntokenizer = ...\ndataset = instruct_dataset(\n    tokenizer=tokenizer,\n    source=\"my/dataset/path\",\n    template=\"import.path.to.CustomTemplate\",\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  _component_: torchtune.datasets.instruct_dataset\n  source: my/dataset/path\n  template: import.path.to.CustomTemplate\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Command line\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\n```\n\n----------------------------------------\n\nTITLE: Converting and Running Inference with GPT-Fast\nDESCRIPTION: Bash commands to convert TorchTune checkpoints to GPT-Fast format and run inference. This demonstrates how to use a fine-tuned model with a different library for potentially faster inference.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncd gpt-fast/\n\n# convert the checkpoints into a format readable by gpt-fast\npython scripts/convert_hf_checkpoint.py \\\n--checkpoint_dir <new_dir>/Llama-2-7B-hf/ \\\n--model 7B\n\n# run inference using the converted model\npython generate.py \\\n--compile \\\n--checkpoint_path <new_dir>/Llama-2-7B-hf/model.pth \\\n--device cuda\n```\n\n----------------------------------------\n\nTITLE: Configuring Components with _component_ in YAML\nDESCRIPTION: Shows how to specify torchtune objects using the _component_ field and custom arguments in a YAML config file. This example configures an alpaca dataset with a custom train_on_input setting.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n  train_on_input: False\n```\n\n----------------------------------------\n\nTITLE: Quantizing Models with torchao in Python\nDESCRIPTION: This snippet demonstrates how to quantize a fine-tuned model using torchao's post-training quantization. It shows the implementation of int4_weight_only quantization, though mentions other techniques are available.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# we also support `int8_weight_only()` and `int8_dynamic_activation_int8_weight()`, see\n# https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques\n# for a full list of techniques that we support\nfrom torchao.quantization.quant_api import quantize_, int4_weight_only\nquantize_(model, int4_weight_only())\n```\n\n----------------------------------------\n\nTITLE: Inspecting QLoRA-Enabled Model Parameters\nDESCRIPTION: Demonstrates how to verify which model parameters have been quantized to NF4Tensor format in a QLoRA-enabled model by examining the parameter types of different projection matrices.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nattn = qlora_model.layers[0].attn\nprint(type(attn.q_proj.weight))  # <class 'torchao.dtypes.nf4tensor.NF4Tensor'>\nprint(type(attn.k_proj.weight))  # <class 'torch.nn.parameter.Parameter'>\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchTune Checkpointer for Training Resumption\nDESCRIPTION: YAML configuration for restarting training from a previous checkpoint, including specifying intermediate checkpoint files and recipe state.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ncheckpointer:\n        _component_: torchtune.utils.FullModelHFCheckpointer\n        checkpoint_dir: <checkpoint_dir>\n        checkpoint_files: [\n            hf_model_0001_0.pt,\n            hf_model_0002_0.pt,\n        ]\n        recipe_checkpoint: recipe_state.pt\n        output_dir: <checkpoint_dir>\n        model_type: LLAMA2\n    resume_from_checkpoint: True\n```\n\n----------------------------------------\n\nTITLE: Loading Built-in Datasets in Python, YAML, and Command Line\nDESCRIPTION: Demonstrates how to use built-in datasets from TorchTune library using Python code, YAML configuration, and command line instructions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import alpaca_dataset\n\n# Load in tokenizer\ntokenizer = ...\ndataset = alpaca_dataset(tokenizer)\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Command line\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\ndataset=torchtune.datasets.alpaca_dataset\n```\n\n----------------------------------------\n\nTITLE: Recipe Setup Method Implementation\nDESCRIPTION: Method to load checkpoints, update recipe state, and initialize components like models, tokenizers, and optimizers.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/recipe_deepdive.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef setup(self, cfg: DictConfig):\n\n    ckpt_dict = self.load_checkpoint(cfg.checkpointer)\n\n    # Setup the model, including FSDP wrapping, setting up activation checkpointing and\n    # loading the state dict\n    self._model = self._setup_model(...)\n    self._tokenizer = self._setup_tokenizer(...)\n\n    # Setup Optimizer, including transforming for FSDP when resuming training\n    self._optimizer = self._setup_optimizer(...)\n    self._loss_fn = self._setup_loss(...)\n    self._sampler, self._dataloader = self._setup_data(...)\n```\n\n----------------------------------------\n\nTITLE: Quantizer Configuration for QAT in YAML\nDESCRIPTION: YAML configuration for Int8DynActInt4WeightQATQuantizer with groupsize parameter set to 256. This defines the quantization approach used during training.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/quantization.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# QAT specific args\nquantizer:\n  _component_: torchtune.utils.quantization.Int8DynActInt4WeightQATQuantizer\n  groupsize: 256\n```\n\n----------------------------------------\n\nTITLE: Starting PES2O API Server for Real-time Search\nDESCRIPTION: Commands to navigate to the API directory and launch the PES2O search server. This enables real-time search against the indexed data through an HTTP API endpoint.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd retrieval-scaling/api/\npython serve_pes2o.py\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchTune Checkpointer for LoRA\nDESCRIPTION: YAML configuration for checkpointing with LoRA, including options for saving adapter weights and resuming training with both base and adapter weights.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ncheckpointer:\n        _component_: torchtune.utils.FullModelHFCheckpointer\n        checkpoint_dir: <checkpoint_dir>\n        checkpoint_files: [\n            pytorch_model-00001-of-00002.bin,\n            pytorch_model-00002-of-00002.bin,\n        ]\n        adapter_checkpoint: adapter_0.pt\n        recipe_checkpoint: recipe_state.pt\n        output_dir: <checkpoint_dir>\n        model_type: LLAMA2\n    resume_from_checkpoint: True\n    save_adapter_weights_only: False\n```\n\n----------------------------------------\n\nTITLE: Loading Hugging Face Datasets in Python, YAML, and Command Line\nDESCRIPTION: Shows how to load datasets from Hugging Face Hub using the text_completion_dataset builder with Python, YAML, and command line approaches.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import text_completion_dataset\n\n# Load in tokenizer\ntokenizer = ...\ndataset = text_completion_dataset(\n    tokenizer,\n    source=\"allenai/c4\",\n    # Keyword-arguments that are passed into load_dataset\n    split=\"train\",\n    data_dir=\"realnewslike\",\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  _component_: torchtune.datasets.text_completion_dataset\n  source: allenai/c4\n  split: train\n  data_dir: realnewslike\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Command line\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\ndataset=torchtune.datasets.text_completion_dataset dataset.source=allenai/c4 \\\ndataset.split=train dataset.data_dir=realnewslike\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama2-7B Model with torchtune\nDESCRIPTION: Command to download the Llama2-7B model using the torchtune download utility, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Llama-2-7b-hf --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Implementing LoRA Linear Layer in PyTorch\nDESCRIPTION: A basic implementation of the LoRA linear layer that demonstrates how low-rank adaptation works with frozen pretrained weights and trainable LoRA parameters. Includes initialization, forward pass, and dropout functionality.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn, Tensor\n\nclass LoRALinear(nn.Module):\n  def __init__(\n    self,\n    in_dim: int,\n    out_dim: int,\n    rank: int,\n    alpha: float,\n    dropout: float\n  ):\n    # These are the weights from the original pretrained model\n    self.linear = nn.Linear(in_dim, out_dim, bias=False)\n\n    # These are the new LoRA params. In general rank << in_dim, out_dim\n    self.lora_a = nn.Linear(in_dim, rank, bias=False)\n    self.lora_b = nn.Linear(rank, out_dim, bias=False)\n\n    # Rank and alpha are commonly-tuned hyperparameters\n    self.rank = rank\n    self.alpha = alpha\n\n    # Most implementations also include some dropout\n    self.dropout = nn.Dropout(p=dropout)\n\n    # The original params are frozen, and only LoRA params are trainable.\n    self.linear.weight.requires_grad = False\n    self.lora_a.weight.requires_grad = True\n    self.lora_b.weight.requires_grad = True\n\n  def forward(self, x: Tensor) -> Tensor:\n    # This would be the output of the original model\n    frozen_out = self.linear(x)\n\n    # lora_a projects inputs down to the much smaller self.rank,\n    # then lora_b projects back up to the output dimension\n    lora_out = self.lora_b(self.lora_a(self.dropout(x)))\n\n    # Finally, scale by the alpha parameter (normalized by rank)\n    # and add to the original model's outputs\n    return frozen_out + (self.alpha / self.rank) * lora_out\n```\n\n----------------------------------------\n\nTITLE: Running LoRA Fine-tuning with Distributed Training\nDESCRIPTION: Bash command to run distributed LoRA fine-tuning using torchtune with 2 GPUs.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora\n```\n\n----------------------------------------\n\nTITLE: Using Interpolations in YAML Config Files\nDESCRIPTION: Demonstrates how to reference other fields in a config file using interpolation syntax, allowing for reuse of values across multiple settings.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\noutput_dir: /tmp/alpaca-llama2-finetune\nmetric_logger:\n  _component_: torchtune.utils.metric_logging.DiskLogger\n  log_dir: ${output_dir}\n```\n\n----------------------------------------\n\nTITLE: Running a Distributed Training Recipe\nDESCRIPTION: Demonstrates how to run a distributed training recipe by passing torchrun arguments like --nnodes and --nproc-per-node to the 'tune run' command.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntune run --nnodes=1 --nproc-per-node=4 lora_finetune_distributed --config llama3/8B_lora\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama2-13B Model with torchtune\nDESCRIPTION: Command to download the Llama2-13B model using the torchtune download utility, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Llama-2-13b-hf --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Configuring Evaluation for Fine-tuned Llama3-8B Model\nDESCRIPTION: This YAML snippet demonstrates how to configure the evaluation settings for a fine-tuned Llama3-8B model, including specifying the model, checkpointer, and tokenizer paths.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodel:\n  _component_: torchtune.models.llama3.llama3_8b\n\ncheckpointer:\n  _component_: torchtune.utils.FullModelMetaCheckpointer\n\n  # directory with the checkpoint files\n  # this should match the output_dir specified during\n  # fine-tuning\n  checkpoint_dir: <checkpoint_dir>\n\n  # checkpoint files for the fine-tuned model. These will be logged\n  # at the end of your fine-tune\n  checkpoint_files: [\n    meta_model_0.pt\n  ]\n\n  output_dir: <checkpoint_dir>\n  model_type: LLAMA3\n\n# Make sure to update the tokenizer path to the right\n# checkpoint directory as well\ntokenizer:\n  _component_: torchtune.models.llama3.llama3_tokenizer\n  path: <checkpoint_dir>/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: Lowering QAT Model to Edge Devices with Executorch\nDESCRIPTION: Command to optimize and convert the QAT model for deployment on edge devices using the XNNPACK backend, with quantization settings for weights and activations.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m examples.models.llama2.export_llama --checkpoint <your QAT checkpoint> -p <params.json> -kv --use_sdpa_with_kv_cache -X -qmode 8da4w --group_size 256 -d fp32 --metadata '{\"get_bos_id\":128000, \"get_eos_id\":128001}' --embedding-quantize 4,32 --output_name=\"llama3_8da4w.pte\"\n```\n\n----------------------------------------\n\nTITLE: Complex Component Configuration with Dependencies\nDESCRIPTION: Example showing how to configure multiple related components in YAML where one depends on another, specifically a tokenizer needed for a dataset.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Tokenizer is needed for the dataset, configure it first\ntokenizer:\n  _component_: torchtune.models.llama2.llama2_tokenizer\n  path: /tmp/tokenizer.model\n\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n```\n\n----------------------------------------\n\nTITLE: Running a Custom Recipe with Custom Config\nDESCRIPTION: Shows how to use the 'tune run' command with custom recipes and configuration files by providing file paths instead of built-in names.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntune run my/fancy_lora.py --config my/configs/8B_fancy_lora.yaml\n```\n\n----------------------------------------\n\nTITLE: Inspecting LoRA Model Architecture\nDESCRIPTION: Shell commands demonstrating how to inspect and compare the architecture of standard and LoRA-enabled Llama2 models, showing the additional LoRA parameters in attention layers.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Print the first layer's self-attention in the usual Llama2 model\n>>> print(base_model.layers[0].attn)\nCausalSelfAttention(\n  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n  (pos_embeddings): RotaryPositionalEmbeddings()\n)\n\n# Print the same for Llama2 with LoRA weights\n>>> print(lora_model.layers[0].attn)\nCausalSelfAttention(\n  (q_proj): LoRALinear(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n  )\n  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n  (v_proj): LoRALinear(\n    (dropout): Dropout(p=0.0, inplace=False)\n    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n  )\n  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n  (pos_embeddings): RotaryPositionalEmbeddings()\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for FSDP2 Recipes in Python\nDESCRIPTION: This code snippet demonstrates how to install the required dependencies for running FSDP2 recipes. It first installs torchtune and then upgrades to the nightly build of PyTorch with CUDA 12.4 support.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/dev/fsdp2_recipes.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torchtune\npip3 install --upgrade --pre torch --index-url https://download.pytorch.org/whl/nightly/cu124\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Instruct Dataset in torchtune\nDESCRIPTION: This snippet demonstrates how to configure a custom instruct dataset using torchtune's command-line interface. It specifies the dataset source and template.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndataset=torchtune.datasets.instruct_dataset dataset.source=my/dataset/path \\\ndataset.template=import.path.to.CustomTemplate\n```\n\n----------------------------------------\n\nTITLE: Complete Component Instantiation with Dependencies\nDESCRIPTION: Shows the full instantiation process for components with dependencies, including the API signatures and how to pass instantiated objects as arguments to other components.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Note the API of the tokenizer we specified - we need to pass in a path\ndef llama2_tokenizer(path: str) -> Llama2Tokenizer:\n\n# Note the API of the dataset we specified - we need to pass in a model tokenizer\n# and any optional keyword arguments\ndef alpaca_dataset(\n    tokenizer: ModelTokenizer,\n    train_on_input: bool = True,\n    max_seq_len: int = 512,\n) -> InstructDataset:\n\nfrom torchtune import config\n\n# Since we've already specified the path in the config, we don't need to pass\n# it in\ntokenizer = config.instantiate(cfg.tokenizer)\n# We pass in the instantiated tokenizer as the first required argument, then\n# we change an optional keyword argument\ndataset = config.instantiate(\n    cfg.dataset,\n    tokenizer,\n    train_on_input=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading a Model from Hugging Face Hub\nDESCRIPTION: Demonstrates how to download a model from the Hugging Face Hub using the 'tune download' command. The example shows downloading the Meta-Llama-3-8B-Instruct model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ tune download meta-llama/Meta-Llama-3-8B-Instruct\nSuccessfully downloaded model repo and wrote to the following locations:\n./model/config.json\n./model/README.md\n./model/model-00001-of-00002.bin\n...\n```\n\n----------------------------------------\n\nTITLE: Instantiating Components from Config in Python\nDESCRIPTION: Demonstrates how to use config.instantiate() to create an instance of an object specified in the config file. This snippet shows the basic usage pattern without arguments.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune import config\n\n# Access the dataset field and create the object instance\ndataset = config.instantiate(cfg.dataset)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Dataset Builder for Llama3 Fine-Tuning\nDESCRIPTION: This function creates a ChatDataset object for Llama3 fine-tuning by loading data from a local CSV file and converting it to the Message format using the message_converter function. It supports configuration of sequence length and passes parameters to the underlying load_dataset function.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef custom_dataset(\n    *,\n    tokenizer: ModelTokenizer,\n    max_seq_len: int = 2048,  # You can expose this if you want to experiment\n) -> ChatDataset:\n\n    return ChatDataset(\n        tokenizer=tokenizer,\n        # For local csv files, we specify \"csv\" as the source, just like in\n        # load_dataset\n        source=\"csv\",\n        # Default split of \"train\" is required for local files\n        split=\"train\",\n        convert_to_messages=message_converter,\n        # Llama3 does not need a chat format\n        chat_format=None,\n        max_seq_len=max_seq_len,\n        # To load a local file we specify it as data_files just like in\n        # load_dataset\n        data_files=\"your_file.csv\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading and Examining CSV Data with Pandas for Llama3 Fine-Tuning\nDESCRIPTION: This snippet demonstrates how to load a CSV file containing question-answer pairs using pandas, and prints the header and first row to understand the data structure before processing it for Llama3 fine-tuning.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.read_csv('your_file.csv', nrows=1)\nprint(\"Header:\", df.columns.tolist())\n# ['input', 'output']\nprint(\"First row:\", df.iloc[0].tolist())\n# [\n#     \"How do GPS receivers communicate with satellites?\",\n#     \"The first thing to know is the communication is one-way...\",\n# ]\n```\n\n----------------------------------------\n\nTITLE: Basic YAML Configuration Structure for torchtune\nDESCRIPTION: Example of a basic YAML configuration file for torchtune showing common parameter definitions like seed, shuffle, device, and dtype settings.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseed: null\nshuftle: True\ndevice: cuda\ndtype: fp32\nenable_fsdp: True\n...\n```\n\n----------------------------------------\n\nTITLE: Loading Local and Remote Datasets in Python\nDESCRIPTION: This code snippet shows how to load datasets from local and remote sources using torchtune's instruct_dataset function. It demonstrates configuring local CSV files and remote JSON files.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import instruct_dataset\n\n# Load in tokenizer\ntokenizer = ...\n# Local files\ndataset = instruct_dataset(\n    tokenizer=tokenizer,\n    source=\"csv\",\n    split=\"train\",\n    template=\"import.path.to.CustomTemplate\"\n    data_files=\"path/to/my/data.csv\",\n)\n# Remote files\ndataset = instruct_dataset(\n    tokenizer=tokenizer,\n    source=\"json\",\n    split=\"train\",\n    template=\"import.path.to.CustomTemplate\"\n    data_files=\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\",\n    # You can also pass in any kwarg that load_dataset accepts\n    field=\"data\",\n)\n```\n\n----------------------------------------\n\nTITLE: Overriding Components via Command Line\nDESCRIPTION: Demonstrates how to change a component and its parameters through command-line overrides using dot notation to access nested fields.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# Change to slimorca_dataset and set train_on_input to True\ntune run lora_finetune_single_device --config my_config.yaml \\\ndataset=torchtune.datasets.slimorca_dataset dataset.train_on_input=True\n```\n\n----------------------------------------\n\nTITLE: Using Custom Unstructured Text Corpus in Python, YAML, and Command Line\nDESCRIPTION: Demonstrates how to use a custom local text corpus for continued pre-training using different configuration methods.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import text_completion_dataset\n\n# Load in tokenizer\ntokenizer = ...\ndataset = text_completion_dataset(\n    tokenizer,\n    source=\"text\",\n    data_files=\"path/to/my_data.txt\",\n    split=\"train\",\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  _component_: torchtune.datasets.text_completion_dataset\n  source: text\n  data_files: path/to/my_data.txt\n  split: train\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Command line\ntune run --nproc_per_node 4 full_finetune_distributed --config llama3/8B_full \\\ndataset=torchtune.datasets.text_completion_dataset dataset.source=text \\\ndataset.data_files=path/to/my_data.txt dataset.split=train\n```\n\n----------------------------------------\n\nTITLE: Listing Available Recipes and Configs with tune CLI\nDESCRIPTION: Uses the tune CLI to list available recipes and configurations for different models and fine-tuning techniques.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntune ls\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Instruct Template and Displaying a Formatted Sample\nDESCRIPTION: Shows how to create and use a custom instruction template by displaying a sample formatted with AlpacaInstructTemplate.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.data import AlpacaInstructTemplate\n\nsample = {\n    \"instruction\": \"Classify the following into animals, plants, and minerals\",\n    \"input\": \"Oak tree, copper ore, elephant\",\n}\nprompt = AlpacaInstructTemplate.format(sample)\nprint(prompt)\n# Below is an instruction that describes a task, paired with an input that provides further context.\n# Write a response that appropriately completes the request.\n#\n# ### Instruction:\n# Classify the following into animals, plants, and minerals\n#\n# ### Input:\n# Oak tree, copper ore, elephant\n#\n# ### Response:\n#\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Generation Settings in YAML for TorchTune\nDESCRIPTION: YAML configuration file for TorchTune generation specifying the checkpointer settings, checkpoint paths, and tokenizer configuration. This config references fine-tuned model checkpoints and sets up the model type as LLAMA2.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ncheckpointer:\n    _component_: torchtune.utils.FullModelHFCheckpointer\n\n    # directory with the checkpoint files\n    # this should match the output_dir specified during\n    # finetuning\n    checkpoint_dir: <checkpoint_dir>\n\n    # checkpoint files for the fine-tuned model. This should\n    # match what's shown in the logs above\n    checkpoint_files: [\n        hf_model_0001_0.pt,\n        hf_model_0002_0.pt,\n    ]\n\n    output_dir: <checkpoint_dir>\n    model_type: LLAMA2\n\n# Make sure to update the tokenizer path to the right\n# checkpoint directory as well\ntokenizer:\n    _component_: torchtune.models.llama2.llama2_tokenizer\n    path: <checkpoint_dir>/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: GPT-Fast Inference Output Example\nDESCRIPTION: Sample output from running inference with GPT-Fast using a converted TorchTune model, showing the generated text and performance metrics including inference time and bandwidth achieved.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nHello, my name is Justin. I am a middle school math teacher\nat WS Middle School ...\n\nTime for inference 5: 1.94 sec total, 103.28 tokens/sec\nBandwidth achieved: 1391.84 GB/sec\n```\n\n----------------------------------------\n\nTITLE: Generating Passage Embeddings\nDESCRIPTION: Generates embeddings for passages using mContriever model\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython generate_passage_embeddings.py \\\n    --model_name_or_path facebook/mcontriever \\\n    --output_dir mcontriever_embeddings  \\\n    --passages psgs_w100.tsv \\\n    --shard_id 0 --num_shards 1 \\\n    --lowercase --normalize_text \\\n```\n\n----------------------------------------\n\nTITLE: HFCheckpointer Configuration in YAML\nDESCRIPTION: YAML configuration example for setting up the HuggingFace checkpointer in torchtune, including checkpoint directory, files, and model type settings.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncheckpointer:\n    # checkpointer to use\n    _component_: torchtune.utils.FullModelHFCheckpointer\n\n    # directory with the checkpoint files\n    # this should match the output_dir above\n    checkpoint_dir: <checkpoint_dir>\n\n    # checkpoint files. For the llama2-7b-hf model we have\n    # 2 .bin files. The checkpointer takes care of sorting\n    # by id and so the order here does not matter\n    checkpoint_files: [\n        pytorch_model-00001-of-00002.bin,\n        pytorch_model-00002-of-00002.bin,\n    ]\n\n    # if we're restarting a previous run, we need to specify\n    # the file with the checkpoint state. More on this in the\n    # next section\n    recipe_checkpoint: null\n\n    # dir for saving the output checkpoints. Usually set\n    # to be the same as checkpoint_dir\n    output_dir: <checkpoint_dir>\n\n    # model_type which specifies how to convert the state_dict\n    # into a format which torchtune understands\n    model_type: LLAMA2\n\n    # set to True if restarting training\n    resume_from_checkpoint: False\n```\n\n----------------------------------------\n\nTITLE: Creating Checkpoint Mapping for GPT-Fast Compatibility\nDESCRIPTION: Python script to create a weight map file for GPT-Fast compatibility. This maps parameter names to their corresponding checkpoint files, enabling the use of TorchTune-trained models with the GPT-Fast inference framework.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport torch\n\n# create the output dictionary\noutput_dict = {\"weight_map\": {}}\n\n# Load the checkpoints\nsd_1 = torch.load('<checkpoint_dir>/hf_model_0001_0.pt', mmap=True, map_location='cpu')\nsd_2 = torch.load('<checkpoint_dir>/hf_model_0002_0.pt', mmap=True, map_location='cpu')\n\n# create the weight map\nfor key in sd_1.keys():\n    output_dict['weight_map'][key] =  \"hf_model_0001_0.pt\"\nfor key in sd_2.keys():\n    output_dict['weight_map'][key] =  \"hf_model_0002_0.pt\"\n\nwith open('<new_dir>/Llama-2-7B-hf/pytorch_model.bin.index.json', 'w') as f:\n    json.dump(output_dict, f)\n```\n\n----------------------------------------\n\nTITLE: Using Command-line Overrides with tune CLI\nDESCRIPTION: Demonstrates how to override config values from the command line when running recipes with the tune command, allowing quick experimentation without modifying the config file.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device \\\n--config llama2/7B_lora_single_device \\\ncheckpointer.checkpoint_dir=/home/my_model_checkpoint \\\ncheckpointer.checkpoint_files=['file_1','file_2'] \\\ntokenizer.path=/home/my_tokenizer_path\n```\n\n----------------------------------------\n\nTITLE: Installing torchtune from PyPI\nDESCRIPTION: Command to install the latest stable version of torchtune from PyPI, which is a fine-tuning library for language models.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install torchtune\n```\n\n----------------------------------------\n\nTITLE: Copying Evaluation Config for EleutherAI's Eval Harness\nDESCRIPTION: This snippet shows how to copy the default evaluation config for use with EleutherAI's evaluation harness in torchtune.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntune cp eleuther_evaluation ./custom_eval_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchTune Checkpointer in YAML\nDESCRIPTION: YAML configuration for setting up a TorchTune checkpointer, specifying checkpoint files, directories, and model type.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ncheckpointer:\n        _component_: torchtune.utils.FullModelHFCheckpointer\n        checkpoint_dir: <checkpoint_dir>\n        checkpoint_files: [consolidated.00.pth]\n        recipe_checkpoint: null\n        output_dir: <checkpoint_dir>\n        model_type: LLAMA2\n    resume_from_checkpoint: False\n```\n\n----------------------------------------\n\nTITLE: Configuring WandBLogger in YAML Config\nDESCRIPTION: YAML configuration for enabling logging to Weights & Biases in torchtune. This specifies the metric_logger component to use the WandBLogger and sets the project name.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/wandb_logging.rst#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# enable logging to the built-in WandBLogger\nmetric_logger:\n  _component_: torchtune.utils.metric_logging.WandBLogger\n  # the W&B project to log to\n  project: torchtune\n```\n\n----------------------------------------\n\nTITLE: Inspecting Meta Format Checkpoint in Python\nDESCRIPTION: Code example showing how to load and inspect a Meta format Llama2 checkpoint file using torch.load(), displaying the state dictionary keys and tensor shapes.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> state_dict = torch.load('consolidated.00.pth', mmap=True, weights_only=True, map_location='cpu')\n>>> # inspect the keys and the shapes of the associated tensors\n>>> for key, value in state_dict.items():\n>>>    print(f'{key}: {value.shape}')\n\ntok_embeddings.weight: torch.Size([32000, 4096])\n...\n...\n>>> print(len(state_dict.keys()))\n292\n```\n\n----------------------------------------\n\nTITLE: Downloading Meta-Llama-3-8B model with torchtune\nDESCRIPTION: Command to download the Llama3 8B model weights using torchtune's download command, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Meta-Llama-3-8B \\\n--output-dir /tmp/Meta-Llama-3-8B \\\n--hf-token <HF_TOKEN> \\\n```\n\n----------------------------------------\n\nTITLE: Running a Recipe with a Specific Config\nDESCRIPTION: Shows how to run a torchtune recipe using a specified configuration with the 'tune run' command.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama3/8B_lora_single_device\n```\n\n----------------------------------------\n\nTITLE: TorchTune Checkpoint State Dictionary Structure\nDESCRIPTION: Python dictionary structure for TorchTune checkpoints, showing the format for both model weights and recipe state.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"key_1\": weight_1,\n    \"key_2\": weight_2,\n    ...\n}\n```\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"optimizer\": ...,\n    \"epoch\": ...,\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading a Gated Model with Access Token\nDESCRIPTION: Shows how to download a model that requires access approval using a Hugging Face access token with the --hf-token parameter.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ tune download meta-llama/Meta-Llama-3-8B-Instruct --hf-token <TOKEN>\nSuccessfully downloaded model repo and wrote to the following locations:\n./model/config.json\n./model/README.md\n./model/model-00001-of-00002.bin\n...\n```\n\n----------------------------------------\n\nTITLE: Inspecting HuggingFace Format Checkpoint in Python\nDESCRIPTION: Code example demonstrating how to load and inspect a HuggingFace format Llama2 checkpoint file, showing the differences in state dictionary structure compared to Meta format.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> state_dict = torch.load('pytorch_model-00001-of-00002.bin', mmap=True, weights_only=True, map_location='cpu')\n>>> # inspect the keys and the shapes of the associated tensors\n>>> for key, value in state_dict.items():\n>>>     print(f'{key}: {value.shape}')\n\nmodel.embed_tokens.weight: torch.Size([32000, 4096])\n...\n...\n>>> print(len(state_dict.keys()))\n241\n```\n\n----------------------------------------\n\nTITLE: Validating a Configuration File\nDESCRIPTION: Shows how to validate that a configuration file is properly formatted using the 'tune validate' command.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ tune validate my_configs/llama3/8B_full.yaml\nConfig is well-formed!\n```\n\n----------------------------------------\n\nTITLE: Running Quantization in Bash\nDESCRIPTION: Command to perform the convert step in the QAT flow, which quantizes the float model to a model with quantized weights.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\ntune run quantize --config custom_quantization.yaml\n```\n\n----------------------------------------\n\nTITLE: Copying a Config File for Customization using torchtune CLI\nDESCRIPTION: Command to copy a predefined configuration to a local file for customization. This allows users to make extensive modifications to the configuration before running the fine-tuning job.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/first_finetune_tutorial.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ tune cp llama2/7B_lora_single_device custom_config.yaml\nCopied file to custom_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Tokenizing with Llama2 Tokenizer\nDESCRIPTION: Demonstration of tokenizing formatted messages using the Llama2 tokenizer with special tokens.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.models.llama2 import llama2_tokenizer\n\ntokenizer = llama2_tokenizer(\"/tmp/Llama-2-7b-hf/tokenizer.model\")\nuser_message = formatted_messages[0].text_content\ntokens = tokenizer.encode(user_message, add_bos=True, add_eos=True)\nprint(tokens)\n```\n\n----------------------------------------\n\nTITLE: Displaying torchtune CLI Help\nDESCRIPTION: Shows how to display the help menu for the torchtune CLI using the --help flag, which lists all available commands and their descriptions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ tune --help\nusage: tune [-h] {download,ls,cp,run,validate} ...\n\nWelcome to the torchtune CLI!\n\noptions:\n-h, --help            show this help message and exit\n\nsubcommands:\n  {download,ls,cp,run,validate}\n    download            Download a model from the Hugging Face Hub.\n    ls                  List all built-in recipes and configs\n    ...\n```\n\n----------------------------------------\n\nTITLE: Enabling Sample Packing in Python, YAML, and Command Line\nDESCRIPTION: Shows how to enable sample packing for datasets to improve training efficiency using various configuration methods.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import alpaca_dataset, PackedDataset\n\n# Load in tokenizer\ntokenizer = ...\ndataset = alpaca_dataset(\n    tokenizer=tokenizer,\n    packed=True,\n)\nprint(isinstance(dataset, PackedDataset))  # True\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n  packed: True\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Command line\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\ndataset.packed=True\n```\n\n----------------------------------------\n\nTITLE: Validating Config Files with tune CLI\nDESCRIPTION: Shows how to use the tune validate CLI command to verify that a config file is well-formed and all components can be instantiated properly.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune cp llama2/7B_lora_single_device ./my_config.yaml\ntune validate ./my_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Downloading a Model with Custom Ignore Patterns\nDESCRIPTION: Demonstrates how to download a model while ignoring specific file patterns, such as including safetensor files that are ignored by default.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ tune download meta-llama/Meta-Llama-3-8B-Instruct --hf-token <TOKEN> --ignore-patterns None\nSuccessfully downloaded model repo and wrote to the following locations:\n./model/config.json\n./model/README.md\n./model/model-00001-of-00030.safetensors\n...\n```\n\n----------------------------------------\n\nTITLE: Recipe Training Loop Implementation\nDESCRIPTION: Core training loop implementation that handles forward/backward passes, optimization steps, and checkpoint saving.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/recipe_deepdive.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train(...):\n\n    self._optimizer.zero_grad()\n    for curr_epoch in range(self.epochs_run, self.total_epochs):\n\n        for idx, batch in enumerate(self._dataloader):\n            ...\n\n            with self._autocast:\n                logits = self._model(...)\n                ...\n                loss = self._loss_fn(logits, labels)\n\n            if self.global_step % self._log_every_n_steps == 0:\n                self._metric_logger.log_dict(...)\n\n            loss.backward()\n            self._optimizer.step()\n            self._optimizer.zero_grad()\n\n            # Update the number of steps when the weights are updated\n            self.global_step += 1\n\n        self.save_checkpoint(epoch=curr_epoch)\n```\n\n----------------------------------------\n\nTITLE: Expected output from torchtune help command\nDESCRIPTION: The expected output when running the torchtune help command, showing the CLI options and available commands.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nusage: tune [-h] {ls,cp,download,run,validate} ...\n\nWelcome to the torchtune CLI!\n\noptions:\n  -h, --help            show this help message and exit\n\n...\n```\n\n----------------------------------------\n\nTITLE: Running EleutherAI Evaluation in Bash\nDESCRIPTION: Command to execute the evaluation recipe with the custom configuration file.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntune run eleuther_eval --config my_eleuther_evaluation.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Dataset via Command Line\nDESCRIPTION: This bash command shows how to configure a custom dataset using torchtune's command-line interface. It specifies the dataset builder and max sequence length.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# Command line - local files\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\ndataset=torchtune.datasets.stack_exchanged_paired_dataset dataset.max_seq_len=512\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Dataset in YAML\nDESCRIPTION: This YAML configuration demonstrates how to use a custom dataset builder function in torchtune. It configures the Alpaca dataset using the stack_exchanged_paired_dataset builder.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\n# This is how you would configure the Alpaca dataset using the builder\ndataset:\n  _component_: torchtune.datasets.stack_exchanged_paired_dataset\n  max_seq_len: 512\n```\n\n----------------------------------------\n\nTITLE: Implementing QLoRA Linear Layer\nDESCRIPTION: Implementation of a quantized LoRA linear layer that supports base weight quantization using NF4Tensor from torchao. The layer combines frozen quantized weights with trainable low-rank adaptation matrices.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torchao.dtypes.nf4tensor import linear_nf4, to_nf4\n\nclass LoRALinear(nn.Module):\n  def __init__(\n    self,\n    in_dim: int,\n    out_dim: int,\n    rank: int,\n    alpha: float,\n    dropout: float,\n    quantize_base: bool\n  ):\n    # These are the weights from the original pretrained model\n    self.linear = nn.Linear(in_dim, out_dim, bias=False)\n    self.linear_weight = self.linear.weight\n    # Use torchao's to_nf4 API to quantize the base weight if needed.\n    if quantize_base:\n      self.linear_weight = to_nf4(self.linear_weight)\n    # These are the new LoRA params. In general rank << in_dim, out_dim\n    self.lora_a = nn.Linear(in_dim, rank, bias=False)\n    self.lora_b = nn.Linear(rank, out_dim, bias=False)\n\n    # Rank and alpha are commonly-tuned hyperparameters\n    self.rank = rank\n    self.alpha = alpha\n\n    # Most implementations also include some dropout\n    self.dropout = nn.Dropout(p=dropout)\n\n    # The original params are frozen, and only LoRA params are trainable.\n    self.linear.weight.requires_grad = False\n    self.lora_a.weight.requires_grad = True\n    self.lora_b.weight.requires_grad = True\n\n  def forward(self, x: Tensor) -> Tensor:\n    # frozen_out would be the output of the original model\n    if quantize_base:\n      # Call into torchao's linear_nf4 to run linear forward pass w/quantized weight.\n      frozen_out  = linear_nf4(x, self.weight)\n    else:\n      frozen_out = F.linear(x, self.weight)\n\n    # lora_a projects inputs down to the much smaller self.rank,\n    # then lora_b projects back up to the output dimension\n    lora_out = self.lora_b(self.lora_a(self.dropout(x)))\n\n    # Finally, scale by the alpha parameter (normalized by rank)\n    # and add to the original model's outputs\n    return frozen_out + (self.alpha / self.rank) * lora_out\n```\n\n----------------------------------------\n\nTITLE: Verifying torchtune installation with help command\nDESCRIPTION: Command to verify that torchtune is installed correctly by displaying the help information.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntune --help\n```\n\n----------------------------------------\n\nTITLE: Overriding configuration parameters via command line\nDESCRIPTION: Example showing how to override configuration parameters directly from the command line when running a torchtune fine-tuning recipe.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device \\\n--config llama2/7B_lora_single_device \\\nbatch_size=8 \\\nenable_activation_checkpointing=True \\\nmax_steps_per_epoch=128\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with Fine-tuned Llama3-8B Model\nDESCRIPTION: This snippet shows how to run text generation using the custom config file with the fine-tuned Llama3-8B model in torchtune.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntune run generate --config ./custom_generation_config.yaml \\\nprompt=\"Hello, my name is\"\n```\n\n----------------------------------------\n\nTITLE: Sample Chat Data Structure\nDESCRIPTION: Python dictionary structure showing how to format chat messages with system, user, and assistant roles.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsample = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful, respectful, and honest assistant.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Who are the most influential hip-hop artists of all time?\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Here is a list of some of the most influential hip-hop \"\n        \"artists of all time: 2Pac, Rakim, N.W.A., Run-D.M.C., and Nas.\",\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Installing torchtune via git clone\nDESCRIPTION: Installation method for developers who want the latest features or want to contribute to the project. This installs torchtune in development mode.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/install.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pytorch/torchtune.git\ncd torchtune\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Uploading Fine-tuned Model to Hugging Face Hub\nDESCRIPTION: Bash command to upload a fine-tuned model to the Hugging Face Hub using the huggingface-cli tool. This allows sharing the model with the community for collaborative use.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli upload <hf-repo-id> <checkpoint-dir>\n```\n\n----------------------------------------\n\nTITLE: Tokenizing with Llama3 Tokenizer\nDESCRIPTION: Example of tokenizing messages using the Llama3 tokenizer with its built-in special tokens handling.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.models.llama3 import llama3_tokenizer\n\ntokenizer = llama3_tokenizer(\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\")\nmessages = [Message.from_dict(msg) for msg in sample]\ntokens, mask = tokenizer.tokenize_messages(messages)\nprint(tokenizer.decode(tokens))\n```\n\n----------------------------------------\n\nTITLE: Running BEIR Benchmark Evaluation\nDESCRIPTION: Command to evaluate model performance on the BEIR benchmark using a specific dataset.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython beireval.py --model_name_or_path contriever-msmarco --dataset scifact\n```\n\n----------------------------------------\n\nTITLE: Formatting Messages with Llama2ChatFormat\nDESCRIPTION: Example of using the Llama2ChatFormat class to format chat messages with proper templating.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/chat.rst#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.data import Llama2ChatFormat, Message\n\nmessages = [Message.from_dict(msg) for msg in sample]\nformatted_messages = Llama2ChatFormat.format(messages)\nprint(formatted_messages)\n```\n\n----------------------------------------\n\nTITLE: Removing Config Fields with Command Line Overrides\nDESCRIPTION: Demonstrates how to remove specific parameters from a config when changing components through command-line overrides using the ~ flag.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# Change to PagedAdamW8bit and remove fused, foreach\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and torchvision for torchtune\nDESCRIPTION: Commands for installing stable or nightly versions of PyTorch and torchvision, which are prerequisites for using torchtune for fine-tuning multimodal LLMs.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Install stable version of PyTorch using pip\npip install torch torchvision\n\n# Nightly install for latest features\npip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu121\n```\n\n----------------------------------------\n\nTITLE: Copying Generation Config for Text Generation\nDESCRIPTION: This snippet demonstrates how to copy the default generation config for use in text generation with the fine-tuned Llama3-8B model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntune cp generation ./custom_generation_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Performing Passage Retrieval\nDESCRIPTION: Script command to retrieve top-100 passages using Contriever model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython passage_retrieval.py \\\n    --model_name_or_path facebook/contriever \\\n    --passages psgs_w100.tsv \\\n    --passages_embeddings \"contriever_embeddings/*\" \\\n    --data nq_dir/test.json \\\n    --output_dir contriever_nq\n```\n\n----------------------------------------\n\nTITLE: Overriding Config Parameters via Command Line\nDESCRIPTION: Demonstrates how to override configuration parameters directly from the command line using the key=value format when running a recipe.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntune run <RECIPE> --config <CONFIG> epochs=1\n```\n\n----------------------------------------\n\nTITLE: Original Optimizer Configuration\nDESCRIPTION: Shows an example optimizer configuration from a built-in config file that will be used in the following command-line override example.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\n# In configs/llama3/8B_full.yaml\noptimizer:\n  _component_: torch.optim.AdamW\n  lr: 2e-5\n  foreach: False\n```\n\n----------------------------------------\n\nTITLE: Installing torchtune nightly build\nDESCRIPTION: Installs the latest nightly build of torchtune with the most recent commits to the main branch. This method is useful for accessing the newest features without cloning the repository.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/install.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install --pre torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu --no-cache-dir\n```\n\n----------------------------------------\n\nTITLE: LoRA Model Configuration in YAML\nDESCRIPTION: YAML configuration for LoRA model settings including attention modules, rank, and alpha parameters.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# Model Arguments\nmodel:\n  _component_: lora_llama2_7b\n  lora_attn_modules: ['q_proj', 'v_proj']\n  lora_rank: 8\n  lora_alpha: 16\n```\n\n----------------------------------------\n\nTITLE: Retrieving Passages and Computing Accuracy\nDESCRIPTION: Performs passage retrieval and computes retrieval accuracy using mContriever model\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\npython passage_retrieval.py \\\n    --model_name_or_path facebook/mcontriever \\\n    --passages psgs_w100.tsv \\\n    --passages_embeddings \"mcontriever_embeddings/*\" \\\n    --data \"xmkqa/*.jsonl\" \\\n    --output_dir mcontriever_xmkqa \\\n    --lowercase --normalize_text \\\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation on Fine-tuned Model Using tune CLI\nDESCRIPTION: Executes the EleutherAI evaluation harness on the fine-tuned Llama2 7B model using the tune CLI and custom config.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune run eleuther_eval --config ./custom_eval_config.yaml\n```\n\n----------------------------------------\n\nTITLE: Recipe Config Parsing Decorator Usage\nDESCRIPTION: Example showing how to use the parse decorator to enable config and CLI override parsing for recipes.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/recipe_deepdive.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@config.parse\ndef recipe_main(cfg: DictConfig) -> None:\n    recipe = FullFinetuneRecipe(cfg=cfg)\n    recipe.setup(cfg=cfg)\n    recipe.train()\n    recipe.cleanup()\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies for ML Project\nDESCRIPTION: Core Python package dependencies including PyTorch 1.11.0, Hugging Face Transformers 4.18.0, and BEIR 1.0.0 for machine learning and information retrieval tasks.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\ntorch==1.11.0\ntransformers==4.18.0\nbeir==1.0.0\n```\n\n----------------------------------------\n\nTITLE: Running Recipe via Command Line\nDESCRIPTION: Command line syntax for running a recipe with config file and optional parameter overrides.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/recipe_deepdive.rst#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntune run <path/to/recipe> --config <path/to/config> k1=v1 k2=v2 ...\n```\n\n----------------------------------------\n\nTITLE: Running Evaluation on Base Model Using tune CLI\nDESCRIPTION: Executes the EleutherAI evaluation harness on the base Llama2 7B model using the tune CLI.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntune run eleuther_eval --config ./custom_eval_config.yaml\ncheckpointer.checkpoint_dir=<checkpoint_dir> \\\ntokenizer.path=<checkpoint_dir>/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: Evaluation Configuration for Quantized Models\nDESCRIPTION: YAML configuration for evaluating a quantized model, specifying the checkpointer component and quantization settings. This is used to modify the default eleuther_evaluation.yaml config.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/quantization.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Currently we only support torchtune checkpoints when\n# evaluating quantized models. For more details on checkpointing see\n# https://pytorch.org/torchtune/main/deep_dives/checkpointer.html\n# Make sure to change the default checkpointer component\ncheckpointer:\n  _component_: torchtune.utils.FullModelTorchTuneCheckpointer\n  ..\n  checkpoint_files: [<quantized_model_checkpoint>]\n\n# Quantization specific args\nquantizer:\n  _component_: torchtune.utils.quantization.Int8DynActInt4WeightQuantizer\n  groupsize: 256\n```\n\n----------------------------------------\n\nTITLE: Downloading Gemma-7B Model with torchtune\nDESCRIPTION: Command to download the Gemma-7B model using torchtune, excluding GGUF format files and requiring Hugging Face authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ntune download google/gemma-7b --ignore-patterns \"gemma-7b.gguf\"  --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama2 Model from Hugging Face Hub using torchtune CLI\nDESCRIPTION: Command to download the Llama2 7B model from Hugging Face Hub to a local directory using the torchtune CLI. Requires an access token for authentication since Llama2 is a gated model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/first_finetune_tutorial.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Llama-2-7b-hf \\\n  --output-dir /tmp/Llama-2-7b-hf \\\n  --hf-token <ACCESS TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Training Multilingual mContriever Model\nDESCRIPTION: Command for training the multilingual mContriever model using 32 GPUs, supporting 29 different languages. Uses bert-base-multilingual-cased as the base model with similar contrastive learning parameters but adjusted for multilingual support.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nTDIR=encoded-data/bert-base-multilingual-cased/\nTRAINDATASETS=\"${TDIR}fr_XX ${TDIR}en_XX ${TDIR}ar_AR ${TDIR}bn_IN ${TDIR}fi_FI ${TDIR}id_ID ${TDIR}ja_XX ${TDIR}ko_KR ${TDIR}ru_RU ${TDIR}sw_KE ${TDIR}hu_HU ${TDIR}he_IL ${TDIR}it_IT ${TDIR}km_KM ${TDIR}ms_MY ${TDIR}nl_XX ${TDIR}no_XX ${TDIR}pl_PL ${TDIR}pt_XX ${TDIR}sv_SE ${TDIR}te_IN ${TDIR}th_TH ${TDIR}tr_TR ${TDIR}vi_VN ${TDIR}zh_CN ${TDIR}zh_TW ${TDIR}es_XX ${TDIR}de_DE ${TDIR}da_DK\"\n\npython train.py \\\n        --retriever_model_id bert-base-multilingual-cased --pooling average \\\n        --train_data ${TRAINDATASETS} --loading_mode split \\\n        --ratio_min 0.1 --ratio_max 0.5 --chunk_length 256 \\\n        --momentum 0.999 --moco_queue 32768 --temperature 0.05 \\\n        --warmup_steps 20000 --total_steps 500000 --lr 0.00005 \\\n        --scheduler linear --optim adamw --per_gpu_batch_size 64 \\\n        --output_dir /checkpoint/gizacard/contriever/xling/mcontriever \\\n```\n\n----------------------------------------\n\nTITLE: Improved Training Performance with Torch Compile\nDESCRIPTION: Code snippet showing the improved training performance after enabling torch.compile, demonstrating approximately 3.95 iterations per second (about 200% speedup).\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n1|228|Loss: 0.8158286809921265:   1%|          | 228/25880 [11:59<1:48:16,  3.95it/s\n```\n\n----------------------------------------\n\nTITLE: Downloading Phi-3 Mini 4k Instruct Model with torchtune\nDESCRIPTION: Command to download the Phi-3 Mini 4k instruct model using torchtune, ignoring no file patterns and requiring Hugging Face authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntune download microsoft/Phi-3-mini-4k-instruct --ignore-patterns None --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Downloading Meta Llama3-8B-Instruct Model with torchtune\nDESCRIPTION: This snippet shows how to download the Meta Llama3-8B-Instruct model using torchtune's download command. It requires a Hugging Face access token.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/llama3.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Meta-Llama-3-8B-Instruct \\\n    --output-dir <checkpoint_dir> \\\n    --hf-token <ACCESS TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Building Index for PES2O Data\nDESCRIPTION: This code demonstrates how to build search indices for PES2O v3 data. It uses a for loop to potentially parallelize the indexing process across 16 shards, with each iteration configured to embed and index a specific shard of the data.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/build_pes2o_index.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndatastore_raw_data_path=$PES2O_V3_JSONL_DIR  # directory that contains the converted jsonl data\nnum_shards=16\n\nfor SLURM_ARRAY_TASK_ID in {0..15}; do  # we recommend to parallelize this with slurm jobs\nPYTHONPATH=.  python ric/main_ric.py \\\n  --config-name=pes2o_v3 \\\n  tasks.datastore.embedding=true \\\n  tasks.datastore.index=true \\\n  datastore.raw_data_path=$datastore_raw_data_path \\\n  datastore.embedding.num_shards=$num_shards \\\n  datastore.embedding.shard_ids=[$SLURM_ARRAY_TASK_ID] \\\n  hydra.job_logging.handlers.file.filename=embedding.log\ndone\n```\n\n----------------------------------------\n\nTITLE: Basic Dataset Component Configuration\nDESCRIPTION: Shows a basic dataset component configuration that will be referenced in the following command-line override example.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n```\n\n----------------------------------------\n\nTITLE: Downloading Meta-Llama3 70B Model Using HuggingFace Safetensors\nDESCRIPTION: Command to download the Meta-Llama-3.1-70B model using the HuggingFace safetensor format, with parameters for authentication token, output directory, and pattern filtering.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Meta-Llama-3.1-70b --hf-token <> --output-dir /tmp/Meta-Llama-3.1-70b --ignore-patterns \"original/consolidated*\"\n```\n\n----------------------------------------\n\nTITLE: Generating Sentence Embeddings\nDESCRIPTION: Shows how to generate embeddings for multiple sentences using the Contriever model and tokenizer.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsentences = [\n    \"Where was Marie Curie born?\",\n    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eug√®ne Curie, a doctor of French Catholic origin from Alsace.\"\n]\n\ninputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\nembeddings = model(**inputs)\n```\n\n----------------------------------------\n\nTITLE: Running Serve Script for API on Hyak\nDESCRIPTION: This snippet shows how to run the serve script that initializes the API server on Hyak.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/api/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython serve2.py\n```\n\n----------------------------------------\n\nTITLE: Downloading CodeLlama-7B Model with torchtune\nDESCRIPTION: Command to download the CodeLlama-7B model using the torchtune download utility, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntune download codellama/CodeLlama-7b-hf --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Computing Similarity Scores\nDESCRIPTION: Demonstrates how to compute similarity scores between sentence embeddings using dot product.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nscore01 = embeddings[0] @ embeddings[1] #1.0473\nscore02 = embeddings[0] @ embeddings[2] #1.0095\n```\n\n----------------------------------------\n\nTITLE: Recipe Class Initialization Method\nDESCRIPTION: Implementation of the recipe class initialization method that sets up recipe state including device, dtype, and other parameters.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/recipe_deepdive.rst#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(...):\n\n    self._device = utils.get_device(device=params.device)\n    self._dtype = utils.get_dtype(dtype=params.dtype, device=self._device)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Downloading Mistral-7B-v0.1 Model with torchtune\nDESCRIPTION: Command to download the Mistral-7B-v0.1 model using the torchtune download utility, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntune download mistralai/Mistral-7B-v0.1 --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Loading Different Contriever Variants\nDESCRIPTION: Shows how to load different pre-trained variants of Contriever including multilingual and MSMARCO-finetuned versions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom src.contriever import Contriever\n\ncontriever = Contriever.from_pretrained(\"facebook/contriever\") \ncontriever_msmarco = Contriever.from_pretrained(\"facebook/contriever-msmarco\")\nmcontriever = Contriever.from_pretrained(\"facebook/mcontriever\")\nmcontriever_msmarco = Contriever.from_pretrained(\"facebook/mcontriever-msmarco\")\n```\n\n----------------------------------------\n\nTITLE: Copying Model Files for GPT-Fast Integration\nDESCRIPTION: Bash commands to copy the necessary model files (checkpoints and tokenizer) from the TorchTune output directory to a new directory structure compatible with GPT-Fast.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncp  <checkpoint_dir>/hf_model_0001_0.pt  <new_dir>/Llama-2-7B-hf/\ncp  <checkpoint_dir>/hf_model_0002_0.pt  <new_dir>/Llama-2-7B-hf/\ncp  <checkpoint_dir>/tokenizer.model     <new_dir>/Llama-2-7B-hf/\n```\n\n----------------------------------------\n\nTITLE: Downloading Gemma-2B Model with torchtune\nDESCRIPTION: Command to download the Gemma-2B model using torchtune, ignoring no file patterns and requiring Hugging Face authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntune download google/gemma-2b --ignore-patterns None  --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Generating Passage Embeddings\nDESCRIPTION: Script command to generate embeddings for Wikipedia passages using Contriever.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython generate_passage_embeddings.py \\\n    --model_name_or_path facebook/contriever \\\n    --output_dir contriever_embeddings  \\\n    --passages psgs_w100.tsv \\\n    --shard_id 0 --num_shards 1\n```\n\n----------------------------------------\n\nTITLE: Copying a configuration file for local modification\nDESCRIPTION: Command to copy a predefined torchtune configuration to a local file for customization, allowing for more extensive modifications.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/README.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ntune cp llama2/7B_full ./my_custom_config.yaml\nCopied to ./7B_full.yaml\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama2-70B Model with torchtune\nDESCRIPTION: Command to download the Llama2-70B model using the torchtune download utility, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Llama-2-70b-hf --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Sequence Length in Python, YAML, and Command Line\nDESCRIPTION: Demonstrates how to set an upper limit on the maximum sequence length for dataset samples using different configuration methods.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.datasets import alpaca_dataset\n\n# Load in tokenizer\ntokenizer = ...\ndataset = alpaca_dataset(\n    tokenizer=tokenizer,\n    max_seq_len=4096,\n)\n```\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n  max_seq_len: 4096\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Command line\ntune run full_finetune_single_device --config llama3/8B_full_single_device \\\ndataset.max_seq_len=4096\n```\n\n----------------------------------------\n\nTITLE: Best Practice: Use Public APIs Only\nDESCRIPTION: Shows how to properly reference components using their public API paths rather than internal implementation paths, ensuring API stability.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n# don't do this\ndataset:\n  _component_: torchtune.datasets._alpaca.alpaca_dataset\n\n# do this\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama3-8B-Instruct Model with torchtune\nDESCRIPTION: Command to download the Llama3-8B-Instruct model using the torchtune download utility, requiring a Hugging Face token for authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Meta-Llama-3-8B-Instruct --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Listing Built-in Recipes and Configs\nDESCRIPTION: Shows how to list all the built-in recipes and configuration files available in torchtune using the 'tune ls' command.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ tune ls\nRECIPE                                   CONFIG\nfull_finetune_single_device              llama2/7B_full_low_memory\n                                         code_llama2/7B_full_low_memory\n                                         llama3/8B_full_single_device\n                                         mistral/7B_full_low_memory\n                                         phi3/mini_full_low_memory\nfull_finetune_distributed                llama2/7B_full\n                                         llama2/13B_full\n                                         llama3/8B_full\n                                         llama3/70B_full\n...\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys for OpenScholar\nDESCRIPTION: Shell commands to set necessary API keys for OpenScholar, including Semantic Scholar API and optional web search API keys.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nexport S2_API_KEY=YOUR_S2_API_KEY\n```\n\nLANGUAGE: sh\nCODE:\n```\nexport YOUR_API_KEY=YOUR_YOU_COM_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple In-Memory Datasets in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up multiple in-memory datasets using torchtune's ConcatDataset interface. It includes examples of instruct and chat datasets.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/datasets.rst#2025-04-21_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# YAML config\ndataset:\n  - _component_: torchtune.datasets.instruct_dataset\n    source: vicgalle/alpaca-gpt4\n    template: torchtune.data.AlpacaInstructTemplate\n    split: train\n    train_on_input: True\n  - _component_: torchtune.datasets.instruct_dataset\n    source: samsum\n    template: torchtune.data.SummarizeTemplate\n    column_map:\n      output: summary\n    split: train\n    train_on_input: False\n  - _component_: torchtune.datasets.chat_dataset\n    ...\n```\n\n----------------------------------------\n\nTITLE: Downloading Llama3-70B-Instruct Model with torchtune\nDESCRIPTION: Command to download the Llama3-70B-Instruct model using torchtune, with pattern exclusion to avoid downloading consolidated files and requiring Hugging Face authentication.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Meta-Llama-3-70B-Instruct --ignore-patterns \"original/consolidated*\" --hf-token <HF_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Copying a Built-in Recipe or Config\nDESCRIPTION: Demonstrates how to copy a built-in recipe or configuration file to a local directory using the 'tune cp' command for customization.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tune_cli.rst#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ tune cp lora_finetune_distributed .\nCopied file to ./lora_finetune_distributed.py\n```\n\n----------------------------------------\n\nTITLE: Running Standard RAG Pipeline with OpenScholar\nDESCRIPTION: Command to run the standard Retrieval-Augmented Generation pipeline using the top 10 retrieved documents with the OpenScholar model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython run.py \\\n    --input_file YOUR_INPUT_FILE \\\n    --model_name OpenScholar/Llama-3.1_OpenScholar-8B \\\n    --use_contexts \\\n    --output_file OUTPUT_FILE_PATH \\\n    --top_n 10 --llama3 --zero_shot\n```\n\n----------------------------------------\n\nTITLE: Downloading Mr. TyDi Swahili Dataset\nDESCRIPTION: Downloads and extracts the Swahili language dataset from Mr. TyDi v1.1 collection\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nwget https://git.uwaterloo.ca/jimmylin/mr.tydi/-/raw/master/data/mrtydi-v1.1-swahili.tar.gz -P mrtydi\ntar -xf mrtydi/mrtydi-v1.1-swahili.tar.gz -C mrtydi\ngzip -d mrtydi/mrtydi-v1.1-swahili/collection/docs.jsonl.gz\n```\n\n----------------------------------------\n\nTITLE: Downloading Qwen2-1.5B Model with torchtune\nDESCRIPTION: Command to download the Qwen2-1.5B model using torchtune, specifying an output directory and ignoring no file patterns.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_models.rst#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntune download Qwen/Qwen2-1.5B-Instruct --output-dir /tmp/Qwen2-1.5B-Instruct --ignore-patterns None\n```\n\n----------------------------------------\n\nTITLE: Copying Evaluation Config Using tune CLI\nDESCRIPTION: Copies the default EleutherAI evaluation config to a local file for customization.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntune cp eleuther_evaluation ./custom_eval_config.yaml \\\n```\n\n----------------------------------------\n\nTITLE: Killing Straggler wandb Processes in Bash\nDESCRIPTION: Bash command to identify and terminate straggler wandb processes that may remain running in the background after a job crashes or exits without proper cleanup.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/wandb_logging.rst#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nps -aux | grep wandb | awk '{ print $2 }' | xargs kill\n```\n\n----------------------------------------\n\nTITLE: Running Retriever+Reranker Pipeline with OpenScholar\nDESCRIPTION: Command to run the OpenScholar pipeline with a dedicated reranker model to improve retrieval quality.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython run.py \\\n    --input_file YOUR_INPUT_FILE \\\n    --model_name OpenScholar/Llama-3.1_OpenScholar-8B \\\n    --use_contexts \\\n    --ranking_ce \\\n    --reranker OpenScholar/OpenScholar_Reranker \\\n    --output_file OUTPUT_FILE_PATH \\\n    --top_n 10 --llama3 --zero_shot\n```\n\n----------------------------------------\n\nTITLE: Converting Mr. TyDi Data to BEIR Format\nDESCRIPTION: Converts the Mr. TyDi Swahili dataset to BEIR format for evaluation\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython data_scripts/convertmrtydi2beir.py mrtydi/mrtydi-v1.1-swahili mrtydi/mrtydi-v1.1-swahili\n```\n\n----------------------------------------\n\nTITLE: Documenting TorchTune Configuration Functions in RST Format\nDESCRIPTION: This RST format documentation outlines the torchtune.config module's key functions. It uses Sphinx autosummary directive to generate documentation for instantiate, parse, validate, and log_config functions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_config.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _config:\n\n================\ntorchtune.config\n================\n\n.. currentmodule:: torchtune.config\n\n.. autosummary::\n    :toctree: generated/\n    :nosignatures:\n\n    instantiate\n    parse\n    validate\n    log_config\n```\n\n----------------------------------------\n\nTITLE: Modifying Evaluation Config for Fine-tuned Model\nDESCRIPTION: Updates the YAML config file to point to the fine-tuned model checkpoints and tokenizer.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ncheckpointer:\n    _component_: torchtune.utils.FullModelHFCheckpointer\n\n    # directory with the checkpoint files\n    # this should match the output_dir specified during\n    # finetuning\n    checkpoint_dir: <checkpoint_dir>\n\n    # checkpoint files for the fine-tuned model. This should\n    # match what's shown in the logs above\n    checkpoint_files: [\n        hf_model_0001_0.pt,\n        hf_model_0002_0.pt,\n    ]\n\n    output_dir: <checkpoint_dir>\n    model_type: LLAMA2\n\n# Make sure to update the tokenizer path to the right\n# checkpoint directory as well\ntokenizer:\n    _component_: torchtune.models.llama2.llama2_tokenizer\n    path: <checkpoint_dir>/tokenizer.model\n```\n\n----------------------------------------\n\nTITLE: Using Proprietary LLMs with OpenScholar Pipeline\nDESCRIPTION: Command to use OpenScholar's retrieval and ranking with proprietary language models like GPT-4o by specifying API details.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/README.md#2025-04-21_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython run.py \\\n    --input_file YOUR_INPUT_FILE \\\n    --model_name \"gpt-4o\" \\\n    --api \"openai\" \\\n    --api_key_fp PATH_TO_YOUR_OPEN_AI_KEY \\ \n    --use_contexts \\\n    --output_file OUTPUT_FILE_PATH \\\n    --top_n 10 --llama3 --zero_shot\n```\n\n----------------------------------------\n\nTITLE: Verifying torchtune installation\nDESCRIPTION: A command to verify that torchtune has been installed correctly by running the CLI command.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/install.rst#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntune\n```\n\n----------------------------------------\n\nTITLE: Building Index for PES2O Data with Parallel Processing\nDESCRIPTION: Script for building a searchable index from PES2O v3 data. It sets up parallel processing across 16 shards and runs the indexing process for each shard. The script handles both embedding generation and index creation.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndatastore_raw_data_path=$PES2O_V3_JSONL_DIR  # directory that contains the converted jsonl data\nnum_shards=16\n\nfor SLURM_ARRAY_TASK_ID in {0..15}; do  # we recommend to parallelize this with slurm jobs\nPYTHONPATH=.  python ric/main_ric.py \\\n  --config-name=pes2o_v3 \\\n  tasks.datastore.embedding=true \\\n  tasks.datastore.index=true \\\n  datastore.raw_data_path=$datastore_raw_data_path \\\n  datastore.embedding.num_shards=$num_shards \\\n  datastore.embedding.shard_ids=[$SLURM_ARRAY_TASK_ID] \\\n  hydra.job_logging.handlers.file.filename=embedding.log\ndone\n```\n\n----------------------------------------\n\nTITLE: Generation Output from TorchTune LLM\nDESCRIPTION: Sample output from running text generation with TorchTune, showing the model's response about Bay Area attractions along with performance metrics like inference time and memory usage.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n[generate.py:92] Exploratorium in San Francisco has made the cover of Time Magazine,\n                 and its awesome. And the bridge is pretty cool...\n\n[generate.py:96] Time for inference: 11.61 sec total, 25.83 tokens/sec\n[generate.py:99] Memory used: 15.72 GB\n```\n\n----------------------------------------\n\nTITLE: Model Download Command for HuggingFace Format\nDESCRIPTION: Bash command to download a model from HuggingFace Hub using tune download utility, excluding safetensors files.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/checkpointer.rst#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntune download meta-llama/Llama-2-7b-hf \\\n--output-dir <checkpoint_dir> \\\n--hf-token <hf-token>\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment for PES2O Data Retrieval\nDESCRIPTION: This snippet shows how to clone the retrieval-scaling repository, checkout the pes2o branch, create a new conda environment, and install dependencies.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/build_pes2o_index.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/RulinShao/retrieval-scaling.git\ncd retrieval-scaling\ngit checkout pes2o\n\n# create a new conda environment and install dependencies\nconda env create -f environment.yml\nconda activate scaling\n```\n\n----------------------------------------\n\nTITLE: Running Self-reflective Generation Pipeline with OpenScholar\nDESCRIPTION: Command to run the OpenScholar self-reflective generation pipeline that includes reranking, feedback loops, and citation normalization.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/README.md#2025-04-21_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython run.py \\\n    --input_file YOUR_INPUT_FILE \\\n    --model_name  OpenScholar/Llama-3.1_OpenScholar-8B \\\n    --use_contexts --output_file OUTPUT_FILE_NAME \\\n    --top_n 10 --llama3 --use_contexts \\\n    --ranking_ce --reranker OpenScholar/OpenScholar_Reranker \\ \n    --posthoc --feedack --ss_retriever \\\n    --use_abstract --norm_cite --zero_shot --max_per_paper 3\n```\n\n----------------------------------------\n\nTITLE: Cloning TorchTune Repository\nDESCRIPTION: Command to clone the forked TorchTune repository from GitHub.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: git\nCODE:\n```\ngit clone https://github.com/<YOUR_GITHUB_USER>/torchtune.git\n```\n\n----------------------------------------\n\nTITLE: Best Practice: Use Single Dataset Entry\nDESCRIPTION: Example showing the recommended approach for configuring components with multiple alternatives, encouraging clear and focused configs.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/configs.rst#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n# dont do this\nalpaca_dataset:\n  _component_: torchtune.datasets.alpaca_dataset\nslimorca_dataset:\n  ...\n\n# do this\ndataset:\n  # change this in config or override when needed\n  _component_: torchtune.datasets.alpaca_dataset\n```\n\n----------------------------------------\n\nTITLE: Installing Weights & Biases and Logging In via Bash\nDESCRIPTION: Commands to install the wandb package via pip and log in with your API key using the W&B CLI.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/wandb_logging.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install wandb\n```\n\nLANGUAGE: bash\nCODE:\n```\nwandb login\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Upload Success Output\nDESCRIPTION: Example output after successfully uploading a model to the Hugging Face Hub, showing the URL to the repository where the model is now available.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/e2e_flow.rst#2025-04-21_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nhttps://huggingface.co/<hf-repo-id>/tree/main/.\n```\n\n----------------------------------------\n\nTITLE: Setting Trainable Parameters for LoRA\nDESCRIPTION: Code snippet showing the imports needed for managing trainable parameters in a LoRA model using torchtune's utility functions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/lora_finetune.rst#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params\n```\n\n----------------------------------------\n\nTITLE: Installing torchtune via PyPI\nDESCRIPTION: Installs the latest stable version of torchtune from PyPI. This is the recommended method for most users.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/install.rst#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torchtune\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-computed Embeddings\nDESCRIPTION: Downloads pre-computed passage embeddings for mContriever and mContriever-msmarco models\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nwget https://dl.fbaipublicfiles.com/contriever/embeddings/mcontriever/wikipedia_embeddings.tar\nwget https://dl.fbaipublicfiles.com/contriever/embeddings/mcontriever-msmarco/wikipedia_embeddings.tar\n```\n\n----------------------------------------\n\nTITLE: Building Documentation\nDESCRIPTION: Command to build HTML documentation from source files.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n# Now open build/html/index.html\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Command to install documentation-related dependencies from requirements.txt.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Contriever Citation Format\nDESCRIPTION: BibTeX citation format for the Contriever paper 'Unsupervised Dense Information Retrieval with Contrastive Learning'\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_19\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{izacard2021contriever,\n      title={Unsupervised Dense Information Retrieval with Contrastive Learning}, \n      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},\n      year={2021},\n      url = {https://arxiv.org/abs/2112.09118},\n      doi = {10.48550/ARXIV.2112.09118},\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Training Performance with PyTorch\nDESCRIPTION: Code snippet showing the training performance of a language model without optimization, displaying the iteration speed at approximately 1.15 iterations per second.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n1|149|Loss: 0.9157477021217346:   1%|          | 149/25880 [02:08<6:14:19,  1.15it/s\n```\n\n----------------------------------------\n\nTITLE: Initializing CKeditor Configuration for OpenScholar in JavaScript\nDESCRIPTION: This script configures CKeditor for the OpenScholar platform, setting up toolbar customizations, responsive behavior, and plugin integration. It defines different toolbar configurations based on screen width and user permissions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/deep_dives/README.txt#2025-04-21_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n(function ($) {\n  Drupal.behaviors.os_wysiwyg = {\n    attach: function (context) {\n      if (!CKEDITOR) {\n        return;\n      }\n\n      var wysiwyg_settings = Drupal.settings.os_wysiwyg;\n\n      CKEDITOR.on('dialogDefinition', function(e) {\n        // Take the dialog name and its definition from the event data.\n        var dialogName = e.data.name;\n        var dialogDefinition = e.data.definition;\n\n        if (dialogName == 'table' || dialogName == 'tableProperties') {\n          var infoTab = dialogDefinition.getContents('info');\n          var selects = ['cellSpacing', 'cellPadding', 'border'];\n          for (var ix in selects) {\n            var select = infoTab.get(selects[ix]);\n            select['default'] = '0';\n            if (selects[ix] == 'border') {\n              select.commit = function(data, selectedElement) {\n                selectedElement.setAttribute(this.id, this.getValue().length ? this.getValue() : '0');\n              };\n            }\n          }\n        }\n\n        $('#content-wrapper').addClass('os-wysiwyg-active');\n\n        for (var i in dialogDefinition.contents) {\n          var contents = dialogDefinition.contents[i];\n          if (contents && contents.elements) {\n            for (var j in contents.elements) {\n              var element = contents.elements[j];\n              if (element && element.type == 'hbox' && element.widths) {\n                delete element.widths;\n              }\n            }\n          }\n        }\n\n        if(dialogName == 'image') {\n          // Get a reference to the \"Link\" tab.\n          var linkTab = dialogDefinition.getContents('Link');\n          var advTab = dialogDefinition.getContents('advanced');\n          // update the existing \"Link Tab\" to have the browser target turned on by default\n          // note that with this CKEditor doesn't really send an null value vs a _blank value correctly\n          // null means open in current window, _blank means open in new window\n          // both result in the same output: <a href=\"link.html\" target=\"_blank\">link</a>\n          var target = linkTab.get('target');\n          target.items = [\n            [ 'Open in current window', ''],\n            [ 'Open in new window', '_blank' ]\n          ];\n        }\n\n      });\n\n      CKEDITOR.on('instanceReady', function(e) {\n        var editor = e.editor;\n\n        // Set the configuration settings for the language detection plugin\n        editor.config.googlegeolangdetectorOpts = {\n          affectsApplication: false,\n          affectsInstance: true,\n          updateInstantly: false,\n          useSelected: true,\n          detectOnPaste: true,\n          charsThreshold: 40,\n          callback: function( detail, editor ) {\n              var locale = Drupal.settings.os_wysiwyg.locale || 'en';\n              var hasLang = ((typeof detail === 'object') && detail !== null && (detail.language || false)),\n                  newLang = (hasLang && detail.language.toLowerCase()) || locale,\n                  msgLang = (hasLang) ? detail.language : locale;\n              // update the editor\n              editor.config.contentsLanguage = newLang;\n              editor.config.contentsLangDirection = (detail && detail.dir) || 'ltr';\n\n              // update the editor UI Direction\n              editor.container.getDirection = function() {\n                var langDir = editor.config.contentsLangDirection;\n                var elem = this.$,\n                  win = elem.ownerDocument.defaultView || elem.ownerDocument.parentWindow;\n\n                return langDir || win.getComputedStyle( elem, \"\").direction || elem.currentStyle.direction || editor.container.getParent().getDirection( 1 ) || editor.container.getDirection( 1 ) || \"ltr\";\n              };\n\n              // hide the notification\n              var notification = editor._.notificationArea.notifications[0];\n              if (notification && notification.type == 'info' && notification.message.indexOf(\"Language\") >= 0) {\n                notification.hide();\n              }\n\n              if (hasLang) {\n                // don't add the markup if there's already language markup\n                // for backward compatibility since we used to show a notification\n                // console.log(\"Language detected: \" + msgLang);\n              }\n          }\n        };\n\n        // always filter out the notification prompt\n        editor.on('notificationShow', function(evt) {\n          var notification = evt.data.notification;\n\n          if (notification.type == 'info' && notification.message.indexOf(\"Language\") >= 0) {\n            evt.cancel();\n          }\n        });\n\n        //Fix jquery ui dialogs and other iframe-in-dialog situations\n        $(editor.element.$).parents('.ui-dialog').css('z-index', 9997);\n\n        $('#content-wrapper').removeClass('os-wysiwyg-active');\n      });\n\n      Drupal.behaviors.os_wysiwyg.initPannels(context);\n\n      Drupal.behaviors.os_wysiwyg.sectionOverride = function(event) {\n        var $menu = $(event.target).closest('.toolbar-menu');\n        if (!$menu.length) {\n          return;\n        }\n\n        var $li = $menu.find('.ckeditor-toc-button').closest('li');\n        var $ul = $li.closest('ul');\n\n        if ($ul.closest('li').length && !$ul.hasClass('dropdown-menu')) {\n          $ul.addClass('dropdown-menu');\n        }\n\n        var open = $menu.find('.open');\n        if (open.find('ul:visible').length == 0) {\n          open.removeClass('open');\n          open.parents('li.open').removeClass('open').find('a[aria-expanded]').attr('aria-expanded', false);\n        }\n      };\n\n      $('body').on('mouseup', Drupal.behaviors.os_wysiwyg.sectionOverride);\n\n      // Open the media browser function\n      Drupal.behaviors.os_wysiwyg.mediaBrowser = function() {\n        // Drupal.behaviors.media provided by os_editor_entity module\n        if (Drupal.behaviors.media === undefined) return;\n        var mediaIframe = Drupal.media.popups.mediaIframe(Drupal.settings.os_wysiwyg.media.url, function (mediaFiles) {\n          var fid = mediaFiles[0].fid;\n          var element = CKEDITOR.dom.element.createFromHtml('<img src=\"\" alt=\"\" data-fid=\"' + fid + '\" />');\n          var widget = CKEDITOR.currentInstance.widgets.initOn(element, 'embeddedContent');\n        });\n      };\n\n      function checkMenuOverflow(jqObj) {\n        if (!jqObj.length) {\n          return;\n        }\n\n        jqObj.css({\n          'max-width': 'none',\n          'width': 'auto'\n        }).removeClass('os-responsive');\n\n        // If too wide, convert to responsive\n        var menuRect = jqObj[0].getBoundingClientRect();\n        var itemsVisibleWidth = menuRect.width;\n        var itemsFullWidth = 0;\n\n        // Calculate the full width of all visible items\n        jqObj.children('li:visible').each(function () {\n          var itemRect = this.getBoundingClientRect();\n          itemsFullWidth += itemRect.width;\n        });\n\n        // If the items are wider than the container, switch to responsive mode\n        if (itemsFullWidth > itemsVisibleWidth) {\n          jqObj.addClass('os-responsive');\n          var visibleWidth = 0;\n          var addedToDropdown = false;\n\n          // Process each item to determine what goes in dropdown\n          jqObj.children('li:visible').each(function () {\n            var $item = $(this);\n            var itemRect = this.getBoundingClientRect();\n            var itemWidth = itemRect.width;\n\n            if (!addedToDropdown && (visibleWidth + itemWidth < itemsVisibleWidth - 50)) {\n              // This item fits, so it stays visible in the main bar\n              visibleWidth += itemWidth;\n              $item.removeClass('os-overflow-item');\n            } else {\n              // This item doesn't fit, add to dropdown\n              $item.addClass('os-overflow-item');\n              addedToDropdown = true;\n            }\n          });\n        }\n      }\n\n      function mediaButtons() {\n        return (Drupal.settings.os_wysiwyg && Drupal.settings.os_wysiwyg.media) ? {\n          items : ['Image', 'oEmbedContent', 'Table', 'CreateDiv', 'HorizontalRule', 'SpecialChar'],\n          toolbar_media : [\n            ['Image', 'oEmbedContent', 'Table', 'CreateDiv', 'HorizontalRule', 'SpecialChar']\n          ],\n          extraPlugins : 'image2,widget,lineutils,widgetbootstrap',\n          toolbar_media_stacked : [\n            ['Image'], ['oEmbedContent'], ['Table'], ['CreateDiv'], ['HorizontalRule'], ['SpecialChar']\n          ]\n        } : {\n          items : ['Table', 'CreateDiv', 'HorizontalRule', 'SpecialChar'],\n          toolbar_media : [\n            ['Table', 'CreateDiv', 'HorizontalRule', 'SpecialChar']\n          ],\n          toolbar_media_stacked : [\n            ['Table'], ['CreateDiv'], ['HorizontalRule'], ['SpecialChar']\n          ]\n        };\n      }\n\n      function configFix(config, section) {\n        if (config.toolbar && config.toolbar.length) {\n          if (section == \"toolbar_media\" || section == \"toolbar_media_stacked\") {\n            var mediaConf = mediaButtons();\n            config.toolbar = config.toolbar.concat(mediaConf[section]);\n          }\n        }\n        return config;\n      }\n\n      // Toolbar responsiveness\n      var prevWidth = window.innerWidth;\n      CKEDITOR.on('instanceReady', function (e) {\n        var editor = e.editor;\n        if (!editor || !editor.ui || !editor.ui.instances) {\n          return;\n        }\n\n        var $toolbar = $(editor.ui.space('toolbar').$);\n        var $toolbarMenus = $toolbar.find('ul.cke_toolgroup, td.cke_toolbar_start + ul.toolbar-menu');\n\n        var windowCheckTimeout = false;\n        function checkToolbarMenus() {\n          // This handles collapsing of the toolbars when they get too wide\n          $toolbarMenus.each(function () {\n            checkMenuOverflow($(this));\n          });\n\n          // This enables responsive mode on mobile/small screens\n          var width = window.innerWidth;\n          if (width !== prevWidth) {\n            prevWidth = width;\n            var config = editor.config;\n\n            if (width <= config.smallScreenWidth) {\n              // On small screens, switch to stacked buttons layout\n              editor.setToolbar('Small');\n            } else {\n              // On larger screens, use normal toolbar\n              editor.setToolbar('Full');\n            }\n          }\n        }\n\n        function scheduleCheck() {\n          if (windowCheckTimeout) {\n            clearTimeout(windowCheckTimeout);\n          }\n          windowCheckTimeout = setTimeout(checkToolbarMenus, 100);\n        }\n\n        $(window).on('resize.cke-toolbar', scheduleCheck);\n        scheduleCheck();\n\n        editor.on('destroy', function() {\n          $(window).off('resize.cke-toolbar');\n        });\n\n        // Handle overflow menus in toolbars\n        $toolbar.on('mouseenter', '.os-responsive', function() {\n          var $menu = $(this);\n          var $overflowItems = $menu.children('li.os-overflow-item');\n          \n          if ($overflowItems.length && !$menu.find('.os-toolbar-dropdown').length) {\n            // Create dropdown button and menu\n            var $dropdown = $('<li class=\"os-toolbar-dropdown cke_button\"><a href=\"#\"><span class=\"cke_button_icon os-toolbar-dropdown-icon\">‚ãÆ</span></a><ul class=\"os-toolbar-dropdown-menu\"></ul></li>');\n            $menu.append($dropdown);\n            \n            // Move overflow items to dropdown menu\n            $overflowItems.each(function() {\n              var $item = $(this).clone();\n              $dropdown.find('.os-toolbar-dropdown-menu').append($item);\n            });\n            \n            // Handle dropdown toggle\n            $dropdown.find('> a').on('click', function(e) {\n              e.preventDefault();\n              e.stopPropagation();\n              $dropdown.toggleClass('open');\n            });\n          }\n        });\n        \n        // Close dropdowns when clicking elsewhere\n        $(document).on('mousedown', function(e) {\n          if (!$(e.target).closest('.os-toolbar-dropdown').length) {\n            $('.os-toolbar-dropdown.open').removeClass('open');\n          }\n        });\n      });\n\n      CKEDITOR.on('instanceCreate', function (e) {\n        var editor = e.editor;\n        \n        // Set responsive breakpoints\n        editor.config.smallScreenWidth = 768;\n        editor.config.mediumScreenWidth = 992;\n\n        // Define toolbar configurations\n        editor.config.toolbar_Full = [\n          ['Bold', 'Italic', 'Strike', 'Subscript', 'Superscript'],\n          ['JustifyLeft', 'JustifyCenter', 'JustifyRight'],\n          ['BulletedList', 'NumberedList', 'Indent', 'Outdent', 'Blockquote'],\n          ['Link', 'Unlink'],\n          ['Format'],\n          ['PasteText', 'PasteFromWord'],\n          ['Sourcedialog']\n        ];\n        \n        editor.config.toolbar_Small = [\n          ['Bold', 'Italic', 'Strike'],\n          ['Subscript', 'Superscript'],\n          ['JustifyLeft', 'JustifyCenter', 'JustifyRight'],\n          ['BulletedList', 'NumberedList'],\n          ['Indent', 'Outdent'],\n          ['Blockquote'],\n          ['Link', 'Unlink'],\n          ['Format'],\n          ['PasteText', 'PasteFromWord'],\n          ['Sourcedialog']\n        ];\n\n        // Fix toolbar configuration for both sizes\n        editor.config = configFix(editor.config, 'toolbar_media');\n        editor.config = configFix(editor.config, 'toolbar_media_stacked');\n\n        editor.setToolbar = function(size) {\n          if (this.mode != 'source') {\n            var toolbar = this.config['toolbar_' + size];\n            this.setActiveEnterMode(CKEDITOR.ENTER_P, CKEDITOR.ENTER_BR);\n            this.ui.space('toolbar').getDocument().getById(this.ui.spaceId('toolbar')).setHtml(this.ui.guiValue('toolbar', toolbar));\n            this.fireOnce('uiReady');\n          }\n        };\n      });\n    },\n    \n    // Initialize OS WYSIWYG for panel content\n    initPannels: function (ctx) {\n      $('.os-slider-widget-box', ctx).once('os-slider-wysiwyg', function () {\n        var $box = $(this);\n        var $textarea = $box.find('textarea.form-textarea:visible');\n        \n        if ($textarea.length) {\n          $textarea.addClass('os-wysiwyg');\n        }\n      });\n    }\n  };\n})(jQuery);\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Conda Environment for PES2O Retrieval System\nDESCRIPTION: Commands to navigate to the retrieval-scaling directory, create a new conda environment from the provided environment.yml file, and activate the environment for working with the PES2O data.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd retrieval-scaling\n\n# create a new conda environment and install dependencies\nconda env create -f environment.yml\nconda activate scaling\n```\n\n----------------------------------------\n\nTITLE: Specifying Documentation Dependencies for OpenScholar\nDESCRIPTION: This requirements file lists the Python packages needed to build the OpenScholar documentation. It includes Sphinx and its extensions for documentation generation, along with packages for styling, interactivity, and visualization. The file pins specific versions for some packages to ensure compatibility.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx-gallery>0.11\nsphinx==5.0.0\nsphinx_design\nsphinx_copybutton\nsphinx-tabs\nmatplotlib\n-e git+https://github.com/pytorch/pytorch_sphinx_theme.git#egg=pytorch_sphinx_theme\n```\n\n----------------------------------------\n\nTITLE: Running Mistral Model Comparison Script in Python\nDESCRIPTION: Command for executing the comparison script that verifies the Mistral implementation against reference implementations. The script outputs values used in associated unit tests to confirm correctness.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/tests/torchtune/models/mistral/scripts/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m tests.torchtune.models.mistral.scripts.compare_mistral\n```\n\n----------------------------------------\n\nTITLE: Handling Menu Icons in OpenScholar PHP\nDESCRIPTION: Function to display menu icons in a site's primary menu. It receives a menu item and returns HTML for displaying an appropriate icon based on the item type and configuration.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/README.txt#2025-04-21_snippet_1\n\nLANGUAGE: PHP\nCODE:\n```\nfunction scholar_site_frontpage_menu_icon($menu_item = NULL) {\n  // Get preset menu display options\n  $menu_display = variable_get('openscholar_menu_category_display', array_fill_keys(array_keys(os_get_menus(TRUE)), 'inline'));\n  $links_display = variable_get('openscholar_menus_links_display', array_fill_keys(array_keys(os_get_menus(TRUE)), ''));\n\n  $menu = menu_get_active_menu_name();\n  $mid = (_menu_get_mid_by_name($menu) !== FALSE) ? _menu_get_mid_by_name($menu) : variable_get('menu_primary_links_source', 'primary-menu');\n\n  if (isset($menu_item['menu_name']) && $mid != $menu_item['menu_name']) {\n    return NULL;\n  }\n\n  if (isset($menu_display[$mid]) && $menu_display[$mid] == 'tab' || ($menu_display[$mid] == 'inline')) {\n    switch ($menu_display[$mid]) {\n      case 'tab':\n        if (isset($links_display[$mid]) && !empty($links_display[$mid])) {\n          $name = $menu_item['link_path'];\n          if (isset($menu_item['options']['query'])) {\n            $options = (is_array($menu_item['options']['query'])) ? $menu_item['options']['query'] : array();\n          }\n          else {\n            $options = array();\n          }\n          $html = '';\n          if (file_exists(drupal_get_path('module', 'scholar_site_frontpage') . \"/images/{$links_display[$mid]}\")) {\n            $html = theme('image', drupal_get_path('module', 'scholar_site_frontpage') . \"/images/{$links_display[$mid]}\");\n          }\n          return $html;\n        }\n      break;\n      case 'inline':\n        if (isset($links_display[$mid]) && !empty($links_display[$mid])) {\n          $html = '';\n          if (file_exists(drupal_get_path('module', 'scholar_site_frontpage') . \"/images/{$links_display[$mid]}\")) {\n            $html = theme('image', drupal_get_path('module', 'scholar_site_frontpage') . \"/images/{$links_display[$mid]}\");\n          }\n          return $html;\n        }\n      break;\n    }\n  }\n  return NULL;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Commands to navigate to the TorchTune directory and install development dependencies using pip.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd torchtune\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Site Access in OpenScholar PHP\nDESCRIPTION: Function that determines whether a user has access to a particular OpenScholar site. It checks site status, user permissions, and role-based access control to enforce site access restrictions.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/README.txt#2025-04-21_snippet_2\n\nLANGUAGE: PHP\nCODE:\n```\nfunction vsite_access_user_access_callback() {\n  global $user;\n\n  $vsite = vsite_get_vsite();\n  if (!$vsite) {\n    return FALSE;\n  }\n\n  // If the site is public, anyone may see it\n  if (!in_array(1, _vsite_access_get_value(array($vsite->id), 'preset'))) {\n    return TRUE;\n  }\n\n  // If user is the manager of this site, they may always see it\n  if ($vsite->access_admin()) {\n    return TRUE;\n  }\n\n  // If the user has any admin-level permission, allow access\n  if (user_access('administer nodes') || user_access('bypass node access') || user_access('administer vsite settings')) {\n    return TRUE;\n  }\n\n  // Get the feature base for spaces\n  $vsite_access_values = _vsite_access_get_value(array($vsite->id));\n\n  // If the site is private to members\n  if ($preset = $vsite_access_values['preset']) {\n    if (in_array('1', $preset)) {\n      // User is a member of the site\n      if (isset($vsite->og->users[$user->uid])) {\n        // Lets see if this user should be allowed access\n        $feature_allowed = TRUE;\n        foreach ($vsite_access_values['feature'] as $value) {\n          if ($value == 1) {\n            $feature_allowed = FALSE;\n            break;\n          }\n        }\n        return $feature_allowed;\n      }\n      return FALSE;\n    }\n  }\n\n  // If we get here and site is not private\n  return TRUE;\n}\n```\n\n----------------------------------------\n\nTITLE: Serving Documentation Locally\nDESCRIPTION: Python command to serve documentation using a simple HTTP server on port 8000.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m http.server 8000\n```\n\n----------------------------------------\n\nTITLE: Function Documentation Block\nDESCRIPTION: Template for automatically documenting a Python function using autofunction directive with name variable substitution.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/_templates/autosummary/function.rst#2025-04-21_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{{ name | underline}}\n\n.. autofunction:: {{ name }}\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-commit Hooks\nDESCRIPTION: Command to install pre-commit hooks for ensuring code style consistency.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks\nDESCRIPTION: Command to manually run pre-commit checks on all files in the repository.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/CONTRIBUTING.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Making Search Request to PES2O API Server\nDESCRIPTION: Sample curl command to query the PES2O search API. It sends a POST request with a JSON payload containing the search query and domain specification to retrieve relevant documents from the index.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST localhost:<FORWARD_PORT>/search -H \"Content-Type: application/json\" -d '{\"query\": \"example query\", \"domains\": \"pes2o\"}'\n```\n\n----------------------------------------\n\nTITLE: Executing Offline Search on PES2O Data Index\nDESCRIPTION: Script to perform offline batch search against the PES2O index. It dynamically constructs an index list for all shards, processes queries from a JSONL file, and retrieves the top N documents for each query. Results are saved to an output directory.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nEVAL_DOMAIN=$EXP_ID  # self-defined exp id to distinguish outpu\nRAW_QUERY=$PATH_TO_QUERY_JSONL_FILE\n\nDS_NAME=pes2o_v3\nNUM_SHARDS=16\nN_DOCS=100\n\nindex_list=\"[[0]\"\nfor (( i=1; i<=$((NUM_SHARDS - 1)); i++ )); do\nindex_list+=\",[$i]\"\ndone\nindex_list+=\"]\"\necho INDEX_IDS:$index_list\n\nPYTHONPATH=.  python ric/main_ric.py \\\n    --config-name pes2o_v3 \\\n    tasks.eval.task_name=lm-eval \\\n    tasks.eval.search=true \\\n    datastore.embedding.num_shards=$NUM_SHARDS \\\n    datastore.embedding.shard_ids=[] \\\n    datastore.index.index_shard_ids=$index_list \\\n    evaluation.domain=$EVAL_DOMAIN \\\n    evaluation.data.eval_data=$RAW_QUERY \\\n    evaluation.search.n_docs=$N_DOCS \\\n    evaluation.eval_output_dir=$EVAL_OUTPUT_DIR # where the retrieved documents will be saved\n```\n\n----------------------------------------\n\nTITLE: Searching PES2O Data with Custom Queries\nDESCRIPTION: This snippet shows how to search through the indexed PES2O data using custom queries. It configures the search parameters including experiment ID, query file path, number of shards, number of documents to retrieve, and builds the index list dynamically based on number of shards.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/build_pes2o_index.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nEVAL_DOMAIN=$EXP_ID  # self-defined exp id to distinguish outpu\nRAW_QUERY=$PATH_TO_QUERY_JSONL_FILE\n\nDS_NAME=pes2o_v3\nNUM_SHARDS=16\nN_DOCS=100\n\nindex_list=\"[[0]\"\nfor (( i=1; i<=$((NUM_SHARDS - 1)); i++ )); do\nindex_list+=\",[$i]\"\ndone\nindex_list+=\"]\"\necho INDEX_IDS:$index_list\n\nPYTHONPATH=.  python ric/main_ric.py \\\n    --config-name pes2o_v3 \\\n    tasks.eval.task_name=lm-eval \\\n    tasks.eval.search=true \\\n    datastore.embedding.num_shards=$NUM_SHARDS \\\n    datastore.embedding.shard_ids=[] \\\n    datastore.index.index_shard_ids=$index_list \\\n    evaluation.domain=$EVAL_DOMAIN \\\n    evaluation.data.eval_data=$RAW_QUERY \\\n    evaluation.search.n_docs=$N_DOCS \\\n    evaluation.eval_output_dir=$EVAL_OUTPUT_DIR # where the retrieved documents will be saved\n```\n\n----------------------------------------\n\nTITLE: Sending POST Request to API Endpoint\nDESCRIPTION: This snippet shows how to send a POST request to the API's search endpoint using curl. It includes specifying the content type as JSON and sending a query with domain parameters.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/api/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -X POST localhost:<FORWARD_PORT>/search -H \"Content-Type: application/json\" -d '{\"query\": \"example query\", \"domains\": \"pes2o\"}'\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText documentation defining the structure and organization of the TorchTune datasets module documentation.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/api_ref_datasets.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _datasets:\n\n==================\ntorchtune.datasets\n==================\n\n.. currentmodule:: torchtune.datasets\n\nFor a detailed general usage guide, please see our :ref:`datasets tutorial <dataset_tutorial_label>`.\n\n\nExample datasets\n----------------\n\ntorchtune supports several widely used datasets to help quickly bootstrap your fine-tuning.\n\n.. autosummary::\n    :toctree: generated/\n    :nosignatures:\n\n    alpaca_dataset\n    alpaca_cleaned_dataset\n    grammar_dataset\n    samsum_dataset\n    slimorca_dataset\n    stack_exchanged_paired_dataset\n    cnn_dailymail_articles_dataset\n    wikitext_dataset\n\nGeneric dataset builders\n------------------------\n\ntorchtune also supports generic dataset builders for common formats like chat models and instruct models.\nThese are especially useful for specifying from a YAML config.\n\n.. autosummary::\n    :toctree: generated/\n    :nosignatures:\n\n    instruct_dataset\n    chat_dataset\n    text_completion_dataset\n\nGeneric dataset classes\n-----------------------\n\nClass representations for the above dataset builders.\n\n.. autosummary::\n    :toctree: generated/\n    :nosignatures:\n\n    InstructDataset\n    ChatDataset\n    TextCompletionDataset\n    ConcatDataset\n    PackedDataset\n    PreferenceDataset\n    SFTDataset\n```\n\n----------------------------------------\n\nTITLE: Forwarding Port to Public Address on Tricycle\nDESCRIPTION: This snippet demonstrates how to SSH into tricycle and forward the API port to a public address. It requires the user's UW ID, a forward port number, the job node, and the serve port.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/api/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nssh <UW_ID>@tricycle.cs.washington.edu\nssh -NL 0.0.0.0:<FORWARD_PORT>:<job node>:<SERVE_PORT> klone-login\n```\n\n----------------------------------------\n\nTITLE: Loading Contriever Model and Tokenizer\nDESCRIPTION: Demonstrates how to load the pre-trained Contriever model and its associated tokenizer using the HuggingFace transformers library.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom src.contriever import Contriever\nfrom transformers import AutoTokenizer\n\ncontriever = Contriever.from_pretrained(\"facebook/contriever\") \ntokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Wikipedia Passages\nDESCRIPTION: Command to download the Wikipedia passages dataset used for retrieval evaluation.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\n```\n\n----------------------------------------\n\nTITLE: Running Verification Scripts Against Reference Implementations in OpenScholar\nDESCRIPTION: Example command for running comparison scripts to verify implementation correctness. The scripts compare OpenScholar's implementations against reference implementations and output values used in unit tests. The example specifically shows how to run the attention comparison script for the Llama2 model.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/tests/torchtune/models/llama2/scripts/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m tests.llm.llama2.scripts.compare_attention\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-computed Embeddings\nDESCRIPTION: Commands to download pre-computed passage embeddings for both Contriever and Contriever-msmarco models.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nwget https://dl.fbaipublicfiles.com/contriever/embeddings/contriever/wikipedia_embeddings.tar\nwget https://dl.fbaipublicfiles.com/contriever/embeddings/contriever-msmarco/wikipedia_embeddings.tar\n```\n\n----------------------------------------\n\nTITLE: Generation Configuration for Quantized Models\nDESCRIPTION: YAML configuration for running inference (text generation) with a quantized model, specifying the checkpointer component and quantization settings.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/recipes/quantization.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Currently we only support torchtune checkpoints when\n# evaluating quantized models. For more details on checkpointing see\n# https://pytorch.org/torchtune/main/deep_dives/checkpointer.html\n# Make sure to change the default checkpointer component\ncheckpointer:\n  _component_: torchtune.utils.FullModelTorchTuneCheckpointer\n  ..\n  checkpoint_files: [<quantized_model_checkpoint>]\n\n# Quantization Arguments\nquantizer:\n  _component_: torchtune.utils.quantization.Int8DynActInt4WeightQuantizer\n  groupsize: 256\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model on Mr. TyDi\nDESCRIPTION: Runs evaluation of mContriever model on the Swahili Mr. TyDi dataset\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython beireval.py --model_name_or_path facebook/mcontriever --dataset mrtydi/mrtydi-v1.1-swahili --normalize_text\n```\n\n----------------------------------------\n\nTITLE: Running Standard LoRA Finetuning for Comparison\nDESCRIPTION: Command to run a standard LoRA finetuning job for comparison with QLoRA, allowing users to observe the memory usage difference between the two approaches.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qlora_finetune.rst#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntune run lora_finetune_single_device --config llama2/7B_lora_single_device\n```\n\n----------------------------------------\n\nTITLE: Downloading MKQA Dataset\nDESCRIPTION: Downloads the multilingual MKQA dataset for cross-lingual evaluation\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/apple/ml-mkqa/master/dataset/mkqa.jsonl.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Hidden Role Definition\nDESCRIPTION: Defines a custom hidden role for RST documentation that allows content to be marked with a 'hidden' CSS class.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/_templates/autosummary/function.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Preprocessing MKQA Data\nDESCRIPTION: Preprocesses the MKQA dataset for cross-lingual evaluation\nSOURCE: https://github.com/akariasai/openscholar/blob/main/retriever/contriever/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython data_scripts/preprocess_xmkqa.py mkqa.jsonl xmkqa\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module Context\nDESCRIPTION: Sets the current module context for documentation using a template variable.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/_templates/autosummary/function.rst#2025-04-21_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Role for Hidden Sections in Sphinx Documentation\nDESCRIPTION: Creates a custom role named 'hidden' with the CSS class 'hidden-section'. This can be used to mark specific sections of the documentation as hidden.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/_templates/autosummary/class.rst#2025-04-21_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for Sphinx Documentation\nDESCRIPTION: Sets the current module context for the documentation. The actual module name is populated using a template variable.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/_templates/autosummary/class.rst#2025-04-21_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Generating Class Documentation with Sphinx AutoClass\nDESCRIPTION: Sets up automatic documentation generation for a class using Sphinx's autoclass directive. The class name is populated from a template variable, and all class members are included in the documentation.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/_templates/autosummary/class.rst#2025-04-21_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Version Number Display - Plaintext\nDESCRIPTION: Simple version number for the openscholar project showing version 0.2.1\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/version.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n0.2.1\n```\n\n----------------------------------------\n\nTITLE: Copying EleutherAI Evaluation Configuration in Bash\nDESCRIPTION: Command to copy the default EleutherAI evaluation configuration to a custom file for modification.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/qat_finetune.rst#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntune cp eleuther_evaluation custom_eleuther_evaluation.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking User Read Access in OpenScholar PHP\nDESCRIPTION: Function to check if a user has read access to a given group by validating membership status or admin permissions. It returns TRUE if the user has access, FALSE otherwise.\nSOURCE: https://github.com/akariasai/openscholar/blob/main/training/docs/source/tutorials/README.txt#2025-04-21_snippet_0\n\nLANGUAGE: PHP\nCODE:\n```\nfunction check_user_read_access($group, $account = NULL) {\n  global $user;\n  $account = isset($account) ? $account : $user;\n\n  // Site admin can always see all\n  if (user_access('administer group', $account)) {\n    return TRUE;\n  }\n\n  // If the account is a group admin or a member of the group\n  if ($group->uid == $account->uid || og_is_member($group->gid, 'user', $account)) {\n    return TRUE;\n  }\n\n  // If the group is not public\n  if (!og_is_public_access($group->gid)) {\n    return FALSE;\n  }\n\n  return TRUE;\n}\n```"
  }
]
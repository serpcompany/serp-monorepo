[
  {
    "owner": "uxlfoundation",
    "repo": "onednn",
    "content": "TITLE: Memory Descriptor with format_tag::any in oneDNN (C++)\nDESCRIPTION: This code snippet shows how to create memory descriptors in oneDNN with `format_tag::any` for source, destination, and weights. Using `format_tag::any` allows the compute-intensive primitives to select the most CPU-friendly data format. This facilitates layout propagation, which is crucial for performance optimization.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nsource_mem_descr = memory::desc(args*, memory::format_tag::any);\ndest_mem_descr = memory::desc(args*, memory::format_tag::any);\nweights_mem_descr = memory::desc(args*, memory::format_tag::any);\n```\n\n----------------------------------------\n\nTITLE: Forward Inference Primitive Creation in oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates how to create a convolution primitive descriptor specifically for forward inference in oneDNN. By specifying `prop_kind::forward_inference`, the primitive avoids storing information needed for backward propagation, optimizing memory usage and performance for inference tasks.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto conv_prim_descr = convolution_forward::primitive_desc(engine, prop_kind::forward_inference, ...);\n```\n\n----------------------------------------\n\nTITLE: Binary Post-op Append C++\nDESCRIPTION: Shows how to append a binary post-op to a primitive using the C++ API. The `alg` parameter specifies the binary algorithm, and `src1` is the memory descriptor for the second operand. The binary post-op fuses a binary operation with the output of the preceding primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nvoid dnnl::post_ops::append_binary(\n        algorithm alg, // binary algorithm to apply\n        const memory::desc &src1 // memory descriptor for a second memory operand\n        );\n```\n\n----------------------------------------\n\nTITLE: Using Post-ops in C++ with oneDNN\nDESCRIPTION: This code snippet demonstrates how to create and use post-ops in oneDNN using the C++ API. It shows how to append post-ops, attach them to primitive attributes, and create a primitive descriptor with the attributes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::post_ops po; // default empty post-ops\nassert(po.len() == 0); // no post-ops attached\n\npo.append_SOMETHING(params); // append some particular post-op\npo.append_SOMETHING_ELSE(other_params); // append one more post-op\n\n// (!) Note that the order in which post-ops are appended matters!\nassert(po.len() == 2);\n\ndnnl::primitive_attr attr; // default attributes\nattr.set_post_ops(po); // attach the post-ops to the attr\n\n// further po changes would not affect attr\n\nprimitive::primitive_desc op_pd(engine, params, attr); // create a pd with the attr\n```\n\n----------------------------------------\n\nTITLE: Convolution Primitive Descriptor Creation with Memory Descriptors in oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates the creation of a convolution primitive descriptor in oneDNN, utilizing the memory descriptors created with `format_tag::any` for source, weights, and destination. This allows the oneDNN library to determine the optimal memory layout for the operation, potentially leading to significant performance improvements.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto conv_prim_descr = convolution_forward::primitive_desc(...,\n            source_mem_descr, weights_mem_descr, dest_mem_descr);\n```\n\n----------------------------------------\n\nTITLE: Initializing Scale and Shift with hipMemsetD32Async - C++\nDESCRIPTION: This code snippet shows how to initialize scale to 1 and shift to 0 when shift and scale are not defined in oneDNN, using `hipMemsetD32Async`. This ensures that MIOpen has the required scale and shift values when shift and scale are not defined in oneDNN during forward training of Batch Normalization.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/amd/README.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nhipMemsetD32Async\n```\n\n----------------------------------------\n\nTITLE: Using Attributes with C++ API\nDESCRIPTION: This snippet shows how to use primitive attributes with the oneDNN C++ API. It demonstrates creating an attribute object, setting parameters, and then using it in a primitive descriptor constructor. The attribute object is automatically destroyed when it goes out of scope.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// ### C++ API ###\n\ndnnl::primitive_attr attr;\nattr.set_SOMETHING(params);\nattr.set_SOMETHING_ELSE(params);\n\nprimitive::primitive_desc pd(..., attr);\n\n// in C++ destroying of attr happens automatically\n```\n\n----------------------------------------\n\nTITLE: Setting Flush-To-Zero Mode for CPU Devices in oneDNN\nDESCRIPTION: This code snippet demonstrates how to enable flush-to-zero mode for denormal numbers on x64 systems using the xmmintrin.h header.  Flushing denormals to zero can improve performance by avoiding precise computations with very small numbers. This is done using the `_MM_SET_FLUSH_ZERO_MODE` macro.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/data_types.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <xmmintrin.h>\n_MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);\n```\n\n----------------------------------------\n\nTITLE: Binary Post-op Fusion with Conv C++\nDESCRIPTION: Shows how to fuse a convolution with a binary post-op with per channel addition. It demonstrates how to create a memory descriptor for the convolution destination and the source of the binary operation. The example sets up a ReLU post-op followed by a binary addition post-op, defines the memory descriptor of the source and shows how to set the binary argument for execution.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::memory::desc conv_dst_md {MB, C, H, W}; /* 2D conv destination memory desc */\n\ndnnl::post_ops po;\n\n/* Append eltwise post-op prior the binary post-op */\npo.append_eltwise(\n        /* alg kind  = */ dnnl::algorithm::eltwise_relu,\n        /* neg slope = */ 0.f,\n        /* unused for relu */ 0.f);\n\n/* Note that `C` coincides with the one from `conv_dst_md`. Also note that only\n * supported memory format for src1 memory is `nchw` (or `abcd`) format. */\npo.append_binary(\n        /* alg kind = */ dnnl::algorithm::binary_add,\n        /* src1_md = */ dnnl::memory::desc(\n                {1, C, 1, 1},\n                dnnl::memory::data_type::f32,\n                dnnl::memory::format_tag::abcd));\n\ndnnl::primitive_attr attr;\nattr.set_post_ops(po);\n\nauto cpd = convolution_forward::primitive_desc(conv, attr, engine);\n\n/* To set memory argument for binary post-op, the following should take place: */\nstd::unordered_map<int, memory> args;\n\nargs.insert(DNNL_ARG_SRC, conv_src_memory);\n...\nint binary_post_op_position = 1; /* hard coded here, but may be queried */\nargs.insert(\n        DNNL_ARG_ATTR_MULTIPLE_POST_OP(binary_post_op_position) | DNNL_ARG_SRC_1, /* note parentheses around index */\n        binary_post_op_src1_memory);\n```\n\n----------------------------------------\n\nTITLE: Engine Creation with SYCL device/context\nDESCRIPTION: Creates a oneDNN engine based on existing SYCL device and context. This allows oneDNN to share the execution context with other DPC++ code, using the same device.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::make_engine(const sycl::device &, const sycl::context &)\n```\n\n----------------------------------------\n\nTITLE: Weights Quantization with oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates the process of quantizing weights using per-output-channel scaling with the Intel oneDNN library. It involves creating memory descriptors for the original f32 weights and the quantized int8 weights, setting up the quantization mask, and creating a reorder primitive to perform the quantization. The reorder operation converts the weights from f32 to int8, applying a scaling factor for each output channel.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_quantization.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n   // weights dimensions\n   const int OC, IC, KH, KW;\n\n   // original f32 weights in plain format\n   dnnl::memory::desc wei_plain_f32_md(\n           {OC, IC, KH, KW},                 // dims\n           dnnl::memory::data_type::f32,     // the data originally in f32\n           dnnl::memory::format_tag::hwigo   // the plain memory format\n           );\n\n   // the scaling factors for quantized weights\n   // An unique scale for each output-channel.\n   std::vector<float> wei_scales(OC) = { /* values */ };\n   dnnl::memory();\n\n   // int8 convolution primitive descriptor\n   dnnl::convolution_forward::primitive_desc conv_pd(/* see the next example */);\n\n   // query the convolution weights memory descriptor\n   dnnl::memory::desc wei_conv_s8_md = conv_pd.weights_desc();\n\n   // prepare the attributes for the reorder\n   dnnl::primitive_attr attr;\n   const int quantization_mask = 0\n       | (1 << 0);  // scale per  OC dimension, which is the dim #0\n   attr.set_scales_mask(DNNL_ARG_DST, quantization_mask);\n\n   // create reorder that would perform:\n   //   wei_s8(oc, ic, kh, kw) <- wei_f32(oc, ic, kh, kw) / scale(oc)\n   // including the data format conversion.\n   auto wei_reorder_pd = dnnl::reorder::primitive_desc(\n           wei_plain_f32_md, engine, // source\n           wei_conv_s8_md, engine, // destination,\n           attr);\n   auto wei_reorder = dnnl::reorder(wei_reorder_pd);\n\n// ...\n```\n\n----------------------------------------\n\nTITLE: Convolution with Quantization (oneDNN, C++)\nDESCRIPTION: This code snippet demonstrates convolution with per-output-channel quantization using Intel oneDNN. It shows how to create memory descriptors for source, weights, and destination, all in s8 format with `any` format tags. The code then sets up attributes for the convolution primitive, including scaling masks for source, weights, and destination, and creates a convolution primitive descriptor. This allows performing convolution computations directly on quantized data.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_quantization.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n   const float src_scale; // src_f32[:] = src_scale * src_s8[:]\n   const float dst_scale; // dst_f32[:] = dst_scale * dst_s8[:]\n\n   // the scaling factors for quantized weights (as declared above)\n   // An unique scale for each output-channel.\n   std::vector<float> wei_scales(OC) = {...};\n\n\n   // Src, weights, and dst memory descriptors for convolution,\n   // with memory format tag == any to allow a convolution implementation\n   // to chose the appropriate memory format\n\n   dnnl::memory::desc src_conv_s8_any_md(\n           {BATCH, IC, IH, IW},          // dims\n           dnnl::memory::data_type::s8,  // the data originally in s8\n           dnnl::memory::format_tag::any // let convolution to choose\n           );\n\n   dnnl::memory::desc wei_conv_s8_any_md(\n           {OC, IC, KH, KW},             // dims\n           dnnl::memory::data_type::s8,  // the data originally in s8\n           dnnl::memory::format_tag::any // let convolution to choose\n           );\n\n   dnnl::memory::desc dst_conv_s8_any_md(...);  // ditto\n\n   // prepare the attributes for the convolution\n   dnnl::primitive_attr attr;\n   const int data_mask = 0; // scale and zero-point per tensor for source and destination\n   const int wei_mask = 0\n       | (1 << 0); // scale per OC dimension, which is the dim #0 on weights tensor:\n                   // (   OC, IC, KH, KW)\n                   //      0   1   2   3\n\n   attr.set_scales_mask(DNNL_ARG_SRC, data_mask);\n   attr.set_zero_points_mask(DNNL_ARG_SRC, data_mask);\n\n   attr.set_scales_mask(DNNL_ARG_WEIGHTS, wei_mask);\n\n   attr.set_scales_mask(DNNL_ARG_DST, data_mask);\n   attr.set_zero_points_mask(DNNL_ARG_DST, data_mask);\n\n   // create a convolution primitive descriptor\n   auto conv_pd = dnnl::convolution_forward::primitive_desc(\n           dnnl::prop_kind::forward_inference,\n           dnnl::algorithm::convolution_direct,\n           src_conv_s8_any_md,                     // what's important is that\n           wei_conv_s8_any_md,                     // we specified that we want\n           dst_conv_s8_any_md,                     // computations in s8\n           strides, padding_l, padding_r,\n           dnnl::padding_kind::zero\n           attr);   // the attributes describe the quantization flow\n// ...\n```\n\n----------------------------------------\n\nTITLE: Prelu Post-op Append C++\nDESCRIPTION: Demonstrates how to append a prelu post-op to a primitive using the C++ API. The `mask` parameter describes the prelu weights broadcast. The weights tensor is passed in runtime using DNNL_ARG_ATTR_MULTIPLE_POST_OP(index) | DNNL_ARG_WEIGHTS mechanism.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nvoid dnnl::post_ops::append_prelu(\n    int mask /*mask describing prelu weights broadcast.*/);\n```\n\n----------------------------------------\n\nTITLE: Creating Convolution Primitive Descriptor with Attributes in oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates how to create a convolution primitive descriptor, passing the `primitive_attr` object that contains the fused ReLU post-op. This creates a fused convolution-ReLU operation, reducing memory accesses and improving performance.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nauto conv_prim_descr = convolution_forward::primitive_desc(..., attrs, engine);\n```\n\n----------------------------------------\n\nTITLE: Running Benchdnn with Verbose Mode\nDESCRIPTION: This example shows how to run the `benchdnn` tool with verbose mode enabled to troubleshoot primitive creation issues. The `ONEDNN_VERBOSE=all` setting enables all verbose messages, and the `benchdnn` command runs a matmul operation with specific dimensions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/verbose.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nONEDNN_VERBOSE=all ./benchdnn --matmul 256x256:25x256\n```\n\n----------------------------------------\n\nTITLE: Creating post_ops for Fused ReLU in oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates how to create a `post_op` for a fused ReLU operation in oneDNN. The `post_ops` object is used to chain operations after a compute-intensive primitive, such as convolution, effectively combining them into a single pass.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\npost_ops ops;\nops.append_eltwise(..., algorithm::eltwise_relu);\n```\n\n----------------------------------------\n\nTITLE: Creating Sorted COO Memory Object with oneDNN (C++)\nDESCRIPTION: This code snippet illustrates how to create a oneDNN memory object using the Sorted Co-ordinate (COO) sparse encoding. It involves creating a memory descriptor, initializing the values, row indices, and column indices vectors, and constructing the memory object. The snippet also verifies the buffer sizes and retrieves data handles.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/sparsity.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n    using namespace dnnl;\n    const memory::dim M = 4, N = 6;\n    const memory::dim nnz = 5;\n    const auto values_dt = memory::data_type::f32;\n    const auto indices_dt = memory::data_type::s32;\n\n    // Create a memory descriptor for COO sparse encoding.\n    const auto coo_md = memory::desc::coo(\n            {M, N}, // Dimensions\n            values_dt, // Data type of values\n            nnz, // Number of non-zero entries\n            indices_dt); // Data type of indices (metadata)\n\n    // A sparse matrix represented in the COO format.\n    std::vector<float> coo_values = {2.5f, 1.5f, 1.5f, 2.5f, 2.0f};\n    std::vector<int32_t> coo_row_indices = {0, 1, 2, 2, 3};\n    std::vector<int32_t> coo_col_indices = {0, 2, 0, 5, 1};\n\n    // Create a memory object for the given buffers with values and metadata.\n    memory coo_mem(coo_md, engine, {\n        coo_values.data(), // Buffer with values\n        coo_row_indices.data(), // Buffer with row indices (metadata)\n        coo_col_indices.data() // Buffer with column indices (metadata)\n        });\n\n    const auto values_sz = coo_mem.get_size(0);\n    const auto indices_sz = coo_mem.get_size(1);\n\n    assert(values_sz == coo_values.size() * sizeof(float));\n    assert(indices_sz == coo_row_indices.size() * sizeof(int32_t));\n    assert(indices_sz == coo_col_indices.size() * sizeof(int32_t));\n\n    void *values_handle = coo_mem.get_data_handle(0);\n    void *row_indices_handle = coo_mem.get_data_handle(1);\n    void *col_indices_handle = coo_mem.get_data_handle(2);\n\n    assert(values_handle == (void *)coo_values.data());\n    assert(row_indices_handle == (void *)coo_row_indices.data());\n    assert(col_indices_handle == (void *)coo_col_indices.data());\n```\n\n----------------------------------------\n\nTITLE: Compute u8/s8 dot product accurately using VPDPBUSD (DL Boost)\nDESCRIPTION: This C++ snippet illustrates how to compute the dot product of u8 and s8 vectors using the VPDPBUSD instruction available on CPUs with Intel DL Boost support.  Unlike the AVX2/AVX-512 version, VPDPBUSD performs the computation without intermediate saturation, providing an accurate result.  This allows using the full range of u8 and s8 without special quantization requirements.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/int8_computations.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// Want to compute:\n// c_s32 += sum{i=0..3}(a_u8[i] * b_s8[i])\nint32_t u8s8s32_compute_avx512_dl_boost(\n        uint8_t a_u8[4], int8_t b_s8[4], int32_t c_s32) {\n\n    // Compute using VPDPBUSD:\n    c_s32 +=\n        (int32_t)a_u8[0] * (int32_t)b_s8[0] +\n        (int32_t)a_u8[1] * (int32_t)b_s8[1] +\n        (int32_t)a_u8[2] * (int32_t)b_s8[2] +\n        (int32_t)a_u8[3] * (int32_t)b_s8[3];\n\n    return c_s32;\n}\n```\n\n----------------------------------------\n\nTITLE: Data Type Conversion with Reorder\nDESCRIPTION: This code demonstrates data type conversion using the reorder primitive. It converts data from f32 (float32) to s8 (int8) and u8 (uint8), showcasing the saturation behavior where values outside the target data type's range are clamped to the minimum or maximum representable value.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/reorder.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nreorder(src={1024, data_type=f32}, dst={, data_type=s8})\n// dst == {127}\n\nreorder(src={-124, data_type=f32}, dst={, data_type=u8})\n// dst == {0}\n```\n\n----------------------------------------\n\nTITLE: Get SYCL device from oneDNN engine\nDESCRIPTION: Retrieves the underlying SYCL device from a oneDNN engine. This enables access to the SYCL device associated with the engine for interoperability purposes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::get_device(const engine &)\n```\n\n----------------------------------------\n\nTITLE: Relu -> Depthwise -> Relu Post-op Fusion C++\nDESCRIPTION: Shows an example of fusing a depthwise convolution with 1x1 convolution, similar to MobileNet. This involves applying a ReLU activation, followed by a depthwise convolution, and then another ReLU activation. The example also demonstrates how to set scales for the destination and depthwise convolution using `dnnl::primitive_attr::set_scales_mask` and how to query the memory descriptors.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::post_ops po;\n\npo.append_eltwise(\n        /* alg kind  = */ dnnl::algorithm::eltwise_relu,\n        /* neg slope = */ 0.f,\n        /* unused for relu */ 0.f);\n\npo.append_dw(\n        /* depthwise weights data type = */ dnnl::memory::data_type::s8,\n        /* depthwise bias data type (undef implies no bias) = */ dnnl::memory::data_type::undef,\n        /* depthwise destination data type = */ dnnl::memory::data_type::u8,\n        /* kernel size of fused depthwise convolution = */ kernel,\n        /* stride size of fused depthwise convolution = */ stride,\n        /* padding size of fused depthwise convolution = */ padding)\n\npo.append_eltwise(\n        /* alg kind  = */ dnnl::algorithm::eltwise_relu,\n        /* neg slope = */ 0.f,\n        /* unused for relu */ 0.f);\n\ndnnl::primitive_attr attr;\nattr.set_scales_mask(DNNL_ARG_DST, 0);\nattr.set_scales_mask(DNNL_ARG_ATTR_POST_OP_DW | DNNL_ARG_DST, 0);\nattr.set_post_ops(po);\n\nauto cpd = convolution_forward::primitive_desc(conv_1x1, attr, engine);\nauto dw_weight_md = cpd.query(query::exec_arg_md,\n                DNNL_ARG_ATTR_POST_OP_DW | DNNL_ARG_WEIGHTS);\nauto dw_bias_md = cpd.query(query::exec_arg_md,\n                DNNL_ARG_ATTR_POST_OP_DW | DNNL_ARG_BIAS);\n```\n\n----------------------------------------\n\nTITLE: Creating CSR Memory Object with oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates how to create a oneDNN memory object using the Compressed Sparse Row (CSR) encoding. It includes creating a memory descriptor, initializing the values, indices, and pointers vectors, and creating the memory object. It then verifies the size of buffers and retrieves the data handles.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/sparsity.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n    using namespace dnnl;\n    const memory::dim M = 4, N = 6;\n    const memory::dim nnz = 5;\n    const auto values_dt = memory::data_type::f32;\n    const auto indices_dt = memory::data_type::s32;\n    const auto pointers_dt = memory::data_type::s32;\n\n    // Create a memory descriptor for CSR sparse encoding.\n    const auto csr_md = memory::desc::csr(\n            {M, N}, // Dimensions\n            values_dt, // Data type of values\n            nnz, // Number of non-zero entries\n            indices_dt, // Data type of indices (metadata)\n            pointers_dt); // Data type of pointers (metadata)\n\n    // A sparse matrix represented in the CSR format.\n    std::vector<float> csr_values = {2.5f, 1.5f, 1.5f, 2.5f, 2.0f};\n    std::vector<int32_t> csr_indices = {0, 2, 0, 5, 1};\n    std::vector<int32_t> csr_pointers = {0, 1, 2, 4, 5, 5};\n\n    // Create a memory object for the given buffers with values and metadata.\n    memory csr_mem(csr_md, engine, {\n        csr_values.data(), // Buffer with values\n        csr_indices.data(), // Buffer with indices (metadata)\n        csr_pointers.data() // Buffer with pointers (metadata)\n        });\n\n    const auto values_sz = csr_mem.get_size(0);\n    const auto indices_sz = csr_mem.get_size(1);\n    const auto pointers_sz = csr_mem.get_size(2);\n\n    assert(values_sz == csr_values.size() * sizeof(float));\n    assert(indices_sz == csr_indices.size() * sizeof(int32_t));\n    assert(pointers_sz == csr_pointers.size() * sizeof(int32_t));\n\n    void *values_handle = csr_mem.get_data_handle(0);\n    void *indices_handle = csr_mem.get_data_handle(1);\n    void *pointers_handle = csr_mem.get_data_handle(2);\n\n    assert(values_handle == (void *)csr_values.data());\n    assert(indices_handle == (void *)csr_indices.data());\n    assert(pointers_handle == (void *)csr_pointers.data());\n```\n\n----------------------------------------\n\nTITLE: Setting Primitive Attributes with post_ops in oneDNN (C++)\nDESCRIPTION: This code snippet shows how to create a `primitive_attr` object and attach the previously created `post_ops` (containing the fused ReLU) to it. This attribute object will then be used when creating the convolution primitive descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nprimitive_attr attr;\nattr.set_post_ops(ops);\n```\n\n----------------------------------------\n\nTITLE: Setting Scales Mask in C++ for oneDNN Primitive Attributes\nDESCRIPTION: This C++ code snippet demonstrates how to set the scales mask for a oneDNN primitive attribute. It takes the argument `arg` which indicates which tensor the scaling should be applied to (e.g., destination tensor) and the `mask` which defines the dimensionality of the scaling factors. The `mask` is calculated as a sum of powers of 2 corresponding to the dimensions to be scaled, and `scales.size()` should be equal to the product of the sizes of the dimensions in the mask.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_quantization.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nvoid dnnl::primitive_attr::set_scales_mask(int arg, int mask);\n```\n\n----------------------------------------\n\nTITLE: Sum -> ReLU Post-op Chain C++\nDESCRIPTION: Illustrates chaining a sum post-op with a ReLU post-op. This is a common pattern in CNN topologies, where the output of a convolution is added to the input and then passed through a ReLU activation. The example shows how to append the post-ops to a `dnnl::post_ops` object and set it as an attribute of a primitive descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::post_ops po;\npo.append_sum();\npo.append_eltwise(\n        /* alg kind  = */ dnnl::algorithm::eltwise_relu,\n        /* neg slope = */ 0.f,\n        /* unused for relu */ 0.f);\n\ndnnl::primitive_attr attr;\nattr.set_post_ops(po);\n\nconvolution_forward::primitive_desc(conv_d, attr, engine);\n```\n\n----------------------------------------\n\nTITLE: USM-based memory creation\nDESCRIPTION: Creates a oneDNN memory object based on a USM pointer. This utilizes the Unified Shared Memory (USM) feature of DPC++ for memory allocation that is accessible from both host and device.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::memory(const memory::desc &, const engine &, void *)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Eltwise Driver Usage\nDESCRIPTION: This snippet shows the general syntax for using the element-wise driver within the benchdnn framework.  It includes options for specifying the direction, data type, memory layout, algorithm, alpha/beta parameters, and other attributes of the element-wise operation. The user should replace `[benchdnn-knobs]`, `[eltwise-knobs]`, and `[eltwise-desc]` with appropriate values.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_eltwise.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --eltwise [benchdnn-knobs] [eltwise-knobs] [eltwise-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Run Specific Binary Primitive Problem\nDESCRIPTION: Demonstrates how to run a specific binary primitive problem with particular configurations. It sets data types, memory formats, operation algorithm, and input tensor shapes. It configures element-wise multiplication, and broadcasts the second source.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_binary.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --binary --sdt=f32:f32 --ddt=f32 --stag=nhwc:nhwc \\\n               --alg=MUL --inplace=false 8x8x3x5:8\n```\n\n----------------------------------------\n\nTITLE: Constructing oneDNN Stream from OpenCL Command Queue\nDESCRIPTION: Creates a oneDNN stream using an existing OpenCL command queue.  This integrates oneDNN operations into the provided OpenCL queue.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::make_stream(const engine &, cl_command_queue)\n```\n\n----------------------------------------\n\nTITLE: Setting Convolution Attributes with Scale and Post-ops\nDESCRIPTION: This code snippet demonstrates how to set convolution attributes in oneDNN, specifically setting the scale and post-operations (post-ops) such as ReLU and sum. It shows how to configure the primitive attributes before performing the convolution operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/convolution.md#_snippet_1\n\nLANGUAGE: pseudo-code\nCODE:\n```\nprimitive_attr attr;\nattr.set_scale(wei, mask=0);\nattr.set_post_ops({\n        { eltwise={scale=gamma, type=relu, alpha=eta, beta=ignored } },\n        { sum={scale=beta} }\n    });\n\nconvolution_forward(src, weights, dst, attr);\n```\n\n----------------------------------------\n\nTITLE: Creating Memory Descriptor with Blocking Structure in oneDNN\nDESCRIPTION: Demonstrates how to create a memory descriptor in oneDNN that represents a blocked memory layout. It initializes a memory descriptor with the `dnnl_nChw8c` tag, defines dimensions and strides, and uses `dnnl_memory_desc_query` to verify the correctness of the descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl_memory_desc_t md; // memory descriptor object\n\n    // logical description, layout independent\n    int ndims = 4;                   // # dimensions\n    dnnl_dims_t dims = {N, C, H, W}; // dimensions themselves\n\n    dnnl_memory_desc_create_with_tag(&md, ndims, dims, dnnl_f32, dnnl_nChw8c);\n\n    ptrdiff_t stride_n = C*H*W;\n    ptrdiff_t stride_C = H*W*8;\n    ptrdiff_t stride_h =   W*8;\n    ptrdiff_t stride_w =     8;\n\n    dnnl_dims_t strides = {stride_n, stride_C, stride_h, stride_w }; // strides between blocks\n    int inner_nblks = 1; // number of blocked dimensions;\n                         // 1, since only channels are blocked\n\n    dnnl_dims_t inner_idxs = {1}; // Only the 1st (c) dimension is blocked\n                                  // n -- 0st dim, w -- 3rd dim\n\n    dnnl_dims_t inner_blks = {8}; // This 1st dimensions is blocked by 8\n\n    dnnl_dims_t *q_strides = nullptr;\n    int *q_inner_nblks = nullptr;\n    dnnl_dims_t *q_inner_idxs = nullptr;\n    dnnl_dims_t *q_inner_blks = nullptr;\n    dnnl_memory_desc_query(md, dnnl_query_strides, &q_strides);\n    dnnl_memory_desc_query(md, dnnl_query_inner_nblks, &q_inner_nblks);\n    dnnl_memory_desc_query(md, dnnl_query_inner_idxs, &q_inner_idxs);\n    dnnl_memory_desc_query(md, dnnl_query_inner_blks, &q_inner_blks);\n\n    assert(memcmp(*q_strides, strides, DNNL_MAX_NDIMS) == 0);\n    assert(*q_inner_nblks == inner_nblks);\n    assert(memcmp(*q_inner_idxs, inner_idxs, DNNL_MAX_NDIMS) == 0);\n    assert(memcmp(*q_inner_blks, inner_blks, DNNL_MAX_NDIMS) == 0);\n```\n\n----------------------------------------\n\nTITLE: Tanh -> Sum -> ScaleShift Post-op C++\nDESCRIPTION: Demonstrates a more complex chain of post-ops: Tanh, Sum, and ScaleShift. It also shows how to set scales for the input, weights, and output using `dnnl::primitive_attr::set_scales_mask`. This example showcases how to combine different operations with scaling and shifting to achieve custom fused operations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::post_ops po;\npo.append_eltwise(\n        /* alg kind  = */ dnnl::algorithm::eltwise_tanh,\n        /* unused for tanh */ 0.f,\n        /* unused for tanh */ 0.f);\npo.append_sum();\npo.append_eltwise(\n        /* alg kind     = */ dnnl::algorithm::eltwise_linear,\n        /* linear scale = */ alpha,\n        /* linear shift = */ beta);\n\ndnnl::primitive_attr attr;\nattr.set_scales_mask(DNNL_ARG_SRC, 0);\nattr.set_scales_mask(DNNL_ARG_WEIGHTS, 0);\nattr.set_scales_mask(DNNL_ARG_DST, 0);\nattr.set_post_ops(po);\n\nconvolution_forward::primitive_desc(conv_d, attr, engine);\n```\n\n----------------------------------------\n\nTITLE: Run batched MatMul with bias\nDESCRIPTION: This command executes a single-precision batched matrix multiplication with bias. The `--bia-dt` option specifies the data type of the bias, and `--bia_mask` indicates the dimensions along which the bias is broadcasted.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --matmul --bia-dt=f32 --bia_mask=4 2x10x30:2x30x20\n```\n\n----------------------------------------\n\nTITLE: User-Managed Scratchpad C++\nDESCRIPTION: This code snippet demonstrates how to use user-managed scratchpad memory in oneDNN. It creates a primitive with the scratchpad mode set to `dnnl::scratchpad_mode::user`, queries the scratchpad memory descriptor, allocates memory for the scratchpad, and then executes the primitive, providing the scratchpad memory as an argument. The size of the scratchpad is determined by the scratchpad memory descriptor. The engine used to create the memory must match the engine of the primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_scratchpad.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Create an empty (default) attributes\ndnnl::primitive_attr attr;\n\n// Default scratchpad mode is `library`:\nassert(attr.get_scratchpad_mode() == dnnl::scratchpad_mode::library);\n\n// Set scratchpad mode to `user`\nattr.set_scratchpad_mode(dnnl::scratchpad_mode::user);\n\n// Create a primitive descriptor with custom attributes\ndnnl::primitive::primitive_desc op_pd(engine, ..., attr);\n\n// Query the scratchpad memory descriptor\ndnnl::memory::desc scratchpad_md = op_pd.scratchpad_desc();\n\n// Note, that a primitive does not consume memory in this configuration:\nassert(op_pd.query_s64(dnnl::query::memory_consumption_s64) == 0);\n\n// Create a primitive\ndnnl::primitive prim(op_pd);\n\n// ...\n\n// Create a scratchpad memory\n// NOTE: if scratchpad is not required for a particular primitive the\n//       scratchpad_md.get_size() will return 0. It is fine to have\n//       scratchpad_ptr == nullptr in this case.\nvoid *scratchpad_ptr = user_memory_manager::allocate(scratchpad_md.get_size());\n// NOTE: engine here must much the engine of the primitive\ndnnl::memory scratchpad(scratchpad_md, engine, scratchpad_ptr);\n\n// Pass a scratchpad memory to a primitive\nprim.execute(stream, {\n        ...,\n        {DNNL_ARG_SCRATCHPAD, scratchpad}});\n```\n\n----------------------------------------\n\nTITLE: Reporting Instruction Cache Misses with VTune\nDESCRIPTION: This command reports instruction cache misses using VTune Profiler. It filters the report by task, formats it as CSV, and displays the 'ICache Misses' column, providing insights into instruction cache-related performance bottlenecks in oneDNN primitives.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -report hotspots -q -r dnnl-vtune-ue -format csv -csv-delimiter ';' -group-by task -column 'ICache Misses' | head -n 10 | column -t -s';'\n```\n\n----------------------------------------\n\nTITLE: OpenCL Engine Cache Example in oneDNN (C++)\nDESCRIPTION: This code demonstrates how to create, store, load, and use a cached OpenCL engine in oneDNN. It obtains the OpenCL device and context, creates an engine, retrieves the cache blob ID and the cache blob. It stores and loads the cache blob from a disk using placeholder functions, and finally creates an OpenCL engine from the loaded cache blob.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/persistent_cache.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace dnnl;\n\n{\n    cl_device_id device = ...;\n    cl_context context = ...;\n\n    engine ocl_engine = ocl_interop::make_engine(device, context);\n    std::vector<uint8_t> key = get_engine_cache_blob_id(ocl_interop::get_device(ocl_engine));\n    std::vector<uint8_t> value = get_engine_cache_blob(ocl_engine);\n\n    store_cache_blob_on_disk(key, value);\n}\n\n{\n    cl_device_id device = ...;\n    cl_context context = ...;\n\n    std::vector<uint8_t> key = get_engine_cache_blob_id(device);\n    std::vector<uint8_t> value = load_cache_blob_from_disk(key);\n    engine ocl_engine = ocl_interop::make_engine(device, context, value);\n}\n```\n\n----------------------------------------\n\nTITLE: Dispatch Verbose Example\nDESCRIPTION: This example shows how to run `benchdnn` with dispatch verbose mode to understand why a given implementation is dispatched. The `ONEDNN_VERBOSE=dispatch` setting enables dispatching verbose messages. It uses `benchdnn` with a matmul operation, specifying the data types.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/verbose.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nONEDNN_VERBOSE=dispatch ./benchdnn --matmul --dt=u8:s8:f32 256x256:256x256\n```\n\n----------------------------------------\n\nTITLE: Accessing OpenCL Context from oneDNN Engine\nDESCRIPTION: Retrieves the underlying OpenCL context from a oneDNN engine object.  The user is responsible for managing the lifetime of the returned OpenCL object.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::get_context(const engine &)\n```\n\n----------------------------------------\n\nTITLE: Reorder with Scales, Zero Points and Post-op Sum\nDESCRIPTION: This example illustrates using the reorder primitive with scaling, zero points, and a sum post-op. It outlines how to configure the primitive attributes for scaling, zero point adjustment, and accumulating the result with a scaled sum, enabling complex data manipulation during reordering.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/reorder.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nreorder(\n            src = {dims={N, C, H, W}, data_type=dt_src, memory_format=fmt_src},\n            dst = {dims={N, C, H, W}, data_type=dt_dst, memory_format=fmt_dst},\n            attr ={\n                scales={ src={mask=0} },\n                zero_points= { src={mask=0}, dst={mask=0} },\n                post-ops = { sum={scale=beta} },\n            })\n```\n\n----------------------------------------\n\nTITLE: Get SYCL buffer from oneDNN memory\nDESCRIPTION: Retrieves the underlying SYCL buffer from a oneDNN memory object. This provides access to the SYCL buffer associated with the oneDNN memory, enabling interoperability with SYCL code.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::get_buffer<T, ndims>(const memory &)\n```\n\n----------------------------------------\n\nTITLE: Constructing oneDNN Memory from USM pointer\nDESCRIPTION: Creates a oneDNN memory object backed by a Unified Shared Memory (USM) pointer. This allows oneDNN to operate on memory shared between the host and OpenCL devices. It's user's responsibility to manage its lifetime.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::make_memory(const memory::desc &, const engine &, ocl_interop::memory_kind, void *)\n```\n\n----------------------------------------\n\nTITLE: BRGeMM with Bias Example\nDESCRIPTION: This snippet shows an example of running the BRGeMM driver with bias. The `--bia_dt` option specifies the data type for the bias.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_brgemm.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --brgemm --bia_dt=f32 1x30:30x1\n```\n\n----------------------------------------\n\nTITLE: Sum Primitive Example (C++)\nDESCRIPTION: This example demonstrates the usage of the sum primitive in C++. The example shows a short snippet of the usage.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/sum.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n@copydetails sum_example_cpp_short\n```\n\n----------------------------------------\n\nTITLE: Buffer-based memory creation with SYCL buffer\nDESCRIPTION: Creates a oneDNN memory object based on a SYCL buffer. This allows oneDNN to operate on data stored in SYCL buffers, facilitating interoperability with SYCL code.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::make_memory(const memory::desc &, const engine &, sycl::buffer<T, ndims> &)\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN with Nvidia Backend\nDESCRIPTION: This code snippet shows how to build oneDNN with Nvidia backend support using CMake. It sets the compiler paths and enables the DPCPP runtime for both CPU and GPU, specifying NVIDIA as the GPU vendor. Ninja is used as the generator.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/nvidia/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport CC=/path/to/dpcpp/install/bin/clang\nexport CXX=/path/to/dpcpp/install/bin/clang++\nmkdir build\ncd build\ncmake -DDNNL_CPU_RUNTIME=DPCPP -DDNNL_GPU_RUNTIME=DPCPP \\\n      -DDNNL_GPU_VENDOR=NVIDIA -G Ninja ..\n```\n\n----------------------------------------\n\nTITLE: s8s8 Computation with u8 Conversion in C++\nDESCRIPTION: This C++ code snippet demonstrates how oneDNN handles s8/s8 computations by converting one of the s8 inputs to u8, performing the computation, and then applying a compensation to account for the conversion. It highlights the potential overflow issue when using the Intel AVX2 or AVX512 instruction sets. The `u8s8s32_compute` function is assumed to be defined elsewhere.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/int8_computations.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nint8_t  a_s8[4] = {127, 127, 0, 0};\nint8_t  b_s8[4] = {127, 127, 0, 0};\nint32_t c_s32 = 0;\n\n// s8s8 the uses u8s8\nauto s8s8s32_compute = [](int8_t a_s8[4], int8_t b_s8[4], int32_t c_s32) {\n    uint8_t a_u8[4] = { 128 + a_s8[0], ...};\n    c_s32 = u8s8s32_compute(a_u8, b_s8, c_s32);\n\n    // apply the compensation\n    c_s32 +=\n        - 128 * b_s8[0]\n        - 128 * b_s8[1]\n        - 128 * b_s8[2]\n        - 128 * b_s8[3];\n\n    return c_s32;\n};\n\nc_s32 = s8s8s32_compute(a_s8, b_s8, c_s32);\n// c_s32 = 255\n//       = 32767 - 128 * (127 + 127 + 0 + 0);\n// While one might expect 32258 !!!\n```\n\n----------------------------------------\n\nTITLE: Setting default fpmath mode using dnnl::set_default_fpmath_mode (C++ API)\nDESCRIPTION: This snippet shows how to set the default floating-point math mode using the `dnnl::set_default_fpmath_mode` function from the oneDNN C++ API.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_fpmath_mode.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n@ref dnnl::set_default_fpmath_mode\n```\n\n----------------------------------------\n\nTITLE: Conditional Reorder in oneDNN (C++)\nDESCRIPTION: This snippet showcases a conditional reorder operation in oneDNN. It checks if the user-provided source memory format matches the format recommended by the primitive descriptor. If they differ, a reorder primitive is created and executed to convert the data to the optimal format for the convolution operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nmemory conv_source_memory = user_source_memory;\nif (conv_prim_descr.src_desc() != user_source_memory.get_desc()) {\n    conv_source_memory = memory(conv_prim_descr.src_desc(), engine);\n    auto reorder_prim_descr = reorder::primitive_desc(user_source_memory, conv_source_memory);\n    reorder(reorder_prim_descr).execute(s, user_source_memory, conv_source_memory);\n}\n```\n\n----------------------------------------\n\nTITLE: Get data handle from oneDNN memory\nDESCRIPTION: Retrieves the data handle (pointer) from a oneDNN memory object. This provides direct access to the memory location used by the oneDNN memory object.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::memory::get_data_handle()\n```\n\n----------------------------------------\n\nTITLE: Constructing oneDNN Memory from OpenCL Buffer\nDESCRIPTION: Creates a oneDNN memory object based on an existing OpenCL buffer. This allows oneDNN to operate on data managed by OpenCL.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::memory(const memory::desc &, const engine &, cl_mem)\n```\n\n----------------------------------------\n\nTITLE: Get SYCL queue from oneDNN stream\nDESCRIPTION: Retrieves the underlying SYCL queue from a oneDNN stream. This allows interacting with the SYCL queue used by the oneDNN stream, for example, to manage dependencies.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::get_queue(const stream &)\n```\n\n----------------------------------------\n\nTITLE: benchdnn Harness Usage\nDESCRIPTION: This code snippet demonstrates how to run benchdnn with a specific driver, common options, driver-specific options, and a problem description. It shows the general command-line structure for using the tool.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/README.md#_snippet_0\n\nLANGUAGE: Shell Script\nCODE:\n```\n ./benchdnn --DRIVER [COMMON-OPTIONS] [DRIVER-OPTIONS] PROBLEM-DESCRIPTION\n```\n\n----------------------------------------\n\nTITLE: Initializing Vanilla RNN Primitive Descriptor C++\nDESCRIPTION: This C++ code snippet shows how to initialize a Vanilla RNN primitive descriptor using dnnl::vanilla_rnn_forward::primitive_desc::primitive_desc(). It takes engine, propagation kind, activation function, direction, and memory descriptors for source layer, source iteration, weights layer, weights iteration, bias, destination layer, and destination iteration as input. The primitive descriptor is essential for creating a Vanilla RNN primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto vanilla_rnn_pd = dnnl::vanilla_rnn_forward::primitive_desc(\n        engine, aprop, activation, direction, src_layer_desc, src_iter_desc,\n        weights_layer_desc, weights_iter_desc, bias_desc, dst_layer_desc,\n        dst_iter_desc);\n```\n\n----------------------------------------\n\nTITLE: Binary Operation with Scales (sh)\nDESCRIPTION: This example runs a binary problem with s8 input data and u8 output data in nc layout. It applies scales to both inputs without any post-operations.  `src` is the first input, `src1` is the second input, and `common` implies that the scale is applied to all elements of the input.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --binary --sdt=u8:s8 --ddt=u8 --stag=nc:nc \\\n               --attr-scales=src:common:1.5+src1:common:2.5 \\\n               100x100:100x100\n```\n\n----------------------------------------\n\nTITLE: Get SYCL context from oneDNN engine\nDESCRIPTION: Retrieves the underlying SYCL context from a oneDNN engine.  This provides access to the SYCL context associated with the engine to interoperate with other SYCL-based code.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::get_context(const engine &)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Eltwise Run From Input File\nDESCRIPTION: This snippet shows how to run the element-wise set from an input file with the default settings using the benchdnn framework. It leverages the `--batch` option to specify the input file containing configurations for element-wise operations. Make sure that the path `inputs/eltwise/shapes_ci` exists and contains properly formatted configurations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_eltwise.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --eltwise --batch=inputs/eltwise/shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Setting default fpmath mode using dnnl_set_default_fpmath_mode (C API)\nDESCRIPTION: This snippet shows how to set the default floating-point math mode using the `dnnl_set_default_fpmath_mode` function from the oneDNN C API.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_fpmath_mode.md#_snippet_2\n\nLANGUAGE: C\nCODE:\n```\n@ref dnnl_set_default_fpmath_mode\n```\n\n----------------------------------------\n\nTITLE: Creating a Winograd Convolution Primitive Descriptor in C++\nDESCRIPTION: This code snippet shows how to create a convolution primitive descriptor using the Winograd algorithm in oneDNN. This involves specifying the engine, propagation kind, algorithm (convolution_winograd), and memory descriptors for source, weights, bias, and destination tensors.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/convolution.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nauto conv1_pd = convolution_forward::primitive_desc(engine,\n    prop_kind::forward_inference, algorithm::convolution_winograd,\n    conv1_src_md, conv1_weights_md, conv1_bias_md, conv1_dst_md,\n    conv1_strides, conv1_padding_l, conv1_padding_r);\n```\n\n----------------------------------------\n\nTITLE: Accessing Data Handle from oneDNN Memory\nDESCRIPTION: Retrieves the data handle from a oneDNN memory object. When using USM, this will return the USM pointer.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::memory::get_data_handle()\n```\n\n----------------------------------------\n\nTITLE: Offset calculation for NHWC data format\nDESCRIPTION: This code snippet defines the offset function for the NHWC data format, which calculates the memory address displacement for a given logical index (n, c, h, w) in a 4D tensor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\noffset_nhwc(n, c, h, w) = n * HWC + h * WC + w * C + c\n```\n\n----------------------------------------\n\nTITLE: LRN Driver Usage\nDESCRIPTION: Shows the basic command structure for running the LRN benchmark driver.\nThe command executes the LRN benchmark with specified knobs and problem descriptions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lrn.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --lrn [benchdnn-knobs] [lrn-knobs] [lrn-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Initializing LBR GRU Primitive in oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates how to initialize a Linear-Before-Reset GRU (LBR GRU) primitive descriptor for the forward pass using the oneDNN library. It takes engine, propagation kind, direction, and memory descriptors for source layer, source iteration, weights layer, weights iteration, bias, destination layer, and destination iteration as input. The resulting primitive descriptor can then be used to create the actual primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nauto lbr_gru_pd = dnnl::lbr_gru_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_desc,\n        weights_layer_desc, weights_iter_desc, bias_desc,\n        dst_layer_desc, dst_iter_desc);\n```\n\n----------------------------------------\n\nTITLE: Constructing oneDNN Engine from OpenCL objects\nDESCRIPTION: Creates a oneDNN engine using existing OpenCL device ID and context.  This allows oneDNN to use the provided OpenCL environment for computation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::make_engine(cl_device_id, cl_context)\n```\n\n----------------------------------------\n\nTITLE: Setting Convolution Attributes with Scale and Zero Points\nDESCRIPTION: This code snippet demonstrates how to set convolution attributes, including scale and zero points for source and destination tensors, and apply post-operations such as ReLU. The configuration affects the convolution operation by scaling and shifting the input and output data.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/convolution.md#_snippet_2\n\nLANGUAGE: pseudo-code\nCODE:\n```\nprimitive_attr attr;\nattr.set_scale(src, mask=0);\nattr.set_zero_point(src, mask=0);\nattr.set_zero_point(dst, mask=0);\nattr.set_post_ops({\n        { eltwise={scale=gamma, type=relu, alpha=eta, beta=ignored } }\n    });\n\nconvolution_forward(src, weights, dst, attr);\n```\n\n----------------------------------------\n\nTITLE: USM/Buffer memory creation with memory_kind\nDESCRIPTION: Constructs a USM-based or buffer-based memory object depending on the memory allocation kind. The `handle` could be one of special values DNNL_MEMORY_ALLOCATE or DNNL_MEMORY_NONE, or it could be a user-provided USM pointer.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::make_memory(const memory::desc &, const engine &, sycl_interop::memory_kind kind, void *handle)\n```\n\n----------------------------------------\n\nTITLE: Workspace usage in forward and backward propagation in C++\nDESCRIPTION: This C++ code snippet demonstrates how to query a primitive descriptor for workspace requirements, create a memory object for the workspace (even if empty), and pass it to the execution function during both forward and backward propagation. The workspace is used to store information required for the backward pass.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/inference_and_training_aspects.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// FWD\n\nauto forward_primitive_desc = ...::primitive_desc(); // create a primitive desc\nauto workspace_md = forward_primitive_desc.workspace_desc(); // query workspace\nmemory workspace(workspace_md, engine); // create a memory (even if empty)\n\nprimitive_forward.execute(stream, {\n        ...,\n        {DNNL_ARG_WORKSPACE, workspace} // this is output\n        });\n// The workspace contains required information for the backward propagation,\n// hence should not be used anywhere else.\n\n// ...\n\n// BWD\nprimitive_backward.execute(stream, {\n        ...,\n        {DNNL_ARG_WORKSPACE, workspace} // this input/output\n        });\n// The state of the workspace is undefined here\n```\n\n----------------------------------------\n\nTITLE: Performance Measurement of F32 Forward Convolutions with ReLU (sh)\nDESCRIPTION: This snippet demonstrates how to measure the performance of f32 forward convolutions with ReLU.  It uses the `--mode=p` flag to indicate performance measurement.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv --mode=p --dt=f32 --dir=FWD_B \\\n               --attr-post-ops=relu --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Specific Sum Problem with Scales\nDESCRIPTION: This example demonstrates how to run a specific sum problem with three inputs of data types s8, s8, and u8, respectively.  It specifies the input memory layout as nhwc for each input, provides individual scaling factors, and requests the output to be in the s32 data type with nhwc layout. The input tensor shape is defined as 16x16x3x5.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_sum.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --sum --sdt=s8:s8:u8 --ddt=s32 \\\n               --stag=nhwc:nhwc:nhwc --dtag=nhwc \\\n               --scales=2:4:1 16x16x3x5\n```\n\n----------------------------------------\n\nTITLE: Report Function Hotspots with VTune\nDESCRIPTION: Reports the top 10 function hotspots identified by VTune Profiler using the `amplxe-cl` command-line tool. The report is formatted as CSV with a semicolon delimiter and grouped by process, module, and function. The output is piped to `head` to limit the results and `column` for formatting.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -report hotspots -q -r dnnl-vtune -format csv -csv-delimiter ';' -group-by process,module,function -column 'CPU Time:Self' | head -n 10 | column -t -s';'\nColumn filter is ON.\nProcess   Module            Function                               CPU Time\nbenchdnn  [Dynamic code]    _jit_avx512_common_conv_fwd_kernel     300.128503\nbenchdnn  [Dynamic code]    _jit_avx512_common_conv_fwd_kernel     293.946143\nbenchdnn  [Dynamic code]    _jit_avx512_common_conv_fwd_kernel     285.549830\nbenchdnn  [Dynamic code]    _jit_avx512_common_conv_fwd_kernel     268.868599\nbenchdnn  [Dynamic code]    _jit_avx512_common_conv_fwd_kernel     256.715527\nbenchdnn  libgomp.so.1.0.0  func@0x194f0                           186.604226\nbenchdnn  libgomp.so.1.0.0  func@0x19370                           82.609694\nbenchdnn  libdnnl.so.1.8    dnnl::impl::cpu::x64::jit_avx512_co..\n35.682241\nbenchdnn  vmlinux           [vmlinux]\n       10.763433\n```\n\n----------------------------------------\n\nTITLE: Enabling Specific oneDNN Primitives with CMake\nDESCRIPTION: This snippet demonstrates how to enable a specific set of primitives during the oneDNN build process using the `ONEDNN_ENABLE_PRIMITIVE` CMake option. Only the listed primitives will be available in the resulting library; attempting to use others will result in an unimplemented status. This helps to reduce the library size.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n-DONEDNN_ENABLE_PRIMITIVE=CONVOLUTION;MATMUL;REORDER\n```\n\n----------------------------------------\n\nTITLE: Registering a Graph Test Suite in CMake\nDESCRIPTION: This function registers a new graph test suite with a specified name and filter. It appends the test suite name and filter to global properties for later use.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(register_graph_test_suite test_suite_name filter)\n    set_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_SUITES ${test_suite_name})\n    set_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_FILTERS ${filter})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Running 3D Spatial Batch Normalization\nDESCRIPTION: This command executes a set of 3D spatial batch normalization benchmarks with 8-bit integer data type, inference propagation kind, and plain physical memory layout with dense channels. It also specifies a set of flags (`GCHR`) for the batch normalization operation. The problem descriptions are read from the `shapes_3d` file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_bnorm.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --bnorm --dt=s8 --tag=ndhwc --dir=FWD_I \\\n               --flags=GCHR --batch=shapes_3d\n```\n\n----------------------------------------\n\nTITLE: Benchmarking on Several Cores with KMP_HW_SUBSET for oneDNN (CPU)\nDESCRIPTION: This snippet provides a configuration to use only one hardware thread per core when benchmarking oneDNN on several cores within a NUMA domain. It utilizes the `KMP_HW_SUBSET` environment variable to limit OpenMP to one hardware thread per core. This is especially important when multiple hardware threads per core are enabled to avoid oversubscription and improve performance.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/perf_settings.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n$ export KMP_HW_SUBSET=1T # Use 1 hardware thread per core\n$ export OMP_PROC_BIND=close\n$ export OMP_PLACES=threads\n$ export OMP_NUM_THREADS=# desired number of cores to use\n$ numactl --membind 0 --cpunodebind 0 ./benchdnn ...\n```\n\n----------------------------------------\n\nTITLE: Initializing LBR AUGRU Primitive in oneDNN (C++)\nDESCRIPTION: This code snippet demonstrates the initialization of a Linear-Before-Reset AUGRU (LBR AUGRU) primitive descriptor for the forward pass using oneDNN. It takes the engine, propagation kind, direction, and memory descriptors for source layer, source iteration, attention, weights layer, weights iteration, bias, destination layer, and destination iteration as input. The returned primitive descriptor can be used to instantiate the LBR AUGRU primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nauto lbr_augru_pd = dnnl::lbr_augru_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_desc, attention_desc,\n        weights_layer_desc, weights_iter_desc, bias_desc,\n        dst_layer_desc, dst_iter_desc);\n```\n\n----------------------------------------\n\nTITLE: oneDNN Attribute Usage Examples\nDESCRIPTION: This snippet showcases the command-line syntax for specifying various oneDNN attributes during benchmarking. It covers attributes like scratchpad mode, fpmath mode, accumulation mode, rounding mode, deterministic mode, dropout, scales, zero points, and post-operations. The snippet provides a general overview of the available options and their respective parameters.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n    --attr-scratchpad=MODE\n    --attr-fpmath=MATHMODE[:APPLY_TO_INT]\n    --attr-acc-mode=ACCMODE\n    --attr-rounding-mode=ARG:MODE[+...]\n    --attr-deterministic=BOOL\n    --attr-dropout=PROBABILITY[:SEED[:TAG]]\n    --attr-scales=ARG:POLICY[:SCALE[:DATA_TYPE[:GROUPS]]][+...]\n    --attr-zero-points=ARG:POLICY[:ZEROPOINT[:DATA_TYPE[:GROUPS]]][+...]\n    --attr-post-ops=SUM[:SCALE[:ZERO_POINT[:DATA_TYPE]]]\n                    ELTWISE[:ALPHA[:BETA[:SCALE]]]\n                    DW:KkSsPp[:DST_DT]\n                    BINARY:DT[:MASK_INPUT[:TAG]]\n                    PRELU[:POLICY]\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN with AMD GPU Support (CMake)\nDESCRIPTION: This code snippet shows the commands to build oneDNN with AMD GPU support using CMake. It sets environment variables for the DPC++ compiler and then uses CMake to configure the build with SYCL runtime for both CPU and GPU, specifying AMD as the GPU vendor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/amd/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport CC=/path/to/dpcpp/install/bin/clang\nexport CXX=/path/to/dpcpp/install/bin/clang++\nmkdir build\ncd build\ncmake -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL \\\n      -DDNNL_GPU_VENDOR=AMD -G Ninja ..\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU ISA with CMake\nDESCRIPTION: This snippet demonstrates how to enable specific GPU instruction set architectures (ISAs) during the oneDNN build process using the `ONEDNN_ENABLE_PRIMITIVE_GPU_ISA` CMake option.  It enables XeLP and XeHP ISAs for just-in-time kernel generation.  OpenCL based kernels and implementations are always available regardless of this option.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n-DONEDNN_ENABLE_PRIMITIVE_GPU_ISA=XELP;XEHP\n```\n\n----------------------------------------\n\nTITLE: Set Constant Tensor Cache Capacity via Environment Variable in Bash\nDESCRIPTION: These commands demonstrate how to set the constant tensor cache capacity using the `ONEDNN_GRAPH_CONSTANT_TENSOR_CACHE_CAPACITY` environment variable. The first example sets the capacity for the CPU engine to 1024 MB. The second example sets the capacity for both CPU and GPU engines to 1024 MB and 2048 MB respectively. The environment variable should be set before the application starts, and the API takes precedence over it.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/graph/constant_tensor_cache.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ONEDNN_GRAPH_CONSTANT_TENSOR_CACHE_CAPACITY=\"cpu:1024\"\nexport ONEDNN_GRAPH_CONSTANT_TENSOR_CACHE_CAPACITY=\"cpu:1024;gpu:2048\"\n```\n\n----------------------------------------\n\nTITLE: Report Primitive Type Hotspots with VTune\nDESCRIPTION: Reports the top 10 primitive type hotspots identified by VTune Profiler. The report is formatted as CSV with a semicolon delimiter and grouped by task. The output is piped to `head` and `column` for filtering and formatting. The output shows the CPU time spent in different primitive types such as convolution and reorder.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -report hotspots -q -r dnnl-vtune -format csv -csv-delimiter ';' -group-by task -column 'CPU Time:Self' | head -n 10 | column -t -s';'\nColumn filter is ON.\nTask Type           CPU Time\nconvolution         1451.459338\n[Outside any task]  280.489764\nreorder             10.434821\n       10.763433\n```\n\n----------------------------------------\n\nTITLE: Run batched MatMul with strides\nDESCRIPTION: This command runs a single-precision batched matrix multiplication with custom strides, resulting in a non-dense memory layout for the destination tensor. The `--strides` option defines the strides for the source, weights, and destination tensors.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --matmul --strides=8x4x1:24x6x1:21x7x1 3x2x4:3x4x6\n```\n\n----------------------------------------\n\nTITLE: Matmul with Weights Decompression (sh)\nDESCRIPTION: This example runs a matmul problem with weights decompression. Weights in bf16 have scales and s8 zero points along the `ic` and `oc` dimensions, and have groups along the `ic` dimension. The --dt parameter specifies the data types for the source, weights and destination.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n  ./benchdnn --matmul --dt=f32:s8:f32 --attr-fpmath=bf16:true \\\n             --attr-scales=wei:per_ocic:bf16:2x1 \\\n             --attr-zero-points=wei:per_ocic:s8:2x1 6x4:4x5\n```\n\n----------------------------------------\n\nTITLE: Running Layer Normalization with Different Data Types\nDESCRIPTION: Executes a layer normalization benchmark with different data types for source and destination (bf16:f32). It uses specific memory formats for source/destination and statistics, iterating through different propagation kinds and flag combinations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lnorm.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --lnorm --dt=bf16:f32 --tag=tnc,ntc --stat_tag=tn,nt \\\n               --dir=FWD_D,BWD_DW --flags=GCH,CH 8x32x1024\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Verbose Mode\nDESCRIPTION: This example shows how to enable verbose mode (-v1) to get more detailed test information when running benchdnn with the graph driver. It also include how to use minibatch override `--mb`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=P -v1 --graph --mb=1,2,3 --case=op/f32/conv_2d.json\n```\n\n----------------------------------------\n\nTITLE: Stream Creation with SYCL queue\nDESCRIPTION: Creates a oneDNN stream based on a SYCL queue and engine. This enables oneDNN primitives to be executed within the specified SYCL queue, sharing the execution context.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/dpcpp_interoperability.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ndnnl::sycl_interop::make_stream(const engine &, sycl::queue &)\n```\n\n----------------------------------------\n\nTITLE: Constant Tensor Cache API Setter and Getter in oneDNN Graph C++\nDESCRIPTION: These are the setter and getter APIs provided by oneDNN Graph to control the constant tensor cache feature. The setter API allows users to enable the cache and set its capacity (in MB) for a specific engine kind. The getter API allows users to query the current cache capacity for a specific engine kind. These APIs provide run-time control over the cache.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/graph/constant_tensor_cache.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// setter API\n@ref dnnl_graph_set_constant_tensor_cache_capacity\n\n// getter API\n@ref dnnl_graph_get_constant_tensor_cache_capacity\n```\n\n----------------------------------------\n\nTITLE: Accessing OpenCL Command Queue from oneDNN Stream\nDESCRIPTION: Retrieves the underlying OpenCL command queue from a oneDNN stream object. The user is responsible for managing the lifetime of the returned OpenCL object.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::get_command_queue(const stream &)\n```\n\n----------------------------------------\n\nTITLE: Padding and Memory Descriptor Verification in oneDNN\nDESCRIPTION: Demonstrates how oneDNN handles padding when the number of channels is not a multiple of the block size. It illustrates how to calculate the padded channel size (`C_padded`) and creates a memory descriptor. It then performs a series of assertions to ensure that the memory descriptor's properties, such as padded dimensions, strides, and inner blocks, are correctly set to reflect the padded memory layout.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nconst int block_size = 8;\n    const int C = 17;\n    const int C_padded = div_up(17, block_size) * block_size;\n\n    const int ndims = 4;\n    memory::dims dims = {N, C, H, W};\n\n    memory::desc(dims, memory::data_type::f32, memory::format_tag::nChw8c);\n\n    memory::dim expect_stride_n =  C_padded * H * W;\n    memory::dim expect_stride_C =  H * W * block_size;\n    memory::dim expect_stride_h =  W * block_size;\n    memory::dim expect_stride_w =  block_size;\n    memory::dim expect_stride_8c = 1;\n\n    const bool expect_true = true\n        && true // logical dims stay as is\n        && md.get_dims()[0] == N\n        && md.get_dims()[1] == C\n        && md.get_dims()[2] == H\n        && md.get_dims()[3] == W\n        && true // padded dims are rounded accordingly\n        && md.get_padded_dims()[0] == N\n        && md.get_padded_dims()[1] == C_padded\n        && md.get_padded_dims()[2] == H\n        && md.get_padded_dims()[3] == W\n        && true // strides between blocks correspond to the physical layout\n        && md.get_strides()[0] == expect_stride_n\n        && md.get_strides()[1] == expect_stride_C\n        && md.get_strides()[2] == expect_stride_h\n        && md.get_strides()[3] == expect_stride_w\n        && true // inner-most blocking\n        && md.get_inner_nblks() == 1 // only 1 dim is blocked (c)\n        && md.get_inner_idxs()[0] == 1 // 1st (c) dim is blocked\n        && md.get_inner_blks()[0] == 8; // the block size is 8\n\n    assert(expect_true);\n```\n\n----------------------------------------\n\nTITLE: BRGeMM Driver Usage\nDESCRIPTION: This snippet shows the basic syntax for running the BRGeMM driver using the `benchdnn` command. It includes placeholders for `benchdnn-knobs`, `brgemm-knobs`, and `brgemm-desc`, which are further defined in the document.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_brgemm.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --brgemm [benchdnn-knobs] [brgemm-knobs] [brgemm-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Setting fpmath mode using dnnl::primitive_attr::set_fpmath_mode (C++ API)\nDESCRIPTION: This snippet shows how to enforce the floating-point math mode to an integral primitive using the `dnnl::primitive_attr::set_fpmath_mode` function from the oneDNN C++ API.  The argument `true` is passed to indicate that the floating-point math mode should be enforced for integral primitives.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_fpmath_mode.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@ref dnnl::primitive_attr::set_fpmath_mode\n```\n\n----------------------------------------\n\nTITLE: Enable ONEDNN_VERBOSE with timestamps (Shell)\nDESCRIPTION: This snippet demonstrates how to enable oneDNN verbose output with timestamps.  It sets the `ONEDNN_VERBOSE` environment variable to `profile` and the `ONEDNN_VERBOSE_TIMESTAMP` variable to `1` before executing the `benchdnn` command. This will produce verbose output including timestamps for each primitive operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/verbose.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nONEDNN_VERBOSE=profile ONEDNN_VERBOSE_TIMESTAMP=1 ./benchdnn --conv ic16ih7oc16oh7kh5ph2n\"wip\"\n```\n\n----------------------------------------\n\nTITLE: Accessing OpenCL Device from oneDNN Engine\nDESCRIPTION: Retrieves the underlying OpenCL device ID from a oneDNN engine object.  The user is responsible for managing the lifetime of the returned OpenCL object.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::get_device(const engine &)\n```\n\n----------------------------------------\n\nTITLE: Running Concat and Deducing Output Layout\nDESCRIPTION: This command demonstrates running a concat problem where the output layout is deduced (undef). It specifies the source data types as `bf16` and `f32`, the destination data type as `f32`, source memory layouts as `nChw16c:nChw16c` and `nchw:nchw`. The input tensor sizes are defined as 16x16x16x16 and 16x32x16x16.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_concat.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --concat --sdt=bf16,f32 --ddt=f32 \\\n               --stag=nChw16c:nChw16c,nchw:nchw --dtag=undef \\\n               16x16x16x16:16x32x16x16\n```\n\n----------------------------------------\n\nTITLE: Running RNN Training from Input File\nDESCRIPTION: This command runs a set of RNN training operations using the shapes defined in the specified input file, using the default settings for other parameters.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_rnn.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --rnn --batch=inputs/rnn/shapes_training\n```\n\n----------------------------------------\n\nTITLE: Convolution with Accumulation and ReLU Post-Op (sh)\nDESCRIPTION: This example demonstrates running a set of f32 forward convolutions without bias. The result is accumulated into the destination, and then ReLU is applied with a scale of 0.5. The shapes are defined using the `shapes_tails` batch.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --conv --cfg=f32 --dir=FWD_D \\\n               --attr-post-ops=sum+relu:0.5 --batch=shapes_tails\n```\n\n----------------------------------------\n\nTITLE: Enabling JIT Dump (GPU)\nDESCRIPTION: This command enables JIT code dumping for a GPU scenario. The ONEDNN_JIT_DUMP environment variable is set to 1 before running the simple-net-cpp example with the 'gpu' argument.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/inspecting_jit.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nONEDNN_JIT_DUMP=1 ./simple-net-cpp gpu\n```\n\n----------------------------------------\n\nTITLE: Sum Problem with Data Type Iteration\nDESCRIPTION: This example showcases how to run a sum problem while iterating over different source data types and physical memory layouts. The output data type is explicitly specified as f32, while the input data types and layouts are defined as comma separated lists. Undefined destination memory layout (`--dtag=undef`) allows the driver to deduce the optimal layout. The input tensor shape is 16x16x16x16.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_sum.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --sum --sdt=bf16:bf16,f32:f32 --ddt=f32 \\\n               --stag=nChw16c:nChw16c,nchw:nchw --dtag=undef 16x16x16x16\n```\n\n----------------------------------------\n\nTITLE: Reporting Top Hotspots with Linux Perf\nDESCRIPTION: This command displays the top hotspots from the performance data after injecting jitdump information. It uses perf report to generate a report showing the most time-consuming functions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n$ perf report -i perf.data.j --stdio | head -n20\n```\n\n----------------------------------------\n\nTITLE: Setting fpmath mode using dnnl_primitive_attr_set_fpmath_mode_v2 (C API)\nDESCRIPTION: This snippet shows how to enforce the floating-point math mode to an integral primitive using the `dnnl_primitive_attr_set_fpmath_mode_v2` function from the oneDNN C API.  The second argument is set to `1` to indicate that the floating-point math mode should be enforced for integral primitives.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_fpmath_mode.md#_snippet_0\n\nLANGUAGE: C\nCODE:\n```\n@ref dnnl_primitive_attr_set_fpmath_mode_v2\n```\n\n----------------------------------------\n\nTITLE: Example of potential saturation in u8/s8 dot product (AVX2/AVX-512)\nDESCRIPTION: This C++ code demonstrates a specific case where the `u8s8s32_compute_avx512` function (using VPMADDUBSW) can lead to saturation and an incorrect result.  It uses specific u8 and s8 values that, when multiplied and accumulated, exceed the s16 range, causing the intermediate `saturate_to_s16` function to clamp the values.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/int8_computations.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n    uint8_t a_u8[4] = {255, 255, 0, 0};\n    int8_t  b_s8[4] = {127, 127, 0, 0};\n    int32_t c_s32 = 0;\n\n    c_s32 = u8s8s32_compute(a_u8, b_s8, c_s32);\n    // c_s32 = 32767\n    //       = 0\n    //       + max(INT16_MIN, min(INT16_MAX, 255 * 127 + 255 * 127))\n    //       + max(INT16_MIN, min(INT16_MAX,   0 *   0 +   0 *   0));\n    // While one might expect 64770 = 255 * 127 + 255 * 127 + 0 * 0 + 0 * 0;\n```\n\n----------------------------------------\n\nTITLE: Running Convolution/Deconvolution Benchmarks (sh)\nDESCRIPTION: This snippet shows the basic syntax for running convolution and deconvolution benchmarks using the benchdnn tool. It includes options for specifying benchmark knobs and convolution knobs, as well as the convolution description.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv [benchdnn-knobs] [conv-knobs] [conv-desc] ...\n./benchdnn --deconv [benchdnn-knobs] [conv-knobs] [conv-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Example oneDNN Verbose Output\nDESCRIPTION: This code snippet displays an example of the verbose output generated by oneDNN when the `ONEDNN_VERBOSE` and `ONEDNN_VERBOSE_TIMESTAMP` environment variables are enabled. It provides information on the oneDNN version, runtime, ISA, engine, and detailed profiling data for each primitive operation (e.g., convolution, reorder) including execution time and cache behavior.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/verbose.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nonednn_verbose,v0,info,oneDNN v3.2.0 (commit 6afab8e57f65a8995685d97ba6f80fa6c24b87a0)\nonednn_verbose,v0,info,cpu,runtime:OpenMP,nthr:128\nonednn_verbose,v0,info,cpu,isa:Intel AVX-512 with Intel DL Boost\nonednn_verbose,v0,info,gpu,runtime:none\nonednn_verbose,v0,info,graph,backend,0:dnnl_backend\nonednn_verbose,v0,primitive,info,template:timestamp,operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time\nonednn_verbose,v0,graph,info,template:timestamp,operation,engine,partition_id,partition_kind,op_names,data_formats,logical_tensors,fpmath_mode,backend,exec_time\nonednn_verbose,v0,1693533460193.346924,primitive,create:cache_miss,cpu,convolution,jit:avx512_core,forward_training,src_f32:a:blocked:aBcd16b::f0 wei_f32:a:blocked:ABcd16b16a::f0 bia_f32:a:blocked:a::f0 dst_f32:a:blocked:aBcd16b::f0,,alg:convolution_direct,mb2_ic16oc16_ih7oh7kh5sh1dh0ph2_iw7ow7kw5sw1dw0pw2,0.709961\nonednn_verbose,v0,1693533460194.199951,primitive,create:cache_hit,cpu,convolution,jit:avx512_core,forward_training,src_f32:a:blocked:aBcd16b::f0 wei_f32:a:blocked:ABcd16b16a::f0 bia_f32:a:blocked:a::f0 dst_f32:a:blocked:aBcd16b::f0,,alg:convolution_direct,mb2_ic16oc16_ih7oh7kh5sh1dh0ph2_iw7ow7kw5sw1dw0pw2,0.0161133\nonednn_verbose,v0,1693533460228.559082,primitive,create:cache_miss,cpu,reorder,jit:uni,undef,src_f32::blocked:abcd::f0 dst_f32::blocked:ABcd16b16a::f0,,,16x16x5x5,0.724854\nonednn_verbose,v0,1693533460229.437012,primitive,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:abcd::f0 dst_f32::blocked:ABcd16b16a::f0,,,16x16x5x5,16.481\nonednn_verbose,v0,1693533460259.165039,primitive,create:cache_miss,cpu,reorder,jit:blk,undef,src_f32::blocked:abcd::f0 dst_f32::blocked:aBcd16b::f0,,,2x16x7x7,0.349854\nonednn_verbose,v0,1693533460259.586914,primitive,exec,cpu,reorder,jit:blk,undef,src_f32::blocked:abcd::f0 dst_f32::blocked:aBcd16b::f0,,,2x16x7x7,12.604\nonednn_verbose,v0,1693533460272.332031,primitive,create:cache_miss,cpu,reorder,simple:any,undef,src_f32::blocked:a::f0 dst_f32::blocked:a::f0,,,16,0.0358887\nonednn_verbose,v0,1693533460272.416992,primitive,exec,cpu,reorder,simple:any,undef,src_f32::blocked:a::f0 dst_f32::blocked:a::f0,,,16,0.052002\nonednn_verbose,v0,1693533460272.561035,primitive,exec,cpu,convolution,jit:avx512_core,forward_training,src_f32:a:blocked:aBcd16b::f0 wei_f32:a:blocked:ABcd16b16a::f0 bia_f32:a:blocked:a::f0 dst_f32:a:blocked:aBcd16b::f0,,alg:convolution_direct,mb2_ic16oc16_ih7oh7kh5sh1dh0ph2_iw7ow7kw5sw1dw0pw2,0.0878906\nonednn_verbose,v0,1693533460313.719971,primitive,create:cache_miss,cpu,reorder,jit:blk,undef,src_f32::blocked:aBcd16b::f0 dst_f32::blocked:abcd::f0,,,2x16x7x7,0.275146\nonednn_verbose,v0,1693533460314.072021,primitive,exec,cpu,reorder,jit:blk,undef,src_f32::blocked:aBcd16b::f0 dst_f32::blocked:abcd::f0,,,2x16x7x7,18.8389\n0:PASSED __REPRO: --conv ic16ih7oc16oh7kh5ph2nwip\n```\n\n----------------------------------------\n\nTITLE: Running Layer Normalization with Specific Parameters\nDESCRIPTION: Executes a layer normalization benchmark with single-precision floating-point data types and specific memory formats for source/destination and statistics. It iterates through different propagation kinds (forward training, backward by data and weights) and flag combinations (GCH, CH).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lnorm.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --lnorm --dt=f32 --tag=tnc,ntc --stat_tag=tn,nt \\\n               --dir=FWD_D,BWD_DW --flags=GCH,CH 8x32x1024\n```\n\n----------------------------------------\n\nTITLE: DNNL CMake Integration\nDESCRIPTION: This code snippet demonstrates two methods for integrating DNNL into a CMake project: using `find_package` or including DNNL as a sub-project.  The example shows how to link the DNNL library to a target application.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/transition-to-dnnl.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# Through find package\nfind_package(dnnl DNNL CONFIG REQUIRED)\ntarget_link_libraries(project_app DNNL::dnnl)\n\n# Or direct sub-project inclusion\nadd_subdirectory(${DNNL_DIR} DNNL)\ninclude_directories(${DNNL_DIR}/include)\ntarget_link_libraries(project_app dnnl)\n```\n\n----------------------------------------\n\nTITLE: Installing the oneDNN Library in CMake\nDESCRIPTION: This snippet installs the built oneDNN library and its associated header files using the `install` command. It specifies the destination directories for the runtime, library, and archive components, and also handles header file installation. The install commands define the location for the library and headers after the build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(LIB_EXPORT_NAME \"${LIB_PACKAGE_NAME}-targets\")\ninstall(TARGETS ${LIB_PACKAGE_NAME}\n    EXPORT \"${LIB_EXPORT_NAME}\"\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR})\n\n# If only Cmake could preserve the directory hierarchy...\nforeach(header ${HEADERS_ROOT})\n    install(FILES ${header} DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/\")\nendforeach()\nforeach(header ${HEADERS_SUBDIR})\n    install(FILES ${header} DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/oneapi/dnnl/\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Running Concat with Input Argument Scales\nDESCRIPTION: This example demonstrates running concat with input argument scales. The source and destination data types are f32. The source and destination layouts are nchw. The scale attributes are set for msrc0 and msrc1 using common scales of 1.5 and 2.5 respectively.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_concat.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --concat --sdt=f32 --ddt=f32 --stag=nchw:nchw --dtag=nchw \\\n               --attr-scales=msrc0:common:1.5+msrc1:common:2.5 \\\n               16x16x16x16:16x32x16x16\n```\n\n----------------------------------------\n\nTITLE: Entitlements plist for macOS hardened runtime\nDESCRIPTION: This XML snippet represents the `Entitlements.plist` file required for macOS hardened runtime. It declares the `com.apple.security.cs.allow-jit` entitlement, enabling Just-In-Time (JIT) code generation, which is necessary for oneDNN.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/link.md#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>com.apple.security.cs.allow-jit</key><true/>\n</dict>\n</plist>\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory Descriptor with Strides in oneDNN (C++)\nDESCRIPTION: This code shows how to initialize a oneDNN memory descriptor with custom strides using the C API.  It requires the `dnnl.h` header. It creates a memory descriptor `md` and initializes it with the specified dimensions, data type, and strides.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl_dims_t dims = {N, C, H, W};\n    dnnl_dims_t strides = {stride_n, stride_c, stride_h, stride_w};\n\n    dnnl_memory_desc_t md;\n    dnnl_memory_desc_init_by_strides(&md, 4, dims, dnnl_f32, strides);\n```\n\n----------------------------------------\n\nTITLE: Running Batch File with Different Algorithms (sh)\nDESCRIPTION: This snippet shows how to run a batch file with different convolution algorithms (DIRECT, WINO, AUTO). It also demonstrates ignoring dnnl_unimplemented errors.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv --alg=DIRECT,WINO,AUTO --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Injecting JITdump Information into Perf Data\nDESCRIPTION: This command injects the information from the jitdump files into the performance data recorded by perf. This allows perf to attribute performance data to JIT-compiled code.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n$ perf inject -j -i perf.data -o perf.data.j\n```\n\n----------------------------------------\n\nTITLE: Running F32 Forward Convolutions with ReLU Post-op (sh)\nDESCRIPTION: This example shows how to run f32 forward convolutions with a ReLU post-operation.  It includes the `--attr-post-ops` flag to specify the ReLU operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv --dt=f32 --dir=FWD_B \\\n               --attr-post-ops=relu --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Enabling GEMM Kernels ISA with CMake\nDESCRIPTION: This snippet illustrates how to selectively enable GEMM kernel instruction set architectures (ISAs) using the `ONEDNN_ENABLE_GEMM_KERNELS_ISA` CMake option. It enables SSE41 and AVX2 ISAs while disabling AVX512 and AMX kernels. The values are ordered such that enabling a higher ISA implies enabling all lower ISAs as well.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n-DONEDNN_ENABLE_GEMM_KERNELS_ISA=AVX2\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Compilation\nDESCRIPTION: This snippet conditionally adds subdirectories to the build process based on the specified runtime.  The `add_subdirectory` command includes the CMakeLists.txt file present in those directories, effectively adding those modules to the overall oneDNN build.  The conditions check for CPU, GPU, and SYCL runtime enablement.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(common)\n\nif(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    add_subdirectory(cpu)\nendif()\n\nif(NOT DNNL_GPU_RUNTIME STREQUAL \"NONE\")\n    add_subdirectory(gpu)\nendif()\n\nif(DNNL_WITH_SYCL OR DNNL_GPU_RUNTIME STREQUAL \"OCL\")\n    add_subdirectory(xpu)\nendif()\n\nif(ONEDNN_BUILD_GRAPH)\n    message(STATUS \"Graph component is enabled\")\n\n    if (NOT DNNL_GPU_RUNTIME STREQUAL \"NONE\" AND NOT DNNL_GPU_VENDOR STREQUAL \"INTEL\" AND NOT DNNL_GPU_VENDOR STREQUAL \"NVIDIA\")\n        message(FATAL_ERROR \"Graph API does not support ${DNNL_GPU_VENDOR} GPU. \"\n            \"Either disable Graph API with ONEDNN_BUILD_GRAPH=OFF or change GPU \"\n            \"vendor to INTEL or NVIDIA.\")\n    endif()\n\n    if (NOT DNNL_ENABLE_PRIMITIVE STREQUAL \"ALL\")\n        message(FATAL_ERROR \"Graph API does not support selecting primitives. \"\n            \"Either disable Graph API with ONEDNN_BUILD_GRAPH=OFF or change \"\n            \"primitive selection with ONEDNN_ENABLE_PRIMITIVE=ALL.\")\n    endif()\n\n    if(ONEDNN_ENABLE_GRAPH_DUMP)\n        message(STATUS \"Graph artifacts dump is enabled\")\n        add_definitions_with_host_compiler(-DDNNL_ENABLE_GRAPH_DUMP)\n    endif()\n\n    add_subdirectory(graph)\nelse()\n    # If graph component is not built, remove the headers from build and installation.\n    list(REMOVE_ITEM HEADERS_SUBDIR\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/dnnl_graph.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/dnnl_graph.hpp\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/dnnl_graph_types.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/dnnl_graph_sycl.h\"\n    \"${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/dnnl_graph_sycl.hpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Named Problem with Different Data Types\nDESCRIPTION: Executes a group normalization benchmark with different data types for source and destination, while iterating over memory formats, propagation kinds, and flag combinations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_gnorm.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --gnorm --dt=bf16:f32 --tag=abc,acb --dir=FWD_D,BWD_DW \\\n               --flags=GCH,CH g5ic5iw10_n\"gnorm_test_shape\"\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Example Execution\nDESCRIPTION: This example showcases how to run a specific graph partition using the benchdnn tool and graph driver, utilizing a JSON file that defines the graph's structure and parameters.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=P --graph --case=./tests/benchdnn/inputs/graph/pattern/f32/conv_post_ops_fusion.json\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN with Threadpool\nDESCRIPTION: This snippet demonstrates how to build oneDNN with threadpool threading support by setting the `ONEDNN_CPU_RUNTIME` CMake option to `THREADPOOL`.  This configuration is considered experimental.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_CPU_RUNTIME=THREADPOOL ..\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Inner Product Performance Measurement Example\nDESCRIPTION: This example demonstrates how to run benchdnn for inner products in performance mode, limiting the maximum time per problem and using the default performance template. It showcases the command-line invocation and a sample output.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --ip --mode=p --max-ms-per-prb=6000 \\\n               --batch=inputs/ip/test_ip_all\n```\n\n----------------------------------------\n\nTITLE: Enabling Precise Division and Square Root for Intel Compiler\nDESCRIPTION: This block checks if the C++ compiler is Intel. If it is, it locates files related to normalization and resampling, and sets compiler flags (`/Qprec-sqrt`, `/Qprec-div` for Windows, `-prec-sqrt`, `-prec-div` otherwise) to enable precise division and square root computations. This aims to improve numerical stability and align jitted code with reference implementations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\")\n    # to make computations more stable and to align the jitted code\n    # with the reference one use precise division and square root\n    # by default\n    file(GLOB FILES_REQUIRED_PREC_SQRT\n        ${CMAKE_CURRENT_SOURCE_DIR}/*normalization*.cpp\n        )\n    file(GLOB FILES_REQUIRED_PREC_DIV\n        ${CMAKE_CURRENT_SOURCE_DIR}/*resampling*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/*normalization*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/ref_eltwise.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/nhwc_pooling.cpp\n        )\n    if(WIN32)\n        set_source_files_properties(${FILES_REQUIRED_PREC_SQRT}\n            PROPERTIES COMPILE_FLAGS \"/Qprec-sqrt\")\n        set_source_files_properties(${FILES_REQUIRED_PREC_DIV}\n            PROPERTIES COMPILE_FLAGS \"/Qprec-div\")\n    else()\n        set_source_files_properties(${FILES_REQUIRED_PREC_SQRT}\n            PROPERTIES COMPILE_FLAGS \"-prec-sqrt\")\n        set_source_files_properties(${FILES_REQUIRED_PREC_DIV}\n            PROPERTIES COMPILE_FLAGS \"-prec-div\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running F32 Forward Convolutions with Bias (sh)\nDESCRIPTION: This example demonstrates how to run a set of f32 forward convolutions with bias using the benchdnn tool. It specifies the data type, direction, and input batch file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv --dt=f32 --dir=FWD_B --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Sum Driver Invocation in benchdnn\nDESCRIPTION: This command invokes the sum driver within the benchdnn framework.  It demonstrates the basic structure of the command, highlighting the use of `benchdnn-knobs`, `sum-knobs`, and `sum-desc` for customizing the benchmark. The command requires the benchdnn executable to be in the system's PATH or the full path to be specified.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_sum.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --sum [benchdnn-knobs] [sum-knobs] [sum-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Running Specific Shuffle Problem\nDESCRIPTION: This command runs a specific shuffle problem with defined parameters like direction, data type, memory layout, group size, and axis. It demonstrates the use of multiple options to configure the shuffle operation with the FWD_D direction, various data types, nchw tag, group size of 4, and axis 2. The problem size is specified as 1x68x56x56.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_shuffle.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --shuffle --dir=FWD_D --dt=f32,s32,s8,u8,bf16 \\\n               --tag=nchw --group=4 --axis=2 1x68x56x56\n```\n\n----------------------------------------\n\nTITLE: Strides specification usage\nDESCRIPTION: This snippet shows the general syntax for specifying strides using the `--strides` option in the benchdnn tool. It outlines how to specify strides for source (SRC), weights (WEI), and destination (DST) tensors.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_strides.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n    --strides=SRC_DIMS[:WEI_DIMS]:DST_DIMS\n```\n\n----------------------------------------\n\nTITLE: Run batched MatMul with strides and memory formats\nDESCRIPTION: This command runs a batched matrix multiplication with specified memory format tags and strides.  The destination memory format is implicitly derived based on the source and weights format and the strides, and therefore cannot be explicitly specified with `--dtag`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n    # --dtag cannot be specified here\n    ./benchdnn --matmul --stag=bax --wtag=abx --strides=::8x4x1 2x2x3:2x3x2\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose Mode with Environment Variable\nDESCRIPTION: This example demonstrates how to enable verbose mode and set the filter using the `ONEDNN_VERBOSE` environment variable. It sets the verbosity to `profile_exec` and filters the output to only include the `graph` component. This is useful for profiling the execution of compiled partitions from the graph API.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/verbose.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nONEDNN_VERBOSE=profile_exec,filter=graph\n```\n\n----------------------------------------\n\nTITLE: Enabling Arm Compute Library (ACL)\nDESCRIPTION: This snippet shows how to enable Arm Compute Library integration in oneDNN using the `ONEDNN_AARCH64_USE_ACL` CMake option.  It assumes the `ACL_ROOT_DIR` environment variable is set.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_AARCH64_USE_ACL=ON ..\n```\n\n----------------------------------------\n\nTITLE: Running Benchdnn with Graph Driver\nDESCRIPTION: This code snippet demonstrates how to execute the benchdnn tool with the graph driver. It shows the basic command structure and mentions optional graph knobs to customize the execution.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --graph [benchdnn-knobs] [graph-knobs] [graph-case] ...\n```\n\n----------------------------------------\n\nTITLE: Running Explicitly Specified Forward Convolution (sh)\nDESCRIPTION: This example demonstrates how to run a specifically defined forward convolution with minibatch and verbose settings for different data types.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv -v1 --mb=4 --dir=FWD_B --dt=f32,u8:s8:u8 \\\n               ic3ih227iw227_oc96oh55ow55_kh11kw11_sh4sw4ph0pw0_n\"alexnet:conv1\"\n```\n\n----------------------------------------\n\nTITLE: Enabling OpenCL GPU Runtime\nDESCRIPTION: This snippet shows how to enable the OpenCL GPU runtime in oneDNN by setting the `ONEDNN_GPU_RUNTIME` CMake option to `OCL` and specifying the path to the OpenCL SDK using `-DOPENCLROOT`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_GPU_RUNTIME=OCL -DOPENCLROOT=/path/to/opencl/sdk ..\n```\n\n----------------------------------------\n\nTITLE: Reorder Example with Data Type and Layout Specifications\nDESCRIPTION: This example demonstrates how to run two specific reorders with s8 source and destination data types and specific input and output physical memory layouts. It also showcases the usage of the `s8s8_comp` flag with a mask.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reorder.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --reorder --sdt=s8 --ddt=s8 --stag=hwio --dtag=OIhw4i16o4i \\\n               32x32x3x3 \\\n               --oflag=s8s8_comp:1 16x32x7x5\n```\n\n----------------------------------------\n\nTITLE: RNN Driver Usage\nDESCRIPTION: This command executes the RNN driver within the benchdnn framework. It takes rnn-knobs to customize the RNN behavior and rnn-desc to define the problem descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_rnn.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --rnn [benchdnn-knobs] [rnn-knobs] [rnn-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Setting Deterministic Attribute in C++ API\nDESCRIPTION: This snippet shows how to set the deterministic attribute for a primitive using the oneDNN C++ API. It employs the `dnnl::primitive_attr::set_deterministic` function to control deterministic execution. Using this API is essential for ensuring consistent and reproducible results, and for debugging purposes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_deterministic.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n@ref dnnl::primitive_attr::set_deterministic\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN with TBB\nDESCRIPTION: This snippet shows how to build oneDNN with Threading Building Blocks (TBB) support by setting the `ONEDNN_CPU_RUNTIME` CMake option to `TBB`. It may also require setting the `TBBROOT` environment variable or passing the TBB installation path directly to CMake.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_CPU_RUNTIME=TBB ..\n```\n\n----------------------------------------\n\nTITLE: General Offset Calculation with Strides\nDESCRIPTION: This snippet defines the general offset function that uses strides to calculate the memory address displacement. It takes into account the strides for each dimension (n, c, h, w).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\noffset(n, c, h, w) = n * stride_n\n                       + c * stride_c\n                       + h * stride_h\n                       + w * stride_w\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN with TBB and specifying TBBROOT\nDESCRIPTION: This snippet demonstrates building oneDNN with TBB support and explicitly specifying the TBB installation path using the `TBBROOT` CMake variable.  This is necessary if TBB is not in the default search path.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_CPU_RUNTIME=TBB -DTBBROOT=/opt/intel/path/tbb ..\n```\n\n----------------------------------------\n\nTITLE: Run a named LRN problem with specific settings\nDESCRIPTION: Runs a specific LRN problem with single-precision data types, iterating through different memory layouts, directions (forward and backward), and algorithms.\nThe example configures data type, memory tag, direction, and algorithm for the LRN benchmark.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lrn.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --lrn --dt=f32 --tag=nchw,nChw8c,nChw16c \\\n               --dir=FWD_D,BWD_D --alg=ACROSS,WITHIN \\\n               mb256ic96_ih55n\"alexnet:norm1\"\n```\n\n----------------------------------------\n\nTITLE: Running Named Problem with Iteration\nDESCRIPTION: This command runs a specific pooling problem with single-precision floating-point data, iterating through different memory layouts (channel blocking of 8 and 16), forward and backward propagation kinds, algorithm combinations (max, avg_np, avg_p), and minibatch sizes (0 and 5).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_pool.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n   ./benchdnn --pool --dt=f32 --tag=nChw8c,nChw16c \\\n               --dir=FWD_D,FWD_I,BWD_D --alg=max,avg_np,avg_p --mb=0,5 \\\n               mb96ic768_ih17oh17_kh3sh1ph1n\"googlenet_v3:ave_pool_mixed_4_pool\"\n```\n\n----------------------------------------\n\nTITLE: Running Batch Normalization from Input File\nDESCRIPTION: This command executes a set of batch normalization benchmarks defined in an input file. It utilizes the default settings for the benchmark and reads the problem descriptions from the specified file, which is located at `inputs/bnorm/shapes_resnet_50`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_bnorm.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --bnorm --batch=inputs/bnorm/shapes_resnet_50\n```\n\n----------------------------------------\n\nTITLE: Sum Problem Descriptor Example\nDESCRIPTION: This example shows the canonical form of a problem descriptor for the sum driver. `N` represents an integer number. The descriptor defines a 3D spatial problem with logical dimensions N, C, D, H, and W. Trailing dimensions can be removed to specify problems with fewer dimensions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_sum.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    NxNxNxNxN\n```\n\n----------------------------------------\n\nTITLE: Including Directories for oneDNN Examples with CMake\nDESCRIPTION: This snippet includes the necessary directories for compiling oneDNN examples. It uses `include_directories_with_host_compiler` to ensure the correct compiler is used when including the header files from both the `include` and `examples` directories.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/examples/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(${PROJECT_SOURCE_DIR}/include)\ninclude_directories_with_host_compiler(${PROJECT_SOURCE_DIR}/examples)\n```\n\n----------------------------------------\n\nTITLE: Run MatMul with shapes_2d input file\nDESCRIPTION: This command executes the default validation set of MatMul using an input file that defines the shapes of the matrices. The `--batch` option specifies the path to the file containing the input shapes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --matmul --batch=inputs/matmul/shapes_2d\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Driver Usage\nDESCRIPTION: This command shows the basic structure for running the batch normalization benchmark. It uses the `benchdnn` tool with the `--bnorm` option. Users can specify various knobs for fine-grained control over the benchmark, including direction, data type, memory layout tag, strides, and normalization flags. The command also requires a problem descriptor defining the specific batch normalization problem to be executed.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_bnorm.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --bnorm [benchdnn-knobs] [bnorm-knobs] [bnorm-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Performance Template Example\nDESCRIPTION: This command-line option provides a custom performance template for benchdnn. The template contains bandwidth metrics (%-Gbw%, %0Gbw%) to inspect best and average bandwidth achieved during execution, which is recommended for cold cache measurements to show real RAM bandwidth.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_cold_cache.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n--perf-template=%-Gbw%,%0Gbw%\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Cold Cache Example with TLB Extension\nDESCRIPTION: This command-line example enables the cold cache feature for all execution arguments (`all` mode) and adds the TLB extension with a size of 2 Gigabytes. This simulates cold cache for all arguments, inducing TLB misses during the performance run.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_cold_cache.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n--cold-cache=all+tlb:2G\n```\n\n----------------------------------------\n\nTITLE: Disabling JIT Profiling\nDESCRIPTION: This code block disables JIT profiling if `DNNL_ENABLE_JIT_PROFILING` is not enabled. It defines a preprocessor macro to disable the profiling interface and removes specific source files related to linux_perf and VTune Profiler from the `SOURCES_EXTRA` list if the target architecture is X64 or AARCH64. Note: the profiling interface itself will still be built.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_ENABLE_JIT_PROFILING)\n    # XXX: the profiling interface will still be built and present\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_JIT_PROFILING=0)\n    # Don't enable support for linux_perf and VTune Profiler\n    if((DNNL_TARGET_ARCH STREQUAL \"X64\") OR (DNNL_TARGET_ARCH STREQUAL \"AARCH64\"))\n        list(REMOVE_ITEM SOURCES_EXTRA\n            \"${CMAKE_CURRENT_SOURCE_DIR}/jit_utils/jitprofiling/jitprofiling.c\"\n            \"${CMAKE_CURRENT_SOURCE_DIR}/jit_utils/linux_perf/linux_perf.cpp\"\n            )\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Eltwise Post-op Parameters in C++\nDESCRIPTION: This code snippet shows the parameters for the eltwise post-op in the C++ API for oneDNN. The parameters `alg`, `alpha`, and `beta` are the same as in the regular eltwise primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_post_ops.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nvoid dnnl::post_ops::append_eltwise(\n        algorithm alg, float alpha, float beta // same as in eltwise primitive\n        );\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose Logging with Logfile Specification\nDESCRIPTION: This bash command enables verbose logging in oneDNN and specifies the path to the logfile. It sets the `ONEDNN_VERBOSE` environment variable to 'all' and `ONEDNN_VERBOSE_LOGFILE` to the desired logfile path before executing the CNN inference example. This will save the verbose outputs generated by oneDNN to the specified logfile.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/experimental.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ONEDNN_VERBOSE=all ONEDNN_VERBOSE_LOGFILE=./logs/cnn_test_logger.log ./examples/cnn-inference-f32-cpp\n```\n\n----------------------------------------\n\nTITLE: Query Scratchpad Memory Consumption C++\nDESCRIPTION: This code snippet demonstrates how to query the amount of memory that a oneDNN primitive will use for its scratchpad when the library manages the scratchpad. It retrieves the memory consumption using `query_s64(dnnl::query::memory_consumption_s64)` and asserts that the scratchpad memory descriptor is empty, as the library manages the allocation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_scratchpad.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// Use default attr, hence the library allocates scratchpad\ndnnl::primitive::primitive_desc op_pd(engine, params, ...);\n\n// Print how much memory would be hold by a primitive due to scratchpad\nstd::cout << \"primitive will use \"\n          << op_pd.query_s64(dnnl::query::memory_consumption_s64)\n          << \" bytes\" << std::endl;\n\n// In this case scratchpad is internal, hence user visible scratchpad memory\n// descriptor should be empty:\nauto zero_md = dnnl::memory::desc();\nassert(op_pd.scratchpad_desc() == zero_md);\n```\n\n----------------------------------------\n\nTITLE: Running Concat with Input Batch File\nDESCRIPTION: This command executes the concat benchmark using a batch file located at `inputs/concat/test_concat_all`. It utilizes the default settings defined in the batch file for the concat operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_concat.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --concat --batch=inputs/concat/test_concat_all\n```\n\n----------------------------------------\n\nTITLE: Run Sum from test_sum_all\nDESCRIPTION: This example executes the sum driver on a set of problems defined in the `test_sum_all` file. It uses the default settings for the benchmark.  The `--batch` option specifies the input file containing the problem definitions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_sum.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --sum --batch=inputs/sum/test_sum_all\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Tests (CMake)\nDESCRIPTION: This snippet adds include directories required for building the tests. It includes the third_party directory and the project source directory, allowing tests to access necessary headers. The `include_directories_with_host_compiler` function is assumed to handle compiler compatibility.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(${PROJECT_SOURCE_DIR}/third_party)\n\n# propagate TEST specific flags\nappend(CMAKE_C_FLAGS \"${CMAKE_TEST_CCXX_FLAGS}\")\nappend(CMAKE_CXX_FLAGS \"${CMAKE_TEST_CCXX_FLAGS}\")\n\nset(CMAKE_TEST_CCXX_NOWARN_FLAGS)\n\n# propagate no warning flags\nappend(CMAKE_TEST_CCXX_NOWARN_FLAGS \"${CMAKE_CCXX_NOWARN_FLAGS}\")\n\n# propagate sanitizer flags\nappend(CMAKE_C_FLAGS \"${CMAKE_CCXX_SANITIZER_FLAGS}\")\nappend(CMAKE_CXX_FLAGS \"${CMAKE_CCXX_SANITIZER_FLAGS}\")\n\n# allow tests to include internal header files with, e.g.\n# include \"src/common/dnnl_thread.hpp\" ...\ninclude_directories_with_host_compiler(${PROJECT_SOURCE_DIR})\n# ... and allow the included files transitively include other headers files\n# e.g. include \"common/dnnl_thread_parallel_nd.hpp\" from \"dnnl_thread.hpp\"\ninclude_directories_with_host_compiler(${PROJECT_SOURCE_DIR}/src)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Batch Execution\nDESCRIPTION: This example shows how to run a batch file containing multiple graph benchmarks using the benchdnn tool and graph driver.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=P --graph --batch=test_graph_ci\n```\n\n----------------------------------------\n\nTITLE: Running PReLU Benchmark\nDESCRIPTION: Executes the PReLU benchmark using the benchdnn tool. Allows specifying benchdnn knobs, prelu knobs, and prelu descriptors. The command structure is `./benchdnn --prelu [benchdnn-knobs] [prelu-knobs] [prelu-desc] ...`\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_prelu.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --prelu [benchdnn-knobs] [prelu-knobs] [prelu-desc] ...\n```\n\n----------------------------------------\n\nTITLE: MatMul Driver Invocation\nDESCRIPTION: This snippet shows the basic syntax for invoking the MatMul driver with `benchdnn`. It includes placeholders for `benchdnn-knobs`, `matmul-knobs`, and `matmul-desc`, which represent the various command-line options and problem descriptors that can be used to configure the MatMul benchmark.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --matmul [benchdnn-knobs] [matmul-knobs] [matmul-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Defining OneDNN Convolution Configuration in JSON\nDESCRIPTION: This JSON snippet configures a convolution operation in OneDNN. It defines the version, engine kind (CPU), fpmath mode, and includes a graph representing the convolution.  The graph details include attributes like strides, padding, data format, dilations, and input/output tensor characteristics such as data type, shape, and stride.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"0.5.0\",\n  \"engine_kind\": \"cpu\",\n  \"fpmath_mode\": \"strict\",\n  \"graph\": [\n    {\n      \"id\": 0,\n      \"name\": \"Convolution\",\n      \"kind\": \"Convolution\",\n      \"attrs\": {\n        \"strides\": {\n          \"type\": \"s64[]\",\n          \"value\": [\n            2,\n            2\n          ]\n        },\n        \"pads_begin\": {\n          \"type\": \"s64[]\",\n          \"value\": [\n            0,\n            0\n          ]\n        },\n        \"auto_pad\": {\n          \"type\": \"string\",\n          \"value\": \"None\"\n        },\n        \"data_format\": {\n          \"type\": \"string\",\n          \"value\": \"NCX\"\n        },\n        \"pads_end\": {\n          \"type\": \"s64[]\",\n          \"value\": [\n            -1,\n            -1\n          ]\n        },\n        \"groups\": {\n          \"type\": \"s64\",\n          \"value\": 1\n        },\n        \"dilations\": {\n          \"type\": \"s64[]\",\n          \"value\": [\n            1,\n            1\n          ]\n        },\n        \"weights_format\": {\n          \"type\": \"string\",\n          \"value\": \"OIX\"\n        }\n      },\n      \"inputs\": [\n        {\n          \"id\": 0,\n          \"dtype\": \"f32\",\n          \"shape\": [\n            28,\n            512,\n            28,\n            28\n          ],\n          \"stride\": [\n            401408,\n            1,\n            14336,\n            512\n          ],\n          \"layout_type\": \"strided\",\n          \"property_type\": \"undef\"\n        },\n        {\n          \"id\": 1,\n          \"dtype\": \"f32\",\n          \"shape\": [\n            1024,\n            512,\n            1,\n            1\n          ],\n          \"stride\": [\n            512,\n            1,\n            1,\n            1\n          ],\n          \"layout_type\": \"strided\",\n          \"property_type\": \"constant\"\n        }\n      ],\n      \"outputs\": [\n        {\n          \"id\": 2,\n          \"dtype\": \"f32\",\n          \"shape\": [\n            28,\n            1024,\n            14,\n            14\n          ],\n          \"stride\": [\n            200704,\n            1,\n            14336,\n            1024\n          ],\n          \"layout_type\": \"strided\",\n          \"property_type\": \"undef\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Select Operation Formula\nDESCRIPTION: This code snippet shows the general formula for the select operation. The output `dst[i]` is selected from `src_0[i]` or `src_1[i]` based on the value of `cond[i]`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/graph/operations/Select.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ndst[i] = cond[i] ? src_0[i] : src_1[i]\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for oneDNN Examples with CMake\nDESCRIPTION: This snippet sets C and C++ compiler flags for oneDNN examples, including flags for SYCL compilation (targeting NVIDIA and AMD GPUs), sanitizer flags, and flags to suppress warnings. It appends the specified flags to the existing CMAKE_C_FLAGS and CMAKE_CXX_FLAGS.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/examples/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nappend(CMAKE_C_FLAGS \"${CMAKE_EXAMPLE_CCXX_FLAGS}\")\nappend(CMAKE_CXX_FLAGS \"${CMAKE_EXAMPLE_CCXX_FLAGS}\")\n\nif(DNNL_WITH_SYCL)\n    if(DNNL_SYCL_GENERIC)\n        CHECK_CXX_COMPILER_FLAG(\"-fsycl -fsycl-targets=nvptx64-nvidia-cuda\" NVIDIA_TARGET_SUPPORTED)\n    endif()\n\n    # Enable linking SYCL kernels.\n    if(DNNL_SYCL_CUDA OR (DNNL_SYCL_GENERIC AND NVIDIA_TARGET_SUPPORTED))\n        append(CMAKE_CXX_FLAGS \"-fsycl-targets=nvptx64-nvidia-cuda\")\n        append(CMAKE_CXX_FLAGS \"-Wno-linker-warnings\")\n    endif()\n\n    if(DNNL_AMD_ENABLE_SYCL_KERNELS)\n        append(CMAKE_CXX_FLAGS \"-fsycl-targets=amdgcn-amd-amdhsa -Xsycl-target-backend --offload-arch=${DNNL_AMD_SYCL_KERNELS_TARGET_ARCH}\")\n    endif()\nendif()\n\n# propagate sanitizer flags\nappend(CMAKE_C_FLAGS \"${CMAKE_CCXX_SANITIZER_FLAGS}\")\nappend(CMAKE_CXX_FLAGS \"${CMAKE_CCXX_SANITIZER_FLAGS}\")\n\n# propagate nowarn flags\nappend(CMAKE_C_FLAGS \"${CMAKE_CCXX_NOWARN_FLAGS}\")\nappend(CMAKE_CXX_FLAGS \"${CMAKE_CCXX_NOWARN_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for GPU ISA\nDESCRIPTION: This code block conditionally compiles the generator code based on the GPU ISA. If DPCPP_HOST_COMPILER_KIND is DEFAULT, it removes the standard generator.cpp from the source list and compiles specialized versions for different GPU ISAs defined in DNNL_ENABLE_PRIMITIVE_GPU_ISA.  The result is multiple object libraries, each compiled with a different DNNL_GPU_ISA_ prefix.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/gemm/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(DPCPP_HOST_COMPILER_KIND STREQUAL \"DEFAULT\")\n   list(REMOVE_ITEM SOURCES ${ONEDNN_GEMMSTONE_DIR}/generator/generator.cpp)\n\n    file (GLOB GENERATOR_SOURCES\n        ${ONEDNN_GEMMSTONE_DIR}/generator/generator.cpp\n     )\n\n    if (DNNL_ENABLE_PRIMITIVE_GPU_ISA STREQUAL \"ALL\")\n        set(DNNL_GPU_ISA_LIST \"GEN9;GEN11;XELP;XEHP;XEHPG;XEHPC;XE2;XE3\")\n    else()\n        foreach(isa ${DNNL_ENABLE_PRIMITIVE_GPU_ISA})\n            string(TOUPPER ${isa} ISA)\n            set(DNNL_GPU_ISA_LIST \"${DNNL_GPU_ISA_LIST};${ISA}\")\n        endforeach()\n    endif()\n\n    foreach(isa ${DNNL_GPU_ISA_LIST})\n        set(GENERATOR_LIB generator${isa})\n        add_library(${GENERATOR_LIB} OBJECT ${GENERATOR_SOURCES})\n        target_compile_definitions(${GENERATOR_LIB} PRIVATE DNNL_GPU_ISA_${isa})\n        set_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n                        $<TARGET_OBJECTS:${GENERATOR_LIB}>)\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Named Problem with Iteration\nDESCRIPTION: Runs a specific group normalization problem with single-precision data types, iterating over memory formats, propagation kinds, and flag combinations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_gnorm.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --gnorm --dt=f32 --tag=abc,acb --dir=FWD_D,BWD_DW \\\n               --flags=GCH,CH g5ic5iw10_n\"gnorm_test_shape\"\n```\n\n----------------------------------------\n\nTITLE: Example Output for Custom Performance Report\nDESCRIPTION: This snippet presents the output from the execution of `benchdnn` with a custom performance template. The template is set to display the canonical problem description, minimum time, and minimum GFLOPS. The snippet demonstrates the generated output line conforming to the defined custom template.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_6\n\nLANGUAGE: Text\nCODE:\n```\nOutput template: %prb%,%-time%,%-Gflops%\nmb112oc1000ic2048n\"resnet:ip1\",0.521973,878.881\n```\n\n----------------------------------------\n\nTITLE: Running Pooling with Input File\nDESCRIPTION: This command runs the pooling benchmark using a set of pooling configurations defined in an input file. It uses the default settings specified in the file for the benchmark.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_pool.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --pool --batch=inputs/pool/shapes_2d\n```\n\n----------------------------------------\n\nTITLE: Convolution Fused with Depthwise Convolution and ReLU (sh)\nDESCRIPTION: This example runs a 1x1 convolution fused with depthwise convolution, followed by ReLU.  Destination scales are set to 0.5 for the 1x1 convolution and 1.5 for the depthwise post-op. The final destination datatype is `s8` in this example, and the weights datatype is inferred based on the convolution configuration.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n  ./benchdnn --conv --cfg=u8s8u8 --attr-scales=dst:per_oc \\\n             --attr-post-ops=relu+dw_k3s1p1:s8+relu \\\n             ic16oc16ih4oh4kh1ph0\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for Source Files in CMake\nDESCRIPTION: This snippet uses set_source_files_properties to set specific compiler flags (-O3 and -funroll-loops) for all source files found in the SOURCES variable.  This is used to optimize the compilation of the source files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/ppc64/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset_source_files_properties(${SOURCES}\n    PROPERTIES COMPILE_FLAGS \"-O3 -funroll-loops\")\n```\n\n----------------------------------------\n\nTITLE: Run Specific Reduction Problem\nDESCRIPTION: This command runs a specific reduction benchmark with predefined parameters.  It sets the source and destination data types to `f32`, the source memory format to `acb`, and the reduction operation to `sum`. The problem size is defined as 1x2x3 reduced to 1x1x3.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reduction.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --reduction --sdt=f32 --ddt=f32 --stag=acb --alg=sum 1x2x3:1x1x3\n```\n\n----------------------------------------\n\nTITLE: Reduction Driver Usage\nDESCRIPTION: This command shows the basic syntax for running the reduction benchmark using the benchdnn tool.  It requires specifying the `--reduction` flag along with optional benchmark knobs, reduction knobs, and a reduction description.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reduction.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --reduction [benchdnn-knobs] [reduction-knobs] [reduction-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Convolution with Binary Post-Op (sh)\nDESCRIPTION: This example runs a convolution problem with binary post-operations.  The first binary operation adds a single int value to the destination memory (common), and the second adds a tensor of the same size with an `nhwc`-like physical memory layout (axb) with float values.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n  ./benchdnn --conv --attr-post-ops=add:s32:common,add:f32:per_tensor:axb \\\n             ic16oc16ih4oh4kh1ph0\n```\n\n----------------------------------------\n\nTITLE: Running Inner Product with Advanced Attributes\nDESCRIPTION: This command demonstrates running the Inner Product driver with specific configurations for backward by data propagation, output scaling, post-operation accumulation, and post-operation tanh activation. It showcases the use of attribute knobs to fine-tune the benchmark execution.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_ip.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --ip --dir=BWD_D \\\n               --attr-scales=dst:common:2.25 \\\n               --attr-post-ops=sum:0.5+tanh \\\n               mb112ic2048_ih1iw1_oc1000_n\"resnet:ip1\"\n```\n\n----------------------------------------\n\nTITLE: Reordering Data with miopenTransform - C++\nDESCRIPTION: This code snippet illustrates the usage of `miopenTransform` function, which is the MIOpen equivalent of oneDNN's reorder function. It transforms the data layout, facilitating data format conversions required by different primitives.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/amd/README.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nmiopenTransform\n```\n\n----------------------------------------\n\nTITLE: Example Output for CSV Performance Report\nDESCRIPTION: This snippet showcases an example output when running `benchdnn` to generate a CSV-style performance report. It includes the template used and the resulting CSV row containing information about the engine, problem name, direction, configuration, attributes, detailed problem description, and performance metrics such as Giga-operations, frequency, time, and Giga-flops.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_4\n\nLANGUAGE: Text\nCODE:\n```\nOutput template: perf,%engine%,%name%,%dir%,%cfg%,%attr%,%DESC%,%Gops%,%Gfreq%,%-time%,%-Gflops%,%0time%,%0Gflops%\nperf,cpu,\"resnet:ip1\",FWD_B,f32,,112,1000,2048,1,1,0.458752,0,0.520264,881.768,0.564043,813.328\n```\n\n----------------------------------------\n\nTITLE: Disabling Optimizations for Specific Files\nDESCRIPTION: This snippet disables compiler optimizations for auto-generated GeMM kernel files to improve build times. It uses `file(GLOB)` to find these files and `set_source_files_properties` to set the `COMPILE_FLAGS` property to `/Od` (MSVC) or `-O0`/`-O1` (other compilers depending on build type).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/x64/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# remove optimizations of files that don't need them for faster build times.\nfile(GLOB FILES_WITHOUT_OPT\n    ${CMAKE_CURRENT_SOURCE_DIR}/gemm/*/*_kern_autogen.cpp)\nif(MSVC)\n    set_source_files_properties(${FILES_WITHOUT_OPT}\n        PROPERTIES COMPILE_FLAGS \"/Od\")\nelse()\n    if(UPPERCASE_CMAKE_BUILD_TYPE STREQUAL \"DEBUG\")\n        # Some compilers enable optimizations by default.\n        set(OPT_LEVEL \"-O0\")\n    else()\n        set(OPT_LEVEL \"-O1\")\n    endif()\n    set_source_files_properties(${FILES_WITHOUT_OPT}\n        PROPERTIES COMPILE_FLAGS \"${OPT_LEVEL}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Deterministic Attribute in C API\nDESCRIPTION: This snippet demonstrates how to set the deterministic attribute for a primitive in the oneDNN C API.  It uses the `dnnl_primitive_attr_set_deterministic` function to enable or disable deterministic execution. This function is crucial for ensuring consistent results across multiple runs, especially for debugging and validation purposes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes_deterministic.md#_snippet_0\n\nLANGUAGE: C\nCODE:\n```\n@ref dnnl_primitive_attr_set_deterministic\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Custom Performance Template Example\nDESCRIPTION: This example demonstrates how to use a custom performance template in benchdnn to report descriptor, minimum time, and corresponding gigaFLOPS. It shows the command and the resulting template.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --ip --mode=p --perf-template=%prb%,%-time%,%-Gflops% \\\n               --batch=inputs/ip/test_ip_all\n```\n\n----------------------------------------\n\nTITLE: File Globbing for Header Files in CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command in CMake to collect all header files (*.h, *.hpp) from the specified directories.  `HEADERS_ROOT` gets headers from the include directory, and `HEADERS_SUBDIR` gets headers from the build and source trees within the oneapi/dnnl subdirectory. The globbing is used to build the library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB HEADERS_ROOT\n    ${CMAKE_CURRENT_SOURCE_DIR}/../include/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/../include/*.hpp\n    )\nfile(GLOB HEADERS_SUBDIR\n    ${PROJECT_BINARY_DIR}/include/oneapi/dnnl/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/../include/oneapi/dnnl/*.hpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Resampling Driver Usage in Shell\nDESCRIPTION: This command executes the resampling benchmark driver. It requires specifying benchmark knobs, resampling knobs, and a problem descriptor. The resampling knobs allow you to control various aspects of the benchmark, such as data types, memory layouts, and the resampling algorithm.  The problem descriptor defines the input and output shapes of the resampling operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_resampling.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --resampling [benchdnn-knobs] [resampling-knobs] [resampling-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Defining gtest SEH for SYCL on Windows with CMake\nDESCRIPTION: This code snippet defines the `GTEST_HAS_SEH` macro to 0 when building on Windows with the Intel oneAPI DPC++ Compiler and the DNNL_WITH_SYCL option is enabled. This is a workaround for an issue where the compiler doesn't support SEH (Structured Exception Handling) with the -fsycl option.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32 AND DNNL_WITH_SYCL)\n    add_definitions(-DGTEST_HAS_SEH=0)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Concat Problem\nDESCRIPTION: This example runs a specific concat problem with three inputs of `s32` data type and `nhwc` memory layout, an output of `u8` data type and `nhwc` layout along the `h` axis (axis=2), resulting in an 8x8x10x5 tensor. It demonstrates setting data types, memory layouts, and the concatenation axis.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_concat.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --concat --sdt=s32 --ddt=u8 --stag=nhwc:nhwc:nhwc \\\n               --dtag=nhwc --axis=2 8x8x3x5:8x8x7x5:8x8x0x5\n```\n\n----------------------------------------\n\nTITLE: BRGeMM with LDB Example\nDESCRIPTION: This snippet demonstrates running the BRGeMM driver with a specific leading dimension for the weights (LDB). The `--ld` option specifies the leading dimensions for the source, weights, and destination.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_brgemm.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --brgemm --ld=:16: 16x16:16x2\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Eltwise Problem Descriptor\nDESCRIPTION: This snippet shows the canonical form of the problem descriptor for the element-wise driver. It represents a 3D spatial problem with logical dimensions N, C, D, H, W. The user should replace `N` with integer numbers that represent the sizes of the dimensions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_eltwise.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    NxNxNxNxN\n```\n\n----------------------------------------\n\nTITLE: Set Kernel Descriptor from Environment\nDESCRIPTION: This snippet demonstrates how to set a specific kernel descriptor via an environment variable and then run benchdnn with that descriptor. This allows for testing with specific kernel configurations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/v2/conv/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# 3. Set kernel descriptor from environment\nexport desc=\"--prop fwd --src axb:f32 --wei axcb:f32 --dst axb:f32 --hw xehpc --fma mad --simd 16 --regs 128 --iter ic16mb16oc32 --tg ow4oc4 --loop-desc kw,kh,kd,ic --load a:2d,b:2d --store c:2d\"\n./build/tests/benchdnn/benchdnn -v5 --engine=gpu --mode=F --conv --impl=v2 --dir=FWD_I --dt=f32 mb128ic256ih56oc64oh56kh1ph0\n...\nperf,gpu,jit:ir_v2,,--mode=F --conv --engine=gpu --dir=FWD_I mb128ic256ih56oc64oh56kh1ph0,13.1533,158.426,1.124,11702.3,1.13858,11552.4\n```\n\n----------------------------------------\n\nTITLE: Accessing OpenCL Buffer from oneDNN Memory\nDESCRIPTION: Retrieves the underlying OpenCL memory object from a oneDNN memory object. The user is responsible for managing the lifetime of the returned OpenCL object.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::get_mem_object(const memory &)\n```\n\n----------------------------------------\n\nTITLE: Disabling Tests if DNNL_BUILD_TESTS is OFF (CMake)\nDESCRIPTION: This snippet checks if the `DNNL_BUILD_TESTS` option is enabled. If not, it immediately returns, skipping the test configuration. This prevents test building when the user has explicitly disabled tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT DNNL_BUILD_TESTS)\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for Intel Compiler\nDESCRIPTION: This snippet sets compiler flags for the Intel compiler to enable precise division and square root calculations, improving numerical stability. It uses `file(GLOB)` to find relevant source files (normalization and resampling) and then uses `set_source_files_properties` to add the `/Qprec-sqrt` and `/Qprec-div` flags (or `-prec-sqrt` and `-prec-div` on non-Windows platforms).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/x64/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\")\n    # to make computations more stable and to align the jitted code\n    # with the reference one use precise division and square root\n    # by default\n    file(GLOB FILES_REQUIRED_PREC_SQRT\n        ${CMAKE_CURRENT_SOURCE_DIR}/*normalization*.cpp\n        )\n    file(GLOB FILES_REQUIRED_PREC_DIV\n        ${CMAKE_CURRENT_SOURCE_DIR}/*resampling*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/*normalization*.cpp\n        )\n    if(WIN32)\n        set_source_files_properties(${FILES_REQUIRED_PREC_SQRT}\n            PROPERTIES COMPILE_FLAGS \"/Qprec-sqrt\")\n        set_source_files_properties(${FILES_REQUIRED_PREC_DIV}\n            PROPERTIES COMPILE_FLAGS \"/Qprec-div\")\n    else()\n        set_source_files_properties(${FILES_REQUIRED_PREC_SQRT}\n            PROPERTIES COMPILE_FLAGS \"-prec-sqrt\")\n        set_source_files_properties(${FILES_REQUIRED_PREC_DIV}\n            PROPERTIES COMPILE_FLAGS \"-prec-div\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Shape Rewrite\nDESCRIPTION: This example demonstrates how to use the `--in-shapes` option to modify the input shapes of a graph when running benchdnn with the graph driver. The example uses the mode C.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# rewrite input shape only\n./benchdnn --mode=C --graph --in-shapes=0:2x64x112x112+1:32x64x2x2 --case=pattern/f32/conv_post_ops_fusion.json\n# rewrite stride only\n./benchdnn --mode=C --graph --in-shapes=0:dcba+1:dcba --case=pattern/f32/conv_post_ops_fusion.json\n# rewrite shape and stride\n./benchdnn --mode=C --graph --in-shapes=0:2x64x112x112*dcba+1:32x64x2x2*dcba --case=pattern/f32/conv_post_ops_fusion.json\n# rewrite rank\n./benchdnn --mode=C --graph --in-shapes=0:2x64x112x112x112+1:32x64x2x2x2 --op-attrs=0:strides:1x1x1*pads_begin:0x0x0*pads_end:0x0x0*dilations:1x1x1 --case=pattern/f32/conv_post_ops_fusion.json\n# rewrite rank and stride\n./benchdnn --mode=C --graph --in-shapes=0:2x64x112x112x112*edcba+1:32x64x2x2x2*edcba --op-attrs=0:strides:1x1x1*pads_begin:0x0x0*pads_end:0x0x0*dilations:1x1x1 --case=pattern/f32/conv_post_ops_fusion.json\n# rewrite rank to 0 rank with shape []\n./benchdnn --mode=C --graph --in-shapes=0:- --case=op/f32/add.json\n# rewrite to 1D tensor with shape [0]\n./benchdnn --mode=C --graph --in-shapes=0:0+1:0 --case=op/f32/add.json\n```\n\n----------------------------------------\n\nTITLE: Reorder Example with Input File\nDESCRIPTION: This example demonstrates how to run the reorder benchmark using an input file containing a set of reorder problems. The default settings are applied to the problems defined in the specified file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reorder.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --reorder --batch=inputs/reorder/test_reorder_all\n```\n\n----------------------------------------\n\nTITLE: Enabling Generic GPU Support with CMake\nDESCRIPTION: This snippet demonstrates how to enable the generic GPU support feature in oneDNN during the build process using the CMake option `DNNL_GPU_VENDOR=GENERIC`. Enabling this option allows the target GPUs to be used via oneDNN engine abstraction.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/sycl/README.md#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nDNNL_GPU_VENDOR=GENERIC\n```\n\n----------------------------------------\n\nTITLE: Setting Global Properties for Library Dependencies\nDESCRIPTION: This snippet sets a global property (`DNNL_LIB_DEPS`) to append the object library as a dependency. This is necessary for linking the object library into other libraries or executables. The use of generator expressions ensures that the correct object files are linked based on the build configuration.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Resampling Benchmark with Input File\nDESCRIPTION: This command runs the resampling benchmark using problem descriptions from an input file.  It uses the default settings for all other parameters. The input file should contain a list of problem descriptors, each specifying the input and output shapes for the resampling operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_resampling.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --resampling --batch=inputs/resampling/shapes_2d\n```\n\n----------------------------------------\n\nTITLE: Setting ONEAPI_DEVICE_SELECTOR for NVIDIA GPU Selection\nDESCRIPTION: This snippet shows how to use the `ONEAPI_DEVICE_SELECTOR` environment variable to specify the NVIDIA GPU for oneDNN execution. This is useful when multiple GPUs are present in the system, allowing the user to target a specific GPU.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/sycl/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nONEAPI_DEVICE_SELECTOR=cuda:*\n```\n\n----------------------------------------\n\nTITLE: Running Layer Normalization Benchmarks\nDESCRIPTION: Executes the layer normalization benchmark using the benchdnn tool. It accepts various command-line arguments to configure the benchmark, including data types, memory formats, and flags. Refer to the linked documents for more details on each knob.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lnorm.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --lnorm [benchdnn-knobs] [lnorm-knobs] [lnorm-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Eltwise Specific Problem Configuration\nDESCRIPTION: This snippet shows how to run a specific element-wise problem with the f32 data type and in-place memory mode, iterating over memory layouts and forward and backward prop kinds. It uses the `--dir`, `--dt`, `--tag`, and `--inplace` options to specify the configuration, followed by the problem descriptor `50x192x55x55`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_eltwise.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --eltwise --dir=FWD_D,BWD_D --dt=f32 --tag=nchw,nChw16c \\\n               --inplace=true 50x192x55x55\n```\n\n----------------------------------------\n\nTITLE: Sparse Encoding Usage in Benchdnn\nDESCRIPTION: This snippet demonstrates how to specify sparse encodings for source, weights, and destination tensors using the `--encoding` option in Benchdnn. The colon-separated encodings correspond to the respective tensors.  Sparsity values can also be optionally specified along with the encoding.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_encoding.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n    --encoding=ENCODING[+SPARSITY]:ENCODING[+SPARSITY]:ENCODING[+SPARSITY]\n```\n\n----------------------------------------\n\nTITLE: Reporting L1 Data Cache Hotspots with VTune\nDESCRIPTION: This command reports L1 data cache hotspots using VTune Profiler. It filters the report by task, formats it as CSV, and displays the 'L1 Bound' column, providing insights into L1 cache-related performance bottlenecks in oneDNN primitives.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -report hotspots -q -r dnnl-vtune-ue -format csv -csv-delimiter ';' -group-by task -column 'L1 Bound' | head -n 10 | column -t -s';'\n```\n\n----------------------------------------\n\nTITLE: Running Shuffle Benchmarks\nDESCRIPTION: This command executes the benchdnn shuffle benchmark with specified options. It shows the general syntax for invoking the shuffle driver with various knobs and a problem descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_shuffle.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --shuffle [benchdnn-knobs] [shuffle-knobs] [shuffle-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Appending Test Sources in CMake\nDESCRIPTION: This snippet appends the content of the `MAIN_SRC_GTEST` variable to the `TEST_SOURCES` list.  This ensures that necessary Google Test related source files are included in the compilation process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/api/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND TEST_SOURCES ${MAIN_SRC_GTEST})\n```\n\n----------------------------------------\n\nTITLE: Performance Data Collection using synthdnn.py\nDESCRIPTION: Collects performance data for a given oneDNN primitive. It requires specifying the engine type, the kind of data to collect, a batch file, and the benchdnn file. The collected data is based on the specified engine and data kind.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/synthdnn/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 synthdnn.py collect --engine=<engine> --collect <data_kind> -b <batch_file> <benchdnn_file>\n```\n\n----------------------------------------\n\nTITLE: BRGeMM Validation Set Example\nDESCRIPTION: This snippet shows an example of running the BRGeMM driver with a default validation set using the `shapes_2d` input file. The `--batch` option specifies the input file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_brgemm.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --brgemm --batch=inputs/brgemm/shapes_2d\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_cpu_x64` from the source files collected in the `SOURCES` variable.  It also sets a global property to append the object library to `DNNL_LIB_DEPS`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/x64/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_cpu_x64)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<$<TARGET_OBJECTS:${OBJ_LIB}>>)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Relative Path Example\nDESCRIPTION: This example shows how to run a benchdnn graph driver with relative path if the JSON file is under `tests/benchdnn/inputs/graph`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=P --graph --case=pattern/f32/conv_post_ops_fusion.json\n```\n\n----------------------------------------\n\nTITLE: Building Documentation (sh)\nDESCRIPTION: These commands set up the environment and build the documentation. It assumes that `conda` is installed and configured to manage Python environments.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nconda env create -f ../doc/environment.yml\nconda activate onednn-doc\n```\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --target doc\n```\n\n----------------------------------------\n\nTITLE: Benchmarking on Several Cores Within a NUMA Domain with oneDNN (CPU)\nDESCRIPTION: This snippet details the configuration for benchmarking oneDNN using several cores within a single NUMA domain. It sets OpenMP thread affinity to 'close', aiming to place threads close to each other within the domain. The `numactl` command binds the process to NUMA domain 0 for both CPU and memory, and the `OMP_NUM_THREADS` variable defines the desired number of cores to utilize.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/perf_settings.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n$ export OMP_PROC_BIND=close\n$ export OMP_PLACES=threads\n$ export OMP_NUM_THREADS=# desired number of cores to use\n$ numactl --membind 0 --cpunodebind 0 ./benchdnn ...\n```\n\n----------------------------------------\n\nTITLE: Build and Test GPU Convolution (v2) in oneDNN\nDESCRIPTION: This snippet demonstrates how to build oneDNN with OpenCL GPU runtime enabled for the experimental v2 convolution implementation. It also shows how to run benchdnn tests with the v2 convolution implementation on the GPU.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/v2/conv/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# 1. Build with OpenCL GPU runtime with experimental support to enable v2 convolution\ncmake . -Bbuild -DONEDNN_GPU_RUNTIME=OCL -DONEDNN_EXPERIMENTAL=ON -DONEDNN_BUILD_GRAPH=OFF\nmake -C build -j `nproc` benchdnn gpu_conv_planner\n\n# 2. Test\n./build/tests/benchdnn/benchdnn -v5 --engine=gpu --mode=F --conv --impl=v2 --dir=FWD_I --batch=shapes_resnet_50_v1_5\n...\nrun: --mode=F --conv --engine=gpu --dir=FWD_I ic64ih56oc64oh56kh3ph1n\"resnet_50_v1_5:res2a_branch2b*3\"\nperf,gpu,jit:ir_v2,\"resnet_50_v1_5:res2a_branch2b*3\",--mode=F --conv --engine=gpu --dir=FWD_I ic64ih56oc64oh56kh3ph1n\"resnet_50_v1_5:res2a_branch2b*3\",0.451478,155.925,0.10656,4236.84,0.107055,4217.25\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Implementation Filtering Example\nDESCRIPTION: This example demonstrates how the `--impl` option is used to filter implementations based on their names. It highlights the importance of precision when matching implementation names and shows how all implementations can be skipped if none match the specified criteria.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_impl_filter.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nbenchdnn --matmul -v6 --impl=gemmx:jit:f32 64x32:32x64\ncreate: --matmul --impl=gemmx:jit:f32 64x32:32x64\n[IMPL_FILTER] Implementation skipped:: brg_matmul:avx512_core\n[IMPL_FILTER] Implementation skipped:: gemm:jit:f32\n[IMPL_FILTER] Implementation skipped:: brg_matmul:avx2\n[IMPL_FILTER] Implementation skipped:: ref:any\nAll implementations were skipped!\nrun: --matmul --impl=gemmx:jit:f32 64x32:32x64\n0:SKIPPED (Skip-impl option hit) __REPRO: --matmul --impl=gemmx:jit:f32 64x32:32x64\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Attributes Override\nDESCRIPTION: This example demonstrates how to override operation attributes using the `--op-attrs` option when running benchdnn with the graph driver.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=P --graph --op-attrs=,0:strides:4x4 --case=op/f32/conv_2d.json\n```\n\n----------------------------------------\n\nTITLE: Binary Driver Invocation\nDESCRIPTION: Illustrates the basic command-line syntax for invoking the binary driver within the benchdnn framework. It shows how to pass general benchdnn knobs, binary-specific knobs, and a binary problem descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_binary.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --binary [benchdnn-knobs] [binary-knobs] [binary-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Creating Executables and Linking Libraries with CMake\nDESCRIPTION: This snippet iterates through the list of source files obtained in the previous step.  For each source file, it extracts the filename, modifies it to create a unique executable name, adds an executable target using `add_executable`, and links the necessary libraries (`dnnl_gtest`, `${DNNL_LIBRARY_NAME}`, and `${EXTRA_SHARED_LIBS}`) to the target using `target_link_libraries`. Finally, it registers the test suite using `register_graph_api_test_suite`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${API_SYCL_TEST_SOURCES})\n    get_filename_component(exe_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_\" exe_name ${exe_name}_sycl)\n    add_executable(${exe_name} ${TEST_FILE} ${COMMON_API_TEST_DEPS})\n    target_link_libraries(${exe_name}\n        dnnl_gtest\n        ${DNNL_LIBRARY_NAME}\n        ${EXTRA_SHARED_LIBS}\n    )\n    register_graph_api_test_suite(${exe_name} ${exe_name})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding Executable Target in CMake\nDESCRIPTION: This snippet creates an executable named `${BINARY_NAME}` (test_graph_unit) using the common unit tests and dependencies.  It links the common tests, unit test dependencies, and library dependencies to create the test executable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(${BINARY_NAME} ${COMMON_UNIT_TESTS} ${UNIT_TEST_DEPS} ${LIB_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Generating Detailed Statistics by Primitive Kind and Shapes in Python\nDESCRIPTION: This snippet extends the previous example by including `shapes` as an additional aggregation key. The script accumulates calls and timings for primitives with the same `prim_kind` and `shapes`, providing a more detailed breakdown.  The output is piped to `column` for formatted display.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n> python3  ./scripts/verbose_converter/verbose_converter.py -i input.log -g breakdown -k prim_kind shapes | column -t -s,\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Cold Cache Usage\nDESCRIPTION: This command-line option enables the cold cache feature in benchdnn. The `MODE` specifies which memory objects are to be replaced with new ones on each execution.  Possible values are `none`, `wei`, `all`, and `custom`.  Extensions, such as `tlb`, can be added using the `+` delimiter.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_cold_cache.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n--cold-cache=MODE[+EXTENSION]\n```\n\n----------------------------------------\n\nTITLE: Running U8S8U8 Forward Convolutions without Bias (sh)\nDESCRIPTION: This example demonstrates how to run u8s8u8 forward convolutions without bias, skip reference implementations, and set a common output scale.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv --dt=u8:s8:u8 --dir=FWD_D --skip-impl=ref \\\n               --attr-scales=dst:common:0.5 --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library in CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_gpu_intel_compute` using the `add_library` command. An object library is a collection of object files without linking, which can be used later to create static or shared libraries. The source files for this library are obtained from the `SOURCES` variable, which was previously populated by the globbing operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/compute/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_intel_compute)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Specific Logsoftmax Problem Execution\nDESCRIPTION: This command executes a specific logsoftmax problem with backward propagation, default memory layouts and data types, in-place memory mode, and an axis size of 64.  The algorithm is set to LOGSOFTMAX.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_softmax.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --softmax --dir=BWD_D --inplace=true \\\n               --alg=LOGSOFTMAX --axis=3 1x2x112x64\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for GPU Vendors (DNNL_GPU_VENDOR) in C++\nDESCRIPTION: This code snippet demonstrates how to conditionally include vendor-specific headers and dispatch function calls based on the `DNNL_GPU_VENDOR` macro. It includes headers for Intel and NVIDIA and calls the appropriate vendor-specific `foo()` function based on the defined vendor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/README.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"oneapi/dnnl/dnnl_config.h\"\n\n#if DNNL_GPU_VENDOR == DNNL_VENDOR_INTEL\n#include \"gpu/intel/foo.hpp\"\n#elif DNNL_GPU_VENDOR == DNNL_VENDOR_NVIDIA\n#include \"gpu/nvidia/foo.hpp\"\n#endif\n\nint foo() {\n#if DNNL_GPU_VENDOR == DNNL_VENDOR_INTEL\n    return gpu::intel::foo();\n#elif DNNL_GPU_VENDOR == DNNL_VENDOR_NVIDIA\n    return gpu::nvidia::foo();\n#endif\n}\n```\n\n----------------------------------------\n\nTITLE: 1D Forward Convolution\nDESCRIPTION: This snippet demonstrates a 1D forward convolution implementation. It shows the nested loops for batch size (MB), output width (OW), output channels (OC), input channels (IC), and kernel width (KW).  The snippet also highlights the use of `load_src`, `load_wei`, and `store_dst` functions, which handle the tensor access.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/conv/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfor mb in range(0, MB):                                          # M\n    for ow in range(0, OW):                                      # M\n        for oc in range(0, OC):                                  # N\n            dst_val = 0\n            for ic in range(0, IC):                              # K\n                for kw in range(0, KW):                          # K\n                    src_val = load_src(mb, ic, ow, kw)\n                    wei_val = load_wei(oc, ic, kw)\n                    dst_val += src_val * wei_val\n            store_dst(mb, oc, ow, dst_val)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Global Implementation Filter Example\nDESCRIPTION: This example showcases the effectiveness of the `--global-impl` option in overriding local filtering policies.  Even if `--skip-impl` is present, `--global-impl` forces the selection of the specified implementation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_impl_filter.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nbenchdnn --matmul -v6 --global-impl=ref --skip-impl=gemm,ref 64x64:64x32\ncreate: --matmul --impl=ref 64x64:64x32\n[IMPL_FILTER] Implementation skipped:: brg_matmul:avx512_core\n[IMPL_FILTER] Implementation skipped:: gemm:jit:f32\n[IMPL_FILTER] Implementation skipped:: brg_matmul:avx2\n...\noneDNN implementation: ref:any\nrun: --matmul --impl=ref 64x64:64x32\n...\n0:PASSED __REPRO: --matmul --impl=ref 64x64:64x32\n```\n\n----------------------------------------\n\nTITLE: Benchdnn CSV-Style Performance Report Example\nDESCRIPTION: This example illustrates how to generate a performance report in CSV format using benchdnn.  It demonstrates the command-line arguments to specify the CSV template and provides a sample output of the generated CSV-style report.  The CSV template includes details about the problem description, configuration, attributes, and performance metrics.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --ip --mode=p --perf-template=csv \\\n               --batch=inputs/ip/test_ip_all\n```\n\n----------------------------------------\n\nTITLE: Run LRN benchmarks from an input file\nDESCRIPTION: Runs LRN benchmarks using shapes defined in the specified input file.\nThe `--batch` option points to a file containing a list of LRN configurations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lrn.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --lrn --batch=inputs/lrn/shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This CMake command uses `GLOB_RECURSE` to find all `.c`, `.h`, `.cpp`, and `.hpp` files in the current source directory and its subdirectories. The found files are stored in the `SOURCES` variable for later use.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/s390x/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Python Module Usage\nDESCRIPTION: Shows how to use the verbose converter as a python module.\nArguments include input string, parser type, action, split option, verbose level, generator, and events.\nReturns status and data. If `split` is true, data is a dictionary. Otherwise, it is a list.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport verbose_converter\n\noutput = verbose_converter.convert(verbose_level, parser, input, action,\n         generator, split_output)\n```\n\n----------------------------------------\n\nTITLE: Softmax Benchmark from Input File\nDESCRIPTION: This command runs the softmax benchmark using a configuration set defined in an input file named `shapes_ci`. It leverages the default settings unless overridden by the file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_softmax.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --softmax --batch=shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Running Inner Product Driver\nDESCRIPTION: This command executes the Inner Product driver with specified benchmarking knobs and problem descriptors. It allows for customization of direction, data types, memory layouts, bias settings, minibatch size, and regular expression matching to select specific problems.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_ip.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --ip [benchdnn-knobs] [ip-knobs] [ip-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Running PReLU Benchmark with Specific Configuration\nDESCRIPTION: Runs a specific PReLU primitive problem with defined direction, data types, memory formats, and tensor sizes. The command uses `--dir`, `--stag`, `--sdt` flags to set the configuration. The tensor sizes are defined using the `NxNxNxNxN:MxMxMxMxM` format.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_prelu.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --prelu --dir=BWD_DW --stag=abx:abx --sdt=f32:f32 \\\n                       256x128x7x7:1x128x1x1\n```\n\n----------------------------------------\n\nTITLE: Running benchdnn tests with ctest\nDESCRIPTION: This command uses `ctest` to run tests related to benchdnn. The `-R` flag allows filtering of tests based on a regular expression. The example restricts the tests to those containing 'benchdnn' in their name.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/benchdnn_general_info.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nctest [-R \".*benchdnn.*\"]\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation using DNNL_X64_ONLY Macro (C++)\nDESCRIPTION: This code snippet shows how to use the `DNNL_X64_ONLY` macro to execute code only on x64 architectures. If the target architecture is x64, `x64_impl_foo()` is called and returned; otherwise, the execution continues to the next line, which calls `generic_impl_foo()`. Requires inclusion of `cpu/platform.hpp`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"cpu/platform.hpp\" // IMPORTANT: INCLUDE THIS FILE!\n\nint generic_foo() {\n    DNNL_X64_ONLY(return x64_impl_foo());\n    return generic_impl_foo();\n}\n```\n\n----------------------------------------\n\nTITLE: BRGeMM Reduced Precision Example\nDESCRIPTION: This snippet demonstrates running the BRGeMM driver with reduced precision (int8) and a batch size of 10. The `--dt` option specifies the data types for the source, weights, and destination, and the `--bs` option sets the batch size.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_brgemm.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --brgemm --dt=u8:s8:u8 --bs=10 10x30:30x20\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver GPU Execution\nDESCRIPTION: This example demonstrates how to run a graph benchmark on the GPU engine using the benchdnn tool and graph driver.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=P --engine=gpu --graph --batch=test_graph_ci\n```\n\n----------------------------------------\n\nTITLE: Custom IR Visitor Example\nDESCRIPTION: This code demonstrates how to create a custom IR visitor to traverse and analyze an IR object. The `loop_visitor_t` class inherits from `ir_visitor_t` and overrides the `_visit` method for `for_t` objects to count the number of loops in the IR. The `visit` method is then used to traverse the IR and collect the loop count.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/ir/README.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// Custom visitor to count the total number of loops in the given IR object.\nclass loop_visitor_t : public ir_visitor_t {\npublic:\n    void _visit(const for_t *obj) override {\n        refs++;\n        // To count nested loops.\n        ir_visitor_t::_visit(obj);\n    }\n    int refs = 0;\n};\n\n// root_stmt is an IR statement\nloop_visitor_t visitor;\nvisitor.visit(root_stmt);\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for oneDNN Library\nDESCRIPTION: This snippet defines the include directories for the oneDNN library using `target_include_directories`. It specifies both build and install interface include paths, ensuring that the library can be found during compilation and after installation. The include directories are set for both the build and install interfaces, allowing proper header resolution.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${LIB_PACKAGE_NAME} PUBLIC\n    $<BUILD_INTERFACE:${PROJECT_BINARY_DIR}/include>\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}/../include>\n    $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing LSTM Primitive Descriptor C++\nDESCRIPTION: This C++ code snippet shows how to initialize an LSTM primitive descriptor using dnnl::lstm_forward::primitive_desc::primitive_desc(). It includes parameters like engine, propagation kind, direction, and memory descriptors for various inputs and outputs such as source layer, source iteration (hidden and cell states), weights layer, weights iteration, bias, destination layer, and destination iteration (hidden and cell states). This descriptor is necessary for creating an LSTM primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nauto lstm_pd = lstm_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_h_desc,\n        src_iter_c_desc, weights_layer_desc, weights_iter_desc, bias_desc,\n        dst_layer_desc, dst_iter_h_desc, dst_iter_c_desc);\n```\n\n----------------------------------------\n\nTITLE: Adding Vendor-Specific Subdirectories with CMake\nDESCRIPTION: These snippets conditionally add subdirectories to the build process based on the value of the `DNNL_GPU_VENDOR` variable. If the vendor is Intel, NVIDIA, or AMD, the corresponding subdirectory is added. This allows for vendor-specific code and configurations to be included.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_GPU_VENDOR STREQUAL \"INTEL\")\n    include_directories_with_host_compiler(${CMAKE_CURRENT_SOURCE_DIR}/intel/jit/config/)\n\n    set(ONEDNN_NGEN_DIR \"${PROJECT_SOURCE_DIR}/third_party/ngen\" CACHE PATH \"Path to nGEN source code\")\n    add_definitions_with_host_compiler(-DNGEN_CONFIG)\n    include_directories_with_host_compiler(${ONEDNN_NGEN_DIR})\n\n    set(ONEDNN_GEMMSTONE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/intel/jit/gemm\" CACHE PATH \"Path to gemmstone source code\")\n    add_definitions_with_host_compiler(-DGEMMSTONE_CONFIG)\n    include_directories_with_host_compiler(${ONEDNN_GEMMSTONE_DIR}/include)\n\n    add_subdirectory(intel)\nendif()\n\nif(DNNL_GPU_VENDOR STREQUAL \"NVIDIA\")\n    add_subdirectory(nvidia)\nendif()\n\nif(DNNL_GPU_VENDOR STREQUAL \"AMD\")\n    add_subdirectory(amd)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Handling Threading Layer for GPU Configurations (CMake)\nDESCRIPTION: This snippet configures the threading layer for GPU-only configurations (SYCL). If SYCL is enabled and CPU runtime is set to NONE, it inserts the project's CMake directory into the module path, finds the TBB package, removes the entry, and handles the TBB target to speed up testing.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_WITH_SYCL AND DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    list(INSERT CMAKE_MODULE_PATH 0 ${PROJECT_SOURCE_DIR}/cmake)\n    find_package_tbb(REQUIRED)\n    list(REMOVE_AT CMAKE_MODULE_PATH 0)\n    handle_tbb_target()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for Target Architectures (X64/AARCH64)\nDESCRIPTION: This conditional block checks if the target architecture is X64 or AARCH64. If true, it uses `file(GLOB_RECURSE)` to find JIT utility source files. Then, it iterates through the found files and appends them to the main `SOURCES` list. These utilities are included when building for X64 or AARCH64 architectures.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif((DNNL_TARGET_ARCH STREQUAL \"X64\") OR (DNNL_TARGET_ARCH STREQUAL \"AARCH64\"))\n    file(GLOB_RECURSE SOURCES_JIT_UTILS\n        ${CMAKE_CURRENT_SOURCE_DIR}/jit_utils/*.[ch]\n        ${CMAKE_CURRENT_SOURCE_DIR}/jit_utils/*.[ch]pp\n    )\n    foreach(SOURCE_FILE ${SOURCES_JIT_UTILS})\n        list(APPEND SOURCES \"${SOURCE_FILE}\")\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing Mean and Variance with hipMemsetD32Async - C++\nDESCRIPTION: This code snippet shows how to initialize the mean and variance with zero using `hipMemsetD32Async`. This is to avoid NaN propagation, as MIOpen requires sensible values for the mean and variance during forward training of Batch Normalization.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/amd/README.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nhipMemsetD32Async\n```\n\n----------------------------------------\n\nTITLE: Finding Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command in CMake to find all header (.h, .hpp) and source (.c, .cpp) files in the current source directory.  The found files are stored in the `SOURCES` variable for later use.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Benchmarking on Single NUMA Domain with oneDNN (CPU)\nDESCRIPTION: This snippet illustrates how to configure the environment and use numactl to bind the process to a single NUMA domain (domain 0 in this example) for both CPU and memory locality. It aims to improve performance by ensuring that computations access memory within the same NUMA domain, reducing cross-node traffic. The number of OpenMP threads is set to the number of cores within the specified NUMA domain.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/perf_settings.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n$ export OMP_PROC_BIND=spread\n$ export OMP_PLACES=threads\n$ export OMP_NUM_THREADS=# number of cores in NUMA domain 0\n$ numactl --membind 0 --cpunodebind 0 ./benchdnn ...\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_gpu` using the files listed in the `SOURCES` variable.  It then appends the object library's target objects to the `DNNL_LIB_DEPS` global property, likely used to manage dependencies for other oneDNN libraries.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: benchdnn Failures Summary Usage\nDESCRIPTION: Demonstrates how to use the `--summary` option to enable or disable the display of failed test cases in benchdnn.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_summary.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n    --summary=[no-]failures\n```\n\n----------------------------------------\n\nTITLE: Running Named Problem with Specific Parameters\nDESCRIPTION: This command runs a specific batch normalization problem with single-precision floating-point data type and skips reference implementations. It iterates over different memory layout tags (`nChw8c`, `nChw16c`), propagation kinds (`FWD_D`, `BWD_D`, `BWD_DW`), and flag combinations (`CHR`, `GCH`, `CH`). The command targets a problem named \"googlenet_v3:conv_4_4_batchnorm\" with a minibatch size of 96 and other specified parameters.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_bnorm.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --bnorm --dt=f32 --skip-impl=ref --tag=nChw8c,nChw16c \\\n               --dir=FWD_D,BWD_D,BWD_DW --flags=CHR,GCH,CH \\\n               mb96_ic192_ih71iw71_n\"googlenet_v3:conv_4_4_batchnorm\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Using DNNL_GPU_<VENDOR>_ONLY Macro in C++\nDESCRIPTION: This example illustrates the usage of the `DNNL_GPU_<VENDOR>_ONLY` macro to conditionally execute code based on the GPU vendor. It includes headers for Intel and NVIDIA and calls the corresponding vendor-specific `foo()` function using the `DNNL_GPU_INTEL_ONLY` and `DNNL_GPU_NVIDIA_ONLY` macros.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/README.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"oneapi/dnnl/dnnl_config.h\"\n\n#if DNNL_GPU_VENDOR == DNNL_VENDOR_INTEL\n#include \"gpu/intel/foo.hpp\"\n#elif DNNL_GPU_VENDOR == DNNL_VENDOR_NVIDIA\n#include \"gpu/nvidia/foo.hpp\"\n#endif\n\nint foo() {\nDNNL_GPU_INTEL_ONLY(return gpu::intel::foo());\nDNNL_GPU_NVIDIA_ONLY(return gpu::nvidia::foo());\n}\n```\n\n----------------------------------------\n\nTITLE: Offset Calculation for nChw8c Layout\nDESCRIPTION: Illustrates the offset calculation for the nChw8c memory layout, explaining how elements are arranged in memory with channel blocking. The code provides a formula to determine the memory offset given the indices n, c, h, and w.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\noffset_nChw8c(n, c, h, w) = n * CHW\n                              + (c / 8) * HW*8\n                              + h * W*8\n                              + w * 8\n                              + (c % 8)\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_gpu_nvidia` from the source files found in the `SOURCES` variable. It then sets a global property `DNNL_LIB_DEPS` to append the object library's objects for linking. This requires the `SOURCES` variable to be previously defined and the `LIB_PACKAGE_NAME` to be set.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/nvidia/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_nvidia)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Globbing OCL Test Sources in CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all C++ source files in the current directory that match the specified patterns. These files are then stored in the `API_OCL_TEST_SOURCES` variable for later use in the build process. This simplifies the management of test files by automatically including new tests without manually updating the CMakeLists.txt file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/ocl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB API_OCL_TEST_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_compiled_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_engine.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_tensor.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Setting BLAS Vendor to ARMPL\nDESCRIPTION: This snippet demonstrates how to set the BLAS vendor to Arm Performance Libraries (ARMPL) for AArch64 builds using the `ONEDNN_BLAS_VENDOR` CMake option. This is typically used with GCC.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_BLAS_VENDOR=ARMPL ..\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files and Compiler Options CMake\nDESCRIPTION: This snippet uses CMake commands to define source files, append to source lists, define compiler flags, and include directories.  It also sets compiler flags specific to Intel compilers for better precision. Dependencies like librt, regex, and socket are found when the operating system is compatible.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE HEADERS\n    ${CMAKE_CURRENT_SOURCE_DIR}/../include/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/../include/*.hpp\n    )\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\nlist(APPEND SOURCES ${TEST_THREAD})\nif(ONEDNN_BUILD_GRAPH)\n    add_definitions_with_host_compiler(-DBUILD_GRAPH)\nelse()\n    file(GLOB_RECURSE GRAPH_SRC ${CMAKE_CURRENT_SOURCE_DIR}/graph/*.cpp)\n    list(REMOVE_ITEM SOURCES ${GRAPH_SRC})\nendif()\ninclude_directories_with_host_compiler(\n    ${CMAKE_CURRENT_SOURCE_DIR}\n    )\n\nif(BENCHDNN_USE_RDPMC)\n    add_definitions_with_host_compiler(-DBENCHDNN_USE_RDPMC)\nendif()\n\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\")\n    append_if(WIN32 CMAKE_CXX_FLAGS \"-Qprec-div -Qprec-sqrt\")\n    append_if(UNIX  CMAKE_CXX_FLAGS \"-prec-div -prec-sqrt -fp-model precise\")\nendif()\n\nif(UNIX AND NOT APPLE AND NOT QNXNTO)\n    find_library(LIBRT rt)\nelseif(QNXNTO)\n    find_library(LIBREGEX regex)\n    find_library(LIBSOCKET socket)\nendif()\nregister_exe(benchdnn \"${SOURCES}\" \"\" \"${LIBRT};${LIBREGEX};${LIBSOCKET}\")\n\nfile(COPY inputs DESTINATION .)\n```\n\n----------------------------------------\n\nTITLE: Setting application name and main source file with CMake\nDESCRIPTION: This code snippet sets the application name to \"gtest\" and defines the main source file for the gtest executable, appending the threading related source file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(APP_NAME \"gtest\")\nset(MAIN_SRC_GTEST ${CMAKE_CURRENT_SOURCE_DIR}/main.cpp)\nlist(APPEND MAIN_SRC_GTEST ${TEST_THREAD})\n```\n\n----------------------------------------\n\nTITLE: Globbing Header Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all header files (`.h` and `.hpp`) in the specified include directory. The resulting list of files is stored in the `HEADERS` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB HEADERS\n    ${PROJECT_SOURCE_DIR}/include/*.h\n    ${PROJECT_SOURCE_DIR}/include/*.hpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Test Source Files with CMake\nDESCRIPTION: This command uses CMake's `file(GLOB)` to find all C++ source files matching the pattern `test_*.cpp` in the current source directory. The found files are stored in the `TEST_SOURCES` variable, which is then used to compile the test executable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/regression/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_*.cpp)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking on Whole Machine with oneDNN (CPU)\nDESCRIPTION: This snippet demonstrates the recommended environment variables and numactl command for benchmarking oneDNN on a whole machine. It sets OpenMP thread affinity to 'spread' across available cores, specifies 'threads' as the placement policy, defines the total number of cores in the system, and uses `numactl` to interleave memory allocation across all NUMA domains for better memory access and reduced run-to-run variation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/perf_settings.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ export OMP_PROC_BIND=spread\n$ export OMP_PLACES=threads\n$ export OMP_NUM_THREADS=# number of cores in the system\n$ numactl --interleave=all ./benchdnn ...\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library with CMake\nDESCRIPTION: This snippet defines an object library named `${LIB_PACKAGE_NAME}_graph_backend_fake` using the `add_library` command. The library is built from the source files listed in the `SOURCES` variable. Object libraries are used to group compiled object files together without creating a final linked library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/fake/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_graph_backend_fake)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Building on Windows (Visual C++)\nDESCRIPTION: These commands generate a Microsoft Visual Studio solution file using CMake and then build the library using the generated solution in Release configuration.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_5\n\nLANGUAGE: bat\nCODE:\n```\nmkdir build\ncd build\ncmake -G \"Visual Studio 16 2019\" ..\n```\n\nLANGUAGE: bat\nCODE:\n```\ncmake --build . --config=Release\n```\n\n----------------------------------------\n\nTITLE: Problem Generation and Performance Data Collection using synthdnn.py\nDESCRIPTION: Combines problem generation and performance data collection in a single step. Requires specifying the primitive, sampling controls, engine, data kind, and benchdnn file.  This performs both the generation of synthetic problems and then directly collects performance metrics.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/synthdnn/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython3 synthdnn.py <primitive> [sampling controls] --engine=<engine> --collect <data_kind> <benchdnn_file>\n```\n\n----------------------------------------\n\nTITLE: Concat Driver Invocation\nDESCRIPTION: This is the basic command to invoke the benchdnn concat driver. It shows the general structure of the command with placeholders for benchdnn and concat specific knobs and the problem description.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_concat.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --concat [benchdnn-knobs] [concat-knobs] [concat-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Defining DNNL_STATUS_NODISCARD macro based on compiler support\nDESCRIPTION: This snippet conditionally defines the `DNNL_STATUS_NODISCARD` macro if the compiler supports alias attributes and the GPU vendor is Intel. This macro is likely used to add the `[[nodiscard]]` attribute (or equivalent) to the `dnnl_status_t` type, which helps prevent ignoring return values from functions that return a status.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(${COMPILER_ALLOWS_ALIAS_ATTRIBUTES})\n    if(DNNL_GPU_VENDOR STREQUAL \"INTEL\")\n        add_definitions_with_host_compiler(-DDNNL_STATUS_NODISCARD)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Object Library with DNNL Dependencies\nDESCRIPTION: This snippet links the created object library with other DNNL libraries. The `set_property` command with the `APPEND` and `DNNL_LIB_DEPS` properties ensures that the objects from the microkernel library are included when linking other DNNL libraries. The `<TARGET_OBJECTS:${OBJ_LIB}>` generator expression resolves to the object files in the `${OBJ_LIB}` target.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/microkernels/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Primitive Cache Example in oneDNN (C++)\nDESCRIPTION: This code demonstrates how to create, store, load, and use a cached convolution primitive in oneDNN. It retrieves the cache blob ID and the cache blob from the primitive descriptor and primitive, respectively. It stores and loads the cache blob from a disk using placeholder functions, and finally creates a convolution primitive from the loaded cache blob.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/persistent_cache.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nusing namespace dnnl;\n\n{\n    convolution_forward::primitive_desc conv_pd(desc, attr, engine);\n    convolution_forward conv(conv_pd);\n\n    std::vector<uint8_t> key = conv_pd.get_cache_blob_id();\n    std::vector<uint8_t> value = conv.get_cache_blob();\n    store_cache_blob_on_disk(key, value);\n}\n\n{\n    convolution_forward::primitive_desc conv_pd(desc, attr, engine);\n    std::vector<uint8_t> key = conv_pd.get_cache_blob_id();\n    std::vector<uint8_t> value = load_cache_blob_from_disk(key);\n    convolution_forward conv_from_cache_blob(conv_pd, value);\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional X64-specific tests inclusion with CMake\nDESCRIPTION: This snippet conditionally includes X64-specific tests based on the DNNL_TARGET_ARCH and DNNL_CPU_RUNTIME settings. It sets the NO_ENGINE_PARAM property for these tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_TARGET_ARCH STREQUAL \"X64\" AND NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    file(GLOB X64_PRIM_TEST_CASES_SRC\n        test_isa_mask.cpp\n        test_isa_hints.cpp\n        test_isa_iface.cpp\n        )\n    foreach(TEST_FILE ${X64_PRIM_TEST_CASES_SRC})\n        list(APPEND PRIM_TEST_CASES_SRC \"${TEST_FILE}\")\n        set_source_files_properties(${TEST_FILE} PROPERTIES NO_ENGINE_PARAM true)\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet uses the `add_subdirectory` command in CMake to include the 'fake' and 'dnnl' directories. This includes the CMakeLists.txt file present in those directories into the current build process, allowing for separate compilation and organization of different modules.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fake)\nadd_subdirectory(dnnl)\n```\n\n----------------------------------------\n\nTITLE: Enabling JIT Dump (CPU)\nDESCRIPTION: This command enables JIT code dumping by setting the ONEDNN_JIT_DUMP environment variable to 1 before running the CNN inference example.  This will cause oneDNN to save the generated JIT code to binary files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/inspecting_jit.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nONEDNN_JIT_DUMP=1 ./cnn-inference-f32-cpp\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: This CMake command uses the `file(GLOB)` command to find all `.hpp` and `.cpp` files in the current source directory. The discovered files are then stored in the `SOURCES` variable for later use in the library creation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing LSTM with Peephole Primitive Descriptor C++\nDESCRIPTION: This C++ code snippet demonstrates initializing an LSTM primitive descriptor with peephole connections using dnnl::lstm_forward::primitive_desc::primitive_desc(). In addition to standard LSTM parameters, it also takes a memory descriptor for peephole weights (weights_peephole_desc). If weights_peephole_desc is a zero memory descriptor, the primitive behaves like a standard LSTM without peephole connections.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nauto lstm_pd = dnnl::lstm_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_h_desc, src_iter_c_desc,\n        weights_layer_desc, weights_iter_desc, weights_peephole_desc,\n        bias_desc, dst_layer_desc, dst_iter_h_desc, dst_iter_c_desc);\n```\n\n----------------------------------------\n\nTITLE: Adding OCL Subdirectory Conditionally in CMake\nDESCRIPTION: This CMake code checks if the `DNNL_GPU_RUNTIME` variable is exactly equal to \"OCL\". If it is, it adds the `ocl` subdirectory to the build using `add_subdirectory`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_GPU_RUNTIME STREQUAL \"OCL\")\n    add_subdirectory(ocl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building on Windows (Intel C++ with SYCL)\nDESCRIPTION: These commands configure CMake to build oneDNN with SYCL support, using the Intel oneAPI DPC++/C++ Compiler with Ninja generator. The environment must first be set up using the `setvars.bat` script from the Intel oneAPI installation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_6\n\nLANGUAGE: bat\nCODE:\n```\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\"\n```\n\nLANGUAGE: bat\nCODE:\n```\nmkdir build\ncd build\n\n:: Set C and C++ compilers\nset CC=icx\nset CXX=icx\ncmake .. -G Ninja -DDNNL_CPU_RUNTIME=SYCL ^\n                  -DDNNL_GPU_RUNTIME=SYCL ^\n                  <extra build options>\n```\n\nLANGUAGE: bat\nCODE:\n```\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Listing Available Columns for VTune Report\nDESCRIPTION: This command lists all available columns for the VTune Profiler report. It uses the '-column=?' option to display detailed information about available performance metrics that can be included in the report.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -report hotspots -q -r dnnl-vtune-ue -format csv -csv-delimiter ';' -group-by task -column=?\n```\n\n----------------------------------------\n\nTITLE: Setting Test Executable Name in CMake\nDESCRIPTION: This snippet sets the name of the test executable to 'test_api' using the `set` command in CMake. This name will be used as the target name when building the test application.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/api/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_EXE test_api)\n```\n\n----------------------------------------\n\nTITLE: Offset calculation for CHWN data format\nDESCRIPTION: This code snippet defines the offset function for the CHWN data format, which calculates the memory address displacement for a given logical index (n, c, h, w) in a 4D tensor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\noffset_chwn(n, c, h, w) = c * HWN + h * WN + w * N + n\n```\n\n----------------------------------------\n\nTITLE: Including fenv.h for CPU Devices in oneDNN\nDESCRIPTION: This code snippet includes the `fenv.h` header, which is part of the C and C++ standards.  This header defines the floating-point environment for CPU devices, allowing for control over rounding modes. This is relevant for controlling the numerical behavior of oneDNN on CPU.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/data_types.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <fenv.h>\n```\n\n----------------------------------------\n\nTITLE: Initializing GRU Primitive Descriptor C++\nDESCRIPTION: This C++ code snippet shows how to initialize a GRU primitive descriptor using dnnl::gru_forward::primitive_desc::primitive_desc(). The initialization takes engine, propagation kind, direction, and memory descriptors for source layer, source iteration, weights layer, weights iteration, bias, destination layer, and destination iteration as input. This allows the creation of a GRU primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto gru_pd = dnnl::gru_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_desc,\n        weights_layer_desc, weights_iter_desc, bias_desc,\n        dst_layer_desc, dst_iter_desc);\n```\n\n----------------------------------------\n\nTITLE: Set AMD SYCL Compiler Flags\nDESCRIPTION: This conditional statement checks if `DNNL_AMD_ENABLE_SYCL_KERNELS` is enabled. If true, it appends the `-fsycl-targets=amdgcn-amd-amdhsa -Xsycl-target-backend --offload-arch=${DNNL_AMD_SYCL_KERNELS_TARGET_ARCH}` flag to the `CMAKE_CXX_FLAGS`, specifying AMD GPU as a target architecture for SYCL compilation. `DNNL_AMD_SYCL_KERNELS_TARGET_ARCH` is a variable defining the specific AMD GPU architecture.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/sycl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_AMD_ENABLE_SYCL_KERNELS)\n    append(CMAKE_CXX_FLAGS \"-fsycl-targets=amdgcn-amd-amdhsa -Xsycl-target-backend --offload-arch=${DNNL_AMD_SYCL_KERNELS_TARGET_ARCH}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Executables for Engine-Independent Tests in CMake\nDESCRIPTION: This CMake code iterates through the list of engine-independent test sources (`API_TEST_ENGINE_INDEPENDENT_SOURCES`). For each file, it extracts the executable name, replaces \"test_\" with \"test_graph_\", adds an executable using `add_executable`, links the executable with necessary libraries (`dnnl_gtest`, `${DNNL_LIBRARY_NAME}`, `${EXTRA_SHARED_LIBS}`), and registers the test using `register_graph_api_test_suite`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(TEST_FILE ${API_TEST_ENGINE_INDEPENDENT_SOURCES})\n    get_filename_component(exe_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_\" exe_name ${exe_name})\n    add_executable(${exe_name} ${TEST_FILE} ${COMMON_API_TEST_DEPS})\n    target_link_libraries(${exe_name}\n        dnnl_gtest\n        ${DNNL_LIBRARY_NAME}\n        ${EXTRA_SHARED_LIBS}\n    )\n    register_graph_api_test_suite(${exe_name} ${exe_name})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Including External CMake Function\nDESCRIPTION: This line includes a custom CMake function defined in the `gen_gpu_kernel_list.cmake` file. This function is likely responsible for generating a list of GPU kernels based on the OpenCL sources.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(\"${PROJECT_SOURCE_DIR}/cmake/gen_gpu_kernel_list.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Initializing LSTM with Projection Primitive Descriptor C++\nDESCRIPTION: This C++ code snippet shows how to initialize an LSTM primitive descriptor with projection using dnnl::lstm_forward::primitive_desc::primitive_desc(). It includes a memory descriptor for the projection weights (weights_projection_desc) in addition to the other standard LSTM parameters. If weights_projection_desc is a zero memory descriptor, the primitive functions as a standard LSTM without projection.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nauto lstm_pd = dnnl::lstm_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_h_desc, src_iter_c_desc,\n        weights_layer_desc, weights_iter_desc, weights_peephole_desc,\n        weights_projection_desc, bias_desc, dst_layer_desc, dst_iter_h_desc,\n        dst_iter_c_desc);\n```\n\n----------------------------------------\n\nTITLE: Creating and Linking the oneDNN Library in CMake\nDESCRIPTION: This snippet creates the oneDNN library using `add_library`, specifies its type (SHARED or STATIC), and links it with necessary dependencies using `target_link_libraries`. It also sets the output name, version, and SOVERSION for the library. The library is configured to include dependencies and set output names for the build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nget_property(LIB_DEPS GLOBAL PROPERTY DNNL_LIB_DEPS)\nget_property(STATIC_LIB_DEPS GLOBAL PROPERTY DNNL_SUBDIR_EXTRA_STATIC_LIBS)\nget_property(SHARED_LIB_DEPS GLOBAL PROPERTY DNNL_SUBDIR_EXTRA_SHARED_LIBS)\nadd_library(${LIB_PACKAGE_NAME} ${DNNL_LIBRARY_TYPE}\n    ${VERSION_RESOURCE_FILE} ${HEADERS_ROOT} ${HEADERS_SUBDIR} ${LIB_DEPS})\n\ntarget_link_libraries(${LIB_PACKAGE_NAME} PRIVATE ${STATIC_LIB_DEPS} ${SHARED_LIB_DEPS})\n\nset_property(TARGET ${LIB_PACKAGE_NAME} PROPERTY OUTPUT_NAME ${DNNL_LIBRARY_NAME})\nset_property(TARGET ${LIB_PACKAGE_NAME} PROPERTY VERSION \"${DNNL_VERSION_MAJOR}.${DNNL_VERSION_MINOR}\")\nset_property(TARGET ${LIB_PACKAGE_NAME} PROPERTY SOVERSION \"${DNNL_VERSION_MAJOR}\")\n```\n\n----------------------------------------\n\nTITLE: Adding Executables for Engine-Dependent Tests in CMake\nDESCRIPTION: This CMake code iterates through the list of engine-dependent test sources (`API_TEST_ENGINE_DEPENDENT_SOURCES`). For each file, it extracts the executable name, replaces \"test_\" with \"test_graph_\", adds an executable using `add_executable`, links the executable with necessary libraries (`dnnl_gtest`, `${DNNL_LIBRARY_NAME}`, `${EXTRA_SHARED_LIBS}`), and registers the test using `register_graph_api_test_suite`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(TEST_FILE ${API_TEST_ENGINE_DEPENDENT_SOURCES})\n    get_filename_component(exe_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_\" exe_name ${exe_name})\n    add_executable(${exe_name} ${TEST_FILE} ${COMMON_API_TEST_DEPS})\n    target_link_libraries(${exe_name}\n        dnnl_gtest\n        ${DNNL_LIBRARY_NAME}\n        ${EXTRA_SHARED_LIBS}\n    )\n    register_graph_api_test_suite(${exe_name} ${exe_name})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library and Setting Dependencies\nDESCRIPTION: This CMake snippet creates an object library named `${LIB_PACKAGE_NAME}_common` from the collected source files in the `SOURCES` variable. It then appends the object library to the global property `DNNL_LIB_DEPS`, making it available as a dependency for other libraries or executables.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/common/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_common)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Initializing AUGRU Primitive in oneDNN (C++)\nDESCRIPTION: This code snippet illustrates how to initialize an Attention Unit GRU (AUGRU) primitive descriptor for the forward pass in oneDNN. It requires the engine, propagation kind, direction, memory descriptors for source layer, source iteration, attention, weights layer, weights iteration, bias, destination layer, and destination iteration. The created primitive descriptor is necessary to instantiate the AUGRU primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/rnn.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nauto augru_pd = dnnl::augru_forward::primitive_desc(\n        engine, aprop, direction, src_layer_desc, src_iter_desc, attention_desc,\n        weights_layer_desc, weights_iter_desc, bias_desc, dst_layer_desc,\n        dst_iter_desc);\n```\n\n----------------------------------------\n\nTITLE: Registering graph test suites (Engine Dependent)\nDESCRIPTION: This snippet iterates through each source file in `TEST_UTILS_ENGINE_DEPENDENT_SOURCES`. It constructs a test suite name and a filter string based on the file name and then registers the test suite using a `register_graph_test_suite` function.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${TEST_UTILS_ENGINE_DEPENDENT_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_unit_utils_\" test_suite_name ${file_name})\n    string(REPLACE \"test_\" \"test_utils_\" filter ${file_name})\n    register_graph_test_suite(${test_suite_name} \"${filter}*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Iterating and Registering Tests in CMake\nDESCRIPTION: This snippet iterates through a list of test files and registers each one using the register_gtest function. It also creates additional buffer targets for SYCL and OCL runtimes if enabled. It also checks if a test needs to be skipped based on the `skip_usm_pattern`\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${PRIM_TEST_CASES_SRC})\n    get_filename_component(exe ${TEST_FILE} NAME_WE)\n    if(NOT ${exe} MATCHES \"${skip_usm_pattern}\")\n        register_gtest(${exe} ${TEST_FILE})\n    endif()\n\n    # Create additional buffer targets for SYCL and OCL.\n    if(DNNL_WITH_SYCL OR DNNL_GPU_RUNTIME STREQUAL \"OCL\")\n        register_gtest(${exe}_buffer ${TEST_FILE})\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Conditional SYCL Support and Optimization Level\nDESCRIPTION: This code snippet conditionally includes the `sycl` subdirectory if `DNNL_WITH_SYCL` is enabled and `DNNL_CPU_RUNTIME` is set to `DPCPP` or `SYCL`. It then identifies JIT kernel source files and sets their compilation flags to `-O0`, effectively disabling optimization for these files when building with SYCL. This might be done for debugging or specific performance considerations related to SYCL.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_WITH_SYCL)\n    if(DNNL_CPU_RUNTIME MATCHES \"^(DPCPP|SYCL)$\")\n        add_subdirectory(sycl)\n    endif()\n\n    set(FILES_WITHNO_OPT)\n    foreach(src in ${SOURCES})\n        string(REGEX MATCH \".*jit.*kern.cpp\" match ${src})\n        if(match)\n            list(APPEND FILES_WITHNO_OPT ${src})\n        endif()\n    endforeach()\n\n    set_source_files_properties(${FILES_WITHNO_OPT}\n        PROPERTIES COMPILE_FLAGS \"-O0\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Expression Operator Overloading in IR\nDESCRIPTION: This code demonstrates operator overloading for IR expressions, allowing for a more natural and concise way to construct and manipulate expressions. It shows how to create variables, combine them using arithmetic operators, and simplify the resulting expression. The `dump()` method is used to print the expression tree.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/ir/README.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nexpr_t a = var_t::make(type_t::s32(), \"a\");\nexpr_t b = var_t::make(type_t::s32(), \"b\");\nexpr_t c = 2 * (a + b) - a;\nexpr_t c_simplified = simplify(c);\n// (gdb) call c.dump()\n// ((2 * (a + b)) - a)\n// (gdb) call c_simplified.dump()\n// (a + (b * 2))\n```\n\n----------------------------------------\n\nTITLE: Filtering Sources Based on RVV Intrinsics in CMake\nDESCRIPTION: This code block conditionally filters the `SOURCES` list based on whether the `DNNL_RISCV_USE_RVV_INTRINSICS` option is enabled. If it's not enabled, it excludes files containing \"rvv_\" in their name, effectively disabling RVV intrinsics-optimized implementations.  It then creates an empty source file if no sources are found.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/rv64/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_RISCV_USE_RVV_INTRINSICS)\n    # Do not compile RISC-V implementations optimized with RVV intrinsics\n    list(FILTER SOURCES EXCLUDE REGEX \"rvv_*\")\n\n    # Registering a library w/o any sources leads to a CMake error.\n    # If required, create an empty source file to prevent it.\n    list(LENGTH SOURCES NUM_OF_SOURCES)\n    if(NUM_OF_SOURCES EQUAL 0)\n        file(WRITE ${CMAKE_CURRENT_BINARY_DIR}/empty.cpp \"\")\n        set(SOURCES ${CMAKE_CURRENT_BINARY_DIR}/empty.cpp)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Microarchitecture Data with VTune\nDESCRIPTION: This command collects microarchitecture exploration data using VTune Profiler. It sets the sampling interval, data limit, and disables summary output to generate a detailed profiling report for a oneDNN benchmark.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -collect uarch-exploration -knob sampling-interval=1 -data-limit=2000  -q -no-summary -r dnnl-vtune-ue ./benchdnn --mode=P --conv --batch=inputs/conv/shapes_alexnet\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This snippet adds the 'dnnl' subdirectory to the build process. CMake will look for a CMakeLists.txt file inside the 'dnnl' directory and process it to include the 'dnnl' component in the build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(dnnl)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation using DNNL_X64 Macro (C++)\nDESCRIPTION: This code snippet demonstrates how to conditionally compile code based on the target architecture using the `DNNL_X64` macro. If `DNNL_X64` is defined (i.e., on x64 architecture), `x64_impl_foo()` is called; otherwise, `generic_impl_foo()` is called. Requires inclusion of `cpu/platform.hpp`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/README.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"cpu/platform.hpp\" // IMPORTANT: INCLUDE THIS FILE!\n\nint generic_foo() {\n#if DNNL_X64\n    return x64_impl_foo();\n#else\n    return generic_impl_foo();\n#endif\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library CMake\nDESCRIPTION: This code creates an object library named `graph_unit_test_dnnl_backend` from the source files collected in `DNNL_OP_EXECUTION_TEST_SOURCES` and `DNNL_COMMON_TEST_SOURCES`. The object library's objects are appended to the `GRAPH_UNIT_TEST_DEPS` global property, making them available for linking to test executables.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB graph_unit_test_dnnl_backend)\nadd_library(${OBJ_LIB} OBJECT ${DNNL_OP_EXECUTION_TEST_SOURCES} ${DNNL_COMMON_TEST_SOURCES})\nset_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_DEPS $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Registering Test Executable in CMake\nDESCRIPTION: This snippet uses a custom function `register_exe` to define the test executable. It takes the executable name (`TEST_EXE`), the source files (`TEST_SOURCES`), a descriptive tag (\"test\"), and a dependency library (`dnnl_gtest`) as arguments. This likely configures the build target and links the necessary libraries.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/api/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nregister_exe(${TEST_EXE} \"${TEST_SOURCES}\" \"test\" \"dnnl_gtest\")\n```\n\n----------------------------------------\n\nTITLE: Setting compile flags for specific test files on Windows with Intel compiler\nDESCRIPTION: This snippet sets specific compile flags (/O1) for certain test files when building on Windows with the Intel compiler. This is a workaround for an Intel compiler bug related to stack unwinding and optimization levels.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32 AND ${CMAKE_CXX_COMPILER_ID} STREQUAL \"Intel\")\n    set_source_files_properties(\n            test_convolution_eltwise_forward_x8s8f32s32.cpp\n            test_convolution_backward_data_f32.cpp\n            test_convolution_backward_weights_f32.cpp\n            PROPERTIES COMPILE_FLAGS \"/O1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning oneDNN Repository (sh)\nDESCRIPTION: This command clones the oneDNN repository from GitHub. It downloads the latest version of the source code to your local machine. This is the first step in building oneDNN from source.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/uxlfoundation/oneDNN.git\n```\n\n----------------------------------------\n\nTITLE: Profiling API Usage with User-Provided Queue (OpenCL)\nDESCRIPTION: This pseudo-code demonstrates how to use the oneDNN profiling API with a user-provided OpenCL queue. It involves creating an engine, a queue with profiling enabled, a oneDNN stream, executing a convolution primitive, resetting the profiler, querying profiling data, and asserting the size of the profiling data vector.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/experimental.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n    dnnl::engine engine(engine::kind::gpu, 0);\n    // Create a queue with enabled profiling mode.\n    cl_command_queue ocl_queue {};\n    cl_queue_properties props[] = {CL_QUEUE_PROPERTIES, CL_QUEUE_PROFILING_ENABLE, 0};\n    ocl_queue = clCreateCommandQueueWithProperties(ocl_interop::get_context(engine),\n        ocl_interop::get_device(engine), props, ...);\n    // Create dnnl::stream with the queue.\n    dnnl::stream stream = ocl_interop::make_stream(engine, ocl_queue);\n    // Create a convolution primitive ... //\n    // Reset profiler's state.\n    dnnl::reset_profiling(stream);\n    // Enqueue same primitive twice and wait for both executions to complete.\n    conv_prim.execute(stream, ...)\n    conv_prim.execute(stream, ...)\n    stream.wait();\n    // Query profiling data. The vector size will be equal to the number of\n    // executions happened on the stream since the last `dnnl::reset_profiling`\n    // call.\n    std::vector<uint64_t> nsecs = dnnl::get_profiling_data(stream, profiling_data_kind::time);\n    assert(nsecs.size() == 2);\n    // Reset profiler's state.\n    dnnl::reset_profiling(stream);\n```\n\n----------------------------------------\n\nTITLE: Using Attributes with C API\nDESCRIPTION: This code demonstrates how to create, set, and destroy primitive attributes using the oneDNN C API. It shows how to set different attributes and then destroy the attribute object after it's been used to create a primitive descriptor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/programming_model/attributes.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// ### C API ###\n\ndnnl_primitive_attr_t attr; // opaque attributes\ndnnl_primitive_attr_create(&attr);\ndnnl_primitive_attr_set_SOMETHING(attr, params); // setting attributes params\ndnnl_primitive_attr_set_SOMETHING_ELSE(attr, other_params);\ndnnl_eltwise_backward_primitive_desc_create(&op_pd, engine, ..., hint_fwd_pd, attr);\n\n// changing attr object here does not have any effect on op_pd\n\n// once attr is no more used we can immediately destroy it\ndnnl_primitive_attr_destroy(attr);\n```\n\n----------------------------------------\n\nTITLE: Creating DPC++ Buffer Target in CMake (Conditional)\nDESCRIPTION: This snippet conditionally creates a DPC++ buffer-specific test target named `${TEST_EXE}_buffer` when `DNNL_WITH_SYCL` is enabled. It registers a separate executable, defines a preprocessor macro `TEST_DNNL_DPCPP_BUFFER`, and appends compiler options to support DPC++ buffer testing.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/api/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_WITH_SYCL)\n    register_exe(${TEST_EXE}_buffer \"${TEST_SOURCES}\" \"test\" \"dnnl_gtest\")\n    target_compile_definitions(${TEST_EXE}_buffer PUBLIC -DTEST_DNNL_DPCPP_BUFFER)\n    append_host_compiler_options(CMAKE_CXX_FLAGS \"-DTEST_DNNL_DPCPP_BUFFER\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Use of Named Constants (C++)\nDESCRIPTION: This example demonstrates the use of properly named constants instead of hardcoded values to improve code readability and maintainability. Replacing magic numbers with meaningful names clarifies the purpose of the values.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CODING_STANDARDS.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nif (x < 4) y = 64;\n```\n\n----------------------------------------\n\nTITLE: Registering Operator Execution Test Suites CMake\nDESCRIPTION: This loop is similar to the previous one but iterates through the files listed in `DNNL_OP_EXECUTION_TEST_SOURCES`. It generates test suite names by replacing \"test_\" with \"test_graph_unit_dnnl_\" in the filename and then registers the test suite. This registers the op execution tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${DNNL_OP_EXECUTION_TEST_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_unit_dnnl_\" test_suite_name ${file_name})\n    register_graph_test_suite(${test_suite_name} \"${file_name}*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Enable Graph Dumping at Runtime (Bash)\nDESCRIPTION: This command sets the `ONEDNN_GRAPH_DUMP` environment variable to enable dumping of both the library graph and subgraphs contained within each partition. It executes the application after setting the environment variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/graph/graph_dump.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nONEDNN_GRAPH_DUMP=graph,subgraph ./application\n```\n\n----------------------------------------\n\nTITLE: Registering gtest Executable in CMake\nDESCRIPTION: This CMake function registers a gtest executable, setting up compiler definitions and target link libraries. It adds support for different runtimes (CPU, GPU, SYCL, OCL) and handles engine parameters. The function checks if the test is a buffer test and sets appropriate definitions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(register_gtest exe src)\n    if(DNNL_ENABLE_STACK_CHECKER AND NOT exe MATCHES \"test_gemm_\")\n        return()\n    endif()\n\n    add_executable(${exe} ${MAIN_SRC_GTEST} ${src})\n    add_definitions_with_host_compiler(-DNOMINMAX) # to allow std::max on Windows with parentheses\n    target_link_libraries(${exe} ${LIB_PACKAGE_NAME} dnnl_gtest ${EXTRA_SHARED_LIBS})\n\n    get_source_file_property(no_engine_param ${src} NO_ENGINE_PARAM)\n\n    set(DPCPP_HOST_COMPILER_FLAGS)\n    # Extract host compiler flags from CMAKE_CXX_FLAGS to DPCPP_HOST_COMPILER_FLAGS\n    string(REGEX MATCH \"-fsycl-host-compiler-options=\\\".*\\\"\"  DPCPP_HOST_COMPILER_FLAGS ${CMAKE_CXX_FLAGS})\n    # Erase host compiler flags from CMAKE_CXX_FLAGS\n    string(REGEX REPLACE \"-fsycl-host-compiler-options=\\\".*\\\"\"  \"\" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})\n\n    if(${exe} MATCHES \"_buffer\")\n        if (DNNL_WITH_SYCL)\n            target_compile_definitions(${exe} PUBLIC -DTEST_DNNL_DPCPP_BUFFER)\n        endif()\n        # Add a flag to build gtest with buffer support\n        append_host_compiler_options(DPCPP_HOST_COMPILER_FLAGS \"-DTEST_DNNL_DPCPP_BUFFER\")\n    else()\n        if(DNNL_GPU_RUNTIME STREQUAL \"OCL\")\n            target_compile_definitions(${exe} PUBLIC -DTEST_DNNL_OCL_USM)\n        endif()\n    endif()\n\n    if (NOT no_engine_param)\n        target_compile_definitions(${exe} PUBLIC -DDNNL_TEST_WITH_ENGINE_PARAM)\n        # Add a flag to enable gtest engine parameter\n        append_host_compiler_options(DPCPP_HOST_COMPILER_FLAGS \"-DDNNL_TEST_WITH_ENGINE_PARAM\")\n    endif()\n\n    # Set flags for each target separately\n    set_target_properties(${exe} PROPERTIES COMPILE_FLAGS \"${DPCPP_HOST_COMPILER_FLAGS}\")\n\n    if(NOT DNNL_GPU_RUNTIME STREQUAL \"NONE\" AND NOT no_engine_param AND NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n        if(NOT ${exe} MATCHES \"_buffer\" OR (${exe} MATCHES \"_buffer\" AND DNNL_CPU_SYCL))\n            add_dnnl_test(${exe}_cpu ${exe} COMMAND ${exe} --engine=cpu)\n            maybe_configure_windows_test(${exe}_cpu TEST)\n        endif()\n\n        add_dnnl_test(${exe}_gpu ${exe} COMMAND ${exe} --engine=gpu)\n        maybe_configure_windows_test(${exe}_gpu TEST)\n    elseif(DNNL_CPU_RUNTIME STREQUAL \"NONE\" AND NOT no_engine_param)\n        add_dnnl_test(${exe}_gpu ${exe} COMMAND ${exe} --engine=gpu)\n        maybe_configure_windows_test(${exe}_gpu TEST)\n    else()\n        add_dnnl_test(${exe} ${exe})\n        maybe_configure_windows_test(${exe} TEST)\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Building on Linux/macOS (GCC/Clang/Intel C++)\nDESCRIPTION: These commands create a build directory, navigate into it, and configure CMake to generate makefiles. Optionally, sets environment variables to use Clang or Intel oneAPI DPC++/C++ Compiler. The last command executes the build using make.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p build\ncd build\n\n# Uncomment the following lines to build with Clang\n# export CC=clang\n# export CXX=clang++\n\n# Uncomment the following lines to build with Intel oneAPI DPC++/C++ Compiler\n# export CC=icx\n# export CXX=icpx\ncmake .. <extra build options>\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Setting Source File Properties in CMake\nDESCRIPTION: This snippet sets the NO_ENGINE_PARAM property to true for specific source files. This property is used later to determine if the test should be run with an engine parameter.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nset_source_files_properties(\n            test_cross_engine_reorder.cpp\n            test_comparison_operators.cpp\n            PROPERTIES NO_ENGINE_PARAM true)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command in CMake to create a list of source files for the SYCL API tests. It searches for files ending with `.cpp` in the current source directory and stores the results in the `API_SYCL_TEST_SOURCES` variable. This list is then used in the following steps to create the test executables.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB API_SYCL_TEST_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_compiled_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_engine.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_tensor.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Run MatMul with runtime dimensions\nDESCRIPTION: This command runs a single-precision matrix multiplication where the dimensions of the matrices are provided at runtime. The `--stag`, `--wtag`, and `--dtag` options specify the memory format tags for the source, weights, and destination matrices, respectively. `--runtime_dims_masks` indicates which dimensions are runtime defined.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --matmul --stag=ab --wtag=ab --dtag=ab \\\n                        --runtime_dims_masks=3:3 10x30:30x20\n```\n\n----------------------------------------\n\nTITLE: Setting Global Library Dependencies with CMake\nDESCRIPTION: This snippet appends the object files from the `${OBJ_LIB}` target to the `DNNL_LIB_DEPS` global property using `set_property`. The `$<TARGET_OBJECTS:${OBJ_LIB}>` generator expression retrieves the object files associated with the object library. This ensures that the object files are included when linking the final oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/fake/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This CMake snippet first sets the name of the object library to `${LIB_PACKAGE_NAME}_gpu_amd`. Then, it creates an object library using `add_library` from the source files specified in the `SOURCES` variable. Finally, it appends the object library to the `DNNL_LIB_DEPS` global property, enabling it to be used as a dependency by other parts of the oneDNN project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/amd/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_amd)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Registering oneDNN Examples as Executables with CMake\nDESCRIPTION: This snippet iterates through the filtered source files, determines the example name, and registers the examples as executables using the `register_exe` macro. It also adds tests using the `add_dnnl_test` macro based on the CPU and GPU runtime configurations. It finds and links against the math library (libm).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/examples/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(src ${sources})\n    file(RELATIVE_PATH src_rel_path ${CMAKE_CURRENT_SOURCE_DIR} ${src})\n    string(REGEX REPLACE \"[/_\\\\.]\" \"-\" example_name ${src_rel_path})\n\n    # Put hw-specific part of the name in front.\n    # It is important for examples in subdirectories.\n    foreach(pat \"cpu-\" \"gpu-\" \"cross-engine-\")\n        string(REGEX REPLACE \"^(.*)${pat}\" \"${pat}\\\\1\"\n            example_name ${example_name})\n    endforeach()\n\n    if(${example_name} MATCHES \"(cross-engine|cpu|gpu)-\")\n        if(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\" OR ${example_name} MATCHES \"gpu-\")\n            # Example name contains cross-engine, cpu or gpu\n            find_libm(LIBM)\n            if(NOT ${example_name} MATCHES \".*opencl\" OR DNNL_GPU_RUNTIME STREQUAL \"OCL\")\n                register_exe(${example_name} ${src} \"test\" ${LIBM})\n            endif()\n        endif()\n    else()\n        set(cpu_rt_pattern \"(SEQ|OMP|TBB|SYCL|DPCPP)\")\n        set(gpu_rt_pattern \"(OCL|SYCL|DPCPP)\")\n        if(${example_name} MATCHES \"sycl.*\")\n            set(cpu_rt_pattern \"(SYCL|DPCPP)\")\n            set(gpu_rt_pattern \"(SYCL|DPCPP)\")\n        endif()\n        if(DNNL_CPU_RUNTIME MATCHES ${cpu_rt_pattern})\n            # Adding test for CPU\n            add_dnnl_test(\"cpu-${example_name}\" \"${example_name}\" cpu)\n            maybe_configure_windows_test(\"cpu-${example_name}\" TEST)\n        endif()\n        if(DNNL_GPU_RUNTIME MATCHES ${gpu_rt_pattern})\n            # Adding test for GPU\n            add_dnnl_test(\"gpu-${example_name}\" \"${example_name}\" gpu)\n            maybe_configure_windows_test(\"gpu-${example_name}\" TEST)\n        endif()\n        find_libm(LIBM)\n        register_exe(${example_name} ${src} \"\" ${LIBM})\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Defining and Setting Properties for the Object Library\nDESCRIPTION: This snippet defines an object library named ${LIB_PACKAGE_NAME}_gpu_intel using the source files found in the SOURCES variable. It then sets the DNNL_LIB_DEPS global property to append the object files from this target. This helps to manage dependencies between different parts of the oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_intel)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Generating oneDNN verbose log\nDESCRIPTION: This command generates a oneDNN verbose log file named `input.log` by setting the `ONEDNN_VERBOSE` environment variable to 1 and redirecting the standard output of the `cnn-inference-f32-cpp` executable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n> ONEDNN_VERBOSE=1 cnn-inference-f32-cpp > input.log\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion\nDESCRIPTION: This snippet conditionally adds the `sycl` and `ocl` subdirectories to the build process based on the `DNNL_WITH_SYCL` and `DNNL_GPU_VENDOR` variables. If `DNNL_WITH_SYCL` is enabled, the `sycl` subdirectory is added. If `DNNL_GPU_VENDOR` is set to \"INTEL\", the `ocl` subdirectory is included.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_WITH_SYCL)\n    add_subdirectory(sycl)\nendif()\n\nif(DNNL_GPU_VENDOR STREQUAL \"INTEL\")\n    add_subdirectory(ocl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Target Architecture\nDESCRIPTION: Detects the target architecture based on CMAKE_SYSTEM_PROCESSOR or defaults to X64. The detected architecture is stored in the DNNL_TARGET_ARCH variable and used for conditional compilation or architecture-specific optimizations. Includes logic for AARCH64, PPC64, S390X, RV64, and X64.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DNNL_TARGET_ARCH)\n    if(CMAKE_SYSTEM_PROCESSOR MATCHES \"^(aarch64.*|AARCH64.*|arm64.*|ARM64.*)\")\n        set(DNNL_TARGET_ARCH \"AARCH64\")\n    elseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^(ppc64.*|PPC64.*|powerpc64.*)\")\n        set(DNNL_TARGET_ARCH \"PPC64\")\n    elseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^(s390x.*|S390X.*)\")\n        set(DNNL_TARGET_ARCH \"S390X\")\n    elseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^(rv.*|RV.*|riscv.*|RISCV.*)\")\n        set(DNNL_TARGET_ARCH \"RV64\")\n    else()\n        set(DNNL_TARGET_ARCH \"X64\")\n    endif()\nendif()\nmessage(STATUS \"DNNL_TARGET_ARCH: ${DNNL_TARGET_ARCH}\")\n```\n\n----------------------------------------\n\nTITLE: Adding SYCL Subdirectory Conditionally in CMake\nDESCRIPTION: This CMake code checks if either `DNNL_CPU_RUNTIME` or `DNNL_GPU_RUNTIME` matches the `sycl_rt_pattern` (SYCL or DPCPP). If it does, it adds the `sycl` subdirectory to the build using `add_subdirectory`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_CPU_RUNTIME MATCHES ${sycl_rt_pattern} OR DNNL_GPU_RUNTIME MATCHES ${sycl_rt_pattern})\n    add_subdirectory(sycl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This snippet defines an object library named `${LIB_PACKAGE_NAME}_xpu_ocl` using `add_library`. The source files for the library are specified using the `SOURCES` variable, which was populated in the previous step. The `set_property` command appends the target objects of the created library to the global property `DNNL_LIB_DEPS`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/ocl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_xpu_ocl)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Appending to a Global Property in CMake\nDESCRIPTION: This snippet appends the object files from the newly created object library `${OBJ_LIB}` to the global property `DNNL_LIB_DEPS`. This is useful for managing dependencies and ensuring that the object files are included in the final build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/interface/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Creating Memory Descriptor with Strides in oneDNN (C++)\nDESCRIPTION: This code shows how to create a oneDNN memory descriptor with strides using the C++ API. The code demonstrates the creation of a memory descriptor object `md` with the specified dimensions, data type (dnnl_f32), and strides.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl_memory_desc_t md; // memory descriptor object\n\n    // logical description, layout independent\n    int ndims = 4;                   // # dimensions\n    dnnl_dims_t dims = {N, C, H, W}; // dimensions themselves\n    dnnl_dims_t strides = {stride_n, stride_c, stride_h, stride_w};\n\n    dnnl_memory_desc_create_with_strides(&md, ndims, dims, dnnl_f32, strides);\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion in CMake\nDESCRIPTION: This snippet conditionally adds the `sycl` subdirectory to the build process based on the values of CMake variables: `DNNL_SYCL_GENERIC`, `DNNL_SYCL_CUDA`, or `DNNL_AMD_ENABLE_SYCL_KERNELS`. It enables SYCL kernels for generic and NVIDIA vendors. For AMD, it only enables SYCL kernels if the target architecture is specified.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_SYCL_GENERIC OR DNNL_SYCL_CUDA OR DNNL_AMD_ENABLE_SYCL_KERNELS)\n    add_subdirectory(sycl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files in CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to recursively find all C and C++ source files in the current source directory. It populates the `SOURCES` variable with the paths to these files, which will later be used for compilation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/rv64/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command in CMake to find all C/C++ source files (`.c`, `.cpp`, `.h`, `.hpp`) in the current source directory. The resulting list of files is stored in the `SOURCES` variable. This approach simplifies the inclusion of source files in the build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/fake/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Test Executable and Sources with CMake\nDESCRIPTION: This CMake snippet defines the test executable `test_api_sycl`, gathers all `test_*.cpp` source files in the current directory, appends the `MAIN_SRC_GTEST` variable to the source list, and registers the executable with the specified sources and dependencies. The test executable is dependent on `dnnl_gtest` library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/sycl/api/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_EXE test_api_sycl)\n\nfile(GLOB TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_*.cpp)\nlist(APPEND TEST_SOURCES ${MAIN_SRC_GTEST})\n\nregister_exe(${TEST_EXE} \"${TEST_SOURCES}\" \"test\" \"dnnl_gtest\")\n```\n\n----------------------------------------\n\nTITLE: Enabling CPU ISA with CMake\nDESCRIPTION: This snippet shows how to enable specific CPU Instruction Set Architectures (ISAs) during the oneDNN build process using the `ONEDNN_ENABLE_PRIMITIVE_CPU_ISA` CMake option. When specified, selected ISA and all \"smaller\" ISAs will be available. CPU dispatcher controls are also affected by this option.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n-DONEDNN_ENABLE_PRIMITIVE_CPU_ISA=AVX2\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library in CMake\nDESCRIPTION: This CMake commands creates an object library named `${DNNL_LIBRARY_NAME}_cpu_zarch` from the source files stored in the `SOURCES` variable. The object library is then added to the `DNNL_LIB_DEPS` global property so that the main oneDNN library can link against it.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/s390x/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${DNNL_LIBRARY_NAME}_cpu_zarch)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library in CMake\nDESCRIPTION: This CMake code snippet creates an object library named 'graph_unit_test_fake_backend' using the add_library command. It specifies the library type as OBJECT and lists the source files to be compiled into the library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/fake/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB graph_unit_test_fake_backend)\n\nadd_library(${OBJ_LIB} OBJECT\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_compiled_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_fake_backend.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_pass.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library in CMake\nDESCRIPTION: This CMake command creates an object library from a list of source files. Object libraries do not produce a linked library, but rather a collection of object files that can be linked into other targets. The variable `TEST_INTERFACE_SYCL_SOURCES` holds the list of files to compile.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB graph_unit_test_interface_sycl)\nadd_library(${OBJ_LIB} OBJECT ${TEST_INTERFACE_SYCL_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Adding include directories with CMake\nDESCRIPTION: This snippet adds several directories to the include path for the project, including the current source directory, gtest, in, and the src directory.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(${CMAKE_CURRENT_SOURCE_DIR}\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/gtest\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/in\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/../../src\n)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: This snippet uses the file(GLOB) and file(GLOB_RECURSE) commands to find all .hpp and .cpp source files in the current source directory and specified subdirectories (generator, generator/pieces, selector).  The found files are then added to the SOURCES variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/gemm/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(DIRS \"generator;generator/pieces;selector\")\n\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\nforeach(d ${DIRS})\n    file(GLOB_RECURSE d_sources\n        ${ONEDNN_GEMMSTONE_DIR}/${d}/*.hpp\n        ${ONEDNN_GEMMSTONE_DIR}/${d}/*.cpp\n        )\n    list(APPEND SOURCES \"${d_sources}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Softmax Driver Invocation\nDESCRIPTION: This command invokes the softmax driver with optional benchdnn and softmax-specific knobs, followed by a softmax problem description. It allows users to benchmark different softmax configurations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_softmax.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --softmax [benchdnn-knobs] [softmax-knobs] [softmax-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Profiling with Linux Perf and JITdump\nDESCRIPTION: This command enables jitdump and perfmap profiling modes and writes jitdump files into the `.debug` directory. It then uses perf to record performance data for a oneDNN benchmark.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n$ JITDUMPDIR=. ONEDNN_JIT_PROFILE=6 perf record -k1 ./tests/benchdnn/benchdnn --conv --mode=P mb1ic32ih14oc32oh14kh3ph1n\"resnet_50:res4a_branch2b*6\"\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Generator with Split Output\nDESCRIPTION: Example showing how to validate multiple primitives with the benchdnn generator using the `-s True` option for splitting output by driver type.\nThe output is split into reorder and conv operations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n> ./scripts/verbose_converter/verbose_converter.py -i input.log -s True\n--reorder\n --reset --allow-enum-tags-only=0 --engine=cpu    --sdt=f32 --ddt=f32  --stag=abcd --dtag=aBcd8b   3x32x13x13\n --reset --allow-enum-tags-only=0 --engine=cpu    --sdt=f32 --ddt=f32  --stag=abcd --dtag=ABcd8b8a   64x32x3x3\n --reset --allow-enum-tags-only=0 --engine=cpu    --sdt=f32 --ddt=f32  --stag=aBcd8b --dtag=abcd   3x64x4x4\n --reset --allow-enum-tags-only=0 --engine=cpu    --sdt=f32 --ddt=f32  --stag=abcd --dtag=aBcd8b   3x32x13x13\n --reset --allow-enum-tags-only=0 --engine=cpu    --sdt=f32 --ddt=f32  --stag=abcde --dtag=Abcde8a   32x1x1x3x3\n --reset --allow-enum-tags-only=0 --engine=cpu    --sdt=f32 --ddt=f32  --stag=aBcd8b --dtag=abcd   3x32x4x4\n\n--conv\n --reset --allow-enum-tags-only=0 --engine=cpu --dir=FWD_B --alg=direct --cfg=f32  --stag=aBcd8b --wtag=ABcd8b8a --dtag=aBcd8b  --attr-post-ops=eltwise_relu mb3_ic32oc64_ih13oh4kh3sh4dh0ph1_iw13ow4kw3sw4dw0pw1\n --reset --allow-enum-tags-only=0 --engine=cpu --dir=FWD_B --alg=direct --cfg=f32  --stag=aBcd8b --wtag=Abcde8a --dtag=aBcd8b  --attr-post-ops=eltwise_relu g32mb3_ic32oc32_ih13oh4kh3sh4dh0ph1_iw13ow4kw3sw4dw0pw1\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Definitions in CMake\nDESCRIPTION: This code snippet uses `add_definitions_with_host_compiler` to define preprocessor macros based on CMake variables. These definitions enable or disable certain features of the library during compilation, such as verbose mode, concurrent execution, and primitive cache.  The macros are used to control the behavior of the compiled oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (DNNL_LIBRARY_TYPE STREQUAL \"SHARED\")\n    add_definitions_with_host_compiler(-DDNNL_DLL_EXPORTS)\nendif()\n\nif(NOT DNNL_VERBOSE)\n    add_definitions_with_host_compiler(-DDISABLE_VERBOSE)\nendif()\n\nif(DNNL_ENABLE_CONCURRENT_EXEC)\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_CONCURRENT_EXEC)\nendif()\n\nif(DNNL_ENABLE_PRIMITIVE_CACHE)\n    message(STATUS \"Primitive cache is enabled\")\nelse()\n    add_definitions_with_host_compiler(-DDNNL_DISABLE_PRIMITIVE_CACHE)\n    message(STATUS \"Primitive cache is disabled\")\nendif()\n\nif(DNNL_DEV_MODE)\n  add_definitions_with_host_compiler(-DDNNL_DEV_MODE)\n  message(STATUS \"Developer mode enabled\")\nendif()\n\nif(DNNL_ENABLE_ITT_TASKS AND NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    # Only supported for certain architectures (see src/common/CMakeLists.txt)\n    if(DNNL_TARGET_ARCH STREQUAL \"AARCH64\" OR DNNL_TARGET_ARCH STREQUAL \"X64\")\n        add_definitions_with_host_compiler(-DDNNL_ENABLE_ITT_TASKS)\n    endif()\nendif()\n\nif(DNNL_ENABLE_MAX_CPU_ISA)\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_MAX_CPU_ISA)\nendif()\n\nif(DNNL_ENABLE_CPU_ISA_HINTS)\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_CPU_ISA_HINTS)\nendif()\n\nif(WIN32)\n    add_definitions_with_host_compiler(-D_WIN)\n    add_definitions_with_host_compiler(-DNOMINMAX)\n    add_definitions_with_host_compiler(-DWIN32_LEAN_AND_MEAN)\nendif()\n\n# Windows does not support weak/strong symbols and no guarrantees by the linker\n# for out_of_memory testing to work. Not tested on macOS\nif(UNIX)\n    if(DNNL_ENABLE_MEM_DEBUG)\n        add_definitions_with_host_compiler(-DDNNL_ENABLE_MEM_DEBUG)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Architecture-Specific Subdirectories\nDESCRIPTION: This code block conditionally adds architecture-specific subdirectories based on the value of `DNNL_TARGET_ARCH`. It checks for X64, AARCH64, PPC64, S390X, and RV64 architectures and adds the corresponding subdirectory if the architecture matches. This allows for architecture-specific code and configurations to be included in the build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif (DNNL_TARGET_ARCH STREQUAL \"X64\")\n    add_subdirectory(x64)\nendif()\nif (DNNL_TARGET_ARCH STREQUAL \"AARCH64\")\n    add_subdirectory(aarch64)\nendif()\nif (DNNL_TARGET_ARCH STREQUAL \"PPC64\")\n    add_subdirectory(ppc64)\nendif()\nif (DNNL_TARGET_ARCH STREQUAL \"S390X\")\n    add_subdirectory(s390x)\nendif()\nif (DNNL_TARGET_ARCH STREQUAL \"RV64\")\n    add_subdirectory(rv64)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Registering Graph API Test Suite in CMake\nDESCRIPTION: This CMake function `register_graph_api_test_suite` adds a test based on the provided test suite name and executable name. It checks if the test suite is CPU or GPU specific and then adds the test accordingly using `add_test` function. It also calls `maybe_configure_windows_test` for windows specific configurations. The test is added either for CPU or GPU based on the `DNNL_CPU_RUNTIME` and `DNNL_GPU_RUNTIME` variables, respectively.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(register_graph_api_test_suite test_suite_name exe_name)\n    if(${test_suite_name} MATCHES \"_cpu\")\n        if(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n            add_test(${test_suite_name} ${exe_name} --engine=cpu)\n            maybe_configure_windows_test(${test_suite_name} TEST)\n        endif()\n    elseif(${test_suite_name} MATCHES \"_gpu\")\n        if(DNNL_GPU_RUNTIME MATCHES ${gpu_rt_pattern})\n            add_test(${test_suite_name} ${exe_name} --engine=gpu)\n            maybe_configure_windows_test(${test_suite_name} TEST)\n        endif()\n    else()\n        if(DNNL_GPU_RUNTIME MATCHES ${gpu_rt_pattern})\n            add_test(${test_suite_name}_gpu ${exe_name} --engine=gpu)\n            maybe_configure_windows_test(${test_suite_name}_gpu TEST)\n        endif()\n\n        if(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n            add_test(${test_suite_name}_cpu ${exe_name} --engine=cpu)\n            maybe_configure_windows_test(${test_suite_name}_cpu TEST)\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library\nDESCRIPTION: Creates an object library named `${LIB_PACKAGE_NAME}_gpu_intel_jit` from the collected source files.  The object files are then added to the global `DNNL_LIB_DEPS` property.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_intel_jit)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Appending to a Global Property in CMake\nDESCRIPTION: This snippet uses `set_property` with the `GLOBAL` scope and `APPEND` option to add the object files from the `${OBJ_LIB}` library to the `DNNL_LIB_DEPS` global property. The `<TARGET_OBJECTS:${OBJ_LIB}>` generator expression expands to the list of object files contained within the object library. This allows linking dependencies during the build process. Requires that `OBJ_LIB` is a valid CMake target, and that the `DNNL_LIB_DEPS` property exists.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Globbing source files (Engine Dependent)\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all C++ source files in the current source directory that end with `test_allocator.cpp`. These are stored in the `TEST_UTILS_ENGINE_DEPENDENT_SOURCES` variable, indicating dependency on the execution engine.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB TEST_UTILS_ENGINE_DEPENDENT_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_allocator.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This CMake command uses the `file(GLOB)` function to find all `.hpp` and `.cpp` files within the current source directory and stores their paths in the `SOURCES` variable. This allows for dynamic inclusion of source files in the build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/amd/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command in CMake to collect all header and source files ('.h', '.hpp', '.c', '.cpp') within the current source directory. The resulting list of files is stored in the `SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/common/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: DNNL v1.1 Example\nDESCRIPTION: This code snippet demonstrates the equivalent code using DNNL v1.1.  It includes the dnnl.hpp header, uses the dnnl namespace, accesses dnnl_memory_desc_t, and invokes the conv.exec function with the DNNL_ARGS_SRC argument.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/transition-to-dnnl.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"dnnl.hpp\"\n\nusing namespace dnnl;\n\ndnnl_memory_desc_t md;\nif (md.format_kind == dnnl_blocked) {}\nconv.exec(stream, {{DNNL_ARGS_SRC, src}, ...});\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: This snippet uses the file(GLOB) command to find all .hpp and .cpp files in the CMAKE_CURRENT_SOURCE_DIR and stores them in the SOURCES variable. This variable is later used to define the object library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB ...)` command to collect all `.hpp` and `.cpp` files within the current source directory. These files will be used as source files for the GPU microkernel library. The `CMAKE_CURRENT_SOURCE_DIR` variable provides the path to the current directory.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/microkernels/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command in CMake to find all `.hpp` and `.cpp` files within the current source directory. The found files are then stored in the `SOURCES` variable for later use in defining the library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for SYCL in CMake\nDESCRIPTION: This snippet conditionally sets compiler flags (`-fsycl-targets`) when DNNL_WITH_SYCL is enabled, specifying target architectures for SYCL compilation (CUDA or generic + NVIDIA).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_WITH_SYCL)\n    if(DNNL_SYCL_CUDA OR (DNNL_SYCL_GENERIC AND NVIDIA_TARGET_SUPPORTED))\n        append(CMAKE_CXX_FLAGS \"-fsycl-targets=nvptx64-nvidia-cuda,spir64\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files\nDESCRIPTION: This snippet uses `file(GLOB)` and `file(GLOB_RECURSE)` to define lists of source files to be included in the oneDNN library build. It considers C++ and C files within the specified directories and subdirectories. The `foreach` loop consolidates the extra sources into the main SOURCES variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp)\nfile(GLOB_RECURSE SOURCES_EXTRA\n    ${CMAKE_CURRENT_SOURCE_DIR}/gemm/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/matmul/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/reorder/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/rnn/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/ukernel/*.[ch]pp\n    )\n\nforeach(SOURCE_FILE ${SOURCES_EXTRA})\n    list(APPEND SOURCES \"${SOURCE_FILE}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Conditional ITT Source Inclusion with CMake\nDESCRIPTION: This CMake snippet conditionally includes Intel Threading Tools (ITT) sources based on whether DNNL_ENABLE_JIT_PROFILING or DNNL_ENABLE_ITT_TASKS are enabled and the target architecture is AARCH64 or X64. If the conditions are met, it globs ITT source files and appends them to the `SOURCES` list. For X64, it also sets compiler flags and includes processor trace sources.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/common/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_ENABLE_JIT_PROFILING OR DNNL_ENABLE_ITT_TASKS)\n    if(DNNL_TARGET_ARCH STREQUAL \"AARCH64\" OR DNNL_TARGET_ARCH STREQUAL \"X64\")\n        file(GLOB ITT_SOURCES\n            ${PROJECT_SOURCE_DIR}/third_party/ittnotify/*.c\n            )\n        list(APPEND SOURCES ${ITT_SOURCES})\n\n        # Add processor trace sources\n        if(DNNL_TARGET_ARCH STREQUAL \"X64\")\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DITT_API_IPT_SUPPORT\")\n            set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DITT_API_IPT_SUPPORT\")\n            if(UNIX OR MINGW)\n                enable_language(ASM)\n                set(ITT_PT ${PROJECT_SOURCE_DIR}/third_party/ittnotify/ittptmark64.S)\n            else()\n                enable_language(ASM_MASM)\n                set(ITT_PT ${PROJECT_SOURCE_DIR}/third_party/ittnotify/ittptmark64.asm)\n            endif()\n            list(APPEND SOURCES ${ITT_PT})\n        endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating GPU Kernel List\nDESCRIPTION: This line calls the `gen_gpu_kernel_list` function, passing the template file, the output source file, and the lists of OpenCL source and header files as arguments. This function is responsible for creating the `ocl_kernel_list.cpp` file based on the provided inputs.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ngen_gpu_kernel_list(\n    \"${kernel_list_templ}\"\n    \"${kernel_list_src}\"\n    \"${CL_SOURCES}\"\n    \"${CL_HEADERS}\")\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Generator Example\nDESCRIPTION: Example of using the benchdnn generator to create test cases from verbose output.\nThe verbose output is piped into the script, generating a benchdnn command.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n> echo \"onednn_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:acb:f0 wei_f32::blocked:aBdc8b:f0 bia_f32::blocked:a:f0 dst_f32::blocked:acb:f0,,alg:convolution_direct,g1mb2_ic3oc16_iw5ow5kw3sw1dw0pw1\" | ./scripts/verbose_converter/verbose_converter.py\n--conv --reset --allow-enum-tags-only=0 --engine=cpu --dir=FWD_B --alg=direct --cfg=f32  --stag=acb --wtag=aBdc8b --dtag=acb   g1mb2_ic3oc16_iw5ow5kw3sw1dw0pw1\n```\n\n----------------------------------------\n\nTITLE: Profiling Hotspots with VTune\nDESCRIPTION: Collects profiling data using VTune Profiler to identify performance hotspots in a oneDNN application. It uses the `amplxe-cl` command-line tool with specific knobs to enable hardware event-based sampling and specifies the output format.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/profilers.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n$ amplxe-cl -collect hotspots -q -no-summary -knob sampling-mode=hw -r dnnl-vtune ./benchdnn --mode=P --conv --batch=inputs/conv/shapes_alexnet\namplxe: Warning: To enable hardware event-base sampling, VTune Profiler has disabled the NMI watchdog timer.\nThe watchdog timer will be re-enabled after collection completes.\nOutput template: perf,%engine%,%impl%,%name%,%prb%,%Gops%,%Gfreq%,%-time%,%-Gflops%,%0time%,%0Gflops%\nperf,cpu,jit:avx512_common,\"alexnet:conv1\",--conv g1mb256ic3ih227oc96oh55kh11sh4ph0n\"alexnet:conv1\",53.9726,0,17.4285,3096.81,22.5851,2389.74\nperf,cpu,jit:avx512_common,\"alexnet:conv2\",--conv g2mb256ic96ih27oc256oh27kh5ph2n\"alexnet:conv2\",104.696,0,20.2195,5177.98,21.9233,4775.56\nperf,cpu,jit:avx512_common,\"alexnet:conv3\",--conv mb256ic256ih13oc384oh13kh3ph1n\"alexnet:conv3\",68.904,0,15.5134,4441.57,18.1391,3798.64\nperf,cpu,jit:avx512_common,\"alexnet:conv4\",--conv g2mb256ic384ih13oc384oh13kh3ph1n\"alexnet:conv4\",51.678,0,11.7397,4401.97,12.4623,4146.76\nperf,cpu,jit:avx512_common,\"alexnet:conv5\",--conv g2mb256ic384ih13oc256oh13kh3ph1n\"alexnet:conv5\",34.452,0,7.77148,4433.13,8.50435,4051.11\ntests:5 passed:5 skipped:0 mistrusted:0 unimplemented:0 failed:0 listed:0\ntotal perf: min(ms):72.6726 avg(ms):83.6142\n```\n\n----------------------------------------\n\nTITLE: Setting Global Library Dependencies in CMake\nDESCRIPTION: This snippet sets the global property `DNNL_LIB_DEPS` to include the object files from the `${LIB_PACKAGE_NAME}_gpu_intel_compute` object library. The `set_property` command appends the target objects to the existing `DNNL_LIB_DEPS` property, which is used to manage dependencies between different parts of the oneDNN project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/compute/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files CMake\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to find all C, H, CPP, and HPP files in the current source directory. These files are then stored in the `SOURCES` variable for later use in the build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/aarch64/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all `.hpp` and `.cpp` files in the current source directory. The found files are stored in the `SOURCES` variable for later use in creating the object library. No specific dependencies are required beyond standard CMake.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/nvidia/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Filtering Source Files for oneDNN Examples with CMake\nDESCRIPTION: This snippet filters the source files based on various build options such as `DNNL_EXPERIMENTAL_UKERNEL`, `DNNL_SYCL_CUDA`, `ONEDNN_BUILD_GRAPH`, `DNNL_SYCL_GENERIC`, `DNNL_SYCL_HIP`, `DNNL_GPU_RUNTIME`, `DNNL_WITH_SYCL`, and `DNNL_CPU_SYCL`.  It removes specific source files from the `sources` list if the corresponding build options are not enabled, ensuring that only the relevant examples are built.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/examples/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_EXPERIMENTAL_UKERNEL)\n    list(REMOVE_ITEM sources ${CMAKE_CURRENT_SOURCE_DIR}/ukernels/cpu_brgemm.cpp)\nendif()\n\n# Remove tests for CUDA which use unimplemented primitives\nif(DNNL_SYCL_CUDA)\n    list(REMOVE_ITEM sources\n        ${CMAKE_CURRENT_SOURCE_DIR}/bnorm_u8_via_binary_postops.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/rnn_training_f32.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/tutorials/matmul/inference_int8_matmul.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/binary.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/lstm.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/layer_normalization.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/reorder.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/shuffle.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/group_normalization.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/vanilla_rnn.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/lbr_gru.cpp)\nendif()\n\n# Remove examples for Graph API if graph component is not enabled\nif(NOT ONEDNN_BUILD_GRAPH)\n    list(REMOVE_ITEM sources\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/cpu_getting_started.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/sycl_getting_started.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/cpu_inference_int8.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/cpu_single_op_partition.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/sycl_single_op_partition.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/gpu_opencl_getting_started.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/sdpa.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/mqa.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/sdpa_stacked_qkv.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/gqa.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/gated_mlp.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/gated_mlp_wei_combined.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/graph/gated_mlp_int4.cpp\n        )\nendif()\n\nif(DNNL_SYCL_GENERIC)\n    list(REMOVE_ITEM sources\n        # XXX: Enable when InnerProduct is implemented\n        ${CMAKE_CURRENT_SOURCE_DIR}/cnn_inference_f32.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/inner_product.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/rnn_training_f32.cpp\n        # XXX: Enable when Reduction is implemented\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/reduction.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/group_normalization.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/lbr_gru.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/lstm.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/vanilla_rnn.cpp)\nendif()\n\nif(DNNL_SYCL_HIP)\n    # Build examples for supported primitives that support required features.\n    set(sources)\n    list(APPEND sources\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/softmax.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/lrn.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/eltwise.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/reduction.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/matmul.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/matmul_perf.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/primitives/inner_product.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/cnn_inference_f32.c\n        ${CMAKE_CURRENT_SOURCE_DIR}/cnn_inference_int8.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/cnn_training_bf16.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/getting_started.cpp)\nendif()\n\n# Skip SYCL, GPU and cross-engine examples\nforeach(f ${sources})\n    get_filename_component(f_name ${f} NAME)\n    if(DNNL_GPU_RUNTIME STREQUAL \"NONE\" AND ${f_name} MATCHES \"^(cross_engine|gpu)\")\n        list(REMOVE_ITEM sources \"${f}\")\n    endif()\n    if(NOT DNNL_WITH_SYCL AND ${f_name} MATCHES \"^sycl\")\n        list(REMOVE_ITEM sources \"${f}\")\n    endif()\nendforeach()\n\n# In case of SYCL, skip CPU examples that directly work with raw pointers\nif(DNNL_CPU_SYCL)\n    foreach(f ${sources})\n        get_filename_component(fname ${f} NAME)\n        if(${fname} MATCHES \"cpu_\")\n            list(REMOVE_ITEM sources \"${f}\")\n        endif()\n    endforeach()\nendif()\n\n# Do not build C examples for TBB threading runtime because\n# TBB doesn't provide C API to do explicit finalization.\nif (DNNL_CPU_RUNTIME STREQUAL \"TBB\" OR DNNL_CPU_SYCL)\n    foreach(f ${sources})\n        get_filename_component(fname ${f} NAME)\n        if(${fname} MATCHES \".*\\\\.c$\")\n            list(REMOVE_ITEM sources \"${f}\")\n        endif()\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Command Line Usage\nDESCRIPTION: Shows how to use the verbose converter from the command line with different arguments.\nArguments include input file, parser type, action, split option, aggregation fields, verbose level, output file, generator, and events.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython3 verbose_converter.py [-h] [-i INPUT] [-p PARSER] [-a ACTION] [-s SPLIT]\n                            [-k AGGREGATE [AGGREGATE ...]] [-v VERBOSE_LEVEL] [-o OUTPUT]\n                            [-g GENERATOR]\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library in CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_graph_interface` from the source files collected in the `SOURCES` variable. An object library is a collection of object files that can be linked into other libraries or executables. This allows for modular compilation and faster build times.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/interface/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_graph_interface)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Implementation Filter Options\nDESCRIPTION: These command-line options are used to filter oneDNN implementations in benchdnn. `--impl` specifies which implementations to search for, while `--skip-impl` specifies which implementations to skip. `--global-impl` and `--global-skip-impl` are global counterparts that override local options.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_impl_filter.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n--impl=STRING_NAME[,STRING_NAME...]\n--skip-impl=STRING_NAME[,STRING_NAME...]\n--global-impl=STRING_NAME[,STRING_NAME...]\n--global-skip-impl=STRING_NAME[,STRING_NAME...]\n```\n\n----------------------------------------\n\nTITLE: Executing Convolution Primitive in oneDNN (C++)\nDESCRIPTION: This code demonstrates how to execute a convolution primitive in oneDNN, passing the appropriate memory objects as arguments. The `DNNL_ARG_SRC`, `DNNL_ARG_WEIGHTS`, and `DNNL_ARG_DST` constants specify the roles of the memory objects within the convolution operation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/usage_models/inference.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nauto conv = convolution_forward(conv_prim_descr);\nconv.execute(s, {\n            {DNNL_ARG_SRC, conv_source_memory},\n            {DNNL_ARG_WEIGHTS, conv_weights_memory},\n            {DNNL_ARG_DST, conv_dest_memory}});\n```\n\n----------------------------------------\n\nTITLE: Layout Abstraction Description\nDESCRIPTION: Describes a memory layout, containing a physical representation of a tensor. It includes data type, number of dimensions, offset to the start of the tensor, and layout blocks with their dimension index, block size, and stride.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/ir/README.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n`4n2c7h7w32n32c` (6 blocks) (`NChw32n32c` in oneDNN convention)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Appending Libraries in CMake\nDESCRIPTION: This snippet conditionally appends `-ldl` and `-lrt` to `EXTRA_SHARED_LIBS` on UNIX systems (excluding Apple), which are required for JIT profiling and ITT tasks.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_ENABLE_JIT_PROFILING OR DNNL_ENABLE_ITT_TASKS)\n    if (UNIX AND NOT APPLE)\n        # Not every compiler adds -ldl and -lrt automatically\n        list(APPEND EXTRA_SHARED_LIBS \"${CMAKE_DL_LIBS}\")\n        list(APPEND EXTRA_SHARED_LIBS rt)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for IntelLLVM or Clang -Wconversion\nDESCRIPTION: This snippet checks if the compiler is IntelLLVM or Clang. If so, it appends a list of -Wconversion flags to CMAKE_CXX_FLAGS and adds a preprocessor definition ENABLE_LLVM_WCONVERSION. This is done to enable more strict warning messages during compilation related to type conversions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\" OR CMAKE_CXX_COMPILER_ID MATCHES \"(Apple)?[Cc]lang\")\n    append(CMAKE_CXX_FLAGS \"-Wbitfield-enum-conversion -Wbool-conversion -Wconstant-conversion -Wenum-conversion -Wimplicit-int-conversion -Wliteral-conversion -Wnon-literal-null-conversion -Wnull-conversion\")\n    add_definitions(\"-DENABLE_LLVM_WCONVERSION\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running F32 Backward Convolutions wrt Weights (sh)\nDESCRIPTION: This example shows how to run a set of f32 backward convolutions with respect to weights. It includes options to set the verbose level and match a specific kernel size (kh=3).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv -v2 --dt=f32 --dir=BWD_W \\\n               --match='.*kh3[^0-9].*' --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Run MatMul with asymmetric quantization\nDESCRIPTION: This command demonstrates running reduced precision (int8) matrix multiplication with asymmetric quantization for source and destination, and symmetric quantization for weights. It also provides zero points at runtime using `--attr-zero-points`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_matmul.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --matmul \\\n               --dt=u8:s8:u8 \\\n               --wtag=any \\\n               --attr-zero-points=src:common:1+dst:common:-2 \\\n               10x30:30x20\n```\n\n----------------------------------------\n\nTITLE: Setting the Binary Name in CMake\nDESCRIPTION: This snippet sets the name of the executable binary to 'test_graph_unit'. This binary will be the compiled output of the graph unit tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(BINARY_NAME test_graph_unit)\n```\n\n----------------------------------------\n\nTITLE: Building on Linux/macOS (GCC targeting AArch64)\nDESCRIPTION: These commands configure CMake for cross-compilation targeting AArch64 using GCC. The CMAKE_SYSTEM_NAME, CMAKE_SYSTEM_PROCESSOR, and CMAKE_LIBRARY_PATH variables are set to specify the target architecture and library locations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nexport CC=aarch64-linux-gnu-gcc\nexport CXX=aarch64-linux-gnu-g++\ncmake .. \\\n          -DCMAKE_SYSTEM_NAME=Linux \\\n          -DCMAKE_SYSTEM_PROCESSOR=AARCH64 \\\n          -DCMAKE_LIBRARY_PATH=/usr/aarch64-linux-gnu/lib \\\n          <extra build options>\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to recursively search for source files with the extensions `.h`, `.hpp`, `.c`, and `.cpp` within the current source directory. The found files are stored in the `SOURCES` variable for later use in building the library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/compute/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet conditionally adds subdirectories to the build based on various CMake flags. These subdirectories represent different components of the oneDNN library, such as the API, internals, regression tests, OCL implementation, SYCL implementation, and graph API.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DNNL_ENABLE_STACK_CHECKER)\n    add_subdirectory(api)\n    add_subdirectory(internals)\n\n    if(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n        add_subdirectory(regression)\n    endif()\n\n    if(DNNL_GPU_RUNTIME STREQUAL \"OCL\")\n        add_subdirectory(ocl)\n    endif()\n\n    if(DNNL_WITH_SYCL)\n        add_subdirectory(sycl)\n    endif()\n\n    if(ONEDNN_BUILD_GRAPH)\n        add_subdirectory(graph)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Performance Template Option\nDESCRIPTION: This snippet shows the command-line option for specifying a performance report template in benchdnn. It allows the user to select a default template (def), a CSV template (csv), or a custom template defined by the user.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n[--perf-template={def [default], csv, CUSTOM_TEMPLATE}]\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library\nDESCRIPTION: This snippet defines the main object library for the GPU JIT GEMM functionality and adds all the source files to it.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/gemm/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_intel_jit_gemm)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Set CMake Minimum Version\nDESCRIPTION: Sets the minimum required CMake version to 3.13. This ensures that the CMake version used to build the project supports the features used in the CMake scripts.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This CMake code snippet uses the `file(GLOB_RECURSE)` command to recursively find all header (.h, .hpp) and source (.c, .cpp) files within the current source directory.  The found files are then stored in the `SOURCES` variable for later use in the build process. This ensures that all relevant files are included in the compilation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Conditional CPU-Specific tests inclusion with CMake\nDESCRIPTION: This snippet conditionally includes CPU-specific tests based on the DNNL_CPU_RUNTIME setting. It uses `file(GLOB)` to find the test files and then appends them to the PRIM_TEST_CASES_SRC list.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    file(GLOB CPU_SPECIFIC_TESTS\n        test_gemm_f16.cpp\n        test_gemm_f32.cpp\n        test_gemm_f16f16f32.cpp\n        test_gemm_bf16bf16f32.cpp\n        test_gemm_bf16bf16bf16.cpp\n        test_gemm_u8s8s32.cpp\n        test_gemm_s8s8s32.cpp\n        test_gemm_s8u8s32.cpp\n        test_gemm_u8u8s32.cpp\n        test_convolution_format_any.cpp\n        test_global_scratchpad.cpp\n        )\n      if(DNNL_CPU_RUNTIME STREQUAL \"THREADPOOL\")\n        list(APPEND CPU_SPECIFIC_TESTS test_iface_threadpool.cpp)\n      endif()\n    foreach(TEST_FILE ${CPU_SPECIFIC_TESTS})\n        list(APPEND PRIM_TEST_CASES_SRC \"${TEST_FILE}\")\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Pooling Benchmarks\nDESCRIPTION: This command executes the pooling benchmark with specified options and problem descriptors using the benchdnn tool. The command allows for customization of various parameters like direction, data type, tag, algorithm, and minibatch size.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_pool.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --pool [benchdnn-knobs] [pool-knobs] [pool-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Setting up project with CMake\nDESCRIPTION: This snippet initializes the CMake project, defines the project name, and specifies the programming languages used (C and C++). It requires CMake version 3.13 or higher. The PROJECT_NAME variable is used to set the name of the project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/other/subproject/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13)\n\nset(PROJECT_NAME \"Project\")\n\nproject(${PROJECT_NAME} C CXX)\n```\n\n----------------------------------------\n\nTITLE: Appending to a List in CMake\nDESCRIPTION: This snippet iterates through the list of extra source files (`SOURCES_EXTRA`) and appends each file to the main `SOURCES` list. This ensures that all source files are included in the build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(SOURCE_FILE ${SOURCES_EXTRA})\n    list(APPEND SOURCES \"${SOURCE_FILE}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Building on Linux/macOS (GCC with Arm Compute Library)\nDESCRIPTION: These commands configure CMake to build oneDNN with support for the Arm Compute Library (ACL) on AArch64 hosts.  The ACL_ROOT_DIR variable must be set to point to the location of the Compute Library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nexport ACL_ROOT_DIR=<path/to/Compute Library>\ncmake .. \\\n          -DDNNL_AARCH64_USE_ACL=ON \\\n          <extra build options>\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Test Script Help Message\nDESCRIPTION: This code snippet displays the help message for the `benchdnn_test.py` script. It shows the available command-line arguments for the script, including options for specifying the path to benchdnn, the dataset, and the input paths.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/tests/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npython3  benchdnn_test.py [-h] [-b BENCHDNN_PATH] [-d DATASET] [-i INPUTS_PATH]\n```\n\n----------------------------------------\n\nTITLE: Getting Global Properties in CMake\nDESCRIPTION: This snippet retrieves global properties `GRAPH_UNIT_TEST_DEPS` and `DNNL_LIB_DEPS`, which likely contain dependencies needed for the unit tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(UNIT_TEST_DEPS GLOBAL PROPERTY GRAPH_UNIT_TEST_DEPS)\nget_property(LIB_DEPS GLOBAL PROPERTY DNNL_LIB_DEPS)\n```\n\n----------------------------------------\n\nTITLE: Configuring Threadpool Implementation for Testing\nDESCRIPTION: This snippet demonstrates how to configure the threadpool implementation used for testing when building oneDNN with threadpool support. It sets `_ONEDNN_TEST_THREADPOOL_IMPL` to `EIGEN` and specifies the Eigen3 directory.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build_options.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n$ cmake -DONEDNN_CPU_RUNTIME=THREADPOOL -D_ONEDNN_TEST_THREADPOOL_IMPL=EIGEN -DEigen3_DIR=/path/to/eigen/share/eigen3/cmake ..\n```\n\n----------------------------------------\n\nTITLE: Setting Additional Include Directories in CMake\nDESCRIPTION: This snippet specifies additional include directories for non-exported headers, including gtest and graph directories, to resolve header dependencies during compilation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(\n    ${PROJECT_SOURCE_DIR}/tests/gtests/graph\n    ${PROJECT_SOURCE_DIR}/tests/gtests/gtest\n    ${PROJECT_SOURCE_DIR}/include\n    ${PROJECT_SOURCE_DIR}/src/graph\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to collect all C/C++ header and source files (*.c, *.h, *.cpp, *.hpp) from the current source directory and its `pm` subdirectory.  The collected files are stored in the `SOURCES` variable for later use in defining the object library. It requires CMake to be installed and configured to run.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/pm/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Benchdnn TLB Extension Syntax\nDESCRIPTION: This syntax enables the TLB extension to the cold cache feature in benchdnn. The `SIZE` parameter specifies the amount of memory to allocate to trigger TLB misses. It should be a floating-point number followed by `M` (Megabytes) or `G` (Gigabytes). The default size is 1 Gigabyte.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_cold_cache.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntlb[:SIZE]\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library in CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_cpu_aarch64_xbyak_aarch64` from the source files collected in the `SOURCES` variable. Object libraries are collections of object files that can be linked into other libraries or executables.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/xbyak_aarch64/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_cpu_aarch64_xbyak_aarch64)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Conditional GPU-only test inclusion with CMake\nDESCRIPTION: This code snippet conditionally includes a GPU-only test case when the CPU runtime is set to NONE. It also sets the NO_ENGINE_PARAM property on the test file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    list(APPEND PRIM_TEST_CASES_SRC test_iface_gpu_only.cpp)\n    set_source_files_properties(test_iface_gpu_only.cpp PROPERTIES NO_ENGINE_PARAM true)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_graph_utils` using the `add_library` command. The object library is built from the source files listed in the `SOURCES` variable, which were previously collected using `file(GLOB)`. The `LIB_PACKAGE_NAME` variable needs to be defined for this to function correctly.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/utils/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_graph_utils)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Handle OpenMP Runtime Conflicts\nDESCRIPTION: Overrides the CPU runtime to be sequential if OpenMP runtime is requested but not found, forcing a sequential execution path when OpenMP is unavailable.  Sets DNNL_CPU_RUNTIME to \"SEQ\" if both DNNL_CPU_RUNTIME is \"OMP\" and DNNL_CPU_THREADING_RUNTIME is \"SEQ\".\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_CPU_RUNTIME STREQUAL \"OMP\" AND\n   DNNL_CPU_THREADING_RUNTIME STREQUAL \"SEQ\")\n    set(DNNL_CPU_RUNTIME \"SEQ\" CACHE STRING \"\" FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Check Platform Architecture\nDESCRIPTION: Checks if the platform is 64-bit. oneDNN only supports 64-bit platforms, so the build process is aborted if the platform is not 64-bit.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT CMAKE_SIZEOF_VOID_P EQUAL 8)\n    message(FATAL_ERROR \"oneDNN supports 64 bit platforms only\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_xpu` from the source files found earlier. It then appends the target objects of this library to the global `DNNL_LIB_DEPS` property.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_xpu)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Finding Source Files with GLOB_RECURSE in CMake\nDESCRIPTION: This CMake snippet uses `file(GLOB_RECURSE)` to recursively search for all `.h`, `.hpp`, `.c`, and `.cpp` files within the current source directory (`CMAKE_CURRENT_SOURCE_DIR`). The found files are stored in the `SOURCES` variable for later use.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Offset calculation for NCHW data format\nDESCRIPTION: This code snippet defines the offset function for the NCHW data format, which calculates the memory address displacement for a given logical index (n, c, h, w) in a 4D tensor.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\noffset_nchw(n, c, h, w) = n * CHW + c * HW + h * W + w\n```\n\n----------------------------------------\n\nTITLE: Glob Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command in CMake to find all header (.h, .hpp) and source files (.c, .cpp) within the current source directory and its subdirectories. It then stores these files in the `SOURCES` variable for later use in the build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Set Compiler Flags for DNNL with SYCL on Windows\nDESCRIPTION: This code snippet appends a compiler flag to suppress a specific warning when using DNNL with SYCL on Windows platforms. It addresses a '__stdcall' calling convention issue.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/gtest/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32 AND DNNL_WITH_SYCL)\n    append(CMAKE_CXX_FLAGS \"-Wno-ignored-attributes\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run Specific Zeropad Problem with Data Type and Layouts\nDESCRIPTION: This command runs a specific zero pad problem with a specified data type (f32) and iterates over the specified memory layouts. It uses the problem size 64x3x60x60.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_zeropad.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --zeropad --dt=f32 --tag=ABcd4a4b,nChw16c 64x3x60x60\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Target in CMake\nDESCRIPTION: This snippet links the specified libraries (dnnl_gtest, EXTRA_SHARED_LIBS, EXTRA_STATIC_LIBS, STATIC_LIB_DEPS, and SHARED_LIB_DEPS) to the `${BINARY_NAME}` target.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${BINARY_NAME}\n    dnnl_gtest\n    ${EXTRA_SHARED_LIBS}\n    ${EXTRA_STATIC_LIBS}\n    ${STATIC_LIB_DEPS}\n    ${SHARED_LIB_DEPS}\n)\n```\n\n----------------------------------------\n\nTITLE: Reorder Driver Usage\nDESCRIPTION: This command executes the reorder benchmark with specified knobs and problem descriptions. The `benchdnn-knobs` represent common benchmark options, `reorder-knobs` represent reorder-specific options, and `reorder-desc` defines the problem size.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reorder.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --reorder [benchdnn-knobs] [reorder-knobs] [reorder-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Setting up Object Library Name in CMake\nDESCRIPTION: This snippet defines the name of the object library for graph unit tests. The `OBJ_LIB` variable will be used later to define the library itself.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB graph_unit_test_interface)\n```\n\n----------------------------------------\n\nTITLE: Running Shuffle Benchmarks from Input File\nDESCRIPTION: This command executes the benchdnn shuffle benchmark by reading input configurations from a file. It demonstrates how to use the '--batch' option to specify an input file containing multiple shuffle problems.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_shuffle.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --shuffle --batch=inputs/shuffle/test_shuffle_all\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all `.hpp` and `.cpp` files in the current source directory and store them in the `SOURCES` variable. This list of source files is later used to create an object library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing primitive test cases with CMake\nDESCRIPTION: This snippet uses `file(GLOB)` to collect all the C++ source files for primitive test cases. These files are then added to the list of source files to be compiled.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB PRIM_TEST_CASES_SRC\n                              test_persistent_cache_api.cpp\n                              test_primitive_cache_mt.cpp\n                              test_iface_primitive_cache.cpp\n                              test_iface_pd.cpp\n                              test_iface_pd_iter.cpp\n                              test_iface_attr.cpp\n                              test_iface_binary_bcast.cpp\n                              test_iface_handle.cpp\n                              test_iface_runtime_dims.cpp\n                              test_iface_attr_quantization.cpp\n                              test_iface_weights_format.cpp\n                              test_iface_wino_convolution.cpp\n                              test_iface_sparse.cpp\n                              test_memory.cpp\n                              test_sum.cpp\n                              test_reorder.cpp\n                              test_cross_engine_reorder.cpp\n                              test_concat.cpp\n                              test_eltwise.cpp\n                              test_pooling_forward.cpp\n                              test_pooling_backward.cpp\n                              test_batch_normalization.cpp\n                              test_inner_product_forward.cpp\n                              test_inner_product_backward_data.cpp\n                              test_inner_product_backward_weights.cpp\n                              test_shuffle.cpp\n                              test_rnn_forward.cpp\n                              test_convolution_forward_f32.cpp\n                              test_convolution_forward_u8s8s32.cpp\n                              test_convolution_forward_u8s8fp.cpp\n                              test_convolution_eltwise_forward_f32.cpp\n                              test_convolution_eltwise_forward_x8s8f32s32.cpp\n                              test_convolution_backward_data_f32.cpp\n                              test_convolution_backward_weights_f32.cpp\n                              test_deconvolution.cpp\n                              test_binary.cpp\n                              test_matmul.cpp\n                              test_resampling.cpp\n                              test_reduction.cpp\n                              test_softmax.cpp\n                              test_concurrency.cpp\n                              test_layer_normalization.cpp\n                              test_lrn.cpp\n                              test_prelu.cpp\n                              test_group_normalization.cpp\n                              )\n```\n\n----------------------------------------\n\nTITLE: Defining and Adding Tests in CMake\nDESCRIPTION: This snippet retrieves the registered test suites and filters, and then adds each test with appropriate engine (CPU or GPU) based on DNNL configuration. Each test is added with a name, the binary to execute, the gtest filter, and the engine to use.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(test_suite_names GLOBAL PROPERTY GRAPH_UNIT_TEST_SUITES)\nget_property(test_filters GLOBAL PROPERTY GRAPH_UNIT_TEST_FILTERS)\nlist(LENGTH test_suite_names count)\nmath(EXPR count \"${count}-1\")\nset(gpu_rt_pattern \"(SYCL|DPCPP|OCL)\")\nforeach(i RANGE ${count})\n    list(GET test_suite_names ${i} s)\n    list(GET test_filters ${i} f)\n    if(${s} MATCHES \"_cpu\")\n        if(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n            add_test(${s} ${BINARY_NAME} --gtest_filter=${f} --engine=cpu)\n            maybe_configure_windows_test(${s} TEST)\n        endif()\n    else()\n        if(DNNL_GPU_RUNTIME MATCHES ${gpu_rt_pattern})\n            add_test(${s}_gpu ${BINARY_NAME} --gtest_filter=${f} --engine=gpu)\n            maybe_configure_windows_test(${s}_gpu TEST)\n        endif()\n\n        if(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n            add_test(${s}_cpu ${BINARY_NAME} --gtest_filter=${f} --engine=cpu)\n            maybe_configure_windows_test(${s}_cpu TEST)\n        endif()\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Flags for Source Files in CMake\nDESCRIPTION: This CMake command sets the `COMPILE_FLAGS` property for all files listed in the `SOURCES` variable.  It sets the optimization level to `-O3` and enables loop unrolling (`-funroll-loops`). These flags are specific to the compiler used and optimize the code for performance.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/s390x/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset_source_files_properties(${SOURCES}\n    PROPERTIES COMPILE_FLAGS \"-O3 -funroll-loops\")\n```\n\n----------------------------------------\n\nTITLE: Set C/C++ Standard\nDESCRIPTION: Sets the C and C++ standards to C99 and C++11, respectively, unless overridden by the user or if DNNL_WITH_SYCL is enabled. Also disables compiler extensions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(UNIX OR MINGW)\n    # CMAKE_<lang>_STANDARD support, so set it to our defaults, unless\n    # overridden by the user\n    if(NOT DEFINED CMAKE_C_STANDARD)\n        set(CMAKE_C_STANDARD 99)\n    endif()\n    if(NOT DEFINED CMAKE_CXX_STANDARD AND NOT DNNL_WITH_SYCL)\n        set(CMAKE_CXX_STANDARD 11)\n    endif()\n\n    # Disable -std=gnuXX and -std=gnu++XX\n    set(CMAKE_C_EXTENSIONS OFF)\n    set(CMAKE_CXX_EXTENSIONS OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library and Setting Properties in CMake\nDESCRIPTION: This snippet creates an object library named ${LIB_PACKAGE_NAME}_cpu_ppc64 from the source files listed in the SOURCES variable.  It also appends the target objects of this library to the DNNL_LIB_DEPS global property, making it available to other DNNL library components.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/ppc64/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_cpu_ppc64)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Globbing Test Sources in CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to collect all C++ source files matching the pattern 'test_*.cpp' in the current source directory. The found files are then stored in the `TEST_SOURCES` variable for further processing.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/api/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_*.cpp)\n```\n\n----------------------------------------\n\nTITLE: Running Inner Product with Input File\nDESCRIPTION: This command runs the Inner Product driver using a batch input file containing problem shapes. It utilizes default settings and processes all inner product configurations defined within the specified file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_ip.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --ip --batch=inputs/ip/shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: These snippets add the 'api' and 'unit' subdirectories to the build process. CMake will process the CMakeLists.txt files in these subdirectories to build the API and unit test components of the oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(api)\nadd_subdirectory(unit)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This snippet sets the include directory for the project. It specifies the directory where the header files are located, allowing the compiler to find them during the compilation process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/noexcept/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${PROJECT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Creating and Linking Executable in CMake\nDESCRIPTION: This snippet defines an executable target named `gpu_conv_planner` using CMake's `add_executable` command.  It compiles `planner_main.cpp` into the executable.  The `target_link_libraries` command then links the executable against the library specified by the CMake variable `${LIB_PACKAGE_NAME}`. The variable `${LIB_PACKAGE_NAME}` must be defined elsewhere in the CMake configuration.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/v2/conv/planner/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(gpu_conv_planner planner_main.cpp)\ntarget_link_libraries(gpu_conv_planner ${LIB_PACKAGE_NAME})\n```\n\n----------------------------------------\n\nTITLE: Tensor Abstraction Description\nDESCRIPTION: Describes a tensor with offsets, stored as IR expressions. Example shows a 32x32x1x1 tensor with [mb_idx, ic_idx, 0, 0] offsets.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/ir/README.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n`32x32x1x1` tensor with `[mb_idx, ic_idx, 0, 0]` offsets (activations for 2D\nconvolution: `N x C x H x W`).\n```\n\n----------------------------------------\n\nTITLE: Setting Global Property in CMake\nDESCRIPTION: This snippet appends the object files from the `OBJ_LIB` target to the global property `GRAPH_UNIT_TEST_DEPS`.  This makes the object files available to other parts of the build process that depend on these tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Registering Test Suites in CMake\nDESCRIPTION: This CMake `foreach` loop iterates through a list of test source files, extracts the filename, and constructs a test suite name using string manipulation. It then registers the test suite using a custom CMake function `register_graph_test_suite`. This function likely handles the actual registration and configuration of the test suite.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/sycl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(TEST_FILE ${TEST_INTERFACE_SYCL_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_unit_interface_\" test_suite_name ${file_name})\n    string(REPLACE \"test_\" \"test_interface_\" filter ${file_name})\n    register_graph_test_suite(${test_suite_name}_sycl \"${filter}*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Concat Problem Descriptor Example\nDESCRIPTION: This snippet demonstrates the format of a concat problem descriptor, specifying the dimensions of the input tensors. `N` represents an integer number defining the size of the dimension. The canonical form is NxNxNxNxN:NxNxNxNxN[:NxNxNxNxN...].\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_concat.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    NxNxNxNxN:NxNxNxNxN[:NxNxNxNxN...]\n```\n\n----------------------------------------\n\nTITLE: Example JIT Dump Output (GPU)\nDESCRIPTION: This shows example output files generated when JIT dump is enabled on Intel Processor Graphics Gen9. The files contain the JIT-compiled code for different GPU operations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/inspecting_jit.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ndnnl_dump_gpu_simple_reorder.0.bin\ndnnl_dump_gpu_gen9_conv_fwd.1.bin\n...\n```\n\n----------------------------------------\n\nTITLE: Registering Executable Tests in CMake\nDESCRIPTION: Iterates through the list of test source files and registers each one as an executable test. It extracts the executable name, and registers the target using `register_exe` passing the name, sources, and test type. The dependencies are set to include `dnnl_gtest` to link with the Google Test framework.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(TEST_FILE ${TEST_SOURCES})\n    get_filename_component(exe ${TEST_FILE} NAME_WE)\n    register_exe(${exe} \"${TEST_SOURCES};${MAIN_SRC_GTEST}\" \"test\" \"dnnl_gtest\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This snippet defines an object library named `${LIB_PACKAGE_NAME}_gpu_generic` using the `add_library` command. It sets the library type to `OBJECT` and uses the `SOURCES` variable (populated by `file(GLOB)`) as the source files. The snippet also appends this library to the `DNNL_LIB_DEPS` global property.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_generic)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Registering Tests Based on Coverage CMake\nDESCRIPTION: This section determines which tests to register based on the `DNNL_TEST_SET_COVERAGE` variable, which can be `DNNL_TEST_SET_SMOKE`, `DNNL_TEST_SET_CI`, or `DNNL_TEST_SET_NIGHTLY`. It collects input files for each driver, filters them, and then calls `register_all_tests` to register the appropriate tests for CPU and GPU engines. It handles various scenarios including gpu_ci inputs and exclusion of certain test files based on coverage type.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# The following section is responsible to register test_benchdnn_ targets\n# depending on DNNL_TEST_SET value.\nset(has_gpu false)\nif(NOT DNNL_GPU_RUNTIME STREQUAL \"NONE\")\n    set(has_gpu true)\nendif()\nset(has_cpu false)\nif(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    set(has_cpu true)\nendif()\n\n# Very sad CMake older then 3.2 does not support continue() command.\nfile(GLOB all_drivers RELATIVE ${CMAKE_CURRENT_SOURCE_DIR}/inputs inputs/*)\n\nif(NOT ONEDNN_BUILD_GRAPH)\n    list(REMOVE_ITEM all_drivers \"graph\")\nendif()\n\nforeach(driver ${all_drivers})\n    set(driver_dir ${CMAKE_CURRENT_SOURCE_DIR}/inputs/${driver})\n    # Collect input files in groups\n    file(GLOB test_files_large_cpu RELATIVE ${driver_dir}\n            inputs/${driver}/test_*_large$)\n    file(GLOB test_files_gpu_ci RELATIVE ${driver_dir}\n            inputs/${driver}/test_*_gpu_ci)\n    file(GLOB test_files_gpu RELATIVE ${driver_dir} inputs/${driver}/test_*_gpu)\n    file(GLOB test_files_ci RELATIVE ${driver_dir} inputs/${driver}/test_*_ci)\n    file(GLOB test_files_smoke RELATIVE ${driver_dir} inputs/${driver}/test_*_smoke)\n    file(GLOB test_files_cpu RELATIVE ${driver_dir} inputs/${driver}/test_*)\n    file(GLOB test_files_with_all RELATIVE ${driver_dir}\n        inputs/brgemm/test_brgemm_all inputs/conv/test_conv_all\n        inputs/graph/test_graph_all inputs/rnn/test_lstm_all)\n\n    if(DNNL_TEST_SET_COVERAGE EQUAL DNNL_TEST_SET_SMOKE)\n        if(has_gpu)\n            register_all_tests(gpu \"${driver}\" \"${test_files_smoke}\")\n        endif()\n        if(has_cpu)\n            register_all_tests(cpu \"${driver}\" \"${test_files_smoke}\")\n        endif()\n    elseif(DNNL_TEST_SET_COVERAGE EQUAL DNNL_TEST_SET_CI)\n        # gpu_ci files may happen if cpu coverage can not be used on gpu\n        # Filter out gpu_ci inputs from ci\n        foreach(test_file ${test_files_gpu_ci})\n            string(REPLACE \"${test_file}\" \"\" test_files_ci \"${test_files_ci}\")\n        endforeach()\n\n        # use gpu_ci if not empty\n        if(has_gpu)\n            if(test_files_gpu_ci)\n                register_all_tests(gpu \"${driver}\" \"${test_files_gpu_ci}\")\n            else()\n                register_all_tests(gpu \"${driver}\" \"${test_files_ci}\")\n            endif()\n        endif()\n        if(has_cpu)\n            register_all_tests(cpu \"${driver}\" \"${test_files_ci}\")\n        endif()\n    elseif(DNNL_TEST_SET_COVERAGE EQUAL DNNL_TEST_SET_NIGHTLY)\n        ## Filter out gpu, large cpu and invalid inputs from cpu\n        foreach(test_file ${test_files_large_cpu} ${test_files_gpu_ci}\n            ${test_files_gpu} ${test_files_ci} ${test_files_smoke})\n            string(REPLACE \"${test_file}\" \"\" test_files_cpu \"${test_files_cpu}\")\n        endforeach()\n        ## Then filter files with \"all\" suffix by exact name since there may\n        ## exist files like test_driver_all_something.\n        foreach(test_file ${test_files_with_all})\n            string(REPLACE \"${test_file};\" \"\" test_files_cpu \"${test_files_cpu}\")\n        endforeach()\n\n        if(has_cpu AND NOT DNNL_USE_CLANG_SANITIZER)\n            register_all_tests(cpu \"${driver}\" \"${test_files_large_cpu}\")\n        endif()\n        if(has_gpu)\n            register_all_tests(gpu \"${driver}\" \"${test_files_gpu}\")\n        endif()\n        if(has_cpu)\n            register_all_tests(cpu \"${driver}\" \"${test_files_cpu}\")\n        endif()\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Set Install Library Directory\nDESCRIPTION: Sets CMAKE_INSTALL_LIBDIR to \"lib\" when DNNL_INSTALL_MODE is \"BUNDLE\" and CMAKE_INSTALL_LIBDIR is not defined. This configures the installation directory for the library in bundle mode.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_INSTALL_MODE STREQUAL \"BUNDLE\" AND NOT DEFINED CMAKE_INSTALL_LIBDIR)\n    # define CMAKE_INSTALL_LIBDIR as \"lib\" in the case of bundle\n    set(CMAKE_INSTALL_LIBDIR \"lib\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up object library name in CMake\nDESCRIPTION: This snippet sets the name of the object library to `graph_unit_test_utils`. This variable is later used to create the actual library using `add_library` command.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB graph_unit_test_utils)\n```\n\n----------------------------------------\n\nTITLE: Appending Additional Test Sources in CMake\nDESCRIPTION: This command appends the contents of the variable `MAIN_SRC_GTEST` to the `TEST_SOURCES` list. This is likely used to include common testing utilities or main function implementations required for Google Test.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/regression/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND TEST_SOURCES ${MAIN_SRC_GTEST})\n```\n\n----------------------------------------\n\nTITLE: Run Reduction Benchmarks from File\nDESCRIPTION: This command executes a set of reduction primitive benchmarks using problem definitions specified in the `inputs/reduction/shapes_ci` file.  It uses the default benchmark settings.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reduction.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --reduction --batch=inputs/reduction/shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Running Group Normalization Benchmark\nDESCRIPTION: Executes the group normalization benchmark with specified options. The general syntax is `.\\benchdnn --gnorm [benchdnn-knobs] [gnorm-knobs] [gnorm-desc] ...`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_gnorm.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --gnorm [benchdnn-knobs] [gnorm-knobs] [gnorm-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Appending to a Global Property in CMake\nDESCRIPTION: This snippet appends the object library target `${OBJ_LIB}` to the `DNNL_LIB_DEPS` global property. This is likely used to manage dependencies for the overall oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/xbyak_aarch64/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Include Custom CMake Modules\nDESCRIPTION: Appends the project's cmake directory to CMAKE_MODULE_PATH to enable finding custom CMake modules.  Includes various custom CMake modules for compatibility, utilities, options, SYCL support, OpenMP, TBB, Threadpool, OpenCL, platform specific settings, SDL, ACL, BLAS, documentation, versioning, coverage, build types, testing, host compiler specifics, and configuring primitive lists.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\")\n\nset(CMAKE_SRC_CCXX_FLAGS)       # SRC specifics\nset(CMAKE_EXAMPLE_CCXX_FLAGS)   # EXAMPLE specifics\nset(CMAKE_TEST_CCXX_FLAGS)      # TESTS specifics\n\nstring(TOUPPER \"${CMAKE_BUILD_TYPE}\" UPPERCASE_CMAKE_BUILD_TYPE)\n\nset(LIB_PACKAGE_NAME \"dnnl\")\n\ninclude(\"cmake/dnnl_compat.cmake\")\n\ninclude(\"cmake/utils.cmake\")\ninclude(\"cmake/options.cmake\")\ninclude(\"cmake/SYCL.cmake\")\ninclude(\"cmake/OpenMP.cmake\")\ninclude(\"cmake/TBB.cmake\")\ninclude(\"cmake/Threadpool.cmake\")\ninclude(\"cmake/OpenCL.cmake\")\ninclude(\"cmake/platform.cmake\")\ninclude(\"cmake/SDL.cmake\")\ninclude(\"cmake/ACL.cmake\")\ninclude(\"cmake/blas.cmake\")\ninclude(\"cmake/doc.cmake\")\ninclude(\"cmake/version.cmake\")\ninclude(\"cmake/coverage.cmake\")\ninclude(\"cmake/build_types.cmake\")\ninclude(\"cmake/testing.cmake\")\ninclude(\"cmake/host_compiler.cmake\")\ninclude(\"cmake/configuring_primitive_list.cmake\")\n```\n\n----------------------------------------\n\nTITLE: Compute u8/s8 dot product with potential saturation (AVX2/AVX-512)\nDESCRIPTION: This C++ snippet demonstrates how to compute the dot product of u8 and s8 vectors using the VPMADDUBSW, VPMADDWD, and VPADDD instruction sequence on CPUs with Intel AVX2 or AVX-512 support. It highlights the potential for intermediate saturation during the VPMADDUBSW step and the importance of proper quantization to avoid inaccuracies.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/int8_computations.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// Want to compute:\n// c_s32 += sum{i=0..3}(a_u8[i] * b_s8[i])\nint32_t u8s8s32_compute_avx512(\n        uint8_t a_u8[4], int8_t b_s8[4], int32_t c_s32) {\n\n    // Compute using VPMADDUBSW, VPMADDWD, VPADDD:\n    int16_t ab_s16[4];\n    for (int i = 0; i < 4; ++i)\n        ab_s16[i] = (int16_t)a_u8[i] * (int16_t)b_s8[i]; // Exact computations\n\n    int16_t VPMADDUBSW_res[2];\n    VPMADDUBSW_res[0] = saturate_to_s16(ab_s16[0] + ab_s16[1]);  // CAUTION: POTENTIAL OVERFLOW / SATURATION\n    VPMADDUBSW_res[1] = saturate_to_s16(ab_s16[2] + ab_s16[3]);  // CAUTION: POTENTIAL OVERFLOW / SATURATION\n\n    c_s32 +=\n        (int32_t)VPMADDUBSW_res[0] +\n        (int32_t)VPMADDUBSW_res[1];\n\n    return c_s32;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Test Sources (CMake)\nDESCRIPTION: This snippet defines the test sources using `file(GLOB)` to find all `test_*.cpp` files in the current source directory and appends the Google Test main source file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/internals/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_*.cpp)\nlist(APPEND TEST_SOURCES ${MAIN_SRC_GTEST})\n```\n\n----------------------------------------\n\nTITLE: Registering graph test suites (Engine Independent)\nDESCRIPTION: This snippet iterates through each source file in `TEST_UTILS_ENGINE_INDEPENDENT_SOURCES`. It constructs a test suite name and a filter string based on the file name (removing \"_cpu\" suffix). It then registers the test suite using a `register_graph_test_suite` function, appending \"_cpu\" to the test suite name.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${TEST_UTILS_ENGINE_INDEPENDENT_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"_cpu\" \"\" file_name ${file_name})\n    string(REPLACE \"test_\" \"test_graph_unit_utils_\" test_suite_name ${file_name})\n    string(REPLACE \"test_\" \"test_utils_\" filter ${file_name})\n    register_graph_test_suite(${test_suite_name}_cpu \"${filter}*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: BRGeMM Problem Descriptor\nDESCRIPTION: This snippet defines the canonical form of the BRGeMM problem descriptor. It represents the dimensions of the source and weights matrices, separated by a colon. `M`, `K`, and `N` are the inner dimensions for the matrix multiplication.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_brgemm.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    MxK:KxN[_nS]\n```\n\n----------------------------------------\n\nTITLE: Conditional Source File Removal\nDESCRIPTION: Conditionally removes source files related to the convolution planner based on the DNNL_EXPERIMENTAL flag. If DNNL_EXPERIMENTAL is not set, the planner source files are removed from the SOURCES list.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_EXPERIMENTAL)\n    list(REMOVE_ITEM SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/v2/conv/planner/planner_main.cpp\")\nelse()\n    file(GLOB planner_sources\n        ${CMAKE_CURRENT_SOURCE_DIR}/v2/conv/planner/*.hpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/v2/conv/planner/*.cpp\n        )\n    foreach(s ${planner_sources})\n        list(REMOVE_ITEM SOURCES \"${s}\")\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This CMake code snippet uses the `file(GLOB_RECURSE)` command to recursively collect all source files with the extensions .h, .hpp, .c, and .cpp from the current source directory.  It populates the SOURCES variable with the paths to these files. This simplifies the process of adding source files to the project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Inclusion in CMake\nDESCRIPTION: This CMake script checks the value of the `ONEDNN_BUILD_GRAPH` variable. If it's true, it includes the `interface`, `backend`, and `utils` subdirectories in the build process using the `add_subdirectory` command. This allows for selectively building graph-related components of oneDNN.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (ONEDNN_BUILD_GRAPH)\n    add_subdirectory(interface)\n    add_subdirectory(backend)\n    add_subdirectory(utils)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Flags for MSVC Big Object Support\nDESCRIPTION: This snippet checks if the compiler is MSVC. If it is, it sets the `/bigobj` compile flag for specific source files (`dnnl_backend.cpp`). This is necessary for large object files in MSVC.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC AND (CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\"))\n    file(GLOB FILES_REQUIRED_BIGOBJ\n        ${CMAKE_CURRENT_SOURCE_DIR}/dnnl_backend.cpp\n        )\n    set_source_files_properties(${FILES_REQUIRED_BIGOBJ}\n        PROPERTIES COMPILE_FLAGS \"/bigobj\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Include Directories with Host Compiler\nDESCRIPTION: This snippet includes specified directories for the host compiler. It is crucial for resolving dependencies during compilation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/gtest/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories_with_host_compiler(${CMAKE_CURRENT_SOURCE_DIR}\n                                       ${CMAKE_CURRENT_SOURCE_DIR}/..\n)\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Graph Driver Correctness Testing\nDESCRIPTION: This example demonstrates how to perform correctness testing using `--mode=C` when running benchdnn with the graph driver.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_graph.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./benchdnn --mode=C --graph --case=op/f32/conv_2d.json\n```\n\n----------------------------------------\n\nTITLE: Removing Architecture-Specific Tests (CMake)\nDESCRIPTION: This snippet conditionally removes x64-specific tests if the target architecture is not x64 or if the CPU runtime is set to NONE.  It ensures that tests appropriate for the target architecture are run.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/internals/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DNNL_TARGET_ARCH STREQUAL \"X64\" OR DNNL_CPU_RUNTIME STREQUAL \"NONE\")\n    list(REMOVE_ITEM TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_brgemm.cpp)\n    list(REMOVE_ITEM TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_float8.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Test Executable Name in CMake\nDESCRIPTION: This CMake command sets the name of the test executable to `test_regression`. This variable is later used when registering the test executable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/regression/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_EXE test_regression)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Variables\nDESCRIPTION: Sets CMake variables for clang-tidy and directories to be included during compilation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_CLANG_TIDY \"\")\n\nset(DIRS \"codegen;config;conv;ir;pass;pooling;reorder;utils;v2\")\n```\n\n----------------------------------------\n\nTITLE: Conditional Threadpool File Removal CMake\nDESCRIPTION: This snippet removes threadpool scheduler files if the `DNNL_CPU_RUNTIME` is not set to `THREADPOOL`. It appends threadpool files to `ACL_THREADPOOL_FILES` then removes them from the `SOURCES` list using `list(REMOVE_ITEM)`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/aarch64/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# If the runtime is not THREADPOOL remove threadpool_scheduler sources.\nif(NOT DNNL_CPU_RUNTIME STREQUAL \"THREADPOOL\")\n    list(APPEND ACL_THREADPOOL_FILES\n        ${CMAKE_CURRENT_SOURCE_DIR}/acl_threadpool_scheduler.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/acl_threadpool_scheduler.hpp\n    )\n    list(REMOVE_ITEM SOURCES ${ACL_THREADPOOL_FILES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Looping Scheme\nDESCRIPTION: This code shows a generic looping scheme that handles mapping the outer loop nest to the kernel grid, along with the thread group size. Includes loops that are mapped to the kernel grid (m_tg_idx, n_tg_idx, k_tg_idx) and loops inside a thread (k_idx).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/conv/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfor m_tg_idx in range(0, m, m_tg_blk):         # Mapped to the kernel grid\n    for n_tg_idx in range(0, n, n_tg_blk):     # Mapped to the kernel grid\n        for k_tg_idx in range(0, k, k_tg_blk): # Mapped to the kernel grid\n            ...\n            for k_idx in range(k_tg_idx, k_tg_idx + k_tg_blk, k_blk): # Loop inside thread\n                ...\n               # Perform C += A * B multiplication cooperatively by a thread group\n               # A is (m_tg_blk x    k_blk)\n               # B is (   k_blk x n_tg_blk)\n               # C is (m_tg_blk x n_tg_blk)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: This snippet adds the 'fake' subdirectory to the build process. CMake will look for a CMakeLists.txt file inside the 'fake' directory and process it to include the 'fake' component in the build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fake)\n```\n\n----------------------------------------\n\nTITLE: Registering Engine Dependent Test Suites in CMake\nDESCRIPTION: This snippet iterates through the engine-dependent source files, extracts the filename, constructs the test suite name, and registers the test suite using `register_graph_test_suite`.  It replaces \"test_\" with \"test_graph_unit_interface_\" to create the test suite name and creates a filter.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(TEST_FILE ${TEST_INTERFACE_ENGINE_DEPENDENT_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_unit_interface_\" test_suite_name ${file_name})\n    string(REPLACE \"test_\" \"test_interface_\" filter ${file_name})\n    register_graph_test_suite(${test_suite_name} \"${filter}.*:${filter}/*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all source files (`.c`, `.cpp`, `.ch`, and `.hpp`) in the current source directory. The resulting list of files is stored in the `SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories in CMake\nDESCRIPTION: This snippet adds subdirectories 'interface', 'backend', and 'utils' to the build process. These directories likely contain related source code for the graph component.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(interface)\nadd_subdirectory(backend)\nadd_subdirectory(utils)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory in CMake\nDESCRIPTION: Adds a subdirectory named 'api' to the current build process. This enables CMake to process the CMakeLists.txt file located within the 'api' directory, allowing it to build the components defined there. It is crucial for incorporating the API-related parts of the oneDNN library into the overall build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(api)\n```\n\n----------------------------------------\n\nTITLE: Including Directories with CMake\nDESCRIPTION: This snippet adds the specified directory to the include search path for the compiler. This allows the compiler to find header files necessary for building the test executable. The path is relative to the current source directory, pointing to the src directory of the oneDNN project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/api/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/../../../../src)\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows Execution Environment CMake\nDESCRIPTION: This snippet configures the execution environment on Windows, by modifying the PATH and creating a batch file to run the tests with the correct environment variables.  It sets the PATH by replacing semicolons with escaped semicolons and prepending the CTESTCONFIG_PATH and environment PATH variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32 AND (NOT DNNL_BUILD_FOR_CI))\n    string(REPLACE  \";\" \"\\;\" PATH \"${CTESTCONFIG_PATH};$ENV{PATH}\")\n    configure_file(\n        \"${PROJECT_SOURCE_DIR}/cmake/run_with_env.bat.in\"\n        \"${PROJECT_BINARY_DIR}/run_with_env.bat\"\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Engine-Independent API Test Sources in CMake\nDESCRIPTION: This CMake code uses `FILE(GLOB)` command to collect all engine-independent API test source files. It stores the result in `API_TEST_ENGINE_INDEPENDENT_SOURCES` variable. The globbing includes various C and C++ API test files such as `test_c_api_add_op.cpp`, `test_c_api_constant_cache.cpp` etc.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nFILE(GLOB API_TEST_ENGINE_INDEPENDENT_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_add_op.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_constant_cache.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_filter.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_graph.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_logical_tensor.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_op.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_constant_cache.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_engine.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_graph.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_logical_tensor.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_op.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_tensor.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: benchdnn Summary Usage\nDESCRIPTION: Shows the general syntax for using the `--summary` option in benchdnn to enable or disable specific settings.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knob_summary.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n    --summary=[no-]SETTING1[+[no-]SETTING2...]\n```\n\n----------------------------------------\n\nTITLE: USM-based or buffer-based memory object identification\nDESCRIPTION: To identify whether a memory object is USM-based or buffer-based, dnnl::ocl_interop::get_memory_kind() query can be used.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_9\n\n\n\n----------------------------------------\n\nTITLE: Conditional GeMM Kernel ISA Exclusion\nDESCRIPTION: These snippets conditionally remove GeMM kernel files based on the value of the `ONEDNN_ENABLE_GEMM_KERNELS_ISA` variable. They use `file(GLOB_RECURSE)` to find files matching specific ISA patterns (AMX, AVX512, AVX, SSE41) and `list(REMOVE_ITEM)` to remove them from the `SOURCES` variable. The `foreach` loop iterates over the found files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/x64/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ONEDNN_ENABLE_GEMM_KERNELS_ISA MATCHES \"^(AVX512|AVX2|SSE41|NONE)$\")\n    file(GLOB_RECURSE SOURCES_AMX ${CMAKE_CURRENT_SOURCE_DIR}/gemm/jit*amx*)\n    foreach(amx_file ${SOURCES_AMX})\n        list(REMOVE_ITEM SOURCES \"${amx_file}\")\n    endforeach()\nendif()\n\nif(ONEDNN_ENABLE_GEMM_KERNELS_ISA MATCHES \"^(AVX2|SSE41|NONE)$\")\n    file(GLOB_RECURSE SOURCES_AVX512 ${CMAKE_CURRENT_SOURCE_DIR}/gemm/jit*avx512*)\n    foreach(avx512_file ${SOURCES_AVX512})\n        list(REMOVE_ITEM SOURCES \"${avx512_file}\")\n    endforeach()\nendif()\n\nif(ONEDNN_ENABLE_GEMM_KERNELS_ISA MATCHES \"^(SSE41|NONE)$\")\n    file(GLOB_RECURSE SOURCES_AVX ${CMAKE_CURRENT_SOURCE_DIR}/gemm/jit*avx*)\n    foreach(avx_file ${SOURCES_AVX})\n        list(REMOVE_ITEM SOURCES \"${avx_file}\")\n    endforeach()\nendif()\n\nif(ONEDNN_ENABLE_GEMM_KERNELS_ISA MATCHES \"^(NONE)$\")\n    file(GLOB_RECURSE SOURCES_SSE41 ${CMAKE_CURRENT_SOURCE_DIR}/gemm/*)\n    foreach(sse41_file ${SOURCES_SSE41})\n        list(REMOVE_ITEM SOURCES \"${sse41_file}\")\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specific Softmax Problem Execution\nDESCRIPTION: This command executes a specific softmax problem with specified forward propagation, plain memory layouts, f32 data types, out-of-place memory mode, and an axis size of 1000. It demonstrates how to set specific parameters for a softmax benchmark.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_softmax.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --softmax --dir=FWD_D --sdt=f32 --ddt=f32 --stag=nc \\\n               --inplace=false --axis=1 256x1000\n```\n\n----------------------------------------\n\nTITLE: Load/Store Functions for NCW/OIW/NCW Layouts\nDESCRIPTION: These functions implement the load/store operations for `ncw`, `oiw`, and `ncw` layouts in a 1D convolution. The `load_src` function includes padding handling by returning 0 for out-of-bounds indices. The functions calculate the memory offset based on the input parameters.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/conv/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef load_src(mb, ic, ow, kw):\n    iw = ow * SW + kw * (DW + 1) - PW\n    if iw < 0 or iw >= IW:\n        return 0\n    off = 0\n    off += mb * IC * IW\n    off += ic * IW\n    off += iw\n    return src[off]\n\ndef load_wei(oc, ic, kw):\n    off = 0\n    off += oc * IC * KW\n    off += ic * KW\n    off += kw\n    return wei[off]\n\ndef store_dst(mb, oc, ow, val):\n    off = 0\n    off += mb * OC * OW\n    off += oc * OW\n    off += ow\n    dst[off] = val\n```\n\n----------------------------------------\n\nTITLE: Globbing Engine Independent Sources in CMake\nDESCRIPTION: This snippet uses `file(GLOB)` to find all source files ending in `.cpp` in the current source directory that are engine independent. These source files are added to the `TEST_INTERFACE_ENGINE_INDEPENDENT_SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB TEST_INTERFACE_ENGINE_INDEPENDENT_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_backend_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_logical_tensor_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_op_schema_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_op_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_shape_infer_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_op_def_constraint_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_value_cpu.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Generalized Convolution Algorithm (GEMM-like)\nDESCRIPTION: This code snippet illustrates a generalized convolution algorithm expressed in a GEMM-like form.  It shows the nested loops and the core computation `c_val += a_val * b_val`, highlighting the similarity between convolution and GEMM operations. The variables `i0`, `j0`, and `k0` represent the M, N, and K dimensions, respectively.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/conv/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfor i0 in range(0, I0):\n    ...\n    for j0 in range(0, J0):\n        ...\n        c_val = 0\n        for k0 in range(0, K0):\n            ...\n            a_val = load_A(i0, ..., k0)\n            b_val = load_B(k0, ..., j0)\n            c_val += a_val * b_val\n        store_C(i0, ..., j0, ..., c_val)\n```\n\n----------------------------------------\n\nTITLE: Define Project Properties\nDESCRIPTION: Defines the project name, full name, and version. These properties are used throughout the build process and for generating package configuration files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(PROJECT_NAME \"oneDNN\")\nset(PROJECT_FULL_NAME \"oneAPI Deep Neural Network Library (oneDNN)\")\nset(PROJECT_VERSION \"3.9.0\")\n\nproject(${PROJECT_NAME} VERSION \"${PROJECT_VERSION}\" LANGUAGES C CXX)\n```\n\n----------------------------------------\n\nTITLE: Conditional ACL File Removal CMake\nDESCRIPTION: This snippet conditionally removes ACL (Arm Compute Library) related files from the `SOURCES` variable if `DNNL_AARCH64_USE_ACL` is not enabled. It uses `file(GLOB_RECURSE)` to find ACL files and `list(REMOVE_ITEM)` to remove them from the `SOURCES` list.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/aarch64/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DNNL_AARCH64_USE_ACL)\n    file(GLOB_RECURSE ACL_FILES\n        ${CMAKE_CURRENT_SOURCE_DIR}/acl_*.[ch]\n        ${CMAKE_CURRENT_SOURCE_DIR}/acl_*.[ch]pp\n        )\n    list(REMOVE_ITEM SOURCES ${ACL_FILES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Registering Executable with CMake\nDESCRIPTION: This snippet calls a custom CMake function 'register_exe' to register the test executable for building and testing. It passes the executable name (TEST_EXE), the list of source files (TEST_SOURCES), the test type ('test'), and the dependency library ('dnnl_gtest').\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/api/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nregister_exe(${TEST_EXE} \"${TEST_SOURCES}\" \"test\" \"dnnl_gtest\")\n```\n\n----------------------------------------\n\nTITLE: Setting a global property to append object files\nDESCRIPTION: This snippet sets a global property `GRAPH_UNIT_TEST_DEPS` and appends the object files associated with the `${OBJ_LIB}` (graph_unit_test_utils) library to it. This likely helps with dependency management in a larger build system.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_DEPS $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Supported Data Types Table for Nvidia Backend\nDESCRIPTION: This table summarizes the supported data types for different computation modes in the oneDNN Nvidia backend. It indicates which data types (f32, f16, s8, bf16) are supported for training and inference.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/nvidia/README.md#_snippet_1\n\nLANGUAGE: none\nCODE:\n```\n| Data Type | Computation Mode            |\n|-----------|-----------------------------|\n| f32       | Training, Inference         |\n| f16       | Inference                   |\n| s8        | Inference (when applicable) |\n| bf16      | Training, Inference         |\n```\n\n----------------------------------------\n\nTITLE: Checking Compiler Support for Alias Attributes (C++)\nDESCRIPTION: This snippet checks if the compiler supports the `warn_unused_result` attribute on type aliases. It defines a minimal C++ source file that uses this attribute and then uses `check_cxx_source_compiles` to determine if the compiler can compile it without errors. The result is stored in the `COMPILER_ALLOWS_ALIAS_ATTRIBUTES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(COMPILER_ALLOWS_ALIAS_ATTRIBUTES_SOURCE\n\"\n    typedef enum { dnnl_status_success = 0, } dnnl_status_t;\n    using status_t __attribute__((warn_unused_result)) = dnnl_status_t;\n    int main() { return 0; }\n\")\ninclude(CheckCXXSourceCompiles)\nset(CMAKE_REQUIRED_FLAGS \"-Werror\")\ncheck_cxx_source_compiles(\"${COMPILER_ALLOWS_ALIAS_ATTRIBUTES_SOURCE}\" COMPILER_ALLOWS_ALIAS_ATTRIBUTES)\nunset(CMAKE_REQUIRED_FLAGS)\n```\n\n----------------------------------------\n\nTITLE: Run Zeropad Set from Input File\nDESCRIPTION: This command executes the benchdnn zero pad driver using a set of problems defined in the specified input file. It uses the default settings for the driver.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_zeropad.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --zeropad --batch=inputs/zeropad/test_zeropad_ci\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library with CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_gpu_microkernels` from the source files collected in the `SOURCES` variable. The `add_library` command is used with the `OBJECT` keyword to create the object library. The `LIB_PACKAGE_NAME` variable is expected to be defined elsewhere.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/microkernels/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_microkernels)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Registering Graph Unit Test Suite\nDESCRIPTION: This CMake code snippet registers a graph unit test suite named 'test_graph_unit_fake' using a custom CMake function named 'register_graph_test_suite'. The test suite uses a pattern 'test_fake_*' to discover relevant tests. The assumption is that 'register_graph_test_suite' is defined elsewhere, likely in another CMake file included by this one.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/fake/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nregister_graph_test_suite(\"test_graph_unit_fake\" \"test_fake_*\")\n```\n\n----------------------------------------\n\nTITLE: Setting Global Property in CMake\nDESCRIPTION: This CMake command appends the object files from the specified object library to a global property. This allows other parts of the build system to easily depend on the object files generated by this library. The `GRAPH_UNIT_TEST_DEPS` property is used to store dependencies for graph unit tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/sycl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Formatting code with clang-format (SH)\nDESCRIPTION: This command formats the code using the `_clang_format` file found in the oneDNN top level directory, ensuring code style consistency when contributing to the project. It's executed before submitting a contribution.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CODING_STANDARDS.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nclang-format -style=file -i foo.cpp\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory CMake\nDESCRIPTION: This snippet adds a subdirectory containing xbyak_aarch64 to the build process.  It includes the CMakeLists.txt file located in `${PROJECT_SOURCE_DIR}/third_party/xbyak_aarch64` to integrate the xbyak_aarch64 library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/aarch64/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(${PROJECT_SOURCE_DIR}/third_party/xbyak_aarch64 xbyak_aarch64)\n```\n\n----------------------------------------\n\nTITLE: Setting Runtime Patterns in CMake\nDESCRIPTION: This CMake code snippet sets variables defining runtime patterns. `gpu_rt_pattern` is set to match SYCL, DPCPP, or OCL, indicating a GPU runtime. `sycl_rt_pattern` is set to match SYCL or DPCPP, specifically targeting SYCL runtimes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(gpu_rt_pattern \"(SYCL|DPCPP|OCL)\")\nset(sycl_rt_pattern \"(SYCL|DPCPP)\")\n```\n\n----------------------------------------\n\nTITLE: Finding Common Unit Tests in CMake\nDESCRIPTION: This snippet finds all `.cpp` files in the current source directory and stores them in the `COMMON_UNIT_TESTS` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB COMMON_UNIT_TESTS\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Linking oneDNN on Linux/macOS using g++/clang++/icpx\nDESCRIPTION: These commands demonstrate how to compile and link a C++ application with the oneDNN library on Linux and macOS using g++, clang++, and icpx. It specifies the include and library directories using the DNNLROOT environment variable and links against the dnnl library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/link.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ng++ -I${DNNLROOT}/include -L${DNNLROOT}/lib getting_started.cpp -ldnnl\nclang++ -I${DNNLROOT}/include -L${DNNLROOT}/lib getting_started.cpp -ldnnl\nicpx -I${DNNLROOT}/include -L${DNNLROOT}/lib getting_started.cpp -ldnnl\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to find all C and C++ source files within the specified directory and its subdirectories. The resulting list of files is stored in the `SOURCES` variable, which is later used to define the library's source files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/x64/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Linking oneDNN on Windows using icx/cl\nDESCRIPTION: These commands demonstrate how to compile and link a C++ application with the oneDNN library on Windows using the Intel C++ Compiler (icx) and Microsoft Visual C++ Compiler (cl). It specifies the include directory and the library file, utilizing the DNNLROOT environment variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/link.md#_snippet_3\n\nLANGUAGE: bat\nCODE:\n```\nicx /EHa /I\"%DNNLROOT%\\include\" getting_started.cpp \"%DNNLROOT%\\lib\\dnnl.lib\"\ncl /EHa /I\"%DNNLROOT%\\include\" getting_started.cpp \"%DNNLROOT%\\lib\\dnnl.lib\"\n```\n\n----------------------------------------\n\nTITLE: Resampling Benchmark with Specific Options\nDESCRIPTION: This command executes a resampling benchmark with specific configurations. It sets the source and destination data types to f32 (single precision), uses blocked memory layouts with channel blocking sizes of 8 and 16, iterates through both forward and backward propagation kinds, uses both nearest and linear resampling algorithms, and overrides the minibatch size to 0 and 5. The problem descriptor is set to mb96ic768_ih17oh34.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_resampling.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --resampling --sdt=f32 --ddt=f32 --tag=nChw8c,nChw16c \\\n               --dir=FWD_D,BWD_D --alg=nearest,linear --mb=0,5 \\\n               mb96ic768_ih17oh34\n```\n\n----------------------------------------\n\nTITLE: Adding Object Library\nDESCRIPTION: This snippet creates an object library named`${LIB_PACKAGE_NAME}_cpu` from the source files defined in the `SOURCES` variable. It then appends the object library to the global property `DNNL_LIB_DEPS`. This allows the object files to be linked into other libraries or executables.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_cpu)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Registering the Main Test Executable (CMake)\nDESCRIPTION: This snippet registers the main test executable with the defined test sources. It uses the `register_exe` macro to create the test target.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/internals/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nregister_exe(${TEST_EXE} \"${TEST_SOURCES}\" \"test\" \"dnnl_gtest\")\n```\n\n----------------------------------------\n\nTITLE: Setting Global Properties for Graph Unit Test Dependencies\nDESCRIPTION: This CMake code snippet sets a global property named 'GRAPH_UNIT_TEST_DEPS' and appends the object library 'graph_unit_test_fake_backend' to it. This ensures that other test suites can link against these object files. The $<TARGET_OBJECTS:${OBJ_LIB}> generator expression is used to obtain the object files associated with the target.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/fake/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY GRAPH_UNIT_TEST_DEPS $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Add Subdirectories\nDESCRIPTION: Adds subdirectories for source code, examples, and tests to the build process.  This allows CMake to process the CMakeLists.txt files in those subdirectories.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(src)\nadd_subdirectory(examples)\nadd_subdirectory(tests)\n```\n\n----------------------------------------\n\nTITLE: Conditional Subdirectory Addition\nDESCRIPTION: Conditionally adds the `v2/conv/planner` subdirectory based on the DNNL_EXPERIMENTAL flag. If set, the subdirectory is included in the build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_EXPERIMENTAL)\n    add_subdirectory(v2/conv/planner)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Engine-Dependent API Test Sources in CMake\nDESCRIPTION: This CMake code uses `FILE(GLOB)` command to collect engine-dependent API test source files. It stores the result in `API_TEST_ENGINE_DEPENDENT_SOURCES` variable. The globbing includes C and C++ API test files related to compilation and partitioning.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nFILE(GLOB API_TEST_ENGINE_DEPENDENT_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_compile_parametrized.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_c_api_compile.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_compile.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_cpp_api_partition.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Forcing CPU runtime and disabling GPU runtime\nDESCRIPTION: This snippet forces the use of the sequential CPU runtime (DNNL_CPU_RUNTIME) and disables the GPU runtime (DNNL_GPU_RUNTIME) for the oneDNN library. It sets CMake cache variables to control the runtime selection. This is likely done for testing or specific deployment scenarios.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/other/subproject/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# force CPU runtime\nset(DNNL_CPU_RUNTIME \"SEQ\" CACHE STRING \"\" FORCE)\n\n# force GPU runtime\nset(DNNL_GPU_RUNTIME \"NONE\" CACHE STRING \"\" FORCE)\n```\n\n----------------------------------------\n\nTITLE: Installing oneDNN (sh)\nDESCRIPTION: This command installs the oneDNN library, headers, and documentation to the specified install directory. It uses the `install` target of the CMake build system. The install directory is controlled by `CMAKE_INSTALL_PREFIX`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Setting compile flags for specific test files on Windows with SYCL\nDESCRIPTION: This snippet sets specific compile flags (-O1) for the test_iface_runtime_dims.cpp file when building on Windows with DNNL_WITH_SYCL enabled. This is likely a workaround for issues related to higher optimization levels and access violations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32 AND DNNL_WITH_SYCL)\n    set_source_files_properties(\n            test_iface_runtime_dims.cpp\n            PROPERTIES COMPILE_FLAGS \"-O1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Tests (Shell)\nDESCRIPTION: Command to execute tests selected by the build configuration. It leverages `ctest` to run the test suite and validate code changes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nctest\n```\n\n----------------------------------------\n\nTITLE: Enabling/Disabling GPU Reference Kernels with CMake\nDESCRIPTION: This snippet checks the value of the `DNNL_DISABLE_GPU_REF_KERNELS` variable. If it's set, a message is displayed indicating that GPU compute reference kernels are disabled. Otherwise, a message is displayed indicating that they are enabled. This allows users to control whether reference kernels are included in the build.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_DISABLE_GPU_REF_KERNELS)\n    message(STATUS \"GPU Compute reference kernels are disabled\")\nelse()\n    message(STATUS \"GPU Compute reference kernels are enabled\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory with CMake\nDESCRIPTION: This CMake command adds the 'api' subdirectory to the current project. It allows for better project organization and modularity by including the API's source files and definitions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(api)\n```\n\n----------------------------------------\n\nTITLE: Define Target Name and Main Source File\nDESCRIPTION: This snippet sets the target name to 'dnnl_gtest' and defines the main source file for the library, associating it with a source group.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/gtest/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME dnnl_gtest)\nset(MAIN_SRC \"src/gtest-all.cc\")\nsource_group(\"\" FILES ${MAIN_SRC})\n```\n\n----------------------------------------\n\nTITLE: Enabling DNNL_STATUS_NODISCARD for RNN Files\nDESCRIPTION: This snippet checks if the compiler allows the `warn_unused_result` attribute on type aliases. If it does, it finds all C++ files in the rnn directory and sets the `COMPILE_FLAGS` property to `-DDNNL_STATUS_NODISCARD` for those files. This ensures that the return status of certain functions must be checked to prevent potential errors.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_WERROR)\n    # check if warn_unused_result can be used in an alias\n    # only try with DNNL_WERROR enabled to avoid spurious warnings\n    set(COMPILER_ALLOWS_ALIAS_ATTRIBUTES_SOURCE\n    \"\n        typedef enum { dnnl_status_success = 0, } dnnl_status_t;\n\n        using status_t __attribute__((warn_unused_result)) = dnnl_status_t;\n\n        int main() { return 0; }\n    \")\n    include(CheckCXXSourceCompiles)\n    check_cxx_source_compiles(\"${COMPILER_ALLOWS_ALIAS_ATTRIBUTES_SOURCE}\" COMPILER_ALLOWS_ALIAS_ATTRIBUTES)\n\n    if(${COMPILER_ALLOWS_ALIAS_ATTRIBUTES})\n        file(GLOB FILES_WITH_STATUS_NODISCARD\n            ${CMAKE_CURRENT_SOURCE_DIR}/rnn/*.cpp\n            )\n        set_source_files_properties(${FILES_WITH_STATUS_NODISCARD}\n            PROPERTIES COMPILE_FLAGS \"-DDNNL_STATUS_NODISCARD\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags (Windows/DPC++)\nDESCRIPTION: Sets compiler flags for DPC++ on Windows in RelWithMdd configuration to workaround issues with unordered_map.  It glob searches for all cpp files and then sets the optimization flag to /Ob1 (inline only functions suitable for debugging).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nstring(TOUPPER \"${CMAKE_BUILD_TYPE}\" UPPERCASE_CMAKE_BUILD_TYPE)\nif(WIN32 AND UPPERCASE_CMAKE_BUILD_TYPE STREQUAL \"RELWITHMDD\" AND CMAKE_BASE_NAME MATCHES \"(icx|icpx)\")\n    file(GLOB_RECURSE FILES_LIMIT_INLINE ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp)\n    set_source_files_properties(${FILES_LIMIT_INLINE} PROPERTIES COMPILE_FLAGS \"/Ob1\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Common Test Sources CMake\nDESCRIPTION: This code uses `FILE(GLOB)` to find all C++ source files in the current source directory that match the specified patterns (e.g., `test_dnnl_infer_shape_cpu.cpp`). These files are stored in the `DNNL_COMMON_TEST_SOURCES` variable, which is later used to compile the tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nFILE(GLOB DNNL_COMMON_TEST_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_dnnl_infer_shape_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_fusion_info_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_insert_ops_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_internal_attrs_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_layout_id_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_layout_propagator_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_logical_tensor_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_memory_planning_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_op_schema_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_partition_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_thread_local_cache_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_dnnl_utils_cpu.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Registering OCL Tests in CMake\nDESCRIPTION: This snippet iterates through the list of OCL test source files, building an executable for each one. It extracts the executable name, replaces \"test_\" with \"test_graph_\", and appends \"_ocl\". It then adds the executable, links it to necessary libraries (dnnl_gtest, DNNL_LIBRARY_NAME, EXTRA_SHARED_LIBS), and registers it within the graph API test suite. This ensures all tests are compiled, linked, and included in the test execution framework.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/ocl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${API_OCL_TEST_SOURCES})\n    get_filename_component(exe_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"test_\" \"test_graph_\" exe_name ${exe_name}_ocl)\n    add_executable(${exe_name} ${TEST_FILE} ${COMMON_API_TEST_DEPS})\n    target_link_libraries(${exe_name}\n        dnnl_gtest\n        ${DNNL_LIBRARY_NAME}\n        ${EXTRA_SHARED_LIBS}\n    )\n    register_graph_api_test_suite(${exe_name}_gpu ${exe_name})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Registering Executable in CMake\nDESCRIPTION: This snippet registers an executable named 'noexcept-cpp' using the `register_exe` function. It specifies the source file (main.cpp) and the target name (test) for the executable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/noexcept/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nregister_exe(noexcept-cpp main.cpp \"test\")\n```\n\n----------------------------------------\n\nTITLE: Find Threads Package and Add Library\nDESCRIPTION: This snippet finds the Threads package, making it a required dependency, and then adds a static library named 'dnnl_gtest' using the specified main source file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/gtest/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Threads REQUIRED)\nadd_library(${TARGET_NAME} STATIC ${MAIN_SRC})\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Flags for Debugging\nDESCRIPTION: This snippet checks if the `_ONEDNN_GRAPH_LAYOUT_DEBUG` option is enabled. If it is, it sets a compile flag `-DDNNL_GRAPH_LAYOUT_DEBUG` for specific source files (`common.cpp` and `layout_id_mgr.cpp`).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(_ONEDNN_GRAPH_LAYOUT_DEBUG)\n    message(STATUS \"Graph: enable layout debug mode in DNNL backend\")\n\n    file(GLOB FILES_REQUIRED_LAYOUT_DEBUG\n        ${CMAKE_CURRENT_SOURCE_DIR}/common.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/layout_id_mgr.cpp\n    )\n\n    set_source_files_properties(${FILES_REQUIRED_LAYOUT_DEBUG}\n        PROPERTIES COMPILE_FLAGS \"-DDNNL_GRAPH_LAYOUT_DEBUG\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for Target Architecture (PPC64)\nDESCRIPTION: This snippet conditionally applies compiler flags based on the target architecture being PPC64. It finds gemm related source files, and unless the build type is DEBUG (case-insensitive), it sets the `COMPILE_FLAGS` property for those files to `-O3 -funroll-loops` to optimize the code.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_TARGET_ARCH STREQUAL \"PPC64\")\n    file(GLOB FILES_REQUIRED_OPT\n        ${CMAKE_CURRENT_SOURCE_DIR}/gemm/*.[ch]pp\n    )\n    if(NOT UPPERCASE_CMAKE_BUILD_TYPE STREQUAL \"DEBUG\")\n        set_source_files_properties(${FILES_REQUIRED_OPT}\n            PROPERTIES COMPILE_FLAGS \"-O3 -funroll-loops\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Source Removal based on DNNL Options\nDESCRIPTION: These snippets conditionally remove source files from the `SOURCES` list based on the values of `DNNL_CPU_RUNTIME` and `DNNL_EXPERIMENTAL_LOGGING`. If `DNNL_CPU_RUNTIME` is not equal to 'THREADPOOL', 'stream_threadpool.cpp' is removed. If `DNNL_EXPERIMENTAL_LOGGING` is false, then 'logging.cpp' will be removed.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/common/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_CPU_RUNTIME STREQUAL \"THREADPOOL\")\n    # avoid building and linking empty objects\n    list(REMOVE_ITEM SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/stream_threadpool.cpp\")\nendif()\n\nif(NOT DNNL_EXPERIMENTAL_LOGGING)\n    list(REMOVE_ITEM SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/logging.cpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example Graph Dump Output (Bash)\nDESCRIPTION: This shows example output from oneDNN when graph dumping is enabled. Each line indicates that a graph has been serialized to a JSON file, with different suffixes indicating the type of graph (e.g., library graph, partitioning graph, or subgraph).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/graph/graph_dump.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nonednn_graph_verbose,info,serialize graph to a json file graph-100001.json\nonednn_graph_verbose,info,serialize graph to a json file graph-100001-partitioning.json\nonednn_graph_verbose,info,serialize graph to a json file graph-100002-1313609102600373579.json\nonednn_graph_verbose,info,serialize graph to a json file graph-100003-12829238476173481280.json\n```\n\n----------------------------------------\n\nTITLE: Adding Definitions for ISA Hints (CMake)\nDESCRIPTION: This snippet conditionally adds definitions based on whether `DNNL_ENABLE_MAX_CPU_ISA` and `DNNL_ENABLE_CPU_ISA_HINTS` are enabled. It uses `add_definitions_with_host_compiler` to ensure the definitions are added correctly.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/internals/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_ENABLE_MAX_CPU_ISA)\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_MAX_CPU_ISA)\nendif()\n\nif(DNNL_ENABLE_CPU_ISA_HINTS)\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_CPU_ISA_HINTS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Source Removal based on Sanitizer\nDESCRIPTION: This snippet checks if clang sanitizers are enabled (`DNNL_USE_CLANG_SANITIZER`). If so, it removes `test_debug_cpu.cpp` from the list of engine-independent source files. This is done because some tests might be incompatible with sanitizers.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_USE_CLANG_SANITIZER)\n    # Due to the following tests are testing out-of-range enum values.\n    list(REMOVE_ITEM TEST_UTILS_ENGINE_INDEPENDENT_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_debug_cpu.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Globbing Operator Execution Test Sources CMake\nDESCRIPTION: Similar to the previous snippet, this uses `FILE(GLOB)` to find C++ source files for operator execution tests. These tests cover various DNNL operations and are stored in the `DNNL_OP_EXECUTION_TEST_SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nFILE(GLOB DNNL_OP_EXECUTION_TEST_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_common.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_constant_cache.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_scratchpad.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_subgraph_pass.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_op_executable.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_select.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_batch_norm.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_binary_op.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_bmm.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_compiled_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_concat.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_convolution.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_convtranspose.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_dequantize.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_eltwise.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_group_norm.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_interpolate.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_large_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_layer_norm.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_matmul.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_mqa_decomp.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_pool.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_prelu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_quantize.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_reduce.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_reorder.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_sdp_decomp.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_softmax.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_typecast.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_pass.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Globbing SYCL Test Sources in CMake\nDESCRIPTION: This CMake command uses GLOB to collect all C++ source files matching a pattern into a variable. It assumes the files are located in the specified source directory and follow a naming convention. The collected files are used later to create an object library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/sycl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB TEST_INTERFACE_SYCL_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_allocator.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Xbyak Label Usage (C++)\nDESCRIPTION: This code snippet illustrates the recommended way to use `Xbyak::Label` for jump targets in Xbyak code, instead of using `char[]`. It creates a local label within a scope, jumps to it, and then defines it.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CODING_STANDARDS.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n{\n        Xbyak::Label barrier_exit_label;\n        ...\n        jmp(barrier_exit_label);\n        ...\n        L(barrier_exit_label);\n  }\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectory\nDESCRIPTION: Adds the `gemm` subdirectory to the build process, allowing its CMakeLists.txt to be processed.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(gemm)\n```\n\n----------------------------------------\n\nTITLE: Registering All Tests for a Driver CMake\nDESCRIPTION: The `register_all_tests` function iterates through a list of test files and registers each one using the `register_benchdnn_test` function. It conditionally skips tests based on whether DNNL_ENABLE_STACK_CHECKER is enabled and the driver matches a specified pattern.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(stack_checker_pattern \"^(bnorm|concat|conv|eltwise|ip|lrn|matmul|pool|reorder|softmax|sum)$\")\n\nfunction(register_all_tests engine driver test_files)\n    if(DNNL_ENABLE_STACK_CHECKER AND NOT ${driver} MATCHES ${stack_checker_pattern})\n        return()\n    endif()\n\n    foreach(test_file ${test_files})\n        register_benchdnn_test(${engine} ${driver} ${test_file})\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests (sh)\nDESCRIPTION: This command executes the unit tests for the oneDNN library using CTest. It validates that the build was successful and that the core functionality is working as expected.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nctest\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library with CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_graph_backend_dnnl` from the specified header and source files. Object libraries are used to group object files together.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_graph_backend_dnnl)\nadd_library(${OBJ_LIB} OBJECT ${HEADERS} ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: MKL-DNN to DNNL Symbol Mapping\nDESCRIPTION: This code snippet shows how Intel MKL-DNN symbols are mapped to DNNL symbols using the C preprocessor in the `mkldnn_dnnl_mangling.h` header file. This ensures API compatibility between the two libraries.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/transition-to-dnnl.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// ...\n#define mkldnn_memory_desc_t           dnnl_memory_desc_t\n#define mkldnn_memory_desc_init_by_tag dnnl_memory_desc_init_by_tag\n// ...\n```\n\n----------------------------------------\n\nTITLE: Registering Common Test Suites CMake\nDESCRIPTION: This loop iterates through the files listed in `DNNL_COMMON_TEST_SOURCES`. For each file, it extracts the filename without the extension, replaces \"_cpu\" with an empty string and prepends \"test_graph_unit_dnnl_\" to form a test suite name. It then registers the test suite with the generated name, specifying a pattern to match the test cases within the file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(TEST_FILE ${DNNL_COMMON_TEST_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"_cpu\" \"\" file_name ${file_name})\n    string(REPLACE \"test_\" \"test_graph_unit_dnnl_\" test_suite_name ${file_name})\n    register_graph_test_suite(${test_suite_name}_cpu \"${file_name}*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding an object library in CMake\nDESCRIPTION: This snippet adds an object library named `${OBJ_LIB}` which is `graph_unit_test_utils`, and it includes the source files specified in `TEST_UTILS_ENGINE_DEPENDENT_SOURCES` and `TEST_UTILS_ENGINE_INDEPENDENT_SOURCES`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${OBJ_LIB} OBJECT ${TEST_UTILS_ENGINE_DEPENDENT_SOURCES} ${TEST_UTILS_ENGINE_INDEPENDENT_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Disabling Compiler Warnings on Windows (CMake)\nDESCRIPTION: This snippet conditionally disables specific compiler warnings on Windows based on the compiler being used (Intel or MSVC). It disables warnings related to data conversion, unused variables, and deprecated functions to reduce noise during test builds. The warnings are disabled using compiler-specific flags.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32 AND NOT MINGW)\n    if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Intel\")\n        # 2415: unused variable\n        append(CMAKE_TEST_CCXX_NOWARN_FLAGS \"/Qdiag-disable:2415\")\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n        # c4244: conversion with possible loss of data\n        # c4996: unsafe / deprecated functions\n        append(CMAKE_TEST_CCXX_NOWARN_FLAGS \"/wd4996 /wd4244\")\n        if (\"${DNNL_CPU_THREADING_RUNTIME}\" STREQUAL \"THREADPOOL\" AND Eigen3_FOUND)\n            # c4267: conversion with possible loss of data\n            append(CMAKE_TEST_CCXX_NOWARN_FLAGS \"/wd4267\")\n        endif()\n    endif()\nendif()\n\nappend(CMAKE_C_FLAGS \"${CMAKE_TEST_CCXX_NOWARN_FLAGS}\")\nappend(CMAKE_CXX_FLAGS \"${CMAKE_TEST_CCXX_NOWARN_FLAGS}\")\n\nappend_host_compiler_options(CMAKE_CXX_FLAGS \"${DPCPP_CXX_NOWARN_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Update Plan Registry (Auto-search) in oneDNN\nDESCRIPTION: This snippet demonstrates how to update the kernel plan registry used by oneDNN's auto-search feature. This is useful when changes are made to the kernel generation process or new features are added.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/v2/conv/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport ONEDNN_GPU_CONV_PLAN_REGISTRY_PATH=plan_registry_data.txt\n./build/src/gpu/intel/jit/v2/conv/planner/gpu_conv_planner --auto-search\ncp ${ONEDNN_GPU_CONV_PLAN_REGISTRY_PATH}.cpp /path/to/onednn/src/gpu/intel/jit/v2/conv/plan_registry_data.cpp\n```\n\n----------------------------------------\n\nTITLE: Conditional Definition Removal in CMake\nDESCRIPTION: This CMake snippet conditionally removes the DNNL_DLL definition if the DNNL_LIBRARY_TYPE is set to SHARED. This is a workaround for issues with initializing constexpr with dllimport symbols when building a shared library. It prevents users from needing to link the library with the DNNL_DLL flag.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (DNNL_LIBRARY_TYPE STREQUAL \"SHARED\")\n    remove_definitions(-DDNNL_DLL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding memory debug definitions on UNIX with CMake\nDESCRIPTION: This snippet conditionally adds definitions for memory debugging on UNIX-based systems. It appends test_malloc.cpp to the main source files list if DNNL_ENABLE_MEM_DEBUG is enabled.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(UNIX)\n    if(DNNL_ENABLE_MEM_DEBUG)\n        add_definitions_with_host_compiler(-DDNNL_ENABLE_MEM_DEBUG)\n        list(APPEND MAIN_SRC_GTEST ${CMAKE_CURRENT_SOURCE_DIR}/test_malloc.cpp)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Unsafe Element-wise Operation on Padded Blocked Layout\nDESCRIPTION: Highlights the potential issue of performing element-wise operations directly on oneDNN memory objects when zero-padding is used. If the operation doesn't preserve zeros (`eltwise_op(0) != 0`), the padded values will be incorrectly modified, leading to incorrect results. This code block serves as a cautionary example.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/understanding_memory_formats.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int e = 0; e < phys_size; ++e)\n    x[e] = eltwise_op(x[e])\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library in CMake\nDESCRIPTION: This CMake code snippet creates an object library named `${LIB_PACKAGE_NAME}_cpu_sycl` from the source files listed in the `SOURCES` variable. It then appends the resulting object files as dependencies to the `DNNL_LIB_DEPS` global property, ensuring they are linked appropriately.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_cpu_sycl)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Conditional inclusion of large test cases with CMake\nDESCRIPTION: This code conditionally includes large test cases based on the DNNL_TEST_SET_COVERAGE setting. It's designed to exclude these longer tests from the CI build set and include them only for coverage or nightly builds. It prevents long-running tests from blocking CI.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_USE_CLANG_SANITIZER)\n    # Due to the following tests have long run-time, move them to Nightly set\n    if(DNNL_TEST_SET_COVERAGE GREATER DNNL_TEST_SET_CI)\n        file(GLOB LARGE_PRIM_TEST_CASES_SRC\n            test_ip_formats.cpp\n            test_reorder_formats.cpp\n            )\n        foreach(TEST_FILE ${LARGE_PRIM_TEST_CASES_SRC})\n            list(APPEND PRIM_TEST_CASES_SRC \"${TEST_FILE}\")\n        endforeach()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: View Abstraction Description\nDESCRIPTION: Describes a \"virtual\" tensor (view) with its underlying tensor/layout. It helps to express out-of-bound and stride conditions. The mapping from view dimensions to tensor dimensions is defined by special functions, and each tensor dimension may have an associated access mask.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/ir/README.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nView example: 2D convolution, 3x3 filter:\n\n- View `V` has 6 dimensions: `mb`, `ic`, `oh`, `ow`, `kh` and `kw`\n- Tensor `T` has 4 dimensions: `mb`, `ic`, `ih`, `iw`\n- Mapping from view to tensor:\n    - `mb` is directly mapped (`t_0 = v_0`)\n    - `ic` is directly mapped (`t_1 = v_1`)\n    - `ih = oh * SH + kh * (DH + 1) - PH`\n    - `iw = ow * SW + kw * (DW + 1) - PW`\n- M/N/K dimension kinds:\n    - M dimensions: `mb`, `oh`, `ow`\n    - K dimensions: `ic`, `kh`, `kw`\n- Access masks:\n    - `mb` mask: empty\n    - `ic` mask: empty\n    - `ih` mask: `ih >= 0 and ih < IH`\n    - `iw` mask: `iw >= 0 and iw < IW`\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags based on Compiler ID (CMake)\nDESCRIPTION: This snippet sets compiler flags specific to the Intel compiler for improved precision. It appends `/fp:precise` on Windows and `-fp-model precise` on Unix systems.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/internals/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\")\n    append_if(WIN32 CMAKE_CXX_FLAGS \"/fp:precise\")\n    append_if(UNIX  CMAKE_CXX_FLAGS \"-fp-model precise\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Appending Compiler Flags in CMake\nDESCRIPTION: This snippet appends the no-exception flags to the C++ compiler flags. This ensures that the compiled code will not use exceptions, which can improve performance and reduce code size.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/noexcept/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nappend(CMAKE_CXX_FLAGS \"${CMAKE_CCXX_NOEXCEPT_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Test Allocator Source File in CMake\nDESCRIPTION: This snippet sets the TEST_ALLOCATOR variable to the path of the test_allocator.cpp file. This variable can be used later in the CMake configuration to build and link the test allocator.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_ALLOCATOR ${CMAKE_CURRENT_SOURCE_DIR}/test_allocator.cpp)\n```\n\n----------------------------------------\n\nTITLE: Registering C API Tests (CMake)\nDESCRIPTION: This snippet conditionally registers a C API test executable if certain conditions are met. It checks that the CPU runtime is not NONE or TBB, and stack checker is disabled, and that the system isn't SYCL. It finds the math library (libm) and then registers an executable named 'api-c' built from 'api.c'.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DNNL_CPU_RUNTIME STREQUAL \"NONE\" AND\n   NOT DNNL_ENABLE_STACK_CHECKER AND\n   # TBB threading runtime requires explicit finalization for tests used\n   # under CTest. Since TBB doesn't provide C API to perform finalization\n   # we have to skip C tests.\n   NOT DNNL_CPU_RUNTIME STREQUAL \"TBB\" AND\n   NOT DNNL_CPU_SYCL)\n    find_libm(LIBM)\n    register_exe(api-c api.c \"test\" \"${LIBM}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Registering a Single Benchdnn Test CMake\nDESCRIPTION: This CMake function, `register_benchdnn_test`, registers a single benchdnn test. It configures the test execution mode (C, R, B), sets a mode modifier for GPU tests, and then creates a custom target to run the test. Windows and non-Windows paths are handled differently.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(register_benchdnn_test engine driver test_file)\n    if (ARGC GREATER 3)\n        message(ERROR \"Incorrect use of function\")\n    endif()\n\n    set(test_mode \"C\")\n    if(DNNL_TEST_SET_HAS_NO_CORR EQUAL 1)\n        set(test_mode \"R\")\n    elseif(DNNL_TEST_SET_HAS_ADD_BITWISE EQUAL 1)\n        # Form a list to iterate over when registring tests\n        set(test_mode \"${test_mode};B\")\n    endif()\n\n    set(mode_modifier \"\")\n    if(${engine} MATCHES \"gpu\" AND NOT WIN32)\n        set(mode_modifier \"--mode-modifier=P\")\n    endif()\n\n    foreach(tm ${test_mode})\n        string(REPLACE \"test_\" \"test_benchdnn_mode${tm}_\" target_name ${test_file})\n        set(cmd \"--mode=${tm} ${mode_modifier} -v1 --engine=${engine} --${driver} --batch=${test_file}\")\n        set(benchdnn_target ${target_name}_${engine})\n\n        if(DNNL_TEST_SET_HAS_GRAPH_EXE EQUAL 1)\n            string(PREPEND cmd \"--execution-mode=graph\")\n        endif()\n\n        if(NOT WIN32 OR DNNL_BUILD_FOR_CI)\n            string(REPLACE \" \" \";\" cmd \"benchdnn ${cmd}\")\n            add_dnnl_test(${benchdnn_target} ${cmd})\n        else()\n            string(REPLACE \" \" \";\" cmd \"$<TARGET_FILE:benchdnn> ${cmd}\")\n\n            if(WIN32)\n                set(cmd \"cmd;/c;${PROJECT_BINARY_DIR}/run_with_env.bat;${cmd}\")\n                set(ARGV2 \"cmd;/c;${PROJECT_BINARY_DIR}/run_with_env.bat;${ARGV2}\")\n            endif()\n\n            add_custom_target(${benchdnn_target}\n                COMMAND ${cmd}\n                DEPENDS benchdnn\n                WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}\n            )\n\n            set_target_properties(${benchdnn_target} PROPERTIES\n                EXCLUDE_FROM_DEFAULT_BUILD TRUE)\n            maybe_configure_windows_test(${benchdnn_target} TARGET)\n\n            # Create non-suffixed target for compatibility\n            if (engine STREQUAL \"cpu\")\n                add_custom_target(${target_name} DEPENDS ${benchdnn_target})\n                maybe_configure_windows_test(${target_name} TARGET)\n            endif()\n        endif()\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This snippet sets up the include directories needed to compile the graph unit tests, pointing to the tests, graph source, and include directories.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(\n    ${PROJECT_SOURCE_DIR}/tests\n    ${PROJECT_SOURCE_DIR}/src/graph\n    ${PROJECT_SOURCE_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library in CMake\nDESCRIPTION: This CMake snippet creates an object library named `${LIB_PACKAGE_NAME}_xpu_sycl` using the files listed in the `SOURCES` variable. The `add_library` command with the `OBJECT` keyword indicates that an object library should be created.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_xpu_sycl)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Problem Generation using synthdnn.py\nDESCRIPTION: Generates a synthetic problem for a specified oneDNN primitive using randomly generated data. The script takes a primitive type, sampling controls, and a batch file as input. The batch file will contain data used by the oneDNN primitive.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/synthdnn/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython3 synthdnn.py <primitive> [sampling controls] -b <batch_file>\n```\n\n----------------------------------------\n\nTITLE: Registering Engine Independent Test Suites in CMake\nDESCRIPTION: This snippet iterates through the engine-independent source files, extracts the filename, removes the \"_cpu\" suffix, constructs the test suite name, and registers the test suite using `register_graph_test_suite`. It replaces \"test_\" with \"test_graph_unit_interface_\" to create the test suite name, creates a filter and appends \"_cpu\" to test suite name.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(TEST_FILE ${TEST_INTERFACE_ENGINE_INDEPENDENT_SOURCES})\n    get_filename_component(file_name ${TEST_FILE} NAME_WE)\n    string(REPLACE \"_cpu\" \"\" file_name ${file_name})\n    string(REPLACE \"test_\" \"test_graph_unit_interface_\" test_suite_name ${file_name})\n    string(REPLACE \"test_\" \"test_interface_\" filter ${file_name})\n    register_graph_test_suite(${test_suite_name}_cpu \"${filter}.*:${filter}/*\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Registering Test Executables with Environment Variables (CMake)\nDESCRIPTION: This snippet registers separate test targets to preserve the testing environment and allow specific functionality to be tested properly. It removes the environment variable test files from the main test source list.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/internals/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nregister_exe(${TEST_EXE}_env_vars_dnnl\n        \"${MAIN_SRC_GTEST};${CMAKE_CURRENT_SOURCE_DIR}/test_env_vars_dnnl.cpp\"\n        \"test\" \"dnnl_gtest\")\nlist(REMOVE_ITEM TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_env_vars_dnnl.cpp)\nregister_exe(${TEST_EXE}_env_vars_onednn\n        \"${MAIN_SRC_GTEST};${CMAKE_CURRENT_SOURCE_DIR}/test_env_vars_onednn.cpp\"\n        \"test\" \"dnnl_gtest\")\nlist(REMOVE_ITEM TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_env_vars_onednn.cpp)\n```\n\n----------------------------------------\n\nTITLE: Running Layer Norms from an Input File\nDESCRIPTION: Executes a set of layer normalization benchmarks using configurations defined in an input file (shapes_ci). This uses the default settings for the benchmarks.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_lnorm.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --lnorm --batch=shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Getting Extra Library Dependencies in CMake\nDESCRIPTION: This snippet retrieves the global properties for extra static and shared library dependencies for the DNNL subdirectory.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nget_property(STATIC_LIB_DEPS GLOBAL PROPERTY DNNL_SUBDIR_EXTRA_STATIC_LIBS)\nget_property(SHARED_LIB_DEPS GLOBAL PROPERTY DNNL_SUBDIR_EXTRA_SHARED_LIBS)\n```\n\n----------------------------------------\n\nTITLE: oneDNN Execute interface with OpenCL events\nDESCRIPTION: Interface enables the user to pass dependencies between primitives using OpenCL events.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/opencl_interoperability.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\ndnnl::ocl_interop::execute()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories in CMake\nDESCRIPTION: This snippet sets the include directories for the object library `${OBJ_LIB}`. It adds the `xbyak_aarch64` subdirectory to the private include directories, making the header files within that directory available during compilation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/xbyak_aarch64/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(\n    ${OBJ_LIB}\n    PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/xbyak_aarch64\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Object Library\nDESCRIPTION: This code defines and creates an object library named `${LIB_PACKAGE_NAME}_gpu_intel_ocl` containing the collected source files in the `SOURCES` variable.  It then adds the library's object files to the global `DNNL_LIB_DEPS` property, which is likely used to manage dependencies for the main DNNL library. The object library is created to facilitate modular compilation and linking.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_intel_ocl)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Appending Test Files to List in CMake\nDESCRIPTION: This snippet appends `TEST_THREAD` and `TEST_ALLOCATOR` to the list of common unit tests. These are likely predefined variables pointing to source files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND COMMON_UNIT_TESTS ${TEST_THREAD})\nlist(APPEND COMMON_UNIT_TESTS ${TEST_ALLOCATOR})\n```\n\n----------------------------------------\n\nTITLE: Adding Object Library to DNNL Dependencies in CMake\nDESCRIPTION: This CMake snippet appends the object library `${OBJ_LIB}` (which expands to `${LIB_PACKAGE_NAME}_xpu_sycl`) to the global `DNNL_LIB_DEPS` property. This property is likely used to manage dependencies for oneDNN libraries. The `$<TARGET_OBJECTS:${OBJ_LIB}>` generator expression is used to retrieve the object files associated with the target.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/sycl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Globbing Engine Dependent Sources in CMake\nDESCRIPTION: This snippet uses `file(GLOB)` to find all source files ending in `.cpp` in the current source directory that are engine dependent. These source files are added to the `TEST_INTERFACE_ENGINE_DEPENDENT_SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB TEST_INTERFACE_ENGINE_DEPENDENT_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_allocator.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_compiled_partition.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_partition_hashing.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_tensor.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Zero Pad Driver Usage\nDESCRIPTION: This command executes the benchdnn zero pad driver with specified knobs and problem description. It takes optional benchdnn and zeropad-specific knobs.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_zeropad.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --zeropad [benchdnn-knobs] [zeropad-knobs] [zeropad-desc] ...\n```\n\n----------------------------------------\n\nTITLE: Globbing Common API Test Dependencies in CMake\nDESCRIPTION: This CMake code snippet uses `FILE(GLOB)` to collect common API test dependencies. It searches for `api_test_main.cpp`, `test_api_common.cpp`, and files specified by the variables `${TEST_ALLOCATOR}` and `${TEST_THREAD}` in the current source directory and stores the results in the `COMMON_API_TEST_DEPS` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/api/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nFILE(GLOB COMMON_API_TEST_DEPS\n    ${CMAKE_CURRENT_SOURCE_DIR}/api_test_main.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_api_common.cpp\n    ${TEST_ALLOCATOR}\n    ${TEST_THREAD}\n)\n```\n\n----------------------------------------\n\nTITLE: Finding Source Files with GLOB_RECURSE in CMake\nDESCRIPTION: This snippet uses the GLOB_RECURSE command to find all C++ and C source files within the current source directory. The list of found files is stored in the variable SOURCES.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/ppc64/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Globbing Additional Source Files Recursively\nDESCRIPTION: This snippet uses the `file(GLOB_RECURSE)` command to find additional source files (`.c`, `.cpp`, `.ch`, and `.hpp`) recursively in the `kernels`, `passes`, and `patterns` subdirectories of the current source directory. The resulting list of files is stored in the `SOURCES_EXTRA` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/backend/dnnl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES_EXTRA\n    ${CMAKE_CURRENT_SOURCE_DIR}/kernels/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/passes/*.[ch]pp\n    ${CMAKE_CURRENT_SOURCE_DIR}/patterns/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files with GLOB in CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to collect source files from the specified directory. It gathers all `.cpp` files matching the given patterns and stores them in the `SOURCES` variable for later use in building the library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/xbyak_aarch64/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/xbyak_aarch64_impl.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/src/util_impl.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Set CUDA SYCL Compiler Flags\nDESCRIPTION: This conditional statement checks if `DNNL_SYCL_CUDA` is enabled or if both `DNNL_SYCL_GENERIC` and `NVIDIA_TARGET_SUPPORTED` are enabled. If either condition is true, it appends the `-fsycl-targets=nvptx64-nvidia-cuda` flag to the `CMAKE_CXX_FLAGS`, specifying CUDA as a target architecture for SYCL compilation. It also suppresses linker warnings.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_SYCL_CUDA OR (DNNL_SYCL_GENERIC AND NVIDIA_TARGET_SUPPORTED))\n    append(CMAKE_CXX_FLAGS \"-fsycl-targets=nvptx64-nvidia-cuda\")\n    # Suppress irrelevant warning about incompatible triplets.\n    append(CMAKE_CXX_FLAGS \"-Wno-linker-warnings\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Set Default Build Type\nDESCRIPTION: Sets the default build type to \"Release\" if CMAKE_BUILD_TYPE is not already set. This ensures a default optimized build if the user doesn't explicitly specify a build type.  It also provides a message regarding Visual Studio generator.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(\"${CMAKE_BUILD_TYPE}\" STREQUAL \"\")\n    message(STATUS \"CMAKE_BUILD_TYPE is unset, defaulting to Release\")\n    set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING\n        \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel RelWithAssert RelWithMDd...\")\nendif()\nif (CMAKE_GENERATOR MATCHES \"^Visual Studio\")\n    message(STATUS\n\"oneDNN build configuration is based on the CMAKE_BUILD_TYPE value, but\n the CMake generator '${CMAKE_GENERATOR}' does not respect it and requires\n using the --config option to choose the build type. Changing the build type\n using the --config option requires rerunning CMake from scratch with a\n matching CMAKE_BUILD_TYPE value.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Manage CMake Policies\nDESCRIPTION: Sets CMake policies for specific versions to ensure consistent behavior. This handles changes in CMake's default behavior across different versions, mitigating potential compatibility issues.  CMP0082 addresses install rules, CMP0148 handles FindPython modules, and CMP0146 concerns the removal of the FindCUDA module.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(POLICY CMP0082)\n    cmake_policy(SET CMP0082 NEW)\nendif()\n\nif(POLICY CMP0148)\n    cmake_policy(SET CMP0148 NEW)\nendif()\n\nif(POLICY CMP0146)\n    cmake_policy(SET CMP0146 OLD)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Registering Test Executable with CMake\nDESCRIPTION: This command registers the test executable within the CMake build system. It takes the test executable name (`TEST_EXE`), the source files (`TEST_SOURCES`), a test identifier (`test`), and a dependency (`dnnl_gtest`). This allows the test to be built and executed as part of the overall build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/regression/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nregister_exe(${TEST_EXE} \"${TEST_SOURCES}\" \"test\" \"dnnl_gtest\")\n```\n\n----------------------------------------\n\nTITLE: Include GNUInstallDirs and CMakePackageConfigHelpers\nDESCRIPTION: Includes the GNUInstallDirs module, which defines standard installation directories, and CMakePackageConfigHelpers for creating package configuration files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(GNUInstallDirs)\ninclude(CMakePackageConfigHelpers)\n```\n\n----------------------------------------\n\nTITLE: Building on Linux/macOS (Intel C++ with SYCL)\nDESCRIPTION: These commands configure CMake to build oneDNN with SYCL support, using the Intel oneAPI DPC++/C++ Compiler. The environment must first be set up using the `setvars.sh` script from the Intel oneAPI installation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/build.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\n```\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p build\ncd build\n\nexport CC=icx\nexport CXX=icpx\n\ncmake .. \\\n          -DDNNL_CPU_RUNTIME=SYCL \\\n          -DDNNL_GPU_RUNTIME=SYCL \\\n          <extra build options>\n```\n\nLANGUAGE: sh\nCODE:\n```\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Setting Implicit Conversion Warning Flag\nDESCRIPTION: This snippet checks the compiler ID and appends a flag to the CXX flags to disable implicit integer conversion warnings if the compiler is IntelLLVM or Clang.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/gemm/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\" OR CMAKE_CXX_COMPILER_ID MATCHES \"(Apple)?[Cc]lang\")\n    append(CMAKE_CXX_FLAGS \" -Wno-implicit-int-conversion\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Include Directories\nDESCRIPTION: Includes the project source directory and binary directory into the include path.  The `include_directories_with_host_compiler_before` function ensures that the host compiler's include directories are added before the standard system include directories to prevent conflicts.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories_with_host_compiler_before(${PROJECT_SOURCE_DIR}/include)\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/include/oneapi/dnnl/dnnl_config.h.in\"\n    \"${PROJECT_BINARY_DIR}/include/oneapi/dnnl/dnnl_config.h\"\n)\ninclude_directories_with_host_compiler_before(${PROJECT_BINARY_DIR}/include)\n```\n\n----------------------------------------\n\nTITLE: Appending Generated Source to Sources\nDESCRIPTION: This line appends the generated kernel list source file (`kernel_list_src`) to the `SOURCES` list, ensuring it is included in the compilation process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND SOURCES ${kernel_list_src})\n```\n\n----------------------------------------\n\nTITLE: Setting Variables for Kernel List Generation\nDESCRIPTION: These lines set the template and source file paths for the kernel list generation process. `kernel_list_templ` specifies the input template file, and `kernel_list_src` specifies the output source file that will be generated.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(kernel_list_templ \"${PROJECT_SOURCE_DIR}/src/gpu/intel/ocl/ocl_kernel_list.cpp.in\")\nset(kernel_list_src \"${PROJECT_BINARY_DIR}/src/gpu/intel/ocl/ocl_kernel_list.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Adding Definition for Graph Dump CMake\nDESCRIPTION: This code snippet conditionally adds a definition (`DNNL_ENABLE_GRAPH_DUMP`) to the compiler flags if the `ONEDNN_ENABLE_GRAPH_DUMP` option is enabled. This definition likely enables graph dumping functionality during the tests, potentially for debugging or visualization purposes.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(ONEDNN_ENABLE_GRAPH_DUMP)\n    add_definitions_with_host_compiler(-DDNNL_ENABLE_GRAPH_DUMP)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchdnn Test Script Example Usage\nDESCRIPTION: This example demonstrates how to run the `benchdnn_test.py` script with the `-d` option to specify the dataset. It shows a sample output where the script runs benchdnn tests for various drivers and batches, indicating whether each test has passed.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/tests/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n$ python3 benchdnn_test.py -d dataset_simple\nBENCHDNN TEST: driver=binary, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=bnorm, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=concat, batch=test_concat_ci: PASSED\nBENCHDNN TEST: driver=conv, batch=shapes_basic: PASSED\nBENCHDNN TEST: driver=conv, batch=shapes_auto: PASSED\nBENCHDNN TEST: driver=conv, batch=shapes_regression_small_spatial: PASSED\nBENCHDNN TEST: driver=deconv, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=eltwise, batch=shapes_eltwise: PASSED\nBENCHDNN TEST: driver=ip, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=lnorm, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=lrn, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=matmul, batch=shapes_2d_ci: PASSED\nBENCHDNN TEST: driver=pooling, batch=shapes_basic: PASSED\nBENCHDNN TEST: driver=prelu, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=reduction, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=resampling, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=rnn, batch=shapes_small: PASSED\nBENCHDNN TEST: driver=shuffle, batch=option_set_min: PASSED\nBENCHDNN TEST: driver=softmax, batch=shapes_ci: PASSED\nBENCHDNN TEST: driver=sum, batch=test_sum_ci: PASSED\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Test Suites (CMake)\nDESCRIPTION: This snippet adds subdirectories containing different test suites: gtests, benchdnn, and conditionally noexcept. The 'noexcept' subdirectory is only added if SYCL is not enabled, stack checker is not enabled, and the platform is Unix-like or MinGW.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(gtests)\nadd_subdirectory(benchdnn)\n\nif(NOT DNNL_WITH_SYCL AND NOT DNNL_ENABLE_STACK_CHECKER)\n    if(UNIX OR MINGW)\n        add_subdirectory(noexcept)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Skipping USM tests in CMake\nDESCRIPTION: This snippet sets up a regular expression pattern to skip USM tests based on the DNNL_WITH_SYCL flag.  If SYCL is enabled, it skips the test_cross_engine_reorder test due to synchronization issues with USM pointers.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nset(skip_usm_pattern \"^$\") # no skip by default\nif(DNNL_WITH_SYCL)\n    # - cross_engine_reorder creates usm pointers on CPU and GPU that belong to\n    #   the different context, which causes synchronization issues.\n    #   FIXME: the library should handle this case gracefully.\n    set(skip_usm_pattern \"(test_cross_engine_reorder)\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enable Testing\nDESCRIPTION: Enables CTest testing framework, used for running unit tests and integration tests for the library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nenable_testing()\n```\n\n----------------------------------------\n\nTITLE: Example Output for Inner Product Performance Measurement\nDESCRIPTION: This snippet presents the expected output from running the `benchdnn` command for measuring the performance of inner products with the standard performance template.  The output includes performance metrics such as engine type, problem name, problem descriptor, Giga-operations, frequency, and execution time.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_perf_report.md#_snippet_2\n\nLANGUAGE: Text\nCODE:\n```\nOutput template: perf,%engine%,%name%,%prb%,%Gops%,%Gfreq%,%-time%,%-Gflops%,%0time%,%0Gflops%\nperf,cpu,\"resnet:ip1\",mb112oc1000ic2048n\"resnet:ip1\",0.458752,0,0.521729,879.293,0.576451,795.822\n```\n\n----------------------------------------\n\nTITLE: Commit Message Example (Git)\nDESCRIPTION: Example demonstrating the format of a commit message for oneDNN contributions. It highlights the scope and a short description, adhering to the prescribed character limit.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: git\nCODE:\n```\ncommon: verbose: fix crash when prim_iface_t is empty\n```\n\n----------------------------------------\n\nTITLE: Appending to Source List with CMake\nDESCRIPTION: This snippet appends the contents of the MAIN_SRC_GTEST variable to the existing TEST_SOURCES list. This likely includes the main source file(s) for Google Test, which is used for the unit tests.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/api/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND TEST_SOURCES ${MAIN_SRC_GTEST})\n```\n\n----------------------------------------\n\nTITLE: Adding gtest subdirectory with CMake\nDESCRIPTION: This snippet adds the gtest subdirectory to the build. This is necessary to include the gtest library as part of the oneDNN project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(${PROJECT_SOURCE_DIR}/third_party/gtest gtest)\n```\n\n----------------------------------------\n\nTITLE: Create Object Library and Set Dependency\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_gpu_generic_sycl` from the source files collected in the `SOURCES` variable. It then appends the object library to the `DNNL_LIB_DEPS` global property, making it a dependency for other libraries or executables in the project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/generic/sycl/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_generic_sycl)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Run Binary Primitive Problems from File\nDESCRIPTION: This example shows how to run a set of binary primitive problems defined in the `inputs/binary/shapes_ci` file.  It leverages the `--batch` option to specify the input file containing the problem definitions.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_binary.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --binary --batch=inputs/binary/shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Generating C Symbol References for Tests (CMake)\nDESCRIPTION: This snippet generates C symbol references for testing. It checks if the compiler is not Clang and the system is Unix-like or MinGW. It then creates a custom command to generate a C file containing symbol references using a shell script. Finally, it registers an executable named 'test_c_symbols-c' built from the generated C file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_CXX_COMPILER_ID MATCHES \"(Apple)?[Cc]lang\" AND (UNIX OR MINGW))\n    get_directory_property(include_dirs INCLUDE_DIRECTORIES)\n    set(test_c_symbols \"${CMAKE_CURRENT_BINARY_DIR}/test_c_symbols.c\")\n    add_custom_command(\n        OUTPUT ${test_c_symbols}\n        COMMAND ${CMAKE_CURRENT_SOURCE_DIR}/generate_c_symbols_refs.sh\n        ${CMAKE_CURRENT_SOURCE_DIR}/.. ${test_c_symbols} ${include_dirs}\n    )\n    register_exe(test_c_symbols-c ${test_c_symbols} \"test\")\n# elseif(WIN32)\n# No Windows support for: test_c_symbols.c\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Debug Header using generate_dnnl_debug.py\nDESCRIPTION: This script generates the oneDNN debug header and source files that map enums to strings. It requires CastXML to parse the dnnl_types.h header file. The script should be run whenever a new tag is added to the API to regenerate the debug header and related source code.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n# Generate dnnl_config.h\n$ (mkdir -p build && cd build && cmake -DONEDNN_BUILD_GRAPH=OFF ..)\n\n# Generate types.xml\n# CastXML can be found at https://github.com/CastXML/CastXML\n$ castxml --castxml-cc-gnu-c clang --castxml-output=1 -Iinclude -Ibuild/include include/oneapi/dnnl/dnnl_types.h -o types.xml\n\n# run generate_dnnl_debug.py\n$ ./scripts/generate_dnnl_debug.py types.xml\n```\n\n----------------------------------------\n\nTITLE: Installing Examples for Bundle Mode with CMake\nDESCRIPTION: This snippet handles the installation of oneDNN examples when the `DNNL_INSTALL_MODE` is set to `BUNDLE`. It configures a `CMakeLists.txt` file, installs the source and header files, and conditionally installs SYCL-related files or tutorials based on whether SYCL is enabled.  It also installs a template file for Windows environments.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/examples/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (DNNL_INSTALL_MODE STREQUAL \"BUNDLE\")\n    set(BUNDLE_EXAMPLES_DIR \"${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME}/examples\")\n\n    configure_file(CMakeLists.txt.in CMakeLists.txt @ONLY)\n    install(FILES\n        ${CMAKE_CURRENT_BINARY_DIR}/CMakeLists.txt\n        ${sources} ${headers}\n        DESTINATION ${BUNDLE_EXAMPLES_DIR})\n\n    if(DNNL_WITH_SYCL)\n        install(FILES\n            ${PROJECT_SOURCE_DIR}/cmake/dpcpp_driver_check.cmake\n            DESTINATION ${BUNDLE_EXAMPLES_DIR})\n    else()\n    # Skip matmul examples with SYCL\n        install(DIRECTORY\n            tutorials\n            DESTINATION ${BUNDLE_EXAMPLES_DIR})\n    endif()\n\n    if(WIN32)\n        install(FILES\n            ${PROJECT_SOURCE_DIR}/cmake/template.vcxproj.user\n            DESTINATION ${BUNDLE_EXAMPLES_DIR})\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Use of Named Constants Improved (C++)\nDESCRIPTION: This example demonstrates how to properly name constants, replacing the previous example's hardcoded values, resulting in improved readability.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CODING_STANDARDS.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nif (x < sizeof(float)) y = cache_line_size;\n```\n\n----------------------------------------\n\nTITLE: Link Libraries Based on OS\nDESCRIPTION: This snippet conditionally links libraries (pthread or regex) based on the operating system.  It links pthread on UNIX-like systems (excluding QNXNTO) and MinGW, and regex on QNXNTO.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/third_party/gtest/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif((UNIX AND NOT QNXNTO) OR MINGW)\n    target_link_libraries(${TARGET_NAME} pthread)\nelif(QNXNTO)\n    target_link_libraries(${TARGET_NAME} regex)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including Directories with Host Compiler in CMake\nDESCRIPTION: This snippet includes the current source directory as an include directory using the host compiler. This ensures that the compiler can find necessary header files for building the oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(\n    ${CMAKE_CURRENT_SOURCE_DIR}\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation Flags for Layout Debugging CMake\nDESCRIPTION: This code snippet conditionally sets compilation flags for layout debugging. If `_ONEDNN_GRAPH_LAYOUT_DEBUG` is enabled, it finds the `test_layout_id.cpp` file and adds the `-DDNNL_GRAPH_LAYOUT_DEBUG` flag to its compilation properties. This likely enables detailed layout information during test execution.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(_ONEDNN_GRAPH_LAYOUT_DEBUG)\n    file(GLOB FILES_REQUIRED_LAYOUT_DEBUG\n        ${CMAKE_CURRENT_SOURCE_DIR}/test_layout_id.cpp\n    )\n    set_source_files_properties(${FILES_REQUIRED_LAYOUT_DEBUG}\n        PROPERTIES COMPILE_FLAGS \"-DDNNL_GRAPH_LAYOUT_DEBUG\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Compiler Definitions (CMake)\nDESCRIPTION: This snippet adds compiler definitions to the build. It defines `NOMINMAX` to allow the use of `std::max` on Windows and `WIN32_LEAN_AND_MEAN` to speed up Windows compilation by excluding less frequently used headers. Additionally, it conditionally defines `DNNL_DISABLE_PRIMITIVE_CACHE` if `DNNL_ENABLE_PRIMITIVE_CACHE` is not enabled.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_definitions_with_host_compiler(-DNOMINMAX) # to allow std::max on Windows with parentheses\nadd_definitions_with_host_compiler(-DWIN32_LEAN_AND_MEAN) # to speedup up Windows compilation\n\nif(NOT DNNL_ENABLE_PRIMITIVE_CACHE)\n    add_definitions_with_host_compiler(-DDNNL_DISABLE_PRIMITIVE_CACHE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Test Executable Name in CMake\nDESCRIPTION: This snippet defines the name of the test executable to be built. The variable TEST_EXE is set to 'test_api_ocl', which will be the name of the compiled executable file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/api/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_EXE test_api_ocl)\n```\n\n----------------------------------------\n\nTITLE: Globbing Test Sources in CMake\nDESCRIPTION: Uses the `file(GLOB)` command to find all C++ test source files in the current directory. The `TEST_SOURCES` variable will contain a list of all files matching the `test_*.cpp` pattern. This enables to easily discover and include all test files without listing them individually.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_*.cpp)\n```\n\n----------------------------------------\n\nTITLE: Example JIT Dump Output (CPU)\nDESCRIPTION: This shows example output files generated when JIT dump is enabled on a CPU with Intel AVX2 support. The files contain the JIT-compiled code for different operations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/performance_considerations/inspecting_jit.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ndnnl_dump_cpu_jit_avx2_conv_fwd_kernel_f32.1.bin\n...\ndnnl_dump_cpu_jit_avx_gemv_t_f32_kern.30.bin\n```\n\n----------------------------------------\n\nTITLE: Configure README file\nDESCRIPTION: Configures the README.binary.in file to create the README file in the binary directory.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/README.binary.in\"\n    \"${PROJECT_BINARY_DIR}/README\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing oneDNN Threadpool Interface with Eigen\nDESCRIPTION: This code snippet demonstrates how to implement the oneDNN threadpool interface using the Eigen threadpool. It defines a `threadpool_t` class that inherits from `dnnl::threadpool_interop::threadpool_iface` and provides implementations for `get_num_threads`, `get_in_parallel`, `get_flags`, and `parallel_for` methods.  It relies on Eigen's ThreadPool for actual thread management.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/threadpool.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"dnnl_threadpool_iface.hpp\"\n\nclass threadpool_t : public dnnl::threadpool_interop::threadpool_iface {\nprivate:\n    // Change to Eigen::NonBlockingThreadPool if using Eigen <= 3.3.7\n    std::unique_ptr<Eigen::ThreadPool> tp_;\n\npublic:\n    explicit threadpool_t(int num_threads = 0) {\n        if (num_threads <= 0)\n            num_threads = (int)std::thread::hardware_concurrency();\n        tp_.reset(new Eigen::ThreadPool(num_threads));\n    }\n    int get_num_threads() const override { return tp_->NumThreads(); }\n    bool get_in_parallel() const override {\n        return tp_->CurrentThreadId() != -1;\n    }\n    uint64_t get_flags() override { return ASYNCHRONOUS; }\n    void parallel_for(int n, const std::function<void(int, int)> &fn) override {\n        int nthr = get_num_threads();\n        int njobs = std::min(n, nthr);\n\n        for (int i = 0; i < njobs; i++) {\n            tp_->Schedule([i, n, njobs, fn]() {\n                int start, end;\n                impl::balance211(n, njobs, i, start, end);\n                for (int j = start; j < end; j++)\n                    fn(j, n);\n            });\n        }\n    };\n};\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories\nDESCRIPTION: This snippet adds several subdirectories to the build process, including 'compute', 'microkernels', 'jit', and 'ocl'. If DNNL_WITH_SYCL is enabled, it also adds the 'sycl' subdirectory. These subdirectories likely contain source code for different parts of the oneDNN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(compute)\nadd_subdirectory(microkernels)\nadd_subdirectory(jit)\nadd_subdirectory(ocl)\n\nif(DNNL_WITH_SYCL)\n    add_subdirectory(sycl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: IR Object Debugging Methods\nDESCRIPTION: Describes the debugging methods available for all IR objects, including overloaded `operator<<` for printing to `std::ostream`, the `str()` method for returning a textual representation, and the `dump()` method for printing under gdb.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/ir/README.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n- Overloaded `operator<<` to use with `std::ostream`\n- `str()` method returning a textual representation of the object\n- `dump()` method to call it under gdb to print a textual representation of\n  the object:\n    - `call obj.dump()` (under gdb)\n```\n\n----------------------------------------\n\nTITLE: Reorder Problem Descriptor Example\nDESCRIPTION: This example showcases the canonical form of the reorder problem descriptor.  N is an integer and signifies logical dimensions N, C, D, H, W.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_reorder.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n    NxNxNxNxN\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library in CMake\nDESCRIPTION: This snippet creates an object library named `graph_unit_test_interface` using the source files defined in `TEST_INTERFACE_ENGINE_DEPENDENT_SOURCES` and `TEST_INTERFACE_ENGINE_INDEPENDENT_SOURCES`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${OBJ_LIB} OBJECT ${TEST_INTERFACE_ENGINE_DEPENDENT_SOURCES} ${TEST_INTERFACE_ENGINE_INDEPENDENT_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Signing oneDNN library for macOS hardened runtime\nDESCRIPTION: This command shows how to sign the oneDNN dynamic library for macOS hardened runtime using the `codesign` tool. It enables the `com.apple.security.cs.allow-jit` entitlement, which is required because oneDNN generates code on the fly. The command uses an entitlements plist file.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/build/link.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncodesign -s \"Your identity\" --options runtime --entitlements Entitlements.plist [other options...] /path/to/libdnnl.dylib\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for GCC\nDESCRIPTION: This snippet sets specific compiler flags for certain source files when using the GNU GCC compiler. It disables LTO (Link Time Optimization) for loop_sequencer.cpp and generator.cpp, and it disables variable tracking for gen_gemm_kernel_db.cpp, likely as workarounds for known issues in GCC versions 10, 11, and 12.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/gemm/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_COMPILER_IS_GNUCC)\n    # Workaround for LTO bug in GCC 10, 11, 12 (possibly other versions)\n    set_source_files_properties(generator/pieces/loop_sequencer.cpp PROPERTIES COMPILE_FLAGS -fno-lto)\n    set_source_files_properties(generator/generator.cpp PROPERTIES COMPILE_FLAGS -fno-lto)\n\n\n    # Workaround for excessively long compile time in GCC 11, 12 (possibly other versions)\n    set_source_files_properties(gen_gemm_kernel_db.cpp PROPERTIES COMPILE_FLAGS -fno-var-tracking)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running PReLU Benchmark with Input File\nDESCRIPTION: Runs the PReLU primitive benchmark using a set of predefined problems from the `inputs/prelu/shapes_ci` file. This is useful for testing a range of input shapes and configurations without specifying each one individually.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_prelu.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n ./benchdnn --prelu --batch=inputs/prelu/shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Generating Format Tags using generate_format_tags.py\nDESCRIPTION: This script generates C++ API tags based on C API tags. It automates the process of adding new tags to the C++ API whenever a new tag is introduced in the C API. The script ensures consistency between the C and C++ APIs.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n$ ./scripts/generate_format_tags.py\n```\n\n----------------------------------------\n\nTITLE: Install Files\nDESCRIPTION: Installs license, third-party programs, and README files to the documentation directory. The installation path depends on whether the DNNL_INSTALL_MODE is set to \"BUNDLE\".\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/CMakeLists.txt#_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_INSTALL_MODE STREQUAL \"BUNDLE\")\n    install(FILES LICENSE DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME})\n    install(FILES THIRD-PARTY-PROGRAMS DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME})\n    install(FILES ${PROJECT_BINARY_DIR}/README DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME})\nelse()\n    # Cannot use CMAKE_INSTALL_DOCDIR since it uses PROJECT_NAME and not DNNL_LIBRARY_NAME\n    install(FILES LICENSE DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME})\n    install(FILES THIRD-PARTY-PROGRAMS DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME})\n    install(FILES ${PROJECT_BINARY_DIR}/README DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/${LIB_PACKAGE_NAME})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reorder with Scales (sh)\nDESCRIPTION: This example runs a 1D-spatial reorder problem with s8 input data and u8 output data. It uses four different physical memory layout combinations ({ncw, ncw}, {ncw, nwc}, {nwc, ncw}, and {nwc, nwc}) and applies a source scale of 2.5 for each source point.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/knobs_attr.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n    ./benchdnn --reorder --sdt=s8 --ddt=u8 \\\n               --stag=ncw,nwc --dtag=ncw,nwc \\\n               --attr-scales=src:common:2.5 2x8x8\n```\n\n----------------------------------------\n\nTITLE: Convolution with Scale and Post-ops (Pseudo-code)\nDESCRIPTION: This pseudo-code demonstrates setting a scale attribute and a sequence of post-ops (sum and eltwise) for a convolution operation.  It shows how to configure scaling for the source and destination, and how the operations are applied.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/primitives/convolution.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nprimitive_attr attr;\n    attr.set_scale(src, mask=0);\n    attr.set_post_ops({\n            { sum={scale=beta} },\n            { eltwise={scale=gamma, type=tanh, alpha=ignore, beta=ignored } }\n        });\n\n    convolution_forward(src, weights, dst, attr);\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses the `file(GLOB)` command to collect all `.cpp`, `.c`, `.hpp`, and `.h` files in the current source directory.  The collected files are stored in the `SOURCES` variable, which is later used to create an object library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/graph/interface/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.[ch]pp\n    )\n```\n\n----------------------------------------\n\nTITLE: Conditional Test Source Removal with Clang Sanitizer in CMake\nDESCRIPTION: This snippet conditionally removes a specific test source file (`test_memory.cpp`) from the `TEST_SOURCES` list if the `DNNL_USE_CLANG_SANITIZER` flag is enabled.  This is done because certain tests may trigger memory sanitizer errors and are excluded when sanitizers are active.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/api/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(DNNL_USE_CLANG_SANITIZER)\n    # Due to the following tests trigger memory sanitizer\n    # `allocation-size-too-big`, remove them.\n    list(REMOVE_ITEM TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_memory.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Convolution Status Table\nDESCRIPTION: This table shows convolution status for the oneDNN Nvidia backend. It shows the weights format, winograd support, supported input format, supported output format, supported data type and limitations.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/nvidia/README.md#_snippet_2\n\nLANGUAGE: none\nCODE:\n```\n| Weights Format | Winograd Supported | Supported Input Format | Supported Output Format | Supported Data Type | Limitations                                                                                                                                                                             |\n|----------------|--------------------|------------------------|-------------------------|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2D NCHW        | YES                | NCHW, NHWC             | NCHW, NHWC              | f32, f16, bf16            | The Winograd algorithm has limitations: <br> * Filter size must be 3x3 or 5x5. <br> * Dilation must be zero for all dimensions. <br> * Horizontal and vertical filter stride must be 1. |\n| 2D NHWC        | NO                 | NHWC                   | NHWC                    | f32, f16, bf16, int8      | * Dilation must be zero in all dimensions. <br> * Output feature maps must be multiple of 4 for `int8` type.                                                                            |\n| 3D NCHW        | NO                 | NCHW, NHWC             | NCHW, NHWC              | f32, f16, bf16            |                                                                                                                                                                                         |\n```\n\n----------------------------------------\n\nTITLE: Running U8S8U8 Backward Convolutions wrt Data (sh)\nDESCRIPTION: This snippet demonstrates how to run a set of u8s8u8 backward convolutions with respect to data. It also shows how to skip specific implementations (ref, x64:gemm).\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_conv.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --conv --dt=u8:s8:u8 --dir=BWD_D \\\n               --skip-impl=ref,x64:gemm --batch=inputs/conv/set_conv_all\n```\n\n----------------------------------------\n\nTITLE: Adding SYCL Subdirectory in CMake\nDESCRIPTION: This snippet checks if either the CPU or GPU runtime uses SYCL or DPCPP. If so, it adds the `sycl` subdirectory to the build, indicating SYCL support is required.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/interface/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(sycl_rt_pattern \"(SYCL|DPCPP)\")\nif(DNNL_CPU_RUNTIME MATCHES ${sycl_rt_pattern} OR DNNL_GPU_RUNTIME MATCHES ${sycl_rt_pattern})\n    add_subdirectory(sycl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting Source Files\nDESCRIPTION: Collects source files (.hpp and .cpp) from the current source directory and subdirectories specified in the DIRS variable. Uses GLOB and GLOB_RECURSE to find files.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\nforeach(d ${DIRS})\n    file(GLOB_RECURSE d_sources\n        ${CMAKE_CURRENT_SOURCE_DIR}/${d}/*.hpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/${d}/*.cpp\n        )\n    list(APPEND SOURCES \"${d_sources}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Including and linking oneDNN library\nDESCRIPTION: This snippet includes the oneDNN library as a subdirectory within the project and links it to the executable. It defines the DNNL_DIR variable to specify the location of the oneDNN source code.  The `include_directories` command adds the oneDNN include directory to the include path, enabling the use of oneDNN headers in the project.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/other/subproject/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# include oneDNN\nset(DNNL_DIR \"../../../\")\nadd_subdirectory(${DNNL_DIR} oneDNN)\ninclude_directories(${DNNL_DIR}/include)\n\nadd_executable(project_app main.c)\ntarget_link_libraries(project_app dnnl)\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library in CMake\nDESCRIPTION: This snippet creates an object library named `${DNNL_LIBRARY_NAME}_cpu_riscv` from the source files listed in the `SOURCES` variable. The `OBJECT` keyword specifies that it's an object library, which contains compiled object files but is not linked into a final executable or shared library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/rv64/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# Build sources into an object library\nset(OBJ_LIB ${DNNL_LIBRARY_NAME}_cpu_riscv)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Including Directories\nDESCRIPTION: This snippet includes the specified directories with host compiler before others.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/gemm/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler_before(${OBJ_LIB} ${ONEDNN_GEMMSTONE_DIR} ${ONEDNN_GEMMSTONE_DIR}/include/gemmstone)\n```\n\n----------------------------------------\n\nTITLE: Including Directories with Host Compiler\nDESCRIPTION: This line includes directories using the `include_directories_with_host_compiler` command, which is likely a custom CMake function. It's used to add the current source directory to the include path, ensuring that the host compiler can find the necessary header files during compilation. The exact implementation details of `include_directories_with_host_compiler` are not shown, but the intent is to include the specified directory.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/sycl/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude_directories_with_host_compiler(${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Running Group Normalizations from Input File\nDESCRIPTION: Runs group normalization benchmarks using shapes defined in an input file with default settings.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/benchdnn/doc/driver_gnorm.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./benchdnn --gnorm --batch=shapes_ci\n```\n\n----------------------------------------\n\nTITLE: Adding Object Files to oneDNN Dependencies in CMake\nDESCRIPTION: This code adds the compiled object files from the object library `${OBJ_LIB}` to the global `DNNL_LIB_DEPS` property.  This ensures that these object files are linked when building the final oneDNN library or executable. The generator expression `$<TARGET_OBJECTS:${OBJ_LIB}>` expands to the list of object files associated with the target `OBJ_LIB`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/rv64/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# Add compiled object files to oneDNN dependencies\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n             $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Applying compiler flags for Intel compiler on Windows with CMake\nDESCRIPTION: This snippet applies a specific compiler flag (/Qlong-double) when using the Intel compiler on Windows, specifically for versions less than 19.1. This addresses a known issue with the compiler.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(WIN32)\n    # Correct 'jnl' macro/jit issue\n    if(${CMAKE_CXX_COMPILER_ID} STREQUAL \"Intel\" AND ${CMAKE_CXX_COMPILER_VERSION} LESS 19.1)\n        append(CMAKE_CXX_FLAGS \"/Qlong-double\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Test Executable Name CMake\nDESCRIPTION: This line sets the name of the test executable to `test_graph_unit_dnnl`. This variable is likely used later when defining the test target.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/backend/dnnl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TEST_EXE test_graph_unit_dnnl)\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files with CMake\nDESCRIPTION: This snippet uses `file(GLOB_RECURSE)` to find all header and source files in the current source directory and its subdirectories.  It searches for files with extensions `.h`, `.hpp`, `.c`, and `.cpp`. The result is stored in the `SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/ocl/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.h\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.c\n    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp\n    )\n```\n\n----------------------------------------\n\nTITLE: Intel MKL-DNN v1.0 Example\nDESCRIPTION: This code snippet illustrates the use of Intel MKL-DNN v1.0. It includes the mkldnn.hpp header, uses the mkldnn namespace, accesses mkldnn_memory_desc_t, and invokes the conv.exec function with the MKLDNN_ARGS_SRC argument.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/doc/advanced/transition-to-dnnl.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"mkldnn.hpp\"\n\nusing namespace mkldnn;\n\nmkldnn_memory_desc_t md;\nif (md.format_kind == mkldnn_blocked) {}\nconv.exec(stream, {{MKLDNN_ARGS_SRC, src}, ...});\n```\n\n----------------------------------------\n\nTITLE: Setting SYCL Compiler Flags in CMake\nDESCRIPTION: This CMake code block conditionally appends compiler flags based on the `DNNL_WITH_SYCL` variable. It checks for CUDA and AMD SYCL support. If CUDA is enabled, it appends `-fsycl-targets=nvptx64-nvidia-cuda,spir64` to the `CMAKE_CXX_FLAGS` and disables linker warnings. If AMD SYCL kernels are enabled, it appends `-fsycl-targets=amdgcn-amd-amdhsa` and `-Xsycl-target-backend --offload-arch=${DNNL_AMD_SYCL_KERNELS_TARGET_ARCH}`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/sycl/api/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(DNNL_WITH_SYCL)\n    if(DNNL_SYCL_GENERIC)\n        CHECK_CXX_COMPILER_FLAG(\"-fsycl -fsycl-targets=nvptx64-nvidia-cuda,spir64\" NVIDIA_TARGET_SUPPORTED)\n    endif()\n\n    # Enable linking SYCL kernels.\n    if(DNNL_SYCL_CUDA OR (DNNL_SYCL_GENERIC AND NVIDIA_TARGET_SUPPORTED))\n        append(CMAKE_CXX_FLAGS \"-fsycl-targets=nvptx64-nvidia-cuda,spir64\")\n        append(CMAKE_CXX_FLAGS \"-Wno-linker-warnings\")\n    endif()\n\n    if(DNNL_AMD_ENABLE_SYCL_KERNELS)\n        append(CMAKE_CXX_FLAGS \"-fsycl-targets=amdgcn-amd-amdhsa -Xsycl-target-backend --offload-arch=${DNNL_AMD_SYCL_KERNELS_TARGET_ARCH}\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library and Setting Properties with CMake\nDESCRIPTION: This snippet first defines a variable `OBJ_LIB` which represents the name of the object library that will be created. The `add_library` command then creates an object library named `${OBJ_LIB}` from the source files listed in the `SOURCES` variable. Finally, `set_property` command appends the objects from the created library to the global `DNNL_LIB_DEPS` property, making them available as dependencies for other DNNL libraries or executables.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/sycl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_gpu_intel_sycl)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<TARGET_OBJECTS:${OBJ_LIB}>)\n```\n\n----------------------------------------\n\nTITLE: Generator Class Usage in JIT Kernels (C++)\nDESCRIPTION: This snippet explains the use of generator classes in oneDNN's JIT kernels.  Specifically, it utilizes the templated `generator_t<ngen::HW>` class, which inherits from `ngen::OpenCLCodeGenerator`. This class controls the emission of instructions for the generated kernels. It depends on the nGEN library.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/jit/README.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\noneDNN uses\nthe templated `generator_t<ngen::HW>` class for jit kernels, which is built off\nof the `ngen::OpenCLCodeGenerator` class.\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example\nDESCRIPTION: This code snippet shows the directory structure for the XPU implementation, highlighting the vendor-agnostic code under the `xpu/` directory and the runtime-specific code under subdirectories like `sycl/` and `ocl/`.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/xpu/README.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nxpu/       # Vendor agnostic code for heterogeneous runtimes\n├── sycl/  # Vendor agnostic code for SYCL runtime\n├── ocl/   # Vendor agnostic code for OpenCL runtime\n└── ...\n```\n\n----------------------------------------\n\nTITLE: Generating Statistics by Primitive Kind in Python\nDESCRIPTION: This snippet shows how to use the `verbose_converter.py` script to gather statistics on `prim_kind` from a oneDNN verbose log file. It accumulates the number of calls and timings for each primitive kind, sorted by total time. The output is piped to `column` for formatted display.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/scripts/verbose_converter/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n> python3  ./scripts/verbose_converter/verbose_converter.py -i input.log -g breakdown -k prim_kind | column -t -s,\n```\n\n----------------------------------------\n\nTITLE: Globbing OpenCL Sources and Headers\nDESCRIPTION: These commands collect all OpenCL source files (.cl) and header files (.h) recursively from the current source directory, storing them in the `CL_SOURCES` and `CL_HEADERS` variables respectively. This prepares the lists for kernel list generation.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/gpu/intel/ocl/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE CL_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/*.cl)\nfile(GLOB_RECURSE CL_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/*.h)\n```\n\n----------------------------------------\n\nTITLE: Adding Object Library CMake\nDESCRIPTION: This snippet creates an object library named `${LIB_PACKAGE_NAME}_cpu_aarch64` using the source files defined in the `SOURCES` variable. It then sets a global property `DNNL_LIB_DEPS` to include the object library's objects, which are required dependencies.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/src/cpu/aarch64/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(OBJ_LIB ${LIB_PACKAGE_NAME}_cpu_aarch64)\nadd_library(${OBJ_LIB} OBJECT ${SOURCES})\nset_property(GLOBAL APPEND PROPERTY DNNL_LIB_DEPS\n    $<$<TARGET_OBJECTS:${OBJ_LIB}>>)\n```\n\n----------------------------------------\n\nTITLE: Globbing source files (Engine Independent)\nDESCRIPTION: This snippet uses the `file(GLOB)` command to find all C++ source files in the current source directory that are considered engine-independent. These are stored in the `TEST_UTILS_ENGINE_INDEPENDENT_SOURCES` variable.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/graph/unit/utils/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB TEST_UTILS_ENGINE_INDEPENDENT_SOURCES\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_attribute_value_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_debug_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_json_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_pattern_matcher_cpu.cpp\n    ${CMAKE_CURRENT_SOURCE_DIR}/test_utils_cpu.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Globbing Test Source Files with CMake\nDESCRIPTION: This snippet uses the file(GLOB) command to collect all C++ source files within the current source directory that match the pattern 'test_*.cpp'. These files are stored in the TEST_SOURCES variable for later use in the executable build process.\nSOURCE: https://github.com/uxlfoundation/onednn/blob/main/tests/gtests/ocl/api/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB TEST_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/test_*.cpp)\n```"
  }
]
[
  {
    "owner": "ph0rk0z",
    "repo": "sageattention2",
    "content": "TITLE: Basic SageAttention Usage Example\nDESCRIPTION: Example showing how to use the sageattn function with q,k,v tensors, supporting both HND and NHD tensor layouts\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sageattention import sageattn\nattn_output = sageattn(q, k, v, tensor_layout=\"HND\", is_causal=False)\n```\n\n----------------------------------------\n\nTITLE: Plug-and-play Attention Replacement\nDESCRIPTION: Code snippet showing how to replace PyTorch's scaled_dot_product_attention with SageAttention for plug-and-play integration\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sageattention import sageattn\nimport torch.nn.functional as F\n\nF.scaled_dot_product_attention = sageattn\n```\n\n----------------------------------------\n\nTITLE: Replacing PyTorch's scaled_dot_product_attention with SageAttention\nDESCRIPTION: Basic integration example showing how to replace PyTorch's F.scaled_dot_product_attention with the sageattn function for faster transformer model inference.\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/example/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sageattention import sageattn\nimport torch.nn.functional as F\n\nF.scaled_dot_product_attention = sageattn\n```\n\n----------------------------------------\n\nTITLE: SageAttention Implementation for CogVideo SAT\nDESCRIPTION: The replacement code using SageAttention that should be used instead of the original PyTorch implementation in CogVideo SAT.\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/example/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nattn_output = sageattn(\n    query_layer, key_layer, value_layer, \n    is_causal=not is_full\n)\n```\n\n----------------------------------------\n\nTITLE: Installing SageAttention from Source\nDESCRIPTION: Commands for installing SageAttention 2.0.1 by cloning the repository and installing from source using pip or setup.py\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/thu-ml/SageAttention.git\ncd sageattention \npip install -e . # or python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Replacing scaled_dot_product_attention in CogVideo SAT\nDESCRIPTION: Code modification for CogVideo SAT showing the original PyTorch implementation that needs to be replaced for SageAttention integration.\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/example/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nattn_output = torch.nn.functional.scaled_dot_product_attention(\n    query_layer, key_layer, value_layer, \n    attn_mask=None,\n    dropout_p=dropout_p,\n    is_causal=not is_full\n)\n```\n\n----------------------------------------\n\nTITLE: Running CogVideo Example with SageAttention\nDESCRIPTION: Command to run the CogVideo example with SageAttention integration using compile mode\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd example\npython sageattn_cogvideo.py --compile\n```\n\n----------------------------------------\n\nTITLE: Running CogvideoX Example with SageAttention\nDESCRIPTION: Command to run the example implementation of SageAttention with CogvideoX, using compilation for faster execution.\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/example/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd example\npython sageattn_cogvideo.py --compile\n```\n\n----------------------------------------\n\nTITLE: Setting Up Parallel SageAttention Inference Environment\nDESCRIPTION: Commands to install the required dependencies for running parallel SageAttention inference, including xDiT and the latest version of diffusers.\nSOURCE: https://github.com/ph0rk0z/sageattention2/blob/main/example/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# install latest xDiT(xfuser).\npip install \"xfuser[flash_attn]\"\n# install latest diffusers (>=0.32.0.dev0), need by latest xDiT.\ngit clone https://github.com/huggingface/diffusers.git\ncd diffusers && python3 setup.py bdist_wheel && cd dist && python3 -m pip install *.whl\n# then run parallel sage attention inference.\n./run_parallel.sh\n```"
  }
]
[
  {
    "owner": "floneum",
    "repo": "floneum",
    "content": "TITLE: Classifying Text with Embeddings and TextClassifier in Rust\nDESCRIPTION: This code demonstrates text classification using embeddings and the `TextClassifier` from the `kalosm_learning` crate. It defines a `SentenceType` enum, creates a dataset of questions and statements, embeds the text using a `Bert` model, trains a classifier on the embeddings, and then prompts the user for input to classify.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# use kalosm_learning::*;\n# #[tokio::main]\n# async fn main() -> Result<(), Box<dyn std::error::Error>> {\n#[derive(Debug, Clone, Copy, Class)]\nenum SentenceType {\n    Question,\n    Statement,\n}\n// Create a dataset for the classifier\nlet bert = Bert::builder()\n    .with_source(BertSource::snowflake_arctic_embed_extra_small())\n    .build()\n    .await?;\nlet mut dataset = TextClassifierDatasetBuilder::<SentenceType, _>::new(&bert);\nconst QUESTIONS: [&str; 10] = [\n    \"What is the capital of France\",\n    \"What is the capital of the United States\",\n    \"What is the best way to learn a new language\",\n    \"What is the best way to learn a new programming language\",\n    \"What is a framework\",\n    \"What is a library\",\n    \"What is a good way to learn a new language\",\n    \"What is a good way to learn a new programming language\",\n    \"What is the city with the most people in the world\",\n    \"What is the most spoken language in the world\",\n];\nconst STATEMENTS: [&str; 10] = [\n    \"The president of France is Emmanuel Macron\",\n    \"The capital of France is Paris\",\n    \"The capital of the United States is Washington, DC\",\n    \"The light bulb was invented by Thomas Edison\",\n    \"The best way to learn a new programming language is to start with the basics and gradually build on them\",\n    \"A framework is a set of libraries and tools that help developers build applications\",\n    \"A library is a collection of code that can be used by other developers\",\n    \"A good way to learn a new language is to practice it every day\",\n    \"The city with the most people in the world is Tokyo\",\n    \"The most spoken language in the United States is English\",\n];\n\nfor question in QUESTIONS {\n    dataset.add(question, SentenceType::Question).await?;\n}\nfor statement in STATEMENTS {\n    dataset.add(statement, SentenceType::Statement).await?;\n}\nlet dev = accelerated_device_if_available()?;\nlet dataset = dataset.build(&dev)?;\n    // Create a classifier\n    let classifier = TextClassifier::<SentenceType>::new(Classifier::new(\n        &dev,\n        ClassifierConfig::new().layers_dims([10]),\n    )?);\n\n    // Train the classifier\n    classifier.train(\n        &dataset, // The dataset to train on\n        100,      // The number of epochs to train for\n        0.0003,   // The learning rate\n        50,       // The batch size\n        |_| {},   // The callback to run as the model trains\n    )?;\n\n    loop {\n        let input = prompt_input(\"Input: \").unwrap();\n        let embedding = bert.embed(input).await?;\n        let output = classifier.run(embedding)?;\n        println!(\"Output: {:?}\", output);\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Retrieval Augmented Generation (RAG) in Rust\nDESCRIPTION: This code snippet demonstrates Retrieval Augmented Generation (RAG) using Kalosm, SurrealDB, and a Llama chat model. It sets up a connection to a SurrealDB database, creates a document table, adds context from URLs, initializes a Llama chat model, prompts the user for a question, searches for relevant context in the database, formats a prompt with the context and question, and uses the Llama model to respond to the user. The system prompt instructs the model to answer questions based on the provided context.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse surrealdb::{engine::local::SurrealKv, Surreal};\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    let exists = std::path::Path::new(\"./db\").exists();\n\n    // Create database connection\n    let db = Surreal::new::<SurrealKv>(\"./db/temp.db\").await?;\n\n    // Select a specific namespace / database\n    db.use_ns(\"test\").use_db(\"test\").await?;\n\n    // Create a table in the surreal database to store the embeddings\n    let document_table = db\n        .document_table_builder(\"documents\")\n        .at(\"./db/embeddings.db\")\n        .build::<Document>()\n        .await?;\n\n    // If the database is new, add documents to it\n    if !exists {\n        std::fs::create_dir_all(\"documents\")?;\n        let context = [\n            \"https://floneum.com/kalosm/docs\",\n            \"https://floneum.com/kalosm/docs/guides/retrieval_augmented_generation\",\n        ]\n        .iter()\n        .map(|url| Url::parse(url).unwrap());\n\n        document_table.add_context(context).await?;\n    }\n\n    // Create a llama chat model\n    let model = Llama::new_chat().await?;\n    let mut chat = model.chat().with_system_prompt(\"The assistant help answer questions based on the context given by the user. The model knows that the information the user gives it is always true.\");\n\n    loop {\n        // Ask the user for a question\n        let user_question = prompt_input(\"\\n> \")?;\n\n        // Search for relevant context in the document engine\n        let context = document_table\n            .search(&user_question)\n            .with_results(1)\n            .await?\n            .into_iter()\n            .map(|document| {\n                format!(\n                    \"Title: {}\\nBody: {}\\n\",\n                    document.record.title(),\n                    document.record.body()\n                )\n            })\n            .collect::<Vec<_>>()\n            .join(\"\\n\");\n\n        // Format a prompt with the question and context\n        let prompt = format!(\n            \"{context}\\n{user_question}\"\n        );\n\n        // Display the prompt to the user for debugging purposes\n        println!(\"{}\", prompt);\n\n        // And finally, respond to the user\n        let mut output_stream = chat(&prompt);\n        print!(\"Bot: \");\n        output_stream.to_std_out().await?;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding-Powered Search with SurrealDB in Rust\nDESCRIPTION: This code illustrates how to use embedding-powered search with SurrealDB in Kalosm for document retrieval.  It establishes a connection to a SurrealDB instance, creates a document table, adds context from a `DocumentFolder`, and performs a search based on user input, returning the nearest 5 results.  It utilizes `tokio` for asynchronous operations.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse surrealdb::{engine::local::SurrealKv, Surreal};\n\n#[tokio::main]\nasync fn main() {\n    // Create database connection\n    let db = Surreal::new::<SurrealKv>(std::env::temp_dir().join(\"temp.db\")).await.unwrap();\n\n    // Select a specific namespace / database\n    db.use_ns(\"search\").use_db(\"documents\").await.unwrap();\n\n    // Create a table in the surreal database to store the embeddings\n    let document_table = db\n        .document_table_builder(\"documents\")\n        .build::<Document>()\n        .await\n        .unwrap();\n\n    // Add documents to the database\n    document_table.add_context(DocumentFolder::new(\"./documents\").unwrap()).await.unwrap();\n\n    loop {\n        // Get the user's question\n        let user_question = prompt_input(\"Query: \").unwrap();\n\n        let nearest_5 = document_table\n            .search(&user_question)\n            .with_results(5)\n            .await\n            .unwrap();\n\n        println!(\"{:?}\", nearest_5);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Text Embeddings with Bert Model in Rust\nDESCRIPTION: This snippet demonstrates how to create text embeddings using the `Bert` model from the `kalosm` crate. It initializes a `Bert` instance, defines a list of sentences, and then generates embeddings for these sentences using the `embed_batch` function. The `tokio` runtime is used for asynchronous execution.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\nlet mut bert = Bert::new().await.unwrap();\nlet sentences = vec![\n    \"Kalosm can be used to build local AI applications\",\n    \"With private LLMs data never leaves your computer\",\n    \"The quick brown fox jumps over the lazy dog\",\n];\nlet embeddings = bert.embed_batch(&sentences).await.unwrap();\n# }\n```\n\n----------------------------------------\n\nTITLE: Text Completion with Constraints using Derived Parser\nDESCRIPTION: This code snippet showcases text completion with constraints using a derived parser. It utilizes a `Pet` struct (defined with the `Parse` derive macro) and generates text conforming to its structure. This ensures the model's output matches the expected format, enabling structured data extraction and processing.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[derive(Parse, Clone, Debug)]\nstruct Pet {\n    name: String,\n    age: u32,\n    description: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new().await.unwrap();\n    // Then create a parser for your data. Any type that implements the `Parse` trait has the `new_parser` method\n    let parser = Pet::new_parser();\n    // Create a text completion stream with the constraints\n    let description = model.complete(\"JSON for an adorable dog named ruffles: \")\n        .with_constraints(parser);\n    // Finally, await the stream to get the parsed response\n    let pet: Pet = description.await.unwrap();\n    println!(\"{pet:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Live Audio Transcription with Whisper in Rust\nDESCRIPTION: This code snippet demonstrates live audio transcription using the Whisper model within the Kalosm library. It initializes the Whisper model, streams audio from the microphone using `MicInput`, transcribes the audio stream in chunks based on voice activity, and prints the transcribed text to the console. It leverages the `kalosm::sound` crate for audio input and processing.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::sound::*;\n\n#[tokio::main]\nasync fn main() -> Result<(), anyhow::Error> {\n    // Create a new whisper model\n    let model = Whisper::new().await?;\n\n    // Stream audio from the microphone\n    let mic = MicInput::default();\n    let stream = mic.stream();\n\n    // The audio into chunks based on voice activity and then transcribe those chunks\n    // The model will transcribe chunks of speech that are separated by silence\n    let mut text_stream = stream.transcribe(model);\n\n    // Finally, print the text to the console\n    text_stream.to_std_out().await.unwrap();\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Text Embedding with Bert Model in Kalosm\nDESCRIPTION: This snippet demonstrates how to use the Bert model for text embedding in Kalosm. It initializes a Bert model, embeds text into a vector space, and calculates the cosine similarity between two embeddings.  Dependencies: tokio, kalosm::language.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_3\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\n#[tokio::main]\nasync fn main() {\n    // First create a model. Bert::new() is a good default embedding model for general tasks\n    let model = Bert::new().await.unwrap();\n    // Then embed some text into the vector space\n    let embedding = model.embed(\"Kalosm is a library for building AI applications\").await.unwrap();\n    // And some more text\n    let embedding = model.embed(prompt_input(\"Text: \").unwrap()).await.unwrap();\n    // You can compare the cosine similarity of the two embeddings to see how similar they are\n    println!(\"cosine similarity: {}\", embedding.cosine_similarity(&embedding));\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Session with Llama in Rust\nDESCRIPTION: This snippet initializes a chat session using the Llama model in the Kalosm library. It creates a chat model, sets a system prompt, and then enters a loop to continuously get input from the user, send it to the chat model, and display the response.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/chat.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\n// Before you create a chat session, you need a model. Llama::new_chat will create a good default chat model.\nlet model = Llama::new_chat().await.unwrap();\n// Then you can build a chat session that uses that model\nlet mut chat = model.chat()\n    // The builder exposes methods for settings like the system prompt and constraints the bot response must follow\n    .with_system_prompt(\"The assistant will act like a pirate\");\n\nloop {\n    // To use the chat session, you need to add messages to it\n    let mut response_stream = chat(&prompt_input(\"\\n> \").unwrap());\n    // And then display the response stream to the user\n    response_stream.to_std_out().await.unwrap();\n}\n# }\n```\n\n----------------------------------------\n\nTITLE: Embedding-Powered Search with Kalosm and SurrealDB\nDESCRIPTION: This snippet demonstrates how to use embedding-powered search with Kalosm and SurrealDB. It creates a database connection, selects a namespace and database, creates a document table, adds documents to the table, and performs a search based on a user's question, returning the nearest 5 results. Dependencies: tokio, kalosm::language, surrealdb.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_6\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\nuse surrealdb::{engine::local::SurrealKv, Surreal};\n\n#[tokio::main]\nasync fn main() {\n    // Create database connection\n    let db = Surreal::new::<SurrealKv>(std::env::temp_dir().join(\"temp.db\")).await.unwrap();\n\n    // Select a specific namespace / database\n    db.use_ns(\"search\").use_db(\"documents\").await.unwrap();\n\n    // Create a table in the surreal database to store the embeddings\n    let document_table = db\n        .document_table_builder(\"documents\")\n        .build::<Document>()\n        .await\n        .unwrap();\n\n    // Add documents to the database\n    document_table.add_context(DocumentFolder::new(\"./documents\").unwrap()).await.unwrap();\n\n    loop {\n        // Get the user's question\n        let user_question = prompt_input(\"Query: \").unwrap();\n\n        let nearest_5 = document_table\n            .search(user_question)\n            .with_results(5)\n            .await\n            .unwrap();\n\n        println!(\"{:?}\", nearest_5);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Local Text Generation with Llama in Rust\nDESCRIPTION: This code snippet demonstrates how to generate text locally using the Llama language model within the Kalosm library. It initializes the Llama model, provides a prompt, and then streams the generated text to standard output. The example leverages the `kalosm::language` crate and `tokio` for asynchronous execution.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let mut llm = Llama::phi_3().await.unwrap();\n    let prompt = \"The following is a 300 word essay about why the capital of France is Paris:\";\n    print!(\"{}\", prompt);\n\n    let mut stream = llm(prompt);\n\n    stream.to_std_out().await.unwrap();\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Text with Bert Model in Rust\nDESCRIPTION: This snippet shows how to embed text using the `Bert` model in Kalosm. It initializes a `Bert` model, embeds text to create an `Embedding`, and calculates the cosine similarity between two embeddings. The `tokio` runtime is used for asynchronous execution.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n#[tokio::main]\nasync fn main() {\n    // First create a model. Bert::new() is a good default embedding model for general tasks\n    let model = Bert::new().await.unwrap();\n    // Then embed some text into the vector space\n    let embedding = model.embed(\"Kalosm is a library for building AI applications\").await.unwrap();\n    // And some more text\n    let embedding = model.embed(prompt_input(\"Text: \").unwrap()).await.unwrap();\n    // You can compare the cosine similarity of the two embeddings to see how similar they are\n    println!(\"cosine similarity: {}\", embedding.cosine_similarity(&embedding));\n}\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Chat Sessions with Kalosm\nDESCRIPTION: This code demonstrates how to save and load a ChatSession using the `to_bytes` and `from_bytes` methods.  It first initializes a Llama chat model, feeds it a question, saves the session to bytes, loads the session from bytes, and then feeds it another question. This allows the model to retain context between sessions.  It depends on the `kalosm::language` crate.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/chat_session.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::io::Write;\n\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new_chat().await.unwrap();\n    let mut chat = llm.chat();\n\n    // Feed some text into the session\n    chat(\"What is the capital of France?\").to_std_out().await.unwrap();\n\n    // Save the session to bytes\n    let session = chat.session().unwrap();\n    let session_as_bytes = session.to_bytes().unwrap();\n    \n    // Load the session from bytes\n    let mut session = LlamaChatSession::from_bytes(&session_as_bytes).unwrap();\n    let mut chat = llm.chat().with_session(session);\n\n    // Feed some more text into the session\n    chat(\"What was my first question?\").to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Structured Generation with Kalosm and Llama Chat Model\nDESCRIPTION: This snippet demonstrates structured generation using Kalosm with a Llama chat model. It defines a `Pet` struct with the `Parse` derive macro for parsing structured data. It then initializes a chat model, creates a parser for the `Pet` struct, defines a task with constraints, and runs the task to generate structured data in the form of a `Pet` struct. Dependencies: tokio, kalosm::language, `Parse` derive macro.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_2\n\nLANGUAGE: Rust\nCODE:\n```\n# use kalosm::language::*;\n# use std::sync::Arc;\n#[derive(Parse, Debug, Clone)]\nstruct Pet {\n    name: String,\n    age: u32,\n    description: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    // First create a model. Chat models tend to work best with structured generation\n    let model = Llama::new_chat().await.unwrap();\n    // Then create a parser for your data. Any type that implements the `Parse` trait has the `new_parser` method\n    let parser = Arc::new(Pet::new_parser());\n    // Then create a task with the parser as constraints\n    let task = model.task(\"You generate realistic JSON placeholders\")\n        .with_constraints(parser);\n    // Finally, run the task\n    let pet: Pet = task(\"Generate a pet in the form {\\\"name\\\": \\\"Pet name\\\", \\\"age\\\": 0, \\\"description\\\": \\\"Pet description\\\"}\").await.unwrap();\n    println!(\"{pet:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Llama Model in Kalosm\nDESCRIPTION: This Rust code snippet demonstrates text generation using the `Llama` model from the `kalosm` crate. It initializes the model, defines a prompt, and streams the generated text to standard output. The code requires the `tokio` runtime for asynchronous execution. The `no_run` tag indicates that this example may require setup or external resources to run correctly.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse std::io::Write;\n\nuse kalosm::{*, language::*};\n\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let prompt = \"The following is a 300 word essay about Paris:\";\n    print!(\"{}\", prompt);\n\n    let mut stream = llm(prompt);\n\n    stream.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Cloud Model Integration with OpenAI in Rust\nDESCRIPTION: This code snippet demonstrates how to integrate with cloud models like GPT-4 using the Kalosm library. It uses the `OpenAICompatibleChatModel` builder to set up a chat model, and then it initiates a chat session to query the model and stream the response to the standard output. Note that the `OPENAI_API_KEY` environment variable must be set for this example to work.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n// You must set the environment variable OPENAI_API_KEY (https://platform.openai.com/account/api-keys) to run this example.\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let mut llm = OpenAICompatibleChatModel::builder()\n        .with_gpt_4o_mini()\n        .build();\n\n    let mut chat = llm.chat();\n\n    chat(\"What is the capital of France?\").to_std_out().await?;\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Text Embeddings with Cosine Similarity in Rust\nDESCRIPTION: This code calculates the cosine similarity between pairs of text embeddings generated by the `Bert` model. It iterates through the embeddings, computes the cosine similarity between each pair, and prints a similarity score along with the corresponding sentences. The cosine similarity is a measure of the angle between two vectors, indicating how similar their directions are.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\n# let mut bert = Bert::new().await.unwrap();\n# let sentences = vec![\n#     \"Kalosm can be used to build local AI applications\",\n#     \"With private LLMs data never leaves your computer\",\n#     \"The quick brown fox jumps over the lazy dog\",\n# ];\n# let embeddings = bert.embed_batch(&sentences).await.unwrap();\n// Find the cosine similarity between each pair of sentences\nlet n_sentences = sentences.len();\nfor (i, e_i) in embeddings.iter().enumerate() {\n    for j in (i + 1)..n_sentences {\n        let e_j = embeddings.get(j).unwrap();\n        let cosine_similarity = e_j.cosine_similarity(e_i);\n        println!(\"score: {cosine_similarity:.2} '{}' '{}'\", sentences[i], sentences[j])\n    }\n}\n# }\n```\n\n----------------------------------------\n\nTITLE: Deriving a Parser for Structured Data\nDESCRIPTION: This snippet defines a `Pet` struct and derives a parser for it using the `Parse` derive macro from the `kalosm::language` crate.  The struct is annotated with `#[derive(Parse, Clone)]`, which automatically generates the necessary parsing logic. This allows for structured generation of data that matches the struct's fields. Requires the `kalosm::language` crate.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n#[derive(Parse, Clone)]\nstruct Pet {\n    name: String,\n    age: u32,\n    description: String,\n}\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio to Text with Whisper Model (Rust)\nDESCRIPTION: This snippet demonstrates transcribing audio to text using the Whisper model.  It gets audio from the microphone, streams it, transcribes the audio to text using `AsyncSourceTranscribeExt::transcribe`, and prints the resulting text to standard output.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-sound/README.md#_snippet_2\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::sound::*;\n#[tokio::main]\nasync fn main() {\n    // Get the default microphone input\n    let mic = MicInput::default();\n    // Stream the audio from the microphone\n    let stream = mic.stream();\n    // Transcribe the audio into text with the default Whisper model\n    let mut transcribe = stream.transcribe(Whisper::new().await.unwrap());\n    // Print the text as it is streamed in\n    transcribe.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Tasks with Constraints Example\nDESCRIPTION: This example demonstrates how to use tasks with constraints to generate structured data. It defines a `Pet` struct, derives a parser for it, and creates a task that generates JSON placeholders. The `with_constraints` method is used to ensure that the output of the task conforms to the `Pet` struct's format.  Requires the `kalosm::language` crate, the tokio runtime, and the `Parse` derive macro.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::sync::Arc;\n\n#[derive(Parse, Clone, Debug)]\nstruct Pet {\n    name: String,\n    age: u32,\n    description: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new_chat().await.unwrap();\n    // Then create a parser for your data.\n    // Any type that implements the `Parse` trait has the `new_parser` method\n    let parser = Pet::new_parser();\n    // Create a task with the constraints\n    let task = model.task(\"You generate realistic JSON placeholders for pets in the form {\\\"name\\\": \\\"Pet name\\\", \\\"age\\\": 0, \\\"description\\\": \\\"Pet description\\\"}\")\n            // The task constraints must be clone. If they don't implement Clone, you can wrap them in an Arc\n        .with_constraints(Arc::new(parser));\n    // Then run the task\n    let pet: Pet = task(\"Ruffles is a 3 year old adorable dog\").await.unwrap();\n    println!(\"{pet:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing a Task with ChatModelExt\nDESCRIPTION: This code snippet demonstrates how to create a task using the `ChatModelExt::task` method in Kalosm.  It initializes a task with a system prompt to act as an editing assistant and then calls the task with an example text. The resulting stream of suggestions is then printed to the standard output. Requires the `kalosm::language` crate and the tokio runtime.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() {\n    let model = Llama::new_chat().await.unwrap();\n    let task = model.task(\"You are an editing assistant who offers suggestions for improving the quality of the text. You will be given some text and will respond with a list of suggestions for how to improve the text.\");\n    let mut stream = task(\"this isnt correct. or is it?\");\n    stream.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Text Completion as a Stream with Llama Model\nDESCRIPTION: This example shows how to use the text completion response as a stream of tokens.  It initializes a Llama model, provides a prompt, and then streams the completion token by token, printing each to standard output.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::io::Write;\n\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let prompt = \"The following is a 300 word essay about why the capital of France is Paris:\";\n    print!(\"{prompt}\");\n    let mut completion = llm\n        .complete(prompt);\n    while let Some(token) = completion.next().await {\n        print!(\"{token}\");\n        std::io::stdout().flush().unwrap();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Detecting Voice Activity in Audio Stream (Rust)\nDESCRIPTION: This snippet demonstrates how to detect voice activity in an audio stream using `MicInput`, `stream`, and `voice_activity_stream`. It streams audio from the microphone, detects voice activity using a VAD model, and prints the probability of voice activity for each chunk.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-sound/README.md#_snippet_0\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::sound::*;\n#[tokio::main]\nasync fn main() {\n    // Get the default microphone input\n    let mic = MicInput::default();\n    // Stream the audio from the microphone\n    let stream = mic.stream();\n    // Detect voice activity in the audio stream\n    let mut vad = stream.voice_activity_stream();\n    while let Some(input) = vad.next().await {\n        println!(\"Probability: {}\", input.probability);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Powered Search Index in Rust\nDESCRIPTION: This code snippet demonstrates how to create an embedding-powered search index using Kalosm and SurrealDB. It establishes a connection to a SurrealDB database, creates a table to store document embeddings, adds documents to the database from a local folder, prompts the user for a query, searches for semantically similar documents, and prints the nearest matches. The example uses a local SurrealDB instance and assumes a `documents` folder exists.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse surrealdb::{engine::local::SurrealKv, Surreal};\n\n#[tokio::main]\nasync fn main() {\n    // Create database connection\n    let db = Surreal::new::<SurrealKv>(std::env::temp_dir().join(\"temp.db\")).await.unwrap();\n\n    // Select a specific namespace / database\n    db.use_ns(\"search\").use_db(\"documents\").await.unwrap();\n\n    // Create a table in the surreal database to store the embeddings\n    let document_table = db\n        .document_table_builder(\"documents\")\n        .build::<Document>()\n        .await\n        .unwrap();\n\n    // Add documents to the database\n    document_table.add_context(DocumentFolder::new(\"./documents\").unwrap()).await.unwrap();\n\n    loop {\n        // Get the user's question\n        let user_question = prompt_input(\"Query: \").unwrap();\n\n        let nearest_5 = document_table\n            .search(&user_question)\n            .with_results(5)\n            .await\n            .unwrap();\n\n        println!(\"{:?}\", nearest_5);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Segmenting Images with SegmentAnything using Points in Rust\nDESCRIPTION: This code snippet shows how to segment an image using the SegmentAnything model in Rust, focusing on specific points. It initializes the model, loads an image, and defines a point of interest. It then runs the segmentation and saves the result to a file named \"out.png\". It relies on the `kalosm::vision` and `image` crates.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/vision.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::vision::*;\n\nlet model = SegmentAnything::builder().build().unwrap();\nlet image = image::open(\"examples/landscape.jpg\").unwrap();\nlet x = image.width() / 2;\nlet y = image.height() / 4;\nlet images = model\n    .segment_from_points(\n        SegmentAnythingInferenceSettings::new(image)\n            .add_goal_point(x, y),\n    )\n    .unwrap();\n\nimages.save(\"out.png\").unwrap();\n```\n\n----------------------------------------\n\nTITLE: Segmenting Images with SegmentAnything in Rust\nDESCRIPTION: This snippet demonstrates how to segment an image into objects using the SegmentAnything model in the Kalosm Vision framework. It loads an image, initializes the model, and segments the image based on a point within the image. The segmented image is then saved as \"out.png\". It relies on `kalosm::vision` and `image` crates.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-vision/README.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::vision::*;\n\nlet model = SegmentAnything::builder().build().unwrap();\nlet image = image::open(\"examples/landscape.jpg\").unwrap();\nlet x = image.width() / 2;\nlet y = image.height() / 4;\nlet images = model\n    .segment_from_points(\n        SegmentAnythingInferenceSettings::new(image)\n            .add_goal_point(x, y),\n    )\n    .unwrap();\n\nimages.save(\"out.png\").unwrap();\n```\n\n----------------------------------------\n\nTITLE: Training Text Classifier in Rust with Kalosm Learning\nDESCRIPTION: This code snippet focuses on training a `TextClassifier` model using a pre-built dataset.  The dataset, representing questions and statements, has been generated in the prior code block. It initializes a `Classifier` with specified configuration and trains it using the prepared dataset, training for a specified number of epochs with a set learning rate and batch size.  The parameters define training behavior.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# use kalosm_learning::*;\n# #[tokio::main]\n# async fn main() -> Result<(), Box<dyn std::error::Error>> {\n# #[derive(Debug, Clone, Copy, Class)]\n# enum SentenceType {\n#     Question,\n#     Statement,\n# }\n# // Create a dataset for the classifier\n# let bert = Bert::builder()\n#     .with_source(BertSource::snowflake_arctic_embed_extra_small())\n#     .build()\n#     .await?;\n# let mut dataset = TextClassifierDatasetBuilder::<SentenceType, _>::new(&bert);\n# const QUESTIONS: [&str; 10] = [\n#     \"What is the capital of France\",\n#     \"What is the capital of the United States\",\n#     \"What is the best way to learn a new language\",\n#     \"What is the best way to learn a new programming language\",\n#     \"What is a framework\",\n#     \"What is a library\",\n#     \"What is a good way to learn a new language\",\n#     \"What is a good way to learn a new programming language\",\n#     \"What is the city with the most people in the world\",\n#     \"What is the most spoken language in the world\",\n# ];\n# const STATEMENTS: [&str; 10] = [\n#     \"The president of France is Emmanuel Macron\",\n#     \"The capital of France is Paris\",\n#     \"The capital of the United States is Washington, DC\",\n#     \"The light bulb was invented by Thomas Edison\",\n#     \"The best way to learn a new programming language is to start with the basics and gradually build on them\",\n#     \"A framework is a set of libraries and tools that help developers build applications\",\n#     \"A library is a collection of code that can be used by other developers\",\n#     \"A good way to learn a new language is to practice it every day\",\n#     \"The city with the most people in the world is Tokyo\",\n#     \"The most spoken language in the United States is English\",\n# ];\n# \n# for question in QUESTIONS {\n#     dataset.add(question, SentenceType::Question).await?;\n# }\n# for statement in STATEMENTS {\n#     dataset.add(statement, SentenceType::Statement).await?;\n# }\n\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Llama Model in Kalosm\nDESCRIPTION: This snippet demonstrates how to use the Llama model for text generation in Kalosm. It initializes a Llama model, provides a prompt, and streams the generated text to the console. It uses `tokio` for asynchronous execution and the `kalosm::language` crate for language model functionalities. Dependencies: tokio, kalosm::language.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_0\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let prompt = \"The following is a 300 word essay about why the capital of France is Paris:\";\n    print!(\"{prompt}\");\n    // Any model that implements the [`TextCompletionModel`] trait can be used to stream text\n    let mut stream = llm.complete(prompt);\n    // You can then use the stream however you need. to_std_out will print the text to the console as it is generated\n    stream.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Text Completion Sessions with Llama in Rust\nDESCRIPTION: This snippet demonstrates how to save a text completion session to bytes and load it back using the `to_bytes` and `from_bytes` methods in kalosm. This allows caching of session state to avoid re-processing text when the session is resumed. It uses the Llama model.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion_session.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::io::Write;\n\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let mut session = llm.new_session().unwrap();\n\n    // Feed some text into the session\n    llm.stream_text_with_callback(&mut session, \"The capital of France is \", GenerationParameters::new().with_max_length(0), |_| Ok(())).await.unwrap();\n\n    // Save the session to bytes\n    let session_as_bytes = session.to_bytes().unwrap();\n    \n    // Load the session from bytes\n    let mut session = LlamaSession::from_bytes(&session_as_bytes).unwrap();\n\n    // Feed some more text into the session\n    llm.stream_text_with_callback(&mut session, \"The capital of France is \", GenerationParameters::new(), |token| {println!(\"{token}\"); Ok(())}).await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Image Generation with Wuerstchen in Rust\nDESCRIPTION: This code snippet demonstrates image generation using the Wuerstchen model within the Kalosm library. It initializes the Wuerstchen model, sets up inference settings with a prompt, runs the model to generate images, and saves the generated images to files with sequential numbering.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::vision::*;\n\n#[tokio::main]\nasync fn main() {\n    let model = Wuerstchen::new().await.unwrap();\n    let settings = WuerstchenInferenceSettings::new(\n        \"a cute cat with a hat in a room covered with fur with incredible detail\",\n    );\n    let mut images = model.run(settings);\n    while let Some(image) = images.next().await {\n        if let Some(buf) = image.generated_image() {\n            buf.save(&format!(\"{}.png\",image.sample_num())).unwrap();\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Text Completion with Llama Model (Async)\nDESCRIPTION: This code snippet demonstrates the simplest way to use a text completion model. It initializes a Llama model, provides a prompt, and awaits the completion to print the full response.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let prompt = \"The following is a 300 word essay about why the capital of France is Paris:\";\n    print!(\"{prompt}\");\n    let mut completion = llm\n        .complete(prompt)\n        .await\n        .unwrap();\n    println!(\"{completion}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Parser from a Regex\nDESCRIPTION: This example shows how to create a parser using a regular expression with the `RegexParser` from the `kalosm::language` crate. It defines a regex pattern to extract pet data from a string in the format `[(\\w+), (\\d+), (\\w+)]`. The resulting parser is then used with a task to generate output that conforms to the specified regex. Unlike derived and custom parsers, regex parsers do not provide a useful output type. Requires the `kalosm::language` crate and the tokio runtime.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new_chat().await.unwrap();\n    // Then create a parser for your data. Any\n    let parser = RegexParser::new(r\"\\[(\\w+), (\\d+), (\\w+)\\]\").unwrap();\n    // Create a task with the constraints\n    let task = model.task(\"You generate realistic JSON placeholders for pets in the form [\\\"Pet name\\\", age number, \\\"Pet description\\\"]\")\n        // The task constraints must be clone. If they don't implement Clone, you can wrap them in an Arc\n        .with_constraints(Arc::new(parser));\n    // Finally, run the task. Unlike derived and custom parsers, regex parsers do not provide a useful output type\n    task(\"Ruffles is a 3 year old adorable dog\").to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Rechunking Audio Based on Voice Activity (Rust)\nDESCRIPTION: This snippet illustrates how to rechunk an audio stream based on voice activity using `rechunk_voice_activity`. It streams audio from the microphone, detects voice activity, chunks the audio into segments of speech, and then prints the duration of each new chunk.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-sound/README.md#_snippet_1\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::sound::*;\nuse rodio::Source;\n#[tokio::main]\nasync fn main() {\n    // Get the default microphone input\n    let mic = MicInput::default();\n    // Stream the audio from the microphone\n    let stream = mic.stream();\n    // Chunk the audio into chunks of speech\n    let vad = stream.voice_activity_stream();\n    let mut audio_chunks = vad.rechunk_voice_activity();\n    // Print the chunks as they are streamed in\n    while let Some(input) = audio_chunks.next().await {\n        println!(\"New voice activity chunk with duration {:?}\", input.total_duration());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Parser from a Regex\nDESCRIPTION: This code snippet demonstrates creating a parser from a regex. It defines a regex pattern to extract three groups: a string, a number, and another string. This parser is then used as a constraint for text completion.  Unlike derived or combinator parsers, the result of this parser is a stream of tokens and not directly a structured type.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new().await.unwrap();\n    // Then create a parser for your data. Any\n    let parser = RegexParser::new(r\"\\[(\\w+), (\\d+), (\\w+)\\]\").unwrap();\n    // Create a text completion stream with the constraints\n    let mut description = model.complete(\"JSON for an adorable dog named ruffles in the form [\\\"Pet name\\\", age number, \\\"Pet description\\\"]:\")\n        .with_constraints(parser);\n    // Finally, run the task. Unlike derived and custom parsers, regex parsers do not provide a useful output type\n    description.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Chat Sessions with Kalosm\nDESCRIPTION: This code demonstrates how to clone a ChatSession using the `try_clone` method. It initializes a Llama chat model, feeds it a question, clones the session, and then feeds the cloned session another question. This allows for branching conversations or maintaining a backup of the session state.  It relies on the `kalosm::language` crate.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/chat_session.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::io::Write;\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new_chat().await.unwrap();\n    let mut chat = llm.chat();\n    // Feed some text into the session\n    chat(\"What is the capital of France?\").to_std_out().await.unwrap();\n    let mut session = chat.session().unwrap();\n    // Clone the session\n    let cloned_session = session.try_clone().unwrap();\n    // Feed some more text into the cloned session\n    let mut chat = llm.chat().with_session(cloned_session);\n    chat(\"What was my first question?\").to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Gathering Context from Various Sources in Rust\nDESCRIPTION: This code snippet shows how to gather text data from different sources using Kalosm. It demonstrates reading from an RSS feed, a local folder of documents, a website, and a search engine. It parses the content from each source and prints the title and body of a document. For the search engine query, it requires the `SERPER_API_KEY` environment variable to be set.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::convert::TryFrom;\nuse std::path::PathBuf;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Read an RSS stream\n    let nyt = RssFeed::new(Url::parse(\"https://rss.nytimes.com/services/xml/rss/nyt/US.xml\").unwrap());\n    // Read a local folder of documents\n    let mut documents = DocumentFolder::try_from(PathBuf::from(\"./documents\")).unwrap();\n    // Read a website (either from the raw HTML or inside of a headless browser)\n    let page = Page::new(Url::parse(\"https://www.nytimes.com/live/2023/09/21/world/zelensky-russia-ukraine-news\").unwrap(), BrowserMode::Static).unwrap();\n    let document = page.article().await.unwrap();\n    println!(\"Title: {}\", document.title());\n    println!(\"Body: {}\", document.body());\n    // Read pages from a search engine (You must have the SERPER_API_KEY environment variable set to run this example)\n    let query = \"What is the capital of France?\";\n    let api_key = std::env::var(\"SERPER_API_KEY\").unwrap();\n    let search_query = SearchQuery::new(query, &api_key, 5);\n    let documents = search_query.into_documents().await.unwrap();\n    let mut text = String::new();\n    for document in documents {\n        for word in document.body().split(' ').take(300) {\n            text.push_str(word);\n            text.push(' ');\n        }\n        text.push('\\n');\n    }\n    println!(\"{}\", text);\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Text Classifier - Rust\nDESCRIPTION: This snippet creates a text classifier with a specified configuration, including the device, and layer dimensions.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_7\n\nLANGUAGE: Rust\nCODE:\n```\nlet classifier = TextClassifier::<SentenceType>::new(Classifier::new(\n    &dev,\n    ClassifierConfig::new().layers_dims([10]),\n)?);\n```\n\n----------------------------------------\n\nTITLE: Structured Generation with LLMs in Rust\nDESCRIPTION: This code snippet illustrates how to use LLMs for structured generation with Kalosm, creating arbitrary data types from natural language input. It defines a custom `Response` struct and an enum `Class` with the `Parse` derive macro to use them as constraints. It then sets up a task using the Llama model, applies constraints via `with_constraints`, and runs the task to parse user input into the defined structure, printing the resulting structured data.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::sync::Arc;\n\n// First, derive an efficient parser for your structured data\n#[derive(Parse, Clone, Debug)]\nenum Class {\n    Thing,\n    Person,\n    Animal,\n}\n\n#[derive(Parse, Clone, Debug)]\nstruct Response {\n    classification: Class,\n}\n\n#[tokio::main]\nasync fn main() {\n    // Then set up a task with a prompt and constraints\n    let llm = Llama::new_chat().await.unwrap();\n    let task = llm.task(\"You classify the user's message as about a person, animal or thing in a JSON format\")\n        .with_constraints(Arc::new(Response::new_parser()));\n\n    // Finally, run the task\n    let response = task(\"The Kalosm library lets you create structured data from natural language inputs\").await.unwrap();\n    println!(\"{:?}\", response);\n}\n```\n\n----------------------------------------\n\nTITLE: Training Text Classifier - Rust\nDESCRIPTION: This snippet trains the text classifier on a given dataset, specifying the number of epochs, learning rate, and batch size. It also includes a callback function that is executed during training.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_8\n\nLANGUAGE: Rust\nCODE:\n```\nclassifier.train(\n    &dataset, // The dataset to train on\n    100,      // The number of epochs to train for\n    0.0003,   // The learning rate\n    50,       // The batch size\n    |_| {},   // The callback to run as the model trains\n)?;\n```\n\n----------------------------------------\n\nTITLE: Deriving a Parser from a Struct for Structured Generation\nDESCRIPTION: This example shows how to derive a parser for structured generation using a struct with the `Parse` derive macro. It defines a `Pet` struct with fields for name, age, and description, and then derives the `Parse` trait. The code uses this parser with a Llama model to generate structured text that conforms to the `Pet` struct's format.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(Parse, Clone)]\nstruct Pet {\n    name: String,\n    age: u32,\n    description: String,\n}\n```\n\n----------------------------------------\n\nTITLE: Structured Text Completion with Derived Parser\nDESCRIPTION: This code snippet demonstrates structured text completion using a derived parser for the `Pet` struct. It initializes a Llama model, creates a parser from the `Pet` struct using `Pet::new_parser()`, and then uses the parser as a constraint for the text completion. The resulting completion is parsed into a `Pet` struct and printed.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new().await.unwrap();\n    // Then create a parser for your data. Any type that implements the `Parse` trait has the `new_parser` method\n    let parser = Pet::new_parser();\n    // Create a text completion stream with the constraints\n    let description = model.complete(\"JSON for an adorable dog named ruffles: \")\n        .with_constraints(parser);\n    // Finally, await the stream to get the parsed response\n    let pet: Pet = description.await.unwrap();\n    println!(\"{pet:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Chat Session in Rust\nDESCRIPTION: This snippet demonstrates how to save and load a chat session to/from the filesystem using Kalosm. It attempts to load a previous session from a file, and if successful, continues the chat from that session. After the chat, it saves the session to the file for later use. This allows preserving the chat history.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/chat.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\n// First, create a model to chat with\nlet model = Llama::new_chat().await.unwrap();\n// Then try to load the chat session from the filesystem\nlet save_path = std::path::PathBuf::from(\"./chat.llama\");\nlet mut chat = model.chat();\nif let Some(old_session) = std::fs::read(&save_path)\n    .ok()\n    .and_then(|bytes| LlamaChatSession::from_bytes(&bytes).ok())\n{\n    chat = chat.with_session(old_session);\n}\n\n// Then you can add messages to the chat session as usual\nlet mut response_stream = chat(&prompt_input(\"\\n> \").unwrap());\n// And then display the response stream to the user\nresponse_stream.to_std_out().await.unwrap();\n\n// After you are done, you can save the chat session to the filesystem\nlet session = chat.session().unwrap();\nlet bytes = session.to_bytes().unwrap();\nstd::fs::write(&save_path, bytes).unwrap();\n# }\n```\n\n----------------------------------------\n\nTITLE: Task Definition and Execution with Kalosm Chat Model\nDESCRIPTION: This snippet shows how to define and execute a task using a chat model in Kalosm. It initializes a Llama chat model, creates a task with a description, and runs the task with input text. The output is then streamed to the console. Dependencies: tokio, kalosm::language.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_1\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\n#[tokio::main]\nasync fn main() {\n    // Create a new model\n    let model = Llama::new_chat().await.unwrap();\n    // Create a new task that summarizes text\n    let task = model.task(\"You take a long description and summarize it into a single short sentence\");\n    let mut output = task(\"You can define a Task with a description then run it with an input. The task will cache the description to repeated calls faster. Tasks work with chat models.\");\n    // Then stream the output to the console\n    output.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Create a New Cargo Project\nDESCRIPTION: Creates a new Rust project named `next-gen-ai` and navigates into the project directory. This assumes that rust and cargo are already installed.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncargo new next-gen-ai\ncd ./next-gen-ai\n```\n\n----------------------------------------\n\nTITLE: Searching Similar Text with VectorDB in Rust\nDESCRIPTION: This snippet creates a vector database using `VectorDB` from the `kalosm` crate. It embeds sentences using a `Bert` model optimized for search, stores the embeddings in the database along with a mapping to their original text. It then embeds a query and searches for the closest matching embedding in the database, printing the distance and corresponding text of the closest match.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n# use std::collections::HashMap;\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() -> anyhow::Result<()> {\n// Create a good default Bert model for search\nlet bert = Bert::new_for_search().await?;\nlet sentences = [\n    \"Kalosm can be used to build local AI applications\",\n    \"With private LLMs data never leaves your computer\",\n    \"The quick brown fox jumps over the lazy dog\",\n];\n// Embed sentences into the vector space\nlet embeddings = bert.embed_batch(sentences).await?;\nprintln!(\"embeddings {:?}\", embeddings);\n\n// Create a vector database from the embeddings along with a map between the embedding ids and the sentences\nlet db = VectorDB::new()?;\nlet embeddings = db.add_embeddings(embeddings)?;\nlet embedding_id_to_sentence: HashMap<EmbeddingId, &str> =\n    HashMap::from_iter(embeddings.into_iter().zip(sentences));\n\n// Embed a query into the vector space. We use `embed_query` instead of `embed` because some models embed queries differently than normal text.\nlet embedding = bert.embed_query(\"What is Kalosm?\").await?;\nlet closest = db.search(&embedding).run()?;\nif let [closest] = closest.as_slice() {\n    let distance = closest.distance;\n    let text = embedding_id_to_sentence.get(&closest.value).unwrap();\n    println!(\"distance: {distance}\");\n    println!(\"closest:  {text}\");\n}\n# Ok(())\n# }\n```\n\n----------------------------------------\n\nTITLE: Run the Cargo Project in Release Mode\nDESCRIPTION: This command builds and runs the Rust project in release mode, optimizing for performance. The release build is placed in the `target/release` directory.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncargo run --release\n```\n\n----------------------------------------\n\nTITLE: Building the UI - Shell\nDESCRIPTION: These commands build the UI for the Floneum project. The first command uses `tailwindcss` to process the input CSS file and output the generated CSS to a public directory, using a watch mode for development. The second command uses `cargo run` to run the project in release mode for a specific target architecture. It assumes `cargo` is installed and configured correctly.\nSOURCE: https://github.com/floneum/floneum/blob/main/floneum/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnpx tailwindcss -i ./input.css -o ./public/tailwind.css --watch\ncargo run --release --target aarch64-apple-darwin # Or whatever the target triple for your current device is\n```\n\n----------------------------------------\n\nTITLE: Semantic Chunking with Kalosm\nDESCRIPTION: This snippet shows how to perform semantic chunking of documents using Kalosm. It initializes a Bert model, creates a `DocumentFolder`, chunks the documents into semantically similar chunks, and prints the chunked results. Dependencies: tokio, kalosm::language.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_5\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // First, create an embedding model for semantic chunking\n    let model = Bert::new().await?;\n    // Then create a document folder with some documents\n    let documents = DocumentFolder::new(\"./documents\")?.into_documents().await?;\n    // Then chunk the documents into sentences\n    let chunked = SemanticChunker::new().chunk_batch(&documents, &model).await?;\n    println!(\"{:?}\", chunked);\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Kalosm and Tokio as Dependencies\nDESCRIPTION: This shell command adds the `kalosm` and `tokio` crates as dependencies to the current Rust project using Cargo. The `--features language` flag enables language model features in Kalosm. Tokio is added with the `full` feature set for asynchronous runtime support. Optional accelerator features like `metal`, `cuda`, or `mkl` can be used depending on hardware support.\nSOURCE: https://github.com/floneum/floneum/blob/main/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n# You can use `--features language,metal`, `--features language,cuda`, or `--features language,mkl` if your machine supports an accelerator\ncargo add kalosm --features language\ncargo add tokio --features full\n```\n\n----------------------------------------\n\nTITLE: Generating Structured Data with a Task and Parser in Rust\nDESCRIPTION: This code demonstrates how to use a parser to generate structured data from a language model using the Kalosm framework. It creates a `Llama` chat model, defines a parser for a `Pet` struct, creates a task with constraints, and runs the task to generate a `Pet` instance. It leverages `tokio` for asynchronous operations and prints the resulting `Pet` to the console.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# use std::sync::Arc;\n# #[derive(Parse, Debug, Clone)]\n# struct Pet {\n#     name: String,\n#     age: u32,\n#     description: String,\n# }\n#[tokio::main]\nasync fn main() {\n    // First create a model. Chat models tend to work best with structured generation\n    let model = Llama::new_chat().await.unwrap();\n    // Then create a parser for your data. Any type that implements the `Parse` trait has the `new_parser` method\n    let parser = Pet::new_parser();\n    // Then create a task with the parser as constraints\n    let task = model.task(\"You generate realistic JSON placeholders\")\n        .with_constraints(Arc::new(parser));\n    // Finally, run the task\n    let pet: Pet = task(\"Generate a pet in the form {\\\"name\\\": \\\"Pet name\\\", \\\"age\\\": 0, \\\"description\\\": \\\"Pet description\\\"}\").await.unwrap();\n    println!(\"{pet:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Text from a Language Model in Rust\nDESCRIPTION: This code snippet demonstrates how to stream text from a language model using the `ModelExt::stream_text` method in Kalosm. It initializes a `Llama` model, defines a prompt, and streams the generated text to standard output. It uses the `tokio` runtime for asynchronous operations.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let prompt = \"The following is a 300 word essay about why the capital of France is Paris:\";\n    print!(\"{prompt}\");\n    // Any model that implements the [`TextCompletionModel`] trait can be used to stream text\n    let mut stream = llm\n        .complete(prompt);\n    // You can then use the stream however you need. to_std_out will print the text to the console as it is generated\n    stream.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Chat Session with Constraints in Rust\nDESCRIPTION: This snippet shows how to use constraints with a chat session to guide the model's response in Kalosm. It creates a constraint that forces the assistant to always start its response with \"Yes!\". The constraint is then applied to the chat session, ensuring all responses adhere to the specified format.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/chat.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\nlet model = Llama::new_chat().await.unwrap();\n// Create constraints that parses Yes! and then stops on the end of the assistant's response\nlet constraints = LiteralParser::new(\"Yes!\")\n    .then(model.default_assistant_constraints());\n// Create a chat session with the model and the constraints\nlet mut chat = model.chat();\n\n// Chat with the user\nloop {\n    let mut output_stream = chat(&prompt_input(\"\\n> \").unwrap()).with_constraints(constraints.clone());\n    output_stream.to_std_out().await.unwrap();\n}\n# }\n```\n\n----------------------------------------\n\nTITLE: Creating a Parser from Prebuilt Combinators\nDESCRIPTION: This code snippet demonstrates how to create a parser from prebuilt combinators provided by Kalosm. It constructs a parser that expects a format like `[string, u8, string]`. The parser is then used to constrain the output of a Llama model, parsing the completed text into a tuple of `((String, u8), String)`.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new().await.unwrap();\n    // Then create a parser for your custom format\n    let parser = LiteralParser::from(\"[\")\n        .ignore_output_then(String::new_parser())\n        .then_literal(\", \")\n        .then(u8::new_parser())\n        .then_literal(\", \")\n        .then(String::new_parser())\n        .then_literal(\"]\");\n    // Create a text completion stream with the constraints\n    let description = model.complete(\"JSON for an adorable dog named ruffles: \")\n        .with_constraints(parser);\n    // Finally, await the stream to get the parsed response\n    let ((name, age), description) = description.await.unwrap();\n    println!(\"{name} {age} {description}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Images from Text with Wuerstchen in Rust\nDESCRIPTION: This snippet demonstrates how to generate images from text using the Wuerstchen model in the Kalosm Vision framework. It initializes the model, sets inference settings with a text prompt, and iterates through the generated images, saving each as a PNG file. It depends on `futures_util` and `kalosm_vision` crates.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-vision/README.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse futures_util::StreamExt;\nuse kalosm_vision::{Wuerstchen, WuerstchenInferenceSettings};\n\n#[tokio::main]\nasync fn main() {\n    let model = Wuerstchen::builder().build().await.unwrap();\n    let settings = WuerstchenInferenceSettings::new(\n        \"a cute cat with a hat in a room covered with fur with incredible detail\",\n    );\n\n    let mut images = model.run(settings);\n    while let Some(image) = images.next().await {\n        if let Some(buf) = image.generated_image() {\n            buf.save(&format!(\"{}.png\", image.sample_num())).unwrap();\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Image Segmentation with SegmentAnything in Rust\nDESCRIPTION: This Rust code snippet demonstrates how to perform image segmentation using the Kalosm library's `SegmentAnything` model. It loads an image from a file, segments it into multiple parts, and saves each segmented image as a separate PNG file. The code requires the `kalosm`, `image`, and `tokio` dependencies. The `SegmentAnything` model is initialized, the image is loaded, segmentation is performed using the `segment_everything` method, and the resulting images are saved to disk.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_12\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::vision::*;\n\n#[tokio::main]\nasync fn main() {\n    let model = SegmentAnything::builder().build().unwrap();\n    let image = image::open(\"examples/landscape.jpg\").unwrap();\n    let images = model.segment_everything(image).unwrap();\n    for (i, img) in images.iter().enumerate() {\n        img.save(&format!(\"{}.png\", i)).unwrap();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining and Running a Task with a Chat Model in Rust\nDESCRIPTION: This snippet shows how to define a task with a description using the `task` method and run it with an input using the Kalosm framework. It initializes a `Llama` chat model, creates a task to summarize text, and streams the output to standard output using `to_std_out`. It utilizes `tokio` for asynchronous execution.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\n// Create a new model\nlet model = Llama::new_chat().await.unwrap();\n// Create a new task that summarizes text\nlet task = model.task(\"You take a long description and summarize it into a single short sentence\");\nlet mut output = task(\"You can define a Task with a description then run it with an input. The task will cache the description to repeated calls faster. Tasks work with chat models.\");\n// Then stream the output to the console\noutput.to_std_out().await.unwrap();\n# }\n```\n\n----------------------------------------\n\nTITLE: Cloning Text Completion Sessions with Llama in Rust\nDESCRIPTION: This snippet shows how to clone a text completion session using the `try_clone` method in kalosm. Cloning allows retaining the original session state while creating a new session to work with. It uses the Llama model. Not all models support cloning.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion_session.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::io::Write;\n\n#[tokio::main]\nasync fn main() {\n    let mut llm = Llama::new().await.unwrap();\n    let mut session = llm.new_session().unwrap();\n\n    // Feed some text into the session\n    llm.stream_text_with_callback(&mut session, \"The capital of France is \", GenerationParameters::new().with_max_length(0), |_| Ok(())).await.unwrap();\n\n    // Clone the session\n    let cloned_session = session.try_clone().unwrap();\n\n    // Feed some more text into the cloned session\n    llm.stream_text_with_callback(&mut session, \"The capital of France is \", GenerationParameters::new(), |token| {println!(\"{token}\"); Ok(())}).await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Article from URL in Rust\nDESCRIPTION: This code demonstrates how to extract content from a URL using `Url::parse` and `into_document` in Kalosm. It parses a URL, extracts the title and body of the article, and prints them to the console. It leverages the `tokio` runtime for asynchronous operations and handles potential errors.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::convert::TryFrom;\nuse std::path::PathBuf;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Try to extract an article from a URL\n    let page = Url::parse(\"https://www.nytimes.com/live/2023/09/21/world/zelensky-russia-ukraine-news\")?;\n    let document = page.into_document().await?;\n    println!(\"Title: {}\", document.title());\n    println!(\"Body: {}\", document.body());\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Building Default Plugins with Floneum CLI\nDESCRIPTION: This command uses the Floneum CLI to build a release version of multiple default plugins. The `--packages` argument specifies a comma-separated list of plugin package names to build.\nSOURCE: https://github.com/floneum/floneum/blob/main/floneum/floneum/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nfloneum build --release --packages floneum_add_embedding,floneum_embedding,floneum_embedding_db,floneum_format,floneum_generate_text,floneum_generate_structured_text,floneum_search,floneum_search_engine,floneum_if,floneum_contains,floneum_write_to_file,floneum_read_from_file,floneum_python,floneum_find_node,floneum_find_child_node,floneum_click_node,floneum_node_text,floneum_type_in_node,floneum_navigate_to,floneum_get_article,floneum_read_rss,floneum_split,floneum_slice,floneum_join,floneum_add_to_list,floneum_new_list,floneum_length,floneum_more_than,floneum_less_than,floneum_equals,floneum_and,floneum_or,floneum_calculate,floneum_not,floneum_add,floneum_subtract,floneum_multiply,floneum_divide,floneum_power,floneum_number,floneum_string\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio to Text using Whisper and AsyncSourceTranscribeExt\nDESCRIPTION: This code shows how to transcribe audio from a microphone stream into text using the `Whisper` model and the `AsyncSourceTranscribeExt::transcribe` method. It initializes a microphone stream, transcribes the audio, and prints the transcribed text to standard output.  It requires `kalosm::sound` and uses the `tokio` runtime for asynchronous operations.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/sound.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::sound::*;\n#[tokio::main]\nasync fn main() {\n    // Get the default microphone input\n    let mic = MicInput::default();\n    // Stream the audio from the microphone\n    let stream = mic.stream();\n    // Transcribe the audio into text with the default Whisper model\n    let mut transcribe = stream.transcribe(Whisper::new().await.unwrap());\n    // Print the text as it is streamed in\n    transcribe.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Images From Text with Wuerstchen in Rust\nDESCRIPTION: This code snippet demonstrates how to generate images from text using the Wuerstchen model in Rust. It initializes the model, sets inference settings with a text prompt, and iterates through the generated images, saving each one as a PNG file. It depends on the `futures_util` and `kalosm_vision` crates.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/vision.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse futures_util::StreamExt;\nuse kalosm_vision::{Wuerstchen, WuerstchenInferenceSettings};\n\n#[tokio::main]\nasync fn main() {\n    let model = Wuerstchen::builder().build().await.unwrap();\n    let settings = WuerstchenInferenceSettings::new(\n        \"a cute cat with a hat in a room covered with fur with incredible detail\",\n    );\n\n    let mut images = model.run(settings);\n    while let Some(image) = images.next().await {\n        if let Some(buf) = image.generated_image() {\n            buf.save(&format!(\"{}.png\", image.sample_num())).unwrap();\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Gathering Context from a URL with Kalosm\nDESCRIPTION: This snippet demonstrates how to gather context from a URL using Kalosm. It parses a URL, converts it into a `Document`, and prints the title and body of the document. Dependencies: tokio, kalosm::language, `url` crate.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm-language/README.md#_snippet_4\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::convert::TryFrom;\nuse std::path::PathBuf;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Try to extract an article from a URL\n    let page = Url::parse(\"https://www.nytimes.com/live/2023/09/21/world/zelensky-russia-ukraine-news\")?;\n    let document = page.into_document().await?;\n    println!(\"Title: {}\", document.title());\n    println!(\"Body: {}\", document.body());\n\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Add Kalosm and Tokio Dependencies\nDESCRIPTION: Adds `kalosm` and `tokio` as dependencies to the Rust project. `kalosm` is fetched directly from the GitHub repository and uses the `full` feature. `tokio` is also added with the `full` feature, needed for async runtime.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncargo add kalosm --git https://github.com/floneum/floneum --features full\ncargo add tokio --features full\n```\n\n----------------------------------------\n\nTITLE: Fusor Tensor Transformation Function\nDESCRIPTION: This Rust code defines a function `exp_add_one` that performs element-wise operations on a 2D tensor. It negates the tensor, applies the exponential function, and adds 1 to each element. This example showcases a custom operation chain that Fusor's kernel fusion compiler would optimize. Note the `ignore` tag indicates the code is not meant to be compiled directly, serving as example documentation.\nSOURCE: https://github.com/floneum/floneum/blob/main/README.md#_snippet_5\n\nLANGUAGE: Rust\nCODE:\n```\nfn exp_add_one(tensor: Tensor<2, f32>) -> Tensor<2, f32> {\n  1. + (-tensor).exp()\n}\n```\n\n----------------------------------------\n\nTITLE: Using Derived Parser with a Task\nDESCRIPTION: This code shows how to use a derived parser (from the previous example) with a Kalosm language model task to generate structured data. It creates a `Pet` struct, derives a parser for it, and then sets up a task that generates realistic JSON placeholders for pets in that format. The generated data is then parsed into a `Pet` instance and printed to the console.  Requires the `kalosm::language` crate, the tokio runtime, and the `Parse` derive macro.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# use std::sync::Arc;\n# #[derive(Parse, Clone, Debug)]\n# struct Pet {\n#     name: String,\n#     age: u32,\n#     description: String,\n# }\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new_chat().await.unwrap();\n    // Then create a parser for your data. Any type that implements the `Parse` trait has the `new_parser` method\n    let parser = Pet::new_parser();\n    // Create a task with the constraints\n    let task = model.task(\"You generate realistic JSON placeholders for pets in the form {\\\"name\\\": \\\"Pet name\\\", \\\"age\\\": 0, \\\"description\\\": \\\"Pet description\\\"}\")\n        // The task constraints must be clone. If they don't implement Clone, you can wrap them in an Arc\n        .with_constraints(Arc::new(parser));\n    // Then run the task\n    let pet: Pet = task(\"Ruffles is a 3 year old adorable dog\").await.unwrap();\n    println!(\"{pet:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Building the UI with Tailwind CSS and Cargo\nDESCRIPTION: These commands build the UI for the Floneum project. The first command uses Tailwind CSS to process the input CSS file and generate the output CSS file. The second command uses Cargo to run the Rust application in release mode, targeting a specific architecture.\nSOURCE: https://github.com/floneum/floneum/blob/main/floneum/floneum/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnpx tailwindcss -i ./input.css -o ./public/tailwind.css\ncargo run --release --target aarch64-apple-darwin # Or whatever the target triple for your current device is\n```\n\n----------------------------------------\n\nTITLE: Text Completion with Custom Sampler\nDESCRIPTION: This snippet demonstrates how to modify the text completion process by providing a custom sampler.  It initializes a Llama model, creates a custom sampler using `GenerationParameters`, and then uses the sampler in the completion request to influence the token selection process.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/completion.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nlet model = Llama::new().await.unwrap();\n// Create the sampler to use for the text completion session\nlet sampler = GenerationParameters::default().sampler();\n// Create a completion request with the sampler\nlet mut stream = model.complete(\"Here is a list of 5 primes: \").with_sampler(sampler);\nstream.to_std_out().await.unwrap();\n```\n\n----------------------------------------\n\nTITLE: Semantic Chunking of Documents in Rust\nDESCRIPTION: This snippet shows how to perform semantic chunking of documents using the `SemanticChunker` in Kalosm. It initializes a `Bert` model, creates a `DocumentFolder`, chunks the documents into semantically similar chunks, and prints the resulting chunks. It uses `tokio` for asynchronous execution.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // First, create an embedding model for semantic chunking\n    let model = Bert::new().await?;\n    // Then create a document folder with some documents\n    let documents = DocumentFolder::new(\"./documents\")?.into_documents().await?;\n    // Then chunk the documents into sentences\n    let chunked = SemanticChunker::new().chunk_batch(&documents, &model).await?;\n    println!(\"{:?}\", chunked);\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Detecting voice activity in an audio stream using VoiceActivityDetectorExt\nDESCRIPTION: This code snippet demonstrates how to use the `VoiceActivityDetectorExt` to detect voice activity in an audio stream. It initializes a microphone input, streams audio, detects voice activity, and prints the probability of voice activity in each chunk. The `tokio` runtime is used for asynchronous operations.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/sound.md#_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::sound::*;\n#[tokio::main]\nasync fn main() {\n    // Get the default microphone input\n    let mic = MicInput::default();\n    // Stream the audio from the microphone\n    let stream = mic.stream();\n    // Detect voice activity in the audio stream\n    let mut vad = stream.voice_activity_stream();\n    while let Some(input) = vad.next().await {\n        println!(\"Probability: {}\", input.probability);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Text Classifier - Rust\nDESCRIPTION: This snippet takes user input, embeds it using a BERT model, and then runs the classifier on the resulting embedding, printing the output. The loop continues to prompt for new inputs.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_9\n\nLANGUAGE: Rust\nCODE:\n```\nloop {\n    let input = prompt_input(\"Input: \").unwrap();\n    let embedding = bert.embed(input).await?;\n    let output = classifier.run(embedding)?;\n    println!(\"Output: {:?}\", output);\n}\n```\n\n----------------------------------------\n\nTITLE: Rechunking Voice Activity Stream with VoiceActivityStreamExt\nDESCRIPTION: This example illustrates how to rechunk an audio stream into chunks of consecutive audio samples with high voice activity probability using `VoiceActivityStreamExt::rechunk_voice_activity`. It takes a microphone stream, detects voice activity, rechunks it, and prints the duration of each chunk. `rodio::Source` is imported but not directly used in the provided snippet, which is potentially misleading.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/sound.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::sound::*;\nuse rodio::Source;\n#[tokio::main]\nasync fn main() {\n    // Get the default microphone input\n    let mic = MicInput::default();\n    // Stream the audio from the microphone\n    let stream = mic.stream();\n    // Chunk the audio into chunks of speech\n    let vad = stream.voice_activity_stream();\n    let mut audio_chunks = vad.rechunk_voice_activity();\n    // Print the chunks as they are streamed in\n    while let Some(input) = audio_chunks.next().await {\n        println!(\"New voice activity chunk with duration {:?}\", input.total_duration());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Parser from Prebuilt Combinators\nDESCRIPTION: This example demonstrates how to create a parser using prebuilt combinators provided by the `kalosm::language` crate. It defines a custom parser format for pet data as `[\"Pet name\", age number, \"Pet description\"]` using `LiteralParser`, `String::new_parser()`, and `u8::new_parser()`. The parser is then used with a task to generate data conforming to this custom format. Requires the `kalosm::language` crate and the tokio runtime.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() {\n    // First create a model\n    let model = Llama::new_chat().await.unwrap();\n    // Then create a parser for your custom format\n    let parser = LiteralParser::from(\"[\")\n        .ignore_output_then(String::new_parser())\n        .then_literal(\", \")\n        .then(u8::new_parser())\n        .then_literal(\", \")\n        .then(String::new_parser())\n        .then_literal(\"]\");\n    // Create a task with the constraints\n    let task = model.task(\"You generate realistic JSON placeholders for pets in the form [\\\"Pet name\\\", age number, \\\"Pet description\\\"]\")\n        // The task constraints must be clone. If they don't implement Clone, you can wrap them in an Arc\n        .with_constraints(Arc::new(parser));\n    // Then run the task\n    let ((name, age), description) = task(\"Ruffles is a 3 year old adorable dog\").await.unwrap();\n    println!(\"{name} {age} {description}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying a Task Response Builder\nDESCRIPTION: This example shows how to modify the `ChatResponseBuilder` using methods like `with_sampler`. It creates a task, calls it with input text, and then applies a custom generation parameter to the response stream before printing the results. Requires the `kalosm::language` crate and the tokio runtime.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/task.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() {\n    let model = Llama::new_chat().await.unwrap();\n    let task = model.task(\"You are an editing assistant who offers suggestions for improving the quality of the text. You will be given some text and will respond with a list of suggestions for how to improve the text.\");\n    let mut stream = task(\"this isnt correct. or is it?\").with_sampler(GenerationParameters::default());\n    stream.to_std_out().await.unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Data Structure with Parse Trait in Rust\nDESCRIPTION: This code defines a simple data structure `Pet` with the `Parse` derive macro from the `kalosm::language` crate. This allows parsing text into this structure using Language Models. It includes fields for `name` (String), `age` (u32), and `description` (String).\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/kalosm/docs/language.md#_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse kalosm::language::*;\n#[derive(Parse, Clone)]\nstruct Pet {\n    name: String,\n    age: u32,\n    description: String,\n}\n```\n\n----------------------------------------\n\nTITLE: Building Default Plugins - Shell\nDESCRIPTION: This command builds the default plugins for the Floneum project. It uses the `floneum` command-line tool with the `build` subcommand, specifying the release build and a list of packages to include in the build process. This command is useful for developers who want to create a release version of the default plugins.\nSOURCE: https://github.com/floneum/floneum/blob/main/floneum/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nfloneum build --release --packages floneum_add_embedding,floneum_embedding,floneum_embedding_db,floneum_format,floneum_generate_text,floneum_generate_structured_text,floneum_search,floneum_search_engine,floneum_if,floneum_contains,floneum_write_to_file,floneum_read_from_file,floneum_python,floneum_find_node,floneum_find_child_node,floneum_click_node,floneum_node_text,floneum_type_in_node,floneum_navigate_to,floneum_get_article,floneum_read_rss,floneum_split,floneum_slice,floneum_join,floneum_add_to_list,floneum_new_list,floneum_length,floneum_more_than,floneum_less_than,floneum_equals,floneum_and,floneum_or,floneum_calculate,floneum_not,floneum_add,floneum_subtract,floneum_multiply,floneum_divide,floneum_power,floneum_number,floneum_string\n```\n\n----------------------------------------\n\nTITLE: Example: Cosine Similarity Scores\nDESCRIPTION: This snippet contains an example output showing cosine similarity scores between pairs of sentences. The scores indicate the semantic similarity, or relatedness, between the sentences. The higher the score, the more similar the sentences are in meaning.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nscore: 0.82 'Kalosm can be used to build local AI applications' 'With private LLMs data never leaves your computer'\nscore: 0.72 'With private LLMs data never leaves your computer' 'The quick brown fox jumps over the lazy dog'\nscore: 0.72 'Kalosm can be used to build local AI applications' 'The quick brown fox jumps over the lazy dog'\n```\n\n----------------------------------------\n\nTITLE: Example: VectorDB Search Output\nDESCRIPTION: This snippet displays the output from searching a VectorDB. It shows the distance between the query and the closest matching text, as well as the text of the closest match.  The distance value indicates dissimilarity: lower scores represent closer, more similar matches.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/embedding.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\ndistance: 0.18480265\nclosest: Kalosm can be used to build local AI applications\n```\n\n----------------------------------------\n\nTITLE: Defining Character Struct for Structured Generation in Kalosm\nDESCRIPTION: This Rust code defines a `Character` struct with fields for name, age, and description, using the `Parse` and `Schema` derives from the `kalosm::language` crate. It showcases structured generation capabilities of Kalosm using custom parsing logic via regex patterns and range constraints. This allows for structured output that conforms to the defined schema.\nSOURCE: https://github.com/floneum/floneum/blob/main/README.md#_snippet_0\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\n\n/// A fictional character\n#[derive(Parse, Schema, Clone, Debug)]\nstruct Character {\n    /// The name of the character\n    #[parse(pattern = \"[A-Z][a-z]{2,10} [A-Z][a-z]{2,10}\")]\n    name: String,\n    /// The age of the character\n    #[parse(range = 1..=100)]\n    age: u8,\n    /// A description of the character\n    #[parse(pattern = \"[A-Za-z ]{40,200}\")]\n    description: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    // First create a model. Chat models tend to work best with structured generation\n    let model = Llama::phi_3().await.unwrap();\n    // Then create a task with the parser as constraints\n    let task = model.task(\"You generate realistic JSON placeholders for characters\")\n        .typed();\n    // Finally, run the task\n    let mut stream = task(\"Create a list of random characters\", &model);\n    stream.to_std_out().await.unwrap();\n    let characters: [Character; 10] = stream.await.unwrap();\n    println!(\"{characters:?}\");\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Chatbot Implementation Using Kalosm\nDESCRIPTION: This Rust code implements a simple chatbot using the `kalosm` crate. It initializes a Llama model, sets a system prompt to define the chatbot's persona (a pirate called Blackbeard), and then enters a loop to continuously prompt the user for input and generate responses. The `tokio` crate is used for asynchronous execution.\nSOURCE: https://github.com/floneum/floneum/blob/main/README.md#_snippet_3\n\nLANGUAGE: Rust\nCODE:\n```\nuse kalosm::language::*;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n  let model = Llama::phi_3().await?;\n  let mut chat = model.chat()\n    .with_system_prompt(\"You are a pirate called Blackbeard\");\n\n  loop {\n    chat(&prompt_input(\"\\n> \")?)\n      .to_std_out()\n      .await?;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Chat Session History with Kalosm\nDESCRIPTION: This code snippet shows how to access the history of a ChatSession using the `history` method.  It initializes a Llama chat model, adds a message to the session, retrieves the history, and then asserts that the history contains the expected message with the correct role and content. It requires the `kalosm::language` crate and tokio runtime.\nSOURCE: https://github.com/floneum/floneum/blob/main/interfaces/language-model/docs/chat_session.md#_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n# use kalosm::language::*;\n# #[tokio::main]\n# async fn main() {\nlet mut llm = Llama::new_chat().await.unwrap();\nlet mut chat = llm.chat();\n// Add a message to the session\nchat(\"Hello, world!\").to_std_out().await.unwrap();\n// Get the history of the session\nlet history = chat.session().unwrap().history();\nassert_eq!(history.len(), 1);\nassert_eq!(history[0].role(), MessageType::UserMessage);\nassert_eq!(history[0].content(), \"Hello, world!\");\n# }\n```\n\n----------------------------------------\n\nTITLE: Creating a New Cargo Project\nDESCRIPTION: This shell command creates a new Rust project named `kalosm-hello-world` using Cargo and then navigates into the newly created directory.  This is the first step in setting up a new project to use the Kalosm library.\nSOURCE: https://github.com/floneum/floneum/blob/main/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ncargo new kalosm-hello-world\ncd ./kalosm-hello-world\n```\n\n----------------------------------------\n\nTITLE: Running the Application\nDESCRIPTION: This shell command runs the Rust application in release mode using Cargo. The `--release` flag optimizes the code for performance, making it suitable for deployment.\nSOURCE: https://github.com/floneum/floneum/blob/main/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ncargo run --release\n```"
  }
]
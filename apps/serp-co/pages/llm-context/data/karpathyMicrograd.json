[
  {
    "owner": "karpathy",
    "repo": "micrograd",
    "content": "TITLE: Training the Neural Network with SGD\nDESCRIPTION: Implements the training loop using stochastic gradient descent. For 100 iterations, it performs forward pass, backward pass (backpropagation), and parameter updates with a decreasing learning rate. Progress is printed at each step.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# optimization\nfor k in range(100):\n    \n    # forward\n    total_loss, acc = loss()\n    \n    # backward\n    model.zero_grad()\n    total_loss.backward()\n    \n    # update (sgd)\n    learning_rate = 1.0 - 0.9*k/100\n    for p in model.parameters():\n        p.data -= learning_rate * p.grad\n    \n    if k % 1 == 0:\n        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n```\n\n----------------------------------------\n\nTITLE: Implementing the Loss Function\nDESCRIPTION: Defines a loss function that computes both the loss and accuracy. It includes a max-margin (SVM) loss with L2 regularization. The function supports mini-batch training by accepting an optional batch_size parameter.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# loss function\ndef loss(batch_size=None):\n    \n    # inline DataLoader :)\n    if batch_size is None:\n        Xb, yb = X, y\n    else:\n        ri = np.random.permutation(X.shape[0])[:batch_size]\n        Xb, yb = X[ri], y[ri]\n    inputs = [list(map(Value, xrow)) for xrow in Xb]\n    \n    # forward the model to get scores\n    scores = list(map(model, inputs))\n    \n    # svm \"max-margin\" loss\n    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n    data_loss = sum(losses) * (1.0 / len(losses))\n    # L2 regularization\n    alpha = 1e-4\n    reg_loss = alpha * sum((p*p for p in model.parameters()))\n    total_loss = data_loss + reg_loss\n    \n    # also get accuracy\n    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n    return total_loss, sum(accuracy) / len(accuracy)\n\ntotal_loss, acc = loss()\nprint(total_loss, acc)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Basic Operations with micrograd's Value Class\nDESCRIPTION: Example showcasing various operations supported by micrograd's autodiff engine including addition, multiplication, exponentiation, ReLU activation, and gradient computation through backpropagation.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom micrograd.engine import Value\n\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\nc += c + 1\nc += 1 + c + (-a)\nd += d * 2 + (b + a).relu()\nd += 3 * d + (b - a).relu()\ne = c - d\nf = e**2\ng = f / 2.0\ng += 10.0 / f\nprint(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass\ng.backward()\nprint(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da\nprint(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db\n```\n\n----------------------------------------\n\nTITLE: Initializing the Neural Network Model\nDESCRIPTION: Creates a multi-layer perceptron (MLP) model with 2 input features, two hidden layers of 16 neurons each, and 1 output neuron. Displays the model architecture and the total number of parameters.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# initialize a model \nmodel = MLP(2, [16, 16, 1]) # 2-layer neural network\nprint(model)\nprint(\"number of parameters\", len(model.parameters()))\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Decision Boundary\nDESCRIPTION: Creates a visualization of the decision boundary learned by the model. It generates a grid of points, evaluates the model on each point, and creates a contour plot showing the decision regions along with the original data points.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# visualize decision boundary\n\nh = 0.25\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nXmesh = np.c_[xx.ravel(), yy.ravel()]\ninputs = [list(map(Value, xrow)) for xrow in Xmesh]\nscores = list(map(model, inputs))\nZ = np.array([s.data > 0 for s in scores])\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n```\n\n----------------------------------------\n\nTITLE: Implementing Graph Tracing and Visualization Functions\nDESCRIPTION: Defines two key functions: trace() to recursively build the computational graph by identifying nodes and edges, and draw_dot() to create a visual representation of the graph using Graphviz. The visualization includes both data values and gradients.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n    \n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n    \n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n    \n    return dot\n```\n\n----------------------------------------\n\nTITLE: Creating a Moon-Shaped Dataset\nDESCRIPTION: Generates a synthetic dataset using the make_moons function from scikit-learn with 100 samples and 0.1 noise level. The targets are transformed to be either -1 or 1, and the data is visualized in a 2D scatter plot.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# make up a dataset\n\nfrom sklearn.datasets import make_moons, make_blobs\nX, y = make_moons(n_samples=100, noise=0.1)\n\ny = y*2 - 1 # make y be -1 or 1\n# visualize in 2D\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n```\n\n----------------------------------------\n\nTITLE: Visualizing Neural Network Computations with draw_dot\nDESCRIPTION: Code snippet that creates a simple 2D neuron, performs a forward pass, and generates a visualization of the computation graph showing both data values and gradients.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom micrograd import nn\nn = nn.Neuron(2)\nx = [Value(1.0), Value(-2.0)]\ny = n(x)\ndot = draw_dot(y)\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Neural Network Neuron with Micrograd\nDESCRIPTION: Demonstrates the visualization of a 2D neuron from Micrograd's neural network module. Creates a neuron with 2 inputs, performs forward and backward passes, and generates a visual representation of the computational graph.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# a simple 2D neuron\nimport random\nfrom micrograd import nn\n\nrandom.seed(1337)\nn = nn.Neuron(2)\nx = [Value(1.0), Value(-2.0)]\ny = n(x)\ny.backward()\n\ndot = draw_dot(y)\ndot\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Basic Computational Graph Visualization\nDESCRIPTION: Creates a simple computational graph using Micrograd's Value class with basic operations (multiplication, addition, and ReLU activation). Performs backpropagation and visualizes the resulting graph.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# a very simple example\nx = Value(1.0)\ny = (x * 2 + 1).relu()\ny.backward()\ndraw_dot(y)\n```\n\n----------------------------------------\n\nTITLE: Importing MicroGrad Components\nDESCRIPTION: Imports the core components from MicroGrad including the Value class from the engine module and neural network components (Neuron, Layer, MLP) from the nn module.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom micrograd.engine import Value\nfrom micrograd.nn import Neuron, Layer, MLP\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with pytest\nDESCRIPTION: Command to execute the test suite which verifies gradient calculations against PyTorch reference implementations.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest\n```\n\n----------------------------------------\n\nTITLE: Setting Random Seeds\nDESCRIPTION: Sets random seeds for both numpy and the random module to ensure reproducibility of results.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1337)\nrandom.seed(1337)\n```\n\n----------------------------------------\n\nTITLE: Importing Value Class from Micrograd Engine\nDESCRIPTION: Imports the core Value class from Micrograd's engine module, which is the fundamental building block for the computational graph.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom micrograd.engine import Value\n```\n\n----------------------------------------\n\nTITLE: Importing Basic Libraries for MicroGrad Demo\nDESCRIPTION: Imports necessary libraries for the demo including random, numpy, and matplotlib for visualization with inline plotting enabled.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Importing Graphviz for Visualization in Python\nDESCRIPTION: Sets up the necessary Graphviz library for creating visual representations of computational graphs. Requires Graphviz to be installed via Homebrew and the Python wrapper via pip.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# brew install graphviz\n# pip install graphviz\nfrom graphviz import Digraph\n```\n\n----------------------------------------\n\nTITLE: Installing micrograd via pip\nDESCRIPTION: Simple pip installation command for the micrograd package.\nSOURCE: https://github.com/karpathy/micrograd/blob/master/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install micrograd\n```\n\n----------------------------------------\n\nTITLE: Rendering Graphviz Visualization to File\nDESCRIPTION: Saves the generated Graphviz visualization to a file named 'gout' in the current directory. The output format is determined by the format parameter specified in the draw_dot function (default is SVG).\nSOURCE: https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndot.render('gout')\n```"
  }
]
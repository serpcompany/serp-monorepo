[
  {
    "owner": "apache",
    "repo": "dolphinscheduler-website",
    "content": "TITLE: Implementing WorkflowAsCode in Python with DolphinScheduler\nDESCRIPTION: Example showing how to create a workflow programmatically using Python API. Demonstrates task definition, dependencies, and workflow properties configuration including scheduling, start time, and tenant settings.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache_dolphinScheduler_2.0.2.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Define workflow properties, including name, scheduling period, start time, tenant, etc.\n\nwith ProcessDefinition(\n    name=\"tutorial\",\n    schedule=\"0 0 0 * * ? *\",\n    start_time=\"2021-01-01\",\n    tenant=\"tenant_exists\",\n) as pd:\n    # Define 4 tasks, which are all shell tasks, the required parameters of shell tasks are task name, command information, here are all the shell commands of echo\n\n    task_parent = Shell(name=\"task_parent\", command=\"echo hello pydolphinscheduler\")\n    task_child_one = Shell(name=\"task_child_one\", command=\"echo 'child one'\")\n    task_child_two = Shell(name=\"task_child_two\", command=\"echo 'child two'\")\n    task_union = Shell(name=\"task_union\", command=\"echo union\")\n\n    # Define dependencies between tasks\n    # Here, task_child_one and task_child_two are first declared as a task group through python's list\n    task_group = [task_child_one, task_child_two]\n    # Use the set_downstream method to declare the task group task_group as the downstream of task_parent, and declare the upstream through set_upstream\n    task_parent.set_downstream(task_group)\n\n    # Use the bit operator << to declare the task_union as the downstream of the task_group, and support declaration through the bit operator >>\n    task_union << task_group\n```\n\n----------------------------------------\n\nTITLE: Implementing HiveClient Parameters Class in Apache DolphinScheduler\nDESCRIPTION: Parameter class for HiveClient task that extends AbstractParameters to provide SQL execution parameters. Includes validation and resource file handling methods.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_5\n\nLANGUAGE: plain\nCODE:\n```\npackage org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.ResourceInfo;\nimport java.util.List;\npublic class HiveClientParameters extends AbstractParameters {\n    /**\n     * The easiest way to execute with HiveClient is to just paste in all the SQL, so we only need one SQL parameter\n     */\n    private String sql;\n    public String getSql() {\n        return sql;\n    }\n    public void setSql(String sql) {\n        this.sql = sql;\n    }\n    @Override\n    public boolean checkParameters() {\n        return sql ! = null;\n    }\n    @Override\n    public List<ResourceInfo> getResourceFilesList() {\n        return null;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a TaskChannel Interface for Task Implementation in Apache DolphinScheduler\nDESCRIPTION: Interface definition for TaskChannel that provides methods for canceling applications and creating executable tasks. Includes an implementation example for HiveClient tasks.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_4\n\nLANGUAGE: plain\nCODE:\n```\nvoid cancelApplication(boolean status);\n    /**\n     * Build executable tasks\n     */\n    AbstractTask createTask(TaskRequest taskRequest);\npublic class HiveClientTaskChannel implements TaskChannel {\n    @Override\n    public void cancelApplication(boolean b) {\n        //do nothing\n    }\n    @Override\n    public AbstractTask createTask(TaskRequest taskRequest) {\n        return new HiveClientTask(taskRequest);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Packaging and Deploying Custom Task Plugin in Apache DolphinScheduler\nDESCRIPTION: Commands for building, packaging, and deploying the custom task plugin into an Apache DolphinScheduler installation. Includes verification steps for checking successful deployment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_8\n\nLANGUAGE: plain\nCODE:\n```\n## 1,Packing\nmvn clean install\n## 2, Deployment\ncp . /target/dolphinscheduler-task-hiveclient-1.0.jar $DOLPHINSCHEDULER_HOME/lib/\n## 3,restart dolphinscheduler server\n```\n\n----------------------------------------\n\nTITLE: User Service Implementation with Database Interaction in Java\nDESCRIPTION: A service implementation that queries user information from the database. This class demonstrates a dependency on the DAO layer and serves as an example for service layer testing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_4\n\nLANGUAGE: java\nCODE:\n```\n@Service\npublic class UsersServiceImpl extends BaseServiceImpl implements UsersService {\n    \n    /**\n     * query user\n     *\n     * @param name name\n     * @param password password\n     * @return user info\n     */\n    @Override\n    public User queryUser(String name, String password) {\n        String md5 = EncryptionUtils.getMd5(password);\n        return userMapper.queryUserByNamePassword(name, md5);\n    }\n    \n}\n```\n\n----------------------------------------\n\nTITLE: Alert Result Handler Implementation\nDESCRIPTION: The alertResultHandler method retrieves the appropriate AlertChannel for a plugin instance, validates warning type compatibility, and asynchronously processes the alert. It handles timeouts, exception handling, and supports both regular alerts and close alert operations.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nprivate @Nullable AlertResult alertResultHandler(AlertPluginInstance instance, AlertData alertData) {\n    String pluginInstanceName = instance.getInstanceName();\n    int pluginDefineId = instance.getPluginDefineId();\n    Optional<AlertChannel> alertChannelOptional = alertPluginManager.getAlertChannel(instance.getPluginDefineId());\n    if (!alertChannelOptional.isPresent()) {\n        String message = String.format(\"Alert Plugin %s send error: the channel doesn't exist, pluginDefineId: %s\",\n                pluginInstanceName,\n                pluginDefineId);\n        logger.error(\"Alert Plugin {} send error : not found plugin {}\", pluginInstanceName, pluginDefineId);\n        return new AlertResult(\"false\", message);\n    }\n    AlertChannel alertChannel = alertChannelOptional.get();\n\n    Map<String, String> paramsMap = JSONUtils.toMap(instance.getPluginInstanceParams());\n    String instanceWarnType = WarningType.ALL.getDescp();\n\n    if (paramsMap != null) {\n        instanceWarnType = paramsMap.getOrDefault(AlertConstants.NAME_WARNING_TYPE, WarningType.ALL.getDescp());\n    }\n\n    WarningType warningType = WarningType.of(instanceWarnType);\n\n    if (warningType == null) {\n        String message = String.format(\"Alert Plugin %s send error : plugin warnType is null\", pluginInstanceName);\n        logger.error(\"Alert Plugin {} send error : plugin warnType is null\", pluginInstanceName);\n        return new AlertResult(\"false\", message);\n    }\n\n    boolean sendWarning = false;\n    switch (warningType) {\n        case ALL:\n            sendWarning = true;\n            break;\n        case SUCCESS:\n            if (alertData.getWarnType() == WarningType.SUCCESS.getCode()) {\n                sendWarning = true;\n            }\n            break;\n        case FAILURE:\n            if (alertData.getWarnType() == WarningType.FAILURE.getCode()) {\n                sendWarning = true;\n            }\n            break;\n        default:\n    }\n\n    if (!sendWarning) {\n        logger.info(\n                \"Alert Plugin {} send ignore warning type not match: plugin warning type is {}, alert data warning type is {}\",\n                pluginInstanceName, warningType.getCode(), alertData.getWarnType());\n        return null;\n    }\n\n    AlertInfo alertInfo = AlertInfo.builder()\n            .alertData(alertData)\n            .alertParams(paramsMap)\n            .alertPluginInstanceId(instance.getId())\n            .build();\n    int waitTimeout = alertConfig.getWaitTimeout();\n    try {\n        AlertResult alertResult;\n        if (waitTimeout <= 0) {\n            if (alertData.getAlertType() == AlertType.CLOSE_ALERT.getCode()) {\n                alertResult = alertChannel.closeAlert(alertInfo);\n            } else {\n                alertResult = alertChannel.process(alertInfo);\n            }\n        } else {\n            CompletableFuture<AlertResult> future;\n            if (alertData.getAlertType() == AlertType.CLOSE_ALERT.getCode()) {\n                future = CompletableFuture.supplyAsync(() -> alertChannel.closeAlert(alertInfo));\n            } else {\n                future = CompletableFuture.supplyAsync(() -> alertChannel.process(alertInfo));\n            }\n            alertResult = future.get(waitTimeout, TimeUnit.MILLISECONDS);\n        }\n        if (alertResult == null) {\n            throw new RuntimeException(\"Alert result cannot be null\");\n        }\n        return alertResult;\n    } catch (InterruptedException e) {\n        logger.error(\"send alert error alert data id :{},\", alertData.getId(), e);\n        Thread.currentThread().interrupt();\n        return new AlertResult(\"false\", e.getMessage());\n    } catch (Exception e) {\n        logger.error(\"send alert error alert data id :{},\", alertData.getId(), e);\n        return new AlertResult(\"false\", e.getMessage());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Service Layer Testing with H2 Database in Java\nDESCRIPTION: A unit test for a service class using H2 in-memory database. This demonstrates how to test database operations without mocking the DAO layer.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n@RunWith(MockitoJUnitRunner.class)\npublic class UsersServiceTest {\n    \n    private static final Logger logger = LoggerFactory.getLogger(UsersServiceTest.class);\n\n    @InjectMocks\n    private UsersServiceImpl usersService;\n\n    @Mock\n    private UserMapper userMapper;\n    \n    @Before\n    public void createUser() {\n        String userName = \"userTest0001\";\n        String userPassword = \"userTest0001\";\n        User user = new User();\n        user.setUserName(userName);\n        user.setUserPassword(userPassword);\n        userMapper.insert(user);\n    }\n    \n    @Test\n    public void testQueryUser() {\n        String userName = \"userTest0001\";\n        String userPassword = \"userTest0001\";\n        User queryUser = usersService.queryUser(userName, userPassword);\n        logger.info(queryUser.toString());\n        Assert.assertTrue(queryUser != null);\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Kerberos Credential Auto-Refresh Implementation\nDESCRIPTION: Implementation of automatic Kerberos credential refresh mechanism to prevent token expiration. Sets up a scheduled job to check and renew Kerberos TGT credentials daily.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n    /**\n     * * Scheduled credential update.\n     */\n    private static void startCheckKeytabTgtAndReloginJob() {\n        // Daily loop, scheduled credential update.\n        Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(() -> {\n            try {\n                UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n                logger.warn(\"Check Kerberos Tgt And Relogin From Keytab Finish.\");\n            } catch (IOException e) {\n                logger.error(\"Check Kerberos Tgt And Relogin From Keytab Error\", e);\n            }\n        }, 0, 1, TimeUnit.DAYS);\n        logger.info(\"Start Check Keytab TGT And Relogin Job Success.\");\n    }\n```\n\nLANGUAGE: java\nCODE:\n```\npublic static boolean loadKerberosConf(String javaSecurityKrb5Conf, String loginUserKeytabUsername,\n                                           String loginUserKeytabPath, Configuration configuration) throws IOException {\n        if (CommonUtils.getKerberosStartupState()) {\n            System.setProperty(Constants.JAVA_SECURITY_KRB5_CONF, StringUtils.defaultIfBlank(javaSecurityKrb5Conf,\n                    PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH)));\n            configuration.set(Constants.HADOOP_SECURITY_AUTHENTICATION, Constants.KERBEROS);\n            UserGroupInformation.setConfiguration(configuration);\n            UserGroupInformation.loginUserFromKeytab(\n                    StringUtils.defaultIfBlank(loginUserKeytabUsername,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME)),\n                    StringUtils.defaultIfBlank(loginUserKeytabPath,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH)));\n            startCheckKeytabTgtAndReloginJob();  // call here\n            return true;\n        }\n        return false;\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing HiveClient Task Execution in Apache DolphinScheduler\nDESCRIPTION: Task implementation class that extends AbstractYarnTask to execute Hive queries. This class builds command strings for execution, writes SQL content to files, and manages task lifecycle.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_6\n\nLANGUAGE: plain\nCODE:\n```\npackage org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.request.TaskRequest;\nimport org.apache.dolphinscheduler.spi.utils.JSONUtils;\nimport java.io;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\npublic class HiveClientTask extends AbstractYarnTask {\n    /**\n     * hive client parameters\n     */\n    private HiveClientParameters hiveClientParameters;\n    /**\n     * taskExecutionContext\n     */\n    private final TaskRequest taskExecutionContext;\n    public HiveClientTask(TaskRequest taskRequest) {\n        super(taskRequest);\n        this.taskExecutionContext = taskRequest;\n    }\n    /**\n     * task init method\n     */\n    @Override\n    public void init() {\n        logger.info(\"hive client task param is {}\", JSONUtils.toJsonString(taskExecutionContext));\n        this.hiveClientParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), HiveClientParameters.class);\n        if (this.hiveClientParameters ! = null && !hiveClientParameters.checkParameters()) {\n            throw new RuntimeException(\"hive client task params is not valid\");\n        }\n    }\n    /**\n     * build task execution command\n     *\n     * @return task execution command or null\n     */\n    @Override\n    protected String buildCommand() {\n        String filePath = getFilePath();\n        if (writeExecutionContentToFile(filePath)) {\n            return \"hive -f \" + filePath;\n        }\n        return null;\n    }\n    /**\n     * get hive sql write path\n     *\n     * @return file write path\n     */\n    private String getFilePath() {\n        return String.format(\"%s/hive-%s-%s.sql\", this.taskExecutionContext.getExecutePath(), this.taskExecutionContext.getTaskName(), this. taskExecutionContext.getTaskInstanceId());\n    }\n    @Override\n    protected void setMainJarName() {\n        //do nothing\n    }\n    /**\n     * write hive sql to filepath\n     *\n     * @param filePath file path\n     * @return write success?\n     */\n    private boolean writeExecutionContentToFile(String filePath) {\n        Path path = Paths.get(filePath);\n        try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {\n            writer.write(this.hiveClientParameters.getSql());\n            logger.info(\"file:\" + filePath + \"write success.\");\n            return true;\n        } catch (IOException e) {\n            logger.error(\"file:\" + filePath + \"write failed. please path auth.\");\n            e.printStackTrace();\n            return false;\n        }\n    }\n    @Override\n    public AbstractParameters getParameters() {\n        return this.hiveClientParameters;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Resource Authorization Implementation in DolphinScheduler\nDESCRIPTION: Implementation of resource authorization logic that determines which resources a user can access. Combines both directly owned resources and authorized resources for proper permission management.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_8\n\nLANGUAGE: java\nCODE:\n```\n        @Override\n        public Set<Integer> listAuthorizedResource(int userId, Logger logger) {\n            List<Resource> relationResources;\n            if (userId == 0) {\n                relationResources = new ArrayList<>();\n            } else {\n                // query resource relation\n                List<Integer> resIds = resourceUserMapper.queryResourcesIdListByUserIdAndPerm(userId, 0);\n                relationResources = CollectionUtils.isEmpty(resIds) ? new ArrayList<>() : resourceMapper.queryResourceListById(resIds);\n            }\n            List<Resource> ownResourceList = resourceMapper.queryResourceListAuthored(userId, -1);\n            relationResources.addAll(ownResourceList);\n            return relationResources.stream().map(Resource::getId).collect(toSet()); // Resolve the issue of invalid resource file authorization.\n//            return ownResourceList.stream().map(Resource::getId).collect(toSet());\n        }\n```\n\n----------------------------------------\n\nTITLE: HiveClient TaskChannelFactory Implementation\nDESCRIPTION: Implementation of TaskChannelFactory interface for creating a custom Hive Client task type, including task channel creation, naming, and parameter configuration.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_3\n\nLANGUAGE: plain\nCODE:\n```\npackage org.apache.dolphinscheduler.plugin.task.hive;\nimport org.apache.dolphinscheduler.spi.params.base.PluginParams;\nimport org.apache.dolphinscheduler.spi.task.TaskChannel;\nimport org.apache.dolphinscheduler.spi.task.TaskChannelFactory;\nimport java.util.List;\npublic class HiveClientTaskChannelFactory implements TaskChannelFactory {\n    @Override\n    public TaskChannel create() {\n        return new HiveClientTaskChannel();\n    }\n    @Override\n    public String getName() {\n        return \"HIVE CLIENT\";\n    }\n    @Override\n    public List<PluginParams> getParams() {\n        List<PluginParams> pluginParams = new ArrayList<>();\n        InputParam nodeName = InputParam.newBuilder(\"name\", \"$t('Node name')\")\n                .addValidate(Validate.newBuilder()\n                        .setRequired(true)\n                        .build())\n                .build();\n        PluginParams runFlag = RadioParam.newBuilder(\"runFlag\", \"RUN_FLAG\")\n                .addParamsOptions(new ParamsOptions(\"NORMAL\", \"NORMAL\", false))\n                .addParamsOptions(new ParamsOptions(\"FORBIDDEN\", \"FORBIDDEN\", false))\n                .build();\n        PluginParams build = CheckboxParam.newBuilder(\"Hive SQL\", \"Test HiveSQL\")\n                .setDisplay(true)\n                .setValue(\"-- author: \\n --desc:\")\n                .build();\n        pluginParams.add(nodeName);\n        pluginParams.add(runFlag);\n        pluginParams.add(build);\n        return pluginParams;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Identifying Looping Workflow Instances with Bash\nDESCRIPTION: A bash command to search through the dolphinscheduler-master log file, extract all workflow instance IDs that are caught in error loops, sort them and return unique values.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/How_does_Apache_Dolphinscheduler_solve_the_infinite_loop_of_Master_service_without_restarting.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngrep WorkflowInstance  dolphinscheduler-master.log|grep \"Start workflow error\" |awk -F 'WorkflowInstance-' '{print $2}'| awk -F']' '{print $1}' |sort |uniq\n```\n\n----------------------------------------\n\nTITLE: Implementing HiveClient Task Executor in Java\nDESCRIPTION: Creates a task executor that extends AbstractYarnTask to implement the execution logic for HiveClient tasks. The implementation writes SQL to a file and executes it using the 'hive -f' command.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npackage org.apache.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.plugin.task.api.AbstractYarnTask;\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.request.TaskRequest;\nimport org.apache.dolphinscheduler.spi.utils.JSONUtils;\n\nimport java.io.BufferedWriter;\nimport java.io.IOException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\n\n\npublic class HiveClientTask extends AbstractYarnTask {\n\n    /**\n     * hive client parameters\n     */\n    private HiveClientParameters hiveClientParameters;\n\n    /**\n     * taskExecutionContext\n     */\n    private final TaskRequest taskExecutionContext;\n\n\n\n    public HiveClientTask(TaskRequest taskRequest) {\n        super(taskRequest);\n        this.taskExecutionContext = taskRequest;\n    }\n\n    /**\n     * task init method\n     */\n    @Override\n    public void init() {\n        logger.info(\"hive client task param is {}\", JSONUtils.toJsonString(taskExecutionContext));\n        this.hiveClientParameters = JSONUtils.parseObject(taskExecutionContext.getTaskParams(), HiveClientParameters.class);\n\n        if (this.hiveClientParameters != null && !hiveClientParameters.checkParameters()) {\n            throw new RuntimeException(\"hive client task params is not valid\");\n        }\n    }\n\n    /**\n     * build task execution command\n     *\n     * @return task execution command or null\n     */\n    @Override\n    protected String buildCommand() {\n        String filePath = getFilePath();\n        if (writeExecutionContentToFile(filePath)) {\n            return \"hive -f \" + filePath;\n        }\n        return null;\n    }\n\n    /**\n     * get hive sql write path\n     *\n     * @return file write path\n     */\n    private String getFilePath() {\n        return String.format(\"%s/hive-%s-%s.sql\", this.taskExecutionContext.getExecutePath(), this.taskExecutionContext.getTaskName(), this.taskExecutionContext.getTaskInstanceId());\n    }\n\n    @Override\n    protected void setMainJarName() {\n        //do nothing\n    }\n\n    /**\n     * write hive sql to filepath\n     *\n     * @param filePath file path\n     * @return write success?\n     */\n    private boolean writeExecutionContentToFile(String filePath) {\n        Path path = Paths.get(filePath);\n        try (BufferedWriter writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8)) {\n            writer.write(this.hiveClientParameters.getSql());\n            logger.info(\"file:\" + filePath + \"write success.\");\n            return true;\n        } catch (IOException e) {\n            logger.error(\"file:\" + filePath + \"write failed.please path auth.\");\n            e.printStackTrace();\n            return false;\n        }\n\n    }\n\n    @Override\n    public AbstractParameters getParameters() {\n        return this.hiveClientParameters;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring SPI Services for Custom Task in Apache DolphinScheduler\nDESCRIPTION: Instructions for setting up SPI (Service Provider Interface) configuration to register the custom task implementation with Apache DolphinScheduler. This includes creating the appropriate metadata files.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_7\n\nLANGUAGE: plain\nCODE:\n```\n# 1,Create META-INF/services folder under Resource, create the file with the same full class name of the interface\nzhang@xiaozhang resources % tree . /\n. /\n└── META-INF\n    └── services\n        └─ org.apache.dolphinscheduler.spi.task.TaskChannelFactory\n# 2, write the fully qualified class name of the implemented class in the file\nzhang@xiaozhang resources % more META-INF/services/org.apache.dolphinscheduler.spi.task.TaskChannelFactory\norg.apache.dolphinscheduler.plugin.task.hive.HiveClientTaskChannelFactory\n```\n\n----------------------------------------\n\nTITLE: Implementing Topological Sort Using BFS in Java\nDESCRIPTION: This Java implementation of topological sorting uses breadth-first traversal to detect cycles in a directed graph and produce a topological ordering of vertices. The algorithm maintains a queue of zero-indegree vertices and systematically removes them while updating indegrees of adjacent vertices, returning both cycle detection status and the sorting result if valid.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DAG.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class TopologicalSort {\n  /**\n   * 判断是否有环及拓扑排序结果\n   *\n   * 有向无环图(DAG)才有拓扑(topological)排序\n   * 广度优先遍历的主要做法：\n   *    1、遍历图中所有的顶点，将入度为0的顶点入队列。\n   *    2、从队列中poll出一个顶点，更新该顶点的邻接点的入度(减1)，如果邻接点的入度减1之后等于0，则将该邻接点入队列。\n   *    3、一直执行第2步，直到队列为空。\n   * 如果无法遍历完所有的结点，则意味着当前的图不是有向无环图。不存在拓扑排序。\n   *\n   *\n   * @return key返回的是状态, 如果成功(无环)为true, 失败则有环， value为拓扑排序结果(可能是其中一种)\n   */\n  private Map.Entry<Boolean, List<Vertex>> topologicalSort() {\n //入度为0的结点队列\n    Queue<Vertex> zeroIndegreeVertexQueue = new LinkedList<>();\n    //保存结果\n    List<Vertex> topoResultList = new ArrayList<>();\n    //保存入度不为0的结点\n    Map<Vertex, Integer> notZeroIndegreeVertexMap = new HashMap<>();\n\n    //扫描所有的顶点,将入度为0的顶点入队列\n    for (Map.Entry<Vertex, VertexInfo> vertices : verticesMap.entrySet()) {\n      Vertex vertex = vertices.getKey();\n      int inDegree = getIndegree(vertex);\n\n      if (inDegree == 0) {\n        zeroIndegreeVertexQueue.add(vertex);\n        topoResultList.add(vertex);\n      } else {\n        notZeroIndegreeVertexMap.put(vertex, inDegree);\n      }\n    }\n\n //扫描完后，没有入度为0的结点，说明有环，直接返回\n    if(zeroIndegreeVertexQueue.isEmpty()){\n      return new AbstractMap.SimpleEntry(false, topoResultList);\n    }\n\n    //采用topology算法, 删除入度为0的结点和它的关联边\n    while (!zeroIndegreeVertexQueue.isEmpty()) {\n      Vertex v = zeroIndegreeVertexQueue.poll();\n      //得到相邻结点\n      Set<Vertex> subsequentNodes = getSubsequentNodes(v);\n\n      for (Vertex subsequentVertex : subsequentNodes) {\n\n        Integer degree = notZeroIndegreeVertexMap.get(subsequentVertex);\n\n        if(--degree == 0){\n          topoResultList.add(subsequentVertex);\n          zeroIndegreeVertexQueue.add(subsequentVertex);\n          notZeroIndegreeVertexMap.remove(subsequentVertex);\n        }else{\n          notZeroIndegreeVertexMap.put(subsequentVertex, degree);\n        }\n\n      }\n    }\n\n    //notZeroIndegreeVertexMap如果为空, 表示没有环\n    AbstractMap.SimpleEntry resultMap = new AbstractMap.SimpleEntry(notZeroIndegreeVertexMap.size() == 0 , topoResultList);\n    return resultMap;\n\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying TaskConstants Class for Custom Time Functions\nDESCRIPTION: This code snippet illustrates the changes made to the TaskConstants class to add custom time functions like milli_unixtime, micro_unixtime, and nano_unixtime for more precise time expressions.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/How_Does_Live-broadcasting_Platform_Adapt_to_Apache_DolphinScheduler.md#2025-04-11_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\npublic static final String TIMESTAMP = \"timestamp\";\npublic static final String UNIX_TIME = \"unixtime\";\npublic static final String MILLI_UNIX_TIME = \"milli_unixtime\";\npublic static final String MICRO_UNIX_TIME = \"micro_unixtime\";\npublic static final String NANO_UNIX_TIME = \"nano_unixtime\";\n```\n\n----------------------------------------\n\nTITLE: Configuring pre-commit hooks for Python code style and linting\nDESCRIPTION: A pre-commit configuration file that includes isort for sorting imports, black for code formatting, flake8 for code quality checks, and autoflake for removing unused imports and variables.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndefault_stages: [commit, push]\ndefault_language_version:\n  # force all python hooks to run python3\n  python: python3\nrepos:\n  # Python API Hooks\n  - repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n      - id: isort\n        name: isort (python)\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n  - repo: https://github.com/pycqa/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\n        additional_dependencies: [\n          'flake8-docstrings>=1.6',\n          'flake8-black>=0.2',\n        ]\n        # pre-commit run in the root, so we have to point out the full path of configuration\n        args: [\n          --config,\n          .flake8\n        ]\n  - repo: https://github.com/pycqa/autoflake\n    rev: v1.4\n    hooks:\n      - id: autoflake\n        args: [\n          --remove-all-unused-imports,\n          --ignore-init-module-imports,\n          --in-place\n        ]\n```\n\n----------------------------------------\n\nTITLE: Implementing TaskLogAppender in Java for DolphinScheduler\nDESCRIPTION: A custom Logback FileAppender that generates log files for each task instance based on process definition ID, process instance ID, and task instance ID. It parses the thread name to extract the log identifier.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/architecture-design.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n/**\n * task log appender\n */\nPublic class TaskLogAppender extends FileAppender<ILoggingEvent> {\n \n     ...\n\n    @Override\n    Protected void append(ILoggingEvent event) {\n\n        If (currentlyActiveFile == null){\n            currentlyActiveFile = getFile();\n        }\n        String activeFile = currentlyActiveFile;\n        // thread name: taskThreadName-processDefineId_processInstanceId_taskInstanceId\n        String threadName = event.getThreadName();\n        String[] threadNameArr = threadName.split(\"-\");\n        // logId = processDefineId_processInstanceId_taskInstanceId\n        String logId = threadNameArr[1];\n        ...\n        super.subAppend(event);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dockerfile with Athena JDBC Driver for DolphinScheduler Worker\nDESCRIPTION: Dockerfile for building a custom DolphinScheduler worker image with Python and the Amazon Athena JDBC driver installed.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_8\n\nLANGUAGE: Dockerfile\nCODE:\n```\n##Example worker image DokcerFile\nFROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-worker:3.1.2\nRUN apt-get update && \\\n     apt-get install -y --no-install-recommends python3 && \\\n     apt-get install -y --no-install-recommends python3-pip && \\\n     rm -rf /var/lib/apt/lists/*\nRUN cd /opt/dolphinscheduler/libs/ && \\\n     wget https://s3.cn-north-1.amazonaws.com.cn/athena-downloads-cn/drivers/JDBC/SimbaAthenaJDBC-2.0.31.1000/AthenaJDBC42.jar\n```\n\n----------------------------------------\n\nTITLE: Creating a Scheduled Job for Kerberos Credential Refresh in Java\nDESCRIPTION: Implements a method that creates a scheduled job to check Kerberos TGT (Ticket Granting Ticket) validity and automatically re-login from keytab. This job runs daily to ensure the authentication credentials remain valid for long-running processes.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n /**\n     * * 定时更新凭证\n     */\n    private static void startCheckKeytabTgtAndReloginJob() {\n        // 每天循环，定时更新凭证\n        Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(() -> {\n            try {\n                UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n                logger.warn(\"Check Kerberos Tgt And Relogin From Keytab Finish.\");\n            } catch (IOException e) {\n                logger.error(\"Check Kerberos Tgt And Relogin From Keytab Error\", e);\n            }\n        }, 0, 1, TimeUnit.DAYS);\n        logger.info(\"Start Check Keytab TGT And Relogin Job Success.\");\n    }\n```\n\n----------------------------------------\n\nTITLE: MasterServer Configuration Properties in YAML\nDESCRIPTION: Configuration properties loaded from application.yml into MasterConfig class that defines core settings for the MasterServer including thread pools and execution parameters.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/2_The_most_comprehensive_introductory_tutorial_written_in_a_month.md#2025-04-11_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmaster:\n  exec-threads: 100\n  pre-exec-threads: 10\n  fetch-command-num: 10\n  heartbeat-interval: 10\n```\n\n----------------------------------------\n\nTITLE: Configuring Task and Workflow JSON Structure in DolphinScheduler\nDESCRIPTION: JSON structure defining task specifications and workflow relationships in DolphinScheduler. Includes task parameters, dependency conditions, retry settings, and execution configurations.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md#2025-04-11_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"id\": \"tasks-67639\",\n\t\"name\": \"f\",\n\t\"code\": \"\",\n\t\"params\": {\n\t\t\"resourceList\": [],\n\t\t\"localParams\": [],\n\t\t\"rawScript\": \"echo 7\"\n\t},\n\t\"desc\": \"\",\n\t\"runFlag\": \"NORMAL\",\n\t\"conditionResult\": {\n\t\t\"successNode\": [\n\t\t\t\"\"\n\t\t],\n\t\t\"failedNode\": [\n\t\t\t\"\"\n\t\t]\n\t},\n\t\"dependence\": {},\n\t\"maxRetryTimes\": \"0\",\n\t\"retryInterval\": \"1\",\n\t\"delayTime\": \"0\",\n\t\"timeout\": {\n\t\t\"strategy\": \"\",\n\t\t\"interval\": null,\n\t\t\"enable\": false\n\t},\n\t\"waitStartTimeout\": {},\n\t\"taskInstancePriority\": \"MEDIUM\",\n\t\"workerGroup\": \"hadoop\",\n\t\"preTasks\": [\n\t\t\"def\"\n\t],\n\t\"depList\": null\n},\n{\n\t\"type\": \"CONDITIONS\",\n\t\"id\": \"tasks-67387\",\n\t\"name\": \"def\",\n\t\"code\": \"\",\n\t\"params\": {},\n\t\"desc\": \"\",\n\t\"runFlag\": \"NORMAL\",\n\t\"conditionResult\": {\n\t\t\"successNode\": [\n\t\t\t\"e\"\n\t\t],\n\t\t\"failedNode\": [\n\t\t\t\"f\"\n\t\t]\n\t},\n\t\"dependence\": {\n\t\t\"relation\": \"AND\",\n\t\t\"dependTaskList\": [{\n\t\t\t\"relation\": \"AND\",\n\t\t\t\"dependItemList\": [{\n\t\t\t\t\t\"depTasks\": \"d\",\n\t\t\t\t\t\"status\": \"SUCCESS\"\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"depTasks\": \"d\",\n\t\t\t\t\t\"status\": \"FAILURE\"\n\t\t\t\t}\n\t\t\t]\n\t\t}]\n\t},\n\t\"maxRetryTimes\": \"0\",\n\t\"retryInterval\": \"1\",\n\t\"delayTime\": \"0\",\n\t\"timeout\": {\n\t\t\"strategy\": \"\",\n\t\t\"interval\": null,\n\t\t\"enable\": false\n\t},\n\t\"waitStartTimeout\": {},\n\t\"taskInstancePriority\": \"MEDIUM\",\n\t\"workerGroup\": \"hadoop\",\n\t\"preTasks\": [\n\t\t\"d\"\n\t],\n\t\"depList\": null\n}\n],\n\"tenantId\": 1,\n\"timeout\": 0\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring External PostgreSQL in Helm Values\nDESCRIPTION: YAML configuration to disable the built-in PostgreSQL and use an external PostgreSQL database with DolphinScheduler deployment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\npostgresql:\n  enabled: false\n  postgresqlUsername: \"root\"\n  postgresqlPassword: \"root\"\n  postgresqlDatabase: \"dolphinscheduler\"\n  persistence:\n    enabled: false\n    size: \"20Gi\"\n    storageClass: \"-\"\n```\n\n----------------------------------------\n\nTITLE: Injecting Service Governance Monitoring in Apache DolphinScheduler\nDESCRIPTION: This code snippet shows the modification made to the handleEvents method in WorkflowExecuteThread class to inject service governance platform data collection code for task monitoring.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/How_Does_Live-broadcasting_Platform_Adapt_to_Apache_DolphinScheduler.md#2025-04-11_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Code modification in handleEvents method\n// Injecting service governance platform data collection\n```\n\n----------------------------------------\n\nTITLE: Registering Alert Plugins in AlertPluginManager\nDESCRIPTION: The installPlugin method in AlertPluginManager discovers all classes implementing AlertChannelFactory and iterates through them to create AlertChannel instances. These instances are then registered in both the database and the channelKeyedById map for later use.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\n    private final Map<Integer, AlertChannel> channelKeyedById = new HashMap<>();\n    \n    @EventListener\n    public void installPlugin(ApplicationReadyEvent readyEvent) {\n        PrioritySPIFactory<AlertChannelFactory> prioritySPIFactory = new PrioritySPIFactory<>(AlertChannelFactory.class);\n        for (Map.Entry<String, AlertChannelFactory> entry : prioritySPIFactory.getSPIMap().entrySet()) {\n            String name = entry.getKey();\n            AlertChannelFactory factory = entry.getValue();\n            logger.info(\"Registering alert plugin: {} - {}\", name, factory.getClass());\n            final AlertChannel alertChannel = factory.create();\n            logger.info(\"Registered alert plugin: {} - {}\", name, factory.getClass());\n            final List<PluginParams> params = new ArrayList<>(factory.params());\n            params.add(0, warningTypeParams);\n            final String paramsJson = PluginParamsTransfer.transferParamsToJson(params);\n            final PluginDefine pluginDefine = new PluginDefine(name, PluginType.ALERT.getDesc(), paramsJson);\n            final int id = pluginDao.addOrUpdatePluginDefine(pluginDefine);\n            channelKeyedById.put(id, alertChannel);\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: ServiceLoader SPI Implementation in Java\nDESCRIPTION: Core implementation of the ServiceLoader class that handles SPI service discovery and loading. Shows how services are located and loaded using META-INF/services directory.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\npublic final class ServiceLoader<S> implements Iterable<S> {\n    private static final String PREFIX = \"META-INF/services/\";\n    private final Class<S> service;\n    private final ClassLoader loader;\n    private class LazyIterator implements Iterator<S> {\n        Class<S> service;\n        ClassLoader loader;\n        Enumeration<URL> configs = null;\n        String nextName = null;\n        private boolean hasNextService() {\n            if (configs == null) {\n                try {\n                    String fullName = PREFIX + service.getName();\n                    if (loader == null)\n                        configs = ClassLoader.getSystemResources(fullName);\n                    else\n                        configs = loader.getResources(fullName);\n                } catch (IOException x) {\n                }\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Kerberos Credential Refresh in DolphinScheduler's loadKerberosConf Method\nDESCRIPTION: Modified version of the loadKerberosConf method that now calls the startCheckKeytabTgtAndReloginJob method to schedule periodic Kerberos credential refreshes. This ensures continued authentication for long-running Hadoop operations.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_11\n\nLANGUAGE: java\nCODE:\n```\npublic static boolean loadKerberosConf(String javaSecurityKrb5Conf, String loginUserKeytabUsername,\n                                           String loginUserKeytabPath, Configuration configuration) throws IOException {\n        if (CommonUtils.getKerberosStartupState()) {\n            System.setProperty(Constants.JAVA_SECURITY_KRB5_CONF, StringUtils.defaultIfBlank(javaSecurityKrb5Conf,\n                    PropertyUtils.getString(Constants.JAVA_SECURITY_KRB5_CONF_PATH)));\n            configuration.set(Constants.HADOOP_SECURITY_AUTHENTICATION, Constants.KERBEROS);\n            UserGroupInformation.setConfiguration(configuration);\n            UserGroupInformation.loginUserFromKeytab(\n                    StringUtils.defaultIfBlank(loginUserKeytabUsername,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_USERNAME)),\n                    StringUtils.defaultIfBlank(loginUserKeytabPath,\n                            PropertyUtils.getString(Constants.LOGIN_USER_KEY_TAB_PATH)));\n            startCheckKeytabTgtAndReloginJob();  // 此处调用\n            return true;\n        }\n        return false;\n    }\n```\n\n----------------------------------------\n\nTITLE: Processing and Sending Alert Messages\nDESCRIPTION: The send method processes a list of alerts by retrieving alert plugin instances for each alert group, building alert data, and dispatching messages to each plugin instance. It tracks success/failure status for each alert and updates the alert status in the database accordingly.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic void send(List<Alert> alerts) {\n    for (Alert alert : alerts) {\n        // get alert group from alert\n        int alertId = Optional.ofNullable(alert.getId()).orElse(0);\n        int alertGroupId = Optional.ofNullable(alert.getAlertGroupId()).orElse(0);\n        List<AlertPluginInstance> alertInstanceList = alertDao.listInstanceByAlertGroupId(alertGroupId);\n        if (CollectionUtils.isEmpty(alertInstanceList)) {\n            logger.error(\"send alert msg fail,no bind plugin instance.\");\n            List<AlertResult> alertResults = Lists.newArrayList(new AlertResult(\"false\",\n                    \"no bind plugin instance\"));\n            alertDao.updateAlert(AlertStatus.EXECUTION_FAILURE, JSONUtils.toJsonString(alertResults), alertId);\n            continue;\n        }\n        AlertData alertData = AlertData.builder()\n                .id(alertId)\n                .content(alert.getContent())\n                .log(alert.getLog())\n                .title(alert.getTitle())\n                .warnType(alert.getWarningType().getCode())\n                .alertType(alert.getAlertType().getCode())\n                .build();\n\n        int sendSuccessCount = 0;\n        List<AlertResult> alertResults = new ArrayList<>();\n        for (AlertPluginInstance instance : alertInstanceList) {\n            AlertResult alertResult = this.alertResultHandler(instance, alertData);\n            if (alertResult != null) {\n                AlertStatus sendStatus = Boolean.parseBoolean(String.valueOf(alertResult.getStatus()))\n                        ? AlertStatus.EXECUTION_SUCCESS\n                        : AlertStatus.EXECUTION_FAILURE;\n                alertDao.addAlertSendStatus(sendStatus, JSONUtils.toJsonString(alertResult), alertId,\n                        instance.getId());\n                if (sendStatus.equals(AlertStatus.EXECUTION_SUCCESS)) {\n                    sendSuccessCount++;\n                    AlertServerMetrics.incAlertSuccessCount();\n                } else {\n                    AlertServerMetrics.incAlertFailCount();\n                }\n                alertResults.add(alertResult);\n            }\n        }\n        AlertStatus alertStatus = AlertStatus.EXECUTION_SUCCESS;\n        if (sendSuccessCount == 0) {\n            alertStatus = AlertStatus.EXECUTION_FAILURE;\n        } else if (sendSuccessCount < alertInstanceList.size()) {\n            alertStatus = AlertStatus.EXECUTION_PARTIAL_SUCCESS;\n        }\n        alertDao.updateAlert(alertStatus, JSONUtils.toJsonString(alertResults), alertId);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing TaskLogFilter in Java for DolphinScheduler\nDESCRIPTION: A custom Logback Filter that accepts log events only from threads with names starting with 'TaskLogInfo-'. This ensures that only relevant task logs are captured and processed.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/architecture-design.md#2025-04-11_snippet_1\n\nLANGUAGE: java\nCODE:\n```\n/**\n * task log filter\n */\nPublic class TaskLogFilter extends Filter<ILoggingEvent> {\n\n    @Override\n    Public FilterReply decide(ILoggingEvent event) {\n        If (event.getThreadName().startsWith(\"TaskLogInfo-\")){\n            Return FilterReply.ACCEPT;\n        }\n        Return FilterReply.DENY;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting task dependencies in PyDolphinScheduler using Python\nDESCRIPTION: This code snippet demonstrates how to set task dependencies in PyDolphinScheduler, showing upstream and downstream relationships between tasks.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Python_API_and_AWS_Support.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntask_parent.set_downstream(task_child_one)\\ntask_parent.set_downstream(task_child_two)\\ntask_parent >> task_child_one >> task_union\\ntask_parent >> task_child_two >> task_union\n```\n\n----------------------------------------\n\nTITLE: Implementing HttpAlertChannel in Java\nDESCRIPTION: This snippet shows the implementation of the AlertChannel interface for HTTP alerts. The process method retrieves alert parameters and sends the HTTP request using the HttpSender class, returning an AlertResult object indicating success or failure.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\npublic final class HttpAlertChannel implements AlertChannel {\n    @Override\n    public AlertResult process(AlertInfo alertInfo) {\n        AlertData alertData = alertInfo.getAlertData();\n        Map<String, String> paramsMap = alertInfo.getAlertParams();\n        if (null == paramsMap) {\n            return new AlertResult(\"false\", \"http params is null\");\n        }\n        return new HttpSender(paramsMap).send(alertData.getContent());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Feature Preprocessing for XGBoost in Python\nDESCRIPTION: This code snippet shows a configuration file for feature preprocessing used in XGBoost model training. It defines the item types, user types, and feature sets for preprocessing, supporting custom transformers and hot updates for both XGBoost and TensorFlow models.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Lizhi-case-study.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n  \"itemType\": \"sound\",\n  \"itemFeatures\": [\"soundId\", \"duration\", \"categoryId\"],\n  \"userType\": \"user\",\n  \"userFeatures\": [\"userId\", \"age\", \"gender\"],\n  \"relatedItemTypes\": [\"category\"],\n  \"relatedItemFeatures\": [\"categoryId\", \"categoryName\"],\n  \"featureTransformers\": {\n    \"duration\": \"log_transform\",\n    \"age\": \"min_max_scaler\",\n    \"gender\": \"one_hot_encoding\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Output Variables in Shell Tasks for Parameter Transfer\nDESCRIPTION: Demonstrates how to set an output variable 'trans' in a shell task that can be used by downstream tasks. The variable is defined using the ${setValue(key=value)} syntax in the task output.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache-DolphinScheduler-2.0.1.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\necho'${setValue(trans=hello trans)}'\n```\n\n----------------------------------------\n\nTITLE: Fixing Resource Authorization for Users in Java\nDESCRIPTION: Java code modification for ResourcePermissionCheckServiceImpl.java to allow regular users to access resources authorized by administrators by returning the combined set of owned and relation resources.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_9\n\nLANGUAGE: java\nCODE:\n```\n@Override\n        public Set<Integer> listAuthorizedResource(int userId, Logger logger) {\n            List<Resource> relationResources;\n            if (userId == 0) {\n                relationResources = new ArrayList<>();\n            } else {\n                // query resource relation\n                List<Integer> resIds = resourceUserMapper.queryResourcesIdListByUserIdAndPerm(userId, 0);\n                relationResources = CollectionUtils.isEmpty(resIds) ? new ArrayList<>() : resourceMapper.queryResourceListById(resIds);\n            }\n            List<Resource> ownResourceList = resourceMapper.queryResourceListAuthored(userId, -1);\n            relationResources.addAll(ownResourceList);\n            return relationResources.stream().map(Resource::getId).collect(toSet()); // 解决资源文件授权无效的问题\n//            return ownResourceList.stream().map(Resource::getId).collect(toSet());\n        }\n```\n\n----------------------------------------\n\nTITLE: Alert Message Polling Thread Implementation\nDESCRIPTION: The run method in AlertSenderService implements a polling thread that continuously queries for pending alerts from the database and processes them. It uses a sleep interval to control polling frequency and handles exceptions to ensure continuous operation.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\n@Override\npublic void run() {\n    logger.info(\"alert sender started\");\n    while (!ServerLifeCycleManager.isStopped()) {\n        try {\n            List<Alert> alerts = alertDao.listPendingAlerts();\n            AlertServerMetrics.registerPendingAlertGauge(alerts::size);\n            this.send(alerts);\n            ThreadUtils.sleep(Constants.SLEEP_TIME_MILLIS * 5L);\n        } catch (Exception e) {\n            logger.error(\"alert sender thread error\", e);\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Dynamic Python Workflow Detection\nDESCRIPTION: Bash script to dynamically find and execute all Python workflow files in a directory.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfor file in $(find . -name \"*.py\"); do\n    python \"$file\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Clearing Workflow Cache using Arthas OGNL\nDESCRIPTION: An OGNL command to remove a problematic workflow instance from the in-memory cache by calling the processInstanceExecCacheManagerImpl bean. This stops the infinite loop by preventing the instance from being reprocessed.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/How_does_Apache_Dolphinscheduler_solve_the_infinite_loop_of_Master_service_without_restarting.md#2025-04-11_snippet_2\n\nLANGUAGE: ognl\nCODE:\n```\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(\"processInstanceExecCacheManagerImpl\").removeByProcessInstanceId(\"工作流实例id\")'\n```\n\n----------------------------------------\n\nTITLE: Cleaning Workflow Database Records using Arthas OGNL\nDESCRIPTION: A set of OGNL commands to delete database records for a problematic workflow instance. These commands access the processServiceImpl bean to remove instance data, subprocesses, process maps, and task instances.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/How_does_Apache_Dolphinscheduler_solve_the_infinite_loop_of_Master_service_without_restarting.md#2025-04-11_snippet_1\n\nLANGUAGE: ognl\nCODE:\n```\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(\"processServiceImpl\").deleteWorkProcessInstanceById(\"工作流实例id\")'\n```\n\nLANGUAGE: ognl\nCODE:\n```\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(\"processServiceImpl\").deleteAllSubWorkProcessByParentId(\"工作流实例id\")'\n```\n\nLANGUAGE: ognl\nCODE:\n```\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicationContext@applicationContext.getBean(\"processServiceImpl\").deleteWorkProcessMapByParentId(\"工作流实例id\")'\n```\n\nLANGUAGE: ognl\nCODE:\n```\nognl  '@org.apache.dolphinscheduler.service.bean.SpringApplicati\\nonContext@applicationContext.getBean(\"processServiceImpl\").deleteWorkTaskInstanceByProcessInstanceId(\"工作流实例id\")'\n```\n\n----------------------------------------\n\nTITLE: MasterServer Component Classification\nDESCRIPTION: The main component types in MasterServer including configuration beans, task processors, event handlers and logging components that work together to manage the scheduling system.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/2_The_most_comprehensive_introductory_tutorial_written_in_a_month.md#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nFirst Type: MasterConfig, MasterRegistryClient, MasterSchedulerService, Scheduler\nSecond Type: Task processors with Processor suffix\nThird Type: EventExecuteService and FailoverExecuteThread\nFourth Type: LoggerRequestProcessor\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Training Parameters in DolphinScheduler\nDESCRIPTION: This snippet demonstrates the configuration of XGBoost training parameters within a DolphinScheduler node. It shows how various hyperparameters and training settings are exposed in the UI for easy adjustment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Lizhi-case-study.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n  \"max_depth\": 6,\n  \"learning_rate\": 0.1,\n  \"n_estimators\": 100,\n  \"objective\": \"binary:logistic\",\n  \"booster\": \"gbtree\",\n  \"subsample\": 0.8,\n  \"colsample_bytree\": 0.8,\n  \"eval_metric\": [\"auc\", \"logloss\"],\n  \"early_stopping_rounds\": 10\n}\n```\n\n----------------------------------------\n\nTITLE: Updating DolphinScheduler Deployment with Helm\nDESCRIPTION: Bash command to update an existing DolphinScheduler deployment with new configurations using Helm.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Implementing Topological Sort using Breadth-First Search in Java\nDESCRIPTION: Java implementation of topological sorting algorithm using breadth-first search. The algorithm detects cycles in the graph and returns both the sorting result and whether the graph is acyclic. It uses a queue for vertices with zero in-degree and maintains a map for tracking in-degrees of remaining vertices.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DAG.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class TopologicalSort {\n  /**\n   * Determine whether there is a ring and the result of topological sorting\n   *\n   * Only directed acyclic graph (DAG) has topological sorting\n   * The main methods of breadth first search:\n   *    1、Iterate over all the vertices in the graph, and put the vertices whose in-degree is 0 into the queue.\n   *    2、A vertex is polled from the queue, and the in-degree of the adjacent point of the vertex is updated (minus 1). If the in-degree of the adjacent point is reduced by 1 and then equals to 0, the adjacent point is entered into the queue.\n   *    3、Keep executing step 2 until the queue is empty.\n   * If it is impossible to traverse all the vertics, it means that the current graph is not a directed acyclic graph. There is no topological sort.\n   *\n   *\n   * @return key returns the state, true if successful (no-loop), value if failed (loop), value is the result of topological sorting (could be one of these)\n   */\n  private Map.Entry<Boolean, List<Vertex>> topologicalSort() {\n // Node queue with an in-degree of 0\n    Queue<Vertex> zeroIndegreeVertexQueue = new LinkedList<>();\n    // Save the results\n    List<Vertex> topoResultList = new ArrayList<>();\n    // Save the nodes whose in-degree is not 0\n    Map<Vertex, Integer> notZeroIndegreeVertexMap = new HashMap<>();\n\n    // Scan all nodes and queue vertices with in-degree 0\n    for (Map.Entry<Vertex, VertexInfo> vertices : verticesMap.entrySet()) {\n      Vertex vertex = vertices.getKey();\n      int inDegree = getIndegree(vertex);\n\n      if (inDegree == 0) {\n        zeroIndegreeVertexQueue.add(vertex);\n        topoResultList.add(vertex);\n      } else {\n        notZeroIndegreeVertexMap.put(vertex, inDegree);\n      }\n    }\n\n // After scanning, there is no node with an in-degree of 0, indicating that there is a loop, and return directly\n    if(zeroIndegreeVertexQueue.isEmpty()){\n      return new AbstractMap.SimpleEntry(false, topoResultList);\n    }\n\n    // Using the topology algorithm, delete the node with an in-degree of 0 and its associated edges\n    while (!zeroIndegreeVertexQueue.isEmpty()) {\n      Vertex v = zeroIndegreeVertexQueue.poll();\n      // Get the adjacent nodes\n      Set<Vertex> subsequentNodes = getSubsequentNodes(v);\n\n      for (Vertex subsequentVertex : subsequentNodes) {\n\n        Integer degree = notZeroIndegreeVertexMap.get(subsequentVertex);\n\n        if(--degree == 0){\n          topoResultList.add(subsequentVertex);\n          zeroIndegreeVertexQueue.add(subsequentVertex);\n          notZeroIndegreeVertexMap.remove(subsequentVertex);\n        }else{\n          notZeroIndegreeVertexMap.put(subsequentVertex, degree);\n        }\n\n      }\n    }\n\n    //notZeroIndegreeVertexMap If it is empty, it means there is no ring\n    AbstractMap.SimpleEntry resultMap = new AbstractMap.SimpleEntry(notZeroIndegreeVertexMap.size() == 0 , topoResultList);\n    return resultMap;\n\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Redshift Operator Implementation in Airflow\nDESCRIPTION: Example of a custom Airflow operator extending PostgresOperator to create a RedshiftOperator. This shows the type of custom code that might need migration to DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\nclass RedshiftOperator(PostgresOperator):\n    def __init__(\n        self,\n        *,\n        sql: str | Iterable[str],\n        my_custom_conn_id: str = 'postgres_default',\n        autocommit: bool = False,\n        parameters: Iterable | Mapping | None = None,\n        database: str | None = None,\n        runtime_parameters: Mapping | None = None,\n        **kwargs,\n    ) -> None:\n        super().__init__(\n            sql=sql,\n            postgres_conn_id=my_custom_conn_id,\n            autocommit=autocommit,\n            parameters=parameters,\n            database=database,\n            runtime_parameters=runtime_parameters,\n            **kwargs,\n        )\n```\n\n----------------------------------------\n\nTITLE: Example taskDefinitionJson Format\nDESCRIPTION: A JSON example showing the format of taskDefinitionJson parameter used in task definition APIs. It includes task details such as name, type, parameters, worker group settings, and timeout configurations.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md#2025-04-11_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[{\"name\":\"test\",\"description\":\"\",\"task_type\":\"SHELL\",\"task_params\":[],\"flag\":0,\"task_priority\":0,\"worker_group\":\"default\",\"fail_retry_times\":0,\"fail_retry_interval\":0,\"timeout_flag\":0,\"timeout_notify_strategy\":0,\"timeout\":0,\"delay_time\":0,\"resource_ids\":\"\"}]\n```\n\n----------------------------------------\n\nTITLE: Modifying WorkflowExecuteThread for Service Governance Integration in Java\nDESCRIPTION: Code snippet showing the modification of the handleEvents method in WorkflowExecuteThread class to inject service governance platform data collection code. This allows task monitoring information to be reported to YY Live's service governance platform.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_Does_Live-broadcasting_Platform_Adapt_to_Apache_DolphinScheduler.md#2025-04-11_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nprivate void handleEvents() throws Exception {\n  // ... existing code ...\n\n  // Injected code for service governance platform\n  if (event.getType() == StateEventType.TASK_STATE_CHANGE) {\n    TaskStateEvent taskStateEvent = (TaskStateEvent) event;\n    // Report task state to service governance platform\n    reportToServiceGovernancePlatform(taskStateEvent);\n  }\n\n  // ... rest of existing code ...\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring tasks in PyDolphinScheduler using Python\nDESCRIPTION: These examples show how to declare different types of tasks in PyDolphinScheduler, including Shell, SQL, Python, and HTTP tasks.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Python_API_and_AWS_Support.md#2025-04-11_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nshell_task = Shell(name=\"shell_task\", command=\"echo hello\")\\nsql_task = Sql(name=\"sql_task\", sql=\"select * from table\")\\npython_task = Python(name=\"python_task\", definition=python_def)\\nhttp_task = Http(name=\"http_task\", url=\"https://www.apple.com\", method=\"GET\")\n```\n\n----------------------------------------\n\nTITLE: Communication-Based Test Style with Mockito in Java\nDESCRIPTION: Unit test using Mockito to mock dependencies and verify interaction between components, demonstrating communication-based testing approach.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_8\n\nLANGUAGE: java\nCODE:\n```\npublic class EmailSenderTest {\n    @Test\n    public void greetUser() {\n        // arrange\n        SendService service = Mockito.mock(SendService.class);\n        EmailSender sender = new EmailSender(service);\n        String email = \"user@email.com\";\n        when(service.send(email, Mockito.anyString())).thenReturn(true);\n        // act\n        boolean actual = sender.greetUser(email);\n        // assert\n        Assert.assertEquals(true, actual);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: MasterSchedulerService Thread Pool Configuration\nDESCRIPTION: Thread pool configuration for workflow execution showing the core and maximum thread settings.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/2_The_most_comprehensive_introductory_tutorial_written_in_a_month.md#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nthreadPoolExecutor:\n  corePoolSize: 100\n  maximumPoolSize: 100\n  keepAliveTime: 60\n  workQueue: LinkedBlockingQueue\n```\n\n----------------------------------------\n\nTITLE: Refactored Order and PriceCalculator Classes in Java\nDESCRIPTION: Refactored version of the Order class with the pricing logic extracted into a separate PriceCalculator class to improve testability and follow single responsibility principle.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_10\n\nLANGUAGE: java\nCODE:\n```\npublic class Order {\n    private Customer customer;\n    private List<Product> products;\n    \n    public String generateDescription() {\n        PriceCalculator calc = new PriceCalculator();\n\t\treturn \"Customer name: \" + customer.getName() +\n            \", total price: \" + calc.getPrice(customer, products);\n    }\n}\n\npublic class PriceCalculator {\n    public double getPrice(Customer customer, List<Product> products) {\n        double basePrice;\t// 基于 products 计算\n        double discounts;\t// 基于 customer 计算\n        double taxes;\t// 基于 products 计算\n        // do some calculation\n        return basePrice - discounts + taxes;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: ServerNodeManager Node Synchronization Process\nDESCRIPTION: Visualization of the node synchronization process showing how data flows between ZooKeeper and the database through ServerNodeManager.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/2_The_most_comprehensive_introductory_tutorial_written_in_a_month.md#2025-04-11_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nload() -> ZK -> MasterPriorityQueue -> Database\nUpdateMasterNodes() -> Real-time sync\nThread(10s) -> Database sync\n```\n\n----------------------------------------\n\nTITLE: Fixing Null Pointer Exception in Upgrade Process - Part 1\nDESCRIPTION: Java code modification for UpgradeDao.java to handle null values in processDefinitionMap during the upgrade process, preventing NullPointerException at line 517.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nif (jsonNodeDefinitionId != null) {\n    if (processDefinitionMap.get(jsonNodeDefinitionId.asInt()) != null) {\n        param.put(\"processDefinitionCode\",processDefinitionMap.get(jsonNodeDefinitionId.asInt()).getCode());\n        param.remove(\"processDefinitionId\");\n    } else {\n        logger.error(\"*******************error\");\n        logger.error(\"*******************param:\" + param);\n        logger.error(\"*******************jsonNodeDefinitionId:\" + jsonNodeDefinitionId);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring External Database and ZooKeeper for DolphinScheduler in Kubernetes\nDESCRIPTION: YAML configuration for setting up external PostgreSQL database and ZooKeeper connections for Apache DolphinScheduler in a Kubernetes deployment. The configuration includes database connection details and options for using either internal or external ZooKeeper instances.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n## external database will be used, otherwise Dolphinscheduler's database will be used.\nexternalDatabase:\ntype: \"postgresql\"\ndriver: \"org.postgresql.Driver\"\nhost: \"192.168.1.100\"\nport: \"5432\"\nusername: \"admin\"\npassword: \"password\"\ndatabase: \"dolphinscheduler\"\nparams: \"characterEncoding=utf8\"\n\n## If not exists external zookeeper, by default, Dolphinscheduler's zookeeper will use it.\nzookeeper:\nenabled: false\nfourlwCommandsWhitelist: \"srvr,ruok,wchs,cons\"\npersistence:\n  enabled: false\n  size: \"20Gi\"\n  storageClass: \"storage-nfs\"\nzookeeperRoot: \"/dolphinscheduler\"\n\n## If exists external zookeeper, and set zookeeper.enable value to false.\n## If zookeeper.enable is false, Dolphinscheduler's zookeeper will use it.\nexternalZookeeper:\nzookeeperQuorum: \"zookeeper-0.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.zookeeper.svc.cluster.local:2181\"\nzookeeperRoot: \"/dolphinscheduler\"\n```\n\n----------------------------------------\n\nTITLE: Service Layer Testing with Mockito in Java\nDESCRIPTION: A unit test for a service class using Mockito to mock DAO layer dependencies. It demonstrates how to test service methods without actual database operations.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_5\n\nLANGUAGE: java\nCODE:\n```\n@RunWith(MockitoJUnitRunner.class)\npublic class UsersServiceTest {\n    \n    private static final Logger logger = LoggerFactory.getLogger(UsersServiceTest.class);\n\n    @InjectMocks\n    private UsersServiceImpl usersService;\n\n    @Mock\n    private UserMapper userMapper;\n    \n    @Test\n    public void testQueryUser() {\n        String userName = \"userTest0001\";\n        String userPassword = \"userTest0001\";\n        when(userMapper.queryUserByNamePassword(userName, EncryptionUtils.getMd5(userPassword))).thenReturn(getGeneralUser());\n        User queryUser = usersService.queryUser(userName, userPassword);\n        logger.info(queryUser.toString());\n        Assert.assertTrue(queryUser != null);\n    }\n    \n    /**\n     * get user\n     */\n    private User getGeneralUser() {\n        User user = new User();\n        user.setUserType(UserType.GENERAL_USER);\n        user.setUserName(\"userTest0001\");\n        user.setUserPassword(\"userTest0001\");\n        return user;\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Scaling DolphinScheduler Components in Kubernetes\nDESCRIPTION: YAML configuration for scaling the number of master, worker, alert, and API component replicas in an Apache DolphinScheduler deployment using Kubernetes StatefulSets and Deployments.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmaster:\n\n ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\npodManagementPolicy: \"Parallel\"\n ## Replicas is the desired number of replicas of the given Template.\nreplicas: \"5\"\n\nworker:\n ## PodManagementPolicy controls how pods are created during initial scale up, when replacing pods on nodes, or when scaling down.\npodManagementPolicy: \"Parallel\"\n ## Replicas is the desired number of replicas of the given Template.\nreplicas: \"5\"\n\n\nalert:\n ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\nreplicas: \"3\"\n\napi:\n ## Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.\nreplicas: \"3\"\n```\n\n----------------------------------------\n\nTITLE: REST Controller Unit Test with MockMvc in Java\nDESCRIPTION: A unit test for a REST controller using MockMvc to simulate HTTP requests. It demonstrates how to test controller endpoints without starting a web server or building a Spring Context.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class RestUserControllerTest extends AbstractRestControllerTest {\n\n    private static final Logger logger = LoggerFactory.getLogger(RestUserControllerTest.class);\n\n    @Test\n    public void testLogin() throws Exception {\n        // (2) Act\n        MvcResult mvcResult = mockMvc.perform(post(\"/login\"))\n                .andExpect(status().isOk())\t// (3) Assert\n                .andReturn();\n\n        MockHttpServletResponse response = mvcResult.getResponse();\n        String content = response.getContentAsString();\n\n        Result result = JSONUtils.parseObject(content, Result.class);\n        // (3) Assert\n        Assert.assertEquals(Status.SUCCESS.getCode(), result.getCode().intValue());\n        logger.info(content);\n    }\n\n    @Override\n    protected Object getTestedController() {\n        // (1) Arrange\n        UserService service = new UserService();\n        return new UserController(service);\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Pulling and Pushing DolphinScheduler Docker Image\nDESCRIPTION: Shell commands for pulling the official DolphinScheduler image, tagging it for a private registry, and pushing it to the private registry. This enables offline installation.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull apache/dolphinscheduler:1.3.9\ndock tag apache/dolphinscheduler:1.3.9 \nharbor.abc.com/apache/dolphinscheduler:1.3.9\ndocker push apache/dolphinscheduler:1.3.9\n```\n\n----------------------------------------\n\nTITLE: Global Variable Definition in Shell Task for Switch Task Condition\nDESCRIPTION: Example showing how to define a global variable named 'id' with value '1' in a shell task that can be used in downstream Switch task conditions to control workflow branching.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Apache-DolphinScheduler-2.0.1.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\necho '${setValue(id=1)}'\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Multiple Workflow Execution\nDESCRIPTION: GitHub Actions configuration for executing multiple DolphinScheduler Python workflows.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: Execute Workflows\non:    \n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        run: |\n          for file in $(find . -name \"*.py\"); do\n            python \"$file\"\n          done\n```\n\n----------------------------------------\n\nTITLE: EmailSender Implementation for Communication-Based Testing in Java\nDESCRIPTION: EmailSender class that depends on an external service for sending emails, demonstrating a class that communicates with dependencies.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_7\n\nLANGUAGE: java\nCODE:\n```\npublic class EmailSender {\n    \n    private SendService service;\n    \n    public EmailSender(SendService service) {\n\t\tthis.service = service;\n    }\n\n    public boolean greetUser(String email) {\n        String message = \"Hello!\";\n        return service.send(email, message);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Build Configuration in Shell\nDESCRIPTION: Commands to customize the documentation source repository and branch using environment variables for local testing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/HOW_PREPARE_WORK.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n# your github id already fork apache/dolphinscheduler\nexport PROJECT_ORG=\"<YOUR-GITHUB-ID>\"\n# the branch name you want to fetch and deploy website's document\nexport PROJECT_BRANCH_NAME=\"<DEPLOY-BRANCH-OF-FORK>\"\n```\n\n----------------------------------------\n\nTITLE: Creating SPI Configuration Files for Task Plugin\nDESCRIPTION: Shows the directory structure and content of the SPI configuration file required to register the custom task plugin with Apache DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n# 1,Resource下创建META-INF/services文件夹,创建接口全类名相同的文件\nzhang@xiaozhang resources % tree ./\n./\n└── META-INF\n    └── services\n        └── org.apache.dolphinscheduler.spi.task.TaskChannelFactory\n# 2,在文件中写入实现类的全限定类名\nzhang@xiaozhang resources % more META-INF/services/org.apache.dolphinscheduler.spi.task.TaskChannelFactory \norg.apache.dolphinscheduler.plugin.task.hive.HiveClientTaskChannelFactory\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Custom DolphinScheduler Image\nDESCRIPTION: Shell commands to build the custom DolphinScheduler Docker image with additional components and push it to a private registry.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t harbor.abc.com/apache/dolphinscheduler:1.3.9-mysql-oracle-datax .\ndocker push harbor.abc.com/apache/dolphinscheduler:1.3.9-mysql-oracle-datax\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Migration Rules with YAML in Air2phin\nDESCRIPTION: YAML configuration example for defining custom migration rules in Air2phin. This example shows how to convert a custom RedshiftOperator to a DolphinScheduler SQL task.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: RedshiftOperator\n\nmigration:\n  module:\n    - action: replace\n      src: utils.operator.RedshiftOperator.RedshiftOperator\n      dest: pydolphinscheduler.tasks.sql.Sql\n    - action: add\n      module: pydolphinscheduler.resources_plugin.Local\n  parameter:\n    - action: replace\n      src: task_id\n      dest: name\n    - action: add\n      arg: datasource_name\n      default:\n        type: str\n        value: \"redshift_read_conn\"\n    - action: add\n      arg: resource_plugin\n      default: \n        type: code\n        value: Local(prefix=\"/path/to/dir/\")\n```\n\n----------------------------------------\n\nTITLE: MasterConfig Configuration Properties\nDESCRIPTION: Configuration properties loaded from application.yml into MasterConfig including thread pools, execution parameters and scheduling settings.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/2_The_most_comprehensive_introductory_tutorial_written_in_a_month.md#2025-04-11_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmaster.exec-threads: 100\nmaster.pre-exec-threads: 10\nmaster.fetch-command-num: 10\nmaster.heartbeat-interval: 10\n```\n\n----------------------------------------\n\nTITLE: Extracting DolphinScheduler Source Package\nDESCRIPTION: Commands to download and extract the Apache DolphinScheduler source package version 3.1.2 on the jump server.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ tar -zxvf apache-dolphinscheduler-<version>-src.tar.gz\n$ cd apache-dolphinscheduler-<version>-src/deploy/kubernetes/dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Setting Resource Requirements for DolphinScheduler Services\nDESCRIPTION: YAML configuration defining CPU and memory resource limits and requests for DolphinScheduler master and worker services.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmaster:\n   resources:\n     limits:\n       memory: \"8Gi\"\n       cpu: \"4\"\n     requests:\n       memory: \"2Gi\"\n       cpu: \"500m\"\nworker:\n   resources:\n     limits:\n       memory: \"8Gi\"\n       cpu: \"4\"\n     requests:\n       memory: \"2Gi\"\n       cpu: \"500m\"\napi:\n   ...\nalert:\n   ...\n```\n\n----------------------------------------\n\nTITLE: Age Validation Method in Java\nDESCRIPTION: Implementation of isAdult method that validates if a person is an adult based on age, throwing an exception for invalid ages.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_2\n\nLANGUAGE: java\nCODE:\n```\npublic boolean isAdult(int age) {\n    if (age < 0 || age > 120) {\n        throw new IllegalArgumentException(\"The age is illegal\");\n    }\n    return age >= 18;\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Alert Channel from AlertPluginManager\nDESCRIPTION: The getAlertChannel method in AlertPluginManager provides access to registered AlertChannel instances by their plugin ID. This method returns an Optional containing the AlertChannel if it exists in the channelKeyedById map.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\npublic Optional<AlertChannel> getAlertChannel(int id) {\n    return Optional.ofNullable(channelKeyedById.get(id));\n}\n```\n\n----------------------------------------\n\nTITLE: Price Engine Implementation for Output-Based Testing in Java\nDESCRIPTION: A PriceEngine class that calculates product discounts based on the number of products, demonstrating a function that returns a value without changing state.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_3\n\nLANGUAGE: java\nCODE:\n```\npublic class PriceEngine {\n    public double calculateDiscount(Product[] products) {\n        double discount = products.length * 0.01;\n        return Math.min(discount, 0.02);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Fixing Null Pointer Exception in Upgrade Process - Part 2\nDESCRIPTION: Java code modification for UpgradeDao.java to handle null values in processCodeTaskNameCodeEntry.getValue() during the upgrade process, preventing NullPointerException at line 675.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nlong taskCode =0;\n                            if (processCodeTaskNameCodeEntry.getValue() != null\n                                    &&processCodeTaskNameCodeEntry.getValue().get(depTasks)!=null){\n                                taskCode =processCodeTaskNameCodeEntry.getValue().get(depTasks);\n                            }else{\n                                logger.error(\"******************** depTasks:\"+depTasks);\n                                logger.error(\"******************** taskCode not in \"+JSONUtils.toJsonString(processCodeTaskNameCodeEntry));\n                            }\n                            dependItem.put(\"depTaskCode\", taskCode);\n```\n\n----------------------------------------\n\nTITLE: Adding Kerberos Principal Pattern to HDFS Configuration in XML\nDESCRIPTION: XML configuration to be added to the hdfs-site.xml file to resolve IllegalArgumentException related to Kerberos principal name in the resource center when using HDFS with Kerberos authentication.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n    <name>dfs.namenode.kerberos.principal.pattern</name>\n    <value>*</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: Setting Shell Task Parameter for Task Communication in DolphinScheduler\nDESCRIPTION: Example of how to set output variables in a shell task that can be passed to downstream tasks. The code uses the ${setValue(trans=hello trans)} syntax to create a variable named 'trans' with the value 'hello trans'.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Apache-DolphinScheduler-2.0.1.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\necho '${setValue(trans=hello trans)}'\n```\n\n----------------------------------------\n\nTITLE: Packaging and Deploying HiveClient Task Plugin\nDESCRIPTION: Shows the commands for packaging the plugin JAR file and deploying it to Apache DolphinScheduler by copying it to the lib directory and restarting the server.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n## 1,打包\nmvn clean install\n## 2,部署\ncp ./target/dolphinscheduler-task-hiveclient-1.0.jar $DOLPHINSCHEDULER_HOME/lib/\n## 3,restart dolphinscheduler server\n```\n\n----------------------------------------\n\nTITLE: Copying Log Files to New Path in Shell\nDESCRIPTION: Shell command to copy log files from the old version's log directory to the new log directory structure after upgrading DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\ncp -r {旧版本dolphinscheduler目录}/logs/[1-9]* {新版本dolphinscheduler目录}/worker-server/logs/*\n```\n\n----------------------------------------\n\nTITLE: Using Multiprocessing with Air2phin Migration Tool\nDESCRIPTION: Command example showing how to use multiprocessing with Air2phin to convert Airflow DAGs more efficiently by using multiple cores.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# use multiprocess to convert the airflow dags files\nair2phin migrate -i --multiprocess 12 ~/airflow/dags\n```\n\n----------------------------------------\n\nTITLE: LDAP Configuration in YAML\nDESCRIPTION: YAML configuration for integrating LDAP authentication with DolphinScheduler in the api-server/conf/application.yaml file.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nsecurity:\n  authentication:\n    # Authentication types (supported types: PASSWORD,LDAP)\n    type: LDAP\n    # IF you set type `LDAP`, below config will be effective\n    ldap:\n      # ldap server config\n      urls: xxx\n      base-dn: xxx\n      username: xxx\n      password: xxx\n      user:\n        # admin userId when you use LDAP login\n        admin: xxx\n        identity-attribute: xxx\n        email-attribute: xxx\n        # action when ldap user is not exist (supported types: CREATE,DENY)\n        not-exist-action: CREATE\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL Database Connection for DolphinScheduler\nDESCRIPTION: Database connection properties for MySQL in the datasource.properties file of the dolphinscheduler-dao module.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DS_run_in_windows.md#2025-04-11_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# mysql\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://192.168.2.227:3306/dolphinschedulerKou?useUnicode=true&characterEncoding=UTF-8\nspring.datasource.username=root\nspring.datasource.password=Dm!23456\n```\n\n----------------------------------------\n\nTITLE: Using Custom Migration Rules with Air2phin\nDESCRIPTION: Command showing how to use custom migration rules defined in YAML files with the Air2phin migration tool.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nair2phin migrate --custom-rules /path/to/RedshiftOperator.yaml ~/airflow/dags\n```\n\n----------------------------------------\n\nTITLE: Updating Task Instance Log Paths in SQL\nDESCRIPTION: SQL command to update log paths in the task instance table after upgrading DolphinScheduler, addressing the issue where log paths changed from '/logs/' to '/worker-server/logs/'.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nupdate t_ds_task_instance set log_path=replace(log_path,'/logs/','/worker-server/logs/');\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Robot Access for Apache DolphinScheduler Website\nDESCRIPTION: This snippet defines the default access rules for all web crawlers and robots. It allows unrestricted access to the entire website by not specifying any disallowed paths.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/public/robots.txt#2025-04-11_snippet_0\n\nLANGUAGE: robotstxt\nCODE:\n```\n# https://www.robotstxt.org/robotstxt.html\nUser-agent: *\nDisallow:\n```\n\n----------------------------------------\n\nTITLE: Installing PyDolphinScheduler using pip in Python\nDESCRIPTION: This command installs the latest version of PyDolphinScheduler using pip package manager in Python.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Python_API_and_AWS_Support.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install apache-dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Configuring MySQL datasource properties for DolphinScheduler\nDESCRIPTION: Database connection configuration for the DolphinScheduler dao module. This specifies the MySQL driver, connection URL, username, and password for the application to connect to the database.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n# mysql\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://localhost:3306/dolphinschedulerTest?useUnicode=true&characterEncoding=UTF-8\nspring.datasource.username=root\nspring.datasource.password=rootroot\n```\n\n----------------------------------------\n\nTITLE: Unit Test for Calculator Using 3A Pattern in Java\nDESCRIPTION: Demonstrates the Arrange-Act-Assert pattern for testing the Calculator's sum method with clear separation of the three test phases.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class CalculatorTest {\n    @Test\n    public void sum() {\n        // Arrange\n        long a = 1L, b = 2L;\n        Calculator calculator = new Calculator();\n        // Act\n        long actual = calculator.sum(a, b);\n        // Assert\n        long expected = 3L;\n        assertEquals(expected, actual);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Deployment in DolphinScheduler\nDESCRIPTION: This configuration snippet shows how model deployment is set up in DolphinScheduler. It specifies the paths for the model and preprocessing configuration files on HDFS, as well as details for inserting a record into the model deployment table.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Lizhi-case-study.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{\n  \"model_hdfs_path\": \"/models/xgboost_recommender.model\",\n  \"preprocess_config_path\": \"/configs/feature_preprocess.json\",\n  \"deployment_table\": \"model_deployments\",\n  \"deployment_record\": {\n    \"model_id\": \"xgboost_recommender_v1\",\n    \"version\": \"1.0\",\n    \"deployment_time\": \"${system.datetime}\",\n    \"status\": \"active\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Feature Issues for JSON Split API Development\nDESCRIPTION: A list of GitHub issues related to the JSON split API implementation in DolphinScheduler. It includes issues for various interfaces including processDefinition, schedule, taskDefinition, and other related modules.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md#2025-04-11_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n[Feature][JsonSplit-api] api module controller design #5498 \n1. [Feature][JsonSplit-api]processDefinition save/update interface  #5499 \n2. [Feature][JsonSplit-api]processDefinition switch interface #5501 \n3. [Feature][JsonSplit-api]processDefinition delete interface #5502 \n4. [Feature][JsonSplit-api]processDefinition copy interface #5503 \n5. [Feature][JsonSplit-api]processDefinition export interface #5504 \n6. [Feature][JsonSplit-api]processDefinition list-paging interface #5505 \n7. [Feature][JsonSplit-api]processDefinition move interface #5506 \n8. [Feature][JsonSplit-api]processDefinition queryProcessDefinitionAllByProjectId interface #5507 \n9. [Feature][JsonSplit-api]processDefinition select-by-id interface #5508 \n10. [Feature][JsonSplit-api]processDefinition view-tree interface #5509 \n11. [Feature][JsonSplit-api]schedule create interface #5510 \n12. [Feature][JsonSplit-api]schedule list-paging interface #5511 \n13. [Feature][JsonSplit-api]schedule update interface #5512 \n14. [Feature][JsonSplit-api]taskDefinition save interface #5513 \n15. [Feature][JsonSplit-api]taskDefinition update interface #5514 \n16. [Feature][JsonSplit-api]taskDefinition switch interface #5515 \n17. [Feature][JsonSplit-api]taskDefinition query interface #5516 \n18. [Feature][JsonSplit-api]taskDefinition delete interface #5517 \n19. [Feature][JsonSplit-api]WorkFlowLineage interface #5518 \n20. [Feature][JsonSplit-api]analysis interface #5519 \n21. [Feature][JsonSplit-api]executors interface #5520 \n22. [Feature][JsonSplit-api]processInstance interface #5521 \n23. [Feature][JsonSplit-api]project interface #5522 \n```\n\n----------------------------------------\n\nTITLE: Clean Solution Using Interface Segregation in Java\nDESCRIPTION: A better solution for testing components with external dependencies by using interfaces and creating test-specific implementations, avoiding code pollution.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_14\n\nLANGUAGE: java\nCODE:\n```\npublic interface Logger {\n    void log(String text);\n}\n\npublic class LoggerImpl implements Logger {\n    @Override\n    public void log(String text) {\n        // log the text\n    }\n}\n\npublic class FakeLoggerImpl implements Logger {\n    @Override\n    public void log(String text) {\n        // do nothing\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DataHub Lineage Backend for Airflow\nDESCRIPTION: Configuration snippet showing DataHub lineage setup parameters for Airflow. This defines the backend as DatahubLineageBackend and specifies connection parameters including the connection ID, cluster environment, and capture settings.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/The_practice_of_Apache_Dolphinscheduler_in_fresh_food_industry.md#2025-04-11_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[lineage]\nbackend = datahub_provider.lineage.datahub.DatahubLineageBackend\ndatahub_kwargs = {\n    \"datahub_conn_id\": \"datahub_rest_default\",\n    \"cluster\": \"prod\",\n    \"capture_ownership_info\": true,\n    \"capture_tags_info\": true,\n\"graceful_exceptions\": true }\n```\n\n----------------------------------------\n\nTITLE: Starting or Stopping DolphinScheduler Services\nDESCRIPTION: Command for starting or stopping specific services in Apache DolphinScheduler after the service splitting refactoring. This command allows administrators to control individual services like master-server, worker-server, api-server, alert-server, etc.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Apache_dolphinScheduler_3.0.0_alpha.md#2025-04-11_snippet_1\n\nLANGUAGE: plain\nCODE:\n```\nbin/dolphinscheduler-daemon.sh <start|stop> <server-name>\n```\n\n----------------------------------------\n\nTITLE: Executing Multiple Python Workflows\nDESCRIPTION: Bash commands to execute multiple workflow files sequentially.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython workflow1.py\npython workflow2.py\npython workflow3.py\n...\npython workflowN.py\n```\n\n----------------------------------------\n\nTITLE: Output-Based Test Style for PriceEngine in Java\nDESCRIPTION: Unit test for the calculateDiscount method using output-based testing style, which verifies the returned result without checking internal state changes.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_4\n\nLANGUAGE: java\nCODE:\n```\npublic class PriceEngineTest {\n    @Test\n    public void calculateDiscount_MinimumDiscount_ReturnMinimumDiscount() {\n        // arrange\n\t\tProduct product1 = new Product(\"Hand wash\");\n        Product product2 = new Product(\"Shampoo\");\n        PriceEngine priceEngine = new PriceEngine();\n        Product[] products = new Product[]{product1, product2};\n        // act\n        double discount = priceEngine.calculateDiscount(products);\n        // assert\n        Assert.assertEquals(0.02, discount);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ingress for DolphinScheduler in Kubernetes\nDESCRIPTION: YAML configuration for setting up an Ingress resource for Apache DolphinScheduler in Kubernetes, providing external access via a domain name with optional TLS support.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ningress:\nenabled: true\nhost: \"ds139.abc.com\"\npath: \"/dolphinscheduler\"\ntls:\n  enabled: false\n  secretName: \"dolphinscheduler-tls\"\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinScheduler Master Server via Daemon Script\nDESCRIPTION: Example of the command used to start the Master Server module through the dolphinscheduler-daemon.sh script. This command initiates the Master component which is responsible for DAG task slicing, task submission monitoring, and health status monitoring.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/The_most_comprehensive_introductory_tutorial_written_in_a_month.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndolphinscheduler-daemon.sh start master-server\n```\n\n----------------------------------------\n\nTITLE: Improved Test Using Interface Implementation in Java\nDESCRIPTION: Unit test using a test-specific implementation of the Logger interface, making the test more maintainable and decoupled from production code implementation details.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_15\n\nLANGUAGE: java\nCODE:\n```\npublic class ControllerTest {\n    @Test\n    public void someMethod_LogText_logNothing {\n        Logger logger = new FakeLoggerImpl();\t// 创建一个专门用于单元测试的 Logger 对象\n        Controller controller = new Controller();\n        \n        controller.someMethod(logger);\n        \n        // assert it won't log nothing\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinScheduler Image in Helm Values\nDESCRIPTION: YAML configuration for specifying the DolphinScheduler Docker image to use in the Helm values file. This allows customizing the image repository, tag and pull policy.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nimage:\n  repository: \"apache/dolphinscheduler\"\n  tag: \"1.3.9\"\n  pullPolicy: \"IfNotPresent\"\n```\n\n----------------------------------------\n\nTITLE: State-Based Test Style for Order Class in Java\nDESCRIPTION: Unit test demonstrating state-based testing by verifying that the internal state of the Order object changes correctly after calling the addProduct method.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_6\n\nLANGUAGE: java\nCODE:\n```\npublic class OrderTest {\n    @Test\n    public void addProduct_AddAProduct() {\n        // arrange\n        Product product = new Product(\"Hand wash\");\n        Order order = new Order();\n        // act\n        order.addProduct(product);\n        // assert\n        Assert.assertEquals(1, order.products.size());\n        Assert.assertEquals(product, order.products.get(0));\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Auto-Increment Value for Process Definition Table in SQL\nDESCRIPTION: SQL command to retrieve the auto-increment value from the process definition table to fix inconsistencies between process definition tables after upgrading.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\n# 查出主键自增值\nselect AUTO_INCREMENT FROM information_schema.TABLES WHERE TABLE_SCHEMA = 'dolphinscheduler' AND TABLE_NAME = 't_ds_process_definition' limit 1\n# 将上面sql的执行结果填写到下方参数处执行\nalter table dolphinscheduler_bak1.t_ds_process_definition_log auto_increment = {max_id};\n```\n\n----------------------------------------\n\nTITLE: Configuring External Database and S3 Storage for DolphinScheduler\nDESCRIPTION: YAML configuration for setting up DolphinScheduler with an external PostgreSQL database and AWS S3 for resource file storage.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\npostgresql:\n   enabled: false\nmysql:\n   enabled: false\nexternalDatabase:\n   type: \"postgresql\"\n   host: \"dolphinscheduler.cluster-xxxxx.us-east-1.rds.amazonaws.com\"\n   port: \"5432\"\n   username: \"postgres\"\n   password: \"xxxxxxxx\"\n   database: \"dolphinscheduler\"\n   params: \"characterEncoding=utf8\"\n  ## Use S3 to store resource files\nconf:\n   common:\n     resource.storage.type: S3\n     resource.aws.access.key.id: xxxxxxx\n     resource.aws.secret.access.key: xxxxxxxxx\n     resource.aws.region: us-east-1\n     resource.aws.s3.bucket.name: dolphinscheduler-resourse\n     resource.aws.s3.endpoint: https://S3.us-east-1.amazonaws.com\n```\n\n----------------------------------------\n\nTITLE: Calculator Unit Test Following 3A Principle in Java\nDESCRIPTION: A unit test for the Calculator class that follows the Arrange-Act-Assert pattern, demonstrating how to structure unit tests according to best practices.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_1\n\nLANGUAGE: java\nCODE:\n```\npublic class CalculatorTest {\n    @Test\n    public void sum() {\n        // Arrange\n        long a = 1L, b = 2L;\n        Calculator calculator = new Calculator();\n        // Act\n        long actual = calculator.sum(a, b);\n        // Assert\n        long expected = 3L;\n        assertEquals(expected, actual);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Simple Calculator Implementation in Java\nDESCRIPTION: A basic Calculator class with a sum method that adds two long integers and returns the result.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class Calculator {\n    public long sum(long a, long b) {\n        return a + b;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring pre-commit hooks for Python API linting\nDESCRIPTION: This YAML configuration sets up pre-commit hooks for the DolphinScheduler Python API. It includes tools for sorting imports, formatting code, removing unused imports, and detecting code style issues.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndefault_stages: [commit, push]\ndefault_language_version:\n  # force all python hooks to run python3\n  python: python3\nrepos:\n  # Python API Hooks\n  - repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n      - id: isort\n        name: isort (python)\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n  - repo: https://github.com/pycqa/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\n        additional_dependencies: [\n          'flake8-docstrings>=1.6',\n          'flake8-black>=0.2',\n        ]\n        # pre-commit run in the root, so we have to point out the full path of configuration\n        args: [\n          --config,\n          .flake8\n        ]\n  - repo: https://github.com/pycqa/autoflake\n    rev: v1.4\n    hooks:\n      - id: autoflake\n        args: [\n          --remove-all-unused-imports,\n          --ignore-init-module-imports,\n          --in-place\n        ]\n```\n\n----------------------------------------\n\nTITLE: Displaying taskRelationJson Format in Markdown\nDESCRIPTION: A JSON example showing the format of taskRelationJson parameter used in process definition APIs. The structure defines task relationships with pre and post task codes, versions, and condition parameters.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md#2025-04-11_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[{\"name\":\"\",\"pre_task_code\":0,\"pre_task_version\":0,\"post_task_code\":123456789,\"post_task_version\":1,\"condition_type\":0,\"condition_params\":{}},{\"name\":\"\",\"pre_task_code\":123456789,\"pre_task_version\":1,\"post_task_code\":123451234,\"post_task_version\":1,\"condition_type\":0,\"condition_params\":{}}]\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting LDAP Email Attribute Configuration in Java\nDESCRIPTION: Java code modification to identify the correct email attribute name in LDAP integration by commenting out the setReturningAttributes line to retrieve all attributes from LDAP.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nctx = new InitialLdapContext(searchEnv, null);\nSearchControls sc = new SearchControls();\n// sc.setReturningAttributes(new String[]{ldapEmailAttribute});\nsc.setSearchScope(SearchControls.SUBTREE_SCOPE);\nEqualsFilter filter = new EqualsFilter(ldapUserIdentifyingAttribute, userId);\nNamingEnumeration<SearchResult> results = ctx.search(ldapBaseDn, filter.toString(), sc);\nif (results.hasMore()) {\n    // get the users DN (distinguishedName) from the result\n    SearchResult result = results.next();\n    NamingEnumeration<? extends Attribute> attrs = result.getAttributes().getAll();\n    while (attrs.hasMore()) {\n        // Open another connection to the LDAP server with the found DN and the password\n        searchEnv.put(Context.SECURITY_PRINCIPAL, result.getNameInNamespace());\n        searchEnv.put(Context.SECURITY_CREDENTIALS, userPwd);\n        try {\n            new InitialDirContext(searchEnv);\n        } catch (Exception e) {\n            logger.warn(\"invalid ldap credentials or ldap search error\", e);\n            return null;\n        }\n        Attribute attr = attrs.next();\n        if (attr.getID().equals(ldapEmailAttribute)) {\n            return (String) attr.get();\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Apache DolphinScheduler Contributors in Markdown\nDESCRIPTION: A markdown code block containing an alphabetically ordered list of all contributors to the Apache DolphinScheduler project. The list includes both English and Chinese names separated by commas.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache_dolphinScheduler_3.0.0.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nAaron Lin、Amy0104、Assert、BaoLiang、Benedict Jin、BenjaminWenqiYu、Brennan Fox、Dannila、Desperado2、Devosend、DingPengfei、DuChaoJiaYou、EdwardYang、Eric Gao、Frank Chen、GaoTianDuo、HanayoZz、HeChuan、HomminLee、Hua Jiang、Hwting、Ivan0626、Jeff Zhan、Jiajie Zhong、JieguangZhou、Jiezhi.G、JinYong Li、J·Y、Kerwin、Kevin.Shin、KingsleyY、Kirs、KyoYang、LinKai、LiuBodong、LongJGun、Luke Yan、Lyle Shaw、Manhua、Martin Huang、Maxwell、Molin Wang、Mr.An、OS、PJ Fanning、Paul Zhang、QuakeWang、ReonYu、SbloodyS、Sheldon、Shiwen Cheng、ShuiMuNianHuaLP、ShuoTiann、SongTao Zhuang、Stalary、Sunny Lei、Tom、Town、Tq、WangJPLeo、Wenjun Ruan、X&Z、XiaochenNan、Yanbin Lin、Yao WANG、Yiming Guo、Zonglei Dong、aCodingAddict、aaronlinv、aiwenmo、caishunfeng、calvin、calvinit、cheney、chouc、chuxing、czeming、devosend、exmy、gaojun2048、guodong、guoshupei、hjli、hstdream、huangxiaohai、janeHe13、jegger、jiachuan.zhu、jon-qj、juzimao、kezhenxu94、labbomb、leiwingqueen、lgcareer、lhjzmn、lidongdai、lifeng、lilyzhou、litiliu、liubo1990、liudi1184、longtb、lvshaokang、lyq、mans2singh、mask、mazhong、mgduoduo、myangle1120、naziD、nobolity、ououtt、ouyangyewei、pinkhello、qianli2022、qinchaofeng、rickchengx、rockfang、ronyang1985、seagle、shuai hou、simsicon、sneh-wha、songjianet、sparklezzz、springmonster、sq-q、syyangs799、uh001、wangbowen、wangqiang、wangxj3、wangyang、wangyizhi、wind、worry、wqxs、xiangzihao、xiaodi wang、xiaoguaiguai、xuhhui、yangyunxi、yc322、yihong、yimaixinchen、youzipi、zchong、zekai-li、zhang、zhangxinruu、zhanqian、zhuxt2015、zixi0825、zwZjut、天仇、小张、弘树丶、张俊杰、旭旭同學、时光、旺阳、王强、百岁、秋天、罗铭涛、阿福Chris、陈家名、陈爽、飞侠美如画\n```\n\n----------------------------------------\n\nTITLE: Modifying Task Instance List Query in SQL\nDESCRIPTION: SQL modification to fix the empty task instance list issue after upgrading by changing the join condition to use process_definition table instead of task_definition_log.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\n    \tselect\n        <include refid=\"baseSqlV2\">\n            <property name=\"alias\" value=\"instance\"/>\n        </include>\n        ,\n        process.name as process_instance_name\n        from t_ds_task_instance instance\n--         left join t_ds_task_definition_log define \n--\t\t\t\ton define.code=instance.task_code and \n--\t\t\t\t\tdefine.version=instance.task_definition_version\n        join t_ds_process_instance process\n        \ton process.id=instance.process_instance_id\n        join t_ds_process_definition define\n        \ton define.code=process.process_definition_code\n        where define.project_code = #{projectCode}\n        <if test=\"startTime != null\">\n            and instance.start_time <![CDATA[ >=]]> #{startTime}\n        </if>\n\t\t......省略多余部分\n```\n\n----------------------------------------\n\nTITLE: Order Class with Complex Private Method in Java\nDESCRIPTION: An Order class with a private pricing method that contains complex business logic, representing an anti-pattern where important logic is hidden in private methods.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npublic class Order {\n    private Customer customer;\n    private List<Product> products;\n    \n    public String generateDescription() {\n\t\treturn \"Customer name: \" + customer.getName() + \", total price: \" + getPrice();\n    }\n    \n    private double getPrice() {\n        double basePrice;\t// 基于 products 计算\n        double discounts;\t// 基于 customer 计算\n        double taxes;\t// 基于 products 计算\n        // do some calculation\n        return basePrice - discounts + taxes;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Network Load Balancer for DolphinScheduler External Access\nDESCRIPTION: Kubernetes YAML manifest to create an AWS Network Load Balancer for accessing DolphinScheduler externally, with annotations for load balancer configuration.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Service\nmetadata:\n   namespace: dolphinscheduler\n   name: service-dolphinscheduler\n   annotations:\n     service.beta.kubernetes.io/aws-load-balancer-type: external\n     service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\n     service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n     service.beta.kubernetes.io/subnets: subnet-xxx, subnet-xxx\nspec:\n   ports:\n     - port: 12345\n       targetPort: 12345\n       protocol: TCP\n   type: LoadBalancer\n   selector:\n     app.kubernetes.io/name: dolphinscheduler-api\n```\n\n----------------------------------------\n\nTITLE: Basic Calculator Class Implementation in Java\nDESCRIPTION: A simple Calculator class with a sum method that adds two long values and returns the result. This serves as an example target class for unit testing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\npublic class Calculator {\n    public long sum(long a, long b) {\n        return a + b;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Order Class for State-Based Testing in Java\nDESCRIPTION: An Order class with a method that modifies internal state by adding products to a list, demonstrating state-changing behavior.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_5\n\nLANGUAGE: java\nCODE:\n```\npublic class Order {\n    public List<Product> products = new ArrayList<>();\n\n    public void addProduct(Product product) {\n        products.add(product);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Null Task Dependencies\nDESCRIPTION: Java code fix for handling null task dependencies during the upgrade process.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nlong taskCode =0;\nif (processCodeTaskNameCodeEntry.getValue() != null\n        &&processCodeTaskNameCodeEntry.getValue().get(depTasks)!=null){\ntaskCode =processCodeTaskNameCodeEntry.getValue().get(depTasks);\n}else{\n        logger.error(\"******************** depTasks:\"+depTasks);\n        logger.error(\"******************** taskCode not in \"+JSONUtils.toJsonString(processCodeTaskNameCodeEntry));\n}\ndependItem.put(\"depTaskCode\", taskCode);\n```\n\n----------------------------------------\n\nTITLE: Anti-Pattern: Test Logic Mirroring Implementation in Java\nDESCRIPTION: Example showing an anti-pattern where test code duplicates the implementation logic, making the test ineffective at catching bugs.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_11\n\nLANGUAGE: java\nCODE:\n```\npublic class CalculatorTest {\n    @Test\n    public void sum() {\n        // arrange\n        long a = 1L, b = 2L;\n        Calculator calculator = new Calculator();\n        // act\n        long actual = calculator.sum(a, b);\n        // assert\n        long expected = a + b;\t// 反例\n        long expected = 3L;\t// 正例\n        assertEquals(expected, actual);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Anti-Pattern: Code Pollution with Test-Only Logic in Java\nDESCRIPTION: Example of code pollution where production code contains logic only used for testing, creating maintenance issues and coupling between test and production code.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_12\n\nLANGUAGE: java\nCODE:\n```\npublic class Logger {\n    private boolean isTestEnvironment;\n    \n    public Logger(boolean isTestEnvironment) {\n        this.isTestEnvironment = isTestEnvironment;\n    }\n    \n    public void log(String text) {\n        if (!isTestEnvironment) {\n            // log the text\n        }\n    }\n}\n\npublic class Controller {\n    public void someMethod(Logger logger) {\n        logger.log(\"someMethod is called\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for DolphinScheduler\nDESCRIPTION: YAML configuration examples showing how to enable shared persistent storage for DolphinScheduler and create a PersistentVolumeClaim linked to an EFS volume.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nsharedStoragePersistence: enabled: true mountPath: \"/opt/soft\" accessModes: - \"ReadWriteMany\" ## storageClassName must support the access mode: ReadWriteMany storageClassName: \"efs-sc\" storage: \"20Gi\"\n\n{{- if .Values.common.sharedStoragePersistence.enabled }}\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n   name: {{ include \"dolphinscheduler.fullname\" . }}-shared\n   labels:\n     app.kubernetes.io/name: {{ include \"dolphinscheduler.fullname\" . }}-shared\n     {{- include \"dolphinscheduler.common.labels\" . | nindent 4 }}\n   annotations:\n     \"helm.sh/resource-policy\": keep\nspec:\n   accessModes:\n   {{- range.Values.common.sharedStoragePersistence.accessModes }}\n     - {{ . | quote }}\n   {{- end }}\n   storageClassName: {{ .Values.common.sharedStoragePersistence.storageClassName | quote }}\n   volumeName: dolphin-efs-pv\n   resources:\n     requests:\n       storage: {{ .Values.common.sharedStoragePersistence.storage | quote }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Submitting Combined Training and Deployment Workflow\nDESCRIPTION: Command to submit a YAML-defined integrated workflow that combines training and deployment steps using PyDolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npydolphinscheduler yaml -f pyds/train_and_deploy.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing and Using Air2phin Migration Tool for Python\nDESCRIPTION: Command examples for installing the Air2phin package via pip and using it to migrate Airflow DAGs to DolphinScheduler Python SDK. Includes multiprocessing option for better performance when converting multiple DAGs.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install package\npython -m pip install --upgrade air2phin\n\n# Migrate airflow's dags\nair2phin migrate -i ~/airflow/dags\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions workflow for linting and executing Python scripts\nDESCRIPTION: This GitHub Actions workflow configuration sets up two jobs: one for linting Python code using pre-commit hooks, and another for executing Python scripts. The execution job only runs on pushes to the main branch.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nname: Execute Workflows\non:\n  push:\n    branches:\n      - main\n  pull_request:\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pre-commit\n      - name: lint\n        run: |\n          pre-commit install\n\t\t  pre-commit run --all-files\n  execute:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push'\n    needs: lint\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: <YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_PORT: <PORT-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_AUTH_TOKEN: ${{ secrets.YOUR-SECRET-NAME }} \n        run: |\n          for file in $(find . -name \"*.py\"); do\n            python \"$file\"\n          done\n```\n\n----------------------------------------\n\nTITLE: Configuring SSO Authentication with Casdoor in Apache DolphinScheduler\nDESCRIPTION: YAML configuration example showing how to integrate Apache DolphinScheduler with Casdoor for Single Sign-On (SSO) authentication. This configuration should be added to the application.yaml file in the dolphinscheduler-api/src/main/resources directory.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Apache_DolphinScheduler_releases_version_3.2.0.md#2025-04-11_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsecurity:\n  authentication:\n    # Authentication types (supported types: PASSWORD,LDAP,CASDOOR_SSO)\n    type: CASDOOR_SSO\ncasdoor:\n  # Your Casdoor server url\n  endpoint:\n  client-id:\n  client-secret:\n  # The certificate may be multi-line, you can use `|-` for ease\n  certificate: \n  # Your organization name added in Casdoor\n  organization-name:\n  # Your application name added in Casdoor\n  application-name:\n  # Doplhinscheduler login url\n  redirect-url: http://localhost:5173/login \n```\n\n----------------------------------------\n\nTITLE: Installing PyDolphinScheduler via pip\nDESCRIPTION: Command to install the Python client library for Apache DolphinScheduler version 3.1.0 using pip package manager.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip install apache-dolphinscheduler==3.1.0\n```\n\n----------------------------------------\n\nTITLE: Starting/Stopping DolphinScheduler Server Components in Bash\nDESCRIPTION: Command for starting or stopping DolphinScheduler server components from the command line. This command takes two parameters: the action (start or stop) and the name of the server to operate on.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache_dolphinScheduler_3.0.0.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbin/dolphinscheduler-daemon.sh <start|stop> <server-name>\n```\n\n----------------------------------------\n\nTITLE: Submitting Model Training Workflow via PyDolphinScheduler\nDESCRIPTION: Command to submit a YAML-defined workflow for model training using PyDolphinScheduler's YAML interface.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\npydolphinscheduler yaml -f pyds/train_model.yaml\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Handling Different Airflow Versions\nDESCRIPTION: YAML configuration example showing how Air2phin handles different Airflow versions by supporting multiple source module paths in the migration rules.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmigration:\n  module:\n    - action: replace\n      src:\n        - airflow.operators.bash.BashOperator\n        - airflow.operators.bash_operator.BashOperator\n      dest: pydolphinscheduler.tasks.shell.Shell\n  parameter:\n    - action: replace\n      src: task_id\n      dest: name\n    - action: replace\n      src: bash_command\n      dest: command\n```\n\n----------------------------------------\n\nTITLE: Defining HTTP Alert Constants in Java\nDESCRIPTION: This snippet defines constants for HTTP alert parameters in the HttpAlertConstants class. It includes definitions for URL, header parameters, and other HTTP-related constants used in the alert plugin.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npackage org.apache.dolphinscheduler.plugin.alert.http;\npublic final class HttpAlertConstants {\n    public static final String URL = \"$t('url')\";\n\n    public static final String NAME_URL = \"url\";\n\n    public static final String HEADER_PARAMS = \"$t('headerParams')\";\n\n    public static final String NAME_HEADER_PARAMS = \"headerParams\";\n\n...........................Omitting redundant code \n\n    private HttpAlertConstants() {\n        throw new UnsupportedOperationException(\"This is a utility class and cannot be instantiated\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: User Controller Implementation in Java\nDESCRIPTION: A REST controller that handles user login requests, demonstrating dependency injection through constructor and HTTP request handling. This serves as an example for controller testing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_2\n\nLANGUAGE: java\nCODE:\n```\n@RestController\npublic class UserController extends BaseController {\n\n    private final UserService service;\n\n    public UserController(UserService service) {\n        this.service = service;\n    }\n\n    @RequestMapping(\"/login\")\n    @ResponseStatus(HttpStatus.OK)\n    public Result login(@RequestParam(value = \"userName\", required = false) String userName, @RequestParam(value = \"userPassword\", required = false) String userPassword) {\n\n        Map<String, Object> result = service.login(userName, userPassword);\n        return returnDataList(result);\n    }\n\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Converted Workflows to DolphinScheduler\nDESCRIPTION: Commands for installing Apache DolphinScheduler Python SDK and submitting the converted workflow definitions to DolphinScheduler server.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Big_Data_Scheduling_Best_Practices_Migrating_from_Airflow_to_Apache_DolphinScheduler.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install apache-dolphinscheduler according to apache DolphinScheduler server you use, ref: https://dolphinscheduler.apache.org/python/main/#version\npython -m pip install apache-dolphinscheduler\n# Submit your dolphinscheduler python sdk definition\npython ~/airflow/dags/tutorial.py\n```\n\n----------------------------------------\n\nTITLE: Checking Scaled DolphinScheduler Pods in Kubernetes\nDESCRIPTION: Bash command output showing the increased number of DolphinScheduler pods after scaling up component replicas, with all pods running successfully in the 'ds139' namespace.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME                  READY STATUS RESTARTS AGE\ndolphinscheduler-alert-96c74dc84-72cc9 1/1  Running 0    43m\ndolphinscheduler-alert-96c74dc84-j6zdh 1/1  Running 0    2m27s\ndolphinscheduler-alert-96c74dc84-rn9wb 1/1  Running 0    2m27s\ndolphinscheduler-api-78db664b7b-6j8rj 1/1  Running 0    2m27s\ndolphinscheduler-api-78db664b7b-bsdgv 1/1  Running 0    2m27s\ndolphinscheduler-api-78db664b7b-gsltq 1/1  Running 0    43m\ndolphinscheduler-master-0       1/1  Running 0    43m\ndolphinscheduler-master-1       1/1  Running 0    43m\ndolphinscheduler-master-2       1/1  Running 0    43m\ndolphinscheduler-master-3       1/1  Running 0    2m27s\ndolphinscheduler-master-4       1/1  Running 0    2m27s\ndolphinscheduler-worker-0       1/1  Running 0    43m\ndolphinscheduler-worker-1       1/1  Running 0    43m\ndolphinscheduler-worker-2       1/1  Running 0    43m\ndolphinscheduler-worker-3       1/1  Running 0    2m27s\ndolphinscheduler-worker-4       1/1  Running 0    2m27s\n```\n\n----------------------------------------\n\nTITLE: Capturing Yarn ID from Logs using YARNID Tags\nDESCRIPTION: A pattern used to extract Yarn job IDs from logs by enclosing them in <YARNID> tags. This approach allows for automated Yarn log retrieval based on the extracted ID.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Application_transformation_based_on_DolphinScheduler_in_financial_technology_data_center.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n<YARNID>1234567890<YARNID>\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper data directories in zoo.cfg\nDESCRIPTION: Configuration settings for the Zookeeper data and log directories in the zoo.cfg file. These properties define where Zookeeper will store its data and transaction logs.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndataDir=I:\\\\setup\\\\apache-zookeeper-3.6.3-bin\\\\data\ndataLogDir=I:\\\\setup\\\\apache-zookeeper-3.6.3-bin\\\\log\n```\n\n----------------------------------------\n\nTITLE: H2 Database Dependency for Testing in Maven XML\nDESCRIPTION: Maven dependency configuration for H2 in-memory database for unit testing. This is used for database testing without requiring a real database environment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>com.h2database</groupId>\n    <artifactId>h2</artifactId>\n    <scope>test</scope>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment for Windows Users\nDESCRIPTION: Warning note about Windows-specific issues when building the website locally, explaining common errors and workarounds for Windows users including administrator privileges and WSL.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\nNote: if you clone the code in Windows, not Mac or Linux. Please read the details below.\nIf you execute the commands like the two steps above, you will get the exception \"UnhandledPromiseRejectionWarning: Error: EPERM: operation not permitted, symlink '2.0.3' -> 'latest'\".\nIf you get the exception \"Can't resolve 'antd' in xxx\",you can run `yarn add antd` and `yarn install`.\nBecause the two steps run command `./scripts/prepare_docs.sh` should Linux environment,so if you are a windwos system you can use WSL do it.\nWhen meeting this problem. You can run two steps in the cmd.exe as an ADMINISTRATOR MEMBER.\n```\n```\n\n----------------------------------------\n\nTITLE: Test with Code Pollution in Java\nDESCRIPTION: Unit test demonstrating how code pollution creates test-specific logic in production code that can lead to maintenance issues when production code changes.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-guideline.md#2025-04-11_snippet_13\n\nLANGUAGE: java\nCODE:\n```\npublic class ControllerTest {\n    @Test\n    public void someMethod_LogText_logNothing {\n        Logger logger = new Logger(true);\t// 设置为 true，表明当前处于测试环境\n        Controller controller = new Controller();\n        \n        controller.someMethod(logger);\n        \n        // assert it won't log nothing\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: H2 In-Memory Database Configuration in YAML\nDESCRIPTION: Spring Boot application configuration for H2 in-memory database initialization. This setup enables automatic schema creation and data loading for testing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ut-template.md#2025-04-11_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nspring:\n  database:\n   \tdriver-class-name: org.h2.Driver\n\turl: jdbc:h2:mem:test\t# test for database name\n    initialization-mode: always\t# always: initialize on each startup\n    schema: classpath:sql/schema.sql\t# SQL file path for schema initialization\n    data: classpath:sql/data.sql\t# SQL file path for data initialization\n\n# Print SQL debug logs\nlogging:\n  level:\n    org.apache.dolphinscheduler.dao.mapper: debug\n```\n\n----------------------------------------\n\nTITLE: Executing cluster.sh Script for DolphinScheduler Cluster Setup in Bash\nDESCRIPTION: This command executes the cluster.sh script to start the DolphinScheduler cluster setup process on AWS EC2 instances. The script combines the EC2 instances into a DolphinScheduler cluster, with the execution time dependent on network speed.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_launches_on_the_AWS_AMI_Application_Marketplace.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./cluster.sh start\n```\n\n----------------------------------------\n\nTITLE: Creating Process Task Relation Database Tables in MySQL\nDESCRIPTION: MySQL schema definitions for process_task_relation and process_task_relation_log tables that store workflow dependencies. These tables track relationships between tasks including pre-task and post-task connections.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/dolphinscheduler_json.md#2025-04-11_snippet_1\n\nLANGUAGE: mysql\nCODE:\n```\nCREATE TABLE `t_ds_process_task_relation` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `name` varchar(200) DEFAULT NULL COMMENT 'relation name',\n  `process_definition_version` int(11) DEFAULT NULL COMMENT 'process version',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `process_definition_code` bigint(20) NOT NULL COMMENT 'process code',\n  `pre_task_code` bigint(20) NOT NULL COMMENT 'pre task code',\n  `pre_task_version` int(11) NOT NULL COMMENT 'pre task version',\n  `post_task_code` bigint(20) NOT NULL COMMENT 'post task code',\n  `post_task_version` int(11) NOT NULL COMMENT 'post task version',\n  `condition_type` tinyint(2) DEFAULT NULL COMMENT 'condition type : 0 none, 1 judge 2 delay',\n  `condition_params` text COMMENT 'condition params(json)',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\nCREATE TABLE `t_ds_process_task_relation_log` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'self-increasing id',\n  `name` varchar(200) DEFAULT NULL COMMENT 'relation name',\n  `process_definition_version` int(11) DEFAULT NULL COMMENT 'process version',\n  `project_code` bigint(20) NOT NULL COMMENT 'project code',\n  `process_definition_code` bigint(20) NOT NULL COMMENT 'process code',\n  `pre_task_code` bigint(20) NOT NULL COMMENT 'pre task code',\n  `pre_task_version` int(11) NOT NULL COMMENT 'pre task version',\n  `post_task_code` bigint(20) NOT NULL COMMENT 'post task code',\n  `post_task_version` int(11) NOT NULL COMMENT 'post task version',\n  `condition_type` tinyint(2) DEFAULT NULL COMMENT 'condition type : 0 none, 1 judge 2 delay',\n  `condition_params` text COMMENT 'condition params(json)',\n  `operator` int(11) DEFAULT NULL COMMENT 'operator user id',\n  `operate_time` datetime DEFAULT NULL COMMENT 'operate time',\n  `create_time` datetime NOT NULL COMMENT 'create time',\n  `update_time` datetime DEFAULT NULL COMMENT 'update time',\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n```\n\n----------------------------------------\n\nTITLE: Configuring Resource Storage Type for AWS S3 Integration\nDESCRIPTION: Configuration setting for specifying the resource storage type in Apache DolphinScheduler. This parameter allows users to configure the system to use Amazon S3 as the resource center storage option alongside local and HDFS storage.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Apache_dolphinScheduler_3.0.0_alpha.md#2025-04-11_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nresource.storage.type\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinScheduler Helm Values\nDESCRIPTION: YAML configuration to modify the image registry address in values.yaml to point to AWS ECR repository.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nimage:\n   registry: \"xxxxxx.dkr.ecr.us-east-1.amazonaws.com\" -- ECR mirror address\n   tag: \"3.1.2\"\n```\n\n----------------------------------------\n\nTITLE: SQL Query for Null Field Check\nDESCRIPTION: SQL query template used to count the number of rows where a specified field is null. This is used as part of the data quality rule definition.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/ipalfish_tech_platform.md#2025-04-11_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\ncount(case when ${field} is null then 1 else null end)\n```\n\n----------------------------------------\n\nTITLE: Submitting Model Deployment Workflow via PyDolphinScheduler\nDESCRIPTION: Command to submit a YAML-defined workflow for model deployment using PyDolphinScheduler's YAML interface.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npydolphinscheduler yaml -f pyds/deploy.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Linkerd Injection in DolphinScheduler Helm\nDESCRIPTION: Configuration snippet showing how to enable Linkerd sidecar injection for DolphinScheduler components through Helm values annotations. This enables service mesh capabilities for master, worker, API, and alert components.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nannotations: #{}\\n   linkerd.io/inject: enabled\n```\n\n----------------------------------------\n\nTITLE: Starting MLflow Tracking Server with Docker\nDESCRIPTION: Docker command to start the MLflow Tracking Server on port 5000. This creates a container named 'mlflow' using the jalonzjg/mlflow:latest image in detached mode.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker run — name mlflow -p 5000:5000 -d jalonzjg/mlflow:latest\n```\n\n----------------------------------------\n\nTITLE: Fixing SubProcess Parameter Upgrade\nDESCRIPTION: Java code modification to handle null process definition references during upgrade script execution.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nif (jsonNodeDefinitionId != null) {\n    if (processDefinitionMap.get(jsonNodeDefinitionId.asInt()) != null) {\n    param.put(\"processDefinitionCode\",processDefinitionMap.get(jsonNodeDefinitionId.asInt()).getCode());\n    param.remove(\"processDefinitionId\");\n} else {\n        logger.error(\"*******************error\");\n    logger.error(\"*******************param:\" + param);\n    logger.error(\"*******************jsonNodeDefinitionId:\" + jsonNodeDefinitionId);\n}\n        }\n```\n\n----------------------------------------\n\nTITLE: Configuring Conda and Python Environment for DolphinScheduler\nDESCRIPTION: This snippet shows how to copy configuration files and update environment variables for Conda and Python in DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncp common.properties apache-dolphinscheduler-3.1.0-bin/standalone-server/conf\necho \"export PATH=$(which conda)/bin:\\$PATH\" >> apache-dolphinscheduler-3.1.0-bin/bin/env/dolphinscheduler_env.sh\necho \"export PYTHON_HOME=$(dirname $(which conda))/python\" >> apache-dolphinscheduler-3.1.0-bin/bin/env/dolphinscheduler_env.sh\n```\n\n----------------------------------------\n\nTITLE: Implementing HttpAlertChannelFactory in Java\nDESCRIPTION: This code implements the AlertChannelFactory interface for HTTP alerts. It defines the plugin name, parameters, and creates an instance of the HttpAlertChannel. The params method sets up input parameters for the HTTP alert configuration.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_to_use_Apache_DolphinScheduler_for_targeted_alarm_plugin_development_for_enterprise_applications.md#2025-04-11_snippet_1\n\nLANGUAGE: Java\nCODE:\n```\n@AutoService(AlertChannelFactory.class)\npublic final class HttpAlertChannelFactory implements AlertChannelFactory {\n    @Override\n    public String name() {\n        return \"Http\";\n    }\n    @Override\n    public List<PluginParams> params() {\n        InputParam url = InputParam.newBuilder(HttpAlertConstants.NAME_URL, HttpAlertConstants.URL)\n                                   .setPlaceholder(\"input request URL\")\n                                   .addValidate(Validate.newBuilder()\n                                                        .setRequired(true)\n                                                        .build())\n                                   .build();\n        InputParam headerParams = InputParam.newBuilder(HttpAlertConstants.NAME_HEADER_PARAMS, HttpAlertConstants.HEADER_PARAMS)\n                                            .setPlaceholder(\"input request headers as JSON format \")\n                                            .addValidate(Validate.newBuilder()\n                                                                 .setRequired(true)\n                                                                 .build())\n                                            .build();\n        InputParam bodyParams = InputParam.newBuilder(HttpAlertConstants.NAME_BODY_PARAMS, HttpAlertConstants.BODY_PARAMS)\n                                          .setPlaceholder(\"input request body as JSON format \")\n                                          .addValidate(Validate.newBuilder()\n                                                               .setRequired(false)\n                                                               .build())\n                                          .build();\n...........................Omitting redundant code \n        return Arrays.asList(url, requestType, headerParams, bodyParams, contentField);\n    }\n    @Override\n    public AlertChannel create() {\n        return new HttpAlertChannel();\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Download Data Workflow via PyDolphinScheduler\nDESCRIPTION: Command to submit a YAML-defined workflow for downloading test data using PyDolphinScheduler's YAML interface.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npydolphinscheduler yaml -f pyds/download_data.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing DVC Repository Workflow via PyDolphinScheduler\nDESCRIPTION: Command to submit a YAML-defined workflow for initializing a DVC data versioning repository using PyDolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\npydolphinscheduler yaml -f pyds/init_dvc_repo.yaml\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinScheduler Cluster Setup Scripts from GitHub\nDESCRIPTION: Commands to download the cluster.sh and cluster_env.sh scripts needed for setting up a DolphinScheduler cluster on AWS EC2 instances. These scripts handle the configuration and deployment of the cluster components.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_launches_on_the_AWS_AMI_Application_Marketplace.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/WhaleOps/pa\ncker_tmpl/main/aws/ami/dolphinscheduler/bin/cluster.sh\nwget https://raw.githubusercontent.com/WhaleOps/packer_tmpl/main/aws/ami/dolphinscheduler/bin/cluster_env.sh\n```\n\n----------------------------------------\n\nTITLE: Submitting Data Preparation Workflow via PyDolphinScheduler\nDESCRIPTION: Command to submit a YAML-defined workflow for data preprocessing using PyDolphinScheduler's YAML interface.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\npydolphinscheduler yaml -f pyds/prepare_data.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Java 8 on Ubuntu\nDESCRIPTION: These commands update the package list, install OpenJDK 8, and verify the Java version.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install openjdk-8-jdk\njava -version\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for DolphinScheduler\nDESCRIPTION: Bash command to create a dedicated Kubernetes namespace for DolphinScheduler deployment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl create namespace dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Starting Apache DolphinScheduler\nDESCRIPTION: These commands navigate to the DolphinScheduler directory and start the standalone server.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd apache-dolphinscheduler-3.1.0-bin\nbash bin/dolphinscheduler-daemon.sh start standalone-server\n```\n\n----------------------------------------\n\nTITLE: Configuring Java Environment Variables\nDESCRIPTION: This snippet shows how to set JAVA_HOME and update PATH in the user's shell configuration file.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Confirm that your jdk is as below and configure the environment variables\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\nexport PATH=$PATH:$JAVA_HOME/bin\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinScheduler Cluster IP Addresses\nDESCRIPTION: Configuration example for the cluster_env.sh script that defines the IP addresses of the EC2 instances used in the DolphinScheduler cluster. This specifies which components run on which instances.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_launches_on_the_AWS_AMI_Application_Marketplace.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ips=\"192.168.1.1,192.168.1.2,192.168.1.3,192.168.1.4,192.168.1.5,192.168.1.6,192.168.1.7,192.168.1.8\"\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow and DVC Python Packages\nDESCRIPTION: This command installs specific versions of MLflow and DVC using pip in the Conda environment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow==1.30.0 dvc\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Apache DolphinScheduler 3.1.0\nDESCRIPTION: These commands download the DolphinScheduler 3.1.0 binary, extract it, and clean up the downloaded archive.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd first-example/install_dolphinscheduler\nwget https://dlcdn.apache.org/dolphinscheduler/3.1.0/apache-dolphinscheduler-3.1.0-bin.tar.gz\ntar -zxvf apache-dolphinscheduler-3.1.0-bin.tar.gz\nrm apache-dolphinscheduler-3.1.0-bin.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Configuring Version Documentation Mapping in Shell\nDESCRIPTION: Example of adding a new version-to-branch mapping in the documentation configuration file for version 10.0.0.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/HOW_PREPARE_WORK.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndeclare -A DEV_RELEASE_DOCS_VERSIONS=(\n  ...\n  [\"10.0.0\"]=\"10.0.0-release\"\n  ...\n)\n```\n\n----------------------------------------\n\nTITLE: Viewing DolphinScheduler Server Logs with tail Command\nDESCRIPTION: Command to monitor the logs of the DolphinScheduler standalone server in real-time using the tail command with the follow option.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ntail -500f standalone-server/logs/dolphinscheduler-standalone.log\n```\n\n----------------------------------------\n\nTITLE: Adding Console Output to Logback Configuration\nDESCRIPTION: XML configuration to add console output in logback-worker.xml, logback-master.xml, and logback-api.xml files.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DS_run_in_windows.md#2025-04-11_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<root level=\"INFO\">\n    <appender-ref ref=\"STDOUT\"/>  <!-- 添加控制台输出 -->\n    <appender-ref ref=\"APILOGFILE\"/>\n    <appender-ref ref=\"SKYWALKING-LOG\"/>\n</root>\n```\n\n----------------------------------------\n\nTITLE: DolphinScheduler Java Gateway Configuration\nDESCRIPTION: Environment variable configuration for DolphinScheduler Java Gateway connection.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport PYDS_JAVA_GATEWAY_ADDRESS=\"<YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER>\"\nexport PYDS_JAVA_GATEWAY_PORT=\"<PORT-RUN-DOLPHINSCHEDULER-API-SERVER>\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ZooKeeper Properties in Windows\nDESCRIPTION: Configuration settings for ZooKeeper data and log directories in the zoo.cfg file.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DS_run_in_windows.md#2025-04-11_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ndataDir=D:\\\\code\\\\apache-zookeeper-3.6.3-bin\\\\data\ndataLogDir=D:\\\\code\\\\apache-zookeeper-3.6.3-bin\\\\log\n```\n\n----------------------------------------\n\nTITLE: Fixing Process Definition Auto-increment\nDESCRIPTION: SQL queries to align primary key auto-increment values between process definition tables.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect AUTO_INCREMENT FROM information_schema.TABLES WHERE TABLE_SCHEMA = 'dolphinscheduler' AND TABLE_NAME = 't_ds_process_definition' limit 1\n\nalter table dolphinscheduler_bak1.t_ds_process_definition_log auto_increment = {max_id};\n```\n\n----------------------------------------\n\nTITLE: Setting VM Options for DolphinScheduler Servers\nDESCRIPTION: VM options required for starting MasterServer, WorkerServer, and ApiApplicationServer.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DS_run_in_windows.md#2025-04-11_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\n-Dlogging.config=classpath:logback-master.xml -Ddruid.mysql.usePingMethod=false\n```\n\nLANGUAGE: properties\nCODE:\n```\n-Dlogging.config=classpath:logback-worker.xml -Ddruid.mysql.usePingMethod=false\n```\n\nLANGUAGE: properties\nCODE:\n```\n-Dlogging.config=classpath:logback-api.xml -Dspring.profiles.active=api\n```\n\n----------------------------------------\n\nTITLE: Setting ZooKeeper Connection for DolphinScheduler\nDESCRIPTION: ZooKeeper connection property in the zookeeper.properties file of the dolphinscheduler-service module.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DS_run_in_windows.md#2025-04-11_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nzookeeper.quorum=localhost:2181\n```\n\n----------------------------------------\n\nTITLE: Adding standard output to logback configuration files\nDESCRIPTION: XML configuration for enabling standard output in logback configuration files. This modification adds the STDOUT appender to the root logger, which allows logs to be displayed in the console.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<root level=\"INFO\">\n    <appender-ref ref=\"STDOUT\"/>  <!-- Add Standard Output -->\n    <appender-ref ref=\"APILOGFILE\"/>\n    <appender-ref ref=\"SKYWALKING-LOG\"/>\n</root>\n```\n\n----------------------------------------\n\nTITLE: Implementing LDAP Authentication in DolphinScheduler\nDESCRIPTION: LDAP authentication implementation that validates user credentials against an LDAP server and retrieves email attributes. The code handles LDAP context initialization, user search, and credential verification.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_7\n\nLANGUAGE: java\nCODE:\n```\n        ctx = new InitialLdapContext(searchEnv, null);\n        SearchControls sc = new SearchControls();\n        sc.setReturningAttributes(new String[]{ldapEmailAttribute});\n        sc.setSearchScope(SearchControls.SUBTREE_SCOPE);\n        EqualsFilter filter = new EqualsFilter(ldapUserIdentifyingAttribute, userId);\n        NamingEnumeration<SearchResult> results = ctx.search(ldapBaseDn, filter.toString(), sc);\n        if (results.hasMore()) {\n            // get the users DN (distinguishedName) from the result\n            SearchResult result = results.next();\n            NamingEnumeration<? extends Attribute> attrs = result.getAttributes().getAll();\n            while (attrs.hasMore()) {\n                // Open another connection to the LDAP server with the found DN and the password\n                searchEnv.put(Context.SECURITY_PRINCIPAL, result.getNameInNamespace());\n                searchEnv.put(Context.SECURITY_CREDENTIALS, userPwd);\n                try {\n                    new InitialDirContext(searchEnv);\n                } catch (Exception e) {\n                    logger.warn(\"invalid ldap credentials or ldap search error\", e);\n                    return null;\n                }\n                Attribute attr = attrs.next();\n                if (attr.getID().equals(ldapEmailAttribute)) {\n                    return (String) attr.get();\n                }\n            }\n        }\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinScheduler Frontend\nDESCRIPTION: Commands to install dependencies and start the DolphinScheduler frontend application.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/DS_run_in_windows.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpm install\nnpm run start\n```\n\n----------------------------------------\n\nTITLE: Executing SparkSQL Tasks on Yarn Cluster in DolphinScheduler\nDESCRIPTION: Implementation of a new SparkSQL task type that allows users to execute Spark tasks by writing SQL, which are then scheduled on Yarn in cluster mode to maximize resource utilization.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Application_transformation_of_the_FinTech_data_center_based_on_DolphinScheduler.md#2025-04-11_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Example SparkSQL query\nSELECT * FROM my_table WHERE condition\n```\n\n----------------------------------------\n\nTITLE: Switching to Production Branch in Git Repository\nDESCRIPTION: Git command to switch to the first-example-production branch, which contains modifications for an integrated workflow.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout first-example-production\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for ML Example Data\nDESCRIPTION: Shell command to create a directory to store all the process data for the machine learning example.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmkdir /tmp/ds-ml-example\n```\n\n----------------------------------------\n\nTITLE: Basic GitHub Actions Workflow\nDESCRIPTION: Hello World example of GitHub Actions configuration showing basic workflow structure.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: GitHub Actions Demo\non:\n  push:\n    branches:\n      - main\njobs:\n  hello-world:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Run my very first GitHub Actions\n        run: echo \"🎉 Hello World.\"\n```\n\n----------------------------------------\n\nTITLE: Cloning DolphinScheduler ML Tutorial Repository\nDESCRIPTION: This snippet shows how to clone the GitHub repository containing the tutorial code and switch to the dev branch.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Quick_Start_with_Apache_DolphinScheduler_Machine_Learning_Workflow.md#2025-04-11_snippet_0\n\nLANGUAGE: git\nCODE:\n```\ngit clone https://github.com/jieguangzhou/dolphinscheduler-ml-tutorial.git\ngit checkout dev\n```\n\n----------------------------------------\n\nTITLE: Specifying Resource Storage Type in Apache DolphinScheduler\nDESCRIPTION: Configuration option to specify the resource storage type in Apache DolphinScheduler, allowing users to choose between local resources, HDFS, or Amazon S3 for resource center storage.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache_dolphinScheduler_3.0.0_alpha.md#2025-04-11_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nresource.storage.type\n```\n\n----------------------------------------\n\nTITLE: DolphinScheduler Java Gateway Configuration\nDESCRIPTION: Environment variable configuration for DolphinScheduler Java Gateway connection\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport PYDS_JAVA_GATEWAY_ADDRESS=\"<YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER>\"\nexport PYDS_JAVA_GATEWAY_PORT=\"<PORT-RUN-DOLPHINSCHEDULER-API-SERVER>\"\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Custom DolphinScheduler Image\nDESCRIPTION: Dockerfile to create a custom DolphinScheduler image that includes additional components like MySQL driver, Oracle driver and DataX.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_4\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM harbor.abc.com/apache/dolphinscheduler:1.3.9\nCOPY *.jar /opt/dolphinscheduler/lib/\nRUN mkdir -p /opt/soft/datax\nCOPY datax /opt/soft/datax\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinScheduler with Helm\nDESCRIPTION: Shell commands to create a Kubernetes namespace and install DolphinScheduler using the Helm chart.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create ns ds139\nhelm install dolphinscheduler . -n ds139\n```\n\n----------------------------------------\n\nTITLE: Implementing HiveClient Parameters Class in Java\nDESCRIPTION: Creates a Parameters class that extends AbstractParameters to deserialize JSON parameters for HiveClient tasks. The class defines a SQL field and implements required methods for parameter validation and resource file listing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/zh-cn/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npackage com.jegger.dolphinscheduler.plugin.task.hive;\n\nimport org.apache.dolphinscheduler.spi.task.AbstractParameters;\nimport org.apache.dolphinscheduler.spi.task.ResourceInfo;\n\nimport java.util.List;\n\npublic class HiveClientParameters extends AbstractParameters {\n    /**\n     * 用HiveClient执行,最简单的方式就是将所有SQL全部贴进去即可,所以我们只需要一个SQL参数\n     */\n    private String sql;\n\n    public String getSql() {\n        return sql;\n    }\n\n    public void setSql(String sql) {\n        this.sql = sql;\n    }\n\n    @Override\n    public boolean checkParameters() {\n        return sql != null;\n    }\n\n    @Override\n    public List<ResourceInfo> getResourceFilesList() {\n        return null;\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Modified Task Instance Query\nDESCRIPTION: Updated SQL query to fix task instance list visibility by modifying table joins and filtering logic.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nselect\n<include refid=\"baseSqlV2\">\n    <property name=\"alias\" value=\"instance\"/>\n</include>\n,\nprocess.name as process_instance_name\nfrom t_ds_task_instance instance\njoin t_ds_process_instance process\n    on process.id=instance.process_instance_id\njoin t_ds_process_definition define\n    on define.code=process.process_definition_code\nwhere define.project_code = #{projectCode}\n<if test=\"startTime != null\">\n    and instance.start_time <![CDATA[ >=]]> #{startTime}\n</if>\n```\n\n----------------------------------------\n\nTITLE: Configuring Zookeeper connection for DolphinScheduler\nDESCRIPTION: Zookeeper connection configuration in the dolphinscheduler-service module. This defines the Zookeeper quorum connection string that DolphinScheduler will use for coordination.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nzookeeper.quorum=localhost:2181\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Storage Integration for DolphinScheduler\nDESCRIPTION: YAML configuration for integrating Apache DolphinScheduler with S3-compatible object storage (like MinIO) in a Kubernetes deployment, specifying storage paths, endpoint, and access credentials.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ncommon:\n ##Configmap\nconfigmap:\n  DOLPHINSCHEDULER_OPTS: \"\"\n  DATA_BASEDIR_PATH: \"/tmp/dolphinscheduler\"\n  RESOURCE_STORAGE_TYPE: \"S3\"\n  RESOURCE_UPLOAD_PATH: \"/dolphinscheduler\"\n  FS_DEFAULT_FS: \"s3a://dfs\"\n  FS_S3A_ENDPOINT: \"http://192.168.1.100:9000\"\n  FS_S3A_ACCESS_KEY: \"admin\"\n  FS_S3A_SECRET_KEY: \"password\"\n```\n\n----------------------------------------\n\nTITLE: Dynamic Python Script Detection and Execution\nDESCRIPTION: Bash script to dynamically find and execute all Python workflow scripts in a directory\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfor file in $(find . -name \"*.py\"); do\n    python \"$file\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Markdown SEO Template for Documentation\nDESCRIPTION: Example of required SEO frontmatter that should be included at the beginning of each Markdown document, with placeholders for title, keywords, and description.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/README.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n```\n---\ntitle: title\nkeywords: keywords1,keywords2, keywords3\ndescription: some description\n---\n```\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Bash Variable Parameter Passing in DolphinScheduler 3.0.0\nDESCRIPTION: This code snippet shows how to use bash variables to dynamically set parameters in downstream tasks. The example downloads a file from GitHub, counts the number of lines, and sets that value as a variable that can be used by other tasks.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache_dolphinScheduler_3.0.0.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlines_num=$(wget https://raw.githubusercontent.com/apache/dolphinscheduler/dev/README.md -q -O - | wc -l | xargs)\necho \"#{setValue(set_val_var=${lines_num})}\"\n```\n\n----------------------------------------\n\nTITLE: Executing Documentation Preparation Script in Shell\nDESCRIPTION: Command to run the preparation script that collects documentation from various sources into the working directory.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/HOW_PREPARE_WORK.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/prepare_docs.sh\n```\n\n----------------------------------------\n\nTITLE: VM Options for starting ApiApplicationServer\nDESCRIPTION: Command line options required for starting the DolphinScheduler ApiApplicationServer. These options configure the logging and set the active Spring profile to 'api'.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n-Dlogging.config=classpath:logback-api.xml -Dspring.profiles.active=api\n```\n\n----------------------------------------\n\nTITLE: Modifying install.sh for Multi-Environment Configuration in DolphinScheduler\nDESCRIPTION: Enhancement to the installation script to support multiple environments (dev, test, product) by adding an input parameter and selecting the appropriate config file.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Application_transformation_of_the_FinTech_data_center_based_on_DolphinScheduler.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ninstall.sh [dev|test|product]\n```\n\n----------------------------------------\n\nTITLE: Running Single Python Workflow Script\nDESCRIPTION: Simple command to execute a single DolphinScheduler Python API workflow script\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tutorial.py\n```\n\n----------------------------------------\n\nTITLE: Parsing SQL to Determine Input and Output Tables in Hive\nDESCRIPTION: Example SQL statement that would be parsed into a syntax tree to automatically determine input and output tables for data dependency management in Apache DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/How_Does_Ziru_Build_A_Job_Scheduling_System_Popular_Among_Data_Analysts.md#2025-04-11_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM a JOIN b ON a.id = b.id\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions with Environment Variables\nDESCRIPTION: Complete GitHub Actions configuration with environment variables and secrets for DolphinScheduler deployment.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: Execute Workflows\non:\n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: <YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_PORT: <PORT-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_AUTH_TOKEN: ${{ secrets.YOUR-SECRET-NAME }} \n        run: |\n          for file in $(find . -name \"*.py\"); do\n            python \"$file\"\n          done\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Values for Private Registry Image\nDESCRIPTION: YAML configuration to update the Helm values file to use the DolphinScheduler image from a private registry instead of the public Docker Hub.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nimage:\n  repository: \"harbor.abc.com/apache/dolphinscheduler\"\n  tag: \"1.3.9\"\n  pullPolicy: \"Always\"\n```\n\n----------------------------------------\n\nTITLE: Running DolphinScheduler 2.0.1 Upgrade Script\nDESCRIPTION: Shell script to automatically upgrade Apache DolphinScheduler from version 1.x to version 2.0.1. This one-line script handles the entire upgrade process while maintaining compatibility with existing workflows.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache-DolphinScheduler-2.0.1.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsh ./script/create-dolphinscheduler.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring HDFS Kerberos Authentication\nDESCRIPTION: XML configuration to fix Kerberos principal name error in Resource Center by setting the namenode principal pattern to wildcard.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<property>\n    <name>dfs.namenode.kerberos.principal.pattern</name>\n    <value>*</value>\n</property>\n```\n\n----------------------------------------\n\nTITLE: VM Options for starting MasterServer\nDESCRIPTION: Command line options required for starting the DolphinScheduler MasterServer. These options configure the logging and disable the Druid MySQL ping method.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n-Dlogging.config=classpath:logback-master.xml -Ddruid.mysql.usePingMethod=false\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions workflow for linting and executing Python scripts\nDESCRIPTION: A GitHub Actions workflow that runs pre-commit checks on all files and conditionally executes Python scripts only when pushing to the main branch. It includes environment configuration for connecting to the DolphinScheduler API server.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nname: Execute Workflows\non:\n  push:\n    branches:\n      - main\n  pull_request:\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pre-commit\n      - name: lint\n        run: |\n          pre-commit install\n\t\t  pre-commit run --all-files\n  execute:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push'\n    needs: lint\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: <YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_PORT: <PORT-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_AUTH_TOKEN: ${{ secrets.YOUR-SECRET-NAME }} \n        run: |\n          for file in $(find . -name \"*.py\"); do\n            python \"$file\"\n          done\n```\n\n----------------------------------------\n\nTITLE: DolphinScheduler SPI Dependencies\nDESCRIPTION: Maven dependencies required for implementing custom task types using DolphinScheduler's SPI framework.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_2\n\nLANGUAGE: plain\nCODE:\n```\n<! --dolphinscheduler spi basic core denpendence -->\n <dependency>\n     <groupId>org.apache.dolphinscheduler</groupId>\n     <artifactId>dolphinscheduler-spi</artifactId>\n     <version>${dolphinscheduler.lib.version}</version\n     <scope>${common.lib.scope}</scope>\n </dependency\n <dependency>\n     <groupId>org.apache.dolphinscheduler</groupId>\n     <artifactId>dolphinscheduler-task-api</artifactId>\n     <version>${dolphinscheduler.lib.version}</version\n     <scope>${common.lib.scope}</scope>\n </dependency\n```\n\n----------------------------------------\n\nTITLE: Starting or Stopping Apache DolphinScheduler Services\nDESCRIPTION: Command to start or stop individual services in Apache DolphinScheduler, such as master, worker, API, alert, or standalone servers.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache_dolphinScheduler_3.0.0_alpha.md#2025-04-11_snippet_1\n\nLANGUAGE: plain\nCODE:\n```\nbin/dolphinscheduler-daemon.sh <start|stop> <server-name>\n```\n\n----------------------------------------\n\nTITLE: Maven Project Creation Command\nDESCRIPTION: Maven command to generate a new project structure for implementing a custom Hive Client task type in DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Fast_Task_Type_Expanding_On_Apache_DolphinScheduler_Tutorial.md#2025-04-11_snippet_1\n\nLANGUAGE: plain\nCODE:\n```\nmvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.dolphinscheduler \\\n    -DarchetypeArtifactId=dolphinscheduler-hive-client-task \\\n    -DarchetypeVersion=1.10.0 \\\n    -DgroupId=org.apache.dolphinscheduler \\\n    -DartifactId=dolphinscheduler-hive-client-task \\\n    -Dversion=0.1 \\\n    -Dpackage=org.apache.dolphinscheduler \\\n    -DinteractiveMode=false\n```\n\n----------------------------------------\n\nTITLE: Basic GitHub Actions Hello World Configuration\nDESCRIPTION: Simple GitHub Actions workflow configuration demonstrating basic functionality\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: GitHub Actions Demo\non:\n  push:\n    branches:\n      - main\njobs:\n  hello-world:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Run my very first GitHub Actions\n        run: echo \"🎉 Hello World.\"\n```\n\n----------------------------------------\n\nTITLE: Deploying DolphinScheduler with Helm\nDESCRIPTION: Bash commands for deploying DolphinScheduler to Kubernetes using Helm, including adding the Bitnami repo and setting deployment parameters.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cd apache-dolphinscheduler-<version>-src/deploy/kubernetes/dolphinscheduler\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\n$ helm dependency update .\n$ helm install dolphinscheduler . --set image.tag=3.1.2 -n dolphinscheduler --set region=us-east-1 --set vpcId=vpc-xxx\n```\n\n----------------------------------------\n\nTITLE: Running Multiple Python Workflow Scripts\nDESCRIPTION: Bash commands to execute multiple DolphinScheduler Python API workflow scripts sequentially\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython workflow1.py\npython workflow2.py\npython workflow3.py\n...\npython workflowN.py\n```\n\n----------------------------------------\n\nTITLE: Scaling DolphinScheduler Components in Kubernetes\nDESCRIPTION: Bash commands to scale the number of replicas for DolphinScheduler API, Master, and Worker components in Kubernetes.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n## Scale up and down api to 3 replicas\nkubectl scale --replicas=3 deploy dolphinscheduler-api -n dolphinscheduler\n## Scale master to 2 replicas\nkubectl scale --replicas=2 sts dolphinscheduler-master -n dolphinscheduler\n## Scale worker to 2 replicas\nkubectl scale --replicas=6 sts dolphinscheduler-worker -n dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Creating EFS Persistent Volume for DolphinScheduler Storage\nDESCRIPTION: Kubernetes YAML manifest to create a Persistent Volume using AWS EFS CSI driver for persistent storage in DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n   name: dolphin-efs-pv\nspec:\n   capacity:\n     storage: 100Gi\n   volumeMode: Filesystem\n   accessModes:\n     - ReadWriteMany\n   persistentVolumeReclaimPolicy: Retain\n   storageClassName: efs-sc\n   csi:\n     driver: efs.csi.aws.com\n     volumeHandle: fs-xxx::fsap-xxx // fsap\n```\n\n----------------------------------------\n\nTITLE: VM Options for starting WorkerServer\nDESCRIPTION: Command line options required for starting the DolphinScheduler WorkerServer. These options configure the logging and disable the Druid MySQL ping method.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n-Dlogging.config=classpath:logback-worker.xml -Ddruid.mysql.usePingMethod=false\n```\n\n----------------------------------------\n\nTITLE: Executing Single Python Workflow\nDESCRIPTION: Basic command to run a single DolphinScheduler Python API workflow script.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_python_api_ci_cd.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tutorial.py\n```\n\n----------------------------------------\n\nTITLE: Declaring Global Variables in Shell Tasks for Switch Conditions\nDESCRIPTION: Shows how to define a global variable named 'id' through a shell task. This variable can be used in downstream switch tasks for conditional branching logic.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Apache-DolphinScheduler-2.0.1.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\necho'${setValue(id=1)}'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Load Balancer DNS Address for DolphinScheduler\nDESCRIPTION: Bash command to retrieve the external DNS address of the network load balancer service created for DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Deploy_the_serverless_Apache_DolphinScheduler_task_scheduling_system_on_AWS.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get service service-dolphinscheduler -n dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Frontend setup commands for DolphinScheduler UI\nDESCRIPTION: NPM commands for installing dependencies and starting the DolphinScheduler frontend application. These commands need to be run in the 'dolphinscheduler-ui' directory.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DS_run_in_windows.md#2025-04-11_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nnpm install\nnpm run start\n```\n\n----------------------------------------\n\nTITLE: Assigning DolphinScheduler Components to EC2 Instances\nDESCRIPTION: Component assignment configuration in the cluster_env.sh script that maps specific EC2 instances to DolphinScheduler components such as masters, workers, alert server, API servers, database, and registry server.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_launches_on_the_AWS_AMI_Application_Marketplace.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport masters=\"192.168.1.1,192.168.1.2\"\nexport workers=\"192.168.1.3:default,192.168.1.4:default,192.168.1.5:default\"\nexport alertServer=\"192.168.1.6\"\nexport apiServers=\"192.168.1.7\"\nexport DATABASE_SERVER=\"192.168.1.8\"\nexport REGISTRY_SERVER=\"192.168.1.8\"\n```\n\n----------------------------------------\n\nTITLE: Setting SSH Access Configuration for DolphinScheduler Cluster\nDESCRIPTION: SSH access configuration in the cluster_env.sh script that defines the user account and the path to the key pair file used for accessing the EC2 instances in the DolphinScheduler cluster.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_launches_on_the_AWS_AMI_Application_Marketplace.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Do not change this if you use this AMI to launch your instance\nexport INSTANCE_USER=${INSTANCE_USER:-\"ubuntu\"}\n# You have to change to your key pair path\nexport INSTANCE_KEY_PAIR=\"/change/to/your/personal/to/key/pair\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Resources for DolphinScheduler Deployment\nDESCRIPTION: Bash commands to inspect the Kubernetes resources created for a DolphinScheduler deployment, including pods, StatefulSets, ConfigMaps, services, and ingress configurations in the 'ds139' namespace.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_Kubernetes_Technology_in_action.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n[root@tpk8s-master01 ~]# kubectl get po -n ds139\nNAME READY STATUS RESTARTS AGE\ndolphinscheduler-alert-96c74dc84-72cc9 1/1 Running 0 22m\ndolphinscheduler-api-78db664b7b-gsltq 1/1 Running 0 22m\ndolphinscheduler-master-0 1/1 Running 0 22m\ndolphinscheduler-master-1 1/1 Running 0 22m\ndolphinscheduler-master-2 1/1 Running 0 22m\ndolphinscheduler-worker-0 1/1 Running 0 22m\ndolphinscheduler-worker-1 1/1 Running 0 22m\ndolphinscheduler-worker-2 1/1 Running 0 22m\n\n[root@tpk8s-master01 ~]# kubectl get statefulset -n ds139\nNAME READY AGE\ndolphinscheduler-master 3/3 22m\ndolphinscheduler-worker 3/3 22m\n\n[root@tpk8s-master01 ~]# kubectl get cm -n ds139\nNAME DATA AGE\ndolphinscheduler-alert 15 23m\ndolphinscheduler-api 1 23m\ndolphinscheduler-common 29 23m\ndolphinscheduler-master 10 23m\ndolphinscheduler-worker 7 23m\n\n[root@tpk8s-master01 ~]# kubectl get service -n ds139\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\ndolphinscheduler-api ClusterIP 10.43.238.5 <none> 12345/TCP 23m\ndolphinscheduler-master-headless ClusterIP None <none> 5678/TCP 23m\ndolphinscheduler-worker-headless ClusterIP None <none> 1234/TCP,50051/TCP 23m\n\n[root@tpk8s-master01 ~]# kubectl get ingress -n ds139\nNAME CLASS HOSTS ADDRESS\ndolphinscheduler <none> ds139.abc.com\n```\n\n----------------------------------------\n\nTITLE: Updating Task Instance Log Paths\nDESCRIPTION: SQL commands to update log paths in the database and copy log files to reflect new directory structure in version 3.1.2.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nupdate t_ds_task_instance set log_path=replace(log_path,'/logs/','/worker-server/logs/');\n\ncp -r {old_dolphinscheduler_directory}/logs/[1-9]* {new_dolphinscheduler_directory}/worker-server/logs/*\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinScheduler Cluster with cluster.sh\nDESCRIPTION: Command to execute the cluster.sh script after configuring cluster_env.sh. This script combines EC2 instances into a DolphinScheduler cluster, with execution time dependent on network speed.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/DolphinScheduler_launches_aws_ami.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./cluster.sh start\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Configuration with Environment Variables\nDESCRIPTION: Complete GitHub Actions workflow with environment variables and token authentication\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nname: Execute Workflows\non:\n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        env:\n          PYDS_JAVA_GATEWAY_ADDRESS: <YOUR-STATIC-IP-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_PORT: <PORT-RUN-DOLPHINSCHEDULER-API-SERVER>\n          PYDS_JAVA_GATEWAY_AUTH_TOKEN: ${{ secrets.YOUR-SECRET-NAME }} \n        run: |\n          for file in $(find . -name \"*.py\"); do\n            python \"$file\"\n          done\n```\n\n----------------------------------------\n\nTITLE: LDAP Authentication Configuration\nDESCRIPTION: YAML configuration template for setting up LDAP authentication in DolphinScheduler.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Issue_documentation_and_resolution_of_the_Apache_DolphinScheduler_upgrade_from1.3.4_to_3.1.2.md#2025-04-11_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nsecurity:\n  authentication:\n    # Authentication types (supported types: PASSWORD,LDAP)\n    type: LDAP\n    # IF you set type `LDAP`, below config will be effective\n    ldap:\n      # ldap server config\n      urls: xxx\n      base-dn: xxx\n      username: xxx\n      password: xxx\n      user:\n        # admin userId when you use LDAP login\n        admin: xxx\n        identity-attribute: xxx\n        email-attribute: xxx\n        # action when ldap user is not exist (supported types: CREATE,DENY)\n        not-exist-action: CREATE\n```\n\n----------------------------------------\n\nTITLE: Starting the StandaloneServer in Apache DolphinScheduler 1.3.9\nDESCRIPTION: A command to start the new StandaloneServer with a one-line command. This simplified startup process uses built-in registry and database components for quick deployment and testing.\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Introducing-Apache-DolphinScheduler-1.3.9.md#2025-04-11_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\nsh ./bin/dolphinscheduler-daemon.sh start standalone-server\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Workflow Execution Configuration\nDESCRIPTION: GitHub Actions configuration for executing DolphinScheduler Python API workflows\nSOURCE: https://github.com/apache/dolphinscheduler-website/blob/master/blog/en-us/Best_Practice_DolphinScheduler_Python_API_CI-CD.md#2025-04-11_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nname: Execute Workflows\non:    \n  push:\n    branches:\n      - main\njobs:\n  execute:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: actions/checkout@v3\n      - name: Execute\n        run: |\n          for file in $(find . -name \"*.py\"); do\n            python \"$file\"\n          done\n```"
  }
]
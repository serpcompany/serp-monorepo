[
  {
    "owner": "intellabs",
    "repo": "coach",
    "content": "TITLE: Creating CoachInterface Instance for Training\nDESCRIPTION: Initializes a CoachInterface object with a preset configuration for CartPole using Clipped PPO algorithm. Includes custom parameters for training steps and checkpoint saving.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncoach = CoachInterface(preset='CartPole_ClippedPPO',\n                       # The optional custom_parameter enables overriding preset settings\n                       custom_parameter='heatup_steps=EnvironmentSteps(5);improve_steps=TrainingSteps(3)',\n                       # Other optional parameters enable easy access to advanced functionalities\n                       num_workers=1, checkpoint_save_secs=10)\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating Trained Model\nDESCRIPTION: Loads the latest checkpoint of the trained model and performs evaluation, followed by cleanup of checkpoint directory.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nimport shutil\n\n# Clearing the previous graph before creating the new one to avoid name conflicts\ntf.reset_default_graph()\n\n# Updating the graph manager's task parameters to restore the latest stored checkpoint from the checkpoints directory\ntask_parameters2 = TaskParameters()\ntask_parameters2.checkpoint_restore_path = my_checkpoint_dir\n\ngraph_manager.create_graph(task_parameters2)\ngraph_manager.evaluate(EnvironmentSteps(5))\n\n# Clearning up\nshutil.rmtree(my_checkpoint_dir)\n```\n\n----------------------------------------\n\nTITLE: Running Single-threaded RL Algorithm with Coach\nDESCRIPTION: Example of running a single-threaded DQN algorithm on the CartPole environment using Coach's preset mechanism. The -p flag is used to specify the preset.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_DQN\n```\n\n----------------------------------------\n\nTITLE: Implementing Categorical DQN Agent Class\nDESCRIPTION: Defines the CategoricalDQNAgent class, inheriting from ValueOptimizationAgent. Implements the core learning algorithm following the Distributional DQN paper.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\n\n\n```\n\n----------------------------------------\n\nTITLE: Coach Package Installation\nDESCRIPTION: Commands to install Coach either via pip for normal usage or from source for development\nSOURCE: https://github.com/intellabs/coach/blob/master/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip3 install rl_coach\n\n# Or for development:\ncd coach\npip3 install -e .\n```\n\n----------------------------------------\n\nTITLE: Implementing Categorical DQN Agent Class\nDESCRIPTION: Core implementation of the Categorical DQN agent class that handles distribution predictions, Q-value calculations, and batch learning. Uses categorical distribution learning for value estimation as described in the C51 paper.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass CategoricalDQNAgent(ValueOptimizationAgent):\n    def __init__(self, agent_parameters, parent: Union['LevelManager', 'CompositeAgent']=None):\n        super().__init__(agent_parameters, parent)\n        self.z_values = np.linspace(self.ap.algorithm.v_min, self.ap.algorithm.v_max, self.ap.algorithm.atoms)\n\n    def distribution_prediction_to_q_values(self, prediction):\n        return np.dot(prediction, self.z_values)\n\n    # prediction's format is (batch,actions,atoms)\n    def get_all_q_values_for_states(self, states: StateType):\n        prediction = self.get_prediction(states)\n        return self.distribution_prediction_to_q_values(prediction)\n\n    def learn_from_batch(self, batch):\n        network_keys = self.ap.network_wrappers['main'].input_embedders_parameters.keys()\n\n        distributed_q_st_plus_1, TD_targets = self.networks['main'].parallel_prediction([\n            (self.networks['main'].target_network, batch.next_states(network_keys)),\n            (self.networks['main'].online_network, batch.states(network_keys))\n        ])\n\n        target_actions = np.argmax(self.distribution_prediction_to_q_values(distributed_q_st_plus_1), axis=1)\n        m = np.zeros((self.ap.network_wrappers['main'].batch_size, self.z_values.size))\n\n        batches = np.arange(self.ap.network_wrappers['main'].batch_size)\n        for j in range(self.z_values.size):\n            tzj = np.fmax(np.fmin(batch.rewards() +\n                                  (1.0 - batch.game_overs()) * self.ap.algorithm.discount * self.z_values[j],\n                                  self.z_values[self.z_values.size - 1]),\n                          self.z_values[0])\n            bj = (tzj - self.z_values[0])/(self.z_values[1] - self.z_values[0])\n            u = (np.ceil(bj)).astype(int)\n            l = (np.floor(bj)).astype(int)\n            m[batches, l] = m[batches, l] + (distributed_q_st_plus_1[batches, target_actions, j] * (u - bj))\n            m[batches, u] = m[batches, u] + (distributed_q_st_plus_1[batches, target_actions, j] * (bj - l))\n\n        TD_targets[batches, batch.actions()] = m\n\n        result = self.networks['main'].train_and_sync_networks(batch.states(network_keys), TD_targets)\n        total_loss, losses, unclipped_grads = result[:3]\n\n        return total_loss, losses, unclipped_grads\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Block Factory in Python for Reinforcement Learning\nDESCRIPTION: This code snippet demonstrates how to implement a custom Block Factory class for creating RL components. It includes a _create_block method that initializes environment, agents, level managers, and a block scheduler.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/graph_managers/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass CustomFactory(BlockFactory):\n    def __init__(self, custom_params):\n        super().__init__()\n\n    def _create_block(self, task_index: int, device=None) -> BlockScheduler:\n        \"\"\"\n        Create all the block modules and the block scheduler\n        :param task_index: the index of the process on which the worker will be run\n        :return: the initialized block scheduler\n        \"\"\"\n\n        # Create env\n        # Create composite agents\n        # Create level managers\n        # Create block scheduler\n\n        return block_scheduler\n```\n\n----------------------------------------\n\nTITLE: Saving and Evaluating RL Models with Coach\nDESCRIPTION: Two commands showing how to save checkpoints during training (-s flag) and then evaluate a saved model (--evaluate flag) without additional training.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_DQN -s 60\ncoach -p CartPole_DQN --evaluate -crd CHECKPOINT_RESTORE_DIR\n```\n\n----------------------------------------\n\nTITLE: Implementing Coach Environment API Methods in Python\nDESCRIPTION: This code block outlines the key methods that need to be implemented when creating a new environment using the Coach API. It includes methods for taking actions, updating state, restarting episodes, rendering, and getting rendered images.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/contributing/add_env.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _take_action(self, action_idx: ActionType) -> None:\n    \"\"\"\n    An environment dependent function that sends an action to the simulator.\n    :param action_idx: the action to perform on the environment\n    :return: None\n    \"\"\"\n\ndef _update_state(self) -> None:\n    \"\"\"\n    Updates the state from the environment.\n    Should update self.observation, self.reward, self.done, self.measurements and self.info\n    :return: None\n    \"\"\"\n\ndef _restart_environment_episode(self, force_environment_reset=False) -> None:\n    \"\"\"\n    Restarts the simulator episode\n    :param force_environment_reset: Force the environment to reset even if the episode is not done yet.\n    :return: None\n    \"\"\"\n\ndef _render(self) -> None:\n    \"\"\"\n    Renders the environment using the native simulator renderer\n    :return: None\n    \"\"\"\n\ndef get_rendered_image(self) -> np.ndarray:\n    \"\"\"\n    Return a numpy array containing the image that will be rendered to the screen.\n    This can be different from the observation. For example, mujoco's observation is a measurements vector.\n    :return: numpy array containing the image that will be rendered to the screen\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Architecture for Robot Learning Agent in Python\nDESCRIPTION: This code defines the neural network architecture for the actor, critic, and RND networks used in the goal-based data collection process. It specifies the layers, embedding schemes, and learning rates for each network component.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncamera_obs_scheme = [\n    Conv2d(32, 8, 4),\n    BatchnormActivationDropout(activation_function='relu'),\n    Conv2d(64, 4, 2),\n    BatchnormActivationDropout(activation_function='relu'),\n    Conv2d(64, 3, 1),\n    BatchnormActivationDropout(activation_function='relu'),\n    Flatten(),\n    Dense(256),\n    BatchnormActivationDropout(activation_function='relu')\n]\n\nactor_network = agent_params.network_wrappers['actor']\nactor_network.input_embedders_parameters = {\n    'measurements': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    agent_params.algorithm.agent_obs_key: InputEmbedderParameters(scheme=camera_obs_scheme, activation_function='none')\n}\n\nactor_network.middleware_parameters.scheme = [Dense(300), Dense(200)]\nactor_network.learning_rate = 1e-4\n\ncritic_network = agent_params.network_wrappers['critic']\ncritic_network.input_embedders_parameters = {\n    'action': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    'measurements': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n    agent_params.algorithm.agent_obs_key: InputEmbedderParameters(scheme=camera_obs_scheme, activation_function='none')\n}\n\ncritic_network.middleware_parameters.scheme = [Dense(400), Dense(300)]\ncritic_network.learning_rate = 1e-4\n\nagent_params.network_wrappers['predictor'].input_embedders_parameters = \\\n    {agent_params.algorithm.env_obs_key: InputEmbedderParameters(scheme=EmbedderScheme.Empty,\n                                                                 input_rescaling={'image': 1.0},\n                                                                 flatten=False)}\nagent_params.network_wrappers['constant'].input_embedders_parameters = \\\n    {agent_params.algorithm.env_obs_key: InputEmbedderParameters(scheme=EmbedderScheme.Empty,\n                                                                 input_rescaling={'image': 1.0},\n                                                                 flatten=False)}\nagent_params.network_wrappers['predictor'].heads_parameters = [RNDHeadParameters(is_predictor=True)]\n```\n\n----------------------------------------\n\nTITLE: Coach Installation Dependencies\nDESCRIPTION: Series of Ubuntu package installation commands required to set up Coach prerequisites including Python packages, Boost libraries, and visualization dependencies.\nSOURCE: https://github.com/intellabs/coach/blob/master/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# General\nsudo -E apt-get install python3-pip cmake zlib1g-dev python3-tk python-opencv -y\n\n# Boost libraries\nsudo -E apt-get install libboost-all-dev -y\n\n# Scipy requirements\nsudo -E apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran -y\n\n# PyGame\nsudo -E apt-get install libsdl-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev\nlibsmpeg-dev libportmidi-dev libavformat-dev libswscale-dev -y\n\n# Dashboard\nsudo -E apt-get install dpkg-dev build-essential python3.5-dev libjpeg-dev  libtiff-dev libsdl1.2-dev libnotify-dev \nfreeglut3 freeglut3-dev libsm-dev libgtk2.0-dev libgtk-3-dev libwebkitgtk-dev libgtk-3-dev libwebkitgtk-3.0-dev\nlibgstreamer-plugins-base1.0-dev -y\n\n# Gym\nsudo -E apt-get install libav-tools libsdl2-dev swig cmake -y\n```\n\n----------------------------------------\n\nTITLE: DQN Training Algorithm Steps in RST\nDESCRIPTION: Detailed steps for training a Deep Q Network, including sampling from replay buffer, calculating Q values, and updating network weights. The algorithm uses two networks - an online network for current predictions and a target network for stable Q-value estimates.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/dqn.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n1. Sample a batch of transitions from the replay buffer.\n\n2. Using the next states from the sampled batch, run the target network to calculate the :math:`Q` values for each of\n   the actions :math:`Q(s_{t+1},a)`, and keep only the maximum value for each state.\n\n3. In order to zero out the updates for the actions that were not played (resulting from zeroing the MSE loss),\n   use the current states from the sampled batch, and run the online network to get the current Q values predictions.\n   Set those values as the targets for the actions that were not actually played.\n\n4. For each action that was played, use the following equation for calculating the targets of the network:â€‹\n   :math:`y_t=r(s_t,a_t )+\\gamma \\cdot max_a Q(s_{t+1})`\n\n5. Finally, train the online network using the current states as inputs, and with the aforementioned targets.\n\n6. Once in every few thousand steps, copy the weights from the online network to the target network.\n```\n\n----------------------------------------\n\nTITLE: Running CoachInterface Training Loop\nDESCRIPTION: Simple execution of the training process using the CoachInterface run method\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncoach.run()\n```\n\n----------------------------------------\n\nTITLE: Core Environment Interface Methods in Coach\nDESCRIPTION: Essential methods that must be implemented when creating a new environment in Coach, including action handling, state updates, episode management, and rendering.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/contributing/add_env.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _take_action(self, action_idx: ActionType) -> None:\n    \"\"\"\n    An environment dependent function that sends an action to the simulator.\n    :param action_idx: the action to perform on the environment\n    :return: None\n    \"\"\"\n\ndef _update_state(self) -> None:\n    \"\"\"\n    Updates the state from the environment.\n    Should update self.observation, self.reward, self.done, self.measurements and self.info\n    :return: None\n    \"\"\"\n\ndef _restart_environment_episode(self, force_environment_reset=False) -> None:\n    \"\"\"\n    Restarts the simulator episode\n    :param force_environment_reset: Force the environment to reset even if the episode is not done yet.\n    :return: None\n    \"\"\"\n\ndef _render(self) -> None:\n    \"\"\"\n    Renders the environment using the native simulator renderer\n    :return: None\n    \"\"\"\n\ndef get_rendered_image(self) -> np.ndarray:\n    \"\"\"\n    Return a numpy array containing the image that will be rendered to the screen.\n    This can be different from the observation. For example, mujoco's observation is a measurements vector.\n    :return: numpy array containing the image that will be rendered to the screen\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing learn_from_batch and choose_action Methods in Python for Coach Agent\nDESCRIPTION: This snippet shows the structure of two key methods that need to be implemented when creating a new agent in Coach: learn_from_batch and choose_action. These methods are responsible for learning from batches of transitions and selecting actions respectively.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/contributing/add_agent.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef learn_from_batch(self, batch) -> Tuple[float, List, List]:\n    \"\"\"\n    Given a batch of transitions, calculates their target values and updates the network.\n    :param batch: A list of transitions\n    :return: The total loss of the training, the loss per head and the unclipped gradients\n    \"\"\"\n\ndef choose_action(self, curr_state):\n    \"\"\"\n    choose an action to act with in the current episode being played. Different behavior might be exhibited when training\n     or testing.\n\n    :param curr_state: the current state to act upon.\n    :return: chosen action, some action value describing the action (q-value, probability, etc)\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring DQN Agent Parameters\nDESCRIPTION: Configures the DQN agent parameters including algorithm settings, neural network configuration, and experience replay memory size for CartPole environment.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nagent_params = DQNAgentParameters()\n\n# DQN params\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nagent_params.algorithm.discount = 0.99\nagent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.00025\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n# ER size\nagent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n\nenv_params = GymVectorEnvironment(level='CartPole-v0')\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Coach Project\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for the Coach project. It includes libraries for various purposes such as machine learning (numpy, scipy), image processing (Pillow, scikit-image), visualization (matplotlib, bokeh), game development (pygame), cloud infrastructure (kubernetes, redis, minio), and testing (pytest).\nSOURCE: https://github.com/intellabs/coach/blob/master/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nannoy>=1.8.3\nPillow>=9.0.1\nmatplotlib>=2.0.2\nnumpy>=1.14.5\npandas>=0.22.0\npygame>=1.9.3\nPyOpenGL>=3.1.0\nscipy>=0.19.0\nscikit-image>=0.13.0\ngym==0.12.5\nbokeh==1.0.4\nkubernetes>=8.0.0b1,<=8.0.1\nredis>=2.10.6\nminio>=4.0.5\npytest>=3.8.2\npsutil>=5.5.0\njoblib>=0.17.0\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula for Clipped PPO Loss Function\nDESCRIPTION: The mathematical representation of the clipped surrogate loss function used in Clipped PPO. It shows how the likelihood ratio is clipped to achieve a similar effect to the adaptive KL divergence penalty in standard PPO.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/cppo.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nL^{CLIP}(\\theta)=E_{t}[min(r_t(\\theta)\\cdot \\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_t)]\n```\n\n----------------------------------------\n\nTITLE: Configuring Bottom Critic Network Parameters\nDESCRIPTION: Sets up the network parameters for the bottom critic agent, including input embedders, middleware parameters, learning rate and batch size configuration.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbottom_critic = bottom_agent_params.network_wrappers['critic']\nbottom_critic.input_embedders_parameters = {'observation': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                            'action': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                            'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\nbottom_critic.embedding_merger_type = EmbeddingMergerType.Concat\nbottom_critic.middleware_parameters.scheme = [Dense([64])] * 3\nbottom_critic.learning_rate = 0.001\nbottom_critic.batch_size = 4096\n```\n\n----------------------------------------\n\nTITLE: GraphManager Training Execution\nDESCRIPTION: Simple execution of the training process using the GraphManager improve method\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Coach\nDESCRIPTION: Command to run distributed Coach with specified preset, memory backend, data store, and number of rollout workers.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 rl_coach/coach.py -p CartPole_ClippedPPO \\\n-dc \\\n-e <experiment-name> \\\n-n 3 \\\n-dcp <path-to-config-file>\n```\n\n----------------------------------------\n\nTITLE: Running Basic Coach Training\nDESCRIPTION: Command to train a DQN agent on the CartPole environment using Coach's preset system. The -p flag specifies the preset and -r enables training mode.\nSOURCE: https://github.com/intellabs/coach/blob/master/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p CartPole_DQN -r\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Exploration Policy Classes in Python\nDESCRIPTION: Template for implementing a custom exploration policy showing the required class structure and methods. Includes both the parameters class that defines configuration and the main policy class that implements the exploration logic. The implementation requires inheriting from ExplorationParameters and ExplorationPolicy base classes.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/exploration_policies/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass CustomExplorationParameters(ExplorationParameters):\n    def __init__(self):\n        super().__init__()\n        ...\n\n    @property\n    def path(self):\n        return 'module_path:class_name'\n\n\nclass CustomExplorationPolicy(ExplorationPolicy):\n    def __init__(self, action_space: ActionSpace, ...):\n        super().__init__(action_space)\n\n    def reset(self):\n        ...\n\n    def get_action(self, action_values: List[ActionType]) -> ActionType:\n        ...\n\n    def change_phase(self, phase):\n        ...\n\n    def get_control_param(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Categorical DQN Algorithm and Exploration Parameters\nDESCRIPTION: Defines the algorithm-specific parameters for Categorical DQN, including v_min, v_max, and number of atoms. Also sets up epsilon-greedy exploration with a linear decay schedule.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.dqn_agent import DQNAlgorithmParameters\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\nfrom rl_coach.schedules import LinearSchedule\n\n\nclass CategoricalDQNAlgorithmParameters(DQNAlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n        self.v_min = -10.0\n        self.v_max = 10.0\n        self.atoms = 51\n\n\nclass CategoricalDQNExplorationParameters(EGreedyParameters):\n    def __init__(self):\n        super().__init__()\n        self.epsilon_schedule = LinearSchedule(1, 0.01, 1000000)\n        self.evaluation_epsilon = 0.001\n```\n\n----------------------------------------\n\nTITLE: Configuring Bottom-Level Agent Parameters\nDESCRIPTION: Defines the configuration for the bottom-level agent including exploration strategy, memory settings, and network architecture specific to the lower level of the hierarchy.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.schedules import ConstantSchedule\nfrom rl_coach.exploration_policies.e_greedy import EGreedyParameters\n\n\nbottom_agent_params = HACDDPGAgentParameters()\nbottom_agent_params.algorithm.in_action_space = goals_space\n\nbottom_agent_params.memory = EpisodicHindsightExperienceReplayParameters()\nbottom_agent_params.memory.max_size = (MemoryGranularity.Transitions, 12000000)\nbottom_agent_params.memory.hindsight_transitions_per_regular_transition = 4\nbottom_agent_params.memory.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Future\nbottom_agent_params.memory.goals_space = goals_space\nbottom_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(16 * 25)  # 25 episodes is one true env episode\nbottom_agent_params.algorithm.num_consecutive_training_steps = 40\nbottom_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\n\nbottom_agent_params.exploration = EGreedyParameters()\nbottom_agent_params.exploration.epsilon_schedule = ConstantSchedule(0.2)\nbottom_agent_params.exploration.evaluation_epsilon = 0\nbottom_agent_params.exploration.continuous_exploration_policy_parameters = OUProcessParameters()\nbottom_agent_params.exploration.continuous_exploration_policy_parameters.theta = 0.1\n\n# actor\nbottom_actor = bottom_agent_params.network_wrappers['actor']\nbottom_actor.input_embedders_parameters = {'observation': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                           'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\nbottom_actor.middleware_parameters.scheme = [Dense([64])] * 3\nbottom_actor.learning_rate = 0.001\nbottom_actor.batch_size = 4096\n```\n\n----------------------------------------\n\nTITLE: Configuring Top-Level Agent Parameters\nDESCRIPTION: Defines the configuration for the top-level agent including memory settings, exploration parameters, and neural network architecture for both actor and critic networks.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.architectures.tensorflow_components.layers import Dense\nfrom rl_coach.base_parameters import VisualizationParameters, EmbeddingMergerType, EmbedderScheme\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.memories.episodic.episodic_hindsight_experience_replay import HindsightGoalSelectionMethod, \\\n    EpisodicHindsightExperienceReplayParameters\nfrom rl_coach.memories.episodic.episodic_hrl_hindsight_experience_replay import \\\n    EpisodicHRLHindsightExperienceReplayParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.spaces import GoalsSpace, ReachingGoal\nfrom rl_coach.exploration_policies.ou_process import OUProcessParameters\nfrom rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps, RunPhase, TrainingSteps\n\n\ntime_limit = 1000\npolar_coordinates = False\ndistance_from_goal_threshold = np.array([0.075, 0.075, 0.75])\ngoals_space = GoalsSpace('achieved_goal',\n                         ReachingGoal(default_reward=-1, goal_reaching_reward=0,\n                                      distance_from_goal_threshold=distance_from_goal_threshold),\n                         lambda goal, state: np.abs(goal - state))  # raw L1 distance\n\ntop_agent_params = HACDDPGAgentParameters()\n\n# memory - Hindsight Experience Replay\ntop_agent_params.memory = EpisodicHRLHindsightExperienceReplayParameters()\ntop_agent_params.memory.max_size = (MemoryGranularity.Transitions, 10000000)\ntop_agent_params.memory.hindsight_transitions_per_regular_transition = 3\ntop_agent_params.memory.hindsight_goal_selection_method = HindsightGoalSelectionMethod.Future\ntop_agent_params.memory.goals_space = goals_space\ntop_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentEpisodes(32)\ntop_agent_params.algorithm.num_consecutive_training_steps = 40\ntop_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(40)\n\n# exploration - OU process\ntop_agent_params.exploration = OUProcessParameters()\ntop_agent_params.exploration.theta = 0.1\n\n# actor - note that the default middleware is overriden with 3 dense layers\ntop_actor = top_agent_params.network_wrappers['actor']\ntop_actor.input_embedders_parameters = {'observation': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                        'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\ntop_actor.middleware_parameters.scheme = [Dense([64])] * 3\ntop_actor.learning_rate = 0.001\ntop_actor.batch_size = 4096\n\n# critic - note that the default middleware is overriden with 3 dense layers\ntop_critic = top_agent_params.network_wrappers['critic']\ntop_critic.input_embedders_parameters = {'observation': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                         'action': InputEmbedderParameters(scheme=EmbedderScheme.Empty),\n                                         'desired_goal': InputEmbedderParameters(scheme=EmbedderScheme.Empty)}\ntop_critic.embedding_merger_type = EmbeddingMergerType.Concat\ntop_critic.middleware_parameters.scheme = [Dense([64])] * 3\ntop_critic.learning_rate = 0.001\ntop_critic.batch_size = 4096\n```\n\n----------------------------------------\n\nTITLE: Initializing Graph Manager for Training\nDESCRIPTION: Setup of the graph manager that coordinates the agent, environment, and visualization parameters for training.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.environments.gym_environment import atari_schedule\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=atari_schedule, vis_params=VisualizationParameters())\ngraph_manager.visualization_parameters.render = True\n```\n\n----------------------------------------\n\nTITLE: Implementing Core Agent Methods in Python\nDESCRIPTION: Shows the two essential methods that must be overridden when creating a new agent: learn_from_batch() to handle training from batches of transitions, and choose_action() to determine the agent's behavior in the environment.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/contributing/add_agent.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef learn_from_batch(self, batch) -> Tuple[float, List, List]:\n    \"\"\"\n    Given a batch of transitions, calculates their target values and updates the network.\n    :param batch: A list of transitions\n    :return: The total loss of the training, the loss per head and the unclipped gradients\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Saving GIFs of Agent Gameplay in Coach\nDESCRIPTION: Command for running A3C on Atari Breakout with 4 threads and GIF dumping enabled. The -dg flag enables saving GIFs of the agent's gameplay after each evaluation episode.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncoach -p Atari_A3C -lvl breakout -n 4 -dg\n```\n\n----------------------------------------\n\nTITLE: Calculating Q-Network Target in Soft Actor-Critic (Python/LaTeX)\nDESCRIPTION: Formula for computing the target values used to train the Q-network in the Soft Actor-Critic algorithm. It incorporates the immediate reward and the discounted future state value.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/sac.rst#2025-04-23_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\ny_t^Q=r(s_t,a_t)+\\gamma \\cdot V(s_{t+1})\n```\n\n----------------------------------------\n\nTITLE: Executing Training Process\nDESCRIPTION: Sets up the task parameters and initiates the training process with TensorFlow framework and specified logging configuration.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlog_path = '../experiments/pendulum_hac'\nif not os.path.exists(log_path):\n    os.makedirs(log_path)\n    \ntask_parameters = TaskParameters(framework_type=Frameworks.tensorflow, \n                                evaluate_only=False,\n                                experiment_path=log_path)\n\ntask_parameters.__dict__['checkpoint_save_secs'] = None\ntask_parameters.__dict__['verbosity'] = 'low'\n\ngraph_manager.create_graph(task_parameters)\n\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Agent Decision Tracking Implementation\nDESCRIPTION: Shows how to access and track agent decisions during inference using a GymEnvironment instance\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# inference\nenv_params = GymVectorEnvironment(level='CartPole-v0')\nenv = GymEnvironment(**env_params.__dict__, visualization_parameters=VisualizationParameters())\n\nresponse = env.reset_internal_state()\nfor _ in range(10):\n    action_info = coach.graph_manager.get_agent().choose_action(response.next_state)\n    print(\"State:{}, Action:{}\".format(response.next_state,action_info.action))\n    response = env.step(action_info.action)\n    print(\"Reward:{}\".format(response.reward))\n```\n\n----------------------------------------\n\nTITLE: TD3 Action Selection Formula\nDESCRIPTION: Mathematical formula showing how actions are selected by adding Gaussian noise to the actor network output for exploration during training.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/td3.rst#2025-04-23_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\mu\n```\n\n----------------------------------------\n\nTITLE: Computing TD3 Critic Network Targets in Continuous Action Spaces\nDESCRIPTION: Mathematical equation for computing the target values used to train the critic networks in TD3. It uses the minimum value from two critic networks to reduce overestimation bias, with added clipped noise for exploration.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/td3.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma \\cdot \\min_{i=1,2} Q_{i}(s_{t+1},\\mu(s_{t+1} )+[\\mathcal{N}(0,\\,\\sigma^{2})]^{MAX\\_NOISE}_{MIN\\_NOISE})\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Preset for CartPole with Clipped PPO in Python\nDESCRIPTION: This code snippet demonstrates how to create a minimal preset for training the CartPole environment using Clipped PPO in Coach. It defines the agent parameters, environment, and a simple schedule to create a BasicRLGraphManager.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/presets/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import SimpleSchedule\n\ngraph_manager = BasicRLGraphManager(\n    agent_params=ClippedPPOAgentParameters(),\n    env_params=GymVectorEnvironment(level='CartPole-v0'),\n    schedule_params=SimpleSchedule()\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Parameters Class in Python\nDESCRIPTION: Shows how to define a parameters class for a new agent (Rainbow in this example). The class initializes algorithm, exploration, memory, and network parameters, and specifies the path to the agent class.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/contributing/add_agent.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RainbowAgentParameters(AgentParameters):\ndef __init__(self):\n    super().__init__(algorithm=RainbowAlgorithmParameters(),\n                     exploration=RainbowExplorationParameters(),\n                     memory=RainbowMemoryParameters(),\n                     networks={\"main\": RainbowNetworkParameters()})\n\n@property\ndef path(self):\n    return 'rainbow.rainbow_agent:RainbowAgent'\n```\n\n----------------------------------------\n\nTITLE: Manual Training Loop Implementation\nDESCRIPTION: Demonstrates manual control over the training process by explicitly calling heatup and training phases while logging iterations\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.environments.gym_environment import GymEnvironment, GymVectorEnvironment\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import EnvironmentSteps\n\ntf.reset_default_graph()\ncoach = CoachInterface(preset='CartPole_ClippedPPO')\n\n# registering an iteration signal before starting to run\ncoach.graph_manager.log_signal('iteration', -1)\n\ncoach.graph_manager.heatup(EnvironmentSteps(100))\n\n# training\nfor it in range(10):\n    # logging the iteration signal during training\n    coach.graph_manager.log_signal('iteration', it)\n    # using the graph manager to train and act a given number of steps\n    coach.graph_manager.train_and_act(EnvironmentSteps(100))\n    # reading signals during training\n    training_reward = coach.graph_manager.get_signal_value('Training Reward')\n```\n\n----------------------------------------\n\nTITLE: Configuring GymEnvironmentParameters for OpenAI Gym API in Python\nDESCRIPTION: This snippet demonstrates how to set up environment parameters using GymEnvironmentParameters for an OpenAI Gym-compatible environment. It specifies the environment path and additional simulator parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/contributing/add_env.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nenv_params = GymEnvironmentParameters()\nenv_params.level = \"rl_coach.environments.mujoco.pendulum_with_goals:PendulumWithGoals\"\nenv_params.additional_simulator_parameters = {\"time_limit\": 1000}\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Action Selection in Python\nDESCRIPTION: Defines how an agent selects actions based on the current state, with potential for different behaviors during training versus testing.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/contributing/add_agent.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef choose_action(self, curr_state):\n    \"\"\"\n    choose an action to act with in the current episode being played. Different behavior might be exhibited when training\n     or testing.\n\n    :param curr_state: the current state to act upon.\n    :return: chosen action, some action value describing the action (q-value, probability, etc)\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Goal-Based Data Collection Process in Python\nDESCRIPTION: This code creates a BasicRLGraphManager with the configured agent and environment parameters, then starts the data collection process by calling the improve() method.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params, schedule_params=schedule_params)\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Configuring Categorical DQN Agent Parameters\nDESCRIPTION: Setup code for configuring the Categorical DQN agent parameters including learning rate for the network wrapper.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.categorical_dqn_agent import CategoricalDQNAgentParameters\n\nagent_params = CategoricalDQNAgentParameters()\nagent_params.network_wrappers['main'].learning_rate = 0.00025\n```\n\n----------------------------------------\n\nTITLE: Calculating Actor Network Gradient in Soft Actor-Critic (Python/LaTeX)\nDESCRIPTION: Formula for computing the gradient used to train the actor network in the Soft Actor-Critic algorithm. It approximates the policy gradient using a batch of states from the replay buffer.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/sac.rst#2025-04-23_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\n\\nabla_{\\theta} J \\approx \\nabla_{\\theta} \\frac{1}{\\vert B \\vert} \\sum_{s_t\\in B} \\left( Q \\left(s_t, \\tilde{a}_\\theta(s_t)\\right) - log\\pi_{\\theta}(\\tilde{a}_{\\theta}(s_t)\\vert s_t) \\right),\\,\\,\\,\\, \\tilde{a} \\sim \\pi(\\cdot \\vert s_t)\n```\n\n----------------------------------------\n\nTITLE: Calculating Double DQN Target Q-Values in Reinforcement Learning\nDESCRIPTION: This snippet shows the mathematical formula for calculating the target Q-values in Double DQN. It combines the immediate reward with the discounted future Q-value, using the online network for action selection and the target network for value estimation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/double_dqn.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:math:`y_t=r(s_t,a_t )+\\gamma \\cdot Q(s_{t+1},argmax_a Q(s_{t+1},a))`\n```\n\n----------------------------------------\n\nTITLE: Setting Up Control Suite Environment Parameters in Python\nDESCRIPTION: Imports and configures the DeepMind Control Suite environment parameters, setting up the cartpole balance level.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/2. Adding an Environment.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.environments.control_suite_environment import ControlSuiteEnvironmentParameters, control_suite_envs\nfrom rl_coach.environments.environment import SingleLevelSelection\n\nenv_params = ControlSuiteEnvironmentParameters(level='cartpole:balance')\n```\n\n----------------------------------------\n\nTITLE: Configuring Experience Replay and Exploration Parameters\nDESCRIPTION: Sets up experience replay memory size and epsilon-greedy exploration schedule parameters for batch reinforcement learning.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexperience_generating_agent_params.memory = EpisodicExperienceReplayParameters()\nexperience_generating_agent_params.memory.max_size = \\\n    (MemoryGranularity.Transitions,\n     experience_generating_schedule_params.heatup_steps.num_steps +\n     experience_generating_schedule_params.improve_steps.num_steps)\n\nexperience_generating_agent_params.exploration.epsilon_schedule = LinearSchedule(1.0, 0.01, DATASET_SIZE)\nexperience_generating_agent_params.exploration.evaluation_epsilon = 0\n\nschedule_params.improve_steps = TrainingSteps(50)\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical DQN Network Parameters\nDESCRIPTION: Creates a custom network parameters class for Categorical DQN, extending the DQN network parameters and specifying the CategoricalQHead.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.dqn_agent import DQNNetworkParameters\n\n\nclass CategoricalDQNNetworkParameters(DQNNetworkParameters):\n    def __init__(self):\n        super().__init__()\n        self.heads_parameters = [CategoricalQHeadParameters()]\n```\n\n----------------------------------------\n\nTITLE: Implementing Categorical DQN Head in TensorFlow\nDESCRIPTION: Defines the CategoricalQHead class and its parameters. Implements the network structure with a softmax output over atoms and cross-entropy loss.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass CategoricalQHeadParameters(HeadParameters):\n    def __init__(self, activation_function: str ='relu', name: str='categorical_q_head_params'):\n        super().__init__(parameterized_class=CategoricalQHead, activation_function=activation_function, name=name)\n\nclass CategoricalQHead(Head):\n    def __init__(self, agent_parameters: AgentParameters, spaces: SpacesDefinition, network_name: str,\n                 head_idx: int = 0, loss_weight: float = 1., is_local: bool = True, activation_function: str ='relu'):\n        super().__init__(agent_parameters, spaces, network_name, head_idx, loss_weight, is_local, activation_function)\n        self.name = 'categorical_dqn_head'\n        self.num_actions = len(self.spaces.action.actions)\n        self.num_atoms = agent_parameters.algorithm.atoms\n        self.return_type = QActionStateValue\n\n    def _build_module(self, input_layer):\n        self.actions = tf.placeholder(tf.int32, [None], name=\"actions\")\n        self.input = [self.actions]\n\n        values_distribution = tf.layers.dense(input_layer, self.num_actions * self.num_atoms, name='output')\n        values_distribution = tf.reshape(values_distribution, (tf.shape(values_distribution)[0], self.num_actions,\n                                                               self.num_atoms))\n        # softmax on atoms dimension\n        self.output = tf.nn.softmax(values_distribution)\n\n        # calculate cross entropy loss\n        self.distributions = tf.placeholder(tf.float32, shape=(None, self.num_actions, self.num_atoms),\n                                            name=\"distributions\")\n        self.target = self.distributions\n        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.target, logits=values_distribution)\n        tf.losses.add_loss(self.loss)\n```\n\n----------------------------------------\n\nTITLE: Computing Q-retrace Values in ACER Algorithm\nDESCRIPTION: Formulas for calculating Q-retrace values which provide a low-variance, unbiased return estimator. It uses importance sampling with truncated importance weights to combine on-policy and off-policy data.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/acer.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nQ^{ret}(s_t,a_t) = r_t +\\gamma \\bar{\\rho}_{t+1}[Q^{ret}(s_{t+1},a_{t+1}) - Q(s_{t+1},a_{t+1})] + \\gamma V(s_{t+1})\n```\n\nLANGUAGE: math\nCODE:\n```\n\\text{where} \\quad \\bar{\\rho}_{t} = \\min{\\left\\{c,\\rho_t\\right\\}},\\quad \\rho_t=\\frac{\\pi (a_t \\mid s_t)}{\\mu (a_t \\mid s_t)}\n```\n\n----------------------------------------\n\nTITLE: Calculating Persistent Advantage Learning (PAL) Targets\nDESCRIPTION: This formula computes the target values for persistent advantage learning. It extends AL by considering the minimum of the current and next state action gaps, providing more stable learning.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/pal.rst#2025-04-23_snippet_3\n\nLANGUAGE: math\nCODE:\n```\ny_t=y_t^{DDQN}-\\alpha \\cdot min(V(s_t )-Q(s_t,a_t ),V(s_{t+1} )-Q(s_{t+1},a_{t+1} ))\n```\n\n----------------------------------------\n\nTITLE: Computing TD3 Actor Network Gradients with Deterministic Policy Gradient\nDESCRIPTION: Mathematical equation for computing the policy gradient in TD3's actor network training. It propagates the critic's gradients with respect to actions through the actor network using the chain rule.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/td3.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\nabla_{\\theta^\\mu } J \\approx E_{s_t \\tilde{} \\rho^\\beta } [\\nabla_a Q_{1}(s,a)|_{s=s_t,a=\\mu (s_t ) } \\cdot \\nabla_{\\theta^\\mu} \\mu(s)|_{s=s_t} ]\n```\n\n----------------------------------------\n\nTITLE: Configuring Experience Generating Agent for Improved Batch RL Dataset\nDESCRIPTION: Sets up parameters for an agent to generate a more diverse dataset for Batch RL training. This agent will be trained and its replay buffer will be used as the dataset for subsequent Batch RL training, providing better coverage of the MDP.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntf.reset_default_graph() # just to clean things up; only needed for the tutorial\n\n# Experience Generating Agent parameters\nexperience_generating_agent_params = DDQNAgentParameters()\n\n# schedule parameters\nexperience_generating_schedule_params = ScheduleParameters()\nexperience_generating_schedule_params.heatup_steps = EnvironmentSteps(1000)\nexperience_generating_schedule_params.improve_steps = TrainingSteps(\n    DATASET_SIZE - experience_generating_schedule_params.heatup_steps.num_steps)\nexperience_generating_schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nexperience_generating_schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n\n# DQN params\nexperience_generating_agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\nexperience_generating_agent_params.algorithm.discount = 0.99\nexperience_generating_agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n\n# NN configuration\nexperience_generating_agent_params.network_wrappers['main'].learning_rate = 0.0001\nexperience_generating_agent_params.network_wrappers['main'].batch_size = 128\nexperience_generating_agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\nexperience_generating_agent_params.network_wrappers['main'].heads_parameters = \\\n[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n```\n\n----------------------------------------\n\nTITLE: Configuring Double DQN Agent for Batch RL Training on Acrobot\nDESCRIPTION: Sets up parameters for a Double DQN agent to be trained using Batch RL on the Acrobot environment. It configures network architecture, learning rate, replay buffer, and disables exploration since it's not applicable in Batch RL.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntf.reset_default_graph() # just to clean things up; only needed for the tutorial\n\n#########\n# Agent #\n#########\nagent_params = DDQNAgentParameters()\nagent_params.network_wrappers['main'].batch_size = 128\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(100)\nagent_params.algorithm.discount = 0.99\n\n# to jump start the agent's q values, and speed things up, we'll initialize the last Dense layer's bias\n# with a number in the order of the discounted reward of a random policy\nagent_params.network_wrappers['main'].heads_parameters = \\\n[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n\n# NN configuration\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\n# ER - we'll need an episodic replay buffer for off-policy evaluation\nagent_params.memory = EpisodicExperienceReplayParameters()\n\n# E-Greedy schedule - there is no exploration in Batch RL. Disabling E-Greedy. \nagent_params.exploration.epsilon_schedule = LinearSchedule(initial_value=0, final_value=0, decay_steps=1)\nagent_params.exploration.evaluation_epsilon = 0\n\n\ngraph_manager = BatchRLGraphManager(agent_params=agent_params,\n                                    env_params=env_params,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n                                    reward_model_num_epochs=30)\ngraph_manager.create_graph(task_parameters)\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Creating Filter Stack with Multiple Filters in Coach\nDESCRIPTION: Example demonstrating how to create and combine multiple filters (rescale, crop, clip) into a filter stack using InputFilter. Shows practical usage with actual parameter values and processing of environment response.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/filters/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom coach.filters.observation.observation_rescale_to_size_filter import ObservationRescaleToSizeFilter, RescaleInterpolationType\nfrom coach.filters.observation.observation_crop_filter import ObservationCropFilter\nfrom coach.filters.reward.reward_clipping_filter import RewardClippingFilter\nfrom environments.environment_interface import ObservationSpace\nimport numpy as np\nfrom core_types import EnvResponse\nfrom filters.filter import InputFilter\nfrom collections import OrderedDict\n\nenv_response = EnvResponse({'observation': np.ones([210, 160])}, reward=100, game_over=False)\n\nrescale = ObservationRescaleToSizeFilter(\n    output_observation_space=ObservationSpace(np.array([110, 84])),\n    rescaling_interpolation_type=RescaleInterpolationType.BILINEAR\n)\n\ncrop = ObservationCropFilter(\n    crop_low=np.array([16, 0]),\n    crop_high=np.array([100, 84])\n)\n\nclip = RewardClippingFilter(\n    clipping_low=-1,\n    clipping_high=1\n)\n\ninput_filter = InputFilter(\n    observation_filters=OrderedDict([('rescale', rescale), ('crop', crop)]),\n    reward_filters=OrderedDict([('clip', clip)])\n)\n\nresult = input_filter.filter(env_response)\n```\n\n----------------------------------------\n\nTITLE: Coach Virtual Environment Setup\nDESCRIPTION: Commands to create and activate a Python virtual environment for Coach installation\nSOURCE: https://github.com/intellabs/coach/blob/master/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo -E pip3 install virtualenv\nvirtualenv -p python3 coach_env\n. coach_env/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Custom Environment Setup with GraphManager\nDESCRIPTION: Demonstrates setting up a custom Gym environment using GraphManager directly with a BitFlip environment and Clipped PPO agent\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import SimpleSchedule\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\n\n# Resetting tensorflow graph as the network has changed.\ntf.reset_default_graph()\n\n# define the environment parameters\nbit_length = 10\nenv_params = GymVectorEnvironment(level='rl_coach.environments.toy_problems.bit_flip:BitFlip')\nenv_params.additional_simulator_parameters = {'bit_length': bit_length, 'mean_zero': True}\n\n# Clipped PPO\nagent_params = ClippedPPOAgentParameters()\nagent_params.network_wrappers['main'].input_embedders_parameters = {\n    'state': InputEmbedderParameters(scheme=[]),\n    'desired_goal': InputEmbedderParameters(scheme=[])\n}\n\ngraph_manager = BasicRLGraphManager(\n    agent_params=agent_params,\n    env_params=env_params,\n    schedule_params=SimpleSchedule()\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Batch RL Graph Manager\nDESCRIPTION: Creates and configures the BatchRLGraphManager with agent parameters, environment settings, and visualization options.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ngraph_manager = BatchRLGraphManager(agent_params=agent_params,\n                                    experience_generating_agent_params=experience_generating_agent_params,\n                                    experience_generating_schedule_params=experience_generating_schedule_params,\n                                    env_params=env_params,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n                                    reward_model_num_epochs=30,\n                                    train_to_eval_ratio=0.5)\ngraph_manager.create_graph(task_parameters)\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Configuring BCQ Agent with Dataset Loading\nDESCRIPTION: Sets up a BCQ (Batch-Constrained Q-learning) agent with specific neural network configurations and dataset loading from CSV.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntf.reset_default_graph()\n\nagent_params = DDQNBCQAgentParameters() \nagent_params.network_wrappers['main'].batch_size = 128\nagent_params.algorithm.num_steps_between_copying_online_weights_to_target = TrainingSteps(100)\nagent_params.algorithm.discount = 0.99\n\nagent_params.network_wrappers['main'].heads_parameters = \\\n[QHeadParameters(output_bias_initializer=tf.constant_initializer(-100))]\n\nagent_params.network_wrappers['main'].learning_rate = 0.0001\nagent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n\nagent_params.memory = EpisodicExperienceReplayParameters()\n\nagent_params.exploration.epsilon_schedule = LinearSchedule(initial_value=0, final_value=0, decay_steps=1)\nagent_params.exploration.evaluation_epsilon = 0\n\nagent_params.algorithm.action_drop_method_parameters = KNNParameters()\n\nDATATSET_PATH = 'acrobot_dataset.csv'\nagent_params.memory = EpisodicExperienceReplayParameters()\nagent_params.memory.load_memory_from_file_path = CsvDataset(DATATSET_PATH, is_episodic = True)\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula - Bellman Update Definition\nDESCRIPTION: Definition of the Bellman update calculation for each atom z_j in the categorical distribution, incorporating reward and discount factor.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/categorical_dqn.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\hat{T}_{z_{j}} := r+\\gamma z_j\n```\n\n----------------------------------------\n\nTITLE: Running Multi-threaded Algorithm in Coach (Python)\nDESCRIPTION: This command shows how to run a multi-threaded algorithm in Coach using the CartPole environment with A3C. The -p flag specifies the preset, and -n sets the number of threads.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_A3C -n 8\n```\n\n----------------------------------------\n\nTITLE: Computing State Value Network Training Targets in Soft Actor-Critic\nDESCRIPTION: Mathematical formula for computing the state value network training targets in SAC. The target is based on the minimum of the two Q-networks minus the log probability of the action sampled from the current policy.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/sac.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\ny_t^V = \\min_{i=1,2}Q_i(s_t,\\tilde{a}) - log\\pi (\\tilde{a} \\vert s),\\,\\,\\,\\, \\tilde{a} \\sim \\pi(\\cdot \\vert s_t)\n```\n\n----------------------------------------\n\nTITLE: Rendering Environment in Coach (Python)\nDESCRIPTION: This command shows how to render the environment during training or evaluation in Coach. It uses the Atari environment with DQN and the -r flag to enable rendering.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncoach -p Atari_DQN -lvl breakout -r\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running Graph Manager in Python\nDESCRIPTION: Creates and runs the BasicRLGraphManager with configured parameters for agent, environment, schedule, and visualization.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/2. Adding an Environment.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=vis_params)\n\n# let the adventure begin\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Starting Training Process\nDESCRIPTION: Command to initiate the training process using the configured graph manager.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Calculating Advantage using Q-value minus Value Function\nDESCRIPTION: Mathematical formula for calculating advantage by estimating Q-value using discounted rewards and bootstrapped value function, then subtracting the current state value. This represents the A_VALUE method for advantage estimation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/ac.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nA(s_t, a_t) = \\underbrace{\\sum_{i=t}^{i=t + k - 1} \\gamma^{i-t}r_i +\\gamma^{k} V(s_{t+k})}_{Q(s_t, a_t)} - V(s_t)\n```\n\n----------------------------------------\n\nTITLE: NAF Mathematical Formula - Q-Learning Target\nDESCRIPTION: The mathematical formula for calculating the Q-learning target value in NAF. It combines the immediate reward with the discounted future value.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/naf.rst#2025-04-23_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma\\cdot V(s_{t+1})\n```\n\n----------------------------------------\n\nTITLE: Running Multi-threaded RL Algorithm with Coach\nDESCRIPTION: Example of running a multi-threaded A3C algorithm on the CartPole environment using Coach. The -p flag specifies the preset and -n flag determines the number of threads to use.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_A3C -n 8\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset for Imitation Learning in Coach (Python)\nDESCRIPTION: This command shows how to create a dataset of human demonstrations for imitation learning in Coach. It uses the Doom environment and enables play mode to record human actions.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncoach -et rl_coach.environments.doom_environment:DoomEnvironmentParameters -lvl Basic --play\n```\n\n----------------------------------------\n\nTITLE: Command Line Options for Distributed Mode\nDESCRIPTION: Shows the command line flags used to enable distributed training mode in Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/design/horizontal_scaling.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--distributed_coach or -dc\n```\n\n----------------------------------------\n\nTITLE: Running Fetch Reach DDPG HER - Single Worker\nDESCRIPTION: Command to execute DDPG HER on the Fetch Reach environment using a single worker process. Uses baseline parameters as described in the referenced paper.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg_her/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Fetch_DDPG_HER_baselines -lvl reach\n```\n\n----------------------------------------\n\nTITLE: Importing DQN Algorithm Parameters in Python\nDESCRIPTION: This code snippet imports the DQNAlgorithmParameters class from the rl_coach.agents.dqn_agent module. It is used to configure the DQN algorithm parameters for training.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/dqn.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.dqn_agent.DQNAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Computing DDPG Actor Network Gradient in Python\nDESCRIPTION: This snippet shows the computation of the gradient for training the actor network in DDPG. It involves calculating the gradient of the Q-value with respect to actions and the gradient of the actor's output with respect to its parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/ddpg.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnabla_theta_mu_J â‰ˆ E_{s_t ~ rho^beta} [nabla_a Q(s,a)|_{s=s_t,a=mu(s_t)} Â· nabla_theta_mu mu(s)|_{s=s_t}]\n```\n\n----------------------------------------\n\nTITLE: Human Play Mode in Coach for Atari Games\nDESCRIPTION: Command for enabling human play mode in Coach for an Atari environment. The --play flag enables human interaction, and the environment and level are specified with -et and -lvl flags.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncoach -et rl_coach.environments.gym_environment:Atari -lvl BreakoutDeterministic-v4 --play\n```\n\n----------------------------------------\n\nTITLE: Configuring Robosuite Environment for Cube Manipulation Task in Python\nDESCRIPTION: This snippet sets up the Robosuite environment parameters for a cube manipulation task using a Franka Panda robot. It configures the robot type, controller, camera settings, and episode parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nenv_params = RobosuiteGoalBasedExpEnvironmentParameters(level='CubeExp')\nenv_params.robot = 'Panda'\nenv_params.custom_controller_config_fpath = '../rl_coach/environments/robosuite/osc_pose.json'\nenv_params.base_parameters.optional_observations = OptionalObservations.CAMERA\nenv_params.base_parameters.render_camera = 'frontview'\nenv_params.base_parameters.camera_names = 'agentview'\nenv_params.base_parameters.camera_depths = False\nenv_params.base_parameters.horizon = 200\nenv_params.base_parameters.ignore_done = False\nenv_params.base_parameters.use_object_obs = True\nenv_params.frame_skip = 1\nenv_params.base_parameters.control_freq = 2\nenv_params.base_parameters.camera_heights = 84\nenv_params.base_parameters.camera_widths = 84\nenv_params.extra_parameters = {'hard_reset': False}\n```\n\n----------------------------------------\n\nTITLE: Saving and Evaluating Checkpoints in Coach (Python)\nDESCRIPTION: These commands show how to save checkpoints during training and then evaluate a saved model. The -s flag sets the checkpoint saving interval, and --evaluate disables training for evaluation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_DQN -s 60\n```\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_DQN --evaluate -crd CHECKPOINT_RESTORE_DIR\n```\n\n----------------------------------------\n\nTITLE: Defining Categorical DQN Agent Parameters\nDESCRIPTION: Creates the main agent parameters class that combines algorithm, exploration, memory, and network parameters for the Categorical DQN agent.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.value_optimization_agent import ValueOptimizationAgent\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import StateType\nfrom rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\n\n\nclass CategoricalDQNAgentParameters(AgentParameters):\n    def __init__(self):\n        super().__init__(algorithm=CategoricalDQNAlgorithmParameters(),\n                         exploration=CategoricalDQNExplorationParameters(),\n                         memory=ExperienceReplayParameters(),\n                         networks={\"main\": CategoricalDQNNetworkParameters()})\n\n    @property\n    def path(self):\n        return 'agents.categorical_dqn_agent:CategoricalDQNAgent'\n```\n\n----------------------------------------\n\nTITLE: Cloning the Coach Repository\nDESCRIPTION: Commands to clone the Coach repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone git@github.com:NervanaSystems/coach.git\n$ cd coach\n```\n\n----------------------------------------\n\nTITLE: Calculating DDQN Target Values in PAL Algorithm\nDESCRIPTION: This formula calculates the initial target values for PAL using the DDQN method. It combines the immediate reward with the discounted future Q-value of the best action in the next state.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/pal.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t^{DDQN}=r(s_t,a_t )+\\gamma Q(s_{t+1},argmax_a Q(s_{t+1},a))\n```\n\n----------------------------------------\n\nTITLE: Switching to MXNet Backend in Coach (Python)\nDESCRIPTION: This command shows how to switch to the MXNet backend in Coach. It uses the Doom Basic environment with DQN and the -f flag to specify the MXNet framework.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncoach -p Doom_Basic_DQN -f mxnet\n```\n\n----------------------------------------\n\nTITLE: Defining Hierarchical Agent Parameters\nDESCRIPTION: Creates an array of agent parameters for the hierarchical structure, containing both top and bottom level agents.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nagents_params = [top_agent_params, bottom_agent_params]\n```\n\n----------------------------------------\n\nTITLE: Loading Replay Buffer for Behavioral Cloning in Coach\nDESCRIPTION: Command for loading a previously recorded replay buffer to train an agent using Behavioral Cloning. Uses the -cp flag to set the memory path parameter.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.core_types import PickledReplayBuffer\ncoach -p Doom_Basic_BC -cp='agent.load_memory_from_file_path=PickledReplayBuffer(\\\"<experiment dir>/replay_buffer.p\\\")'\n```\n\n----------------------------------------\n\nTITLE: Running Half Cheetah with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Half Cheetah environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl half_cheetah\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Schedule Parameters in Python\nDESCRIPTION: Sets up the training schedule with heatup steps to initialize the agent memory buffers.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/2. Adding an Environment.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.graph_managers.graph_manager import SimpleSchedule\nfrom rl_coach.core_types import EnvironmentSteps\n\nschedule_params = SimpleSchedule()\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n```\n\n----------------------------------------\n\nTITLE: Calculating Projected Bellman Update for Categorical DQN in Python\nDESCRIPTION: This snippet shows the mathematical formula for calculating the i-th component of the projected Bellman update in Categorical DQN. It uses LaTeX syntax within Python strings to represent the complex equation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/categorical_dqn.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n:math:`(\\Phi \\hat{T} Z_{\\theta}(s_t,a_t))_i=\\sum_{j=0}^{N-1}\\Big[1-\\frac{\\lvert[\\hat{T}_{z_{j}}]^{V_{MAX}}_{V_{MIN}}-z_i\\rvert}{\\Delta z}\\Big]^1_0 \\ p_j(s_{t+1}, \\pi(s_{t+1}))`\n```\n\n----------------------------------------\n\nTITLE: Calculating DDPG Critic Network Target in Python and LaTeX\nDESCRIPTION: This snippet demonstrates how to calculate the target for training the critic network in DDPG. It uses the current reward, discount factor, and the Q-value of the next state-action pair.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/ddpg.rst#2025-04-23_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma \\cdot Q(s_{t+1},\\mu(s_{t+1} ))\n```\n\n----------------------------------------\n\nTITLE: Calculating Advantage Learning (AL) Targets\nDESCRIPTION: This formula computes the target values for advantage learning by subtracting a weighted action gap from the DDQN targets. The action gap is weighted by a predefined parameter alpha.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/pal.rst#2025-04-23_snippet_2\n\nLANGUAGE: math\nCODE:\n```\ny_t=y_t^{DDQN}-\\alpha \\cdot (V(s_t )-Q(s_t,a_t ))\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Node Algorithm in Coach (Python)\nDESCRIPTION: This command demonstrates how to run a multi-node algorithm in Coach using the CartPole environment with Clipped PPO. It uses the -dc flag for distributed mode and -dcp for specifying the configuration file.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_ClippedPPO -dc -dcp <path-to-config-file> -n 8\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setting Up Environment for Batch RL with Coach\nDESCRIPTION: Imports required libraries and sets up environment parameters for using Batch RL with the Coach framework. It configures the Acrobot-v1 Gym environment and sets basic parameters like dataset size.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom copy import deepcopy\nimport tensorflow as tf\nimport os\n\nfrom rl_coach.agents.dqn_agent import DQNAgentParameters\nfrom rl_coach.agents.ddqn_bcq_agent import DDQNBCQAgentParameters, KNNParameters\nfrom rl_coach.base_parameters import VisualizationParameters\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, CsvDataset\nfrom rl_coach.environments.gym_environment import GymVectorEnvironment\nfrom rl_coach.graph_managers.batch_rl_graph_manager import BatchRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.memories.memory import MemoryGranularity\nfrom rl_coach.schedules import LinearSchedule\nfrom rl_coach.memories.episodic import EpisodicExperienceReplayParameters\nfrom rl_coach.architectures.head_parameters import QHeadParameters\nfrom rl_coach.agents.ddqn_agent import DDQNAgentParameters\nfrom rl_coach.base_parameters import TaskParameters\nfrom rl_coach.spaces import SpacesDefinition, DiscreteActionSpace, VectorObservationSpace, StateSpace, RewardSpace\n\n# Get all the outputs of this tutorial out of the 'Resources' folder\nos.chdir('Resources')\n\n# the dataset size to collect \nDATASET_SIZE = 50000\n\ntask_parameters = TaskParameters(experiment_path='.')\n\n####################\n# Graph Scheduling #\n####################\n\nschedule_params = ScheduleParameters()\n\n# 100 epochs (we run train over all the dataset, every epoch) of training\nschedule_params.improve_steps = TrainingSteps(100)\n\n# we evaluate the model every epoch\nschedule_params.steps_between_evaluation_periods = TrainingSteps(1)\n\n# only for when we have an enviroment\nschedule_params.evaluation_steps = EnvironmentEpisodes(10)\nschedule_params.heatup_steps = EnvironmentSteps(DATASET_SIZE)\n\n################\n#  Environment #\n################\nenv_params = GymVectorEnvironment(level='Acrobot-v1')\n```\n\n----------------------------------------\n\nTITLE: Running TD3 on Half Cheetah Mujoco Environment\nDESCRIPTION: Command for training a TD3 agent on the Half Cheetah environment using a single worker. Uses the parameters specified in the original TD3 paper and runs for 1M environment steps with 5 seeds.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/td3/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_TD3 -lvl half_cheetah\n```\n\n----------------------------------------\n\nTITLE: Observation Space Class Documentation\nDESCRIPTION: Documentation for the base ObservationSpace class and its specialized variants including vector, planar maps, and image observation spaces.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/spaces.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.spaces.ObservationSpace\n   :members:\n   :inherited-members:\n\n.. autoclass:: rl_coach.spaces.VectorObservationSpace\n\n.. autoclass:: rl_coach.spaces.PlanarMapsObservationSpace\n\n.. autoclass:: rl_coach.spaces.ImageObservationSpace\n```\n\n----------------------------------------\n\nTITLE: Defining EnvironmentParameters for Doom in Python\nDESCRIPTION: This snippet shows how to create a custom EnvironmentParameters class for a Doom environment. It includes setting default input and output filters, specifying camera types, and defining the path to the environment class.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/contributing/add_env.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DoomEnvironmentParameters(EnvironmentParameters):\n    def __init__(self):\n        super().__init__()\n        self.default_input_filter = DoomInputFilter\n        self.default_output_filter = DoomOutputFilter\n        self.cameras = [DoomEnvironment.CameraTypes.OBSERVATION]\n\n    @property\n    def path(self):\n        return 'rl_coach.environments.doom_environment:DoomEnvironment'\n```\n\n----------------------------------------\n\nTITLE: N-Step Q Learning Reward Calculation Formula\nDESCRIPTION: Mathematical formula for calculating N-step Q targets. The equation accumulates rewards from N consecutive steps and includes a discounted future value term.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/n_step.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nR(s_t, a_t) = \\sum_{i=t}^{i=t + k - 1} \\gamma^{i-t}r_i +\\gamma^{k} V(s_{t+k})\n```\n\n----------------------------------------\n\nTITLE: Calculating N-step Return in Neural Episodic Control\nDESCRIPTION: This formula calculates the N-step return used for training the NEC network. It sums the discounted rewards over N steps and bootstraps the last value from the network if necessary.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/nec.rst#2025-04-23_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\ny_t=\\sum_{j=0}^{N-1}\\gamma^j r(s_{t+j},a_{t+j} ) +\\gamma^N   max_a Q(s_{t+N},a)\n```\n\n----------------------------------------\n\nTITLE: Rendering Environment During RL Training in Coach\nDESCRIPTION: Command for running DQN on Atari Breakout with environment rendering enabled. The -r flag enables rendering of the environment during training or evaluation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncoach -p Atari_DQN -lvl breakout -r\n```\n\n----------------------------------------\n\nTITLE: Synchronization Types Usage in Coach Configuration\nDESCRIPTION: Demonstrates how to specify synchronization type in agent parameters using DistributedCoachSynchronizationType enum. This setting controls how policy checkpoints are synchronized between training and rollout workers.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/design/horizontal_scaling.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nagent_params.algorithm.distributed_coach_synchronization_type\n```\n\n----------------------------------------\n\nTITLE: Running SAC on Walker 2D Environment\nDESCRIPTION: Command to execute SAC algorithm on the Mujoco Walker 2D environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/sac/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_SAC -lvl walker2d\n```\n\n----------------------------------------\n\nTITLE: Running Dueling DDQN on Pong\nDESCRIPTION: Command to execute Dueling DDQN training on the Atari Pong game using a single worker configuration.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dueling_ddqn/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Dueling_DDQN -lvl pong\n```\n\n----------------------------------------\n\nTITLE: Implementing HAC DDPG Agent Class\nDESCRIPTION: Core implementation of the Hierarchical Actor Critic agent that extends DDPG agent functionality with hierarchical learning capabilities and subgoal management.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass HACDDPGAgent(DDPGAgent):\n    def __init__(self, agent_parameters, parent: Union['LevelManager', 'CompositeAgent']=None):\n        super().__init__(agent_parameters, parent)\n        self.sub_goal_testing_rate = self.ap.algorithm.sub_goal_testing_rate\n        self.graph_manager = None\n\n    def choose_action(self, curr_state):\n        # top level decides, for each of his generated sub-goals, for all the layers beneath him if this is a sub-goal\n        # testing phase\n        graph_manager = self.parent_level_manager.parent_graph_manager\n        if self.ap.is_a_highest_level_agent:\n            graph_manager.should_test_current_sub_goal = np.random.rand() < self.sub_goal_testing_rate\n\n        if self.phase == RunPhase.TRAIN:\n            if graph_manager.should_test_current_sub_goal:\n                self.exploration_policy.change_phase(RunPhase.TEST)\n            else:\n                self.exploration_policy.change_phase(self.phase)\n\n        action_info = super().choose_action(curr_state)\n        return action_info\n\n    def update_transition_before_adding_to_replay_buffer(self, transition):\n        graph_manager = self.parent_level_manager.parent_graph_manager\n\n        # deal with goals given from a higher level agent\n        if not self.ap.is_a_highest_level_agent:\n            transition.state['desired_goal'] = self.current_hrl_goal\n            transition.next_state['desired_goal'] = self.current_hrl_goal\n            self.distance_from_goal.add_sample(self.spaces.goal.distance_from_goal(\n                self.current_hrl_goal, transition.next_state))\n            goal_reward, sub_goal_reached = self.spaces.goal.get_reward_for_goal_and_state(\n                self.current_hrl_goal, transition.next_state)\n            transition.reward = goal_reward\n            transition.game_over = transition.game_over or sub_goal_reached\n\n        # each level tests its own generated sub goals\n        if not self.ap.is_a_lowest_level_agent and graph_manager.should_test_current_sub_goal:\n            _, sub_goal_reached = self.spaces.goal.get_reward_for_goal_and_state(\n                transition.action, transition.next_state)\n\n            sub_goal_is_missed = not sub_goal_reached\n\n            if sub_goal_is_missed:\n                    transition.reward = -self.ap.algorithm.time_limit\n        return transition\n\n    def set_environment_parameters(self, spaces: SpacesDefinition):\n        super().set_environment_parameters(spaces)\n\n        if self.ap.is_a_highest_level_agent:\n            # the rest of the levels already have an in_action_space set to be of type GoalsSpace, thus they will have\n            # their GoalsSpace set to the in_action_space in agent.set_environment_parameters()\n            self.spaces.goal = self.spaces.action\n            self.spaces.goal.set_target_space(self.spaces.state[self.spaces.goal.goal_name])\n\n        if not self.ap.is_a_highest_level_agent:\n            self.spaces.reward.reward_success_threshold = self.spaces.goal.reward_type.goal_reaching_reward\n```\n\n----------------------------------------\n\nTITLE: Configuring TD3 Goal-Based Agent Parameters for Robot Learning in Python\nDESCRIPTION: This snippet sets up the TD3 goal-based agent parameters for the data collection task. It configures algorithm-specific settings, exploration noise, RND parameters, and replay buffer options.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nagent_params = TD3GoalBasedAgentParameters()\nagent_params.algorithm.use_non_zero_discount_for_terminal_states = False\nagent_params.algorithm.identity_goal_sample_rate = 0.04\nagent_params.exploration.noise_schedule = LinearSchedule(1.5, 0.5, 300000)\n\nagent_params.algorithm.rnd_sample_size = 2000\nagent_params.algorithm.rnd_batch_size = 500\nagent_params.algorithm.rnd_optimization_epochs = 4\nagent_params.algorithm.td3_training_ratio = 1.0\nagent_params.algorithm.identity_goal_sample_rate = 0.0\nagent_params.algorithm.env_obs_key = 'camera'\nagent_params.algorithm.agent_obs_key = 'obs-goal'\nagent_params.algorithm.replay_buffer_save_steps = 25000\nagent_params.algorithm.replay_buffer_save_path = './Resources'\n\nagent_params.input_filter = NoInputFilter()\nagent_params.output_filter = NoOutputFilter()\n```\n\n----------------------------------------\n\nTITLE: Using Imitation Learning in Coach (Python)\nDESCRIPTION: This command demonstrates how to use imitation learning in Coach with a recorded replay buffer. It uses the Doom Basic environment with Behavioral Cloning (BC) and loads a previously recorded replay buffer.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.core_types import PickledReplayBuffer\ncoach -p Doom_Basic_BC -cp='agent.load_memory_from_file_path=PickledReplayBuffer(\\\"<experiment dir>/replay_buffer.p\\\")'\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for HAC Agent\nDESCRIPTION: Sets up necessary imports including DDPG agent components and core Coach utilities. The imports establish the foundation for implementing the hierarchical reinforcement learning agent.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nmodule_path = os.path.abspath(os.path.join('..'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n    sys.path.append(module_path + '/rl_coach')\n    \nfrom typing import Union\nimport numpy as np\nfrom rl_coach.agents.ddpg_agent import DDPGAgent, DDPGAgentParameters, DDPGAlgorithmParameters\nfrom rl_coach.spaces import SpacesDefinition\nfrom rl_coach.core_types import RunPhase\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Coach\nDESCRIPTION: Command to run Distributed Coach with specified parameters and configuration.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 rl_coach/coach.py -p CartPole_ClippedPPO \\\n-dc \\\n-e <experiment-name> \\\n-n 3 \\\n-dcp <path-to-config-file>\n```\n\n----------------------------------------\n\nTITLE: Initializing Coach Module Imports\nDESCRIPTION: Sets up the Python environment by adding Coach module paths to sys.path and importing the CoachInterface class\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Adding module path to sys path if not there, so rl_coach submodules can be imported\nimport os\nimport sys\nimport tensorflow as tf\nmodule_path = os.path.abspath(os.path.join('..'))\nresources_path = os.path.abspath(os.path.join('Resources'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\nif resources_path not in sys.path:\n    sys.path.append(resources_path)\n    \nfrom rl_coach.coach import CoachInterface\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula - Categorical DQN Projection\nDESCRIPTION: Formula for projecting the Bellman update to the set of atoms representing Q-value distributions. This equation calculates the i-th component of the projected update using probability distributions and bounded operations.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/categorical_dqn.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\n(\\Phi \\hat{T} Z_{\\theta}(s_t,a_t))_i=\\sum_{j=0}^{N-1}\\Big[1-\\frac{\\lvert[\\hat{T}_{z_{j}}]^{V_{MAX}}_{V_{MIN}}-z_i\\rvert}{\\Delta z}\\Big]^1_0 \\ p_j(s_{t+1}, \\pi(s_{t+1}))\n```\n\n----------------------------------------\n\nTITLE: Defining HAC Algorithm Parameters\nDESCRIPTION: Extends DDPG algorithm parameters to include HAC-specific parameters like subgoal testing rate and time limit.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass HACDDPGAlgorithmParameters(DDPGAlgorithmParameters):\n    def __init__(self):\n        super().__init__()\n        self.sub_goal_testing_rate = 0.5\n        self.time_limit = 40\n\n\nclass HACDDPGAgentParameters(DDPGAgentParameters):\n    def __init__(self):\n        super().__init__()\n        self.algorithm = DDPGAlgorithmParameters()\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Base Coach Image\nDESCRIPTION: Dockerfile contents for creating the base Coach Docker image with required dependencies.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nFROM nvidia/cuda:9.0-cudnn7-runtime-ubuntu16.04\n\n################################\n# Install apt-get Requirements #\n################################\n\n# General\nRUN apt-get update && \\\n    apt-get install -y python3-pip cmake zlib1g-dev python3-tk python-opencv \\\n    # Boost libraries\n    libboost-all-dev \\\n    # Scipy requirements\n    libblas-dev liblapack-dev libatlas-base-dev gfortran \\\n    # Pygame requirements\n    libsdl-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev \\\n    libsmpeg-dev libportmidi-dev libavformat-dev libswscale-dev \\\n    # Dashboard\n    dpkg-dev build-essential python3.5-dev libjpeg-dev  libtiff-dev libsdl1.2-dev libnotify-dev \\\n    freeglut3 freeglut3-dev libsm-dev libgtk2.0-dev libgtk-3-dev libwebkitgtk-dev libgtk-3-dev \\\n    libwebkitgtk-3.0-dev libgstreamer-plugins-base1.0-dev \\\n    # Gym\n    libav-tools libsdl2-dev swig cmake \\\n    # Mujoco_py\n    curl libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev software-properties-common \\\n    # ViZDoom\n    build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n    nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n    libopenal-dev timidity libwildmidi-dev unzip wget && \\\n    apt-get clean autoclean && \\\n    apt-get autoremove -y\n\n############################\n# Install Pip Requirements #\n############################\nRUN pip3 install --upgrade pip\nRUN pip3 install setuptools==39.1.0 && pip3 install pytest && pip3 install pytest-xdist\n\nRUN curl -o /usr/local/bin/patchelf https://s3-us-west-2.amazonaws.com/openai-sci-artifacts/manual-builds/patchelf_0.9_amd64.elf \\\n    && chmod +x /usr/local/bin/patchelf\n```\n\n----------------------------------------\n\nTITLE: Computing Actor Network Gradient in Soft Actor-Critic\nDESCRIPTION: Mathematical formula for computing the gradient for training the actor/policy network in SAC. The objective combines maximizing Q-values and action entropy to encourage exploration.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/sac.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: math\nCODE:\n```\n\\nabla_{\\theta} J \\approx \\nabla_{\\theta} \\frac{1}{\\vert B \\vert} \\sum_{s_t\\in B} \\left( Q \\left(s_t, \\tilde{a}_\\theta(s_t)\\right) - log\\pi_{\\theta}(\\tilde{a}_{\\theta}(s_t)\\vert s_t) \\right),\\,\\,\\,\\, \\tilde{a} \\sim \\pi(\\cdot \\vert s_t)\n```\n\n----------------------------------------\n\nTITLE: Computing Q Retrace in ACER Algorithm\nDESCRIPTION: These LaTeX formulas define the Q retrace calculation in ACER. It involves a recursive computation using rewards, discount factor, and importance sampling ratios.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/acer.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\nQ^{ret}(s_t,a_t) = r_t +\\gamma \\bar{\\rho}_{t+1}[Q^{ret}(s_{t+1},a_{t+1}) - Q(s_{t+1},a_{t+1})] + \\gamma V(s_{t+1})\n```\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\text{where} \\quad \\bar{\\rho}_{t} = \\min{\\left\\{c,\\rho_t\\right\\}},\\quad \\rho_t=\\frac{\\pi (a_t \\mid s_t)}{\\mu (a_t \\mid s_t)}\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies for Coach\nDESCRIPTION: Commands to install the required Python packages for building the Coach documentation. Installs Sphinx, recommonmark, sphinx_rtd_theme, sphinx-autobuild, and sphinx-argparse.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install Sphinx\npip install recommonmark\npip install sphinx_rtd_theme\npip install sphinx-autobuild\npip install sphinx-argparse\n```\n\n----------------------------------------\n\nTITLE: Calculating Projected Bellman Update for Rainbow in Python and LaTeX\nDESCRIPTION: This snippet shows the mathematical formula for calculating the i-th component of the projected Bellman update in the Rainbow algorithm. It uses a combination of Python-like syntax and LaTeX for mathematical notation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/rainbow.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n(\\Phi \\hat{T} Z_{\\theta}(s_t,a_t))_i=\\sum_{j=0}^{N-1}\\Big[1-\\frac{\\lvert[\\hat{T}_{z_{j}}]^{V_{MAX}}_{V_{MIN}}-z_i\\rvert}{\\Delta z}\\Big]^1_0 \\ p_j(s_{t+1}, \\pi(s_{t+1}))\n```\n\n----------------------------------------\n\nTITLE: Training Quantile Regression DQN in Python\nDESCRIPTION: Describes the steps for training a Quantile Regression DQN network. Includes sampling from replay buffer, predicting quantiles, calculating targets, and updating the network using quantile regression loss.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/qr_dqn.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n1. Sample a batch of transitions from the replay buffer.\n\n2. First, the next state quantiles are predicted. These are used in order to calculate the targets for the network,\n   by following the Bellman equation.\n   Next, the current quantile locations for the current states are predicted, sorted, and used for calculating the\n   quantile midpoints targets.\n\n3. The network is trained with the quantile regression loss between the resulting quantile locations and the target\n   quantile locations. Only the targets of the actions that were actually taken are updated.\n\n4. Once in every few thousand steps, weights are copied from the online network to the target network.\n```\n\n----------------------------------------\n\nTITLE: Creating HRL Graph Manager\nDESCRIPTION: Initializes the Hierarchical Reinforcement Learning Graph Manager with the configured parameters and sets visualization options.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngraph_manager = HRLGraphManager(agents_params=agents_params, env_params=env_params,\n                                schedule_params=schedule_params, vis_params=vis_params,\n                                consecutive_steps_to_run_each_level=EnvironmentSteps(40))\ngraph_manager.visualization_parameters.render = True\n```\n\n----------------------------------------\n\nTITLE: Structuring Interactive Algorithm Selection UI in HTML\nDESCRIPTION: This HTML snippet defines the structure for an interactive questionnaire and algorithm badges. It includes radio buttons for action types, checkboxes for task characteristics, and div elements for displaying algorithm information.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/selecting_an_algorithm.rst#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"bordered-container\">\n   <div class=\"questionnaire\">\n      What are the type of actions your task requires?\n      <div style=\"margin-left: 12px;\">\n         <input type=\"radio\" id=\"discrete\" name=\"actions\" checked>Discrete actions<br>\n         <input type=\"radio\" id=\"continuous\" name=\"actions\">Continuous actions<br>\n      </div>\n      <input type=\"checkbox\" id=\"imitation\" checked=\"True\">Do you have expert demonstrations for your task?<br>\n      <input type=\"checkbox\" id=\"on-policy\" checked=\"True\">Can you collect new data for your task dynamically?<br>\n      <input type=\"checkbox\" id=\"requires-multi-worker\" checked=\"True\">Do you have a simulator for your task?<br>\n   </div>\n\n   <br>\n   <div class=\"badges-wrapper\">\n      <div class=\"algorithm discrete off-policy\" data-year=\"201300\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/dqn.html\">DQN</a>\n            <br>\n            Learns action values for discrete actions, and allows learning from a replay buffer with old experiences\n         </span>\n      </div>\n      <!-- Additional algorithm divs omitted for brevity -->\n   </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Setting up Graph Manager with Custom Exploration\nDESCRIPTION: Creates a graph manager with custom exploration policy and configures checkpoint saving parameters for model persistence.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom exploration import MyExplorationParameters\n\n# Overriding the default DQN Agent exploration policy with my exploration policy\nagent_params.exploration = MyExplorationParameters()\n\n# Creating a graph manager to train a DQN agent to solve CartPole\ngraph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n\n# Resources path was defined at the top of this notebook\nmy_checkpoint_dir = resources_path + '/checkpoints'\n\n# Checkpoints will be stored every 5 seconds to the given directory\ntask_parameters1 = TaskParameters()\ntask_parameters1.checkpoint_save_dir = my_checkpoint_dir\ntask_parameters1.checkpoint_save_secs = 5\n\ngraph_manager.create_graph(task_parameters1)\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Actor Network Training Gradient\nDESCRIPTION: Mathematical formula expressing the policy gradient used for training the actor network, showing the relationship between state-action values and policy parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/wolpertinger.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\nabla_{\\theta^\\mu } J \\approx E_{s_t \\tilde{} \\rho^\\beta } [\\nabla_a Q(s,a)|_{s=s_t,a=\\mu (s_t ) } \\cdot \\nabla_{\\theta^\\mu} \\mu(s)|_{s=s_t} ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Reward Filter in Coach\nDESCRIPTION: Template for creating a custom reward filter class that inherits from RewardFilter. Shows required method implementations including filter, get_filtered_reward_space, and reset.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/filters/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom coach.filters.filter import RewardFilter\n\nclass CustomFilter(RewardFilter):\n  def __init__(self):\n    ...\n  def filter(self, env_response: EnvResponse) -> EnvResponse:\n    ...\n  def get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n    ...\n  def reset(self):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Importing NetworkWrapper Class in Python\nDESCRIPTION: This snippet imports the NetworkWrapper class from the rl_coach.architectures.network_wrapper module. NetworkWrapper likely provides a high-level interface for managing and interacting with neural network architectures in the Coach project.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/architectures/index.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.architectures.network_wrapper.NetworkWrapper\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Calculating Clipped PPO Loss Function in Python\nDESCRIPTION: This snippet defines the loss function for Clipped PPO, which is the minimum between the standard surrogate loss and an epsilon clipped surrogate loss. It uses mathematical notation to represent the likelihood ratio and advantage estimation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/cppo.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nL^{CLIP}(\\theta)=E_{t}[min(r_t(\\theta)\\cdot \\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_t)]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment and Schedule Parameters\nDESCRIPTION: Configures the Mujoco environment parameters, visualization settings, and training schedule parameters for the reinforcement learning setup.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/3. Implementing a Hierarchical RL Graph.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nenv_params = Mujoco()\nenv_params.level = \"rl_coach.environments.mujoco.pendulum_with_goals:PendulumWithGoals\"\nenv_params.additional_simulator_parameters = {\"time_limit\": time_limit,\n                                              \"random_goals_instead_of_standing_goal\": False,\n                                              \"polar_coordinates\": polar_coordinates,\n                                              \"goal_reaching_thresholds\": distance_from_goal_threshold}\nenv_params.frame_skip = 10\nenv_params.custom_reward_threshold = -time_limit + 1\n\nvis_params = VisualizationParameters()\nvis_params.video_dump_methods = [SelectedPhaseOnlyDumpMethod(RunPhase.TEST)]\nvis_params.dump_mp4 = False\nvis_params.native_rendering = False\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = EnvironmentEpisodes(40 * 4 * 64)  # 40 epochs\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(4 * 64)  # 4 small batches of 64 episodes\nschedule_params.evaluation_steps = EnvironmentEpisodes(64)\nschedule_params.heatup_steps = EnvironmentSteps(0)\n```\n\n----------------------------------------\n\nTITLE: Calculating Policy Gradients with Bias Correction in ACER\nDESCRIPTION: This LaTeX formula shows the computation of policy gradients with bias correction in ACER. It includes importance sampling and expectation terms.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/acer.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\hat{g}_t^{policy} & = & \\bar{\\rho}_{t} \\nabla \\log \\pi (a_t \\mid s_t) [Q^{ret}(s_t,a_t) - V(s_t)] \\\\\n                    & & + \\mathbb{E}_{a \\sim \\pi} \\left(\\left[\\frac{\\rho_t(a)-c}{\\rho_t(a)}\\right] \\nabla \\log \\pi (a \\mid s_t) [Q(s_t,a) - V(s_t)] \\right)\n```\n\n----------------------------------------\n\nTITLE: Exploration Policies Class Reference Table\nDESCRIPTION: A table showing the compatibility of different exploration policies with discrete and continuous action spaces. Green checkmarks (V) indicate compatibility while red X's indicate incompatibility.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/exploration_policies/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n+----------------------+-----------------------+------------------+\n| Exploration Policy   | Discrete Action Space | Box Action Space |\n+======================+=======================+==================+\n| AdditiveNoise        | :red:`X`              | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| Boltzmann            | :green:`V`            | :red:`X`         |\n+----------------------+-----------------------+------------------+\n| Bootstrapped         | :green:`V`            | :red:`X`         |\n+----------------------+-----------------------+------------------+\n| Categorical          | :green:`V`            | :red:`X`         |\n+----------------------+-----------------------+------------------+\n| ContinuousEntropy    | :red:`X`              | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| EGreedy              | :green:`V`            | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| Greedy               | :green:`V`            | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| OUProcess            | :red:`X`              | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| ParameterNoise       | :green:`V`            | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| TruncatedNormal      | :red:`X`              | :green:`V`       |\n+----------------------+-----------------------+------------------+\n| UCB                  | :green:`V`            | :red:`X`         |\n+----------------------+-----------------------+------------------+\n```\n\n----------------------------------------\n\nTITLE: Q-Head Gradient Calculation in ACER\nDESCRIPTION: Formula for computing gradients for the Q-function network head using Mean Squared Error (MSE) between the retrace Q-values and predicted Q-values.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/acer.rst#2025-04-23_snippet_3\n\nLANGUAGE: math\nCODE:\n```\n\\hat{g}_t^{Q} = (Q^{ret}(s_t,a_t) - Q(s_t,a_t)) \\nabla Q(s_t,a_t)\\\\\n```\n\n----------------------------------------\n\nTITLE: Implementing Interactive Algorithm Selection in JavaScript\nDESCRIPTION: This JavaScript code enables interactive filtering and sorting of reinforcement learning algorithms based on user selections. It handles checkbox and radio button inputs to dynamically show/hide algorithm badges and sorts them by publication date.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/selecting_an_algorithm.rst#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n$(document).ready(function() {\n   // descending order of the agent badges according to their publish year\n   function order_badges() {\n      $(\".badges-wrapper\").find('.algorithm').sort(function(a, b) {\n         // dataset.year is the concatenated year and month of the paper publishing date\n         return b.dataset.year - a.dataset.year;\n      }).appendTo($(\".badges-wrapper\"));\n   }\n\n   function update_algorithms_list() {\n      // show all the badges\n      $(\"input:checkbox, input:radio\").each(function(){\n         $('.' + this.id).show();\n      });\n\n      // remove all that don't fit the task\n      $(\"input:checkbox\").each(function(){\n         if (!this.checked) {\n            $('.' + this.id).hide();\n         }\n      });\n      $(\"input:radio\").each(function(){\n         if (this.checked) {\n            $('.algorithm').not('.' + this.id).hide();\n         }\n      });\n\n      order_badges();\n   }\n\n   // toggle badges according to the checkbox change\n   $('input:checkbox, input:radio').click(update_algorithms_list);\n\n   update_algorithms_list();\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Coach Agent Base Classes\nDESCRIPTION: References to the core Agent classes in Coach, including AgentParameters base class and the Agent class with its members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.AgentParameters\n\n.. autoclass:: rl_coach.agents.agent.Agent\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Policy Gradient Calculation with Bias Correction in ACER\nDESCRIPTION: Formula for computing policy gradients with bias correction using importance sampling and retrace values, which helps reduce variance while maintaining unbiased estimates.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/acer.rst#2025-04-23_snippet_2\n\nLANGUAGE: math\nCODE:\n```\n\\hat{g}_t^{policy} & = & \\bar{\\rho}_{t} \\nabla \\log \\pi (a_t \\mid s_t) [Q^{ret}(s_t,a_t) - V(s_t)] \\\\\n                    & & + \\mathbb{E}_{a \\sim \\pi} \\left(\\left[\\frac{\\rho_t(a)-c}{\\rho_t(a)}\\right] \\nabla \\log \\pi (a \\mid s_t) [Q(s_t,a) - V(s_t)] \\right)\n```\n\n----------------------------------------\n\nTITLE: Calculating Monte Carlo Targets in Mixed Monte Carlo Algorithm\nDESCRIPTION: This formula computes the Monte Carlo targets by summing up the discounted rewards across the entire episode. It represents the total discounted return from the current state to the end of the episode.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/mmc.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\ny_t^{MC}=\\sum_{j=0}^T\\gamma^j r(s_{t+j},a_{t+j} )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Spaces Definition and Final Graph Manager\nDESCRIPTION: Defines state/action spaces and initializes the final BatchRLGraphManager with all configured parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/4. Batch Reinforcement Learning.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nspaces = SpacesDefinition(state=StateSpace({'observation': VectorObservationSpace(shape=6)}),\n                          goal=None,\n                          action=DiscreteActionSpace(3),\n                          reward=RewardSpace(1))\n\ngraph_manager = BatchRLGraphManager(agent_params=agent_params,\n                                    env_params=None,\n                                    spaces_definition=spaces,\n                                    schedule_params=schedule_params,\n                                    vis_params=VisualizationParameters(dump_signals_to_csv_every_x_episodes=1),\n                                    reward_model_num_epochs=30,\n                                    train_to_eval_ratio=0.4)\ngraph_manager.create_graph(task_parameters)\ngraph_manager.improve()\n```\n\n----------------------------------------\n\nTITLE: Core Types Class Documentation in RST\nDESCRIPTION: ReStructuredText documentation for core RL Coach classes using autodoc directives to generate API documentation from docstrings.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/core_types.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.core_types.ActionInfo\n   :members:\n   :inherited-members:\n\n.. autoclass:: rl_coach.core_types.Batch\n   :members:\n   :inherited-members:\n\n.. autoclass:: rl_coach.core_types.EnvResponse\n   :members:\n   :inherited-members:\n\n.. autoclass:: rl_coach.core_types.Episode\n   :members:\n   :inherited-members:\n\n.. autoclass:: rl_coach.core_types.Transition\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Target Value Calculation in DDQN\nDESCRIPTION: Mathematical formula for calculating the initial target values in Double DQN (DDQN) using rewards and discounted future values.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/pal.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t^{DDQN}=r(s_t,a_t )+\\gamma Q(s_{t+1},argmax_a Q(s_{t+1},a))\n```\n\n----------------------------------------\n\nTITLE: Calculating State Values in ACER Algorithm\nDESCRIPTION: This LaTeX formula represents the calculation of state values in the ACER algorithm. It computes the expected Q-value over all actions according to the policy distribution.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/acer.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\nV(s_t) = \\mathbb{E}_{a \\sim \\pi} [Q(s_t,a)]\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Name-Based Selection\nDESCRIPTION: Example of running tests filtered by name using PyTest's -k flag\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest rl_coach/tests -k Doom\n```\n\n----------------------------------------\n\nTITLE: Action Filter Class Import References\nDESCRIPTION: Python class references from the rl_coach.filters.action module implementing various action filtering mechanisms including attention discretization, box discretization, box masking, and action space mapping.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/filters/output_filters.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.filters.action import AttentionDiscretization\nfrom rl_coach.filters.action import BoxDiscretization\nfrom rl_coach.filters.action import BoxMasking\nfrom rl_coach.filters.action import PartialDiscreteActionSpaceMap\nfrom rl_coach.filters.action import FullDiscreteActionSpaceMap\nfrom rl_coach.filters.action import LinearBoxToBoxMap\n```\n\n----------------------------------------\n\nTITLE: Combining DDQN and Monte Carlo Targets in Mixed Monte Carlo Algorithm\nDESCRIPTION: This snippet shows how the DDQN and Monte Carlo targets are combined using a mixing ratio Î± to get the final targets in the Mixed Monte Carlo algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/mmc.rst#2025-04-23_snippet_2\n\nLANGUAGE: math\nCODE:\n```\ny_t=(1-\\alpha)\\cdot y_t^{DDQN}+\\alpha \\cdot y_t^{MC}\n```\n\n----------------------------------------\n\nTITLE: Importing Coach Entry Point\nDESCRIPTION: The main entry point script that parses command line arguments and invokes sub-processes for experiments.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/design/control_flow.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncoach.py\n```\n\n----------------------------------------\n\nTITLE: Running A3C on Space Invaders with 16 Workers\nDESCRIPTION: Command to run A3C algorithm on the Space Invaders Atari game using 16 workers.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/a3c/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_A3C -lvl space_invaders -n 16\n```\n\n----------------------------------------\n\nTITLE: Importing PPO Algorithm Parameters in Python\nDESCRIPTION: This code snippet imports the PPOAlgorithmParameters class from the rl_coach.agents.ppo_agent module. It is used to configure the hyperparameters and settings for the PPO algorithm implementation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/ppo.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.ppo_agent.PPOAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Goal-Based Data Collection in Python\nDESCRIPTION: This snippet imports necessary modules from RL Coach for setting up the agent, environment, and training parameters. It includes imports for TD3 agent, network architectures, training schedules, and Robosuite environment settings.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.td3_exp_agent import TD3GoalBasedAgentParameters\nfrom rl_coach.architectures.embedder_parameters import InputEmbedderParameters\nfrom rl_coach.architectures.layers import Dense, Conv2d, BatchnormActivationDropout, Flatten\nfrom rl_coach.base_parameters import EmbedderScheme\nfrom rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\nfrom rl_coach.environments.robosuite_environment import RobosuiteGoalBasedExpEnvironmentParameters, \\\n    OptionalObservations\nfrom rl_coach.filters.filter import NoInputFilter, NoOutputFilter\nfrom rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\nfrom rl_coach.graph_managers.graph_manager import ScheduleParameters\nfrom rl_coach.architectures.head_parameters import RNDHeadParameters\nfrom rl_coach.schedules import LinearSchedule\n```\n\n----------------------------------------\n\nTITLE: Automatic Documentation Build Script for Coach\nDESCRIPTION: Command to run the automatic documentation build script from the docs_raw directory. This script installs required packages, generates HTML, and copies documents to the coach/docs/ directory.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./build_docs.sh\n```\n\n----------------------------------------\n\nTITLE: Importing Non-Episodic Memory Classes in RL Coach\nDESCRIPTION: This snippet demonstrates the import statements for non-episodic memory classes in RL Coach. These classes handle various types of experience replay and transition collections.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/memories/index.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom rl_coach.memories.non_episodic import BalancedExperienceReplay\nfrom rl_coach.memories.non_episodic import QDND\nfrom rl_coach.memories.non_episodic import ExperienceReplay\nfrom rl_coach.memories.non_episodic import PrioritizedExperienceReplay\nfrom rl_coach.memories.non_episodic import TransitionCollection\n```\n\n----------------------------------------\n\nTITLE: Policy Gradient Loss Function Formula\nDESCRIPTION: Mathematical formula defining the policy head loss function used in training. The loss uses a PolicyGradientRescaler to reduce variance in gradient updates.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/pg.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nL=-log (\\pi) \\cdot PolicyGradientRescaler\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formula for N-step Return Calculation\nDESCRIPTION: LaTeX formula representing the N-step return calculation used in the NEC algorithm. It shows how the return is computed using consecutive steps and bootstrapped values.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/nec.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\ny_t=\\sum_{j=0}^{N-1}\\gamma^j r(s_{t+j},a_{t+j} ) +\\gamma^N   max_a Q(s_{t+N},a)\n```\n\n----------------------------------------\n\nTITLE: Running Humanoid with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Humanoid environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl humanoid\n```\n\n----------------------------------------\n\nTITLE: Applying Trust Region Update in ACER Algorithm\nDESCRIPTION: These LaTeX formulas show the trust region update calculation in ACER. It modifies the policy loss gradient to ensure stability by limiting the difference between the updated policy and the average policy.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/acer.rst.txt#2025-04-23_snippet_4\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\hat{g}_t^{trust-region} = \\hat{g}_t^{policy} - \\max \\left\\{0, \\frac{k^T \\hat{g}_t^{policy} - \\delta}{\\lVert k \\rVert_2^2}\\right\\} k\n```\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\text{where} \\quad k = \\nabla D_{KL}[\\pi_{avg} \\parallel \\pi]\n```\n\n----------------------------------------\n\nTITLE: Calculating Actor Gradient in Wolpertinger Algorithm (Python)\nDESCRIPTION: This snippet presents the equation for calculating the actor gradient in the Wolpertinger algorithm. It uses the policy gradient theorem to update the actor network based on the critic's evaluation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/wolpertinger.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nâˆ‡_Î¸Î¼ J â‰ˆ E_{s_t ~ ÏÎ²} [âˆ‡_a Q(s,a)|_{s=s_t,a=Î¼(s_t)} Â· âˆ‡_Î¸Î¼ Î¼(s)|_{s=s_t}]\n```\n\n----------------------------------------\n\nTITLE: Running Dueling DDQN with PER on Breakout\nDESCRIPTION: Command to train a Dueling DDQN agent with Prioritized Experience Replay on the Atari Breakout game using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dueling_ddqn_with_per/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Dueling_DDQN_with_PER_OpenAI -lvl breakout\n```\n\n----------------------------------------\n\nTITLE: Importing Control Suite Environment\nDESCRIPTION: Auto-documented class for DeepMind Control Suite environments powered by MuJoCo physics engine.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/environments/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.control_suite_environment.ControlSuiteEnvironment\n```\n\n----------------------------------------\n\nTITLE: Estimating State Value in PAL Algorithm\nDESCRIPTION: This formula estimates the value of a state by taking the maximum Q-value across all actions in that state. It's used in calculating the action gap for PAL.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/pal.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nV(s_t )=max_a Q(s_t,a)\n```\n\n----------------------------------------\n\nTITLE: Running ACER on Breakout with 16 workers\nDESCRIPTION: This command runs the ACER algorithm on the Atari game Breakout using 16 workers. It uses the Atari_ACER preset in the Coach framework.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/acer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_ACER -lvl breakout -n 16\n```\n\n----------------------------------------\n\nTITLE: Running Ant with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Ant environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl ant\n```\n\n----------------------------------------\n\nTITLE: Running Hopper with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Hopper environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl hopper\n```\n\n----------------------------------------\n\nTITLE: Running Inverted Pendulum with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Inverted Pendulum environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl inverted_pendulum\n```\n\n----------------------------------------\n\nTITLE: Running Dueling DDQN on Space Invaders\nDESCRIPTION: Command to execute Dueling DDQN training on the Atari Space Invaders game using a single worker configuration.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dueling_ddqn/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Dueling_DDQN -lvl space_invaders\n```\n\n----------------------------------------\n\nTITLE: Initializing Training Schedule Parameters\nDESCRIPTION: Sets up the training schedule parameters including improvement steps, evaluation periods, and heatup steps for the DQN agent training process.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/0. Quick Start Guide.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntf.reset_default_graph()\n\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(4000)\nschedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\nschedule_params.evaluation_steps = EnvironmentEpisodes(1)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n```\n\n----------------------------------------\n\nTITLE: Running Walker 2D with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Walker 2D environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl walker2d\n```\n\n----------------------------------------\n\nTITLE: Running A3C on Inverted Pendulum with Multiple Workers\nDESCRIPTION: Commands to run A3C algorithm on the Inverted Pendulum environment with 1, 2, 4, 8, and 16 workers using the Mujoco simulator.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/a3c/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_A3C -lvl inverted_pendulum -n 1\ncoach -p Mujoco_A3C -lvl inverted_pendulum -n 2\ncoach -p Mujoco_A3C -lvl inverted_pendulum -n 4\ncoach -p Mujoco_A3C -lvl inverted_pendulum -n 8\ncoach -p Mujoco_A3C -lvl inverted_pendulum -n 16\n```\n\n----------------------------------------\n\nTITLE: Running Single-threaded Algorithm in Coach (Python)\nDESCRIPTION: This command demonstrates how to run a single-threaded algorithm in Coach using the CartPole environment with DQN. The -p flag is used to specify the preset.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_DQN\n```\n\n----------------------------------------\n\nTITLE: Action Gap Calculation\nDESCRIPTION: Formula for calculating the action gap as the difference between the maximum Q-value (V) and the Q-value for the current state-action pair.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/pal.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nV(s_t )=max_a Q(s_t,a)\n```\n\n----------------------------------------\n\nTITLE: Calculating DDPG Actor Network Gradient in Python and LaTeX\nDESCRIPTION: This snippet shows the calculation of the gradient for training the actor network in DDPG. It involves the expected gradient of the Q-value with respect to actions, multiplied by the gradient of the actor's policy.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/ddpg.rst#2025-04-23_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\nabla_{\\theta^\\mu } J \\approx E_{s_t \\tilde{} \\rho^\\beta } [\\nabla_a Q(s,a)|_{s=s_t,a=\\mu (s_t ) } \\cdot \\nabla_{\\theta^\\mu} \\mu(s)|_{s=s_t} ]\n```\n\n----------------------------------------\n\nTITLE: Running TD3 on Reacher Mujoco Environment\nDESCRIPTION: Command for training a TD3 agent on the Reacher environment using a single worker. Uses the parameters specified in the original TD3 paper and runs for 1M environment steps with 5 seeds.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/td3/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_TD3 -lvl reacher\n```\n\n----------------------------------------\n\nTITLE: Running TD3 on Walker2D Mujoco Environment\nDESCRIPTION: Command for training a TD3 agent on the Walker2D environment using a single worker. Uses the parameters specified in the original TD3 paper and runs for 1M environment steps with 5 seeds.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/td3/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_TD3 -lvl walker2d\n```\n\n----------------------------------------\n\nTITLE: Documenting VisualizationParameters Class in Python for RL Coach\nDESCRIPTION: Auto-generated documentation for the VisualizationParameters class from the rl_coach.base_parameters module.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/additional_parameters.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.VisualizationParameters\n```\n\n----------------------------------------\n\nTITLE: Advantage Learning Target Calculation\nDESCRIPTION: Formula for calculating the target values in Advantage Learning (AL) by subtracting the weighted action gap from DDQN targets.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/pal.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: math\nCODE:\n```\ny_t=y_t^{DDQN}-\\alpha \\cdot (V(s_t )-Q(s_t,a_t ))\n```\n\n----------------------------------------\n\nTITLE: Running SAC on Inverted Pendulum Environment\nDESCRIPTION: Command to execute SAC algorithm on the Mujoco Inverted Pendulum environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/sac/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_SAC -lvl inverted_pendulum\n```\n\n----------------------------------------\n\nTITLE: Running TD3 on Ant Mujoco Environment\nDESCRIPTION: Command for training a TD3 agent on the Ant environment using a single worker. Uses the parameters specified in the original TD3 paper and runs for 1M environment steps with 5 seeds.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/td3/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_TD3 -lvl ant\n```\n\n----------------------------------------\n\nTITLE: Running Distributed RL Training with Coach\nDESCRIPTION: Example of running distributed training with Clipped PPO on CartPole. Uses the -dc flag to enable distributed mode and -dcp to specify configuration parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncoach -p CartPole_ClippedPPO -dc -dcp <path-to-config-file> -n 8\n```\n\n----------------------------------------\n\nTITLE: Mathematical Transition Definition\nDESCRIPTION: Mathematical representation of a single transition step containing state, action, reward and next state information.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/design/control_flow.rst#2025-04-23_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n(s_{t}, a_{t}, r_{t}, s_{t+1}, \\textrm{terminal signal})\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Hopper\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo hopper environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl hopper\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Ant\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo ant environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl ant\n```\n\n----------------------------------------\n\nTITLE: Running TD3 on Hopper Mujoco Environment\nDESCRIPTION: Command for training a TD3 agent on the Hopper environment using a single worker. Uses the parameters specified in the original TD3 paper and runs for 1M environment steps with 5 seeds.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/td3/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_TD3 -lvl hopper\n```\n\n----------------------------------------\n\nTITLE: Running ACER on Pong with 16 workers\nDESCRIPTION: This command initiates the ACER algorithm on the Atari game Pong using 16 workers. It employs the Atari_ACER preset in the Coach framework.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/acer/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_ACER -lvl pong -n 16\n```\n\n----------------------------------------\n\nTITLE: Dumping GIFs in Coach (Python)\nDESCRIPTION: This command demonstrates how to dump GIF files of the agent's gameplay in Coach. It uses the Atari environment with A3C and the -dg flag to enable GIF dumping.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncoach -p Atari_A3C -lvl breakout -n 4 -dg\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Observation Filter in Coach\nDESCRIPTION: Template for creating a custom observation filter class that inherits from ObservationFilter. Shows required method implementations including filter, get_filtered_observation_space, validate_input_observation_space, and reset.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/filters/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom coach.filters.filter import ObservationFilter\n\nclass CustomFilter(ObservationFilter):\n  def __init__(self):\n    ...\n  def filter(self, env_response: EnvResponse) -> EnvResponse:\n    ...\n  def get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n    ...\n  def validate_input_observation_space(self, input_observation_space: ObservationSpace):\n    ...\n  def reset(self):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Importing Reward Filters in RL Coach\nDESCRIPTION: This snippet demonstrates how to import various reward filters from the RL Coach library. These filters are used to modify and normalize reward signals in reinforcement learning algorithms.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/filters/input_filters.rst#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom rl_coach.filters.reward import RewardClippingFilter, RewardNormalizationFilter, RewardRescaleFilter\n```\n\n----------------------------------------\n\nTITLE: Running Doom Basic DFP with 8 Workers\nDESCRIPTION: Command to run the DFP algorithm on the basic Doom environment using 8 parallel workers.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dfp/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Doom_Basic_DFP -n 8\n```\n\n----------------------------------------\n\nTITLE: Running Doom Health Supreme DFP with 8 Workers\nDESCRIPTION: Command to run the DFP algorithm on the Doom Health Supreme (D2: Navigation) environment using 8 parallel workers.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dfp/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Doom_Health_Supreme_DFP -n 8\n```\n\n----------------------------------------\n\nTITLE: Bellman Update for Atom Formula\nDESCRIPTION: Mathematical formula defining the Bellman update calculation for a specific atom z_j, incorporating rewards and discount factors across multiple timesteps.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/rainbow.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\hat{T}_{z_{j}} := r_t+\\gamma r_{t+1} + ... + \\gamma r_{t+n-1} + \\gamma^{n-1} z_j\n```\n\n----------------------------------------\n\nTITLE: Running A3C on Half Cheetah with 16 Workers\nDESCRIPTION: Command to run A3C algorithm on the Half Cheetah environment using 16 workers in the Mujoco simulator.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/a3c/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_A3C -lvl half_cheetah -n 16\n```\n\n----------------------------------------\n\nTITLE: Running DQN for Pong in Atari Environment\nDESCRIPTION: This command runs the DQN algorithm on the Pong game using the Atari_DQN preset in the Coach framework. It utilizes a single worker for the training process.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dqn/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_DQN -lvl pong\n```\n\n----------------------------------------\n\nTITLE: Autoclass Reference for CIL Algorithm Parameters in Python\nDESCRIPTION: Documentation reference for the CILAlgorithmParameters class from the rl_coach.agents.cil_agent module. This class defines the parameters used to configure the Conditional Imitation Learning algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/imitation/cil.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.cil_agent.CILAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Importing PPO Algorithm Parameters in Python\nDESCRIPTION: This code snippet imports the PPOAlgorithmParameters class from the rl_coach.agents.ppo_agent module. This class likely contains the hyperparameters and configuration options for the PPO algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/ppo.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.ppo_agent.PPOAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Running Fetch Slide DDPG HER - 8 Workers\nDESCRIPTION: Command to execute DDPG HER on the Fetch Slide environment using 8 parallel workers for improved training efficiency.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg_her/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Fetch_DDPG_HER_baselines -lvl slide -n 8\n```\n\n----------------------------------------\n\nTITLE: Calculating DDQN Targets in Mixed Monte Carlo Algorithm\nDESCRIPTION: This snippet shows the mathematical formula for calculating the Double DQN (DDQN) targets in the Mixed Monte Carlo algorithm. It uses the current reward and the Q-value of the next state-action pair.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/mmc.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t^{DDQN}=r(s_t,a_t )+\\gamma Q(s_{t+1},argmax_a Q(s_{t+1},a))\n```\n\n----------------------------------------\n\nTITLE: Action Space Class Documentation\nDESCRIPTION: Documentation for the ActionSpace base class and its specialized variants including attention, box, discrete, multi-select and compound action spaces.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/spaces.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.spaces.ActionSpace\n   :members:\n   :inherited-members:\n\n.. autoclass:: rl_coach.spaces.AttentionActionSpace\n\n.. autoclass:: rl_coach.spaces.BoxActionSpace\n\n.. autoclass:: rl_coach.spaces.DiscreteActionSpace\n\n.. autoclass:: rl_coach.spaces.MultiSelectActionSpace\n\n.. autoclass:: rl_coach.spaces.CompoundActionSpace\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Walker 2D\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo walker2d environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl walker2d\n```\n\n----------------------------------------\n\nTITLE: Running Bootstrapped DQN for Pong in Atari environment\nDESCRIPTION: This command runs a Bootstrapped DQN experiment for the Pong game using the Coach framework. It utilizes the Atari_Bootstrapped_DQN preset with a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/bootstrapped_dqn/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Bootstrapped_DQN -lvl pong\n```\n\n----------------------------------------\n\nTITLE: NAF Value Function Reference\nDESCRIPTION: Mathematical notation for referring to the value function output from the network's head.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/naf.rst#2025-04-23_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\nV(s_{t+1})\n```\n\n----------------------------------------\n\nTITLE: Referencing PolicyGradientRescaler Configuration Options\nDESCRIPTION: A code reference to the PolicyGradientRescaler parameter which is used to configure variance reduction strategies for the policy gradient algorithm. This is part of the reStructuredText documentation format.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/pg.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:code:`PolicyGradientRescaler`\n```\n\n----------------------------------------\n\nTITLE: N-Step Q Learning Algorithm Class Import\nDESCRIPTION: Python class import statement for the N-Step Q Learning algorithm parameters class from the RL Coach framework.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/n_step.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.n_step_q_agent import NStepQAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Running A3C on Ant with 16 Workers\nDESCRIPTION: Command to run A3C algorithm on the Ant environment using 16 workers in the Mujoco simulator.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/a3c/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_A3C -lvl ant -n 16\n```\n\n----------------------------------------\n\nTITLE: Critic Training Target Formula\nDESCRIPTION: Mathematical formula defining the target values used for training the critic network, incorporating rewards and discounted future value estimates.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/wolpertinger.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma \\cdot Q(s_{t+1},\\mu(s_{t+1} ))\n```\n\n----------------------------------------\n\nTITLE: Defining Space Class Reference\nDESCRIPTION: Base class documentation for the fundamental Space class in RL Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/spaces.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.spaces.Space\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Running Bootstrapped DQN for Space Invaders in Atari environment\nDESCRIPTION: This command initiates a Bootstrapped DQN experiment for the Space Invaders game using the Coach framework. It employs the Atari_Bootstrapped_DQN preset with a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/bootstrapped_dqn/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Bootstrapped_DQN -lvl space_invaders\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Mujoco Environment\nDESCRIPTION: Dockerfile contents for creating a Coach image with Mujoco environment support.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nFROM coach-base:master as builder\n\n# prep mujoco and any of its related requirements.\n# Mujoco\nRUN mkdir -p ~/.mujoco \\\n    && wget https://www.roboti.us/download/mjpro150_linux.zip -O mujoco.zip \\\n    && unzip -n mujoco.zip -d ~/.mujoco \\\n    && rm mujoco.zip\nARG MUJOCO_KEY\nENV MUJOCO_KEY=$MUJOCO_KEY\nENV LD_LIBRARY_PATH /root/.mujoco/mjpro150/bin:$LD_LIBRARY_PATH\nRUN echo $MUJOCO_KEY | base64 --decode > /root/.mujoco/mjkey.txt\nRUN pip3 install mujoco_py\n\n# add coach source starting with files that could trigger\n# re-build if dependencies change.\nRUN mkdir /root/src\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nRUN pip3 install -r /root/src/requirements.txt\n\nFROM coach-base:master\nWORKDIR /root/src\nCOPY --from=builder /root/.mujoco /root/.mujoco\nENV LD_LIBRARY_PATH /root/.mujoco/mjpro150/bin:$LD_LIBRARY_PATH\nCOPY --from=builder /root/.cache /root/.cache\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nCOPY README.md /root/src/.\nRUN pip3 install mujoco_py && pip3 install -e .[all] && rm -rf /root/.cache\nCOPY . /root/src\n```\n\n----------------------------------------\n\nTITLE: Running QR-DQN on Atari Breakout\nDESCRIPTION: Command to execute QR-DQN algorithm on the Breakout Atari game using a single worker configuration\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/qr_dqn/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_QR_DQN -lvl breakout\n```\n\n----------------------------------------\n\nTITLE: Importing CIL Algorithm Parameters in Python\nDESCRIPTION: This code snippet shows how to import the CILAlgorithmParameters class from the rl_coach.agents.cil_agent module. This class likely contains the necessary parameters for configuring the Conditional Imitation Learning algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/imitation/cil.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.agents.cil_agent import CILAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Running Dueling DDQN with PER on Pong\nDESCRIPTION: Command to train a Dueling DDQN agent with Prioritized Experience Replay on the Atari Pong game using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dueling_ddqn_with_per/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Dueling_DDQN_with_PER_OpenAI -lvl pong\n```\n\n----------------------------------------\n\nTITLE: Calculating State Value Network Target in Soft Actor-Critic (Python/LaTeX)\nDESCRIPTION: Formula for computing the target values used to train the State Value network in the Soft Actor-Critic algorithm. It uses the minimum Q-value and the log probability of the sampled action.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/sac.rst#2025-04-23_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\ny_t^V = \\min_{i=1,2}Q_i(s_t,\\tilde{a}) - log\\pi (\\tilde{a} \\vert s),\\,\\,\\,\\, \\tilde{a} \\sim \\pi(\\cdot \\vert s_t)\n```\n\n----------------------------------------\n\nTITLE: Running Dueling DDQN with PER on Space Invaders\nDESCRIPTION: Command to train a Dueling DDQN agent with Prioritized Experience Replay on the Atari Space Invaders game using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dueling_ddqn_with_per/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Dueling_DDQN_with_PER_OpenAI -lvl space_invaders\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for ViZDoom Environment\nDESCRIPTION: Dockerfile contents for creating a Coach image with ViZDoom environment support.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nFROM coach-base:master as builder\n\n# prep vizdoom and any of its related requirements.\nRUN pip3 install vizdoom\n\n# add coach source starting with files that could trigger\n# re-build if dependencies change.\nRUN mkdir /root/src\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nRUN pip3 install -r /root/src/requirements.txt\n\nFROM coach-base:master\nWORKDIR /root/src\nCOPY --from=builder /root/.cache /root/.cache\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nCOPY README.md /root/src/.\nRUN pip3 install vizdoom && pip3 install -e .[all] && rm -rf /root/.cache\nCOPY . /root/src\n```\n\n----------------------------------------\n\nTITLE: Running DQN for Breakout in Atari Environment\nDESCRIPTION: This command executes the DQN algorithm on the Breakout game using the Atari_DQN preset in the Coach framework. It uses a single worker for training.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dqn/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_DQN -lvl breakout\n```\n\n----------------------------------------\n\nTITLE: Importing Mixed Monte Carlo Algorithm Parameters in Python\nDESCRIPTION: This snippet demonstrates how to import the MixedMonteCarloAlgorithmParameters class from the rl_coach.agents.mmc_agent module. This class likely contains the configuration parameters for the Mixed Monte Carlo algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/mmc.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.mmc_agent.MixedMonteCarloAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Importing Environment Base Class\nDESCRIPTION: Auto-documented base Environment class that defines the core environment interface.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/environments/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.environment.Environment\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: TD3 Critic Network Training Target\nDESCRIPTION: Mathematical formula for calculating the target values used to train the twin critic networks, incorporating clipped noise and minimum value between critics.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/td3.rst#2025-04-23_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma \\cdot \\min_{i=1,2} Q_{i}(s_{t+1},\\mu(s_{t+1} )+[\\mathcal{N}(0,\\,\\sigma^{2})]^{MAX\\_NOISE}_{MIN\\_NOISE})\n```\n\n----------------------------------------\n\nTITLE: Calculating Critic Target in Wolpertinger Algorithm (Python)\nDESCRIPTION: This snippet shows the equation for calculating the critic target (y_t) in the Wolpertinger algorithm. It combines the immediate reward with the discounted future Q-value estimated by the target networks.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/wolpertinger.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny_t = r(s_t, a_t) + Î³ Â· Q(s_{t+1}, Î¼(s_{t+1}))\n```\n\n----------------------------------------\n\nTITLE: Running DQN for Space Invaders in Atari Environment\nDESCRIPTION: This command executes the DQN algorithm on the Space Invaders game using the Atari_DQN preset in the Coach framework. It employs a single worker for training.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dqn/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_DQN -lvl space_invaders\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Reacher\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo reacher environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl reacher\n```\n\n----------------------------------------\n\nTITLE: Setting Mujoco Key Environment Variable\nDESCRIPTION: Command to set the Mujoco key as an environment variable for building the Mujoco environment container.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ export MUJOCO_KEY=<mujoco_key>\n```\n\n----------------------------------------\n\nTITLE: Running SAC on Humanoid Environment\nDESCRIPTION: Command to execute SAC algorithm on the Mujoco Humanoid environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/sac/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_SAC -lvl humanoid\n```\n\n----------------------------------------\n\nTITLE: Starcraft II Environment Reference\nDESCRIPTION: Auto-generated documentation for the Starcraft II environment wrapper.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/environments/index.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.starcraft2_environment.StarCraft2Environment\n```\n\n----------------------------------------\n\nTITLE: TD3 Actor Network Training Gradient\nDESCRIPTION: Mathematical formula showing the policy gradient used to train the actor network, using the first critic's gradients with respect to actions.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/td3.rst#2025-04-23_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\n\\nabla_{\\theta^\\mu } J \\approx E_{s_t \\tilde{} \\rho^\\beta } [\\nabla_a Q_{1}(s,a)|_{s=s_t,a=\\mu (s_t ) } \\cdot \\nabla_{\\theta^\\mu} \\mu(s)|_{s=s_t} ]\n```\n\n----------------------------------------\n\nTITLE: Running QR-DQN on Atari Pong\nDESCRIPTION: Command to execute QR-DQN algorithm on the Pong Atari game using a single worker configuration\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/qr_dqn/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_QR_DQN -lvl pong\n```\n\n----------------------------------------\n\nTITLE: Running Dueling DDQN on Breakout\nDESCRIPTION: Command to execute Dueling DDQN training on the Atari Breakout game using a single worker configuration.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dueling_ddqn/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Dueling_DDQN -lvl breakout\n```\n\n----------------------------------------\n\nTITLE: Running A3C on Hopper with 16 Workers\nDESCRIPTION: Command to run A3C algorithm on the Hopper environment using 16 workers in the Mujoco simulator.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/a3c/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_A3C -lvl hopper -n 16\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Directory\nDESCRIPTION: Command to create a directory for Docker-related files.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir docker\n```\n\n----------------------------------------\n\nTITLE: Running SAC on Half Cheetah Environment\nDESCRIPTION: Command to execute SAC algorithm on the Mujoco Half Cheetah environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/sac/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_SAC -lvl half_cheetah\n```\n\n----------------------------------------\n\nTITLE: Running SAC on Hopper Environment\nDESCRIPTION: Command to execute SAC algorithm on the Mujoco Hopper environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/sac/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_SAC -lvl hopper\n```\n\n----------------------------------------\n\nTITLE: NFSDataStore Class Documentation\nDESCRIPTION: RestructuredText documentation reference for the NFSDataStore class used for Network File System storage integration in RL Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/data_stores/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: rl_coach.data_stores.nfs_data_store.NFSDataStore\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Base Coach Image\nDESCRIPTION: Dockerfile content for creating the base Coach Docker image with necessary dependencies.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nFROM nvidia/cuda:9.0-cudnn7-runtime-ubuntu16.04\n\n################################\n# Install apt-get Requirements #\n################################\n\n# General\nRUN apt-get update && \\\n    apt-get install -y python3-pip cmake zlib1g-dev python3-tk python-opencv \\\n    # Boost libraries\n    libboost-all-dev \\\n    # Scipy requirements\n    libblas-dev liblapack-dev libatlas-base-dev gfortran \\\n    # Pygame requirements\n    libsdl-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev \\\n    libsmpeg-dev libportmidi-dev libavformat-dev libswscale-dev \\\n    # Dashboard\n    dpkg-dev build-essential python3.5-dev libjpeg-dev  libtiff-dev libsdl1.2-dev libnotify-dev \\\n    freeglut3 freeglut3-dev libsm-dev libgtk2.0-dev libgtk-3-dev libwebkitgtk-dev libgtk-3-dev \\\n    libwebkitgtk-3.0-dev libgstreamer-plugins-base1.0-dev \\\n    # Gym\n    libav-tools libsdl2-dev swig cmake \\\n    # Mujoco_py\n    curl libgl1-mesa-dev libgl1-mesa-glx libglew-dev libosmesa6-dev software-properties-common \\\n    # ViZDoom\n    build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n    nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n    libopenal-dev timidity libwildmidi-dev unzip wget && \\\n    apt-get clean autoclean && \\\n    apt-get autoremove -y\n\n############################\n# Install Pip Requirements #\n############################\nRUN pip3 install --upgrade pip\nRUN pip3 install setuptools==39.1.0 && pip3 install pytest && pip3 install pytest-xdist\n\nRUN curl -o /usr/local/bin/patchelf https://s3-us-west-2.amazonaws.com/openai-sci-artifacts/manual-builds/patchelf_0.9_amd64.elf \\\n    && chmod +x /usr/local/bin/patchelf\n```\n\n----------------------------------------\n\nTITLE: Bellman Update Projection Formula\nDESCRIPTION: Mathematical formula for projecting the Bellman update to the set of atoms representing Q-value distribution. The formula calculates the i-th component of the projected update using a complex distribution calculation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/rainbow.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\n(\\Phi \\hat{T} Z_{\\theta}(s_t,a_t))_i=\\sum_{j=0}^{N-1}\\Big[1-\\frac{\\lvert[\\hat{T}_{z_{j}}]^{V_{MAX}}_{V_{MIN}}-z_i\\rvert}{\\Delta z}\\Big]^1_0 \\ p_j(s_{t+1}, \\pi(s_{t+1}))\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Gym Environment\nDESCRIPTION: Dockerfile contents for creating a Coach image with Gym environment support.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nFROM coach-base:master as builder\n\n# prep gym and any of its related requirements.\nRUN pip3 install gym[atari,box2d,classic_control]==0.10.5\n\n# add coach source starting with files that could trigger\n# re-build if dependencies change.\nRUN mkdir /root/src\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nRUN pip3 install -r /root/src/requirements.txt\n\nFROM coach-base:master\nWORKDIR /root/src\nCOPY --from=builder /root/.cache /root/.cache\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nCOPY README.md /root/src/.\nRUN pip3 install gym[atari,box2d,classic_control]==0.10.5 && pip3 install -e .[all] && rm -rf /root/.cache\nCOPY . /root/src\n```\n\n----------------------------------------\n\nTITLE: Importing ACER Algorithm Parameters in Python\nDESCRIPTION: This Python code snippet imports the ACERAlgorithmParameters class from the RL Coach framework, which likely contains the implementation details and hyperparameters for the ACER algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/acer.rst.txt#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom rl_coach.agents.acer_agent import ACERAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Defining RainbowAgentParameters Class in Python for Coach\nDESCRIPTION: This code snippet demonstrates how to define a parameters class for a new agent (RainbowAgent) in Coach. It inherits from AgentParameters and initializes with algorithm, exploration, memory, and network parameters. It also defines a path property that returns the location of the agent class.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/contributing/add_agent.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass RainbowAgentParameters(AgentParameters):\ndef __init__(self):\n    super().__init__(algorithm=RainbowAlgorithmParameters(),\n                     exploration=RainbowExplorationParameters(),\n                     memory=RainbowMemoryParameters(),\n                     networks={\"main\": RainbowNetworkParameters()})\n\n@property\ndef path(self):\n    return 'rainbow.rainbow_agent:RainbowAgent'\n```\n\n----------------------------------------\n\nTITLE: Bootstrapped DQN Target Calculation Formula\nDESCRIPTION: Mathematical formula showing how the Bootstrapped DQN calculates the target Q-values for training. This formula represents the standard DQN update rule used for heads that should learn from a given transition.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/bs_dqn.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma\\cdot max_a Q(s_{t+1},a)\n```\n\n----------------------------------------\n\nTITLE: Running Inverted Double Pendulum with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Inverted Double Pendulum environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl inverted_double_pendulum\n```\n\n----------------------------------------\n\nTITLE: Creating Coach Configuration File\nDESCRIPTION: Sample content for the Coach configuration file specifying Docker image, memory backend, and data store settings.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n[coach]\nimage = <user-name>/<image-name>:<tag>\nmemory_backend = redispubsub\ndata_store = s3\ns3_end_point = s3.amazonaws.com\ns3_bucket_name = <bucket-name>\ns3_creds_file = <path-to-aws-credentials>\n```\n\n----------------------------------------\n\nTITLE: Defining Bellman Update for Atom in Rainbow Algorithm using LaTeX\nDESCRIPTION: This snippet defines the Bellman update for atom z_j in the Rainbow algorithm. It uses LaTeX notation to express the mathematical formula.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/rainbow.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\hat{T}_{z_{j}} := r_t+\\gamma r_{t+1} + ... + \\gamma r_{t+n-1} + \\gamma^{n-1} z_j\n```\n\n----------------------------------------\n\nTITLE: Implementing Environment Parameters Class for Doom\nDESCRIPTION: Example implementation of an EnvironmentParameters subclass for the Doom environment, showing parameter initialization and path specification.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/contributing/add_env.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DoomEnvironmentParameters(EnvironmentParameters):\n    def __init__(self):\n        super().__init__()\n        self.default_input_filter = DoomInputFilter\n        self.default_output_filter = DoomOutputFilter\n        self.cameras = [DoomEnvironment.CameraTypes.OBSERVATION]\n\n    @property\n    def path(self):\n        return 'rl_coach.environments.doom_environment:DoomEnvironment'\n```\n\n----------------------------------------\n\nTITLE: Running Bootstrapped DQN for Breakout in Atari environment\nDESCRIPTION: This command executes a Bootstrapped DQN experiment for the Breakout game using the Coach framework. It uses the Atari_Bootstrapped_DQN preset with a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/bootstrapped_dqn/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_Bootstrapped_DQN -lvl breakout\n```\n\n----------------------------------------\n\nTITLE: DQN Q-Value Update Rule\nDESCRIPTION: Mathematical formula for updating Q-values in the Bootstrapped DQN algorithm. The formula calculates the target value using the reward and discounted future Q-value.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/bs_dqn.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t=r(s_t,a_t )+\\gamma\\cdot max_a Q(s_{t+1},a)\n```\n\n----------------------------------------\n\nTITLE: Running Swimmer with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Swimmer environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl swimmer\n```\n\n----------------------------------------\n\nTITLE: Creating Coach Configuration File\nDESCRIPTION: Sample configuration file contents for Distributed Coach setup.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n[coach]\nimage = <user-name>/<image-name>:<tag>\nmemory_backend = redispubsub\ndata_store = s3\ns3_end_point = s3.amazonaws.com\ns3_bucket_name = <bucket-name>\ns3_creds_file = <path-to-aws-credentials>\n```\n\n----------------------------------------\n\nTITLE: Manual Build Process for Coach Documentation\nDESCRIPTION: Series of commands to manually build the documentation website and deploy it to the docs directory. Uses make to generate HTML, copies custom CSS, and replaces the existing docs directory with the new build.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake html\ncp source/_static/css/custom.css build/html/_static/css/\nrm -rf ../docs/\nmkdir ../docs\ntouch ../docs/.nojekyll\ncp -R build/html/* ../docs/\n```\n\n----------------------------------------\n\nTITLE: Calculating Monte Carlo Targets in Mixed Monte Carlo Algorithm\nDESCRIPTION: This snippet presents the formula for computing Monte Carlo targets in the Mixed Monte Carlo algorithm. It sums up the discounted rewards across the entire episode.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/mmc.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\ny_t^{MC}=\\sum_{j=0}^T\\gamma^j r(s_{t+j},a_{t+j} )\n```\n\n----------------------------------------\n\nTITLE: Defining Training Schedule for Goal-Based Data Collection in Python\nDESCRIPTION: This code sets up the training schedule parameters for the data collection process. It specifies the number of improvement steps, evaluation frequency, and heatup steps for the agent.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nschedule_params = ScheduleParameters()\nschedule_params.improve_steps = TrainingSteps(300000)\nschedule_params.steps_between_evaluation_periods = TrainingSteps(300000)\nschedule_params.evaluation_steps = EnvironmentEpisodes(0)\nschedule_params.heatup_steps = EnvironmentSteps(1000)\n```\n\n----------------------------------------\n\nTITLE: Running Reacher with Clipped PPO\nDESCRIPTION: Command to train a single worker Clipped PPO agent on the MuJoCo Reacher environment using Coach framework\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/clipped_ppo/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_ClippedPPO -lvl reacher\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset for Imitation Learning in Doom\nDESCRIPTION: Command for playing a Doom environment to generate a dataset for imitation learning. The gameplay will be recorded and saved as a replay buffer file.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncoach -et rl_coach.environments.doom_environment:DoomEnvironmentParameters -lvl Basic --play\n```\n\n----------------------------------------\n\nTITLE: Dueling DQN Math Notation\nDESCRIPTION: Mathematical notation showing the separation of Q values into advantage (A) and value (V) components\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/dueling_dqn.rst#2025-04-23_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n:math:`Q`, :math:`A`, :math:`V`, :math:`Q`\n```\n\n----------------------------------------\n\nTITLE: Running A3C on Walker2D with 16 Workers\nDESCRIPTION: Command to run A3C algorithm on the Walker2D environment using 16 workers in the Mujoco simulator.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/a3c/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_A3C -lvl walker2d -n 16\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Verbose Output\nDESCRIPTION: Commands to run tests with verbose progress tracking using -v flag\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest rl_coach/tests -v -m golden_test\nOR\npython3 -m pytest rl_coach/tests -v -k Doom\n```\n\n----------------------------------------\n\nTITLE: Mixing DDQN and Monte Carlo Targets in Mixed Monte Carlo Algorithm\nDESCRIPTION: This formula combines the DDQN and Monte Carlo targets using a mixing ratio Î±. It allows for a balance between the two approaches, potentially leveraging the strengths of both methods.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/mmc.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\ny_t=(1-\\alpha)\\cdot y_t^{DDQN}+\\alpha \\cdot y_t^{MC}\n```\n\n----------------------------------------\n\nTITLE: Running Fetch Push DDPG HER - 8 Workers\nDESCRIPTION: Command to execute DDPG HER on the Fetch Push environment using 8 parallel workers for improved training efficiency.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg_her/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Fetch_DDPG_HER_baselines -lvl push -n 8\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Inverted Double Pendulum\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo inverted double pendulum environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl inverted_double_pendulum\n```\n\n----------------------------------------\n\nTITLE: Human Interaction with Environment in Coach (Python)\nDESCRIPTION: This command demonstrates how to interact with an environment as a human in Coach. It uses the Atari environment with the Breakout game and enables play mode with the --play flag.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/usage.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncoach -et rl_coach.environments.gym_environment:Atari -lvl BreakoutDeterministic-v4 --play\n```\n\n----------------------------------------\n\nTITLE: Calculating DDPG Critic Network Target in Python\nDESCRIPTION: This snippet demonstrates how to calculate the target value for training the critic network in DDPG. It involves using the actor target network to get the next action, then using the critic target network to estimate the Q-value.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/ddpg.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny_t = r(s_t, a_t) + gamma * Q(s_{t+1}, mu(s_{t+1}))\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Swimmer\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo swimmer environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl swimmer\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Inverted Pendulum\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo inverted pendulum environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl inverted_pendulum\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Half Cheetah\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo half cheetah environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl half_cheetah\n```\n\n----------------------------------------\n\nTITLE: Converting Position to Coordinates in Python\nDESCRIPTION: Function to convert x and y positions to grid coordinates within a predefined range. It ensures the positions are within bounds and scales them to a 0-99 range.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef pos2cord(x, y):\n    x = max(min(x, x_range[1]), x_range[0])\n    y = max(min(y, y_range[1]), y_range[0])\n    x = int(((x - x_range[0])/(x_range[1] - x_range[0]))*99)\n    y = int(((y - y_range[0])/(y_range[1] - y_range[0]))*99)\n    return x, y\n```\n\n----------------------------------------\n\nTITLE: RST Image Inclusion\nDESCRIPTION: ReStructuredText markup for including the NEC network structure diagram with specified width and alignment properties.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/nec.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. image:: /_static/img/design_imgs/nec.png\n   :width: 500px\n   :align: center\n```\n\n----------------------------------------\n\nTITLE: Policy Gradient Rescaling Formula in reStructuredText\nDESCRIPTION: Mathematical formula that defines the policy gradient loss function. The loss is calculated as the negative log of the policy probability multiplied by a rescaler factor that helps reduce variance in the gradient updates.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/pg.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:math:`L=-log (\\pi) \\cdot  PolicyGradientRescaler`\n```\n\n----------------------------------------\n\nTITLE: Executing Golden Tests for Performance Verification\nDESCRIPTION: Command to run golden tests that verify preset performance against known score thresholds\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest rl_coach/tests -m golden_test\n```\n\n----------------------------------------\n\nTITLE: Running ACER on Space Invaders with 16 workers\nDESCRIPTION: This command executes the ACER algorithm on the Atari game Space Invaders using 16 workers. It utilizes the Atari_ACER preset in the Coach framework.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/acer/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Atari_ACER -lvl space_invaders -n 16\n```\n\n----------------------------------------\n\nTITLE: Setting Up Atari Environment\nDESCRIPTION: Configuration code for setting up the Atari environment with Breakout game parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.environments.gym_environment import Atari, atari_deterministic_v4\n\nenv_params = Atari(level='BreakoutDeterministic-v4')\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantile Regression DQN Training Loop\nDESCRIPTION: A pseudo-algorithmic representation of the Quantile Regression DQN training process. The algorithm samples transitions from replay buffer, predicts quantiles for next states, calculates targets using Bellman equation, and updates the network using quantile regression loss.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/qr_dqn.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# 1. Sample transitions batch from replay buffer\n# 2. Predict next state quantiles and calculate targets\n# 3. Predict current quantile locations for current states\n# 4. Train network with quantile regression loss\n# 5. Periodically copy weights to target network\n```\n\n----------------------------------------\n\nTITLE: Running Fetch Pick And Place DDPG HER - 8 Workers\nDESCRIPTION: Command to execute DDPG HER on the Fetch Pick And Place environment using 8 parallel workers for improved training efficiency.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg_her/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Fetch_DDPG_HER -lvl pick_and_place -n 8\n```\n\n----------------------------------------\n\nTITLE: HTML Questionnaire Structure for Algorithm Selection\nDESCRIPTION: HTML structure defining a questionnaire with radio buttons for action types (discrete/continuous) and checkboxes for other task requirements. This forms the user interface for the interactive algorithm selection tool.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/selecting_an_algorithm.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"bordered-container\">\n   <div class=\"questionnaire\">\n      What are the type of actions your task requires?\n      <div style=\"margin-left: 12px;\">\n         <input type=\"radio\" id=\"discrete\" name=\"actions\" checked>Discrete actions<br>\n         <input type=\"radio\" id=\"continuous\" name=\"actions\">Continuous actions<br>\n      </div>\n      <input type=\"checkbox\" id=\"imitation\" checked=\"True\">Do you have expert demonstrations for your task?<br>\n      <input type=\"checkbox\" id=\"on-policy\" checked=\"True\">Can you collect new data for your task dynamically?<br>\n      <input type=\"checkbox\" id=\"requires-multi-worker\" checked=\"True\">Do you have a simulator for your task?<br>\n   </div>\n\n   <br>\n   <div class=\"badges-wrapper\">\n      <div class=\"algorithm discrete off-policy\" data-year=\"201300\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/dqn.html\">DQN</a>\n            <br>\n            Learns action values for discrete actions, and allows learning from a replay buffer with old experiences\n         </span>\n      </div>\n      <div class=\"algorithm  discrete off-policy\" data-year=\"201710\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/rainbow.html\">Rainbow</a>\n            <br>\n            Combines multiple recent innovations on top of DQN for discrete controls, and achieves\n            much better results on known benchmarks\n         </span>\n      </div>\n      <div class=\"algorithm continuous off-policy\" data-year=\"201712\">\n         <span class=\"badge\">\n            <a href=\"components/agents/policy_optimization/hac.html\">HAC</a>\n            <br>\n            Works only for continuous actions, and uses hierarchy of agents to make the learning\n            more simple\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy data-year=\"201509\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/ddqn.html\">DDQN</a>\n            <br>\n            An improvement over DQN, which learns more accurate action values, and therefore achieves better results\n            on known benchmarks\n         </span>\n      </div>\n      <div class=\"algorithm discrete on-policy\" data-year=\"201611\">\n         <span class=\"badge\">\n            <a href=\"components/agents/other/dfp.html\">DFP</a>\n            <br>\n            Works only for discrete actions, by learning to predict the future values of a set of\n            measurements from the environment, and then using a goal vector to weight the importance of each of the\n            measurements\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy\" data-year=\"201606\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/mmc.html\">MMC</a>\n            <br>\n            A simple modification to DQN, which instead of learning action values only by bootstrapping the current\n            action value prediction, it mixes in the total discounted return as well. This helps learn the correct\n            action values faster, and is particularly useful for environments with delayed rewards.\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy\" data-year=\"201512\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/pal.html\">PAL</a>\n            <br>\n            An improvement over DQN, that tries to deal with the approximation errors present in reinforcement\n            learning by increasing the gap between the value of the best action and the second best action.\n         </span>\n      </div>\n      <div class=\"algorithm continuous off-policy\" data-year=\"201603\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/naf.html\">NAF</a>\n            <br>\n            A variant of Q learning for continuous control.\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy\" data-year=\"201703\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/ddqn.html\">NEC</a>\n            <br>\n            Uses a memory to \"memorize\" its experience and learn much faster by querying the memory on newly\n            seen states.\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy\" data-year=\"201710\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/qr_dqn.html\">QR DQN</a>\n            <br>\n            Uses quantile regression to learn a distribution over the action values instead of only their mean.\n            This boosts performance on known benchmarks.\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy\" data-year=\"201602\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/bs_dqn.html\">Bootstrapped DQN</a>\n            <br>\n            Uses an ensemble of DQN networks, where each network learns from a different subset of the experience\n            in order to improve exploration.\n         </span>\n      </div>\n      <div class=\"algorithm discrete on-policy requires-multi-worker\" data-year=\"201602\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/n_step.html\">N-Step Q Learning</a>\n            <br>\n            A variant of Q learning that uses bootstrapping of N steps ahead, instead of 1 step. Doing this\n            makes the algorithm on-policy and therefore requires having multiple workers training in parallel in\n            order for it to work well.\n         </span>\n      </div>\n      <div class=\"algorithm discrete off-policy\" data-year=\"201706\">\n         <span class=\"badge\">\n            <a href=\"components/agents/value_optimization/categorical_dqn.html\">Categorical DQN</a>\n            <br>\n            Learns a distribution over the action values instead of only their mean. This boosts performance on\n            known algorithms but requires knowing the range of possible values for the accumulated rewards before hand.\n         </span>\n      </div>\n      <div class=\"algorithm continuous discrete on-policy\"  data-year=\"199200\">\n         <span class=\"badge\">\n            <a href=\"components/agents/policy_optimization/pg.html\">Policy Gradient</a>\n            <br>\n            Based on the REINFORCE algorithm, this algorithm learn a probability distribution over the actions.\n            This is the most simple algorithm available in Coach, but also has the worse results.\n         </span>\n      </div>\n      <div class=\"algorithm discrete continuous on-policy requires-multi-worker\" data-year=\"201602\">\n         <span class=\"badge\">\n            <a href=\"components/agents/policy_optimization/ac.html\">Actor Critic (A3C / A2C)</a>\n            <br>\n            Combines REINFORCE with a learned baseline (Critic) to improve stability of learning. It also\n            introduced the parallel learning of multiple workers to speed up data collection and improve the\n            learning stability and speed, both for discrete and continuous action spaces.\n         </span>\n      </div>\n      <div class=\"algorithm discrete on-policy requires-multi-worker\" data-year=\"201707\">\n         <span class=\"badge\">\n            <a href=\"components/agents/policy_optimization/acer.html\">ACER</a>\n            <br>\n            Similar to A3C with the addition of experience replay and off-policy training. to reduce variance and\n            improve stability it also employs bias correction and trust region optimization techniques.\n         </span>\n      </div>\n      <div class=\"algorithm continuous off-policy\" data-year=\"201808\">\n         <span class=\"badge\">\n```\n\n----------------------------------------\n\nTITLE: Training Process for Direct Future Prediction Network\nDESCRIPTION: This snippet describes the training process for the DFP neural network. It explains how transition batches are used to update the network's predictions, with targets being the actual measurements for taken actions and current predictions for actions not taken.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/other/dfp.rst#2025-04-23_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nGiven a batch of transitions, run them through the network to get the current predictions of the future measurements\nper action, and set them as the initial targets for training the network. For each transition\n:math:`(s_t,a_t,r_t,s_{t+1} )` in the batch, the target of the network for the action that was taken, is the actual\nmeasurements that were seen in time-steps :math:`t+1,t+2,t+4,t+8,t+16` and :math:`t+32`.\nFor the actions that were not taken, the targets are the current values.\n```\n\n----------------------------------------\n\nTITLE: Importing Architecture Class in Python\nDESCRIPTION: This snippet imports the Architecture class from the rl_coach.architectures.architecture module. The Architecture class is likely an abstract base class for implementing neural network architectures in the Coach project.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/architectures/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.architectures.architecture.Architecture\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Running DDPG on Humanoid\nDESCRIPTION: Command to train a DDPG agent on the MuJoCo humanoid environment using a single worker.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/ddpg/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Mujoco_DDPG -lvl humanoid\n```\n\n----------------------------------------\n\nTITLE: Loading Collected Dataset for Visualization in Python\nDESCRIPTION: This snippet loads the collected dataset from a saved file for visualization purposes. It uses the joblib library to load the compressed dataset.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport joblib\n\nprint('Loading data-set (this can take several minutes)...')\nrb_path = os.path.join('./Resources', 'RB_TD3GoalBasedAgent.joblib.bz2')\nepisodes = joblib.load(rb_path)\nprint('Done')\n```\n\n----------------------------------------\n\nTITLE: Defining Bellman Update for Categorical DQN Atom in Python\nDESCRIPTION: This snippet defines the Bellman update for an atom z_j in the Categorical DQN algorithm. It uses LaTeX syntax within a Python string to represent the mathematical formula.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/categorical_dqn.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n:math:`\\hat{T}_{z_{j}} := r+\\gamma z_j`\n```\n\n----------------------------------------\n\nTITLE: Setting Visualization Parameters in Python\nDESCRIPTION: Configures visualization parameters to enable rendering of the simulation.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/2. Adding an Environment.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom rl_coach.base_parameters import VisualizationParameters\n\nvis_params = VisualizationParameters(render=True)\n```\n\n----------------------------------------\n\nTITLE: Displaying Reinforcement Learning Algorithm Information in HTML\nDESCRIPTION: This HTML snippet creates a structured layout for displaying information about various reinforcement learning algorithms. It uses div elements with class attributes to categorize and style each algorithm's description.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/selecting_an_algorithm.rst#2025-04-23_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"algorithm continuous off-policy\" data-year=\"201806\">\n   <span class=\"badge\">\n      <a href=\"components/agents/policy_optimization/sac.html\">SAC</a>\n      <br>\n      Soft Actor-Critic is an algorithm which optimizes a stochastic policy in an off-policy way.\n      One of the key features of SAC is that it solves a maximum entropy reinforcement learning problem.\n   </span>\n</div>\n<div class=\"algorithm continuous off-policy\" data-year=\"201509\">\n   <span class=\"badge\">\n      <a href=\"components/agents/policy_optimization/ddpg.html\">DDPG</a>\n      <br>\n      An actor critic scheme for continuous action spaces which assumes that the policy is deterministic,\n      and therefore it is able to use a replay buffer in order to improve sample efficiency.\n   </span>\n</div>\n<div class=\"algorithm continuous off-policy\" data-year=\"201509\">\n   <span class=\"badge\">\n      <a href=\"components/agents/policy_optimization/td3.html\">TD3</a>\n      <br>\n      Very similar to DDPG, i.e. an actor-critic for continuous action spaces, that uses a replay buffer in\n      order to improve sample efficiency. TD3 uses two critic networks in order to mitigate the overestimation\n      in the Q state-action value prediction, slows down the actor updates in order to increase stability and\n      adds noise to actions while training the critic in order to smooth out the critic's predictions.\n   </span>\n</div>\n<div class=\"algorithm continuous discrete on-policy\" data-year=\"201706\">\n   <span class=\"badge\">\n      <a href=\"components/agents/policy_optimization/ppo.html\">PPO</a>\n      <br>\n      An actor critic scheme which uses bounded updates to the policy in order to make the learning process\n      very stable.\n   </span>\n</div>\n<div class=\"algorithm discrete continuous on-policy\" data-year=\"201706\">\n   <span class=\"badge\">\n      <a href=\"components/agents/policy_optimization/cppo.html\">Clipped PPO</a>\n      <br>\n      A simplification of PPO, that reduces the code complexity while achieving similar results.\n   </span>\n</div>\n<div class=\"algorithm discrete continuous imitation off-policy\" data-year=\"199700\">\n   <span class=\"badge\">\n      <a href=\"components/agents/imitation/bc.html\">BC</a>\n      <br>\n      The simplest form of imitation learning. Uses supervised learning on a dataset of expert demonstrations\n      in order to imitate the expert behavior.\n   </span>\n</div>\n<div class=\"algorithm discrete continuous imitation off-policy\" data-year=\"201710\">\n   <span class=\"badge\">\n      <a href=\"components/agents/imitation/cil.html\">CIL</a>\n      <br>\n      A variant of behavioral cloning, where the learned policy is disassembled to several skills\n      (such as turning left or right in an intersection), and each skill is learned separately from the\n      human demonstrations.\n   </span>\n</div>\n```\n\n----------------------------------------\n\nTITLE: DFP Network Structure RST Definition\nDESCRIPTION: ReStructuredText markup defining the network structure diagram display for DFP architecture\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/other/dfp.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. image:: /_static/img/design_imgs/dfp.png\n   :width: 600px\n   :align: center\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Agent Documentation in RST\nDESCRIPTION: RST code for generating a table of contents that organizes various reinforcement learning agents by categories, including policy optimization, imitation learning, value optimization, and other approaches.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/index.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :caption: Agents\n\n   policy_optimization/ac\n   policy_optimization/acer\n   imitation/bc\n   value_optimization/bs_dqn\n   value_optimization/categorical_dqn\n   imitation/cil\n   policy_optimization/cppo\n   policy_optimization/ddpg\n   other/dfp\n   value_optimization/double_dqn\n   value_optimization/dqn\n   value_optimization/dueling_dqn\n   value_optimization/mmc\n   value_optimization/n_step\n   value_optimization/naf\n   value_optimization/nec\n   value_optimization/pal\n   policy_optimization/pg\n   policy_optimization/ppo\n   value_optimization/rainbow\n   value_optimization/qr_dqn\n   policy_optimization/sac\n   policy_optimization/td3\n   policy_optimization/wolpertinger\n```\n\n----------------------------------------\n\nTITLE: Algorithm Parameters Class Reference\nDESCRIPTION: Auto-documentation reference for the N-Step Q Algorithm parameters class\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/n_step.rst#2025-04-23_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: rl_coach.agents.n_step_q_agent.NStepQAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: OpenAI Gym Environment Reference\nDESCRIPTION: Auto-generated documentation for the OpenAI Gym environment wrapper that supports native Gym environments and extensions.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/environments/index.rst.txt#2025-04-23_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.gym_environment.GymEnvironment\n```\n\n----------------------------------------\n\nTITLE: Mathematical Definition of Likelihood Ratio in Clipped PPO\nDESCRIPTION: The formula defining the likelihood ratio used in the Clipped PPO algorithm, which compares the probability of taking an action under the current policy versus the old policy.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/cppo.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nr_t(\\theta) =\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}\n```\n\n----------------------------------------\n\nTITLE: Renaming Input Embedder Parameters in Python\nDESCRIPTION: Renames the input embedder key from 'observation' to 'measurements' for both actor and critic network wrappers.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/2. Adding an Environment.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nagent_params.network_wrappers['actor'].input_embedders_parameters['measurements'] = \\\n    agent_params.network_wrappers['actor'].input_embedders_parameters.pop('observation')\nagent_params.network_wrappers['critic'].input_embedders_parameters['measurements'] = \\\n    agent_params.network_wrappers['critic'].input_embedders_parameters.pop('observation')\n```\n\n----------------------------------------\n\nTITLE: Documenting ActionInfo class in Python using Sphinx\nDESCRIPTION: Uses Sphinx autoclass directive to generate documentation for the ActionInfo class from the rl_coach.core_types module. Includes all members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/core_types.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: rl_coach.core_types.ActionInfo\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Environment Filter in Coach\nDESCRIPTION: This code snippet shows the structure of a CustomFilter class that extends InputFilter. It includes method stubs for filtering environment responses, defining observation and reward spaces, validating input spaces, and resetting the filter.\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/environments/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nfrom coach.filters.input_filter import InputFilter\n\nclass CustomFilter(InputFilter):\n  def __init__(self):\n    ...\n  def _filter(self, env_response: EnvResponse) -> EnvResponse:\n    ...\n  def _get_filtered_observation_space(self, input_observation_space: ObservationSpace) -> ObservationSpace:\n    ...\n  def _get_filtered_reward_space(self, input_reward_space: RewardSpace) -> RewardSpace:\n    ...\n  def _validate_input_observation_space(self, input_observation_space: ObservationSpace):\n    ...\n  def _reset(self):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Tests for Coach\nDESCRIPTION: Commands to build and run the Docker image that verifies Coach installation and component functionality\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd docker\nmake build_base && make build\nmake run\n```\n\n----------------------------------------\n\nTITLE: Control Suite Environment Reference\nDESCRIPTION: Auto-generated documentation for the DeepMind Control Suite environment wrapper.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/environments/index.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.control_suite_environment.ControlSuiteEnvironment\n```\n\n----------------------------------------\n\nTITLE: Actor-Critic Loss Function Calculation\nDESCRIPTION: Mathematical formula for the policy gradient loss function used in Actor-Critic method. It calculates the negative expectation of the log policy probability multiplied by the advantage estimate.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/ac.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nL = -\\mathop{\\mathbb{E}} [log (\\pi) \\cdot A]\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Categorical DQN Head\nDESCRIPTION: Imports necessary modules and packages for implementing the Categorical DQN head, including TensorFlow and Coach components.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/1. Implementing an Algorithm.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nmodule_path = os.path.abspath(os.path.join('..'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nimport tensorflow as tf\nfrom rl_coach.architectures.tensorflow_components.heads.head import Head\nfrom rl_coach.architectures.head_parameters import HeadParameters\nfrom rl_coach.base_parameters import AgentParameters\nfrom rl_coach.core_types import QActionStateValue\nfrom rl_coach.spaces import SpacesDefinition\n```\n\n----------------------------------------\n\nTITLE: Documenting RedisPubSubBackend Class for RL Coach\nDESCRIPTION: This snippet uses Python's autodoc feature to automatically generate documentation for the RedisPubSubBackend class from the rl_coach.memories.backend.redis module. It's part of the memory backends used in the RL Coach project for reinforcement learning.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/memory_backends/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: rl_coach.memories.backend.redis.RedisPubSubBackend\n```\n\n----------------------------------------\n\nTITLE: Documenting DistributedTaskParameters Class in Python for RL Coach\nDESCRIPTION: Auto-generated documentation for the DistributedTaskParameters class from the rl_coach.base_parameters module.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/additional_parameters.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.DistributedTaskParameters\n```\n\n----------------------------------------\n\nTITLE: Importing Starcraft II Environment\nDESCRIPTION: Auto-documented class for Blizzard Starcraft II environment with DeepMind's Python interface.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/environments/index.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.starcraft2_environment.StarCraft2Environment\n```\n\n----------------------------------------\n\nTITLE: Environment Class Reference\nDESCRIPTION: Auto-generated documentation for the base Environment class in RL Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/environments/index.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.environment.Environment\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Computing Q-Head Gradients in ACER Algorithm\nDESCRIPTION: This LaTeX formula represents the calculation of Q-Head gradients using Mean Squared Error (MSE) in the ACER algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/acer.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\hat{g}_t^{Q} = (Q^{ret}(s_t,a_t) - Q(s_t,a_t)) \\nabla Q(s_t,a_t)\\\\\n```\n\n----------------------------------------\n\nTITLE: Network Structure Reference\nDESCRIPTION: Reference to the DQN network structure diagram used in the N-Step Q Learning implementation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/n_step.rst#2025-04-23_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. image:: /_static/img/design_imgs/dqn.png\n   :align: center\n```\n\n----------------------------------------\n\nTITLE: Importing Gym Environment\nDESCRIPTION: Auto-documented class for OpenAI Gym environments and its extensions including Roboschool, Gym Extensions, and PyBullet.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/environments/index.rst#2025-04-23_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.gym_environment.GymEnvironment\n```\n\n----------------------------------------\n\nTITLE: Python Class Reference\nDESCRIPTION: Reference to the Python class containing Policy Gradient algorithm parameters and implementation details.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/pg.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrl_coach.agents.policy_gradients_agent.PolicyGradientAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Importing Doom Environment\nDESCRIPTION: Auto-documented class for ViZDoom environment for reinforcement learning from visual information.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/environments/index.rst#2025-04-23_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.doom_environment.DoomEnvironment\n```\n\n----------------------------------------\n\nTITLE: Importing NAF Algorithm Parameters in Python\nDESCRIPTION: This snippet shows the autoclass directive for importing the NAFAlgorithmParameters class, which likely contains the configuration parameters for the NAF algorithm.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/naf.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.naf_agent.NAFAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Trust Region Policy Update in ACER\nDESCRIPTION: Formulas for implementing trust region policy optimization which constrains policy updates to prevent large divergences from the average policy, ensuring stability during training.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/acer.rst#2025-04-23_snippet_4\n\nLANGUAGE: math\nCODE:\n```\n\\hat{g}_t^{trust-region} = \\hat{g}_t^{policy} - \\max \\left\\{0, \\frac{k^T \\hat{g}_t^{policy} - \\delta}{\\lVert k \\rVert_2^2}\\right\\} k\n```\n\nLANGUAGE: math\nCODE:\n```\n\\text{where} \\quad k = \\nabla D_{KL}[\\pi_{avg} \\parallel \\pi]\n```\n\n----------------------------------------\n\nTITLE: Auto-generating DQNAgent Class Documentation\nDESCRIPTION: This snippet uses the autoclass directive to generate documentation for the DQNAgent class, including its members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/test.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: rl_coach.agents.dqn_agent.DQNAgent\n      :members:\n      :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Doom Environment Reference\nDESCRIPTION: Auto-generated documentation for the ViZDoom environment wrapper.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/environments/index.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.doom_environment.DoomEnvironment\n```\n\n----------------------------------------\n\nTITLE: Switching to MXNet Backend in Coach\nDESCRIPTION: Command for using MXNet instead of TensorFlow as the deep learning backend. The -f flag allows switching between supported frameworks.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/usage.rst.txt#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncoach -p Doom_Basic_DQN -f mxnet\n```\n\n----------------------------------------\n\nTITLE: CARLA Environment Reference\nDESCRIPTION: Auto-generated documentation for the CARLA autonomous driving environment wrapper.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/environments/index.rst.txt#2025-04-23_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.carla_environment.CarlaEnvironment\n```\n\n----------------------------------------\n\nTITLE: Importing NetworkParameters Class in Python\nDESCRIPTION: This snippet shows the import statement for the NetworkParameters class from the rl_coach.base_parameters module. NetworkParameters is likely used to define and configure neural network architectures in the Coach project.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/architectures/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.NetworkParameters\n```\n\n----------------------------------------\n\nTITLE: Computing Q-Network Training Targets in Soft Actor-Critic\nDESCRIPTION: Mathematical formula for computing the Q-network training targets in SAC. The target uses the current reward plus the discounted value of the next state obtained from the target value network.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/sac.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\ny_t^Q=r(s_t,a_t)+\\gamma \\cdot V(s_{t+1})\n```\n\n----------------------------------------\n\nTITLE: Adding Important Note in reStructuredText\nDESCRIPTION: This snippet demonstrates how to add an important note using reStructuredText syntax.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/test.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. important:: Its a note! in markdown!\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests with PyTest\nDESCRIPTION: Command to execute unit tests that verify sub-components of Coach using pytest framework\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest rl_coach/tests -m unit_test\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Reinforcement Learning Algorithms\nDESCRIPTION: HTML markup organizing and describing various reinforcement learning algorithms with their characteristics and links to detailed documentation. Includes algorithms like SAC, DDPG, TD3, PPO, Clipped PPO, BC, and CIL with their descriptions.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/selecting_an_algorithm.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"algorithm continuous off-policy\" data-year=\"201509\">\n   <span class=\"badge\">\n      <a href=\"components/agents/policy_optimization/sac.html\">SAC</a>\n      <br>\n      Soft Actor-Critic is an algorithm which optimizes a stochastic policy in an off-policy way.\n      One of the key features of SAC is that it solves a maximum entropy reinforcement learning problem.\n   </span>\n</div>\n```\n\n----------------------------------------\n\nTITLE: S3DataStore Class Documentation\nDESCRIPTION: RestructuredText documentation reference for the S3DataStore class used for AWS S3 storage integration in RL Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/data_stores/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: rl_coach.data_stores.s3_data_store.S3DataStore\n```\n\n----------------------------------------\n\nTITLE: Persistent Advantage Learning Target Calculation\nDESCRIPTION: Final target calculation for PAL that considers the minimum action gap between current and next states.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/pal.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: math\nCODE:\n```\ny_t=y_t^{DDQN}-\\alpha \\cdot min(V(s_t )-Q(s_t,a_t ),V(s_{t+1} )-Q(s_{t+1},a_{t+1} ))\n```\n\n----------------------------------------\n\nTITLE: Installing CARLA Python Dependencies\nDESCRIPTION: Commands for installing the CARLA simulator Python client and its dependencies using pip3. This is required for setting up the CARLA environment for Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/README.md#2025-04-23_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -r PythonClient/requirements.txt\npip3 install PythonClient\n```\n\n----------------------------------------\n\nTITLE: Displaying Reinforcement Learning Algorithms Diagram in RST\nDESCRIPTION: RST code for displaying an image of Coach's reinforcement learning algorithms hierarchy, setting the width to 600px and centering it on the page.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/index.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. image:: /_static/img/algorithms.png\n   :width: 600px\n   :align: center\n```\n\n----------------------------------------\n\nTITLE: Q-Head Value Expression\nDESCRIPTION: Mathematical expression representing multiple Q heads used for value estimation in the Bootstrapped DQN architecture.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/bs_dqn.rst#2025-04-23_snippet_1\n\nLANGUAGE: math\nCODE:\n```\nQ\n```\n\n----------------------------------------\n\nTITLE: Documenting Episode class in Python using Sphinx\nDESCRIPTION: Uses Sphinx autoclass directive to generate documentation for the Episode class from the rl_coach.core_types module. Includes all members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/core_types.rst#2025-04-23_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: rl_coach.core_types.Episode\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Documenting Batch class in Python using Sphinx\nDESCRIPTION: Uses Sphinx autoclass directive to generate documentation for the Batch class from the rl_coach.core_types module. Includes all members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/core_types.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: rl_coach.core_types.Batch\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Calculating Action Values in Direct Future Prediction Algorithm\nDESCRIPTION: This snippet outlines the mathematical process for choosing actions in the DFP algorithm. It explains how predicted future measurements are combined with goal vectors to produce action values which are then passed to an exploration policy for selection.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/other/dfp.rst#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. The current states (observations and measurements) and the corresponding goal vector are passed as an input to the network.\n   The output of the network is the predicted future measurements for time-steps :math:`t+1,t+2,t+4,t+8,t+16` and\n   :math:`t+32` for each possible action.\n2. For each action, the measurements of each predicted time-step are multiplied by the goal vector,\n   and the result is a single vector of future values for each action.\n3. Then, a weighted sum of the future values of each action is calculated, and the result is a single value for each action. \n4. The action values are passed to the exploration policy to decide on the action to use.\n```\n\n----------------------------------------\n\nTITLE: Building Environment-Specific Docker Container\nDESCRIPTION: Command to build a Docker container for a specific environment (gym, mujoco, or doom).\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build --build-arg MUJOCO_KEY=${MUJOCO_KEY} -t <user-name>/<image-name>:<tag> -f docker/Dockerfile.<env> .\n```\n\n----------------------------------------\n\nTITLE: Structuring Features Documentation with toctree in reStructuredText\nDESCRIPTION: A toctree directive that organizes the features section of the Coach documentation. It sets a maximum depth of 1 and includes links to pages for algorithms, environments, benchmarks, and batch reinforcement learning.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/features/index.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :caption: Features\n\n   algorithms\n   environments\n   benchmarks\n   batch_rl\n```\n\n----------------------------------------\n\nTITLE: RST Class Documentation Reference\nDESCRIPTION: ReStructuredText directive for auto-documenting the NEC algorithm parameters class.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/nec.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.agents.nec_agent.NECAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Documenting TaskParameters Class in Python for RL Coach\nDESCRIPTION: Auto-generated documentation for the TaskParameters class from the rl_coach.base_parameters module.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/additional_parameters.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.TaskParameters\n```\n\n----------------------------------------\n\nTITLE: Documenting Transition class in Python using Sphinx\nDESCRIPTION: Uses Sphinx autoclass directive to generate documentation for the Transition class from the rl_coach.core_types module. Includes all members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/core_types.rst#2025-04-23_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: rl_coach.core_types.Transition\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Goal Space Class Documentation\nDESCRIPTION: Documentation for the GoalsSpace class used for defining reinforcement learning goals.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/spaces.rst#2025-04-23_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.spaces.GoalsSpace\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for Presets\nDESCRIPTION: Command to run integration tests that verify all presets can start without errors\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pytest rl_coach/tests -m integration_test\n```\n\n----------------------------------------\n\nTITLE: Building Environment-Specific Docker Container\nDESCRIPTION: Command to build a Docker container for a specific environment (gym, mujoco, or doom).\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build --build-arg MUJOCO_KEY=${MUJOCO_KEY} -t <user-name>/<image-name>:<tag> -f docker/Dockerfile.<env> .\n```\n\n----------------------------------------\n\nTITLE: Documenting PresetValidationParameters Class in Python for RL Coach\nDESCRIPTION: Auto-generated documentation for the PresetValidationParameters class from the rl_coach.base_parameters module.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/additional_parameters.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.PresetValidationParameters\n```\n\n----------------------------------------\n\nTITLE: Defining Space Documentation Structure in RST\nDESCRIPTION: This RST (reStructuredText) defines the documentation structure for the Space class and its derived classes in RL Coach. It organizes spaces into categories of Observation, Action, and Goal spaces, using Sphinx autodoc directives to generate API documentation from docstrings.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/spaces.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nSpaces\n======\n\nSpace\n-----\n.. autoclass:: rl_coach.spaces.Space\n   :members:\n   :inherited-members:\n\n\n\nObservation Spaces\n------------------\n.. autoclass:: rl_coach.spaces.ObservationSpace\n   :members:\n   :inherited-members:\n\nVectorObservationSpace\n++++++++++++++++++++++\n.. autoclass:: rl_coach.spaces.VectorObservationSpace\n\nPlanarMapsObservationSpace\n++++++++++++++++++++++++++\n.. autoclass:: rl_coach.spaces.PlanarMapsObservationSpace\n\nImageObservationSpace\n+++++++++++++++++++++\n.. autoclass:: rl_coach.spaces.ImageObservationSpace\n\n\n\nAction Spaces\n-------------\n.. autoclass:: rl_coach.spaces.ActionSpace\n   :members:\n   :inherited-members:\n\nAttentionActionSpace\n++++++++++++++++++++\n.. autoclass:: rl_coach.spaces.AttentionActionSpace\n\nBoxActionSpace\n++++++++++++++\n.. autoclass:: rl_coach.spaces.BoxActionSpace\n\nDiscreteActionSpace\n++++++++++++++++++++\n.. autoclass:: rl_coach.spaces.DiscreteActionSpace\n\nMultiSelectActionSpace\n++++++++++++++++++++++\n.. autoclass:: rl_coach.spaces.MultiSelectActionSpace\n\nCompoundActionSpace\n+++++++++++++++++++\n.. autoclass:: rl_coach.spaces.CompoundActionSpace\n\n\n\nGoal Spaces\n-----------\n.. autoclass:: rl_coach.spaces.GoalsSpace\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Documenting EnvResponse class in Python using Sphinx\nDESCRIPTION: Uses Sphinx autoclass directive to generate documentation for the EnvResponse class from the rl_coach.core_types module. Includes all members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/core_types.rst#2025-04-23_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: rl_coach.core_types.EnvResponse\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Auto-rebuild Documentation for Development\nDESCRIPTION: Command to automatically rebuild the documentation after every change while editing files. Uses sphinx-autobuild to monitor source files and update the build directory.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsphinx-autobuild source build/html\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Gym Environment\nDESCRIPTION: Dockerfile content for creating a Coach image with Gym environment support.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nFROM coach-base:master as builder\n\n# prep gym and any of its related requirements.\nRUN pip3 install gym[atari,box2d,classic_control]==0.10.5\n\n# add coach source starting with files that could trigger\n# re-build if dependencies change.\nRUN mkdir /root/src\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nRUN pip3 install -r /root/src/requirements.txt\n\nFROM coach-base:master\nWORKDIR /root/src\nCOPY --from=builder /root/.cache /root/.cache\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nCOPY README.md /root/src/.\nRUN pip3 install gym[atari,box2d,classic_control]==0.10.5 && pip3 install -e .[all] && rm -rf /root/.cache\nCOPY . /root/src\n```\n\n----------------------------------------\n\nTITLE: Calculating State Values in ACER Algorithm\nDESCRIPTION: Formula for calculating the expected state values in ACER by computing the expectation of Q-values over actions sampled from the policy distribution.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/acer.rst#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nV(s_t) = \\mathbb{E}_{a \\sim \\pi} [Q(s_t,a)]\n```\n\n----------------------------------------\n\nTITLE: Defining Features Documentation Structure in RST\nDESCRIPTION: Sphinx documentation toctree directive that creates a hierarchical structure for feature documentation, setting maxdepth to 1 and including sections for algorithms, environments, benchmarks and batch reinforcement learning.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/features/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :caption: Features\n\n   algorithms\n   environments\n   benchmarks\n   batch_rl\n```\n\n----------------------------------------\n\nTITLE: Importing Episodic Memory Classes in RL Coach\nDESCRIPTION: This snippet shows the import statements for episodic memory classes in RL Coach. These classes handle different types of episodic experience replay and buffers.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/memories/index.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom rl_coach.memories.episodic import EpisodicExperienceReplay\nfrom rl_coach.memories.episodic import EpisodicHindsightExperienceReplay\nfrom rl_coach.memories.episodic import EpisodicHRLHindsightExperienceReplay\nfrom rl_coach.memories.episodic import SingleEpisodeBuffer\n```\n\n----------------------------------------\n\nTITLE: Configuring Gym Environment Parameters in Python\nDESCRIPTION: Example of setting up environment parameters for a Pendulum environment using GymEnvironmentParameters with additional simulator parameters.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/contributing/add_env.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nenv_params = GymEnvironmentParameters()\nenv_params.level = \"rl_coach.environments.mujoco.pendulum_with_goals:PendulumWithGoals\"\nenv_params.additional_simulator_parameters = {\"time_limit\": 1000}\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for ViZDoom Environment\nDESCRIPTION: Dockerfile content for creating a Coach image with ViZDoom environment support.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nFROM coach-base:master as builder\n\n# prep vizdoom and any of its related requirements.\nRUN pip3 install vizdoom\n\n# add coach source starting with files that could trigger\n# re-build if dependencies change.\nRUN mkdir /root/src\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nRUN pip3 install -r /root/src/requirements.txt\n\nFROM coach-base:master\nWORKDIR /root/src\nCOPY --from=builder /root/.cache /root/.cache\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nCOPY README.md /root/src/.\nRUN pip3 install vizdoom && pip3 install -e .[all] && rm -rf /root/.cache\nCOPY . /root/src\n```\n\n----------------------------------------\n\nTITLE: Running Doom Health DFP with 8 Workers\nDESCRIPTION: Command to run the DFP algorithm on the Doom Health (D1: Basic) environment using 8 parallel workers.\nSOURCE: https://github.com/intellabs/coach/blob/master/benchmarks/dfp/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncoach -p Doom_Health_DFP -n 8\n```\n\n----------------------------------------\n\nTITLE: Documenting Agent Classes with Autodoc in RST\nDESCRIPTION: RST code using Sphinx autodoc extension to automatically generate documentation for AgentParameters and Agent classes, including all members and inherited members for the Agent class.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/index.rst.txt#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.base_parameters.AgentParameters\n\n.. autoclass:: rl_coach.agents.agent.Agent\n   :members:\n   :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Autodoc Class Reference for Policy Gradient Parameters\nDESCRIPTION: An autodoc directive that automatically includes the documentation for the PolicyGradientAlgorithmParameters class from the rl_coach.agents.policy_gradients_agent module.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/policy_optimization/pg.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.agents.policy_gradients_agent.PolicyGradientAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Memory Classes Documentation Structure in reStructuredText\nDESCRIPTION: A structured documentation outline defining various memory implementation classes in RL Coach, organized into episodic and non-episodic categories. The document uses reStructuredText format with autoclass directives to generate class documentation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/memories/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nMemories\n========\n\nEpisodic Memories\n-----------------\n\nEpisodicExperienceReplay\n++++++++++++++++++++++++\n.. autoclass:: rl_coach.memories.episodic.EpisodicExperienceReplay\n\nEpisodicHindsightExperienceReplay\n+++++++++++++++++++++++++++++++++\n.. autoclass:: rl_coach.memories.episodic.EpisodicHindsightExperienceReplay\n\nEpisodicHRLHindsightExperienceReplay\n++++++++++++++++++++++++++++++++++++\n.. autoclass:: rl_coach.memories.episodic.EpisodicHRLHindsightExperienceReplay\n\nSingleEpisodeBuffer\n+++++++++++++++++++\n.. autoclass:: rl_coach.memories.episodic.SingleEpisodeBuffer\n\n\nNon-Episodic Memories\n---------------------\nBalancedExperienceReplay\n++++++++++++++++++++++++\n.. autoclass:: rl_coach.memories.non_episodic.BalancedExperienceReplay\n\nQDND\n++++\n.. autoclass:: rl_coach.memories.non_episodic.QDND\n\nExperienceReplay\n++++++++++++++++\n.. autoclass:: rl_coach.memories.non_episodic.ExperienceReplay\n\nPrioritizedExperienceReplay\n+++++++++++++++++++++++++++\n.. autoclass:: rl_coach.memories.non_episodic.PrioritizedExperienceReplay\n\nTransitionCollection\n++++++++++++++++++++\n.. autoclass:: rl_coach.memories.non_episodic.TransitionCollection\n```\n\n----------------------------------------\n\nTITLE: Initializing Color-Rotation Mapping for Rubik's Cube in Python\nDESCRIPTION: Defines an OrderedDict mapping cube colors to specific rotations represented by Euler angles. This is used to determine the top face color based on the cube's orientation.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nCOLOR_ROTATION_MAP = OrderedDict([\n    (CubeColor.YELLOW, (0, 2, [np.array([0, 0]),\n                               np.array([np.pi, np.pi]), np.array([-np.pi, -np.pi]),\n                               np.array([-np.pi, np.pi]), np.array([np.pi, -np.pi])])),\n    (CubeColor.CYAN, (0, 2, [np.array([0, np.pi]), np.array([0, -np.pi]),\n                             np.array([np.pi, 0]), np.array([-np.pi, 0])])),\n    (CubeColor.WHITE, (1, 2, [np.array([-np.pi / 2])])),\n    (CubeColor.RED, (1, 2, [np.array([np.pi / 2])])),\n    (CubeColor.GREEN, (0, 2, [np.array([np.pi / 2, 0])])),\n    (CubeColor.BLUE, (0, 2, [np.array([-np.pi / 2, 0])])),\n])\n```\n\n----------------------------------------\n\nTITLE: Running Parallel Trace Tests\nDESCRIPTION: Command to execute trace tests that compare preset outputs against golden CSV files\nSOURCE: https://github.com/intellabs/coach/blob/master/rl_coach/tests/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 rl_coach/tests/trace_tests.py -prl\n```\n\n----------------------------------------\n\nTITLE: Behavioral Cloning Configuration in RST\nDESCRIPTION: Documentation structure written in ReStructuredText format that defines behavioral cloning algorithm's network structure, training process, and parameters. Includes image reference and autoclass documentation.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/imitation/bc.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nBehavioral Cloning\n==================\n\n**Actions space:** Discrete | Continuous\n\nNetwork Structure\n-----------------\n\n.. image:: /_static/img/design_imgs/pg.png\n   :align: center\n\n\nAlgorithm Description\n---------------------\n\nTraining the network\n++++++++++++++++++++\n\nThe replay buffer contains the expert demonstrations for the task.\nThese demonstrations are given as state, action tuples, and with no reward.\nThe training goal is to reduce the difference between the actions predicted by the network and the actions taken by\nthe expert for each state.\n\n1. Sample a batch of transitions from the replay buffer.\n2. Use the current states as input to the network, and the expert actions as the targets of the network.\n3. For the network head, we use the policy head, which uses the cross entropy loss function.\n\n\n.. autoclass:: rl_coach.agents.bc_agent.BCAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cube Positions by Color using Matplotlib in Python\nDESCRIPTION: Creates a figure with six subplots, each representing a different cube color. It plots the positions of the cube for each color using scatter plots, with customized appearance and labels.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfig = plt.figure(figsize=(15.0, 5.0))\naxes = []\nfor j in range(6):\n    axes.append(subplot(1, 6, j + 1))\n    xy = np.array(positions)[np.array(colors) == list(COLOR_MAP.keys())[j]]\n    axes[-1].scatter(xy[:, 1], xy[:, 0], c=COLOR_MAP[j], alpha=0.01, edgecolors='black')\n    plt.xlim(y_range)\n    plt.ylim(x_range)\n    plt.xticks([])\n    plt.yticks([])\n    axes[-1].set_aspect('equal', adjustable='box')\n    title = 'N=' + str(xy.shape[0])\n    plt.title(title)\n\nfor ax in axes:\n    ax.invert_yaxis()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Visualization Parameters for Collected Dataset in Python\nDESCRIPTION: This code defines parameters and enumerations for visualizing the collected dataset. It includes color mappings for the cube faces and range definitions for the plot.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport numpy as np\nfrom collections import OrderedDict\nfrom enum import IntEnum\nfrom pylab import subplot\nfrom gym.envs.robotics.rotations import quat2euler, mat2euler, quat2mat\nimport matplotlib.pyplot as plt\n\n\nclass CubeColor(IntEnum):\n    YELLOW = 0\n    CYAN = 1\n    WHITE = 2\n    RED = 3\n    GREEN = 4\n    BLUE = 5\n    UNKNOWN = 6\n\n\nx_range = [-0.3, 0.3]\ny_range = [-0.3, 0.3]\n\nCOLOR_MAP = OrderedDict([\n    (int(CubeColor.YELLOW), 'yellow'),\n    (int(CubeColor.CYAN), 'cyan'),\n    (int(CubeColor.WHITE), 'white'),\n    (int(CubeColor.RED), 'red'),\n    (int(CubeColor.GREEN), 'green'),\n    (int(CubeColor.BLUE), 'blue'),\n    (int(CubeColor.UNKNOWN), 'black'),\n])\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Container to Registry\nDESCRIPTION: Command to push the built Docker container to a container registry.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ docker push <user-name>/<image-name>:<tag>\n```\n\n----------------------------------------\n\nTITLE: Implementing Interactive Algorithm Selection Filter with jQuery\nDESCRIPTION: A JavaScript function that dynamically filters and sorts algorithm badges based on user selections in a questionnaire. It shows/hides algorithm options based on checkbox and radio button selections and orders the results by publication date in descending order.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/selecting_an_algorithm.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n$(document).ready(function() {\n   // descending order of the agent badges according to their publish year\n   function order_badges() {\n      $(\".badges-wrapper\").find('.algorithm').sort(function(a, b) {\n         // dataset.year is the concatenated year and month of the paper publishing date\n         return b.dataset.year - a.dataset.year;\n      }).appendTo($(\".badges-wrapper\"));\n   }\n\n   function update_algorithms_list() {\n      // show all the badges\n      $(\"input:checkbox, input:radio\").each(function(){\n         $('.' + this.id).show();\n      });\n\n      // remove all that don't fit the task\n      $(\"input:checkbox\").each(function(){\n         if (!this.checked) {\n            $('.' + this.id).hide();\n         }\n      });\n      $(\"input:radio\").each(function(){\n         if (this.checked) {\n            $('.algorithm').not('.' + this.id).hide();\n         }\n      });\n\n      order_badges();\n   }\n\n   // toggle badges according to the checkbox change\n   $('input:checkbox, input:radio').click(update_algorithms_list);\n\n   update_algorithms_list();\n});\n```\n\n----------------------------------------\n\nTITLE: Determining Cube Top Color from Quaternion in Python\nDESCRIPTION: Function to determine the top face color of the cube based on its quaternion representation. It converts the quaternion to Euler angles and compares with predefined rotations.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef get_cube_top_color(cube_quat, atol):\n    euler = mat2euler(quat2mat(cube_quat))\n    for color, (start_dim, end_dim, xy_rotations) in COLOR_ROTATION_MAP.items():\n        if any(list(np.allclose(euler[start_dim:end_dim], xy_rotation, atol=atol) for xy_rotation in xy_rotations)):\n            return color\n    return CubeColor.UNKNOWN\n```\n\n----------------------------------------\n\nTITLE: Creating Docker Directory\nDESCRIPTION: Command to create a directory for Docker-related files.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ mkdir docker\n```\n\n----------------------------------------\n\nTITLE: Extracting Cube Positions and Colors from Episodes in Python\nDESCRIPTION: Processes episode data to extract cube positions and colors. It uses the previously defined functions to determine colors from quaternions and convert positions to coordinates.\nSOURCE: https://github.com/intellabs/coach/blob/master/tutorials/5. Goal-Based Data Collection.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npos_idx = 25\nquat_idx = 28\npositions = []\ncolors = []\nprint('Extracting cube positions and colors...')\nfor episode in episodes:\n    for transition in episode:\n        x, y = transition.state['measurements'][pos_idx:pos_idx+2]\n        positions.append([x, y])\n        angle = quat2euler(transition.state['measurements'][quat_idx:quat_idx+4])\n        colors.append(int(get_cube_top_color(transition.state['measurements'][quat_idx:quat_idx+4], np.pi / 4)))\n\n        x_cord, y_cord = pos2cord(x, y)\n\n    x, y = episode[-1].next_state['measurements'][pos_idx:pos_idx+2]\n    positions.append([x, y])\n    colors.append(int(get_cube_top_color(episode[-1].next_state['measurements'][quat_idx:quat_idx+4], np.pi / 4)))\n\n    x_cord, y_cord = pos2cord(x, y)\nprint('Done')\n```\n\n----------------------------------------\n\nTITLE: NAF Action Mean Formula\nDESCRIPTION: Mathematical notation for the action mean output from the network.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/value_optimization/naf.rst#2025-04-23_snippet_2\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\mu(s_t)\n```\n\n----------------------------------------\n\nTITLE: DFP Class Import Definition\nDESCRIPTION: ReStructuredText markup for auto-documenting the DFP algorithm parameters class\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/other/dfp.rst.txt#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.agents.dfp_agent.DFPAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for Coach Contributing Guidelines\nDESCRIPTION: Structured markdown document outlining the contribution process, testing requirements, documentation standards and release procedures for the Coach project. Includes detailed instructions for PRs, issue filing, and contact information.\nSOURCE: https://github.com/intellabs/coach/blob/master/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Contributing to Coach\n\nThe following is a set of guidelines for contributing to Coach.\nWe'd like Coach to be useful to students, data scientists and researchers, and the purpose of these guidelines is to help maintain a level of quality and reliability for the Coach users community. \nThanks for taking the time to contribute!\n\n## Proposing a PR\nIf you would like to make code changes to Coach, whether adding new functionality or fixing a bug, please make sure to follow this list:\n1. Create a new [branch](https://help.github.com/articles/about-branches/) for your work (give it a short but meaningful name related to the functionality or bug you are working on), then add commits as necessary.  Once its in a reviewable state [open up a pull request](https://help.github.com/articles/about-pull-requests/)\n1. Add unit tests to any new component, or update existing tests for modified component's functionality\n1. Make sure [regression tests](https://github.com/IntelLabs/coach/tree/master/rl_coach/tests) are passing. See the [Testing section](#testing) for more details on the Coach testing methodology \n1. Update documentation to reflect any API changes, or added algorithm or environment. See the [Documentation section](#documentation) for more details on Coach documentation.\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of the Sphinx documentation using reStructuredText directives. It sets up the table of contents tree (toctree) for different sections of the documentation, including Intro, Design, Contributing, and Components.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/index.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :caption: Intro\n   :titlesonly:\n\n   usage\n   dist_usage\n   features/index\n   selecting_an_algorithm\n   dashboard\n\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Design\n\n   design/control_flow\n   design/network\n   design/horizontal_scaling\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Contributing\n\n   contributing/add_agent\n   contributing/add_env\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Components\n\n   components/agents/index\n   components/architectures/index\n   components/data_stores/index\n   components/environments/index\n   components/exploration_policies/index\n   components/filters/index\n   components/memories/index\n   components/memory_backends/index\n   components/orchestrators/index\n   components/core_types\n   components/spaces\n   components/additional_parameters\n```\n\n----------------------------------------\n\nTITLE: Behavioral Cloning Configuration in RST\nDESCRIPTION: RST documentation showing the network structure visualization and explanation of the BC training process through replay buffer sampling and action prediction.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/agents/imitation/bc.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nBehavioral Cloning\n==================\n\n**Actions space:** Discrete | Continuous\n\nNetwork Structure\n-----------------\n\n.. image:: /_static/img/design_imgs/pg.png\n   :align: center\n\n\nAlgorithm Description\n---------------------\n\nTraining the network\n++++++++++++++++++++\n\nThe replay buffer contains the expert demonstrations for the task.\nThese demonstrations are given as state, action tuples, and with no reward.\nThe training goal is to reduce the difference between the actions predicted by the network and the actions taken by\nthe expert for each state.\n\n1. Sample a batch of transitions from the replay buffer.\n2. Use the current states as input to the network, and the expert actions as the targets of the network.\n3. For the network head, we use the policy head, which uses the cross entropy loss function.\n\n\n.. autoclass:: rl_coach.agents.bc_agent.BCAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Mujoco Environment\nDESCRIPTION: Dockerfile content for creating a Coach image with Mujoco environment support.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nFROM coach-base:master as builder\n\n# prep mujoco and any of its related requirements.\n# Mujoco\nRUN mkdir -p ~/.mujoco \\\n    && wget https://www.roboti.us/download/mjpro150_linux.zip -O mujoco.zip \\\n    && unzip -n mujoco.zip -d ~/.mujoco \\\n    && rm mujoco.zip\nARG MUJOCO_KEY\nENV MUJOCO_KEY=$MUJOCO_KEY\nENV LD_LIBRARY_PATH /root/.mujoco/mjpro150/bin:$LD_LIBRARY_PATH\nRUN echo $MUJOCO_KEY | base64 --decode > /root/.mujoco/mjkey.txt\nRUN pip3 install mujoco_py\n\n# add coach source starting with files that could trigger\n# re-build if dependencies change.\nRUN mkdir /root/src\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nRUN pip3 install -r /root/src/requirements.txt\n\nFROM coach-base:master\nWORKDIR /root/src\nCOPY --from=builder /root/.mujoco /root/.mujoco\nENV LD_LIBRARY_PATH /root/.mujoco/mjpro150/bin:$LD_LIBRARY_PATH\nCOPY --from=builder /root/.cache /root/.cache\nCOPY setup.py /root/src/.\nCOPY requirements.txt /root/src/.\nCOPY README.md /root/src/.\nRUN pip3 install mujoco_py && pip3 install -e .[all] && rm -rf /root/.cache\nCOPY . /root/src\n```\n\n----------------------------------------\n\nTITLE: Importing Observation Filters in RL Coach\nDESCRIPTION: This snippet shows how to import various observation filters from the RL Coach library. These filters are used to preprocess and modify observation data in reinforcement learning tasks.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/filters/input_filters.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom rl_coach.filters.observation import ObservationClippingFilter, ObservationCropFilter, ObservationMoveAxisFilter, ObservationNormalizationFilter, ObservationReductionBySubPartsNameFilter, ObservationRescaleSizeByFactorFilter, ObservationRescaleToSizeFilter, ObservationRGBToYFilter, ObservationSqueezeFilter, ObservationStackingFilter, ObservationToUInt8Filter\n```\n\n----------------------------------------\n\nTITLE: Calculating Advantages for Actor-Critic in Python\nDESCRIPTION: This snippet demonstrates two methods for calculating advantages in the Actor-Critic algorithm: A_VALUE for direct estimation and GAE for Generalized Advantage Estimation. The advantages are used to accumulate gradients for training the network.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/policy_optimization/ac.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nA(s_t, a_t) = sum(gamma^(i-t) * r_i for i in range(t, t+k)) + gamma^k * V(s_t+k) - V(s_t)\n```\n\nLANGUAGE: python\nCODE:\n```\nL = -E[log(pi) * A]\n```\n\n----------------------------------------\n\nTITLE: Calculating DDQN Targets in Mixed Monte Carlo Algorithm\nDESCRIPTION: This formula calculates the targets for Double DQN (DDQN) in the Mixed Monte Carlo algorithm. It combines the immediate reward with the discounted Q-value of the next state.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/mmc.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\ny_t^{DDQN}=r(s_t,a_t )+\\gamma Q(s_{t+1},argmax_a Q(s_{t+1},a))\n```\n\n----------------------------------------\n\nTITLE: Calculating NAF Training Targets in Python\nDESCRIPTION: This snippet shows the mathematical formula for calculating the training targets in the NAF algorithm. It uses the current reward, discount factor, and the value of the next state.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/naf.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ny_t = r(s_t, a_t) + gamma * V(s_{t+1})\n```\n\n----------------------------------------\n\nTITLE: N-Step Q-Learning Reward Calculation Formula\nDESCRIPTION: Mathematical formula for calculating N-step Q targets by accumulating rewards from N consecutive steps, where k is T_max - State_Index for each state in the batch.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/n_step.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: math\nCODE:\n```\nR(s_t, a_t) = \\sum_{i=t}^{i=t + k - 1} \\gamma^{i-t}r_i +\\gamma^{k} V(s_{t+k})\n```\n\n----------------------------------------\n\nTITLE: Initializing Mixed Monte Carlo Algorithm Parameters in Python\nDESCRIPTION: This code snippet defines the MixedMonteCarloAlgorithmParameters class, which likely contains the configuration parameters for the Mixed Monte Carlo algorithm implementation in RL Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/components/agents/value_optimization/mmc.rst.txt#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: rl_coach.agents.mmc_agent.MixedMonteCarloAlgorithmParameters\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoClass Documentation for DQNAgent\nDESCRIPTION: ReStructuredText directive for automatically generating class documentation for the DQNAgent class from RL Coach, including all members and inherited members.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/test.rst.txt#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. important:: Its a note! in markdown!\n\n.. autoclass:: rl_coach.agents.dqn_agent.DQNAgent\n      :members:\n      :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Building Base Docker Container\nDESCRIPTION: Command to build the base Docker container for Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build -t coach-base:master -f docker/Dockerfile.base .\n```\n\n----------------------------------------\n\nTITLE: Cloning Coach Repository\nDESCRIPTION: Commands to clone the Coach repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/dist_usage.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone git@github.com:NervanaSystems/coach.git\n$ cd coach\n```\n\n----------------------------------------\n\nTITLE: Importing CARLA Environment\nDESCRIPTION: Auto-documented class for CARLA autonomous driving simulator environment.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs_raw/source/components/environments/index.rst#2025-04-23_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: rl_coach.environments.carla_environment.CarlaEnvironment\n```\n\n----------------------------------------\n\nTITLE: Building Base Docker Container\nDESCRIPTION: Command to build the base Docker container for Coach.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build -t coach-base:master -f docker/Dockerfile.base .\n```\n\n----------------------------------------\n\nTITLE: Setting Mujoco Key Environment Variable\nDESCRIPTION: Command to set the Mujoco key as an environment variable for Docker builds.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ export MUJOCO_KEY=<mujoco_key>\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Container to Registry\nDESCRIPTION: Command to push the built Docker container to a container registry.\nSOURCE: https://github.com/intellabs/coach/blob/master/docs/_sources/dist_usage.rst.txt#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ docker push <user-name>/<image-name>:<tag>\n```"
  }
]